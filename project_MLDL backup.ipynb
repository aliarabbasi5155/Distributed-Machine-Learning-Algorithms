{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVF2_gqB0EDy",
        "outputId": "d30ab9fc-a12a-41d6-f66a-a1fd1c26307d"
      },
      "outputs": [],
      "source": [
        "# Install missing dependencies\n",
        "# !pip install -q torchinfo torchmetrics wandb\n",
        "# !pip install torch\n",
        "# !pip install torchvision\n",
        "# !pip install numpy\n",
        "# !pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGJAcU7dTxJ1"
      },
      "source": [
        "### Import important libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bnVg3M4l0EyA"
      },
      "outputs": [],
      "source": [
        "# Import required libraries/code\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import wandb\n",
        "import pickle\n",
        "\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import  random_split, DataLoader, Subset\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2mj0Wd-T73T"
      },
      "source": [
        "### Build the directory for checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQg3CXCtgB-5",
        "outputId": "7b5a2e28-b427-4b64-f013-8c1977e97423"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybM87poAtHnl"
      },
      "source": [
        "### Set device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FC7BMY9z0H3L",
        "outputId": "3a9750af-17c1-4e3c-f816-20ff79567d9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QH26cxftUt3"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "knmNFhHg0CwT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define a class for handling CIFAR100 data\n",
        "class CIFAR100Data:\n",
        "    \"\"\"\n",
        "    A class used to represent the CIFAR100 dataset.\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    batch_size : int\n",
        "        the number of samples that will be propagated through the network simultaneously\n",
        "    original_train_set : torchvision.datasets.CIFAR100\n",
        "        the original training set downloaded from CIFAR100\n",
        "    original_test_set : torchvision.datasets.CIFAR100\n",
        "        the original test set downloaded from CIFAR100\n",
        "    train_set : torch.utils.data.Subset\n",
        "        the training set after splitting the original training set\n",
        "    validation_set : torch.utils.data.Subset\n",
        "        the validation set after splitting the original training set\n",
        "    test_set : torchvision.datasets.CIFAR100\n",
        "        the test set, same as the original test set\n",
        "    original_train_loader : torch.utils.data.DataLoader\n",
        "        data loader for the original training set\n",
        "    original_test_loader : torch.utils.data.DataLoader\n",
        "        data loader for the original test set\n",
        "    train_loader : torch.utils.data.DataLoader\n",
        "        data loader for the training set\n",
        "    validation_loader : torch.utils.data.DataLoader\n",
        "        data loader for the validation set\n",
        "    test_loader : torch.utils.data.DataLoader\n",
        "        data loader for the test set\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    compute_mean_std(loader)\n",
        "        Computes the mean and standard deviation of the images in the loader.\n",
        "    download_data()\n",
        "        Downloads the CIFAR100 dataset.\n",
        "    split_data(original_train_set, validation_ratio=0.2)\n",
        "        Splits the original training set into a training set and a validation set.\n",
        "    compute_statistics(train_set)\n",
        "        Computes the mean and standard deviation of the training set.\n",
        "    apply_transforms(train_mean, train_std, is_validation_set_available = False)\n",
        "        Defines and applies the transformations for the training set, validation set, and test set.\n",
        "    save_data(data_loader, data_set, file_name: str)\n",
        "        Saves the data loader to Google Drive.\n",
        "    load_data(file_name: str)\n",
        "        Loads the data loader from Google Drive.\n",
        "    create_and_save_data_loaders(train_set, test_set, validation_set=None)\n",
        "        Creates data loaders for the training, validation, and test sets and saves them to Google Drive.\n",
        "    prepare_data(validation_ratio = None)\n",
        "        Prepares the data by downloading it, splitting it, computing statistics, applying transforms, and creating and saving data loaders.\n",
        "    train_valid_test(validation_ratio=0.2)\n",
        "        Loads or prepares the data loaders for the training, validation, and test sets and returns them.\n",
        "    train_test()\n",
        "        Loads or prepares the data loaders for the original training and test sets and returns them.\n",
        "    iid_shards(num_shards=2)\n",
        "        Loads or prepares the data loaders for the shards of the original training set and returns them.\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size=64):\n",
        "        \"\"\"\n",
        "        Initialize the CIFAR100Data object with the given batch size.\n",
        "\n",
        "        Parameters:\n",
        "        batch_size (int): The size of the batches for the data loaders.\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.original_train_set = None\n",
        "        self.original_test_set = None\n",
        "        self.train_set = None\n",
        "        self.validation_set = None\n",
        "        self.test_set = None\n",
        "\n",
        "        self.original_train_loader = None\n",
        "        self.original_test_loader = None\n",
        "        self.train_loader = None\n",
        "        self.validation_loader = None\n",
        "        self.test_loader = None\n",
        "\n",
        "    def compute_mean_std(self, loader):\n",
        "        \"\"\"\n",
        "        Compute the mean and standard deviation of the images in the loader.\n",
        "\n",
        "        Parameters:\n",
        "        loader (DataLoader): The DataLoader object containing the image data.\n",
        "\n",
        "        Returns:\n",
        "        mean (Tensor): The mean of the images.\n",
        "        std (Tensor): The standard deviation of the images.\n",
        "        \"\"\"\n",
        "        channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "        for data, _ in loader:\n",
        "            channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
        "            channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
        "            num_batches += 1\n",
        "        mean = channels_sum / num_batches\n",
        "        std = torch.sqrt((channels_squared_sum / num_batches) - mean**2)\n",
        "        return mean, std\n",
        "\n",
        "    def download_data(self):\n",
        "        \"\"\"\n",
        "        Download the CIFAR100 dataset and store it in instance variables.\n",
        "        \"\"\"\n",
        "        self.original_train_set = CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "        self.original_test_set = CIFAR100(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "    def split_data(self, original_train_set, validation_ratio=0.2):\n",
        "        \"\"\"\n",
        "        Split the original training set into a training set and a validation set.\n",
        "\n",
        "        Parameters:\n",
        "        original_train_set (Dataset): The original training set.\n",
        "        validation_ratio (float): The ratio of the original training set to use for validation.\n",
        "\n",
        "        Returns:\n",
        "        train_set (Subset): The new training set.\n",
        "        validation_set (Subset): The new validation set.\n",
        "        \"\"\"\n",
        "        train_len = int(len(original_train_set) * (1 - validation_ratio))\n",
        "        val_len = len(original_train_set) - train_len\n",
        "        train_set, validation_set = random_split(original_train_set, [train_len, val_len])\n",
        "\n",
        "        return train_set, validation_set\n",
        "\n",
        "    def compute_statistics(self, train_set):\n",
        "        \"\"\"\n",
        "        Compute the mean and standard deviation of the train set.\n",
        "\n",
        "        Parameters:\n",
        "        train_set (Dataset/Subset): The training set.\n",
        "\n",
        "        Returns:\n",
        "        train_mean (Tensor): The mean of the training set.\n",
        "        train_std (Tensor): The standard deviation of the training set.\n",
        "        \"\"\"\n",
        "        trainloader_tmp = DataLoader(train_set, batch_size=self.batch_size, shuffle=True)\n",
        "        train_mean, train_std = self.compute_mean_std(trainloader_tmp)\n",
        "\n",
        "        return train_mean, train_std\n",
        "\n",
        "    def apply_transforms(self, train_mean, train_std, is_validation_set_available = False):\n",
        "        \"\"\"\n",
        "        Define the transformations for the training set, validation set, and test set\n",
        "        and apply them to the datasets.\n",
        "\n",
        "        Parameters:\n",
        "        train_mean (Tensor): The mean of the training set.\n",
        "        train_std (Tensor): The standard deviation of the training set.\n",
        "        is_validation_set_available (bool): Whether a validation set is available.\n",
        "        \"\"\"\n",
        "\n",
        "        # Transformations for the training set\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(train_mean.tolist(), train_std.tolist())\n",
        "        ])\n",
        "\n",
        "        # Transformations for the validation and test sets\n",
        "        test_val_transforms = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(train_mean.tolist(), train_std.tolist())\n",
        "        ])\n",
        "\n",
        "        # Apply the transformations to the datasets\n",
        "        if is_validation_set_available:\n",
        "            self.train_set.transform = train_transforms\n",
        "            self.validation_set.transform = test_val_transforms\n",
        "            self.test_set.transform = test_val_transforms\n",
        "        else:\n",
        "            self.original_train_set.transform = train_transforms\n",
        "            self.original_test_set.transform = test_val_transforms\n",
        "\n",
        "    def save_data(self, data_loader, file_name: str):\n",
        "        \"\"\"\n",
        "        Save the given data loader and data set to Google Drive.\n",
        "\n",
        "        Parameters:\n",
        "        data_loader (DataLoader): The data loader to save.\n",
        "        file_name (str): The name of the file to save the data loader and data set to.\n",
        "        \"\"\"\n",
        "        # Check if the directory exists, if not, create it\n",
        "        # if not os.path.exists('/content/gdrive/MyDrive/data/data_loaders/'):\n",
        "        #     os.makedirs('/content/gdrive/MyDrive/data/data_loaders/')\n",
        "\n",
        "        # Open each file in write-binary mode on Google Drive and dump (pickle) the data loader into it\n",
        "        # with open(f'/content/gdrive/MyDrive/data/data_loaders/{self.batch_size}_{file_name}_loader.pkl', 'wb') as f:\n",
        "        #     pickle.dump(data_loader, f)\n",
        "        if not os.path.exists(f'./data/data_loaders/{self.batch_size}/'):\n",
        "            os.makedirs(f'./data/data_loaders/{self.batch_size}/')\n",
        "        \n",
        "        open(f'./data/data_loaders/{self.batch_size}/{file_name}_loader.pkl', 'wb').write(pickle.dumps(data_loader))\n",
        "\n",
        "    def load_data(self, file_name: str):\n",
        "        \"\"\"\n",
        "        Load a data loader from Google Drive.\n",
        "\n",
        "        Parameters:\n",
        "        file_name (str): The name of the file to load the data loader from.\n",
        "\n",
        "        Returns:\n",
        "        data_loader (DataLoader): The loaded data loader, or None if the file does not exist.\n",
        "        \"\"\"\n",
        "        # Check if the file exists\n",
        "        # if os.path.exists(f'/content/gdrive/MyDrive/data/data_loaders/{self.batch_size}_{file_name}_loader.pkl'):\n",
        "        #     # If it exists, open the file in read-binary mode and load (unpickle) the data loader from it\n",
        "        #     with open(f'/content/gdrive/MyDrive/data/data_loaders/{self.batch_size}_{file_name}_loader.pkl', 'rb') as f:\n",
        "        #         return pickle.load(f)\n",
        "        # else:\n",
        "        #     return None\n",
        "        if os.path.exists(f'./data/data_loaders/{self.batch_size}/{file_name}_loader.pkl'):\n",
        "            return pickle.loads(open(f'./data/data_loaders/{self.batch_size}/{file_name}_loader.pkl', 'rb').read())\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def create_and_save_data_loaders(self, train_set, test_set, train_name: str, test_name: str, validation_set=None):\n",
        "        \"\"\"\n",
        "        Create data loaders for the training, validation, and test sets and save them to Google Drive.\n",
        "\n",
        "        Parameters:\n",
        "        train_set (Subset): The training set.\n",
        "        test_set (Subset): The test set.\n",
        "        validation_set (Subset, optional): The validation set.\n",
        "\n",
        "        Returns:\n",
        "        train_loader (DataLoader): The data loader for the training set.\n",
        "        validation_loader (DataLoader): The data loader for the validation set, if it exists.\n",
        "        test_loader (DataLoader): The data loader for the test set.\n",
        "        \"\"\"\n",
        "        train_loader = DataLoader(train_set, batch_size=self.batch_size, shuffle=True, num_workers =8)\n",
        "        test_loader = DataLoader(test_set, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "        # Save the newly created data loaders to Google Drive\n",
        "        self.save_data(train_loader, train_name)\n",
        "        self.save_data(test_loader, test_name)\n",
        "\n",
        "        if validation_set is not None:\n",
        "            validation_loader = DataLoader(validation_set, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
        "            self.save_data(validation_loader, 'validation')\n",
        "            return train_loader, validation_loader, test_loader\n",
        "\n",
        "        return train_loader, test_loader\n",
        "\n",
        "    def prepare_data(self, validation_ratio = None):\n",
        "        \"\"\"\n",
        "        Prepare the data by downloading it, splitting it into training, validation, and test sets,\n",
        "        computing statistics, applying transformations, and creating and saving data loaders.\n",
        "\n",
        "        Parameters:\n",
        "        validation_ratio (float, optional): The ratio of the original training set to use for validation.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.download_data()\n",
        "        except IOError:\n",
        "            print(\"Error downloading data\")\n",
        "            return\n",
        "\n",
        "        if validation_ratio is not None:\n",
        "            self.train_set, self.validation_set = self.split_data(self.original_train_set, validation_ratio)\n",
        "            if self.validation_set is None:\n",
        "                print(\"Validation set is not available\")\n",
        "                return\n",
        "            self.test_set = self.original_test_set\n",
        "            train_mean, train_std = self.compute_statistics(self.train_set)\n",
        "            self.apply_transforms(train_mean, train_std, is_validation_set_available = True)\n",
        "\n",
        "            # Create and save data loaders\n",
        "            self.train_loader, self.validation_loader, self.test_loader = self.create_and_save_data_loaders(self.train_set, self.test_set, 'train', 'test', self.validation_set)\n",
        "\n",
        "        else:\n",
        "            train_mean, train_std = self.compute_statistics(self.original_train_set)\n",
        "            self.apply_transforms(train_mean, train_std)\n",
        "\n",
        "            # Create and save data loaders\n",
        "            self.original_train_loader, self.original_test_loader = self.create_and_save_data_loaders(self.original_train_set, self.original_test_set, 'original_train', 'original_test')\n",
        "\n",
        "    def train_valid_test(self, validation_ratio=0.2):\n",
        "        \"\"\"\n",
        "        Load the training, validation, and test data loaders from Google Drive, or prepare the data if they do not exist.\n",
        "\n",
        "        Parameters:\n",
        "        validation_ratio (float): The ratio of the original training set to use for validation.\n",
        "\n",
        "        Returns:\n",
        "        train_loader (DataLoader): The data loader for the training set.\n",
        "        validation_loader (DataLoader): The data loader for the validation set.\n",
        "        test_loader (DataLoader): The data loader for the test set.\n",
        "        \"\"\"\n",
        "        self.train_loader = self.load_data('train')\n",
        "        self.validation_loader = self.load_data('validation')\n",
        "        self.test_loader = self.load_data('test')\n",
        "\n",
        "        if self.train_loader is None or self.validation_loader is None or self.test_loader is None:\n",
        "            self.prepare_data(validation_ratio)\n",
        "\n",
        "        # Return the data loaders\n",
        "        return self.train_loader, self.validation_loader, self.test_loader\n",
        "\n",
        "    def train_test(self):\n",
        "        \"\"\"\n",
        "        Load the original training and test data loaders from Google Drive, or prepare the data if they do not exist.\n",
        "\n",
        "        Returns:\n",
        "        original_train_loader (DataLoader): The data loader for the original training set.\n",
        "        original_test_loader (DataLoader): The data loader for the original test set.\n",
        "        \"\"\"\n",
        "        self.original_train_loader = self.load_data('original_train')\n",
        "        self.original_test_loader = self.load_data('original_test')\n",
        "\n",
        "        if self.original_train_loader is None or self.original_test_loader is None:\n",
        "            self.prepare_data()\n",
        "\n",
        "        # Return the data loaders\n",
        "        return self.original_train_loader, self.original_test_loader\n",
        "\n",
        "    def iid_shards(self, num_shards=2):\n",
        "        \"\"\"\n",
        "        Create or load independent and identically distributed (IID) shards of the original training set.\n",
        "\n",
        "        Parameters:\n",
        "        num_shards (int): The number of shards to create.\n",
        "\n",
        "        Returns:\n",
        "        shard_loaders (list of DataLoader): The data loaders for the shards.\n",
        "        \"\"\"\n",
        "        # Try to load the shard datasets and their corresponding data loaders from Google Drive\n",
        "        shard_loaders = []\n",
        "        for i in range(num_shards):\n",
        "            shard_loader = self.load_data(f'iid_sharding/{num_shards}_chunk_{i+1}')\n",
        "            if shard_loader is None:\n",
        "                break\n",
        "            shard_loaders.append(shard_loader)\n",
        "\n",
        "        # If all shard data loaders were successfully loaded, return them\n",
        "        if len(shard_loaders) == num_shards:\n",
        "            return shard_loaders\n",
        "\n",
        "        # If not all shard data loaders were successfully loaded, create them\n",
        "        if self.original_train_set is None:\n",
        "            self.download_data()\n",
        "            train_mean, train_std = self.compute_statistics(self.original_train_set)\n",
        "            self.apply_transforms(train_mean, train_std)\n",
        "\n",
        "        # Shuffle the indices\n",
        "        indices = torch.randperm(len(self.original_train_set))\n",
        "\n",
        "        # Split the indices into K chunks\n",
        "        shard_size = len(indices) // num_shards\n",
        "        shards = [indices[i*shard_size:(i+1)*shard_size] for i in range(num_shards)]\n",
        "\n",
        "        # Create subsets for each shard\n",
        "        shard_datasets = [Subset(self.original_train_set, shard) for shard in shards]\n",
        "\n",
        "        # Create data loaders for each shard\n",
        "        shard_loaders = [DataLoader(shard_dataset, batch_size=self.batch_size, shuffle=True) for shard_dataset in shard_datasets]\n",
        "\n",
        "        # Save each shard dataset and its corresponding data loader\n",
        "        for i, shard_loader in enumerate(shard_loaders):\n",
        "            self.save_data(shard_loader, f'iid_sharding_{num_shards}_chunk_{i+1}')\n",
        "\n",
        "        return shard_loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KE_ErnYB06JF"
      },
      "outputs": [],
      "source": [
        "data = CIFAR100Data()\n",
        "train_loader, validation_loader, test_loader = data.train_valid_test(validation_ratio=0.2)\n",
        "original_train_loader, original_test_loader = data.train_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fjzE9q4UOPU"
      },
      "source": [
        "### Define the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KaLfzyjo0Pzk"
      },
      "outputs": [],
      "source": [
        "# Define the model architecture\n",
        "# Check if it is LeNet-5 or similar to LeNet-5 we want similar.\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=0) # 28x28\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=0) # 10x10\n",
        "        self.pool = nn.MaxPool2d(2, 2) # 14x14 for conv1 and 5x5 for conv2\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(5 * 5 * 64, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x, indexing=None):\n",
        "        intermediate_outputs = {}\n",
        "\n",
        "        # Convolutional and pooling layers\n",
        "        x = F.relu(self.conv1(x))\n",
        "        if indexing == 'conv1':\n",
        "            return x\n",
        "        intermediate_outputs['conv1'] = x\n",
        "\n",
        "        x = self.pool(x)\n",
        "        if indexing == 'pool1':\n",
        "            return x\n",
        "        intermediate_outputs['pool1'] = x\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        if indexing == 'conv2':\n",
        "            return x\n",
        "        intermediate_outputs['conv2'] = x\n",
        "\n",
        "        x = self.pool(x)\n",
        "        if indexing == 'pool2':\n",
        "            return x\n",
        "        intermediate_outputs['pool2'] = x\n",
        "\n",
        "        # Flatten layer\n",
        "        x = self.flatten(x)\n",
        "        if indexing == 'flatten':\n",
        "            return x\n",
        "        intermediate_outputs['flatten'] = x\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        if indexing == 'fc1':\n",
        "            return x\n",
        "        intermediate_outputs['fc1'] = x\n",
        "\n",
        "        x = F.relu(self.fc2(x))\n",
        "        if indexing == 'fc2':\n",
        "            return x\n",
        "        intermediate_outputs['fc2'] = x\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        if indexing == 'fc3':\n",
        "            return x\n",
        "        intermediate_outputs['fc3'] = x\n",
        "\n",
        "        # If no indexing, return final output\n",
        "        return x if indexing is None else intermediate_outputs.get(indexing, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-4HtbSQ20ea5"
      },
      "outputs": [],
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEseiEKFt0mR"
      },
      "source": [
        "### Define some basic functions for train and test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JZNSgcBp0jEl"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, loss_fn, accumulation_steps=1, device=device, is_wandb=False):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    optimizer.zero_grad()  # Initialize gradients to zero at the start of each epoch\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        loss.backward()  # Backpropagation\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # Gradient accumulation\n",
        "        if (i+1) % accumulation_steps == 0 or i+1 == len(dataloader):\n",
        "            optimizer.step()  # Update model parameters\n",
        "            optimizer.zero_grad()  # Reset gradients to zero\n",
        "\n",
        "    train_loss = running_loss / len(dataloader)\n",
        "    train_accuracy = 100. * correct / total\n",
        "\n",
        "    if is_wandb:\n",
        "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": train_accuracy})\n",
        "\n",
        "    return train_loss, train_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "m6dkKxXh0i7n"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader, loss_fn, device = device, is_wandb= False):\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if is_wandb:\n",
        "              # Log the loss and accuracy values at each step\n",
        "              wandb.log({\n",
        "                  'Test Loss': test_loss / (batch_idx + 1),\n",
        "                  'Test Accuracy': 100 * correct / total\n",
        "              })\n",
        "\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_accuracy = 100. * correct / total\n",
        "\n",
        "    return test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZD7Vge_wYwvX"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, epoch, batch_size, optimizer_name, hyperparameters):\n",
        "    # dir_path = f\"/content/gdrive/MyDrive/{optimizer_name}/{batch_size}/\"\n",
        "    dir_path = f\"./train/{optimizer_name}/{batch_size}/\"\n",
        "    for key, value in hyperparameters.items():\n",
        "        dir_path = os.path.join(dir_path, f\"{key}_{value}/\")\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "    path = os.path.join(dir_path, f\"epoch_{epoch:03}.pt\")\n",
        "    torch.save(state, path)\n",
        "\n",
        "    # Get list of all files\n",
        "    list_of_files = glob.glob(os.path.join(dir_path, f\"epoch_*.pt\"))\n",
        "    # Sort files by creation time\n",
        "    list_of_files.sort(key=os.path.getctime)\n",
        "    # If there are more than 2 files, delete the second last one\n",
        "    if len(list_of_files) > 1:\n",
        "        os.remove(list_of_files[-2])\n",
        "\n",
        "def load_checkpoint(optimizer_name, batch_size, hyperparameters):\n",
        "    # dir_path = f\"/content/gdrive/MyDrive/{optimizer_name}/{batch_size}/\"\n",
        "    dir_path = f\"./train/{optimizer_name}/{batch_size}/\"\n",
        "    for key, value in hyperparameters.items():\n",
        "        dir_path = os.path.join(dir_path, f\"{key}_{value}/\")\n",
        "    list_of_files = glob.glob(os.path.join(dir_path, f\"epoch_*.pt\")) # * means all if need specific format then *.csv\n",
        "    if not list_of_files:  # I'm using glob which can return an empty list\n",
        "        return None\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    if os.path.isfile(latest_file):\n",
        "        return torch.load(latest_file)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9Yjx-yZf-Sbz"
      },
      "outputs": [],
      "source": [
        "def run_training(\n",
        "    num_epochs,\n",
        "    model,\n",
        "    trainloader,\n",
        "    validationloader,\n",
        "    testloader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    optimizer_name: str,\n",
        "    accumulation_steps=1,\n",
        "    hyperparameters=None,\n",
        "    is_wandb = False,\n",
        "    n_epochs_stop = None,\n",
        "    warmup_ratio = 0\n",
        "  ):\n",
        "\n",
        "  best_accuracy = 0\n",
        "  epochs_no_improve = 0\n",
        "  warmup_steps = int(warmup_ratio * num_epochs)\n",
        "  lr = optimizer.param_groups[0]['lr']\n",
        "  \n",
        "  start_epoch = 0\n",
        "  run_id = None\n",
        "  run_name = None\n",
        "  # Load checkpoint if available\n",
        "  checkpoint = load_checkpoint(optimizer_name, trainloader.batch_size, hyperparameters)\n",
        "  if checkpoint is not None:\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      start_epoch = checkpoint['epoch'] + 1\n",
        "      run_id = checkpoint.get('wandb_run_id', None)\n",
        "      run_name = checkpoint.get('wandb_run_name', None)\n",
        "\n",
        "  else:\n",
        "    run_name = \" \".join([f\"{key}={value}\" for key, value in hyperparameters.items()])\n",
        "\n",
        "  if is_wandb:\n",
        "    # Initialize a wandb run with the given hyperparameters\n",
        "    wandb.init(id=run_id, name=run_name, project=f'cifar100-training-mldl2024-baseline-{optimizer_name}',\n",
        "                   config=hyperparameters if hyperparameters is not None else {},\n",
        "                   resume=\"allow\", reinit=True)\n",
        "\n",
        "  for epoch in range(start_epoch, num_epochs):\n",
        "      if epoch < warmup_steps:\n",
        "        optimizer.param_groups[0]['lr'] = lr * epoch / warmup_steps\n",
        "      elif epoch == warmup_steps:\n",
        "        optimizer.param_groups[0]['lr'] = lr\n",
        "          \n",
        "      # Call the training function for each epoch\n",
        "      train_loss, train_acc = train(model, trainloader, optimizer, loss_fn, accumulation_steps, device, is_wandb=is_wandb)\n",
        "      print(f'[{epoch+1}/{num_epochs}]: Training Loss: {train_loss}, Training Accuracy: {train_acc}')\n",
        "      scheduler.step() # Update learning rate based on scheduler\n",
        "\n",
        "      # Save checkpoint\n",
        "      save_checkpoint({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),\n",
        "                    'loss': criterion,\n",
        "                    'wandb_run_id': wandb.run.id if is_wandb else None,\n",
        "                    'wandb_run_name': wandb.run.name if is_wandb else None,\n",
        "                    }, epoch, trainloader.batch_size, optimizer_name, hyperparameters)\n",
        "\n",
        "      if validationloader is not None:\n",
        "        val_loss, val_acc = test(model, validationloader, criterion)\n",
        "        print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_acc}')\n",
        "\n",
        "\n",
        "        if n_epochs_stop is not None:\n",
        "          if val_acc > best_accuracy:\n",
        "                best_accuracy = val_acc\n",
        "                epochs_no_improve = 0\n",
        "          else:\n",
        "              epochs_no_improve += 1\n",
        "              if epochs_no_improve == n_epochs_stop:\n",
        "                  print('Early stopping!')\n",
        "                  break\n",
        "\n",
        "  print('*'*70)\n",
        "  test_loss, test_acc = test(model, testloader, criterion, is_wandb = is_wandb)\n",
        "  print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')\n",
        "\n",
        "\n",
        "  # Finish the wandb run after all epochs\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taX_5ElNuKxC"
      },
      "source": [
        "# **Centeralised baseline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "8hxWmieIE7jF"
      },
      "outputs": [],
      "source": [
        "\n",
        "learning_rates = [1e-03, 1e-02]\n",
        "weight_decays = [1e-04, 1e-03, 4e-04]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SGDM (Stochastic Gradient Descent with Momentum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8587c890f2a847e293a101405d64fc0d",
            "175043eb44e44b4f9aefaf51aae6fb10",
            "db36d0cc7691440a9792ac779e161e62",
            "c6a68af25a2847e98201847781419670",
            "71e710b571d142a3a08cd233590b7171",
            "2671c3bf3cfd49aa9ad6841c289068cc",
            "45bdcbbed9ae47ffb24bd2a962345eaa",
            "bcbeb5bacc824f3aacd20e50d38bb70a",
            "2991ead42a2a4637b2902ca26837d74b",
            "9e0bb90e33ec4e198857c255080ef6f0",
            "77790c4dcd124f73907619689fb8d37c",
            "1fef0782f0804736881f03c2e31c3d64",
            "3606009883cf4212b7e1ed6e4f182845",
            "39aecb8ffe0645fbadccf8b0f8ed2486",
            "13e7a0db2aa74e35b7d1948224358d76",
            "7064cd1e5d794c258dbeeb63a9ae9f9b",
            "5cf516644d1a4ebf82c7e9dfa13e560b",
            "36a546b812d6403cbf5ed693318a9744",
            "aed05a015df946c7bae33226c3a8ddc6",
            "d601ef3ebb6c4b688ac53a65979aa445",
            "e4a5f301338a4e5185c042a22684ed4a",
            "48c7de7682f242fd86a234d1607187f4",
            "403dcd5f10de42ddbb67c8108e3bc34c",
            "a1d013e4d3e942b6b362ab5ade6d1a80",
            "e851c69b66ca4e8a80779a69435b855f",
            "4b733fc6800e4a0ab36bba52ff84f1a5",
            "dcbb358118e644a2a75d44d8adb6747b",
            "829bba2a5bb947a0bcf121c527902255",
            "7e2cc873f917444c833e152f7c2555de",
            "49727de0590f41dca6addf3f4ffe33f2",
            "60957598b6c64ab0987d583b788dd674",
            "e4ad17a7648245069c70f2f3fb79105a",
            "431cf75dd1cc4840b14f678876f45bc9",
            "a5f3871113994f1eb993c126d793d147",
            "7903555c1e884408b70c9951ecae84ec",
            "2d65b320aaed40cd87b8f78c99c614e7",
            "346856a22f0e45b2be0c14aa7902261f",
            "397b6d4daaf04a1181413adc5e30db0c",
            "ac87babb2f2a4781ab863c3b8f613807",
            "b3970cfd36ed4450b7c8f958a6499dc4",
            "df97494c2f654ffbaceb9ca801ff3113",
            "e1e5353cb6a1427a8b0b9d91d6067ef7",
            "77c702b6d9c04f00b8e624f31a591f01",
            "8795a05c814c469083b31be62e79289f",
            "829eee71e4d74403a22941b15dfcf9bc",
            "ab0b891f55f648cdbace9bc4540c51c6",
            "e6d67121f0ab4daf96d0bdd2c433259b",
            "3ee2c346d4d74faa915ef8315e3303c8"
          ]
        },
        "id": "9rGKhc7FJB8u",
        "outputId": "9b4eddc5-dc51-4af2-fdbe-4ba3175d3642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240611_235508-gw8sn909</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909' target=\"_blank\">learning_rate=0.001 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10/150]: Training Loss: 3.7955315692901612, Training Accuracy: 12.3825\n",
            "Validation Loss: 3.7704276780413974, Validation Accuracy: 12.97\n",
            "[11/150]: Training Loss: 3.693796951675415, Training Accuracy: 14.2675\n",
            "Validation Loss: 3.6661312990127857, Validation Accuracy: 14.76\n",
            "[12/150]: Training Loss: 3.5919596023559572, Training Accuracy: 16.0125\n",
            "Validation Loss: 3.5912161389733575, Validation Accuracy: 16.26\n",
            "[13/150]: Training Loss: 3.5116733703613283, Training Accuracy: 17.3775\n",
            "Validation Loss: 3.5196323546634356, Validation Accuracy: 16.95\n",
            "[14/150]: Training Loss: 3.4402521675109865, Training Accuracy: 18.835\n",
            "Validation Loss: 3.4998342930131656, Validation Accuracy: 17.78\n",
            "[15/150]: Training Loss: 3.374294019317627, Training Accuracy: 19.835\n",
            "Validation Loss: 3.412280123704558, Validation Accuracy: 19.05\n",
            "[16/150]: Training Loss: 3.3126377914428713, Training Accuracy: 20.9225\n",
            "Validation Loss: 3.3567575497232425, Validation Accuracy: 20.21\n",
            "[17/150]: Training Loss: 3.264142190551758, Training Accuracy: 21.7475\n",
            "Validation Loss: 3.3158142976700122, Validation Accuracy: 20.54\n",
            "[18/150]: Training Loss: 3.212448757171631, Training Accuracy: 22.925\n",
            "Validation Loss: 3.288792388454364, Validation Accuracy: 21.5\n",
            "[19/150]: Training Loss: 3.16608261680603, Training Accuracy: 23.6325\n",
            "Validation Loss: 3.247776625262704, Validation Accuracy: 22.28\n",
            "[20/150]: Training Loss: 3.1185284183502198, Training Accuracy: 24.565\n",
            "Validation Loss: 3.2615917640127194, Validation Accuracy: 21.82\n",
            "[21/150]: Training Loss: 3.0692719429016115, Training Accuracy: 25.24\n",
            "Validation Loss: 3.188312981538712, Validation Accuracy: 23.06\n",
            "[22/150]: Training Loss: 3.0227535221099853, Training Accuracy: 26.3125\n",
            "Validation Loss: 3.189725389905796, Validation Accuracy: 23.3\n",
            "[23/150]: Training Loss: 2.9822758083343506, Training Accuracy: 26.9425\n",
            "Validation Loss: 3.1865678501736587, Validation Accuracy: 24.06\n",
            "[24/150]: Training Loss: 2.9383605140686035, Training Accuracy: 27.7575\n",
            "Validation Loss: 3.1210442075304163, Validation Accuracy: 25.28\n",
            "[25/150]: Training Loss: 2.8941220123291016, Training Accuracy: 28.8275\n",
            "Validation Loss: 3.0783458576080904, Validation Accuracy: 26.38\n",
            "[26/150]: Training Loss: 2.846640188598633, Training Accuracy: 29.6625\n",
            "Validation Loss: 3.0489486524253895, Validation Accuracy: 26.15\n",
            "[27/150]: Training Loss: 2.803463589859009, Training Accuracy: 30.53\n",
            "Validation Loss: 3.0650304320511546, Validation Accuracy: 26.3\n",
            "[28/150]: Training Loss: 2.7623793058395387, Training Accuracy: 31.4175\n",
            "Validation Loss: 3.016696295161156, Validation Accuracy: 27.16\n",
            "[29/150]: Training Loss: 2.724277519607544, Training Accuracy: 31.9075\n",
            "Validation Loss: 3.001434786304547, Validation Accuracy: 27.8\n",
            "[30/150]: Training Loss: 2.6789874454498293, Training Accuracy: 33.0275\n",
            "Validation Loss: 3.016883801502787, Validation Accuracy: 27.3\n",
            "[31/150]: Training Loss: 2.639483213806152, Training Accuracy: 33.66\n",
            "Validation Loss: 2.98876147361318, Validation Accuracy: 28.67\n",
            "[32/150]: Training Loss: 2.5927667430877683, Training Accuracy: 34.5325\n",
            "Validation Loss: 2.994789396881298, Validation Accuracy: 27.54\n",
            "[33/150]: Training Loss: 2.5531089670181273, Training Accuracy: 35.2525\n",
            "Validation Loss: 2.997880709399084, Validation Accuracy: 28.54\n",
            "[34/150]: Training Loss: 2.5080887552261353, Training Accuracy: 36.16\n",
            "Validation Loss: 2.9557720460709493, Validation Accuracy: 29.05\n",
            "[35/150]: Training Loss: 2.470765545463562, Training Accuracy: 37.03\n",
            "Validation Loss: 2.960252163516488, Validation Accuracy: 29.2\n",
            "[36/150]: Training Loss: 2.4253519346237185, Training Accuracy: 37.9225\n",
            "Validation Loss: 2.9324972644733016, Validation Accuracy: 29.32\n",
            "[37/150]: Training Loss: 2.3800090463638304, Training Accuracy: 39.06\n",
            "Validation Loss: 2.9341223923264037, Validation Accuracy: 30.21\n",
            "[38/150]: Training Loss: 2.3413858253479005, Training Accuracy: 39.57\n",
            "Validation Loss: 2.9640257130762575, Validation Accuracy: 29.88\n",
            "[39/150]: Training Loss: 2.301597819519043, Training Accuracy: 40.4175\n",
            "Validation Loss: 3.0218158800890493, Validation Accuracy: 28.21\n",
            "[40/150]: Training Loss: 2.2570854791641235, Training Accuracy: 41.3725\n",
            "Validation Loss: 3.0180870347721562, Validation Accuracy: 29.27\n",
            "[41/150]: Training Loss: 2.2198365371704103, Training Accuracy: 42.375\n",
            "Validation Loss: 2.938889775306556, Validation Accuracy: 30.41\n",
            "[42/150]: Training Loss: 2.1788083906173705, Training Accuracy: 43.24\n",
            "Validation Loss: 2.931537726882157, Validation Accuracy: 30.24\n",
            "[43/150]: Training Loss: 2.130757416725159, Training Accuracy: 44.2975\n",
            "Validation Loss: 2.935111481672639, Validation Accuracy: 30.94\n",
            "[44/150]: Training Loss: 2.0930950580596925, Training Accuracy: 45.0275\n",
            "Validation Loss: 2.9872301232283283, Validation Accuracy: 30.88\n",
            "[45/150]: Training Loss: 2.0451603315353393, Training Accuracy: 46.14\n",
            "Validation Loss: 2.9818537857881777, Validation Accuracy: 30.2\n",
            "[46/150]: Training Loss: 2.004084638786316, Training Accuracy: 47.0275\n",
            "Validation Loss: 2.995785168022107, Validation Accuracy: 30.23\n",
            "[47/150]: Training Loss: 1.9692718086242675, Training Accuracy: 47.845\n",
            "Validation Loss: 3.0542795536624396, Validation Accuracy: 30.14\n",
            "[48/150]: Training Loss: 1.9216952280044555, Training Accuracy: 48.8675\n",
            "Validation Loss: 2.9834766873888148, Validation Accuracy: 31.12\n",
            "[49/150]: Training Loss: 1.8784988208770752, Training Accuracy: 49.9625\n",
            "Validation Loss: 3.0277192516691365, Validation Accuracy: 31.11\n",
            "[50/150]: Training Loss: 1.8381427211761474, Training Accuracy: 50.9275\n",
            "Validation Loss: 3.062338437244391, Validation Accuracy: 31.08\n",
            "[51/150]: Training Loss: 1.7901163187026978, Training Accuracy: 52.055\n",
            "Validation Loss: 3.1062804225144114, Validation Accuracy: 30.8\n",
            "[52/150]: Training Loss: 1.7526374937057496, Training Accuracy: 52.695\n",
            "Validation Loss: 3.121828522651818, Validation Accuracy: 31.1\n",
            "[53/150]: Training Loss: 1.7035991048812866, Training Accuracy: 53.87\n",
            "Validation Loss: 3.1324675629852683, Validation Accuracy: 30.77\n",
            "[54/150]: Training Loss: 1.6695509649276734, Training Accuracy: 54.6\n",
            "Validation Loss: 3.1344476976212423, Validation Accuracy: 30.74\n",
            "[55/150]: Training Loss: 1.6191298002243042, Training Accuracy: 55.795\n",
            "Validation Loss: 3.2295576159361823, Validation Accuracy: 29.87\n",
            "[56/150]: Training Loss: 1.5762285459518433, Training Accuracy: 56.7875\n",
            "Validation Loss: 3.242111048121361, Validation Accuracy: 30.56\n",
            "[57/150]: Training Loss: 1.5393764123916627, Training Accuracy: 58.005\n",
            "Validation Loss: 3.303940733526922, Validation Accuracy: 29.53\n",
            "[58/150]: Training Loss: 1.4886947209358214, Training Accuracy: 59.32\n",
            "Validation Loss: 3.306815612088343, Validation Accuracy: 30.63\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 12.757227435992782, Test Accuracy: 12.53\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>██▁▂▂▂▂▁▁▁▁▂▂▂▃▃▃▃▃▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>Test Loss</td><td>▁▄▆▇▇▇███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>Train Loss</td><td>██▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>12.53</td></tr><tr><td>Test Loss</td><td>12.75723</td></tr><tr><td>Train Accuracy</td><td>59.32</td></tr><tr><td>Train Loss</td><td>1.48869</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240611_235508-gw8sn909/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240611_235923-9i8aijvl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl' target=\"_blank\">learning_rate=0.001 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605354942321777, Training Accuracy: 1.0075\n",
            "Validation Loss: 4.604883777108162, Validation Accuracy: 0.9\n",
            "[2/150]: Training Loss: 4.602973918151855, Training Accuracy: 1.0475\n",
            "Validation Loss: 4.6016397293965525, Validation Accuracy: 0.9\n",
            "[3/150]: Training Loss: 4.596875128936768, Training Accuracy: 1.3425\n",
            "Validation Loss: 4.590152442834939, Validation Accuracy: 2.52\n",
            "[4/150]: Training Loss: 4.556022624969483, Training Accuracy: 2.9175\n",
            "Validation Loss: 4.472016604842653, Validation Accuracy: 3.55\n",
            "[5/150]: Training Loss: 4.280242394256592, Training Accuracy: 4.6625\n",
            "Validation Loss: 4.20257385673037, Validation Accuracy: 5.95\n",
            "[6/150]: Training Loss: 4.122820203781128, Training Accuracy: 6.5925\n",
            "Validation Loss: 4.098185727550725, Validation Accuracy: 6.72\n",
            "[7/150]: Training Loss: 4.0448949375152585, Training Accuracy: 7.655\n",
            "Validation Loss: 4.033748802865387, Validation Accuracy: 8.54\n",
            "[8/150]: Training Loss: 3.977578921508789, Training Accuracy: 9.06\n",
            "Validation Loss: 3.970679567118359, Validation Accuracy: 8.73\n",
            "[9/150]: Training Loss: 3.9144488010406495, Training Accuracy: 9.9875\n",
            "Validation Loss: 3.902484763200116, Validation Accuracy: 10.84\n",
            "[10/150]: Training Loss: 3.8504066452026366, Training Accuracy: 11.2025\n",
            "Validation Loss: 3.853308222096437, Validation Accuracy: 11.05\n",
            "[11/150]: Training Loss: 3.776761824798584, Training Accuracy: 12.605\n",
            "Validation Loss: 3.7683646602995076, Validation Accuracy: 13.16\n",
            "[12/150]: Training Loss: 3.6920017036437986, Training Accuracy: 14.09\n",
            "Validation Loss: 3.712872941023225, Validation Accuracy: 13.54\n",
            "[13/150]: Training Loss: 3.617317741394043, Training Accuracy: 15.2475\n",
            "Validation Loss: 3.619570322097487, Validation Accuracy: 14.95\n",
            "[14/150]: Training Loss: 3.5433271072387695, Training Accuracy: 16.7275\n",
            "Validation Loss: 3.5456171734317854, Validation Accuracy: 16.81\n",
            "[15/150]: Training Loss: 3.47977327041626, Training Accuracy: 17.6525\n",
            "Validation Loss: 3.503120666856219, Validation Accuracy: 17.12\n",
            "[16/150]: Training Loss: 3.4149503967285155, Training Accuracy: 18.85\n",
            "Validation Loss: 3.449874220380358, Validation Accuracy: 17.98\n",
            "[17/150]: Training Loss: 3.357447999191284, Training Accuracy: 20.3025\n",
            "Validation Loss: 3.3906334479143667, Validation Accuracy: 19.6\n",
            "[18/150]: Training Loss: 3.3055746353149416, Training Accuracy: 20.985\n",
            "Validation Loss: 3.34384480859064, Validation Accuracy: 20.15\n",
            "[19/150]: Training Loss: 3.249339769363403, Training Accuracy: 21.955\n",
            "Validation Loss: 3.3408105813773576, Validation Accuracy: 20.75\n",
            "[20/150]: Training Loss: 3.2052097175598147, Training Accuracy: 22.84\n",
            "Validation Loss: 3.3127595497544404, Validation Accuracy: 21.29\n",
            "[21/150]: Training Loss: 3.150541979598999, Training Accuracy: 23.8475\n",
            "Validation Loss: 3.267914258750381, Validation Accuracy: 22.13\n",
            "[22/150]: Training Loss: 3.1069109004974367, Training Accuracy: 24.64\n",
            "Validation Loss: 3.2093709350391557, Validation Accuracy: 23.03\n",
            "[23/150]: Training Loss: 3.0725105766296386, Training Accuracy: 25.155\n",
            "Validation Loss: 3.2081045861456805, Validation Accuracy: 23.23\n",
            "[24/150]: Training Loss: 3.0264828220367432, Training Accuracy: 26.165\n",
            "Validation Loss: 3.153175331225061, Validation Accuracy: 23.9\n",
            "[25/150]: Training Loss: 2.9784529972076417, Training Accuracy: 27.0575\n",
            "Validation Loss: 3.120233716478773, Validation Accuracy: 24.62\n",
            "[26/150]: Training Loss: 2.9407661106109617, Training Accuracy: 27.7025\n",
            "Validation Loss: 3.122152067293787, Validation Accuracy: 25.03\n",
            "[27/150]: Training Loss: 2.9034343135833742, Training Accuracy: 28.395\n",
            "Validation Loss: 3.062598638473802, Validation Accuracy: 25.75\n",
            "[28/150]: Training Loss: 2.859911437988281, Training Accuracy: 29.1425\n",
            "Validation Loss: 3.0851597072212558, Validation Accuracy: 26.24\n",
            "[29/150]: Training Loss: 2.825396254348755, Training Accuracy: 29.9575\n",
            "Validation Loss: 3.0798455939930713, Validation Accuracy: 25.39\n",
            "[30/150]: Training Loss: 2.783854722213745, Training Accuracy: 30.46\n",
            "Validation Loss: 2.994018077850342, Validation Accuracy: 27.4\n",
            "[31/150]: Training Loss: 2.7530001544952394, Training Accuracy: 31.445\n",
            "Validation Loss: 3.023551480785297, Validation Accuracy: 26.3\n",
            "[32/150]: Training Loss: 2.715887685775757, Training Accuracy: 31.8725\n",
            "Validation Loss: 2.966492583037941, Validation Accuracy: 27.56\n",
            "[33/150]: Training Loss: 2.673046026611328, Training Accuracy: 32.72\n",
            "Validation Loss: 2.9745204934648646, Validation Accuracy: 27.99\n",
            "[34/150]: Training Loss: 2.6407437208175657, Training Accuracy: 33.34\n",
            "Validation Loss: 2.942354439170497, Validation Accuracy: 29.0\n",
            "[35/150]: Training Loss: 2.5946556980133058, Training Accuracy: 34.7025\n",
            "Validation Loss: 2.930390673837844, Validation Accuracy: 28.88\n",
            "[36/150]: Training Loss: 2.560257712173462, Training Accuracy: 35.2025\n",
            "Validation Loss: 2.9051667292406607, Validation Accuracy: 29.88\n",
            "[37/150]: Training Loss: 2.5238379081726072, Training Accuracy: 35.76\n",
            "Validation Loss: 2.890098687190159, Validation Accuracy: 29.33\n",
            "[38/150]: Training Loss: 2.486947275352478, Training Accuracy: 36.69\n",
            "Validation Loss: 2.9050150206134577, Validation Accuracy: 29.6\n",
            "[39/150]: Training Loss: 2.452709559249878, Training Accuracy: 37.475\n",
            "Validation Loss: 2.8899250182376544, Validation Accuracy: 29.65\n",
            "[40/150]: Training Loss: 2.4208646841049193, Training Accuracy: 37.9575\n",
            "Validation Loss: 2.9285842643421924, Validation Accuracy: 29.49\n",
            "[41/150]: Training Loss: 2.3780534410476686, Training Accuracy: 38.925\n",
            "Validation Loss: 2.896095122501349, Validation Accuracy: 30.27\n",
            "[42/150]: Training Loss: 2.3445595056533812, Training Accuracy: 39.4975\n",
            "Validation Loss: 2.903249968389037, Validation Accuracy: 29.49\n",
            "[43/150]: Training Loss: 2.3128848968505857, Training Accuracy: 40.2375\n",
            "Validation Loss: 2.8930906854617366, Validation Accuracy: 30.18\n",
            "[44/150]: Training Loss: 2.275460436248779, Training Accuracy: 40.965\n",
            "Validation Loss: 2.847154028096776, Validation Accuracy: 30.73\n",
            "[45/150]: Training Loss: 2.2384806400299073, Training Accuracy: 41.705\n",
            "Validation Loss: 2.898577917912963, Validation Accuracy: 30.18\n",
            "[46/150]: Training Loss: 2.214237283706665, Training Accuracy: 42.415\n",
            "Validation Loss: 2.8367908441337053, Validation Accuracy: 31.94\n",
            "[47/150]: Training Loss: 2.174029080581665, Training Accuracy: 43.2825\n",
            "Validation Loss: 2.8599718254842577, Validation Accuracy: 31.4\n",
            "[48/150]: Training Loss: 2.134048504257202, Training Accuracy: 44.0225\n",
            "Validation Loss: 2.8570211207031444, Validation Accuracy: 31.32\n",
            "[49/150]: Training Loss: 2.1011022747039796, Training Accuracy: 44.7675\n",
            "Validation Loss: 2.876888126324696, Validation Accuracy: 31.24\n",
            "[50/150]: Training Loss: 2.066174629211426, Training Accuracy: 45.84\n",
            "Validation Loss: 2.873898560833779, Validation Accuracy: 31.01\n",
            "[51/150]: Training Loss: 2.0310755935668947, Training Accuracy: 46.2775\n",
            "Validation Loss: 2.8654664003165666, Validation Accuracy: 31.98\n",
            "[52/150]: Training Loss: 1.9909400806427002, Training Accuracy: 47.45\n",
            "Validation Loss: 2.8746210936528103, Validation Accuracy: 31.69\n",
            "[53/150]: Training Loss: 1.9579231163024902, Training Accuracy: 48.0525\n",
            "Validation Loss: 2.8748430902031576, Validation Accuracy: 31.57\n",
            "[54/150]: Training Loss: 1.9230639179229736, Training Accuracy: 48.785\n",
            "Validation Loss: 2.910501542364716, Validation Accuracy: 31.56\n",
            "[55/150]: Training Loss: 1.8876561931610107, Training Accuracy: 49.645\n",
            "Validation Loss: 2.8805068404811203, Validation Accuracy: 32.02\n",
            "[56/150]: Training Loss: 1.8495587772369384, Training Accuracy: 50.57\n",
            "Validation Loss: 2.9215301510634695, Validation Accuracy: 31.68\n",
            "[57/150]: Training Loss: 1.8102108228683471, Training Accuracy: 51.2975\n",
            "Validation Loss: 2.926473318391545, Validation Accuracy: 32.35\n",
            "[58/150]: Training Loss: 1.7818908670425415, Training Accuracy: 52.1325\n",
            "Validation Loss: 2.9246792929947, Validation Accuracy: 32.57\n",
            "[59/150]: Training Loss: 1.7456034435272216, Training Accuracy: 52.775\n",
            "Validation Loss: 2.984062427168439, Validation Accuracy: 31.53\n",
            "[60/150]: Training Loss: 1.706142727279663, Training Accuracy: 53.955\n",
            "Validation Loss: 2.9781496843714623, Validation Accuracy: 32.57\n",
            "[61/150]: Training Loss: 1.6744635740280152, Training Accuracy: 54.59\n",
            "Validation Loss: 3.0356672569444982, Validation Accuracy: 31.58\n",
            "[62/150]: Training Loss: 1.6349090488433837, Training Accuracy: 55.5975\n",
            "Validation Loss: 3.0081513763233354, Validation Accuracy: 32.09\n",
            "[63/150]: Training Loss: 1.6007863130569457, Training Accuracy: 56.41\n",
            "Validation Loss: 3.032350327558578, Validation Accuracy: 32.3\n",
            "[64/150]: Training Loss: 1.5609112140655517, Training Accuracy: 57.235\n",
            "Validation Loss: 3.077729872077893, Validation Accuracy: 31.42\n",
            "[65/150]: Training Loss: 1.5253083936691285, Training Accuracy: 58.1475\n",
            "Validation Loss: 3.0732312840261278, Validation Accuracy: 32.72\n",
            "[66/150]: Training Loss: 1.4928287380218506, Training Accuracy: 59.1925\n",
            "Validation Loss: 3.0616249688871346, Validation Accuracy: 31.97\n",
            "[67/150]: Training Loss: 1.4535140877723693, Training Accuracy: 60.1225\n",
            "Validation Loss: 3.124178699627044, Validation Accuracy: 32.15\n",
            "[68/150]: Training Loss: 1.4196137544631957, Training Accuracy: 60.7675\n",
            "Validation Loss: 3.2008153435530935, Validation Accuracy: 31.3\n",
            "[69/150]: Training Loss: 1.3877691086769104, Training Accuracy: 61.4575\n",
            "Validation Loss: 3.1977471834535054, Validation Accuracy: 31.22\n",
            "[70/150]: Training Loss: 1.3564362874031066, Training Accuracy: 62.2125\n",
            "Validation Loss: 3.2483551836317512, Validation Accuracy: 31.49\n",
            "[71/150]: Training Loss: 1.3180213891029358, Training Accuracy: 63.6275\n",
            "Validation Loss: 3.3253093616218323, Validation Accuracy: 31.54\n",
            "[72/150]: Training Loss: 1.2782609523773194, Training Accuracy: 64.6325\n",
            "Validation Loss: 3.300645703722717, Validation Accuracy: 32.03\n",
            "[73/150]: Training Loss: 1.2435202094078064, Training Accuracy: 65.5975\n",
            "Validation Loss: 3.340803805430224, Validation Accuracy: 31.19\n",
            "[74/150]: Training Loss: 1.2111891516685487, Training Accuracy: 66.105\n",
            "Validation Loss: 3.3482626076716526, Validation Accuracy: 31.98\n",
            "[75/150]: Training Loss: 1.1782402634620666, Training Accuracy: 67.3425\n",
            "Validation Loss: 3.442917116128715, Validation Accuracy: 31.68\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 15.762403002210483, Test Accuracy: 12.36\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▄█▂▁▂▁▂▂▁▁▁▂▂▂▂▂▃▂▃▂▂▃▃▃▃▃▃▄▄▃▄▄▃▃▄▃▃▃▄▃</td></tr><tr><td>Test Loss</td><td>▃▁▄▄▆▇▇█▇▇▇▇▇▇█▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>███▇▇▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>12.36</td></tr><tr><td>Test Loss</td><td>15.7624</td></tr><tr><td>Train Accuracy</td><td>67.3425</td></tr><tr><td>Train Loss</td><td>1.17824</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240611_235923-9i8aijvl/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_002202-8lcpcbs2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2' target=\"_blank\">learning_rate=0.001 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605505518341064, Training Accuracy: 1.0675\n",
            "Validation Loss: 4.60469646514601, Validation Accuracy: 0.9\n",
            "[2/150]: Training Loss: 4.603742761993408, Training Accuracy: 1.0775\n",
            "Validation Loss: 4.602513228252435, Validation Accuracy: 0.97\n",
            "[3/150]: Training Loss: 4.60020831451416, Training Accuracy: 1.42\n",
            "Validation Loss: 4.596951505940432, Validation Accuracy: 1.39\n",
            "[4/150]: Training Loss: 4.590583048248291, Training Accuracy: 1.345\n",
            "Validation Loss: 4.579934718502555, Validation Accuracy: 1.81\n",
            "[5/150]: Training Loss: 4.522595316314697, Training Accuracy: 2.9\n",
            "Validation Loss: 4.390095902096694, Validation Accuracy: 3.43\n",
            "[6/150]: Training Loss: 4.228593465805054, Training Accuracy: 5.2625\n",
            "Validation Loss: 4.173950313762495, Validation Accuracy: 5.48\n",
            "[7/150]: Training Loss: 4.1090137573242185, Training Accuracy: 6.88\n",
            "Validation Loss: 4.087003700292794, Validation Accuracy: 7.44\n",
            "[8/150]: Training Loss: 4.035317427825928, Training Accuracy: 8.06\n",
            "Validation Loss: 4.015521998618059, Validation Accuracy: 8.52\n",
            "[9/150]: Training Loss: 3.960421589279175, Training Accuracy: 9.445\n",
            "Validation Loss: 3.9580009849208175, Validation Accuracy: 9.3\n",
            "[10/150]: Training Loss: 3.8591443000793455, Training Accuracy: 11.34\n",
            "Validation Loss: 3.84393062105604, Validation Accuracy: 11.61\n",
            "[11/150]: Training Loss: 3.7639447425842287, Training Accuracy: 12.9725\n",
            "Validation Loss: 3.7741037219952625, Validation Accuracy: 12.76\n",
            "[12/150]: Training Loss: 3.6964004222869873, Training Accuracy: 14.225\n",
            "Validation Loss: 3.6935869903321477, Validation Accuracy: 14.02\n",
            "[13/150]: Training Loss: 3.6246248401641847, Training Accuracy: 15.4325\n",
            "Validation Loss: 3.6313245053503924, Validation Accuracy: 15.35\n",
            "[14/150]: Training Loss: 3.5586987380981445, Training Accuracy: 16.58\n",
            "Validation Loss: 3.570704687932494, Validation Accuracy: 16.63\n",
            "[15/150]: Training Loss: 3.492145662689209, Training Accuracy: 17.56\n",
            "Validation Loss: 3.5174384117126465, Validation Accuracy: 17.05\n",
            "[16/150]: Training Loss: 3.4223767150878905, Training Accuracy: 18.875\n",
            "Validation Loss: 3.4360159157188077, Validation Accuracy: 18.52\n",
            "[17/150]: Training Loss: 3.3623727890014647, Training Accuracy: 19.63\n",
            "Validation Loss: 3.3825773327213944, Validation Accuracy: 19.78\n",
            "[18/150]: Training Loss: 3.3074194034576414, Training Accuracy: 20.8975\n",
            "Validation Loss: 3.3404667316728336, Validation Accuracy: 20.19\n",
            "[19/150]: Training Loss: 3.251063732147217, Training Accuracy: 21.8975\n",
            "Validation Loss: 3.312105529627223, Validation Accuracy: 20.9\n",
            "[20/150]: Training Loss: 3.204967420578003, Training Accuracy: 22.7225\n",
            "Validation Loss: 3.2530917027953326, Validation Accuracy: 22.31\n",
            "[21/150]: Training Loss: 3.15532322807312, Training Accuracy: 23.64\n",
            "Validation Loss: 3.229210142876692, Validation Accuracy: 22.45\n",
            "[22/150]: Training Loss: 3.1103267768859864, Training Accuracy: 24.38\n",
            "Validation Loss: 3.213347544336015, Validation Accuracy: 23.38\n",
            "[23/150]: Training Loss: 3.0626929641723635, Training Accuracy: 25.55\n",
            "Validation Loss: 3.181753307391124, Validation Accuracy: 24.04\n",
            "[24/150]: Training Loss: 3.01508925819397, Training Accuracy: 26.1675\n",
            "Validation Loss: 3.159736381214895, Validation Accuracy: 24.07\n",
            "[25/150]: Training Loss: 2.977297193527222, Training Accuracy: 26.95\n",
            "Validation Loss: 3.139521881273598, Validation Accuracy: 24.65\n",
            "[26/150]: Training Loss: 2.9348321487426756, Training Accuracy: 27.76\n",
            "Validation Loss: 3.129030877617514, Validation Accuracy: 25.02\n",
            "[27/150]: Training Loss: 2.892203674316406, Training Accuracy: 28.71\n",
            "Validation Loss: 3.1089744279339055, Validation Accuracy: 24.88\n",
            "[28/150]: Training Loss: 2.8514965145111084, Training Accuracy: 29.41\n",
            "Validation Loss: 3.0390360476864373, Validation Accuracy: 26.37\n",
            "[29/150]: Training Loss: 2.8064930957794187, Training Accuracy: 30.3275\n",
            "Validation Loss: 3.0866621254356046, Validation Accuracy: 26.25\n",
            "[30/150]: Training Loss: 2.771508701324463, Training Accuracy: 30.8325\n",
            "Validation Loss: 3.034489528388734, Validation Accuracy: 26.79\n",
            "[31/150]: Training Loss: 2.726985428237915, Training Accuracy: 32.06\n",
            "Validation Loss: 2.994590279403006, Validation Accuracy: 27.49\n",
            "[32/150]: Training Loss: 2.693068378448486, Training Accuracy: 32.6375\n",
            "Validation Loss: 2.991791993949064, Validation Accuracy: 27.57\n",
            "[33/150]: Training Loss: 2.6538521224975584, Training Accuracy: 33.305\n",
            "Validation Loss: 2.9930253712234984, Validation Accuracy: 27.59\n",
            "[34/150]: Training Loss: 2.6168819108963013, Training Accuracy: 33.9675\n",
            "Validation Loss: 2.9458029012011875, Validation Accuracy: 28.63\n",
            "[35/150]: Training Loss: 2.5724296060562133, Training Accuracy: 34.885\n",
            "Validation Loss: 2.9706625437280936, Validation Accuracy: 28.37\n",
            "[36/150]: Training Loss: 2.532205513381958, Training Accuracy: 35.7275\n",
            "Validation Loss: 3.0205048026552626, Validation Accuracy: 27.62\n",
            "[37/150]: Training Loss: 2.501378486442566, Training Accuracy: 36.655\n",
            "Validation Loss: 2.9459367466580337, Validation Accuracy: 29.16\n",
            "[38/150]: Training Loss: 2.4599771045684813, Training Accuracy: 37.265\n",
            "Validation Loss: 2.9162613555883907, Validation Accuracy: 29.42\n",
            "[39/150]: Training Loss: 2.416532469558716, Training Accuracy: 38.265\n",
            "Validation Loss: 2.9226647676176327, Validation Accuracy: 29.71\n",
            "[40/150]: Training Loss: 2.377360425758362, Training Accuracy: 39.1\n",
            "Validation Loss: 2.912419487716286, Validation Accuracy: 30.24\n",
            "[41/150]: Training Loss: 2.3416698053359983, Training Accuracy: 39.7725\n",
            "Validation Loss: 2.9336505536061184, Validation Accuracy: 29.72\n",
            "[42/150]: Training Loss: 2.302257305908203, Training Accuracy: 40.6025\n",
            "Validation Loss: 2.9164519552971906, Validation Accuracy: 30.29\n",
            "[43/150]: Training Loss: 2.2648201442718507, Training Accuracy: 41.4425\n",
            "Validation Loss: 2.917926338068239, Validation Accuracy: 30.33\n",
            "[44/150]: Training Loss: 2.2238695373535156, Training Accuracy: 42.1875\n",
            "Validation Loss: 2.920909085091512, Validation Accuracy: 30.59\n",
            "[45/150]: Training Loss: 2.186175981903076, Training Accuracy: 43.1875\n",
            "Validation Loss: 2.979639395027404, Validation Accuracy: 29.65\n",
            "[46/150]: Training Loss: 2.1421424005508425, Training Accuracy: 44.26\n",
            "Validation Loss: 2.955421671745883, Validation Accuracy: 29.75\n",
            "[47/150]: Training Loss: 2.1072659519195556, Training Accuracy: 44.7325\n",
            "Validation Loss: 2.9603501687383957, Validation Accuracy: 30.38\n",
            "[48/150]: Training Loss: 2.0704134420394897, Training Accuracy: 45.5625\n",
            "Validation Loss: 2.9289260366160397, Validation Accuracy: 30.82\n",
            "[49/150]: Training Loss: 2.0253924531936645, Training Accuracy: 46.8725\n",
            "Validation Loss: 2.9537550051500845, Validation Accuracy: 30.64\n",
            "[50/150]: Training Loss: 1.9894214233398437, Training Accuracy: 47.5425\n",
            "Validation Loss: 2.992744015280608, Validation Accuracy: 30.54\n",
            "[51/150]: Training Loss: 1.9478775398254395, Training Accuracy: 48.1625\n",
            "Validation Loss: 3.0440484505550116, Validation Accuracy: 30.87\n",
            "[52/150]: Training Loss: 1.9146845523834228, Training Accuracy: 48.9825\n",
            "Validation Loss: 2.9890038989911414, Validation Accuracy: 31.24\n",
            "[53/150]: Training Loss: 1.8723485668182374, Training Accuracy: 50.01\n",
            "Validation Loss: 2.993290390937951, Validation Accuracy: 31.22\n",
            "[54/150]: Training Loss: 1.8348793195724487, Training Accuracy: 50.52\n",
            "Validation Loss: 3.0035920021640266, Validation Accuracy: 31.61\n",
            "[55/150]: Training Loss: 1.7904879375457763, Training Accuracy: 52.175\n",
            "Validation Loss: 3.0537699574877504, Validation Accuracy: 30.68\n",
            "[56/150]: Training Loss: 1.7518022777557374, Training Accuracy: 52.58\n",
            "Validation Loss: 3.0826165934277188, Validation Accuracy: 30.36\n",
            "[57/150]: Training Loss: 1.723713356781006, Training Accuracy: 53.2725\n",
            "Validation Loss: 3.0921939679771473, Validation Accuracy: 30.79\n",
            "[58/150]: Training Loss: 1.673542138671875, Training Accuracy: 54.7525\n",
            "Validation Loss: 3.114953031965122, Validation Accuracy: 30.97\n",
            "[59/150]: Training Loss: 1.6370127866744995, Training Accuracy: 55.3475\n",
            "Validation Loss: 3.1347598710637183, Validation Accuracy: 31.13\n",
            "[60/150]: Training Loss: 1.595728307723999, Training Accuracy: 56.3775\n",
            "Validation Loss: 3.211446317138186, Validation Accuracy: 30.86\n",
            "[61/150]: Training Loss: 1.5595929719924926, Training Accuracy: 57.445\n",
            "Validation Loss: 3.1916901260424573, Validation Accuracy: 31.69\n",
            "[62/150]: Training Loss: 1.5153404804229735, Training Accuracy: 58.605\n",
            "Validation Loss: 3.2764444821959087, Validation Accuracy: 30.21\n",
            "[63/150]: Training Loss: 1.4795546808242799, Training Accuracy: 59.165\n",
            "Validation Loss: 3.2651574581292024, Validation Accuracy: 31.32\n",
            "[64/150]: Training Loss: 1.443065357875824, Training Accuracy: 60.3525\n",
            "Validation Loss: 3.2850031807164477, Validation Accuracy: 30.88\n",
            "[65/150]: Training Loss: 1.3952457666397096, Training Accuracy: 61.3125\n",
            "Validation Loss: 3.334014499263399, Validation Accuracy: 30.51\n",
            "[66/150]: Training Loss: 1.3616252402305602, Training Accuracy: 62.2875\n",
            "Validation Loss: 3.3781587804199025, Validation Accuracy: 30.85\n",
            "[67/150]: Training Loss: 1.3209700021743775, Training Accuracy: 63.1475\n",
            "Validation Loss: 3.4392695958447304, Validation Accuracy: 30.92\n",
            "[68/150]: Training Loss: 1.2754105567932128, Training Accuracy: 64.4\n",
            "Validation Loss: 3.487312679837464, Validation Accuracy: 30.89\n",
            "[69/150]: Training Loss: 1.2461662047386168, Training Accuracy: 65.2125\n",
            "Validation Loss: 3.470762025019166, Validation Accuracy: 30.79\n",
            "[70/150]: Training Loss: 1.2068843828201294, Training Accuracy: 66.345\n",
            "Validation Loss: 3.5463279022532666, Validation Accuracy: 30.57\n",
            "[71/150]: Training Loss: 1.174416847038269, Training Accuracy: 66.87\n",
            "Validation Loss: 3.5970414428953914, Validation Accuracy: 30.5\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 17.28561769473325, Test Accuracy: 11.15\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▆▁▃▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▂▃▃▃▃▃▃▃▃▃▄▄▃▃▄▄▄▄▄▄</td></tr><tr><td>Test Loss</td><td>▂▁▅▄▆▆▆▇▇▆▆▇█▇██▇█████▇▇▇███████████████</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>███▇▇▇▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>11.15</td></tr><tr><td>Test Loss</td><td>17.28562</td></tr><tr><td>Train Accuracy</td><td>66.87</td></tr><tr><td>Train Loss</td><td>1.17442</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_002202-8lcpcbs2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.0001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_002809-yufpefeq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq' target=\"_blank\">learning_rate=0.01 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.394053651046753, Training Accuracy: 3.235\n",
            "Validation Loss: 4.1179144488778086, Validation Accuracy: 6.51\n",
            "[2/150]: Training Loss: 3.940752759170532, Training Accuracy: 9.1825\n",
            "Validation Loss: 3.793641860318032, Validation Accuracy: 10.96\n",
            "[3/150]: Training Loss: 3.607163610458374, Training Accuracy: 14.85\n",
            "Validation Loss: 3.445422892357893, Validation Accuracy: 18.4\n",
            "[4/150]: Training Loss: 3.32619926071167, Training Accuracy: 19.315\n",
            "Validation Loss: 3.202780070578217, Validation Accuracy: 21.55\n",
            "[5/150]: Training Loss: 3.0974891372680666, Training Accuracy: 23.9725\n",
            "Validation Loss: 3.0388081802684033, Validation Accuracy: 24.97\n",
            "[6/150]: Training Loss: 2.901975968170166, Training Accuracy: 27.42\n",
            "Validation Loss: 3.0498315498327755, Validation Accuracy: 25.89\n",
            "[7/150]: Training Loss: 2.72861491394043, Training Accuracy: 31.1425\n",
            "Validation Loss: 2.817266490049423, Validation Accuracy: 30.26\n",
            "[8/150]: Training Loss: 2.565713974761963, Training Accuracy: 33.8575\n",
            "Validation Loss: 2.7873742094465124, Validation Accuracy: 30.1\n",
            "[9/150]: Training Loss: 2.394580401802063, Training Accuracy: 37.745\n",
            "Validation Loss: 2.714815883879449, Validation Accuracy: 32.43\n",
            "[10/150]: Training Loss: 2.2325665201187133, Training Accuracy: 41.275\n",
            "Validation Loss: 2.6796328748107716, Validation Accuracy: 33.37\n",
            "[11/150]: Training Loss: 2.0732334920883178, Training Accuracy: 44.825\n",
            "Validation Loss: 2.743416508291937, Validation Accuracy: 33.16\n",
            "[12/150]: Training Loss: 1.9020709432601928, Training Accuracy: 48.52\n",
            "Validation Loss: 2.761919692823082, Validation Accuracy: 34.09\n",
            "[13/150]: Training Loss: 1.723027162361145, Training Accuracy: 52.47\n",
            "Validation Loss: 2.781366317894808, Validation Accuracy: 33.87\n",
            "[14/150]: Training Loss: 1.5519161633491516, Training Accuracy: 56.5925\n",
            "Validation Loss: 2.8901124114443544, Validation Accuracy: 32.73\n",
            "[15/150]: Training Loss: 1.3848727501869202, Training Accuracy: 60.6225\n",
            "Validation Loss: 3.061296709024223, Validation Accuracy: 32.5\n",
            "[16/150]: Training Loss: 1.2385452840805053, Training Accuracy: 64.0375\n",
            "Validation Loss: 3.1685607524434474, Validation Accuracy: 33.0\n",
            "[17/150]: Training Loss: 1.0766600145339966, Training Accuracy: 68.1675\n",
            "Validation Loss: 3.470290554556877, Validation Accuracy: 32.52\n",
            "[18/150]: Training Loss: 0.953560617685318, Training Accuracy: 71.2375\n",
            "Validation Loss: 3.737919813508441, Validation Accuracy: 32.63\n",
            "[19/150]: Training Loss: 0.8452390188217163, Training Accuracy: 74.1975\n",
            "Validation Loss: 3.9338995074010956, Validation Accuracy: 32.13\n",
            "[20/150]: Training Loss: 0.7454726006031036, Training Accuracy: 76.935\n",
            "Validation Loss: 4.16999766325495, Validation Accuracy: 32.08\n",
            "[21/150]: Training Loss: 0.6607641342639923, Training Accuracy: 79.455\n",
            "Validation Loss: 4.340858131457287, Validation Accuracy: 31.19\n",
            "[22/150]: Training Loss: 0.5830736963987351, Training Accuracy: 81.7975\n",
            "Validation Loss: 4.7213960696177875, Validation Accuracy: 32.35\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 31.791706814128123, Test Accuracy: 14.4\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▂▃█▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>Test Loss</td><td>▁▅█▇█▇██▇▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇███▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>14.4</td></tr><tr><td>Test Loss</td><td>31.79171</td></tr><tr><td>Train Accuracy</td><td>81.7975</td></tr><tr><td>Train Loss</td><td>0.58307</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_002809-yufpefeq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_003027-5ijvpyn3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3' target=\"_blank\">learning_rate=0.01 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.452998904800415, Training Accuracy: 2.615\n",
            "Validation Loss: 4.1346032179085315, Validation Accuracy: 5.28\n",
            "[2/150]: Training Loss: 3.9527258255004885, Training Accuracy: 8.88\n",
            "Validation Loss: 3.7435999432946465, Validation Accuracy: 11.55\n",
            "[3/150]: Training Loss: 3.6381911922454835, Training Accuracy: 14.3725\n",
            "Validation Loss: 3.50478922181828, Validation Accuracy: 15.58\n",
            "[4/150]: Training Loss: 3.3785652015686036, Training Accuracy: 18.865\n",
            "Validation Loss: 3.3451961089091697, Validation Accuracy: 19.0\n",
            "[5/150]: Training Loss: 3.17810930519104, Training Accuracy: 22.375\n",
            "Validation Loss: 3.1223374703887163, Validation Accuracy: 23.6\n",
            "[6/150]: Training Loss: 3.0013810291290284, Training Accuracy: 25.565\n",
            "Validation Loss: 3.015145599462424, Validation Accuracy: 25.56\n",
            "[7/150]: Training Loss: 2.8544215950012206, Training Accuracy: 28.7175\n",
            "Validation Loss: 2.917889818264421, Validation Accuracy: 28.04\n",
            "[8/150]: Training Loss: 2.6971351528167724, Training Accuracy: 31.3825\n",
            "Validation Loss: 2.8436176351680875, Validation Accuracy: 29.6\n",
            "[9/150]: Training Loss: 2.567851602554321, Training Accuracy: 34.1025\n",
            "Validation Loss: 2.748332465530201, Validation Accuracy: 31.45\n",
            "[10/150]: Training Loss: 2.422238304901123, Training Accuracy: 37.085\n",
            "Validation Loss: 2.7193879473740887, Validation Accuracy: 32.08\n",
            "[11/150]: Training Loss: 2.3000989530563354, Training Accuracy: 39.695\n",
            "Validation Loss: 2.736810536141608, Validation Accuracy: 32.35\n",
            "[12/150]: Training Loss: 2.170335920906067, Training Accuracy: 42.5\n",
            "Validation Loss: 2.668315727239961, Validation Accuracy: 34.0\n",
            "[13/150]: Training Loss: 2.037977534675598, Training Accuracy: 45.47\n",
            "Validation Loss: 2.6606684786498924, Validation Accuracy: 34.72\n",
            "[14/150]: Training Loss: 1.9109795570373536, Training Accuracy: 48.245\n",
            "Validation Loss: 2.677434118690005, Validation Accuracy: 34.85\n",
            "[15/150]: Training Loss: 1.7840767404556275, Training Accuracy: 50.9475\n",
            "Validation Loss: 2.6786925458604363, Validation Accuracy: 35.46\n",
            "[16/150]: Training Loss: 1.6501886499404907, Training Accuracy: 54.325\n",
            "Validation Loss: 2.749260063383989, Validation Accuracy: 34.73\n",
            "[17/150]: Training Loss: 1.5220398645401, Training Accuracy: 57.045\n",
            "Validation Loss: 2.9064030396710536, Validation Accuracy: 34.13\n",
            "[18/150]: Training Loss: 1.3921052120208741, Training Accuracy: 60.4275\n",
            "Validation Loss: 2.978949681968446, Validation Accuracy: 33.67\n",
            "[19/150]: Training Loss: 1.2853214281082153, Training Accuracy: 62.9225\n",
            "Validation Loss: 3.0623086941470006, Validation Accuracy: 33.54\n",
            "[20/150]: Training Loss: 1.1686985822677611, Training Accuracy: 65.5875\n",
            "Validation Loss: 3.099342126755198, Validation Accuracy: 34.34\n",
            "[21/150]: Training Loss: 1.0535273086547852, Training Accuracy: 68.77\n",
            "Validation Loss: 3.2730220791640554, Validation Accuracy: 32.71\n",
            "[22/150]: Training Loss: 0.9473227613449097, Training Accuracy: 71.6425\n",
            "Validation Loss: 3.5567993069909942, Validation Accuracy: 33.18\n",
            "[23/150]: Training Loss: 0.8736372189521789, Training Accuracy: 73.5275\n",
            "Validation Loss: 3.4931609501504592, Validation Accuracy: 33.71\n",
            "[24/150]: Training Loss: 0.7797131684780121, Training Accuracy: 76.18\n",
            "Validation Loss: 3.6084122536288703, Validation Accuracy: 31.79\n",
            "[25/150]: Training Loss: 0.72144877743721, Training Accuracy: 77.925\n",
            "Validation Loss: 3.6706639156220064, Validation Accuracy: 33.34\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 17.253872677019448, Test Accuracy: 18.45\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▄█▄▅▃▄▄▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Test Loss</td><td>▁▂▆▄▃▅▆▇▇▆▆▇▇▇███▇▇▇██▇▇█▇██████████████</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>18.45</td></tr><tr><td>Test Loss</td><td>17.25387</td></tr><tr><td>Train Accuracy</td><td>77.925</td></tr><tr><td>Train Loss</td><td>0.72145</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_003027-5ijvpyn3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_003346-pvvd2my8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8' target=\"_blank\">learning_rate=0.01 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.436210793685913, Training Accuracy: 2.8025\n",
            "Validation Loss: 4.1512272403498365, Validation Accuracy: 5.38\n",
            "[2/150]: Training Loss: 3.943741687011719, Training Accuracy: 9.175\n",
            "Validation Loss: 3.791590502307673, Validation Accuracy: 11.54\n",
            "[3/150]: Training Loss: 3.6135140613555907, Training Accuracy: 14.57\n",
            "Validation Loss: 3.473675949558331, Validation Accuracy: 17.5\n",
            "[4/150]: Training Loss: 3.3465395004272462, Training Accuracy: 19.22\n",
            "Validation Loss: 3.3301091406755385, Validation Accuracy: 19.19\n",
            "[5/150]: Training Loss: 3.1310076709747316, Training Accuracy: 22.94\n",
            "Validation Loss: 3.0678081117617855, Validation Accuracy: 24.65\n",
            "[6/150]: Training Loss: 2.9402838905334474, Training Accuracy: 26.4875\n",
            "Validation Loss: 2.9766739310732313, Validation Accuracy: 25.68\n",
            "[7/150]: Training Loss: 2.7704892574310302, Training Accuracy: 29.855\n",
            "Validation Loss: 2.8771827342403924, Validation Accuracy: 28.79\n",
            "[8/150]: Training Loss: 2.6034393648147582, Training Accuracy: 33.84\n",
            "Validation Loss: 2.7646109807263515, Validation Accuracy: 31.02\n",
            "[9/150]: Training Loss: 2.436837389945984, Training Accuracy: 37.1175\n",
            "Validation Loss: 2.731329696193622, Validation Accuracy: 32.37\n",
            "[10/150]: Training Loss: 2.273189482879639, Training Accuracy: 40.3275\n",
            "Validation Loss: 2.663352646645467, Validation Accuracy: 33.62\n",
            "[11/150]: Training Loss: 2.1113974294662476, Training Accuracy: 44.0125\n",
            "Validation Loss: 2.70243866884025, Validation Accuracy: 33.48\n",
            "[12/150]: Training Loss: 1.9719651878356934, Training Accuracy: 46.755\n",
            "Validation Loss: 2.7540456124931385, Validation Accuracy: 34.46\n",
            "[13/150]: Training Loss: 1.8135078485488891, Training Accuracy: 50.6725\n",
            "Validation Loss: 2.7649777801173507, Validation Accuracy: 34.67\n",
            "[14/150]: Training Loss: 1.663066688156128, Training Accuracy: 53.855\n",
            "Validation Loss: 2.8507347205641924, Validation Accuracy: 34.61\n",
            "[15/150]: Training Loss: 1.5029290641784667, Training Accuracy: 57.7575\n",
            "Validation Loss: 2.961932021341506, Validation Accuracy: 33.61\n",
            "[16/150]: Training Loss: 1.3492597907066346, Training Accuracy: 61.345\n",
            "Validation Loss: 3.0608204777833, Validation Accuracy: 33.71\n",
            "[17/150]: Training Loss: 1.2147304633140563, Training Accuracy: 64.7325\n",
            "Validation Loss: 3.1523648325804694, Validation Accuracy: 33.57\n",
            "[18/150]: Training Loss: 1.0741954045295716, Training Accuracy: 68.3375\n",
            "Validation Loss: 3.4972750517972715, Validation Accuracy: 32.75\n",
            "[19/150]: Training Loss: 0.9752286433696746, Training Accuracy: 70.7775\n",
            "Validation Loss: 3.703600254787761, Validation Accuracy: 32.29\n",
            "[20/150]: Training Loss: 0.8703240002632141, Training Accuracy: 73.66\n",
            "Validation Loss: 3.6860399884023485, Validation Accuracy: 32.6\n",
            "[21/150]: Training Loss: 0.7578078193187714, Training Accuracy: 76.6025\n",
            "Validation Loss: 4.062916931832672, Validation Accuracy: 31.29\n",
            "[22/150]: Training Loss: 0.673084812450409, Training Accuracy: 79.0375\n",
            "Validation Loss: 4.114260547480006, Validation Accuracy: 31.62\n",
            "[23/150]: Training Loss: 0.6178978152275085, Training Accuracy: 80.77\n",
            "Validation Loss: 4.332119194565306, Validation Accuracy: 32.24\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 24.731220123874156, Test Accuracy: 17.1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▆▁▅▆▇█▆▆▆▇▇▆▇▇▇▇▇████████▇██▇▇▇▇███████</td></tr><tr><td>Test Loss</td><td>▁▄▆▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>17.1</td></tr><tr><td>Test Loss</td><td>24.73122</td></tr><tr><td>Train Accuracy</td><td>80.77</td></tr><tr><td>Train Loss</td><td>0.6179</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_003346-pvvd2my8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "\n",
        "for lr in learning_rates:\n",
        "  for wd in weight_decays:\n",
        "\n",
        "    print('='*50)\n",
        "    print(f'Hyperparameter with lr:{lr} and wd:{wd}')\n",
        "    print('='*50)\n",
        "\n",
        "    hyperparameters = {'learning_rate': lr,\n",
        "                       'weight_decay' : wd\n",
        "                       }\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(num_epochs, model, train_loader, validation_loader, test_loader, optimizer, scheduler, criterion, device, 'SGDM-HyperParameterTuning', hyperparameters=hyperparameters, is_wandb = True, n_epochs_stop = 10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training Using Best Hyperparameters Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f9dca98d1b8c402db8fa03f1354a051b",
            "3445c5fe76514de0b6b9cc31200c8544",
            "eafc1e77174c4758a3780b481d694ae0",
            "e6a773393e084de9a117fbbf43b7a59e",
            "b0748d32c99741d39b18512400e07f30",
            "dc43eb867d6b446cb0cb8e5debae57e7",
            "6c30f9a9e6c44d78ad39f43a5f11a6d0",
            "81500a36ef5e4a9e9b36aef01bee3697"
          ]
        },
        "id": "i9Q1MpZmVAUp",
        "outputId": "00a406f8-47bc-405c-d5e8-e10713b7f5fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_013938-4i9h8t3n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n' target=\"_blank\">learning_rate=0.01 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.089274020146226, Training Accuracy: 7.336\n",
            "Validation Loss: 3.675565988394865, Validation Accuracy: 11.94\n",
            "[2/150]: Training Loss: 3.4894355224526445, Training Accuracy: 16.328\n",
            "Validation Loss: 3.239326788361665, Validation Accuracy: 21.43\n",
            "[3/150]: Training Loss: 3.1558719864281852, Training Accuracy: 21.82\n",
            "Validation Loss: 2.9393957374961515, Validation Accuracy: 26.07\n",
            "[4/150]: Training Loss: 2.9186559430778485, Training Accuracy: 26.722\n",
            "Validation Loss: 2.7213429053118277, Validation Accuracy: 30.34\n",
            "[5/150]: Training Loss: 2.7546427048685604, Training Accuracy: 29.83\n",
            "Validation Loss: 2.6445619574018346, Validation Accuracy: 32.87\n",
            "[6/150]: Training Loss: 2.6314107673552334, Training Accuracy: 32.316\n",
            "Validation Loss: 2.463979342940507, Validation Accuracy: 35.97\n",
            "[7/150]: Training Loss: 2.525907842399519, Training Accuracy: 34.748\n",
            "Validation Loss: 2.402360181899587, Validation Accuracy: 37.43\n",
            "[8/150]: Training Loss: 2.4479937202790203, Training Accuracy: 36.512\n",
            "Validation Loss: 2.4091407804732112, Validation Accuracy: 36.77\n",
            "[9/150]: Training Loss: 2.3761757938453303, Training Accuracy: 37.836\n",
            "Validation Loss: 2.266651909062817, Validation Accuracy: 40.12\n",
            "[10/150]: Training Loss: 2.3189178927780114, Training Accuracy: 39.182\n",
            "Validation Loss: 2.248874021943208, Validation Accuracy: 40.42\n",
            "[11/150]: Training Loss: 2.2658593403104015, Training Accuracy: 40.134\n",
            "Validation Loss: 2.259856101054295, Validation Accuracy: 40.67\n",
            "[12/150]: Training Loss: 2.2226978584628583, Training Accuracy: 41.142\n",
            "Validation Loss: 2.1725807235499097, Validation Accuracy: 42.45\n",
            "[13/150]: Training Loss: 2.1647636545893483, Training Accuracy: 42.432\n",
            "Validation Loss: 2.1585155209158637, Validation Accuracy: 43.43\n",
            "[14/150]: Training Loss: 2.130936350206585, Training Accuracy: 43.382\n",
            "Validation Loss: 2.099344393250289, Validation Accuracy: 44.47\n",
            "[15/150]: Training Loss: 2.100127648514555, Training Accuracy: 44.078\n",
            "Validation Loss: 2.1766397307632834, Validation Accuracy: 43.16\n",
            "[16/150]: Training Loss: 2.0619241555633447, Training Accuracy: 44.864\n",
            "Validation Loss: 2.086525283042033, Validation Accuracy: 45.45\n",
            "[17/150]: Training Loss: 2.030578180042374, Training Accuracy: 45.488\n",
            "Validation Loss: 2.056034764666466, Validation Accuracy: 45.56\n",
            "[18/150]: Training Loss: 2.0153322762540538, Training Accuracy: 45.894\n",
            "Validation Loss: 2.0836284441553103, Validation Accuracy: 45.49\n",
            "[19/150]: Training Loss: 1.9813076870520707, Training Accuracy: 46.512\n",
            "Validation Loss: 2.123422172418825, Validation Accuracy: 44.8\n",
            "[20/150]: Training Loss: 1.9613309851692766, Training Accuracy: 47.156\n",
            "Validation Loss: 2.0666351356324117, Validation Accuracy: 45.67\n",
            "[21/150]: Training Loss: 1.9383105931379605, Training Accuracy: 47.68\n",
            "Validation Loss: 2.002842481728572, Validation Accuracy: 47.53\n",
            "[22/150]: Training Loss: 1.9150681045963942, Training Accuracy: 48.088\n",
            "Validation Loss: 2.0622900238462316, Validation Accuracy: 47.17\n",
            "[23/150]: Training Loss: 1.893286463854563, Training Accuracy: 48.534\n",
            "Validation Loss: 2.00721144600279, Validation Accuracy: 46.89\n",
            "[24/150]: Training Loss: 1.890704987908873, Training Accuracy: 48.7\n",
            "Validation Loss: 2.038584551993449, Validation Accuracy: 46.81\n",
            "[25/150]: Training Loss: 1.864747319989802, Training Accuracy: 49.254\n",
            "Validation Loss: 2.0571977568280166, Validation Accuracy: 46.35\n",
            "[26/150]: Training Loss: 1.851232278225062, Training Accuracy: 49.578\n",
            "Validation Loss: 2.067047737206623, Validation Accuracy: 45.89\n",
            "[27/150]: Training Loss: 1.8424793141882132, Training Accuracy: 49.854\n",
            "Validation Loss: 1.9625371656600077, Validation Accuracy: 48.1\n",
            "[28/150]: Training Loss: 1.8046449944186393, Training Accuracy: 50.832\n",
            "Validation Loss: 2.0106217762467207, Validation Accuracy: 47.7\n",
            "[29/150]: Training Loss: 1.8060023488900852, Training Accuracy: 50.666\n",
            "Validation Loss: 1.972709655002424, Validation Accuracy: 48.73\n",
            "[30/150]: Training Loss: 1.794560846770206, Training Accuracy: 50.516\n",
            "Validation Loss: 1.9357355255989512, Validation Accuracy: 49.77\n",
            "[31/150]: Training Loss: 1.7700339468848674, Training Accuracy: 51.572\n",
            "Validation Loss: 2.0069477284789845, Validation Accuracy: 47.09\n",
            "[32/150]: Training Loss: 1.760896964146353, Training Accuracy: 51.7\n",
            "Validation Loss: 1.9989952729765776, Validation Accuracy: 48.32\n",
            "[33/150]: Training Loss: 1.7410227035927346, Training Accuracy: 52.406\n",
            "Validation Loss: 1.912184556578375, Validation Accuracy: 50.02\n",
            "[34/150]: Training Loss: 1.7288355792269987, Training Accuracy: 52.308\n",
            "Validation Loss: 1.960264290973639, Validation Accuracy: 49.04\n",
            "[35/150]: Training Loss: 1.726897613929056, Training Accuracy: 52.332\n",
            "Validation Loss: 1.9354495113822305, Validation Accuracy: 49.72\n",
            "[36/150]: Training Loss: 1.7120373965529225, Training Accuracy: 52.802\n",
            "Validation Loss: 1.9664037690800467, Validation Accuracy: 48.71\n",
            "[37/150]: Training Loss: 1.693096386502161, Training Accuracy: 53.136\n",
            "Validation Loss: 2.0150347925295495, Validation Accuracy: 47.45\n",
            "[38/150]: Training Loss: 1.6896419387949093, Training Accuracy: 53.464\n",
            "Validation Loss: 1.8958269729735746, Validation Accuracy: 50.19\n",
            "[39/150]: Training Loss: 1.6746401542897724, Training Accuracy: 53.896\n",
            "Validation Loss: 1.8924665306783786, Validation Accuracy: 50.21\n",
            "[40/150]: Training Loss: 1.658155962481828, Training Accuracy: 54.248\n",
            "Validation Loss: 2.013761671485415, Validation Accuracy: 47.68\n",
            "[41/150]: Training Loss: 1.6497482628468663, Training Accuracy: 54.48\n",
            "Validation Loss: 1.9099182338471625, Validation Accuracy: 49.93\n",
            "[42/150]: Training Loss: 1.640857491423102, Training Accuracy: 54.522\n",
            "Validation Loss: 1.9500497716247656, Validation Accuracy: 48.99\n",
            "[43/150]: Training Loss: 1.627800954272375, Training Accuracy: 54.828\n",
            "Validation Loss: 1.8859608940258148, Validation Accuracy: 50.08\n",
            "[44/150]: Training Loss: 1.6115469687125261, Training Accuracy: 55.468\n",
            "Validation Loss: 1.8382089783431619, Validation Accuracy: 51.49\n",
            "[45/150]: Training Loss: 1.5990354193141088, Training Accuracy: 55.408\n",
            "Validation Loss: 1.9672614282863155, Validation Accuracy: 49.18\n",
            "[46/150]: Training Loss: 1.597626144090272, Training Accuracy: 55.548\n",
            "Validation Loss: 1.8989077024399095, Validation Accuracy: 50.63\n",
            "[47/150]: Training Loss: 1.583931565284729, Training Accuracy: 55.81\n",
            "Validation Loss: 1.9013854675232225, Validation Accuracy: 50.7\n",
            "[48/150]: Training Loss: 1.5654289053224237, Training Accuracy: 56.032\n",
            "Validation Loss: 1.9721670173535681, Validation Accuracy: 49.36\n",
            "[49/150]: Training Loss: 1.5542657918789808, Training Accuracy: 56.446\n",
            "Validation Loss: 1.9395824830243542, Validation Accuracy: 49.82\n",
            "[50/150]: Training Loss: 1.5429580295482255, Training Accuracy: 56.698\n",
            "Validation Loss: 1.9366691735140078, Validation Accuracy: 50.0\n",
            "[51/150]: Training Loss: 1.539840473086023, Training Accuracy: 56.68\n",
            "Validation Loss: 1.93095024452088, Validation Accuracy: 49.97\n",
            "[52/150]: Training Loss: 1.522039294928846, Training Accuracy: 57.282\n",
            "Validation Loss: 1.901054722488306, Validation Accuracy: 50.91\n",
            "[53/150]: Training Loss: 1.505999029597358, Training Accuracy: 57.376\n",
            "Validation Loss: 1.945236231870712, Validation Accuracy: 49.27\n",
            "[54/150]: Training Loss: 1.507185840469492, Training Accuracy: 57.802\n",
            "Validation Loss: 1.8845598515431592, Validation Accuracy: 50.54\n",
            "[55/150]: Training Loss: 1.4943511856486424, Training Accuracy: 57.754\n",
            "Validation Loss: 1.908029711170561, Validation Accuracy: 50.57\n",
            "[56/150]: Training Loss: 1.4784885358322613, Training Accuracy: 58.39\n",
            "Validation Loss: 1.9093056567914926, Validation Accuracy: 50.32\n",
            "[57/150]: Training Loss: 1.4689392635736929, Training Accuracy: 58.69\n",
            "Validation Loss: 1.9369963088612647, Validation Accuracy: 50.18\n",
            "[58/150]: Training Loss: 1.4634843205704409, Training Accuracy: 58.688\n",
            "Validation Loss: 1.9295313540537646, Validation Accuracy: 50.43\n",
            "[59/150]: Training Loss: 1.4444945978996393, Training Accuracy: 59.07\n",
            "Validation Loss: 1.8427119877687685, Validation Accuracy: 52.33\n",
            "[60/150]: Training Loss: 1.4354293743515258, Training Accuracy: 59.374\n",
            "Validation Loss: 1.910806885950125, Validation Accuracy: 50.94\n",
            "[61/150]: Training Loss: 1.4113788747269174, Training Accuracy: 59.82\n",
            "Validation Loss: 1.8786808958478793, Validation Accuracy: 51.43\n",
            "[62/150]: Training Loss: 1.4087011503898883, Training Accuracy: 59.934\n",
            "Validation Loss: 1.93228034608683, Validation Accuracy: 50.3\n",
            "[63/150]: Training Loss: 1.4004798267046203, Training Accuracy: 60.114\n",
            "Validation Loss: 1.862087261145282, Validation Accuracy: 51.42\n",
            "[64/150]: Training Loss: 1.3885550134627105, Training Accuracy: 60.078\n",
            "Validation Loss: 1.839821860289118, Validation Accuracy: 52.23\n",
            "[65/150]: Training Loss: 1.3659701371741721, Training Accuracy: 61.178\n",
            "Validation Loss: 1.8993338536305033, Validation Accuracy: 51.3\n",
            "[66/150]: Training Loss: 1.359891347720495, Training Accuracy: 61.062\n",
            "Validation Loss: 1.808910168659915, Validation Accuracy: 52.34\n",
            "[67/150]: Training Loss: 1.350066394376023, Training Accuracy: 61.206\n",
            "Validation Loss: 1.842593222666698, Validation Accuracy: 52.23\n",
            "[68/150]: Training Loss: 1.3413548037371672, Training Accuracy: 61.606\n",
            "Validation Loss: 1.8450721175807296, Validation Accuracy: 52.19\n",
            "[69/150]: Training Loss: 1.328629597907176, Training Accuracy: 61.842\n",
            "Validation Loss: 1.9130320769206735, Validation Accuracy: 51.75\n",
            "[70/150]: Training Loss: 1.308861137579774, Training Accuracy: 62.402\n",
            "Validation Loss: 1.8209870947394402, Validation Accuracy: 52.76\n",
            "[71/150]: Training Loss: 1.3000667644736101, Training Accuracy: 62.528\n",
            "Validation Loss: 1.8656981424161583, Validation Accuracy: 52.29\n",
            "[72/150]: Training Loss: 1.2866845129395994, Training Accuracy: 62.902\n",
            "Validation Loss: 1.8237224574301654, Validation Accuracy: 53.36\n",
            "[73/150]: Training Loss: 1.2716818537248675, Training Accuracy: 63.328\n",
            "Validation Loss: 1.872310409879988, Validation Accuracy: 52.51\n",
            "[74/150]: Training Loss: 1.2592318182253777, Training Accuracy: 63.678\n",
            "Validation Loss: 1.8924590835146085, Validation Accuracy: 52.18\n",
            "[75/150]: Training Loss: 1.2489444612694518, Training Accuracy: 63.924\n",
            "Validation Loss: 1.8441433739510311, Validation Accuracy: 53.18\n",
            "[76/150]: Training Loss: 1.2271763168637404, Training Accuracy: 64.204\n",
            "Validation Loss: 1.826472075881472, Validation Accuracy: 53.48\n",
            "[77/150]: Training Loss: 1.2162439644031817, Training Accuracy: 64.628\n",
            "Validation Loss: 1.7922283798266367, Validation Accuracy: 53.91\n",
            "[78/150]: Training Loss: 1.1974031528091187, Training Accuracy: 65.212\n",
            "Validation Loss: 1.854033857394176, Validation Accuracy: 52.77\n",
            "[79/150]: Training Loss: 1.1963484688945438, Training Accuracy: 65.298\n",
            "Validation Loss: 1.8417060686524507, Validation Accuracy: 53.83\n",
            "[80/150]: Training Loss: 1.170646650559457, Training Accuracy: 65.906\n",
            "Validation Loss: 1.8188444110238629, Validation Accuracy: 54.0\n",
            "[81/150]: Training Loss: 1.1599712444998114, Training Accuracy: 66.228\n",
            "Validation Loss: 1.832872725596094, Validation Accuracy: 53.93\n",
            "[82/150]: Training Loss: 1.1514770396987495, Training Accuracy: 66.602\n",
            "Validation Loss: 1.8412558204808813, Validation Accuracy: 53.79\n",
            "[83/150]: Training Loss: 1.1428028439621791, Training Accuracy: 66.712\n",
            "Validation Loss: 1.8457995410178119, Validation Accuracy: 53.19\n",
            "[84/150]: Training Loss: 1.1221959683901208, Training Accuracy: 67.186\n",
            "Validation Loss: 1.8420210499672374, Validation Accuracy: 53.69\n",
            "[85/150]: Training Loss: 1.1132261551859435, Training Accuracy: 67.302\n",
            "Validation Loss: 1.8613816719905587, Validation Accuracy: 53.54\n",
            "[86/150]: Training Loss: 1.095082609473592, Training Accuracy: 68.072\n",
            "Validation Loss: 1.8083385081048224, Validation Accuracy: 53.78\n",
            "[87/150]: Training Loss: 1.0820598827908412, Training Accuracy: 68.22\n",
            "Validation Loss: 1.8537752772592435, Validation Accuracy: 54.26\n",
            "[88/150]: Training Loss: 1.0588698522818973, Training Accuracy: 68.838\n",
            "Validation Loss: 1.8358791383208743, Validation Accuracy: 54.18\n",
            "[89/150]: Training Loss: 1.0537138239806876, Training Accuracy: 68.858\n",
            "Validation Loss: 1.9032178441430354, Validation Accuracy: 53.39\n",
            "[90/150]: Training Loss: 1.0420529545115693, Training Accuracy: 69.19\n",
            "Validation Loss: 1.8331271857972358, Validation Accuracy: 54.72\n",
            "[91/150]: Training Loss: 1.0190018734053883, Training Accuracy: 69.946\n",
            "Validation Loss: 1.81914554811587, Validation Accuracy: 54.61\n",
            "[92/150]: Training Loss: 1.0052901713744453, Training Accuracy: 70.376\n",
            "Validation Loss: 1.8224602520086204, Validation Accuracy: 54.54\n",
            "[93/150]: Training Loss: 0.9959899578100581, Training Accuracy: 70.576\n",
            "Validation Loss: 1.8335816890570769, Validation Accuracy: 54.66\n",
            "[94/150]: Training Loss: 0.9820676271034323, Training Accuracy: 70.882\n",
            "Validation Loss: 1.8163665267312603, Validation Accuracy: 55.18\n",
            "[95/150]: Training Loss: 0.9656730821675352, Training Accuracy: 71.258\n",
            "Validation Loss: 1.8308387478445745, Validation Accuracy: 54.6\n",
            "[96/150]: Training Loss: 0.9520570190666277, Training Accuracy: 71.578\n",
            "Validation Loss: 1.8336243507968393, Validation Accuracy: 55.03\n",
            "[97/150]: Training Loss: 0.938745556661235, Training Accuracy: 71.902\n",
            "Validation Loss: 1.8366246033625997, Validation Accuracy: 54.91\n",
            "[98/150]: Training Loss: 0.9180464198827134, Training Accuracy: 72.722\n",
            "Validation Loss: 1.8760136365890503, Validation Accuracy: 54.5\n",
            "[99/150]: Training Loss: 0.9068292014281768, Training Accuracy: 73.062\n",
            "Validation Loss: 1.7993891132864983, Validation Accuracy: 55.5\n",
            "[100/150]: Training Loss: 0.8922816610625942, Training Accuracy: 73.34\n",
            "Validation Loss: 1.8412930228907591, Validation Accuracy: 54.68\n",
            "[101/150]: Training Loss: 0.8739150777421034, Training Accuracy: 73.706\n",
            "Validation Loss: 1.8339118517128525, Validation Accuracy: 55.67\n",
            "[102/150]: Training Loss: 0.8564372022667199, Training Accuracy: 74.472\n",
            "Validation Loss: 1.831563648904205, Validation Accuracy: 54.7\n",
            "[103/150]: Training Loss: 0.8455693187463619, Training Accuracy: 74.832\n",
            "Validation Loss: 1.8586692977103458, Validation Accuracy: 55.09\n",
            "[104/150]: Training Loss: 0.8356970895815383, Training Accuracy: 74.91\n",
            "Validation Loss: 1.891693490326025, Validation Accuracy: 54.66\n",
            "[105/150]: Training Loss: 0.8192636847038708, Training Accuracy: 75.622\n",
            "Validation Loss: 1.836436649037015, Validation Accuracy: 55.18\n",
            "[106/150]: Training Loss: 0.8021497989783202, Training Accuracy: 75.904\n",
            "Validation Loss: 1.8664625199737064, Validation Accuracy: 55.06\n",
            "[107/150]: Training Loss: 0.7924835441438743, Training Accuracy: 76.238\n",
            "Validation Loss: 1.8400274932764138, Validation Accuracy: 56.23\n",
            "[108/150]: Training Loss: 0.7697334062031773, Training Accuracy: 76.922\n",
            "Validation Loss: 1.852326028665919, Validation Accuracy: 55.58\n",
            "[109/150]: Training Loss: 0.7663251587268337, Training Accuracy: 77.026\n",
            "Validation Loss: 1.851918256206877, Validation Accuracy: 55.79\n",
            "[110/150]: Training Loss: 0.7508895213113111, Training Accuracy: 77.64\n",
            "Validation Loss: 1.8553174369654077, Validation Accuracy: 55.56\n",
            "[111/150]: Training Loss: 0.7446020825973252, Training Accuracy: 77.658\n",
            "Validation Loss: 1.8480755607033992, Validation Accuracy: 55.4\n",
            "[112/150]: Training Loss: 0.7242154034278582, Training Accuracy: 78.302\n",
            "Validation Loss: 1.8599452805367245, Validation Accuracy: 55.63\n",
            "[113/150]: Training Loss: 0.710430224609497, Training Accuracy: 78.58\n",
            "Validation Loss: 1.853841427405169, Validation Accuracy: 55.72\n",
            "[114/150]: Training Loss: 0.6952832066418265, Training Accuracy: 79.15\n",
            "Validation Loss: 1.8361253859890494, Validation Accuracy: 56.53\n",
            "[115/150]: Training Loss: 0.6855566057631427, Training Accuracy: 79.384\n",
            "Validation Loss: 1.8501070935255404, Validation Accuracy: 55.8\n",
            "[116/150]: Training Loss: 0.670332043791366, Training Accuracy: 79.848\n",
            "Validation Loss: 1.871697852565984, Validation Accuracy: 55.87\n",
            "[117/150]: Training Loss: 0.659062210518076, Training Accuracy: 80.28\n",
            "Validation Loss: 1.8932185765284641, Validation Accuracy: 56.33\n",
            "[118/150]: Training Loss: 0.6481513134049027, Training Accuracy: 80.47\n",
            "Validation Loss: 1.8721413916083658, Validation Accuracy: 56.69\n",
            "[119/150]: Training Loss: 0.6378583989256178, Training Accuracy: 80.804\n",
            "Validation Loss: 1.8575806663294507, Validation Accuracy: 56.3\n",
            "[120/150]: Training Loss: 0.6237786744561646, Training Accuracy: 81.468\n",
            "Validation Loss: 1.8885631751103007, Validation Accuracy: 56.27\n",
            "[121/150]: Training Loss: 0.6136706139311157, Training Accuracy: 81.672\n",
            "Validation Loss: 1.8672110297877318, Validation Accuracy: 56.64\n",
            "[122/150]: Training Loss: 0.598588163380885, Training Accuracy: 82.084\n",
            "Validation Loss: 1.8859197356898314, Validation Accuracy: 56.85\n",
            "[123/150]: Training Loss: 0.5942039575494463, Training Accuracy: 82.472\n",
            "Validation Loss: 1.8791207416801696, Validation Accuracy: 56.65\n",
            "[124/150]: Training Loss: 0.5779005947625241, Training Accuracy: 82.892\n",
            "Validation Loss: 1.8723569349118858, Validation Accuracy: 56.69\n",
            "[125/150]: Training Loss: 0.5656420003689463, Training Accuracy: 83.386\n",
            "Validation Loss: 1.8776653800041052, Validation Accuracy: 56.51\n",
            "[126/150]: Training Loss: 0.5588299318423966, Training Accuracy: 83.53\n",
            "Validation Loss: 1.8926861441818772, Validation Accuracy: 56.77\n",
            "[127/150]: Training Loss: 0.5484652115065424, Training Accuracy: 83.916\n",
            "Validation Loss: 1.8761122143192657, Validation Accuracy: 56.82\n",
            "[128/150]: Training Loss: 0.5397156641230254, Training Accuracy: 84.144\n",
            "Validation Loss: 1.883816226272826, Validation Accuracy: 56.91\n",
            "[129/150]: Training Loss: 0.5299459375307688, Training Accuracy: 84.524\n",
            "Validation Loss: 1.885110402942463, Validation Accuracy: 56.93\n",
            "[130/150]: Training Loss: 0.5226550506203985, Training Accuracy: 84.69\n",
            "Validation Loss: 1.8858621705109906, Validation Accuracy: 57.03\n",
            "[131/150]: Training Loss: 0.5145904917622466, Training Accuracy: 85.08\n",
            "Validation Loss: 1.898423175902883, Validation Accuracy: 56.86\n",
            "[132/150]: Training Loss: 0.5080187612444239, Training Accuracy: 85.244\n",
            "Validation Loss: 1.8937967233597093, Validation Accuracy: 57.24\n",
            "[133/150]: Training Loss: 0.501667047438719, Training Accuracy: 85.274\n",
            "Validation Loss: 1.8932398701928983, Validation Accuracy: 56.88\n",
            "[134/150]: Training Loss: 0.49395688687977585, Training Accuracy: 85.592\n",
            "Validation Loss: 1.8888862345628679, Validation Accuracy: 57.11\n",
            "[135/150]: Training Loss: 0.48916787244474796, Training Accuracy: 85.884\n",
            "Validation Loss: 1.8910012594453849, Validation Accuracy: 57.08\n",
            "[136/150]: Training Loss: 0.48000375615894947, Training Accuracy: 86.098\n",
            "Validation Loss: 1.9055567767210067, Validation Accuracy: 56.92\n",
            "[137/150]: Training Loss: 0.48404536522029307, Training Accuracy: 86.152\n",
            "Validation Loss: 1.8957096717919513, Validation Accuracy: 57.23\n",
            "[138/150]: Training Loss: 0.4733923572637236, Training Accuracy: 86.464\n",
            "Validation Loss: 1.8970948966445438, Validation Accuracy: 57.23\n",
            "[139/150]: Training Loss: 0.4687954626424843, Training Accuracy: 86.406\n",
            "Validation Loss: 1.8926855910355878, Validation Accuracy: 57.49\n",
            "[140/150]: Training Loss: 0.4656825878126237, Training Accuracy: 86.552\n",
            "Validation Loss: 1.8944045935466791, Validation Accuracy: 57.25\n",
            "[141/150]: Training Loss: 0.46502622329365567, Training Accuracy: 86.75\n",
            "Validation Loss: 1.892073856797188, Validation Accuracy: 57.21\n",
            "[142/150]: Training Loss: 0.45881695900579245, Training Accuracy: 86.948\n",
            "Validation Loss: 1.8972379872753362, Validation Accuracy: 57.33\n",
            "[143/150]: Training Loss: 0.4567206385152419, Training Accuracy: 86.978\n",
            "Validation Loss: 1.8969501234163904, Validation Accuracy: 57.41\n",
            "[144/150]: Training Loss: 0.45361602742729895, Training Accuracy: 87.108\n",
            "Validation Loss: 1.8974497530870378, Validation Accuracy: 57.36\n",
            "[145/150]: Training Loss: 0.45476321409196807, Training Accuracy: 87.206\n",
            "Validation Loss: 1.9007372218332472, Validation Accuracy: 57.39\n",
            "[146/150]: Training Loss: 0.45061131850685304, Training Accuracy: 87.172\n",
            "Validation Loss: 1.8978507139120893, Validation Accuracy: 57.36\n",
            "[147/150]: Training Loss: 0.4471297430832063, Training Accuracy: 87.32\n",
            "Validation Loss: 1.8995944998066896, Validation Accuracy: 57.45\n",
            "[148/150]: Training Loss: 0.44544086216584494, Training Accuracy: 87.358\n",
            "Validation Loss: 1.8984677867524942, Validation Accuracy: 57.57\n",
            "[149/150]: Training Loss: 0.45087983754589733, Training Accuracy: 87.206\n",
            "Validation Loss: 1.8988231648305418, Validation Accuracy: 57.57\n",
            "[150/150]: Training Loss: 0.445312842879149, Training Accuracy: 87.314\n",
            "Validation Loss: 1.898690512985181, Validation Accuracy: 57.55\n",
            "**********************************************************************\n",
            "Test Loss: 1.898690512985181, Test Accuracy: 57.55\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▆█▄▃▄▃▂▁▂▂▂▂▁▁▁▁▂▂▂▁▁▁▁▂▁▂▁▂▂▂▂▁▂▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>█▁▄▅▅▅▆▇▅▆▅▄▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▄</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>57.55</td></tr><tr><td>Test Loss</td><td>1.89869</td></tr><tr><td>Train Accuracy</td><td>87.314</td></tr><tr><td>Train Loss</td><td>0.44531</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_013938-4i9h8t3n/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "\n",
        "hyperparameters = {'learning_rate': lr,\n",
        "                  'weight_decay' : wd}\n",
        "\n",
        "# Load the model\n",
        "model_0 = LeNet5().to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer_0 = torch.optim.SGD(model_0.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_0, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# Training\n",
        "run_training(num_epochs, model_0, original_train_loader, original_test_loader, original_test_loader, optimizer_0, scheduler, criterion, device, optimizer_name='SGDM', hyperparameters=hyperparameters, is_wandb=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### AdamW (Adam with Weight Decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ba6f9560801c461a90a0fd2f8c20d918",
            "5055c91b4ed34c1baf249fa25948e84f",
            "1feabb0772134ace893d71cb905e9b12",
            "fb5f2e328dff44018f41180c2a2ec871",
            "33050a2d80c24235aecc36cb34e79684",
            "2bc1423c729a4faa9a184ae092eda8ad",
            "1b1acf7a1f5f43739e1fe0894ba48950",
            "0d710da02bfe4c8e80d205158d9d64c7",
            "d770fa4de3bd479886dbf8e1f6369543",
            "c20d73b37f6a497a8847e2fd20a98d16",
            "edd61f4569b54b869669aebf91420508",
            "a6ba8ab59be54b8ca096631627ee9713",
            "15a0bbd1e2b14cef8a2f9accf120c1ad",
            "e181391aa4424c04804ac665abd6f594",
            "f8ce66183b2543e7bf19b71d90d1c53e",
            "904f899f441149e9b707699a0ebf31b5",
            "f76ed5acf91a4edf92950dcb88356f23",
            "9387a738135842cf965db860499510f5",
            "33a438b911c04f1f96c67b4f7ba7477d",
            "35d90a90d5854bcaa2bc5f385cdad931",
            "5630d5fcb50a46f2887e0cce74a005a6",
            "fed90f96881140119ee97a0d2120b615",
            "ff7b6956e7e34db68cd38dee19c798f1",
            "d71c8b5d30df474ca6f30976d0a33c71",
            "cbe10db0bb8d45609a0f3d59a30c8f61",
            "07ff6351429c48c2b835305d3111af40",
            "ab580ef13b18428c994fc4f8b80885b6",
            "497de066b1964cd4b931476e0bd50c66",
            "c5de35a9063345b1a12e212718a02575",
            "e86e517239b54ca4a6767a300aa7e01d",
            "eb268a732bc74b2d81ec3c3a6ecd0362",
            "f2915a147a4246dba7984a90f23c34ae",
            "2f39d5f73c7647f1804e4212cb39924d",
            "6e7ede02663f4eb0927872bf17858f18",
            "a562d76741f74b10b8095be14da779d1",
            "f4e7759ba0f544d8a28a9922e06e890b",
            "0a9ee7dc8817456aab4af03cbfa4c6ed",
            "567cc1542f09433f8cc8b3a39237f198",
            "8dc245e02cac40f1b601fa42a0e6e77a",
            "1b093d744b374154afe2058e4a6de822",
            "4302f47b1ff043aca2523212b015e080",
            "371471be80ca47dbb005c853b7832f04",
            "e75a0d8ae24e462aa3075a813aad302d",
            "fd2cd8045a5c4387b98d080782289c48",
            "1574e69b88de479da2aadc2dfdd43261",
            "f523ae2aca8c46878f0a6ddc1f798ef5",
            "0b42ea1995bb410b99e653780dad3916",
            "f3ebd9b7bf7a4d24b54ab6673322f74b"
          ]
        },
        "id": "Toi1eWRqJuhK",
        "outputId": "bd028d6d-1e8d-4fff-bccf-88ed9aa1c4ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:ymdv8k6d) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁</td></tr><tr><td>Train Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>7.7625</td></tr><tr><td>Train Loss</td><td>4.01706</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.00095 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/ymdv8k6d' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/ymdv8k6d</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_034825-ymdv8k6d/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:ymdv8k6d). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_034959-j16vayol</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol' target=\"_blank\">learning_rate=0.001 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.032126424407959, Training Accuracy: 7.645\n",
            "Validation Loss: 3.633875617555752, Validation Accuracy: 13.81\n",
            "[2/150]: Training Loss: 3.4117616390228274, Training Accuracy: 17.9375\n",
            "Validation Loss: 3.291252329091358, Validation Accuracy: 19.57\n",
            "[3/150]: Training Loss: 3.0980306770324706, Training Accuracy: 23.7325\n",
            "Validation Loss: 3.0133402590539045, Validation Accuracy: 25.49\n",
            "[4/150]: Training Loss: 2.875119793319702, Training Accuracy: 27.905\n",
            "Validation Loss: 2.9483105938905365, Validation Accuracy: 26.77\n",
            "[5/150]: Training Loss: 2.6911334384918213, Training Accuracy: 31.615\n",
            "Validation Loss: 2.851942961383018, Validation Accuracy: 28.77\n",
            "[6/150]: Training Loss: 2.5383362628936768, Training Accuracy: 34.6425\n",
            "Validation Loss: 2.8010447116414454, Validation Accuracy: 30.38\n",
            "[7/150]: Training Loss: 2.3951899276733397, Training Accuracy: 37.7375\n",
            "Validation Loss: 2.772052654035532, Validation Accuracy: 31.08\n",
            "[8/150]: Training Loss: 2.2677825828552245, Training Accuracy: 40.2175\n",
            "Validation Loss: 2.740394227823634, Validation Accuracy: 32.51\n",
            "[9/150]: Training Loss: 2.1415996942520144, Training Accuracy: 43.1725\n",
            "Validation Loss: 2.7748732961666813, Validation Accuracy: 31.82\n",
            "[10/150]: Training Loss: 2.016268748664856, Training Accuracy: 45.7725\n",
            "Validation Loss: 2.766294874203433, Validation Accuracy: 33.51\n",
            "[11/150]: Training Loss: 1.8994886245727538, Training Accuracy: 48.375\n",
            "Validation Loss: 2.840820978401573, Validation Accuracy: 32.8\n",
            "[12/150]: Training Loss: 1.7866277021408081, Training Accuracy: 51.045\n",
            "Validation Loss: 2.8579562796149283, Validation Accuracy: 32.9\n",
            "[13/150]: Training Loss: 1.6882859018325806, Training Accuracy: 53.12\n",
            "Validation Loss: 2.923183766899595, Validation Accuracy: 32.48\n",
            "[14/150]: Training Loss: 1.5779105269432068, Training Accuracy: 55.655\n",
            "Validation Loss: 3.072260461795102, Validation Accuracy: 32.29\n",
            "[15/150]: Training Loss: 1.478017513847351, Training Accuracy: 58.1575\n",
            "Validation Loss: 3.1435716137005265, Validation Accuracy: 32.08\n",
            "[16/150]: Training Loss: 1.3899440537452699, Training Accuracy: 59.9975\n",
            "Validation Loss: 3.176926696376436, Validation Accuracy: 31.97\n",
            "[17/150]: Training Loss: 1.3004323136329652, Training Accuracy: 62.3725\n",
            "Validation Loss: 3.3300343015391354, Validation Accuracy: 32.02\n",
            "[18/150]: Training Loss: 1.2033100715637206, Training Accuracy: 64.74\n",
            "Validation Loss: 3.478040464364799, Validation Accuracy: 31.7\n",
            "[19/150]: Training Loss: 1.1342673721313477, Training Accuracy: 66.51\n",
            "Validation Loss: 3.621978481863714, Validation Accuracy: 31.07\n",
            "[20/150]: Training Loss: 1.057302718448639, Training Accuracy: 68.56\n",
            "Validation Loss: 3.8245642261140667, Validation Accuracy: 31.16\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 198.79228443704594, Test Accuracy: 3.39\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▇▃▃▁▁▂▂▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▁▆▇▅████████▇█▇▇██▇█▇███▇███▇▇█▇███▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>3.39</td></tr><tr><td>Test Loss</td><td>198.79228</td></tr><tr><td>Train Accuracy</td><td>68.56</td></tr><tr><td>Train Loss</td><td>1.0573</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_034959-j16vayol/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_035204-l4zoq5dd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd' target=\"_blank\">learning_rate=0.001 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.108658624267578, Training Accuracy: 6.6075\n",
            "Validation Loss: 3.7040131304674087, Validation Accuracy: 12.1\n",
            "[2/150]: Training Loss: 3.5060665573120118, Training Accuracy: 16.015\n",
            "Validation Loss: 3.3598362321306947, Validation Accuracy: 18.42\n",
            "[3/150]: Training Loss: 3.1865245040893555, Training Accuracy: 21.7275\n",
            "Validation Loss: 3.128495198146553, Validation Accuracy: 22.84\n",
            "[4/150]: Training Loss: 2.9679495239257814, Training Accuracy: 25.8525\n",
            "Validation Loss: 2.9615708293428846, Validation Accuracy: 26.83\n",
            "[5/150]: Training Loss: 2.797335900115967, Training Accuracy: 29.515\n",
            "Validation Loss: 2.9264626108157406, Validation Accuracy: 26.96\n",
            "[6/150]: Training Loss: 2.6617756172180176, Training Accuracy: 32.13\n",
            "Validation Loss: 2.8755724490827816, Validation Accuracy: 28.35\n",
            "[7/150]: Training Loss: 2.54268462638855, Training Accuracy: 34.435\n",
            "Validation Loss: 2.7718013547788, Validation Accuracy: 30.25\n",
            "[8/150]: Training Loss: 2.424749851608276, Training Accuracy: 37.1025\n",
            "Validation Loss: 2.7567353567500024, Validation Accuracy: 31.11\n",
            "[9/150]: Training Loss: 2.3229350467681886, Training Accuracy: 39.08\n",
            "Validation Loss: 2.7088757032042095, Validation Accuracy: 32.08\n",
            "[10/150]: Training Loss: 2.2215481660842897, Training Accuracy: 41.4275\n",
            "Validation Loss: 2.75471066821153, Validation Accuracy: 31.83\n",
            "[11/150]: Training Loss: 2.134213170814514, Training Accuracy: 43.25\n",
            "Validation Loss: 2.8348169676057853, Validation Accuracy: 31.64\n",
            "[12/150]: Training Loss: 2.051020913696289, Training Accuracy: 44.9475\n",
            "Validation Loss: 2.7630925406316282, Validation Accuracy: 32.98\n",
            "[13/150]: Training Loss: 1.960823282814026, Training Accuracy: 46.7775\n",
            "Validation Loss: 2.7420302629470825, Validation Accuracy: 33.4\n",
            "[14/150]: Training Loss: 1.8790180908203125, Training Accuracy: 48.855\n",
            "Validation Loss: 2.7926843044864142, Validation Accuracy: 33.79\n",
            "[15/150]: Training Loss: 1.80182436504364, Training Accuracy: 50.6875\n",
            "Validation Loss: 2.8919099911003356, Validation Accuracy: 33.19\n",
            "[16/150]: Training Loss: 1.727850655937195, Training Accuracy: 52.3025\n",
            "Validation Loss: 2.8964282950018623, Validation Accuracy: 33.5\n",
            "[17/150]: Training Loss: 1.6616035480499267, Training Accuracy: 53.85\n",
            "Validation Loss: 3.033423226350432, Validation Accuracy: 32.53\n",
            "[18/150]: Training Loss: 1.5868734314918518, Training Accuracy: 55.5\n",
            "Validation Loss: 3.032342692089688, Validation Accuracy: 33.04\n",
            "[19/150]: Training Loss: 1.5333876008987426, Training Accuracy: 56.7525\n",
            "Validation Loss: 3.079486154446936, Validation Accuracy: 32.97\n",
            "[20/150]: Training Loss: 1.4575365795135498, Training Accuracy: 58.77\n",
            "Validation Loss: 3.181258452166418, Validation Accuracy: 32.82\n",
            "[21/150]: Training Loss: 1.39640816450119, Training Accuracy: 60.14\n",
            "Validation Loss: 3.216413262543405, Validation Accuracy: 32.76\n",
            "[22/150]: Training Loss: 1.337559293270111, Training Accuracy: 61.5275\n",
            "Validation Loss: 3.3633130872325534, Validation Accuracy: 31.73\n",
            "[23/150]: Training Loss: 1.2836213255882263, Training Accuracy: 62.6875\n",
            "Validation Loss: 3.4558656868661286, Validation Accuracy: 32.33\n",
            "[24/150]: Training Loss: 1.2338986722946168, Training Accuracy: 64.2825\n",
            "Validation Loss: 3.4873077155678134, Validation Accuracy: 32.02\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 156.72609671817463, Test Accuracy: 3.33\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▅▁▂▃▂▃▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>Test Loss</td><td>▇▇▁▂▁▄▄▇█▆▃▄▅▆▅▅▆▆▆▆▅▇▇▆▆▇▆▆▅▆▆▆▅▆▅▅▆▆▅▅</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>3.33</td></tr><tr><td>Test Loss</td><td>156.7261</td></tr><tr><td>Train Accuracy</td><td>64.2825</td></tr><tr><td>Train Loss</td><td>1.2339</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_035204-l4zoq5dd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_035441-y4ask5ys</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys' target=\"_blank\">learning_rate=0.001 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.053023485183716, Training Accuracy: 7.3\n",
            "Validation Loss: 3.71551227873298, Validation Accuracy: 12.6\n",
            "[2/150]: Training Loss: 3.4803551902770997, Training Accuracy: 16.4825\n",
            "Validation Loss: 3.3104240119836894, Validation Accuracy: 19.6\n",
            "[3/150]: Training Loss: 3.1457452793121337, Training Accuracy: 22.71\n",
            "Validation Loss: 3.1647931253834134, Validation Accuracy: 22.36\n",
            "[4/150]: Training Loss: 2.9213612785339356, Training Accuracy: 26.8375\n",
            "Validation Loss: 2.967867093481076, Validation Accuracy: 26.13\n",
            "[5/150]: Training Loss: 2.7346919048309326, Training Accuracy: 30.745\n",
            "Validation Loss: 2.870804744161618, Validation Accuracy: 28.43\n",
            "[6/150]: Training Loss: 2.580346668434143, Training Accuracy: 33.8975\n",
            "Validation Loss: 2.813369401700937, Validation Accuracy: 29.97\n",
            "[7/150]: Training Loss: 2.4525743492126466, Training Accuracy: 36.445\n",
            "Validation Loss: 2.8111886962963517, Validation Accuracy: 31.76\n",
            "[8/150]: Training Loss: 2.3342738655090334, Training Accuracy: 38.8\n",
            "Validation Loss: 2.7751463211266096, Validation Accuracy: 31.29\n",
            "[9/150]: Training Loss: 2.230911629486084, Training Accuracy: 40.89\n",
            "Validation Loss: 2.733239137443008, Validation Accuracy: 32.32\n",
            "[10/150]: Training Loss: 2.127202660560608, Training Accuracy: 43.0575\n",
            "Validation Loss: 2.740311449500406, Validation Accuracy: 32.67\n",
            "[11/150]: Training Loss: 2.0320646129608155, Training Accuracy: 45.3025\n",
            "Validation Loss: 2.7493710031934606, Validation Accuracy: 33.28\n",
            "[12/150]: Training Loss: 1.9452043891906738, Training Accuracy: 47.2675\n",
            "Validation Loss: 2.776471818328663, Validation Accuracy: 33.2\n",
            "[13/150]: Training Loss: 1.8665077058792114, Training Accuracy: 49.035\n",
            "Validation Loss: 2.801089420440091, Validation Accuracy: 32.56\n",
            "[14/150]: Training Loss: 1.782764630126953, Training Accuracy: 50.66\n",
            "Validation Loss: 2.869801163673401, Validation Accuracy: 33.19\n",
            "[15/150]: Training Loss: 1.6995231729507447, Training Accuracy: 52.77\n",
            "Validation Loss: 3.0037558374890856, Validation Accuracy: 32.92\n",
            "[16/150]: Training Loss: 1.6300209772109986, Training Accuracy: 54.165\n",
            "Validation Loss: 2.9989138818850183, Validation Accuracy: 32.86\n",
            "[17/150]: Training Loss: 1.5460361848831177, Training Accuracy: 56.605\n",
            "Validation Loss: 3.0711293569795646, Validation Accuracy: 32.69\n",
            "[18/150]: Training Loss: 1.48129998254776, Training Accuracy: 57.9875\n",
            "Validation Loss: 3.1773584519222284, Validation Accuracy: 31.98\n",
            "[19/150]: Training Loss: 1.4184319323539734, Training Accuracy: 59.43\n",
            "Validation Loss: 3.278900881481778, Validation Accuracy: 31.73\n",
            "[20/150]: Training Loss: 1.3525760270118714, Training Accuracy: 60.9775\n",
            "Validation Loss: 3.3168472605905714, Validation Accuracy: 31.51\n",
            "[21/150]: Training Loss: 1.275265542125702, Training Accuracy: 63.035\n",
            "Validation Loss: 3.5034019506660994, Validation Accuracy: 31.12\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 67.07529235645464, Test Accuracy: 5.39\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▁▂▇▃▁▂▁▂▂▂▃▃▃▄▄▄▄▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>Test Loss</td><td>▁█▅▃▃▃▃▄▅▄▃▂▂▃▃▂▄▄▄▄▃▃▃▃▃▃▃▃▃▂▃▃▃▃▃▃▃▂▂▂</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>5.39</td></tr><tr><td>Test Loss</td><td>67.07529</td></tr><tr><td>Train Accuracy</td><td>63.035</td></tr><tr><td>Train Loss</td><td>1.27527</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_035441-y4ask5ys/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.0001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_035656-za8h22vt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt' target=\"_blank\">learning_rate=0.01 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.622148120880127, Training Accuracy: 0.935\n",
            "Validation Loss: 4.609954472560032, Validation Accuracy: 0.99\n",
            "[2/150]: Training Loss: 4.609106838226318, Training Accuracy: 0.9475\n",
            "Validation Loss: 4.611106529357327, Validation Accuracy: 0.9\n",
            "[3/150]: Training Loss: 4.608888108062744, Training Accuracy: 0.9925\n",
            "Validation Loss: 4.608767968074531, Validation Accuracy: 0.84\n",
            "[4/150]: Training Loss: 4.608837638854981, Training Accuracy: 0.935\n",
            "Validation Loss: 4.610516976399027, Validation Accuracy: 0.89\n",
            "[5/150]: Training Loss: 4.608827792358398, Training Accuracy: 0.9775\n",
            "Validation Loss: 4.610510003035236, Validation Accuracy: 0.9\n",
            "[6/150]: Training Loss: 4.608845700073243, Training Accuracy: 0.935\n",
            "Validation Loss: 4.609803257474474, Validation Accuracy: 0.83\n",
            "[7/150]: Training Loss: 4.608741146087646, Training Accuracy: 0.9625\n",
            "Validation Loss: 4.60897787665106, Validation Accuracy: 0.89\n",
            "[8/150]: Training Loss: 4.608720026397705, Training Accuracy: 0.9775\n",
            "Validation Loss: 4.610442146374162, Validation Accuracy: 0.89\n",
            "[9/150]: Training Loss: 4.60900821762085, Training Accuracy: 1.005\n",
            "Validation Loss: 4.609640285467646, Validation Accuracy: 0.9\n",
            "[10/150]: Training Loss: 4.609175831604004, Training Accuracy: 0.9175\n",
            "Validation Loss: 4.609878120908312, Validation Accuracy: 0.95\n",
            "[11/150]: Training Loss: 4.609065142822265, Training Accuracy: 0.8625\n",
            "Validation Loss: 4.610616553361249, Validation Accuracy: 0.9\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 19.343990089027745, Test Accuracy: 1.01\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▃▃▆▆▆▆▇▇▇▇▇▇▇▇█▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Test Loss</td><td>▅▄█▅▅▄▂▄▅▃▃▃▃▄▃▃▃▃▃▃▂▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▅▅▇▅▇▅▆▇█▄▁</td></tr><tr><td>Train Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.01</td></tr><tr><td>Test Loss</td><td>19.34399</td></tr><tr><td>Train Accuracy</td><td>0.8625</td></tr><tr><td>Train Loss</td><td>4.60907</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_035656-za8h22vt/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_035822-1oq8f1cm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm' target=\"_blank\">learning_rate=0.01 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.625637398529053, Training Accuracy: 0.885\n",
            "Validation Loss: 4.60909015509733, Validation Accuracy: 1.06\n",
            "[2/150]: Training Loss: 4.608977200317383, Training Accuracy: 0.94\n",
            "Validation Loss: 4.609363568056921, Validation Accuracy: 0.92\n",
            "[3/150]: Training Loss: 4.608765270996094, Training Accuracy: 0.9275\n",
            "Validation Loss: 4.610460181145152, Validation Accuracy: 0.9\n",
            "[4/150]: Training Loss: 4.609055269622803, Training Accuracy: 0.855\n",
            "Validation Loss: 4.609649321076217, Validation Accuracy: 0.91\n",
            "[5/150]: Training Loss: 4.608893507385254, Training Accuracy: 0.965\n",
            "Validation Loss: 4.610334041012321, Validation Accuracy: 1.15\n",
            "[6/150]: Training Loss: 4.609225297546387, Training Accuracy: 1.0125\n",
            "Validation Loss: 4.608022522774472, Validation Accuracy: 1.16\n",
            "[7/150]: Training Loss: 4.609016616821289, Training Accuracy: 0.9875\n",
            "Validation Loss: 4.609603547746209, Validation Accuracy: 0.82\n",
            "[8/150]: Training Loss: 4.609039859008789, Training Accuracy: 1.0025\n",
            "Validation Loss: 4.608962860836345, Validation Accuracy: 0.88\n",
            "[9/150]: Training Loss: 4.608584417724609, Training Accuracy: 0.98\n",
            "Validation Loss: 4.609565239803047, Validation Accuracy: 0.92\n",
            "[10/150]: Training Loss: 4.609195093536377, Training Accuracy: 0.9675\n",
            "Validation Loss: 4.609652103132503, Validation Accuracy: 0.9\n",
            "[11/150]: Training Loss: 4.608826132202148, Training Accuracy: 0.975\n",
            "Validation Loss: 4.610933561993253, Validation Accuracy: 0.95\n",
            "[12/150]: Training Loss: 4.609026128387451, Training Accuracy: 1.0425\n",
            "Validation Loss: 4.609673776444356, Validation Accuracy: 0.81\n",
            "[13/150]: Training Loss: 4.60898267364502, Training Accuracy: 0.9275\n",
            "Validation Loss: 4.6093105510541585, Validation Accuracy: 0.89\n",
            "[14/150]: Training Loss: 4.608909271240234, Training Accuracy: 0.885\n",
            "Validation Loss: 4.610904125650977, Validation Accuracy: 0.82\n",
            "[15/150]: Training Loss: 4.609115404510498, Training Accuracy: 0.925\n",
            "Validation Loss: 4.609269661508548, Validation Accuracy: 0.9\n",
            "[16/150]: Training Loss: 4.608642578887939, Training Accuracy: 0.9875\n",
            "Validation Loss: 4.610215545459917, Validation Accuracy: 0.81\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 51.00807974748551, Test Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▆▅▇▇███▇▇▇▆▆▆▆▆▇▇▇▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>Test Loss</td><td>▁▇█▆▅▆▆▇█▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▂▄▄▁▅▇▆▇▆▅▅█▄▂▄▆</td></tr><tr><td>Train Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.0</td></tr><tr><td>Test Loss</td><td>51.00808</td></tr><tr><td>Train Accuracy</td><td>0.9875</td></tr><tr><td>Train Loss</td><td>4.60864</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_035822-1oq8f1cm/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_040031-bgr70v9g</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g' target=\"_blank\">learning_rate=0.01 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.626314550018311, Training Accuracy: 0.9725\n",
            "Validation Loss: 4.610879454643103, Validation Accuracy: 0.88\n",
            "[2/150]: Training Loss: 4.609021481323242, Training Accuracy: 0.9225\n",
            "Validation Loss: 4.6096283493527945, Validation Accuracy: 1.0\n",
            "[3/150]: Training Loss: 4.608779692077637, Training Accuracy: 0.9575\n",
            "Validation Loss: 4.610723838684665, Validation Accuracy: 1.07\n",
            "[4/150]: Training Loss: 4.609119167327881, Training Accuracy: 0.9125\n",
            "Validation Loss: 4.60863508236636, Validation Accuracy: 0.89\n",
            "[5/150]: Training Loss: 4.608814185333252, Training Accuracy: 0.95\n",
            "Validation Loss: 4.611634175488903, Validation Accuracy: 0.93\n",
            "[6/150]: Training Loss: 4.6092648048400875, Training Accuracy: 0.8575\n",
            "Validation Loss: 4.610189516832874, Validation Accuracy: 0.92\n",
            "[7/150]: Training Loss: 4.6090187705993655, Training Accuracy: 1.005\n",
            "Validation Loss: 4.610130288798338, Validation Accuracy: 0.88\n",
            "[8/150]: Training Loss: 4.608886881256104, Training Accuracy: 1.01\n",
            "Validation Loss: 4.610127786162552, Validation Accuracy: 0.84\n",
            "[9/150]: Training Loss: 4.608884384155274, Training Accuracy: 1.0175\n",
            "Validation Loss: 4.610361232879056, Validation Accuracy: 0.95\n",
            "[10/150]: Training Loss: 4.60871681137085, Training Accuracy: 1.0025\n",
            "Validation Loss: 4.6113608263100785, Validation Accuracy: 1.03\n",
            "[11/150]: Training Loss: 4.608806131744385, Training Accuracy: 1.035\n",
            "Validation Loss: 4.609321922253651, Validation Accuracy: 0.94\n",
            "[12/150]: Training Loss: 4.6090567108154294, Training Accuracy: 0.9075\n",
            "Validation Loss: 4.6090145414801915, Validation Accuracy: 0.9\n",
            "[13/150]: Training Loss: 4.608706290435791, Training Accuracy: 0.9225\n",
            "Validation Loss: 4.609259438362851, Validation Accuracy: 0.82\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 7.142075587230123, Test Accuracy: 1.08\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▃█▃▁▁▁▃▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>Test Loss</td><td>▁▅▇█▆▆▆▆▇▆▆▆▆▆▆▆▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▆▄▅▃▅▁▇▇▇▇█▃▄</td></tr><tr><td>Train Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.08</td></tr><tr><td>Test Loss</td><td>7.14208</td></tr><tr><td>Train Accuracy</td><td>0.9225</td></tr><tr><td>Train Loss</td><td>4.60871</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_040031-bgr70v9g/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "\n",
        "for lr in learning_rates:\n",
        "  for wd in weight_decays:\n",
        "\n",
        "    print('='*50)\n",
        "    print(f'Hyperparameter with lr:{lr} and wd:{wd}')\n",
        "    print('='*50)\n",
        "\n",
        "    hyperparameters = {'learning_rate': lr,\n",
        "                       'weight_decay' : wd\n",
        "                       }\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(num_epochs, model, train_loader, validation_loader, test_loader, optimizer, scheduler, criterion, device, 'AdamW-HyperParameterTuning', hyperparameters=hyperparameters, is_wandb = True, n_epochs_stop = 10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training Using Best Hyperparameters Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "63dd47b50bdc44e88edd97d14585f7ea",
            "5e0116dc735e4ad995a5b1a039c6a55a",
            "8cf1e952ed294ccdbc3bb2275b5d3efd",
            "7421027bfcb34262a1b99d297e49bf8e",
            "76dfea14213f4e66b269b901150c6cba",
            "d1e4a03094b94c34ac235ffbd0c7fa06",
            "c8e410ff55fc448bbe87c8975a552e88",
            "f49251ce876f4d9292a4179b48ebb22a"
          ]
        },
        "id": "dC9sTmuvEymk",
        "outputId": "66657750-ed1c-435f-c9e5-a2cc4271560e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:z7dx25wy) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/z7dx25wy' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/z7dx25wy</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_002941-z7dx25wy/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:z7dx25wy). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_003350-ulxra0pg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg' target=\"_blank\">learning_rate=0.001 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 3.863059723773576, Training Accuracy: 10.28\n",
            "Validation Loss: 3.371582930255088, Validation Accuracy: 17.92\n",
            "[2/150]: Training Loss: 3.2885083447941734, Training Accuracy: 19.766\n",
            "Validation Loss: 3.0681341438536434, Validation Accuracy: 23.43\n",
            "[3/150]: Training Loss: 3.0068617961595736, Training Accuracy: 25.216\n",
            "Validation Loss: 2.8509516169311135, Validation Accuracy: 28.09\n",
            "[4/150]: Training Loss: 2.8243453850221756, Training Accuracy: 28.812\n",
            "Validation Loss: 2.6608721253218923, Validation Accuracy: 31.85\n",
            "[5/150]: Training Loss: 2.7198512532826884, Training Accuracy: 30.66\n",
            "Validation Loss: 2.614684703243766, Validation Accuracy: 32.43\n",
            "[6/150]: Training Loss: 2.6302451995937415, Training Accuracy: 32.706\n",
            "Validation Loss: 2.523006957807359, Validation Accuracy: 35.09\n",
            "[7/150]: Training Loss: 2.5562818654053046, Training Accuracy: 34.01\n",
            "Validation Loss: 2.4693225895523265, Validation Accuracy: 36.29\n",
            "[8/150]: Training Loss: 2.503678919409242, Training Accuracy: 35.054\n",
            "Validation Loss: 2.4447873132244036, Validation Accuracy: 36.7\n",
            "[9/150]: Training Loss: 2.4561512488538346, Training Accuracy: 36.162\n",
            "Validation Loss: 2.41026672663962, Validation Accuracy: 37.97\n",
            "[10/150]: Training Loss: 2.4110919961234187, Training Accuracy: 37.32\n",
            "Validation Loss: 2.3787831933635055, Validation Accuracy: 37.75\n",
            "[11/150]: Training Loss: 2.374972592686753, Training Accuracy: 37.942\n",
            "Validation Loss: 2.419552489450783, Validation Accuracy: 38.02\n",
            "[12/150]: Training Loss: 2.341363592220999, Training Accuracy: 38.71\n",
            "Validation Loss: 2.353040744544594, Validation Accuracy: 39.18\n",
            "[13/150]: Training Loss: 2.3113136654314785, Training Accuracy: 39.284\n",
            "Validation Loss: 2.329031374044479, Validation Accuracy: 40.04\n",
            "[14/150]: Training Loss: 2.286358056928191, Training Accuracy: 39.702\n",
            "Validation Loss: 2.353606297711658, Validation Accuracy: 38.89\n",
            "[15/150]: Training Loss: 2.255754057251279, Training Accuracy: 40.382\n",
            "Validation Loss: 2.313938088477797, Validation Accuracy: 40.46\n",
            "[16/150]: Training Loss: 2.2417570858660256, Training Accuracy: 40.766\n",
            "Validation Loss: 2.245134223798278, Validation Accuracy: 41.86\n",
            "[17/150]: Training Loss: 2.229791578886759, Training Accuracy: 40.774\n",
            "Validation Loss: 2.2972328845103074, Validation Accuracy: 40.73\n",
            "[18/150]: Training Loss: 2.198040244981761, Training Accuracy: 41.792\n",
            "Validation Loss: 2.284319670337021, Validation Accuracy: 40.6\n",
            "[19/150]: Training Loss: 2.1809934542307157, Training Accuracy: 41.802\n",
            "Validation Loss: 2.2219127932931206, Validation Accuracy: 41.97\n",
            "[20/150]: Training Loss: 2.172169676827043, Training Accuracy: 41.862\n",
            "Validation Loss: 2.3093747029638596, Validation Accuracy: 40.82\n",
            "[21/150]: Training Loss: 2.1469304264353974, Training Accuracy: 42.644\n",
            "Validation Loss: 2.2553204001894422, Validation Accuracy: 41.67\n",
            "[22/150]: Training Loss: 2.1338748418156754, Training Accuracy: 43.018\n",
            "Validation Loss: 2.300890388002821, Validation Accuracy: 40.66\n",
            "[23/150]: Training Loss: 2.120842968259016, Training Accuracy: 43.254\n",
            "Validation Loss: 2.2334311372914892, Validation Accuracy: 41.97\n",
            "[24/150]: Training Loss: 2.0970673977261614, Training Accuracy: 43.616\n",
            "Validation Loss: 2.2538574728996132, Validation Accuracy: 42.09\n",
            "[25/150]: Training Loss: 2.0897668813500565, Training Accuracy: 43.946\n",
            "Validation Loss: 2.2414238969231866, Validation Accuracy: 41.72\n",
            "[26/150]: Training Loss: 2.0756560088423512, Training Accuracy: 44.038\n",
            "Validation Loss: 2.1973593523547907, Validation Accuracy: 43.01\n",
            "[27/150]: Training Loss: 2.057910572537376, Training Accuracy: 44.59\n",
            "Validation Loss: 2.2248903679999574, Validation Accuracy: 42.7\n",
            "[28/150]: Training Loss: 2.043146767274803, Training Accuracy: 45.032\n",
            "Validation Loss: 2.210173088274184, Validation Accuracy: 42.56\n",
            "[29/150]: Training Loss: 2.03314662345535, Training Accuracy: 45.244\n",
            "Validation Loss: 2.2085875697955966, Validation Accuracy: 42.59\n",
            "[30/150]: Training Loss: 2.028535710576245, Training Accuracy: 45.074\n",
            "Validation Loss: 2.2007097385491536, Validation Accuracy: 43.4\n",
            "[31/150]: Training Loss: 2.009151526576723, Training Accuracy: 45.862\n",
            "Validation Loss: 2.201018148926413, Validation Accuracy: 43.24\n",
            "[32/150]: Training Loss: 1.9960764486466527, Training Accuracy: 46.118\n",
            "Validation Loss: 2.207929955925911, Validation Accuracy: 43.19\n",
            "[33/150]: Training Loss: 1.988575564930811, Training Accuracy: 46.282\n",
            "Validation Loss: 2.2023386712286883, Validation Accuracy: 43.24\n",
            "[34/150]: Training Loss: 1.9779068563905213, Training Accuracy: 46.246\n",
            "Validation Loss: 2.1736220698447744, Validation Accuracy: 43.85\n",
            "[35/150]: Training Loss: 1.9696562569159681, Training Accuracy: 46.612\n",
            "Validation Loss: 2.2412819467532406, Validation Accuracy: 42.97\n",
            "[36/150]: Training Loss: 1.9518849866469499, Training Accuracy: 47.002\n",
            "Validation Loss: 2.142467891334728, Validation Accuracy: 44.48\n",
            "[37/150]: Training Loss: 1.942737179491526, Training Accuracy: 47.244\n",
            "Validation Loss: 2.1808249912444193, Validation Accuracy: 43.96\n",
            "[38/150]: Training Loss: 1.934893620441027, Training Accuracy: 47.278\n",
            "Validation Loss: 2.194337487220764, Validation Accuracy: 44.03\n",
            "[39/150]: Training Loss: 1.9254444508296449, Training Accuracy: 47.672\n",
            "Validation Loss: 2.1627620777506738, Validation Accuracy: 44.64\n",
            "[40/150]: Training Loss: 1.9116085487253525, Training Accuracy: 47.786\n",
            "Validation Loss: 2.143426476770146, Validation Accuracy: 44.69\n",
            "[41/150]: Training Loss: 1.8990542265155432, Training Accuracy: 48.106\n",
            "Validation Loss: 2.1570872994744854, Validation Accuracy: 44.18\n",
            "[42/150]: Training Loss: 1.8952381147448059, Training Accuracy: 48.148\n",
            "Validation Loss: 2.179479962701251, Validation Accuracy: 44.42\n",
            "[43/150]: Training Loss: 1.8831077039699116, Training Accuracy: 48.608\n",
            "Validation Loss: 2.14893990337469, Validation Accuracy: 44.65\n",
            "[44/150]: Training Loss: 1.8653588525169646, Training Accuracy: 49.024\n",
            "Validation Loss: 2.139679863954046, Validation Accuracy: 44.98\n",
            "[45/150]: Training Loss: 1.8571323093855778, Training Accuracy: 49.132\n",
            "Validation Loss: 2.1511670715489966, Validation Accuracy: 45.05\n",
            "[46/150]: Training Loss: 1.851196085248152, Training Accuracy: 49.268\n",
            "Validation Loss: 2.1228003342440176, Validation Accuracy: 45.15\n",
            "[47/150]: Training Loss: 1.8476994010188696, Training Accuracy: 49.152\n",
            "Validation Loss: 2.2052687976011045, Validation Accuracy: 44.33\n",
            "[48/150]: Training Loss: 1.834682262919443, Training Accuracy: 49.48\n",
            "Validation Loss: 2.167970584456328, Validation Accuracy: 44.5\n",
            "[49/150]: Training Loss: 1.8160154127403902, Training Accuracy: 50.016\n",
            "Validation Loss: 2.1706748312445963, Validation Accuracy: 44.33\n",
            "[50/150]: Training Loss: 1.8177930782823002, Training Accuracy: 49.864\n",
            "Validation Loss: 2.1794446357496224, Validation Accuracy: 44.96\n",
            "[51/150]: Training Loss: 1.8067406710913725, Training Accuracy: 50.086\n",
            "Validation Loss: 2.1342575109688338, Validation Accuracy: 45.96\n",
            "[52/150]: Training Loss: 1.7917756205019744, Training Accuracy: 50.474\n",
            "Validation Loss: 2.1779661793617686, Validation Accuracy: 45.2\n",
            "[53/150]: Training Loss: 1.7835827201528622, Training Accuracy: 50.574\n",
            "Validation Loss: 2.105961331136667, Validation Accuracy: 46.26\n",
            "[54/150]: Training Loss: 1.775593501527596, Training Accuracy: 50.768\n",
            "Validation Loss: 2.135809161860472, Validation Accuracy: 45.59\n",
            "[55/150]: Training Loss: 1.774858244087385, Training Accuracy: 50.898\n",
            "Validation Loss: 2.129430113324694, Validation Accuracy: 45.8\n",
            "[56/150]: Training Loss: 1.7574954881997364, Training Accuracy: 51.258\n",
            "Validation Loss: 2.1614714724243065, Validation Accuracy: 45.34\n",
            "[57/150]: Training Loss: 1.7551402684367832, Training Accuracy: 51.536\n",
            "Validation Loss: 2.116780410906312, Validation Accuracy: 45.57\n",
            "[58/150]: Training Loss: 1.7323481986284865, Training Accuracy: 51.87\n",
            "Validation Loss: 2.1469044146264435, Validation Accuracy: 46.29\n",
            "[59/150]: Training Loss: 1.731923724684264, Training Accuracy: 52.004\n",
            "Validation Loss: 2.103028847913074, Validation Accuracy: 46.36\n",
            "[60/150]: Training Loss: 1.7310465676400362, Training Accuracy: 52.0\n",
            "Validation Loss: 2.1536702441561753, Validation Accuracy: 45.7\n",
            "[61/150]: Training Loss: 1.7244189166656845, Training Accuracy: 51.944\n",
            "Validation Loss: 2.140022467655741, Validation Accuracy: 45.48\n",
            "[62/150]: Training Loss: 1.7049367572645397, Training Accuracy: 52.6\n",
            "Validation Loss: 2.1410929146845628, Validation Accuracy: 46.43\n",
            "[63/150]: Training Loss: 1.6979511242998226, Training Accuracy: 52.706\n",
            "Validation Loss: 2.159957699714952, Validation Accuracy: 45.77\n",
            "[64/150]: Training Loss: 1.6934019074110729, Training Accuracy: 52.83\n",
            "Validation Loss: 2.1361425302590535, Validation Accuracy: 46.59\n",
            "[65/150]: Training Loss: 1.6794907693058023, Training Accuracy: 53.024\n",
            "Validation Loss: 2.1693425915043827, Validation Accuracy: 46.15\n",
            "[66/150]: Training Loss: 1.6717879327056964, Training Accuracy: 53.454\n",
            "Validation Loss: 2.1304182694975737, Validation Accuracy: 46.71\n",
            "[67/150]: Training Loss: 1.6595926739828055, Training Accuracy: 53.614\n",
            "Validation Loss: 2.120200866346906, Validation Accuracy: 46.39\n",
            "[68/150]: Training Loss: 1.648870440395287, Training Accuracy: 54.184\n",
            "Validation Loss: 2.098656458459842, Validation Accuracy: 46.87\n",
            "[69/150]: Training Loss: 1.63317440499735, Training Accuracy: 53.856\n",
            "Validation Loss: 2.10397704163934, Validation Accuracy: 47.28\n",
            "[70/150]: Training Loss: 1.6387051284465644, Training Accuracy: 54.282\n",
            "Validation Loss: 2.0964533455052954, Validation Accuracy: 47.14\n",
            "[71/150]: Training Loss: 1.6188073368633495, Training Accuracy: 54.606\n",
            "Validation Loss: 2.12077300032233, Validation Accuracy: 46.67\n",
            "[72/150]: Training Loss: 1.6113011437608762, Training Accuracy: 54.874\n",
            "Validation Loss: 2.1084914108750166, Validation Accuracy: 47.47\n",
            "[73/150]: Training Loss: 1.6181168059253936, Training Accuracy: 54.584\n",
            "Validation Loss: 2.0926199581972353, Validation Accuracy: 47.8\n",
            "[74/150]: Training Loss: 1.6009672808525202, Training Accuracy: 55.14\n",
            "Validation Loss: 2.1137902524061265, Validation Accuracy: 47.2\n",
            "[75/150]: Training Loss: 1.5877168545942477, Training Accuracy: 55.52\n",
            "Validation Loss: 2.0988229885222807, Validation Accuracy: 47.29\n",
            "[76/150]: Training Loss: 1.5841481110933797, Training Accuracy: 55.536\n",
            "Validation Loss: 2.130301916675203, Validation Accuracy: 47.01\n",
            "[77/150]: Training Loss: 1.572215504810938, Training Accuracy: 55.894\n",
            "Validation Loss: 2.149995141727909, Validation Accuracy: 47.05\n",
            "[78/150]: Training Loss: 1.5624580438179738, Training Accuracy: 55.868\n",
            "Validation Loss: 2.111784276688934, Validation Accuracy: 47.42\n",
            "[79/150]: Training Loss: 1.5614525537051813, Training Accuracy: 55.984\n",
            "Validation Loss: 2.114105687019931, Validation Accuracy: 47.26\n",
            "[80/150]: Training Loss: 1.539920823577115, Training Accuracy: 56.65\n",
            "Validation Loss: 2.113597672456389, Validation Accuracy: 48.04\n",
            "[81/150]: Training Loss: 1.5321300439822398, Training Accuracy: 56.72\n",
            "Validation Loss: 2.115057561048277, Validation Accuracy: 47.52\n",
            "[82/150]: Training Loss: 1.5347046963394146, Training Accuracy: 56.472\n",
            "Validation Loss: 2.120380314292422, Validation Accuracy: 47.69\n",
            "[83/150]: Training Loss: 1.5234107173922118, Training Accuracy: 56.854\n",
            "Validation Loss: 2.117662426772391, Validation Accuracy: 47.66\n",
            "[84/150]: Training Loss: 1.5176254797469624, Training Accuracy: 57.066\n",
            "Validation Loss: 2.1148886255397916, Validation Accuracy: 47.15\n",
            "[85/150]: Training Loss: 1.5007459478610008, Training Accuracy: 57.452\n",
            "Validation Loss: 2.114618365931663, Validation Accuracy: 47.39\n",
            "[86/150]: Training Loss: 1.4968964526872806, Training Accuracy: 57.466\n",
            "Validation Loss: 2.1021699586491676, Validation Accuracy: 47.78\n",
            "[87/150]: Training Loss: 1.4844805052518235, Training Accuracy: 57.862\n",
            "Validation Loss: 2.1039314968570784, Validation Accuracy: 47.72\n",
            "[88/150]: Training Loss: 1.4738873809652255, Training Accuracy: 58.114\n",
            "Validation Loss: 2.097645890181232, Validation Accuracy: 48.24\n",
            "[89/150]: Training Loss: 1.483021430271056, Training Accuracy: 57.876\n",
            "Validation Loss: 2.108822873443555, Validation Accuracy: 47.97\n",
            "[90/150]: Training Loss: 1.4626602787343437, Training Accuracy: 58.702\n",
            "Validation Loss: 2.0972186729406856, Validation Accuracy: 47.88\n",
            "[91/150]: Training Loss: 1.4539960583152673, Training Accuracy: 58.564\n",
            "Validation Loss: 2.1077453277672933, Validation Accuracy: 48.02\n",
            "[92/150]: Training Loss: 1.4446301146236527, Training Accuracy: 58.7\n",
            "Validation Loss: 2.0844076651676446, Validation Accuracy: 48.44\n",
            "[93/150]: Training Loss: 1.4391108949471008, Training Accuracy: 59.026\n",
            "Validation Loss: 2.093005408147338, Validation Accuracy: 48.37\n",
            "[94/150]: Training Loss: 1.423930914581889, Training Accuracy: 59.616\n",
            "Validation Loss: 2.1331047563795833, Validation Accuracy: 47.89\n",
            "[95/150]: Training Loss: 1.41947230582347, Training Accuracy: 59.502\n",
            "Validation Loss: 2.1126899483856882, Validation Accuracy: 48.0\n",
            "[96/150]: Training Loss: 1.4149078359384366, Training Accuracy: 59.56\n",
            "Validation Loss: 2.0845493116196554, Validation Accuracy: 48.7\n",
            "[97/150]: Training Loss: 1.3990580768841308, Training Accuracy: 60.132\n",
            "Validation Loss: 2.1009285654991294, Validation Accuracy: 48.44\n",
            "[98/150]: Training Loss: 1.4016986463380896, Training Accuracy: 60.082\n",
            "Validation Loss: 2.1022102696121117, Validation Accuracy: 48.23\n",
            "[99/150]: Training Loss: 1.391786497572194, Training Accuracy: 60.44\n",
            "Validation Loss: 2.087019986407772, Validation Accuracy: 48.33\n",
            "[100/150]: Training Loss: 1.3808096985682807, Training Accuracy: 60.598\n",
            "Validation Loss: 2.1079373640619266, Validation Accuracy: 48.46\n",
            "[101/150]: Training Loss: 1.375501683377244, Training Accuracy: 60.572\n",
            "Validation Loss: 2.1078098131592866, Validation Accuracy: 48.82\n",
            "[102/150]: Training Loss: 1.3738657930470488, Training Accuracy: 60.484\n",
            "Validation Loss: 2.118699251466496, Validation Accuracy: 47.93\n",
            "[103/150]: Training Loss: 1.366849816973557, Training Accuracy: 60.848\n",
            "Validation Loss: 2.07916833564734, Validation Accuracy: 49.02\n",
            "[104/150]: Training Loss: 1.3610880574606874, Training Accuracy: 60.932\n",
            "Validation Loss: 2.087844172860407, Validation Accuracy: 48.91\n",
            "[105/150]: Training Loss: 1.34294292833799, Training Accuracy: 61.462\n",
            "Validation Loss: 2.1156119442289802, Validation Accuracy: 49.17\n",
            "[106/150]: Training Loss: 1.3525332610320557, Training Accuracy: 61.288\n",
            "Validation Loss: 2.0910707043994003, Validation Accuracy: 48.55\n",
            "[107/150]: Training Loss: 1.331598934538834, Training Accuracy: 61.91\n",
            "Validation Loss: 2.1001909211942347, Validation Accuracy: 49.0\n",
            "[108/150]: Training Loss: 1.3267096242179042, Training Accuracy: 61.816\n",
            "Validation Loss: 2.1083290622492505, Validation Accuracy: 48.74\n",
            "[109/150]: Training Loss: 1.3154139440230397, Training Accuracy: 62.34\n",
            "Validation Loss: 2.1171723535865734, Validation Accuracy: 49.04\n",
            "[110/150]: Training Loss: 1.3305041518662593, Training Accuracy: 61.83\n",
            "Validation Loss: 2.0921388515241586, Validation Accuracy: 49.21\n",
            "[111/150]: Training Loss: 1.3154786750483696, Training Accuracy: 61.994\n",
            "Validation Loss: 2.1219316781706112, Validation Accuracy: 48.77\n",
            "[112/150]: Training Loss: 1.3147668346876988, Training Accuracy: 61.946\n",
            "Validation Loss: 2.0896254671607046, Validation Accuracy: 48.89\n",
            "[113/150]: Training Loss: 1.3001932930915863, Training Accuracy: 62.724\n",
            "Validation Loss: 2.103232970662937, Validation Accuracy: 49.02\n",
            "[114/150]: Training Loss: 1.2976091802120209, Training Accuracy: 62.64\n",
            "Validation Loss: 2.089232644457726, Validation Accuracy: 48.88\n",
            "[115/150]: Training Loss: 1.2918279953015126, Training Accuracy: 62.894\n",
            "Validation Loss: 2.098377283971021, Validation Accuracy: 49.17\n",
            "[116/150]: Training Loss: 1.2901173764482483, Training Accuracy: 63.008\n",
            "Validation Loss: 2.096894028080497, Validation Accuracy: 49.39\n",
            "[117/150]: Training Loss: 1.275385139619603, Training Accuracy: 63.266\n",
            "Validation Loss: 2.1110763952230953, Validation Accuracy: 49.35\n",
            "[118/150]: Training Loss: 1.2755424442803462, Training Accuracy: 63.438\n",
            "Validation Loss: 2.1062107443050215, Validation Accuracy: 49.34\n",
            "[119/150]: Training Loss: 1.2707537008673333, Training Accuracy: 63.144\n",
            "Validation Loss: 2.094398064977804, Validation Accuracy: 49.44\n",
            "[120/150]: Training Loss: 1.2619870495613275, Training Accuracy: 63.662\n",
            "Validation Loss: 2.109912445590754, Validation Accuracy: 49.31\n",
            "[121/150]: Training Loss: 1.2651302716921053, Training Accuracy: 63.564\n",
            "Validation Loss: 2.1148870439286442, Validation Accuracy: 48.87\n",
            "[122/150]: Training Loss: 1.2517427335614744, Training Accuracy: 63.992\n",
            "Validation Loss: 2.111772822726304, Validation Accuracy: 49.14\n",
            "[123/150]: Training Loss: 1.2525572213522917, Training Accuracy: 63.8\n",
            "Validation Loss: 2.097744130784539, Validation Accuracy: 49.4\n",
            "[124/150]: Training Loss: 1.2481763019891041, Training Accuracy: 63.982\n",
            "Validation Loss: 2.110331610509544, Validation Accuracy: 49.5\n",
            "[125/150]: Training Loss: 1.242631159322646, Training Accuracy: 64.258\n",
            "Validation Loss: 2.113812481521801, Validation Accuracy: 49.25\n",
            "[126/150]: Training Loss: 1.2439294564144692, Training Accuracy: 64.134\n",
            "Validation Loss: 2.106003999710083, Validation Accuracy: 49.44\n",
            "[127/150]: Training Loss: 1.2376054860746768, Training Accuracy: 64.1\n",
            "Validation Loss: 2.1138451304405357, Validation Accuracy: 49.45\n",
            "[128/150]: Training Loss: 1.2359750416425184, Training Accuracy: 64.344\n",
            "Validation Loss: 2.0956578482488157, Validation Accuracy: 49.5\n",
            "[129/150]: Training Loss: 1.2426931114910205, Training Accuracy: 64.084\n",
            "Validation Loss: 2.102140469915548, Validation Accuracy: 49.39\n",
            "[130/150]: Training Loss: 1.2271092739099128, Training Accuracy: 64.588\n",
            "Validation Loss: 2.1065317156967844, Validation Accuracy: 49.55\n",
            "[131/150]: Training Loss: 1.2293265251552357, Training Accuracy: 64.606\n",
            "Validation Loss: 2.1087451816364458, Validation Accuracy: 49.71\n",
            "[132/150]: Training Loss: 1.2165279118606196, Training Accuracy: 64.626\n",
            "Validation Loss: 2.1100895678161815, Validation Accuracy: 49.58\n",
            "[133/150]: Training Loss: 1.2260109276112998, Training Accuracy: 64.716\n",
            "Validation Loss: 2.107479982315355, Validation Accuracy: 49.48\n",
            "[134/150]: Training Loss: 1.2170597407823938, Training Accuracy: 64.91\n",
            "Validation Loss: 2.109668661075033, Validation Accuracy: 49.21\n",
            "[135/150]: Training Loss: 1.2119625063655932, Training Accuracy: 64.864\n",
            "Validation Loss: 2.1121888259413897, Validation Accuracy: 49.6\n",
            "[136/150]: Training Loss: 1.2143927402508534, Training Accuracy: 64.752\n",
            "Validation Loss: 2.1146486792594765, Validation Accuracy: 49.42\n",
            "[137/150]: Training Loss: 1.20208662275768, Training Accuracy: 65.196\n",
            "Validation Loss: 2.1086552408850117, Validation Accuracy: 49.53\n",
            "[138/150]: Training Loss: 1.2066055967679719, Training Accuracy: 65.132\n",
            "Validation Loss: 2.106570492884156, Validation Accuracy: 49.36\n",
            "[139/150]: Training Loss: 1.2045007688767464, Training Accuracy: 65.206\n",
            "Validation Loss: 2.1134595893750525, Validation Accuracy: 49.61\n",
            "[140/150]: Training Loss: 1.209533206413469, Training Accuracy: 65.066\n",
            "Validation Loss: 2.111029122285782, Validation Accuracy: 49.59\n",
            "[141/150]: Training Loss: 1.2018247985321542, Training Accuracy: 64.974\n",
            "Validation Loss: 2.112992768834351, Validation Accuracy: 49.59\n",
            "[142/150]: Training Loss: 1.2081030414384955, Training Accuracy: 65.38\n",
            "Validation Loss: 2.1114872906618056, Validation Accuracy: 49.54\n",
            "[143/150]: Training Loss: 1.1987551257128606, Training Accuracy: 65.484\n",
            "Validation Loss: 2.11025989283422, Validation Accuracy: 49.53\n",
            "[144/150]: Training Loss: 1.204268764961711, Training Accuracy: 65.084\n",
            "Validation Loss: 2.1116166456489807, Validation Accuracy: 49.52\n",
            "[145/150]: Training Loss: 1.2022794322741917, Training Accuracy: 65.088\n",
            "Validation Loss: 2.1134905511406576, Validation Accuracy: 49.47\n",
            "[146/150]: Training Loss: 1.1945363251144623, Training Accuracy: 65.452\n",
            "Validation Loss: 2.1135922700736174, Validation Accuracy: 49.52\n",
            "[147/150]: Training Loss: 1.1900141044803287, Training Accuracy: 65.494\n",
            "Validation Loss: 2.113608984430884, Validation Accuracy: 49.49\n",
            "[148/150]: Training Loss: 1.2009751156467916, Training Accuracy: 65.298\n",
            "Validation Loss: 2.1133065656491907, Validation Accuracy: 49.47\n",
            "[149/150]: Training Loss: 1.2031257037464005, Training Accuracy: 65.434\n",
            "Validation Loss: 2.113412342253764, Validation Accuracy: 49.47\n",
            "[150/150]: Training Loss: 1.1963014528726983, Training Accuracy: 65.458\n",
            "Validation Loss: 2.113395432757724, Validation Accuracy: 49.47\n",
            "**********************************************************************\n",
            "Test Loss: 2.113395432757724, Test Accuracy: 49.47\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▅▃▂▁▂▁▁▂▁▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>Test Loss</td><td>▁▃▆▃█▆▅▇▅▅▃▃▃▄▄▃▃▃▄▅▆▅▅▄▄▅▅▅▄▄▄▄▄▄▄▄▄▄▃▃</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>Train Loss</td><td>█▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>49.47</td></tr><tr><td>Test Loss</td><td>2.1134</td></tr><tr><td>Train Accuracy</td><td>65.458</td></tr><tr><td>Train Loss</td><td>1.1963</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_003350-ulxra0pg/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 1e-03\n",
        "wd = 4e-04\n",
        "\n",
        "hyperparameters = {'learning_rate': lr,\n",
        "                    'weight_decay' : wd\n",
        "                  }\n",
        "\n",
        "# Load the model\n",
        "model_1 = LeNet5().to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer_1 = torch.optim.AdamW(model_1.parameters(), lr=lr, weight_decay=wd)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_1, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# Training\n",
        "run_training(num_epochs, model_1, original_train_loader, original_test_loader, original_test_loader, optimizer_1, scheduler, criterion, device, optimizer_name='AdamW', hyperparameters=hyperparameters, is_wandb = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeMWj_f0sbQE"
      },
      "source": [
        "# **Large Batch Optimizers**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LARS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WkZmVFG0q90m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class LARS(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements LARS (Layer-wise Adaptive Rate Scaling).\n",
        "\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "        lr (float): learning rate (default: 1e-3)\n",
        "        momentum (float, optional): momentum factor (default: 0)\n",
        "        trust_coef (float, optional): LARS coefficient as used in the paper (default: 1e-3)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        dampening (float, optional): dampening for momentum (default: 0)\n",
        "        nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
        "        epsilon (float, optional): epsilon to prevent zero division (default: 0)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            params,\n",
        "            lr: float = 1e-3,\n",
        "            momentum: float = 0,\n",
        "            trust_coef: float = 1e-3,\n",
        "            dampening: float = 0,\n",
        "            weight_decay: float = 0,\n",
        "            nesterov=False,\n",
        "            epsilon: float = 1e-9\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a new instance of the LARS optimizer.\n",
        "\n",
        "        Args:\n",
        "            params: iterable of parameters to optimize or dicts defining\n",
        "            lr: learning rate\n",
        "            momentum: momentum factor\n",
        "            trust_coef: LARS coefficient as used in the paper\n",
        "            weight_decay: weight decay (L2 penalty)\n",
        "            dampening: dampening for momentum\n",
        "            nesterov: enables Nesterov momentum\n",
        "            epsilon: epsilon to prevent zero division\n",
        "        \"\"\"\n",
        "\n",
        "        if lr <= 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            trust_coef=trust_coef,\n",
        "            dampening=dampening,\n",
        "            weight_decay=weight_decay,\n",
        "            nesterov=nesterov,\n",
        "            epsilon=epsilon)\n",
        "\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(LARS, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        \"\"\"\n",
        "        Sets the state of the optimizer.\n",
        "\n",
        "        Args:\n",
        "            state: The state to set the optimizer to.\n",
        "        \"\"\"\n",
        "        super(LARS, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def _compute_local_lr(self, p, weight_decay, trust_coef, epsilon):\n",
        "        \"\"\"\n",
        "        Computes the local learning rate for a given parameter.\n",
        "\n",
        "        Args:\n",
        "            p: The parameter to compute the local learning rate for.\n",
        "            weight_decay: The weight decay factor.\n",
        "            trust_coef: The trust coefficient.\n",
        "            epsilon: A small constant for numerical stability.\n",
        "\n",
        "        Returns:\n",
        "            float: The computed local learning rate.\n",
        "        \"\"\"\n",
        "        w_norm = torch.norm(p.data)\n",
        "        g_norm = torch.norm(p.grad.data)\n",
        "        if w_norm * g_norm > 0:\n",
        "            return trust_coef * w_norm / (g_norm + weight_decay * w_norm + epsilon)\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    def _update_params(self, p, d_p, local_lr, lr, momentum, buf,\n",
        "                       dampening, nesterov, weight_decay):\n",
        "        \"\"\"\n",
        "        Updates the parameters with the computed update.\n",
        "\n",
        "        Args:\n",
        "            p: The parameter to be updated.\n",
        "            d_p: The computed update for the parameter.\n",
        "            local_lr: The local learning rate.\n",
        "            lr: The global learning rate.\n",
        "            momentum: The momentum factor.\n",
        "            buf: The buffer for the momentum.\n",
        "            dampening: The dampening for the momentum.\n",
        "            nesterov: A flag indicating whether to use Nesterov momentum.\n",
        "            weight_decay: The weight decay factor.\n",
        "        \"\"\"\n",
        "        if weight_decay != 0:\n",
        "            d_p.add_(weight_decay, p.data)\n",
        "        if momentum != 0:\n",
        "            param_state = self.state[p]\n",
        "            if 'momentum_buffer' not in param_state:\n",
        "                buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "            else:\n",
        "                buf = param_state['momentum_buffer']\n",
        "            buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "            if nesterov:\n",
        "                d_p = d_p.add(momentum, buf)\n",
        "            else:\n",
        "                d_p = buf\n",
        "\n",
        "        p.data.add_(-local_lr * lr, d_p)\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        \"\"\"\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            trust_coef = group['trust_coef']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "            epsilon = group['epsilon']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                local_lr = self._compute_local_lr(p, weight_decay, trust_coef, epsilon)\n",
        "                self._update_params(p, p.grad.data, local_lr, group['lr'], momentum, None, dampening, nesterov, weight_decay)\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "dh421LdmM9XP"
      },
      "outputs": [],
      "source": [
        "learning_rates = [1e-02, 5e-02, 1e-01, 5e-01, 1, 1.5, 2]\n",
        "wd = 1e-03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LARS Hyperparameter Tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "30d70d57987c4349b55560e5d9626201",
            "36ac25551e574241bed4d7e5a4301d95",
            "6898349b9c754ca7be91bea29cabbba5",
            "623f947ef06c41569be7fcdda4141174",
            "d268f74266044451b230f20756bab2f6",
            "6896790e80d1450c821918c7ef41e42f",
            "a30700c8bfb449cda98a40d75828f3d3",
            "77beee57b67d4bad826f616e5483b87a",
            "c59ee0d8b9fb4694ada362115bf965a9",
            "ab62d64100a84b1faae744ee0f99501a",
            "6abbcbe71a314356ba04f8349d1fc127",
            "97c3921b8e454ccdb7b4db95d4440c64",
            "50b4ba3147b540abad020ae780353f27",
            "584e5a6aada04535b64c40c3ece566e8",
            "d3326a99729a416abca13d3122196e64",
            "1d534596e41340438fc8d083fff6aa8b",
            "19ed5f5d4f714246bbdf44513ecc50f0",
            "1f9d7ac8fa8a4bcdb76fe2b2cb443f66",
            "71c0cc303cfc44ba8d989d5f8feca119",
            "07dc9b3c563e4615bec4dbd3233bf4ba",
            "a3ccc0b32e9a4ceebd6045dff63e6621",
            "104402d4cba5477499593d972bc48e9d",
            "80fe0a51b14b4e548454d4f83215336c",
            "8e13a8bef6e14973912966984e83cd4c"
          ]
        },
        "id": "dzsCB_Q9NnCD",
        "outputId": "56d144bd-faa8-4df2-a19c-7cc15394622d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_022555-nh2g7isx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx' target=\"_blank\">learning_rate=0.01 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605613048553467, Training Accuracy: 1.0325\n",
            "Validation Loss: 4.605481047539195, Validation Accuracy: 1.01\n",
            "[2/150]: Training Loss: 4.603893094635009, Training Accuracy: 1.2825\n",
            "Validation Loss: 4.603514659176966, Validation Accuracy: 1.37\n",
            "[3/150]: Training Loss: 4.601452378845215, Training Accuracy: 1.7625\n",
            "Validation Loss: 4.600479241389378, Validation Accuracy: 1.96\n",
            "[4/150]: Training Loss: 4.597398109436035, Training Accuracy: 1.815\n",
            "Validation Loss: 4.595512341541849, Validation Accuracy: 1.41\n",
            "[5/150]: Training Loss: 4.590549831390381, Training Accuracy: 1.5175\n",
            "Validation Loss: 4.587287993947411, Validation Accuracy: 1.68\n",
            "[6/150]: Training Loss: 4.579249038696289, Training Accuracy: 1.93\n",
            "Validation Loss: 4.573939116897097, Validation Accuracy: 2.37\n",
            "[7/150]: Training Loss: 4.560530513763427, Training Accuracy: 2.69\n",
            "Validation Loss: 4.5522112117451465, Validation Accuracy: 2.64\n",
            "[8/150]: Training Loss: 4.5309873321533205, Training Accuracy: 2.92\n",
            "Validation Loss: 4.519049043108703, Validation Accuracy: 2.85\n",
            "[9/150]: Training Loss: 4.488997451782226, Training Accuracy: 2.9475\n",
            "Validation Loss: 4.475813519423175, Validation Accuracy: 3.12\n",
            "[10/150]: Training Loss: 4.439574425506592, Training Accuracy: 3.3375\n",
            "Validation Loss: 4.429785430811013, Validation Accuracy: 3.32\n",
            "[11/150]: Training Loss: 4.391272030639648, Training Accuracy: 3.61\n",
            "Validation Loss: 4.386288050633327, Validation Accuracy: 3.95\n",
            "[12/150]: Training Loss: 4.346567308807373, Training Accuracy: 4.215\n",
            "Validation Loss: 4.348948326839763, Validation Accuracy: 4.02\n",
            "[13/150]: Training Loss: 4.3075446601867675, Training Accuracy: 4.48\n",
            "Validation Loss: 4.312188755934406, Validation Accuracy: 4.38\n",
            "[14/150]: Training Loss: 4.274280471801758, Training Accuracy: 4.9575\n",
            "Validation Loss: 4.284171402074729, Validation Accuracy: 4.91\n",
            "[15/150]: Training Loss: 4.24782478981018, Training Accuracy: 5.3275\n",
            "Validation Loss: 4.261963683328811, Validation Accuracy: 5.05\n",
            "[16/150]: Training Loss: 4.226929823303223, Training Accuracy: 5.7425\n",
            "Validation Loss: 4.2434815540435205, Validation Accuracy: 5.46\n",
            "[17/150]: Training Loss: 4.210706592941285, Training Accuracy: 5.9375\n",
            "Validation Loss: 4.230875344792748, Validation Accuracy: 5.78\n",
            "[18/150]: Training Loss: 4.197167670059204, Training Accuracy: 6.23\n",
            "Validation Loss: 4.218225898256727, Validation Accuracy: 5.69\n",
            "[19/150]: Training Loss: 4.1860190193176265, Training Accuracy: 6.505\n",
            "Validation Loss: 4.208940963076937, Validation Accuracy: 5.86\n",
            "[20/150]: Training Loss: 4.175828193664551, Training Accuracy: 6.6025\n",
            "Validation Loss: 4.201019479970264, Validation Accuracy: 6.0\n",
            "[21/150]: Training Loss: 4.167100285339355, Training Accuracy: 6.8025\n",
            "Validation Loss: 4.1936384735593375, Validation Accuracy: 5.97\n",
            "[22/150]: Training Loss: 4.158846283340454, Training Accuracy: 6.92\n",
            "Validation Loss: 4.185141735016161, Validation Accuracy: 6.28\n",
            "[23/150]: Training Loss: 4.151738377380371, Training Accuracy: 7.0375\n",
            "Validation Loss: 4.178911400448745, Validation Accuracy: 6.83\n",
            "[24/150]: Training Loss: 4.144738430023193, Training Accuracy: 7.23\n",
            "Validation Loss: 4.172337419667821, Validation Accuracy: 6.52\n",
            "[25/150]: Training Loss: 4.1379581127166745, Training Accuracy: 7.265\n",
            "Validation Loss: 4.165943450988478, Validation Accuracy: 6.63\n",
            "[26/150]: Training Loss: 4.132068264770508, Training Accuracy: 7.4525\n",
            "Validation Loss: 4.160335552920202, Validation Accuracy: 7.0\n",
            "[27/150]: Training Loss: 4.126062253570557, Training Accuracy: 7.4725\n",
            "Validation Loss: 4.157630066962758, Validation Accuracy: 6.66\n",
            "[28/150]: Training Loss: 4.120529958724975, Training Accuracy: 7.52\n",
            "Validation Loss: 4.150649539983956, Validation Accuracy: 7.07\n",
            "[29/150]: Training Loss: 4.115550531768799, Training Accuracy: 7.6175\n",
            "Validation Loss: 4.145238416210102, Validation Accuracy: 7.01\n",
            "[30/150]: Training Loss: 4.109972083282471, Training Accuracy: 7.74\n",
            "Validation Loss: 4.140822541182208, Validation Accuracy: 6.87\n",
            "[31/150]: Training Loss: 4.1054605201721195, Training Accuracy: 7.7775\n",
            "Validation Loss: 4.135137576206475, Validation Accuracy: 7.26\n",
            "[32/150]: Training Loss: 4.100883611297608, Training Accuracy: 7.9175\n",
            "Validation Loss: 4.13245949623691, Validation Accuracy: 7.37\n",
            "[33/150]: Training Loss: 4.096106577682495, Training Accuracy: 7.965\n",
            "Validation Loss: 4.1272796597450405, Validation Accuracy: 7.21\n",
            "[34/150]: Training Loss: 4.092031303024292, Training Accuracy: 8.075\n",
            "Validation Loss: 4.1231977316983945, Validation Accuracy: 7.19\n",
            "[35/150]: Training Loss: 4.087661064910889, Training Accuracy: 8.0775\n",
            "Validation Loss: 4.119425037104612, Validation Accuracy: 7.42\n",
            "[36/150]: Training Loss: 4.083959820175171, Training Accuracy: 8.14\n",
            "Validation Loss: 4.117144297642313, Validation Accuracy: 7.43\n",
            "[37/150]: Training Loss: 4.079987169265747, Training Accuracy: 8.2575\n",
            "Validation Loss: 4.112940530108798, Validation Accuracy: 7.33\n",
            "[38/150]: Training Loss: 4.076256622695923, Training Accuracy: 8.275\n",
            "Validation Loss: 4.10887487071335, Validation Accuracy: 7.72\n",
            "[39/150]: Training Loss: 4.072722610473633, Training Accuracy: 8.2625\n",
            "Validation Loss: 4.1064621779569395, Validation Accuracy: 7.62\n",
            "[40/150]: Training Loss: 4.06919368019104, Training Accuracy: 8.4025\n",
            "Validation Loss: 4.102874081605559, Validation Accuracy: 7.77\n",
            "[41/150]: Training Loss: 4.065893580627441, Training Accuracy: 8.425\n",
            "Validation Loss: 4.101947505003328, Validation Accuracy: 7.99\n",
            "[42/150]: Training Loss: 4.062419548797608, Training Accuracy: 8.64\n",
            "Validation Loss: 4.096677272942416, Validation Accuracy: 7.69\n",
            "[43/150]: Training Loss: 4.059447618484497, Training Accuracy: 8.6025\n",
            "Validation Loss: 4.096899234565201, Validation Accuracy: 7.72\n",
            "[44/150]: Training Loss: 4.056013444137573, Training Accuracy: 8.5725\n",
            "Validation Loss: 4.091373089772121, Validation Accuracy: 7.88\n",
            "[45/150]: Training Loss: 4.053135622024536, Training Accuracy: 8.6275\n",
            "Validation Loss: 4.088051226488345, Validation Accuracy: 8.07\n",
            "[46/150]: Training Loss: 4.049987061309815, Training Accuracy: 8.63\n",
            "Validation Loss: 4.085329578180981, Validation Accuracy: 8.03\n",
            "[47/150]: Training Loss: 4.047172613143921, Training Accuracy: 8.7125\n",
            "Validation Loss: 4.082841596785625, Validation Accuracy: 7.78\n",
            "[48/150]: Training Loss: 4.0441587448120115, Training Accuracy: 8.745\n",
            "Validation Loss: 4.081506820241357, Validation Accuracy: 8.21\n",
            "[49/150]: Training Loss: 4.04135802230835, Training Accuracy: 8.82\n",
            "Validation Loss: 4.079341964357218, Validation Accuracy: 8.03\n",
            "[50/150]: Training Loss: 4.0387205871582035, Training Accuracy: 8.7775\n",
            "Validation Loss: 4.075337004509701, Validation Accuracy: 8.12\n",
            "[51/150]: Training Loss: 4.036031353759766, Training Accuracy: 8.9025\n",
            "Validation Loss: 4.074757421092623, Validation Accuracy: 8.17\n",
            "[52/150]: Training Loss: 4.033370276260376, Training Accuracy: 8.91\n",
            "Validation Loss: 4.071030196110914, Validation Accuracy: 8.29\n",
            "[53/150]: Training Loss: 4.031033205032348, Training Accuracy: 8.965\n",
            "Validation Loss: 4.069437462812776, Validation Accuracy: 8.36\n",
            "[54/150]: Training Loss: 4.028417000579834, Training Accuracy: 9.095\n",
            "Validation Loss: 4.067116659917649, Validation Accuracy: 8.11\n",
            "[55/150]: Training Loss: 4.0260539081573485, Training Accuracy: 9.045\n",
            "Validation Loss: 4.065322780305413, Validation Accuracy: 8.45\n",
            "[56/150]: Training Loss: 4.0235095344543454, Training Accuracy: 9.085\n",
            "Validation Loss: 4.064038521165301, Validation Accuracy: 8.38\n",
            "[57/150]: Training Loss: 4.02125683631897, Training Accuracy: 9.18\n",
            "Validation Loss: 4.0599597381178745, Validation Accuracy: 8.62\n",
            "[58/150]: Training Loss: 4.019049766159058, Training Accuracy: 9.2225\n",
            "Validation Loss: 4.058757024206174, Validation Accuracy: 8.53\n",
            "[59/150]: Training Loss: 4.016744113540649, Training Accuracy: 9.3325\n",
            "Validation Loss: 4.056453751910264, Validation Accuracy: 8.65\n",
            "[60/150]: Training Loss: 4.014383445358276, Training Accuracy: 9.33\n",
            "Validation Loss: 4.054738797959248, Validation Accuracy: 8.49\n",
            "[61/150]: Training Loss: 4.012673494720459, Training Accuracy: 9.4\n",
            "Validation Loss: 4.05312654015365, Validation Accuracy: 8.45\n",
            "[62/150]: Training Loss: 4.0104282833099365, Training Accuracy: 9.3375\n",
            "Validation Loss: 4.050798089640915, Validation Accuracy: 8.61\n",
            "[63/150]: Training Loss: 4.0085177280426025, Training Accuracy: 9.345\n",
            "Validation Loss: 4.047672698452215, Validation Accuracy: 9.08\n",
            "[64/150]: Training Loss: 4.007076532363891, Training Accuracy: 9.44\n",
            "Validation Loss: 4.047075613289122, Validation Accuracy: 8.55\n",
            "[65/150]: Training Loss: 4.004718017959595, Training Accuracy: 9.47\n",
            "Validation Loss: 4.044435259642874, Validation Accuracy: 8.66\n",
            "[66/150]: Training Loss: 4.002854734802246, Training Accuracy: 9.58\n",
            "Validation Loss: 4.043384468479521, Validation Accuracy: 8.68\n",
            "[67/150]: Training Loss: 4.000918030929565, Training Accuracy: 9.59\n",
            "Validation Loss: 4.042230905241268, Validation Accuracy: 8.87\n",
            "[68/150]: Training Loss: 3.9992192554473878, Training Accuracy: 9.5775\n",
            "Validation Loss: 4.040329585409468, Validation Accuracy: 8.83\n",
            "[69/150]: Training Loss: 3.9973626461029053, Training Accuracy: 9.615\n",
            "Validation Loss: 4.037929029221747, Validation Accuracy: 9.01\n",
            "[70/150]: Training Loss: 3.9957307056427003, Training Accuracy: 9.69\n",
            "Validation Loss: 4.037050957892351, Validation Accuracy: 8.67\n",
            "[71/150]: Training Loss: 3.9943285522460936, Training Accuracy: 9.7375\n",
            "Validation Loss: 4.0364253475407885, Validation Accuracy: 8.88\n",
            "[72/150]: Training Loss: 3.9925530368804933, Training Accuracy: 9.7025\n",
            "Validation Loss: 4.033815119676529, Validation Accuracy: 8.93\n",
            "[73/150]: Training Loss: 3.9908373458862303, Training Accuracy: 9.7625\n",
            "Validation Loss: 4.0321422671056855, Validation Accuracy: 8.97\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 4.837270994854581, Test Accuracy: 5.7\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▇▂▂▂▂▂▁▁▁▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▁▁▇██▇▇█▇▇▆▇▇▇▇▆▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▃▃▄▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>████▇▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>5.7</td></tr><tr><td>Test Loss</td><td>4.83727</td></tr><tr><td>Train Accuracy</td><td>9.7625</td></tr><tr><td>Train Loss</td><td>3.99084</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_022555-nh2g7isx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.05 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_023355-cthymqs8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8' target=\"_blank\">learning_rate=0.05 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.600790646362305, Training Accuracy: 1.1675\n",
            "Validation Loss: 4.590667490746565, Validation Accuracy: 1.26\n",
            "[2/150]: Training Loss: 4.559383809661865, Training Accuracy: 2.1575\n",
            "Validation Loss: 4.506907958133965, Validation Accuracy: 2.54\n",
            "[3/150]: Training Loss: 4.4036731437683105, Training Accuracy: 3.8525\n",
            "Validation Loss: 4.321598778864381, Validation Accuracy: 4.84\n",
            "[4/150]: Training Loss: 4.246265041351318, Training Accuracy: 5.2175\n",
            "Validation Loss: 4.227905522486207, Validation Accuracy: 6.02\n",
            "[5/150]: Training Loss: 4.176178216934204, Training Accuracy: 6.1675\n",
            "Validation Loss: 4.179626170237353, Validation Accuracy: 6.38\n",
            "[6/150]: Training Loss: 4.135982835769654, Training Accuracy: 6.6675\n",
            "Validation Loss: 4.148335417364813, Validation Accuracy: 6.64\n",
            "[7/150]: Training Loss: 4.108450821304321, Training Accuracy: 7.2275\n",
            "Validation Loss: 4.130975275282648, Validation Accuracy: 6.89\n",
            "[8/150]: Training Loss: 4.0844139430999755, Training Accuracy: 7.6\n",
            "Validation Loss: 4.101699337078507, Validation Accuracy: 7.5\n",
            "[9/150]: Training Loss: 4.065156806564331, Training Accuracy: 7.93\n",
            "Validation Loss: 4.086441223788413, Validation Accuracy: 7.13\n",
            "[10/150]: Training Loss: 4.045691847991943, Training Accuracy: 8.4825\n",
            "Validation Loss: 4.07577043733779, Validation Accuracy: 8.03\n",
            "[11/150]: Training Loss: 4.029001393127442, Training Accuracy: 8.6075\n",
            "Validation Loss: 4.055504823186595, Validation Accuracy: 8.33\n",
            "[12/150]: Training Loss: 4.011618738937378, Training Accuracy: 9.0575\n",
            "Validation Loss: 4.039901239856793, Validation Accuracy: 8.48\n",
            "[13/150]: Training Loss: 3.9964084663391115, Training Accuracy: 9.2925\n",
            "Validation Loss: 4.02936831097694, Validation Accuracy: 8.91\n",
            "[14/150]: Training Loss: 3.9814702980041505, Training Accuracy: 9.4925\n",
            "Validation Loss: 4.011358877655807, Validation Accuracy: 9.08\n",
            "[15/150]: Training Loss: 3.967844100570679, Training Accuracy: 9.735\n",
            "Validation Loss: 4.00843292133064, Validation Accuracy: 9.01\n",
            "[16/150]: Training Loss: 3.9526515048980713, Training Accuracy: 10.11\n",
            "Validation Loss: 3.983490101091421, Validation Accuracy: 9.73\n",
            "[17/150]: Training Loss: 3.9376510147094725, Training Accuracy: 10.2675\n",
            "Validation Loss: 3.9689508137429597, Validation Accuracy: 9.61\n",
            "[18/150]: Training Loss: 3.9245626121520996, Training Accuracy: 10.52\n",
            "Validation Loss: 3.9620496361118973, Validation Accuracy: 9.98\n",
            "[19/150]: Training Loss: 3.910297552108765, Training Accuracy: 10.8175\n",
            "Validation Loss: 3.952874835129756, Validation Accuracy: 10.09\n",
            "[20/150]: Training Loss: 3.8983414665222167, Training Accuracy: 11.1225\n",
            "Validation Loss: 3.930051308528633, Validation Accuracy: 10.44\n",
            "[21/150]: Training Loss: 3.8838251056671145, Training Accuracy: 11.2975\n",
            "Validation Loss: 3.9188870700301637, Validation Accuracy: 10.53\n",
            "[22/150]: Training Loss: 3.870637998199463, Training Accuracy: 11.44\n",
            "Validation Loss: 3.903455512538837, Validation Accuracy: 10.81\n",
            "[23/150]: Training Loss: 3.857129457092285, Training Accuracy: 11.84\n",
            "Validation Loss: 3.8918681099156665, Validation Accuracy: 11.32\n",
            "[24/150]: Training Loss: 3.8432881324768067, Training Accuracy: 12.0075\n",
            "Validation Loss: 3.8916967568124177, Validation Accuracy: 11.36\n",
            "[25/150]: Training Loss: 3.829104020690918, Training Accuracy: 12.395\n",
            "Validation Loss: 3.874841636912838, Validation Accuracy: 11.71\n",
            "[26/150]: Training Loss: 3.814932382965088, Training Accuracy: 12.6375\n",
            "Validation Loss: 3.862990617752075, Validation Accuracy: 11.7\n",
            "[27/150]: Training Loss: 3.801380111312866, Training Accuracy: 12.8\n",
            "Validation Loss: 3.850984105638638, Validation Accuracy: 12.17\n",
            "[28/150]: Training Loss: 3.7865677406311034, Training Accuracy: 13.18\n",
            "Validation Loss: 3.830880017796899, Validation Accuracy: 12.18\n",
            "[29/150]: Training Loss: 3.7712997707366944, Training Accuracy: 13.4425\n",
            "Validation Loss: 3.816112823547072, Validation Accuracy: 12.61\n",
            "[30/150]: Training Loss: 3.7579914638519285, Training Accuracy: 13.83\n",
            "Validation Loss: 3.817626942494872, Validation Accuracy: 12.9\n",
            "[31/150]: Training Loss: 3.7429426765441893, Training Accuracy: 14.14\n",
            "Validation Loss: 3.795013807381794, Validation Accuracy: 12.82\n",
            "[32/150]: Training Loss: 3.7270426654815676, Training Accuracy: 14.46\n",
            "Validation Loss: 3.774455428882769, Validation Accuracy: 13.53\n",
            "[33/150]: Training Loss: 3.7124608081817625, Training Accuracy: 14.53\n",
            "Validation Loss: 3.7734299799439253, Validation Accuracy: 13.65\n",
            "[34/150]: Training Loss: 3.695247130584717, Training Accuracy: 15.0575\n",
            "Validation Loss: 3.749705171888801, Validation Accuracy: 14.01\n",
            "[35/150]: Training Loss: 3.679954329299927, Training Accuracy: 15.12\n",
            "Validation Loss: 3.7417220279669308, Validation Accuracy: 13.98\n",
            "[36/150]: Training Loss: 3.6635797924041746, Training Accuracy: 15.48\n",
            "Validation Loss: 3.7247597214522634, Validation Accuracy: 14.42\n",
            "[37/150]: Training Loss: 3.6480526805877687, Training Accuracy: 15.7375\n",
            "Validation Loss: 3.7059438805671254, Validation Accuracy: 14.5\n",
            "[38/150]: Training Loss: 3.6313486305236817, Training Accuracy: 16.3075\n",
            "Validation Loss: 3.6903846977622647, Validation Accuracy: 14.92\n",
            "[39/150]: Training Loss: 3.6153011852264405, Training Accuracy: 16.3475\n",
            "Validation Loss: 3.6778608826315327, Validation Accuracy: 14.79\n",
            "[40/150]: Training Loss: 3.60027672958374, Training Accuracy: 16.67\n",
            "Validation Loss: 3.6614270756958396, Validation Accuracy: 15.31\n",
            "[41/150]: Training Loss: 3.585800159072876, Training Accuracy: 16.7725\n",
            "Validation Loss: 3.655038889805982, Validation Accuracy: 15.05\n",
            "[42/150]: Training Loss: 3.5694461936950685, Training Accuracy: 17.0375\n",
            "Validation Loss: 3.636945672855256, Validation Accuracy: 15.93\n",
            "[43/150]: Training Loss: 3.5568551288604735, Training Accuracy: 17.3975\n",
            "Validation Loss: 3.627324414101376, Validation Accuracy: 16.21\n",
            "[44/150]: Training Loss: 3.5419315227508545, Training Accuracy: 17.7\n",
            "Validation Loss: 3.6199980754001886, Validation Accuracy: 16.01\n",
            "[45/150]: Training Loss: 3.529905016708374, Training Accuracy: 17.7675\n",
            "Validation Loss: 3.6078376861134913, Validation Accuracy: 16.36\n",
            "[46/150]: Training Loss: 3.5179548221588135, Training Accuracy: 18.195\n",
            "Validation Loss: 3.591066009679418, Validation Accuracy: 16.38\n",
            "[47/150]: Training Loss: 3.5055387844085693, Training Accuracy: 18.2875\n",
            "Validation Loss: 3.5880215836178726, Validation Accuracy: 16.91\n",
            "[48/150]: Training Loss: 3.4944853992462157, Training Accuracy: 18.445\n",
            "Validation Loss: 3.573359035382605, Validation Accuracy: 17.28\n",
            "[49/150]: Training Loss: 3.4836856788635253, Training Accuracy: 18.615\n",
            "Validation Loss: 3.5583009431316595, Validation Accuracy: 17.54\n",
            "[50/150]: Training Loss: 3.4725227031707764, Training Accuracy: 18.9225\n",
            "Validation Loss: 3.5552009609854145, Validation Accuracy: 17.27\n",
            "[51/150]: Training Loss: 3.461646089553833, Training Accuracy: 19.095\n",
            "Validation Loss: 3.5449378551191586, Validation Accuracy: 17.49\n",
            "[52/150]: Training Loss: 3.451679636383057, Training Accuracy: 19.2575\n",
            "Validation Loss: 3.5295108943987805, Validation Accuracy: 17.59\n",
            "[53/150]: Training Loss: 3.44256662979126, Training Accuracy: 19.475\n",
            "Validation Loss: 3.5264732868048796, Validation Accuracy: 17.73\n",
            "[54/150]: Training Loss: 3.4335047622680666, Training Accuracy: 19.5175\n",
            "Validation Loss: 3.521607499213735, Validation Accuracy: 17.96\n",
            "[55/150]: Training Loss: 3.425080271530151, Training Accuracy: 19.8125\n",
            "Validation Loss: 3.513597962203299, Validation Accuracy: 18.01\n",
            "[56/150]: Training Loss: 3.4155618144989015, Training Accuracy: 19.8425\n",
            "Validation Loss: 3.5052308413633115, Validation Accuracy: 18.2\n",
            "[57/150]: Training Loss: 3.4078545150756834, Training Accuracy: 20.0575\n",
            "Validation Loss: 3.498734108202017, Validation Accuracy: 18.36\n",
            "[58/150]: Training Loss: 3.399192015457153, Training Accuracy: 20.1425\n",
            "Validation Loss: 3.4935538814326, Validation Accuracy: 18.5\n",
            "[59/150]: Training Loss: 3.3918968856811524, Training Accuracy: 20.275\n",
            "Validation Loss: 3.4842984858591843, Validation Accuracy: 18.83\n",
            "[60/150]: Training Loss: 3.3846318199157714, Training Accuracy: 20.45\n",
            "Validation Loss: 3.475666542721402, Validation Accuracy: 18.75\n",
            "[61/150]: Training Loss: 3.378842526626587, Training Accuracy: 20.395\n",
            "Validation Loss: 3.47740289360095, Validation Accuracy: 18.53\n",
            "[62/150]: Training Loss: 3.369043716430664, Training Accuracy: 20.85\n",
            "Validation Loss: 3.469373791081131, Validation Accuracy: 18.88\n",
            "[63/150]: Training Loss: 3.363127135467529, Training Accuracy: 20.8125\n",
            "Validation Loss: 3.453160047531128, Validation Accuracy: 19.11\n",
            "[64/150]: Training Loss: 3.355681411361694, Training Accuracy: 21.0925\n",
            "Validation Loss: 3.466009464992839, Validation Accuracy: 19.12\n",
            "[65/150]: Training Loss: 3.3498121349334715, Training Accuracy: 21.1625\n",
            "Validation Loss: 3.4505164547331013, Validation Accuracy: 19.11\n",
            "[66/150]: Training Loss: 3.343057912063599, Training Accuracy: 21.225\n",
            "Validation Loss: 3.4455762957311737, Validation Accuracy: 19.05\n",
            "[67/150]: Training Loss: 3.3380538593292237, Training Accuracy: 21.2275\n",
            "Validation Loss: 3.448872314137258, Validation Accuracy: 18.99\n",
            "[68/150]: Training Loss: 3.3309446399688722, Training Accuracy: 21.4425\n",
            "Validation Loss: 3.434380317189891, Validation Accuracy: 19.65\n",
            "[69/150]: Training Loss: 3.325505153656006, Training Accuracy: 21.5125\n",
            "Validation Loss: 3.428423096419899, Validation Accuracy: 19.77\n",
            "[70/150]: Training Loss: 3.319794240951538, Training Accuracy: 21.7375\n",
            "Validation Loss: 3.4293241318623733, Validation Accuracy: 19.58\n",
            "[71/150]: Training Loss: 3.3128502590179445, Training Accuracy: 21.7775\n",
            "Validation Loss: 3.422350405128139, Validation Accuracy: 19.89\n",
            "[72/150]: Training Loss: 3.308202914047241, Training Accuracy: 21.9175\n",
            "Validation Loss: 3.4211007197191763, Validation Accuracy: 20.03\n",
            "[73/150]: Training Loss: 3.3036910511016844, Training Accuracy: 21.85\n",
            "Validation Loss: 3.412458062931231, Validation Accuracy: 19.99\n",
            "[74/150]: Training Loss: 3.2999632190704347, Training Accuracy: 22.05\n",
            "Validation Loss: 3.410503987294094, Validation Accuracy: 19.96\n",
            "[75/150]: Training Loss: 3.2934399642944334, Training Accuracy: 21.9825\n",
            "Validation Loss: 3.4070342604521735, Validation Accuracy: 20.1\n",
            "[76/150]: Training Loss: 3.288022666168213, Training Accuracy: 22.4075\n",
            "Validation Loss: 3.3981059220186465, Validation Accuracy: 20.33\n",
            "[77/150]: Training Loss: 3.283489876937866, Training Accuracy: 22.37\n",
            "Validation Loss: 3.403488172846995, Validation Accuracy: 20.14\n",
            "[78/150]: Training Loss: 3.279419002151489, Training Accuracy: 22.4725\n",
            "Validation Loss: 3.39810815434547, Validation Accuracy: 20.23\n",
            "[79/150]: Training Loss: 3.275748169708252, Training Accuracy: 22.3675\n",
            "Validation Loss: 3.3924497191313727, Validation Accuracy: 20.38\n",
            "[80/150]: Training Loss: 3.2700943855285645, Training Accuracy: 22.625\n",
            "Validation Loss: 3.394149239655513, Validation Accuracy: 20.3\n",
            "[81/150]: Training Loss: 3.266976011657715, Training Accuracy: 22.6325\n",
            "Validation Loss: 3.389360663237845, Validation Accuracy: 20.25\n",
            "[82/150]: Training Loss: 3.2631465213775637, Training Accuracy: 22.7725\n",
            "Validation Loss: 3.3781265529098023, Validation Accuracy: 20.61\n",
            "[83/150]: Training Loss: 3.2582495727539063, Training Accuracy: 22.885\n",
            "Validation Loss: 3.380337715148926, Validation Accuracy: 20.88\n",
            "[84/150]: Training Loss: 3.2537473976135254, Training Accuracy: 23.075\n",
            "Validation Loss: 3.3771434042863784, Validation Accuracy: 20.89\n",
            "[85/150]: Training Loss: 3.2507594604492187, Training Accuracy: 22.9875\n",
            "Validation Loss: 3.3780695936482426, Validation Accuracy: 20.92\n",
            "[86/150]: Training Loss: 3.246923331832886, Training Accuracy: 23.055\n",
            "Validation Loss: 3.3708357325025426, Validation Accuracy: 20.88\n",
            "[87/150]: Training Loss: 3.2429458431243896, Training Accuracy: 23.1625\n",
            "Validation Loss: 3.36568513493629, Validation Accuracy: 21.0\n",
            "[88/150]: Training Loss: 3.239496036148071, Training Accuracy: 23.09\n",
            "Validation Loss: 3.369001481183775, Validation Accuracy: 20.84\n",
            "[89/150]: Training Loss: 3.235803797531128, Training Accuracy: 23.2875\n",
            "Validation Loss: 3.362776779065466, Validation Accuracy: 21.19\n",
            "[90/150]: Training Loss: 3.2327476997375486, Training Accuracy: 23.39\n",
            "Validation Loss: 3.3616829556264696, Validation Accuracy: 21.16\n",
            "[91/150]: Training Loss: 3.229936269760132, Training Accuracy: 23.3875\n",
            "Validation Loss: 3.360888686149743, Validation Accuracy: 21.41\n",
            "[92/150]: Training Loss: 3.2263083686828615, Training Accuracy: 23.4225\n",
            "Validation Loss: 3.359251875786265, Validation Accuracy: 21.08\n",
            "[93/150]: Training Loss: 3.224231428909302, Training Accuracy: 23.5575\n",
            "Validation Loss: 3.3559617753241473, Validation Accuracy: 21.38\n",
            "[94/150]: Training Loss: 3.2209563293457033, Training Accuracy: 23.52\n",
            "Validation Loss: 3.3566071850479027, Validation Accuracy: 21.38\n",
            "[95/150]: Training Loss: 3.2181186485290527, Training Accuracy: 23.5925\n",
            "Validation Loss: 3.3525171613996956, Validation Accuracy: 21.11\n",
            "[96/150]: Training Loss: 3.2156093044281007, Training Accuracy: 23.6025\n",
            "Validation Loss: 3.3487717983828986, Validation Accuracy: 21.48\n",
            "[97/150]: Training Loss: 3.211993044281006, Training Accuracy: 23.67\n",
            "Validation Loss: 3.3509595333390934, Validation Accuracy: 21.34\n",
            "[98/150]: Training Loss: 3.2086189796447755, Training Accuracy: 23.7725\n",
            "Validation Loss: 3.35127718585312, Validation Accuracy: 21.49\n",
            "[99/150]: Training Loss: 3.2077421699523927, Training Accuracy: 23.7925\n",
            "Validation Loss: 3.345605060553095, Validation Accuracy: 21.5\n",
            "[100/150]: Training Loss: 3.2047137928009035, Training Accuracy: 23.7025\n",
            "Validation Loss: 3.3447778088271996, Validation Accuracy: 21.56\n",
            "[101/150]: Training Loss: 3.202946053314209, Training Accuracy: 23.8775\n",
            "Validation Loss: 3.342098644584607, Validation Accuracy: 21.49\n",
            "[102/150]: Training Loss: 3.2004322288513185, Training Accuracy: 23.8375\n",
            "Validation Loss: 3.343631648713616, Validation Accuracy: 21.61\n",
            "[103/150]: Training Loss: 3.1983728965759277, Training Accuracy: 23.9425\n",
            "Validation Loss: 3.3422155471364405, Validation Accuracy: 21.24\n",
            "[104/150]: Training Loss: 3.196205286026001, Training Accuracy: 23.9975\n",
            "Validation Loss: 3.3361381269564294, Validation Accuracy: 21.8\n",
            "[105/150]: Training Loss: 3.194296379852295, Training Accuracy: 23.9125\n",
            "Validation Loss: 3.3376316750884816, Validation Accuracy: 21.59\n",
            "[106/150]: Training Loss: 3.1917529289245605, Training Accuracy: 23.9425\n",
            "Validation Loss: 3.3328404289901634, Validation Accuracy: 21.83\n",
            "[107/150]: Training Loss: 3.1901755290985108, Training Accuracy: 24.06\n",
            "Validation Loss: 3.335774242498313, Validation Accuracy: 21.85\n",
            "[108/150]: Training Loss: 3.188256001663208, Training Accuracy: 24.01\n",
            "Validation Loss: 3.335664199416045, Validation Accuracy: 21.84\n",
            "[109/150]: Training Loss: 3.186541044998169, Training Accuracy: 24.1\n",
            "Validation Loss: 3.331627411447513, Validation Accuracy: 21.98\n",
            "[110/150]: Training Loss: 3.184773151397705, Training Accuracy: 24.0825\n",
            "Validation Loss: 3.3308697946512016, Validation Accuracy: 21.79\n",
            "[111/150]: Training Loss: 3.1835228912353517, Training Accuracy: 24.1275\n",
            "Validation Loss: 3.3312593797209917, Validation Accuracy: 21.68\n",
            "[112/150]: Training Loss: 3.181557007980347, Training Accuracy: 24.2275\n",
            "Validation Loss: 3.3289258191539983, Validation Accuracy: 21.87\n",
            "[113/150]: Training Loss: 3.180008267211914, Training Accuracy: 24.3675\n",
            "Validation Loss: 3.3277820234845397, Validation Accuracy: 21.73\n",
            "[114/150]: Training Loss: 3.1787550163269045, Training Accuracy: 24.2975\n",
            "Validation Loss: 3.326631183077575, Validation Accuracy: 21.95\n",
            "[115/150]: Training Loss: 3.177647034072876, Training Accuracy: 24.285\n",
            "Validation Loss: 3.3255794716488785, Validation Accuracy: 22.03\n",
            "[116/150]: Training Loss: 3.176307448196411, Training Accuracy: 24.385\n",
            "Validation Loss: 3.3254039242009448, Validation Accuracy: 21.9\n",
            "[117/150]: Training Loss: 3.175141114425659, Training Accuracy: 24.3075\n",
            "Validation Loss: 3.3223649164673628, Validation Accuracy: 22.03\n",
            "[118/150]: Training Loss: 3.173803305053711, Training Accuracy: 24.4125\n",
            "Validation Loss: 3.323437298938727, Validation Accuracy: 21.99\n",
            "[119/150]: Training Loss: 3.1727485507965087, Training Accuracy: 24.345\n",
            "Validation Loss: 3.3220133781433105, Validation Accuracy: 22.09\n",
            "[120/150]: Training Loss: 3.171308903121948, Training Accuracy: 24.475\n",
            "Validation Loss: 3.320924275999616, Validation Accuracy: 22.15\n",
            "[121/150]: Training Loss: 3.1707864654541016, Training Accuracy: 24.485\n",
            "Validation Loss: 3.3217681638754097, Validation Accuracy: 21.98\n",
            "[122/150]: Training Loss: 3.169487755203247, Training Accuracy: 24.5\n",
            "Validation Loss: 3.3205978354071357, Validation Accuracy: 22.2\n",
            "[123/150]: Training Loss: 3.1687754665374754, Training Accuracy: 24.4775\n",
            "Validation Loss: 3.3199286217902118, Validation Accuracy: 22.03\n",
            "[124/150]: Training Loss: 3.1677217502593993, Training Accuracy: 24.5225\n",
            "Validation Loss: 3.3194276329818044, Validation Accuracy: 22.2\n",
            "[125/150]: Training Loss: 3.1672953506469725, Training Accuracy: 24.53\n",
            "Validation Loss: 3.3200536381666828, Validation Accuracy: 22.05\n",
            "[126/150]: Training Loss: 3.166370540237427, Training Accuracy: 24.53\n",
            "Validation Loss: 3.3180840486174175, Validation Accuracy: 22.01\n",
            "[127/150]: Training Loss: 3.165336986541748, Training Accuracy: 24.565\n",
            "Validation Loss: 3.318746434655159, Validation Accuracy: 22.24\n",
            "[128/150]: Training Loss: 3.164968849182129, Training Accuracy: 24.535\n",
            "Validation Loss: 3.3186944214401732, Validation Accuracy: 22.05\n",
            "[129/150]: Training Loss: 3.1642740074157714, Training Accuracy: 24.5175\n",
            "Validation Loss: 3.3177084133123897, Validation Accuracy: 22.21\n",
            "[130/150]: Training Loss: 3.16352066116333, Training Accuracy: 24.655\n",
            "Validation Loss: 3.318562794642843, Validation Accuracy: 22.01\n",
            "[131/150]: Training Loss: 3.1630500473022463, Training Accuracy: 24.5975\n",
            "Validation Loss: 3.317300656798539, Validation Accuracy: 22.06\n",
            "[132/150]: Training Loss: 3.1626014945983885, Training Accuracy: 24.66\n",
            "Validation Loss: 3.317058933768303, Validation Accuracy: 22.16\n",
            "[133/150]: Training Loss: 3.161920825958252, Training Accuracy: 24.64\n",
            "Validation Loss: 3.3171579837799072, Validation Accuracy: 22.02\n",
            "[134/150]: Training Loss: 3.161545040512085, Training Accuracy: 24.6375\n",
            "Validation Loss: 3.316502076045723, Validation Accuracy: 22.18\n",
            "[135/150]: Training Loss: 3.1611102500915527, Training Accuracy: 24.585\n",
            "Validation Loss: 3.3166633107859615, Validation Accuracy: 22.06\n",
            "[136/150]: Training Loss: 3.160765283203125, Training Accuracy: 24.6325\n",
            "Validation Loss: 3.3164391669498126, Validation Accuracy: 22.14\n",
            "[137/150]: Training Loss: 3.160325936508179, Training Accuracy: 24.6675\n",
            "Validation Loss: 3.315987286294342, Validation Accuracy: 22.11\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 5.462109195198982, Test Accuracy: 10.88\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▆█▂▁▂▂▁▁▂▂▃▃▂▂▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▃▁▄▆▆▇██▇▇▆▇▇▇▇▇▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Train Loss</td><td>█▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>10.88</td></tr><tr><td>Test Loss</td><td>5.46211</td></tr><tr><td>Train Accuracy</td><td>24.6675</td></tr><tr><td>Train Loss</td><td>3.16033</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.05 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_023355-cthymqs8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.1 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_024913-26wygp07</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07' target=\"_blank\">learning_rate=0.1 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.554393922424317, Training Accuracy: 2.2\n",
            "Validation Loss: 4.421840394378468, Validation Accuracy: 3.74\n",
            "[2/150]: Training Loss: 4.284681567382813, Training Accuracy: 4.52\n",
            "Validation Loss: 4.239770348664302, Validation Accuracy: 4.95\n",
            "[3/150]: Training Loss: 4.181965134429932, Training Accuracy: 5.585\n",
            "Validation Loss: 4.184003517126581, Validation Accuracy: 5.53\n",
            "[4/150]: Training Loss: 4.125413941192627, Training Accuracy: 6.6775\n",
            "Validation Loss: 4.123335443484556, Validation Accuracy: 6.52\n",
            "[5/150]: Training Loss: 4.084817845535278, Training Accuracy: 7.4525\n",
            "Validation Loss: 4.096477279237881, Validation Accuracy: 6.87\n",
            "[6/150]: Training Loss: 4.048351853179931, Training Accuracy: 8.1125\n",
            "Validation Loss: 4.069233142646255, Validation Accuracy: 7.8\n",
            "[7/150]: Training Loss: 4.018422792816162, Training Accuracy: 8.56\n",
            "Validation Loss: 4.0364193445558, Validation Accuracy: 8.44\n",
            "[8/150]: Training Loss: 3.989247378540039, Training Accuracy: 8.9625\n",
            "Validation Loss: 4.010960247865908, Validation Accuracy: 8.51\n",
            "[9/150]: Training Loss: 3.9593318000793456, Training Accuracy: 9.5725\n",
            "Validation Loss: 3.9864684988738626, Validation Accuracy: 8.73\n",
            "[10/150]: Training Loss: 3.935969552612305, Training Accuracy: 9.97\n",
            "Validation Loss: 3.959391053315181, Validation Accuracy: 9.43\n",
            "[11/150]: Training Loss: 3.910384812927246, Training Accuracy: 10.53\n",
            "Validation Loss: 3.9335520723063473, Validation Accuracy: 10.05\n",
            "[12/150]: Training Loss: 3.8837633472442628, Training Accuracy: 11.015\n",
            "Validation Loss: 3.9218593843423637, Validation Accuracy: 9.91\n",
            "[13/150]: Training Loss: 3.85968777885437, Training Accuracy: 11.265\n",
            "Validation Loss: 3.884523991566555, Validation Accuracy: 10.94\n",
            "[14/150]: Training Loss: 3.83401321182251, Training Accuracy: 12.19\n",
            "Validation Loss: 3.8702170302154153, Validation Accuracy: 11.29\n",
            "[15/150]: Training Loss: 3.8098957332611083, Training Accuracy: 12.6325\n",
            "Validation Loss: 3.840197701363047, Validation Accuracy: 11.62\n",
            "[16/150]: Training Loss: 3.7832756935119627, Training Accuracy: 13.2\n",
            "Validation Loss: 3.8233511766810326, Validation Accuracy: 12.45\n",
            "[17/150]: Training Loss: 3.755026846694946, Training Accuracy: 13.5\n",
            "Validation Loss: 3.7900661996975065, Validation Accuracy: 12.91\n",
            "[18/150]: Training Loss: 3.7291974296569825, Training Accuracy: 14.06\n",
            "Validation Loss: 3.7731176181963293, Validation Accuracy: 13.22\n",
            "[19/150]: Training Loss: 3.703822989273071, Training Accuracy: 14.6625\n",
            "Validation Loss: 3.742501725057128, Validation Accuracy: 13.6\n",
            "[20/150]: Training Loss: 3.676954047012329, Training Accuracy: 15.14\n",
            "Validation Loss: 3.718170521365609, Validation Accuracy: 14.24\n",
            "[21/150]: Training Loss: 3.6502103706359863, Training Accuracy: 15.59\n",
            "Validation Loss: 3.6871351967951296, Validation Accuracy: 14.81\n",
            "[22/150]: Training Loss: 3.622364068222046, Training Accuracy: 16.105\n",
            "Validation Loss: 3.67376760464565, Validation Accuracy: 14.79\n",
            "[23/150]: Training Loss: 3.5969187736511232, Training Accuracy: 16.585\n",
            "Validation Loss: 3.6408486214413007, Validation Accuracy: 15.59\n",
            "[24/150]: Training Loss: 3.568799534988403, Training Accuracy: 17.215\n",
            "Validation Loss: 3.6294803862359113, Validation Accuracy: 15.79\n",
            "[25/150]: Training Loss: 3.5421108798980714, Training Accuracy: 17.57\n",
            "Validation Loss: 3.598143538092352, Validation Accuracy: 16.37\n",
            "[26/150]: Training Loss: 3.5124191093444823, Training Accuracy: 18.205\n",
            "Validation Loss: 3.5703049495721317, Validation Accuracy: 16.98\n",
            "[27/150]: Training Loss: 3.484529112625122, Training Accuracy: 18.7275\n",
            "Validation Loss: 3.5376776236637384, Validation Accuracy: 17.48\n",
            "[28/150]: Training Loss: 3.455819899749756, Training Accuracy: 19.0425\n",
            "Validation Loss: 3.5291991643844898, Validation Accuracy: 17.68\n",
            "[29/150]: Training Loss: 3.4284223934173585, Training Accuracy: 19.5625\n",
            "Validation Loss: 3.4991380257211673, Validation Accuracy: 18.43\n",
            "[30/150]: Training Loss: 3.4028552402496337, Training Accuracy: 20.0525\n",
            "Validation Loss: 3.4806957472661497, Validation Accuracy: 18.35\n",
            "[31/150]: Training Loss: 3.376880758666992, Training Accuracy: 20.5925\n",
            "Validation Loss: 3.450026891793415, Validation Accuracy: 19.53\n",
            "[32/150]: Training Loss: 3.354429434585571, Training Accuracy: 21.165\n",
            "Validation Loss: 3.435450357995975, Validation Accuracy: 19.72\n",
            "[33/150]: Training Loss: 3.3340842437744143, Training Accuracy: 21.175\n",
            "Validation Loss: 3.418211444927629, Validation Accuracy: 19.67\n",
            "[34/150]: Training Loss: 3.314141171646118, Training Accuracy: 21.5925\n",
            "Validation Loss: 3.4085868847597935, Validation Accuracy: 20.29\n",
            "[35/150]: Training Loss: 3.2981444416046144, Training Accuracy: 21.85\n",
            "Validation Loss: 3.3930132495369882, Validation Accuracy: 20.07\n",
            "[36/150]: Training Loss: 3.280233634567261, Training Accuracy: 22.4375\n",
            "Validation Loss: 3.3805624132703063, Validation Accuracy: 20.71\n",
            "[37/150]: Training Loss: 3.2627798179626466, Training Accuracy: 22.555\n",
            "Validation Loss: 3.365730774630407, Validation Accuracy: 21.1\n",
            "[38/150]: Training Loss: 3.246723106765747, Training Accuracy: 22.8925\n",
            "Validation Loss: 3.3649645671722994, Validation Accuracy: 21.17\n",
            "[39/150]: Training Loss: 3.23333472366333, Training Accuracy: 23.1725\n",
            "Validation Loss: 3.340107489543356, Validation Accuracy: 21.48\n",
            "[40/150]: Training Loss: 3.2185105766296385, Training Accuracy: 23.425\n",
            "Validation Loss: 3.3351985269291387, Validation Accuracy: 21.9\n",
            "[41/150]: Training Loss: 3.2041201099395753, Training Accuracy: 23.65\n",
            "Validation Loss: 3.320353848159693, Validation Accuracy: 22.37\n",
            "[42/150]: Training Loss: 3.1920853351593017, Training Accuracy: 23.97\n",
            "Validation Loss: 3.3151075627393785, Validation Accuracy: 21.98\n",
            "[43/150]: Training Loss: 3.178889651107788, Training Accuracy: 24.015\n",
            "Validation Loss: 3.3039617690311114, Validation Accuracy: 22.11\n",
            "[44/150]: Training Loss: 3.164986518096924, Training Accuracy: 24.4275\n",
            "Validation Loss: 3.2982496850809473, Validation Accuracy: 22.16\n",
            "[45/150]: Training Loss: 3.1540319355010986, Training Accuracy: 24.705\n",
            "Validation Loss: 3.2881052099215755, Validation Accuracy: 22.49\n",
            "[46/150]: Training Loss: 3.1423869262695314, Training Accuracy: 24.745\n",
            "Validation Loss: 3.285053641932785, Validation Accuracy: 22.32\n",
            "[47/150]: Training Loss: 3.1313994647979735, Training Accuracy: 25.0425\n",
            "Validation Loss: 3.2773830009873506, Validation Accuracy: 22.64\n",
            "[48/150]: Training Loss: 3.1199681632995606, Training Accuracy: 25.2175\n",
            "Validation Loss: 3.2647413463349553, Validation Accuracy: 22.66\n",
            "[49/150]: Training Loss: 3.107323546600342, Training Accuracy: 25.5575\n",
            "Validation Loss: 3.277184949559011, Validation Accuracy: 22.72\n",
            "[50/150]: Training Loss: 3.0971016899108887, Training Accuracy: 25.77\n",
            "Validation Loss: 3.2499756160055755, Validation Accuracy: 22.69\n",
            "[51/150]: Training Loss: 3.0865638957977293, Training Accuracy: 25.915\n",
            "Validation Loss: 3.28796663102071, Validation Accuracy: 22.45\n",
            "[52/150]: Training Loss: 3.075482699203491, Training Accuracy: 26.0775\n",
            "Validation Loss: 3.256385016593204, Validation Accuracy: 22.97\n",
            "[53/150]: Training Loss: 3.0676188301086427, Training Accuracy: 26.045\n",
            "Validation Loss: 3.2297865843317313, Validation Accuracy: 23.53\n",
            "[54/150]: Training Loss: 3.0562880493164064, Training Accuracy: 26.5925\n",
            "Validation Loss: 3.2346752266974965, Validation Accuracy: 23.32\n",
            "[55/150]: Training Loss: 3.046115529251099, Training Accuracy: 26.525\n",
            "Validation Loss: 3.229158471344383, Validation Accuracy: 23.27\n",
            "[56/150]: Training Loss: 3.0363171295166014, Training Accuracy: 26.5875\n",
            "Validation Loss: 3.2251109287237667, Validation Accuracy: 23.85\n",
            "[57/150]: Training Loss: 3.0262594100952147, Training Accuracy: 26.96\n",
            "Validation Loss: 3.20999311793382, Validation Accuracy: 23.73\n",
            "[58/150]: Training Loss: 3.018730586242676, Training Accuracy: 26.925\n",
            "Validation Loss: 3.206504849111958, Validation Accuracy: 23.66\n",
            "[59/150]: Training Loss: 3.007856759262085, Training Accuracy: 27.315\n",
            "Validation Loss: 3.2090084021258507, Validation Accuracy: 24.02\n",
            "[60/150]: Training Loss: 3.0000602096557616, Training Accuracy: 27.39\n",
            "Validation Loss: 3.2029919001706846, Validation Accuracy: 23.96\n",
            "[61/150]: Training Loss: 2.9893322017669677, Training Accuracy: 27.6525\n",
            "Validation Loss: 3.207233451733923, Validation Accuracy: 23.61\n",
            "[62/150]: Training Loss: 2.9812552192687987, Training Accuracy: 27.855\n",
            "Validation Loss: 3.191280894978031, Validation Accuracy: 23.71\n",
            "[63/150]: Training Loss: 2.972245023727417, Training Accuracy: 27.83\n",
            "Validation Loss: 3.191427630224046, Validation Accuracy: 24.21\n",
            "[64/150]: Training Loss: 2.9653648654937745, Training Accuracy: 28.0175\n",
            "Validation Loss: 3.189020594214178, Validation Accuracy: 24.07\n",
            "[65/150]: Training Loss: 2.957701969909668, Training Accuracy: 28.245\n",
            "Validation Loss: 3.183226536793314, Validation Accuracy: 23.98\n",
            "[66/150]: Training Loss: 2.9480487686157226, Training Accuracy: 28.3725\n",
            "Validation Loss: 3.1869888670125586, Validation Accuracy: 24.08\n",
            "[67/150]: Training Loss: 2.9404784973144533, Training Accuracy: 28.5625\n",
            "Validation Loss: 3.1826100653144205, Validation Accuracy: 24.61\n",
            "[68/150]: Training Loss: 2.9308408599853517, Training Accuracy: 28.73\n",
            "Validation Loss: 3.1697433404861743, Validation Accuracy: 24.11\n",
            "[69/150]: Training Loss: 2.924504146194458, Training Accuracy: 28.735\n",
            "Validation Loss: 3.164285852650928, Validation Accuracy: 24.76\n",
            "[70/150]: Training Loss: 2.9181696743011476, Training Accuracy: 29.045\n",
            "Validation Loss: 3.1617295559804153, Validation Accuracy: 24.5\n",
            "[71/150]: Training Loss: 2.909669787979126, Training Accuracy: 28.97\n",
            "Validation Loss: 3.16842480222131, Validation Accuracy: 24.55\n",
            "[72/150]: Training Loss: 2.901284480667114, Training Accuracy: 29.165\n",
            "Validation Loss: 3.16541398710506, Validation Accuracy: 24.39\n",
            "[73/150]: Training Loss: 2.8958903297424317, Training Accuracy: 29.41\n",
            "Validation Loss: 3.156350550378204, Validation Accuracy: 24.81\n",
            "[74/150]: Training Loss: 2.8880927780151366, Training Accuracy: 29.51\n",
            "Validation Loss: 3.1484538536922186, Validation Accuracy: 24.79\n",
            "[75/150]: Training Loss: 2.8816709129333495, Training Accuracy: 29.6575\n",
            "Validation Loss: 3.1521760521421007, Validation Accuracy: 24.75\n",
            "[76/150]: Training Loss: 2.874811888885498, Training Accuracy: 29.6025\n",
            "Validation Loss: 3.1412497159022434, Validation Accuracy: 24.86\n",
            "[77/150]: Training Loss: 2.868447174453735, Training Accuracy: 29.75\n",
            "Validation Loss: 3.148860450003557, Validation Accuracy: 25.12\n",
            "[78/150]: Training Loss: 2.8620141311645506, Training Accuracy: 30.02\n",
            "Validation Loss: 3.1491585096735863, Validation Accuracy: 25.07\n",
            "[79/150]: Training Loss: 2.8542796688079832, Training Accuracy: 30.2175\n",
            "Validation Loss: 3.1315311428847585, Validation Accuracy: 25.26\n",
            "[80/150]: Training Loss: 2.8495312446594236, Training Accuracy: 30.2625\n",
            "Validation Loss: 3.1400332511610287, Validation Accuracy: 25.12\n",
            "[81/150]: Training Loss: 2.8422299156188964, Training Accuracy: 30.4475\n",
            "Validation Loss: 3.129277274866772, Validation Accuracy: 25.51\n",
            "[82/150]: Training Loss: 2.8352813999176028, Training Accuracy: 30.645\n",
            "Validation Loss: 3.1319479532302563, Validation Accuracy: 25.37\n",
            "[83/150]: Training Loss: 2.830805637359619, Training Accuracy: 30.595\n",
            "Validation Loss: 3.1387126217981813, Validation Accuracy: 25.34\n",
            "[84/150]: Training Loss: 2.824944213485718, Training Accuracy: 30.7875\n",
            "Validation Loss: 3.1277991054923673, Validation Accuracy: 25.58\n",
            "[85/150]: Training Loss: 2.8196789070129396, Training Accuracy: 31.02\n",
            "Validation Loss: 3.1215793433462737, Validation Accuracy: 25.59\n",
            "[86/150]: Training Loss: 2.814448028564453, Training Accuracy: 31.1025\n",
            "Validation Loss: 3.1227446027622103, Validation Accuracy: 25.41\n",
            "[87/150]: Training Loss: 2.8087932884216307, Training Accuracy: 31.3075\n",
            "Validation Loss: 3.123080830665151, Validation Accuracy: 25.3\n",
            "[88/150]: Training Loss: 2.8027573429107666, Training Accuracy: 31.35\n",
            "Validation Loss: 3.1130096623851995, Validation Accuracy: 25.94\n",
            "[89/150]: Training Loss: 2.7979101371765136, Training Accuracy: 31.195\n",
            "Validation Loss: 3.1245606795997376, Validation Accuracy: 25.59\n",
            "[90/150]: Training Loss: 2.79326137008667, Training Accuracy: 31.37\n",
            "Validation Loss: 3.1252101788854905, Validation Accuracy: 25.47\n",
            "[91/150]: Training Loss: 2.7884928009033203, Training Accuracy: 31.3925\n",
            "Validation Loss: 3.115745410797702, Validation Accuracy: 25.52\n",
            "[92/150]: Training Loss: 2.7844152694702147, Training Accuracy: 31.5825\n",
            "Validation Loss: 3.1132962870749696, Validation Accuracy: 25.67\n",
            "[93/150]: Training Loss: 2.7792159679412842, Training Accuracy: 31.665\n",
            "Validation Loss: 3.111494478906036, Validation Accuracy: 25.84\n",
            "[94/150]: Training Loss: 2.773549281311035, Training Accuracy: 31.6525\n",
            "Validation Loss: 3.1067730256706287, Validation Accuracy: 25.89\n",
            "[95/150]: Training Loss: 2.76964068031311, Training Accuracy: 31.895\n",
            "Validation Loss: 3.1083280220153227, Validation Accuracy: 25.78\n",
            "[96/150]: Training Loss: 2.7651722465515136, Training Accuracy: 31.8825\n",
            "Validation Loss: 3.1057740730844485, Validation Accuracy: 25.67\n",
            "[97/150]: Training Loss: 2.761124114608765, Training Accuracy: 32.1025\n",
            "Validation Loss: 3.1073889534944183, Validation Accuracy: 25.55\n",
            "[98/150]: Training Loss: 2.756427172470093, Training Accuracy: 32.1475\n",
            "Validation Loss: 3.1095221422280477, Validation Accuracy: 25.75\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 6.147166403995198, Test Accuracy: 12.53\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▄█▆▄▅▃▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▅▁▆▆▇███▇▇▆▇█▇▇▇▇▇▇▇▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>12.53</td></tr><tr><td>Test Loss</td><td>6.14717</td></tr><tr><td>Train Accuracy</td><td>32.1475</td></tr><tr><td>Train Loss</td><td>2.75643</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.1 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_024913-26wygp07/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.5 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_030015-a8qnvlbp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp' target=\"_blank\">learning_rate=0.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.377298123168945, Training Accuracy: 3.2725\n",
            "Validation Loss: 4.2101050941807445, Validation Accuracy: 5.2\n",
            "[2/150]: Training Loss: 4.07323479423523, Training Accuracy: 7.05\n",
            "Validation Loss: 3.997956647994412, Validation Accuracy: 8.69\n",
            "[3/150]: Training Loss: 3.9166299026489257, Training Accuracy: 9.98\n",
            "Validation Loss: 3.8700176090191882, Validation Accuracy: 10.85\n",
            "[4/150]: Training Loss: 3.76255333404541, Training Accuracy: 12.6475\n",
            "Validation Loss: 3.7763638131937403, Validation Accuracy: 12.03\n",
            "[5/150]: Training Loss: 3.6385873168945313, Training Accuracy: 14.8\n",
            "Validation Loss: 3.5995885247637514, Validation Accuracy: 15.3\n",
            "[6/150]: Training Loss: 3.5313883621215822, Training Accuracy: 16.6625\n",
            "Validation Loss: 3.547201700271315, Validation Accuracy: 16.33\n",
            "[7/150]: Training Loss: 3.4311797870635985, Training Accuracy: 18.0425\n",
            "Validation Loss: 3.447270431336324, Validation Accuracy: 18.49\n",
            "[8/150]: Training Loss: 3.3406018447875976, Training Accuracy: 20.2325\n",
            "Validation Loss: 3.3745645914867426, Validation Accuracy: 19.81\n",
            "[9/150]: Training Loss: 3.2601512760162352, Training Accuracy: 21.73\n",
            "Validation Loss: 3.3083346163391307, Validation Accuracy: 21.21\n",
            "[10/150]: Training Loss: 3.190315942764282, Training Accuracy: 23.0925\n",
            "Validation Loss: 3.2459553244766917, Validation Accuracy: 22.4\n",
            "[11/150]: Training Loss: 3.120881378555298, Training Accuracy: 24.2625\n",
            "Validation Loss: 3.2441095774340782, Validation Accuracy: 22.07\n",
            "[12/150]: Training Loss: 3.065099639892578, Training Accuracy: 25.1825\n",
            "Validation Loss: 3.205546066259882, Validation Accuracy: 22.44\n",
            "[13/150]: Training Loss: 3.002047216796875, Training Accuracy: 26.4075\n",
            "Validation Loss: 3.1507282090035216, Validation Accuracy: 24.42\n",
            "[14/150]: Training Loss: 2.947496301269531, Training Accuracy: 27.6075\n",
            "Validation Loss: 3.1272822519776167, Validation Accuracy: 24.2\n",
            "[15/150]: Training Loss: 2.8929669227600097, Training Accuracy: 28.1625\n",
            "Validation Loss: 3.08642372052381, Validation Accuracy: 25.55\n",
            "[16/150]: Training Loss: 2.8405557304382323, Training Accuracy: 29.4725\n",
            "Validation Loss: 3.021532262206837, Validation Accuracy: 26.7\n",
            "[17/150]: Training Loss: 2.7890852066040037, Training Accuracy: 30.505\n",
            "Validation Loss: 3.042905480998337, Validation Accuracy: 26.24\n",
            "[18/150]: Training Loss: 2.74124369392395, Training Accuracy: 31.39\n",
            "Validation Loss: 2.9991517309929914, Validation Accuracy: 26.63\n",
            "[19/150]: Training Loss: 2.692549164581299, Training Accuracy: 32.36\n",
            "Validation Loss: 2.9798509998685994, Validation Accuracy: 27.88\n",
            "[20/150]: Training Loss: 2.6449439025878907, Training Accuracy: 33.1025\n",
            "Validation Loss: 3.0654538998937912, Validation Accuracy: 26.5\n",
            "[21/150]: Training Loss: 2.6013897836685183, Training Accuracy: 34.135\n",
            "Validation Loss: 2.937944315041706, Validation Accuracy: 29.27\n",
            "[22/150]: Training Loss: 2.5583987575531006, Training Accuracy: 35.0375\n",
            "Validation Loss: 2.965156334980278, Validation Accuracy: 28.48\n",
            "[23/150]: Training Loss: 2.5067957414627076, Training Accuracy: 36.145\n",
            "Validation Loss: 2.940655703757219, Validation Accuracy: 28.99\n",
            "[24/150]: Training Loss: 2.4618710725784303, Training Accuracy: 37.005\n",
            "Validation Loss: 2.9297352444594074, Validation Accuracy: 28.88\n",
            "[25/150]: Training Loss: 2.4159774208068847, Training Accuracy: 37.9375\n",
            "Validation Loss: 2.9579435579336373, Validation Accuracy: 28.85\n",
            "[26/150]: Training Loss: 2.3772740001678465, Training Accuracy: 38.8375\n",
            "Validation Loss: 2.892203370476984, Validation Accuracy: 29.97\n",
            "[27/150]: Training Loss: 2.331508861351013, Training Accuracy: 39.695\n",
            "Validation Loss: 2.902201515853785, Validation Accuracy: 29.95\n",
            "[28/150]: Training Loss: 2.292740362548828, Training Accuracy: 40.25\n",
            "Validation Loss: 2.884886794788822, Validation Accuracy: 30.54\n",
            "[29/150]: Training Loss: 2.246493480873108, Training Accuracy: 41.2375\n",
            "Validation Loss: 2.9104482884619647, Validation Accuracy: 30.14\n",
            "[30/150]: Training Loss: 2.203635430717468, Training Accuracy: 42.4475\n",
            "Validation Loss: 2.9059300164508213, Validation Accuracy: 31.3\n",
            "[31/150]: Training Loss: 2.1624910346984865, Training Accuracy: 43.2625\n",
            "Validation Loss: 2.884638991325524, Validation Accuracy: 31.26\n",
            "[32/150]: Training Loss: 2.122718844985962, Training Accuracy: 44.17\n",
            "Validation Loss: 2.9034518907024602, Validation Accuracy: 30.71\n",
            "[33/150]: Training Loss: 2.0803083070755006, Training Accuracy: 45.055\n",
            "Validation Loss: 2.8911248453103813, Validation Accuracy: 31.08\n",
            "[34/150]: Training Loss: 2.0393711433410644, Training Accuracy: 45.9575\n",
            "Validation Loss: 2.9181400818429934, Validation Accuracy: 31.06\n",
            "[35/150]: Training Loss: 2.001698533630371, Training Accuracy: 46.8225\n",
            "Validation Loss: 2.9216579084943053, Validation Accuracy: 30.6\n",
            "[36/150]: Training Loss: 1.9612565605163574, Training Accuracy: 47.845\n",
            "Validation Loss: 2.95554546945414, Validation Accuracy: 31.47\n",
            "[37/150]: Training Loss: 1.9143695009231567, Training Accuracy: 48.7825\n",
            "Validation Loss: 2.9769522748934993, Validation Accuracy: 30.95\n",
            "[38/150]: Training Loss: 1.8757219652175903, Training Accuracy: 49.6925\n",
            "Validation Loss: 2.954901927595685, Validation Accuracy: 31.79\n",
            "[39/150]: Training Loss: 1.8348310264587402, Training Accuracy: 50.46\n",
            "Validation Loss: 2.97764066374226, Validation Accuracy: 31.41\n",
            "[40/150]: Training Loss: 1.7973475383758546, Training Accuracy: 51.6725\n",
            "Validation Loss: 3.0162993069666966, Validation Accuracy: 31.12\n",
            "[41/150]: Training Loss: 1.7525395877838135, Training Accuracy: 52.525\n",
            "Validation Loss: 3.0236745138836514, Validation Accuracy: 30.91\n",
            "[42/150]: Training Loss: 1.7114470052719115, Training Accuracy: 53.885\n",
            "Validation Loss: 3.051269774224348, Validation Accuracy: 31.17\n",
            "[43/150]: Training Loss: 1.6800853149414063, Training Accuracy: 54.0875\n",
            "Validation Loss: 3.0829398237216243, Validation Accuracy: 31.28\n",
            "[44/150]: Training Loss: 1.636157625389099, Training Accuracy: 55.3375\n",
            "Validation Loss: 3.110968621673098, Validation Accuracy: 31.14\n",
            "[45/150]: Training Loss: 1.589929871749878, Training Accuracy: 56.7225\n",
            "Validation Loss: 3.142918248085459, Validation Accuracy: 31.26\n",
            "[46/150]: Training Loss: 1.5445106566429139, Training Accuracy: 57.9225\n",
            "Validation Loss: 3.1339099574240907, Validation Accuracy: 31.8\n",
            "[47/150]: Training Loss: 1.5067133940696715, Training Accuracy: 58.8375\n",
            "Validation Loss: 3.1682807533604325, Validation Accuracy: 31.84\n",
            "[48/150]: Training Loss: 1.4696476486206054, Training Accuracy: 59.655\n",
            "Validation Loss: 3.2345377184023523, Validation Accuracy: 30.89\n",
            "[49/150]: Training Loss: 1.4380368849754332, Training Accuracy: 60.32\n",
            "Validation Loss: 3.2836993484740047, Validation Accuracy: 31.58\n",
            "[50/150]: Training Loss: 1.3966082113265992, Training Accuracy: 61.33\n",
            "Validation Loss: 3.369933787424853, Validation Accuracy: 30.61\n",
            "[51/150]: Training Loss: 1.3530357138633728, Training Accuracy: 62.43\n",
            "Validation Loss: 3.357933571384211, Validation Accuracy: 31.19\n",
            "[52/150]: Training Loss: 1.31355241689682, Training Accuracy: 63.4325\n",
            "Validation Loss: 3.41107276138986, Validation Accuracy: 30.17\n",
            "[53/150]: Training Loss: 1.2827554839134216, Training Accuracy: 64.0425\n",
            "Validation Loss: 3.376580183673057, Validation Accuracy: 30.85\n",
            "[54/150]: Training Loss: 1.2412437489509582, Training Accuracy: 65.2825\n",
            "Validation Loss: 3.48424918332677, Validation Accuracy: 30.12\n",
            "[55/150]: Training Loss: 1.1999455925941467, Training Accuracy: 66.39\n",
            "Validation Loss: 3.4844332743602195, Validation Accuracy: 30.82\n",
            "[56/150]: Training Loss: 1.1624719073295593, Training Accuracy: 67.315\n",
            "Validation Loss: 3.535801676428242, Validation Accuracy: 30.86\n",
            "[57/150]: Training Loss: 1.1305819693565369, Training Accuracy: 67.9275\n",
            "Validation Loss: 3.6139490315868597, Validation Accuracy: 30.44\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 16.0335865749675, Test Accuracy: 13.22\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▄▂▂▂▁▂▁▁▁▁▁▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>█▁▅▄▄▅▆▇▅▄▃▄▅▄▅▅▄▅▄▅▅▄▄▄▄▄▄▄▄▅▅▅▅▅▄▅▅▅▄▄</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>13.22</td></tr><tr><td>Test Loss</td><td>16.03359</td></tr><tr><td>Train Accuracy</td><td>67.9275</td></tr><tr><td>Train Loss</td><td>1.13058</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_030015-a8qnvlbp/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:1 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_030647-hgowk59s</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s' target=\"_blank\">learning_rate=1 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.390103232955933, Training Accuracy: 2.935\n",
            "Validation Loss: 4.160131826522244, Validation Accuracy: 5.54\n",
            "[2/150]: Training Loss: 4.031327721786499, Training Accuracy: 7.5025\n",
            "Validation Loss: 3.9958771960750505, Validation Accuracy: 8.6\n",
            "[3/150]: Training Loss: 3.800543330383301, Training Accuracy: 11.665\n",
            "Validation Loss: 3.7379826664165328, Validation Accuracy: 12.06\n",
            "[4/150]: Training Loss: 3.6242603660583494, Training Accuracy: 14.345\n",
            "Validation Loss: 3.559469526740396, Validation Accuracy: 15.28\n",
            "[5/150]: Training Loss: 3.48455090675354, Training Accuracy: 16.9325\n",
            "Validation Loss: 3.491581111956554, Validation Accuracy: 16.56\n",
            "[6/150]: Training Loss: 3.3448009601593016, Training Accuracy: 19.435\n",
            "Validation Loss: 3.3586603653658726, Validation Accuracy: 19.41\n",
            "[7/150]: Training Loss: 3.229361641693115, Training Accuracy: 21.4375\n",
            "Validation Loss: 3.2139996130754995, Validation Accuracy: 21.78\n",
            "[8/150]: Training Loss: 3.1254781677246095, Training Accuracy: 23.4825\n",
            "Validation Loss: 3.168266389020689, Validation Accuracy: 22.86\n",
            "[9/150]: Training Loss: 3.0173661666870117, Training Accuracy: 25.6925\n",
            "Validation Loss: 3.1283769850518293, Validation Accuracy: 24.17\n",
            "[10/150]: Training Loss: 2.931245880126953, Training Accuracy: 26.99\n",
            "Validation Loss: 3.016676799506898, Validation Accuracy: 25.79\n",
            "[11/150]: Training Loss: 2.8445968715667727, Training Accuracy: 28.6675\n",
            "Validation Loss: 2.968725839238258, Validation Accuracy: 26.94\n",
            "[12/150]: Training Loss: 2.769311902618408, Training Accuracy: 30.345\n",
            "Validation Loss: 2.935349525160091, Validation Accuracy: 27.73\n",
            "[13/150]: Training Loss: 2.698523984146118, Training Accuracy: 31.47\n",
            "Validation Loss: 2.9356894462731233, Validation Accuracy: 28.11\n",
            "[14/150]: Training Loss: 2.6179113666534426, Training Accuracy: 33.2625\n",
            "Validation Loss: 2.8293940914664297, Validation Accuracy: 30.0\n",
            "[15/150]: Training Loss: 2.5539515073776244, Training Accuracy: 34.565\n",
            "Validation Loss: 2.8150154101620815, Validation Accuracy: 30.54\n",
            "[16/150]: Training Loss: 2.482784992599487, Training Accuracy: 36.2825\n",
            "Validation Loss: 2.801185814438352, Validation Accuracy: 30.73\n",
            "[17/150]: Training Loss: 2.416481968307495, Training Accuracy: 37.2175\n",
            "Validation Loss: 2.778297721200688, Validation Accuracy: 31.36\n",
            "[18/150]: Training Loss: 2.3516669242858885, Training Accuracy: 38.785\n",
            "Validation Loss: 2.803610542017943, Validation Accuracy: 31.44\n",
            "[19/150]: Training Loss: 2.2785828254699707, Training Accuracy: 40.36\n",
            "Validation Loss: 2.770737362515395, Validation Accuracy: 31.54\n",
            "[20/150]: Training Loss: 2.2117202407836913, Training Accuracy: 42.02\n",
            "Validation Loss: 2.751694551698721, Validation Accuracy: 32.2\n",
            "[21/150]: Training Loss: 2.1421232624053954, Training Accuracy: 43.1125\n",
            "Validation Loss: 2.7352929327897963, Validation Accuracy: 32.64\n",
            "[22/150]: Training Loss: 2.0760102186203, Training Accuracy: 44.805\n",
            "Validation Loss: 2.7648525397489023, Validation Accuracy: 32.85\n",
            "[23/150]: Training Loss: 2.0119320999145507, Training Accuracy: 46.1375\n",
            "Validation Loss: 2.7834541448362313, Validation Accuracy: 33.19\n",
            "[24/150]: Training Loss: 1.946039645767212, Training Accuracy: 47.555\n",
            "Validation Loss: 2.805889447023914, Validation Accuracy: 33.52\n",
            "[25/150]: Training Loss: 1.8788161462783814, Training Accuracy: 49.055\n",
            "Validation Loss: 2.7862223508251702, Validation Accuracy: 34.25\n",
            "[26/150]: Training Loss: 1.799899059486389, Training Accuracy: 50.915\n",
            "Validation Loss: 2.804932374863108, Validation Accuracy: 33.77\n",
            "[27/150]: Training Loss: 1.7454933450698853, Training Accuracy: 52.16\n",
            "Validation Loss: 2.861256376193587, Validation Accuracy: 33.83\n",
            "[28/150]: Training Loss: 1.6650921211242675, Training Accuracy: 53.9225\n",
            "Validation Loss: 2.877333830116661, Validation Accuracy: 34.25\n",
            "[29/150]: Training Loss: 1.6064131809234619, Training Accuracy: 55.555\n",
            "Validation Loss: 2.973996728089205, Validation Accuracy: 33.39\n",
            "[30/150]: Training Loss: 1.543812055683136, Training Accuracy: 56.7775\n",
            "Validation Loss: 2.943861024394916, Validation Accuracy: 34.15\n",
            "[31/150]: Training Loss: 1.4620859157562256, Training Accuracy: 58.88\n",
            "Validation Loss: 2.9692332015675342, Validation Accuracy: 33.88\n",
            "[32/150]: Training Loss: 1.4009520672798157, Training Accuracy: 60.3925\n",
            "Validation Loss: 3.024310210707841, Validation Accuracy: 33.42\n",
            "[33/150]: Training Loss: 1.3313846939086913, Training Accuracy: 62.155\n",
            "Validation Loss: 3.1067024993289047, Validation Accuracy: 33.18\n",
            "[34/150]: Training Loss: 1.2518226915359496, Training Accuracy: 64.225\n",
            "Validation Loss: 3.307321997964458, Validation Accuracy: 33.31\n",
            "[35/150]: Training Loss: 1.1938729809761048, Training Accuracy: 65.5075\n",
            "Validation Loss: 3.3076086089869214, Validation Accuracy: 32.76\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 18.36642926210051, Test Accuracy: 13.77\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▅▁▂▁▁▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▁▂▅▆▇▆▇█▇▇▇▇▇██▇███████▇▇███████████████</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>13.77</td></tr><tr><td>Test Loss</td><td>18.36643</td></tr><tr><td>Train Accuracy</td><td>65.5075</td></tr><tr><td>Train Loss</td><td>1.19387</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=1 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_030647-hgowk59s/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:1.5 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_031055-hfm8dgbz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz' target=\"_blank\">learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.426865048217773, Training Accuracy: 2.45\n",
            "Validation Loss: 4.219578746018136, Validation Accuracy: 4.19\n",
            "[2/150]: Training Loss: 4.060279036712647, Training Accuracy: 6.7225\n",
            "Validation Loss: 3.8940811293899635, Validation Accuracy: 9.43\n",
            "[3/150]: Training Loss: 3.7707011901855467, Training Accuracy: 11.6925\n",
            "Validation Loss: 3.8030508642743346, Validation Accuracy: 11.0\n",
            "[4/150]: Training Loss: 3.58028080368042, Training Accuracy: 15.05\n",
            "Validation Loss: 3.516739834645751, Validation Accuracy: 16.1\n",
            "[5/150]: Training Loss: 3.4152388591766356, Training Accuracy: 17.82\n",
            "Validation Loss: 3.411277610025588, Validation Accuracy: 17.62\n",
            "[6/150]: Training Loss: 3.2448327434539794, Training Accuracy: 20.7725\n",
            "Validation Loss: 3.2892891753251385, Validation Accuracy: 19.86\n",
            "[7/150]: Training Loss: 3.1129616333007815, Training Accuracy: 23.41\n",
            "Validation Loss: 3.155244686041668, Validation Accuracy: 22.75\n",
            "[8/150]: Training Loss: 2.9936462223052978, Training Accuracy: 25.73\n",
            "Validation Loss: 3.020396041262681, Validation Accuracy: 25.53\n",
            "[9/150]: Training Loss: 2.8851455352783204, Training Accuracy: 28.0475\n",
            "Validation Loss: 2.9794276216227535, Validation Accuracy: 26.31\n",
            "[10/150]: Training Loss: 2.7872766895294188, Training Accuracy: 29.955\n",
            "Validation Loss: 2.873067584007409, Validation Accuracy: 28.57\n",
            "[11/150]: Training Loss: 2.6929493949890135, Training Accuracy: 31.84\n",
            "Validation Loss: 2.8678578267431565, Validation Accuracy: 28.72\n",
            "[12/150]: Training Loss: 2.60749265499115, Training Accuracy: 33.46\n",
            "Validation Loss: 2.831394789325204, Validation Accuracy: 29.21\n",
            "[13/150]: Training Loss: 2.5269983169555665, Training Accuracy: 35.2325\n",
            "Validation Loss: 2.8061546094857963, Validation Accuracy: 29.73\n",
            "[14/150]: Training Loss: 2.430880810165405, Training Accuracy: 37.0475\n",
            "Validation Loss: 2.777110814288923, Validation Accuracy: 31.06\n",
            "[15/150]: Training Loss: 2.357077407836914, Training Accuracy: 38.8175\n",
            "Validation Loss: 2.7613005273661035, Validation Accuracy: 32.32\n",
            "[16/150]: Training Loss: 2.2730004482269286, Training Accuracy: 40.5\n",
            "Validation Loss: 2.7338021667140304, Validation Accuracy: 32.77\n",
            "[17/150]: Training Loss: 2.183533114242554, Training Accuracy: 42.27\n",
            "Validation Loss: 2.764839885341134, Validation Accuracy: 32.81\n",
            "[18/150]: Training Loss: 2.1031003602981566, Training Accuracy: 43.9825\n",
            "Validation Loss: 2.7428993381512394, Validation Accuracy: 32.82\n",
            "[19/150]: Training Loss: 2.0166405237197877, Training Accuracy: 46.195\n",
            "Validation Loss: 2.792896580544247, Validation Accuracy: 32.59\n",
            "[20/150]: Training Loss: 1.9404266941070556, Training Accuracy: 47.745\n",
            "Validation Loss: 2.742409686374057, Validation Accuracy: 34.58\n",
            "[21/150]: Training Loss: 1.8538161834716798, Training Accuracy: 49.4875\n",
            "Validation Loss: 2.809636064395783, Validation Accuracy: 33.93\n",
            "[22/150]: Training Loss: 1.768288578414917, Training Accuracy: 51.4775\n",
            "Validation Loss: 2.7697540058451855, Validation Accuracy: 34.68\n",
            "[23/150]: Training Loss: 1.684787001991272, Training Accuracy: 53.0825\n",
            "Validation Loss: 2.8915080067458425, Validation Accuracy: 33.94\n",
            "[24/150]: Training Loss: 1.6049392827987672, Training Accuracy: 55.145\n",
            "Validation Loss: 2.8930992533446878, Validation Accuracy: 33.94\n",
            "[25/150]: Training Loss: 1.511135014438629, Training Accuracy: 57.31\n",
            "Validation Loss: 2.9712191205115834, Validation Accuracy: 33.37\n",
            "[26/150]: Training Loss: 1.431588893699646, Training Accuracy: 59.12\n",
            "Validation Loss: 3.04860136311525, Validation Accuracy: 34.26\n",
            "[27/150]: Training Loss: 1.359563471508026, Training Accuracy: 60.8075\n",
            "Validation Loss: 3.205587582983029, Validation Accuracy: 33.21\n",
            "[28/150]: Training Loss: 1.2744348917961121, Training Accuracy: 63.0075\n",
            "Validation Loss: 3.1817259317750386, Validation Accuracy: 32.84\n",
            "[29/150]: Training Loss: 1.2077363389968871, Training Accuracy: 64.38\n",
            "Validation Loss: 3.2625666909916387, Validation Accuracy: 33.74\n",
            "[30/150]: Training Loss: 1.146615783405304, Training Accuracy: 66.3775\n",
            "Validation Loss: 3.439337396317986, Validation Accuracy: 32.76\n",
            "[31/150]: Training Loss: 1.0637209503173828, Training Accuracy: 68.4275\n",
            "Validation Loss: 3.464182601612844, Validation Accuracy: 32.3\n",
            "[32/150]: Training Loss: 1.0153738078117371, Training Accuracy: 69.42\n",
            "Validation Loss: 3.585658008125937, Validation Accuracy: 32.99\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 20.296875631733304, Test Accuracy: 15.72\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▆▁▄▃▄▄▄▄▄▄▄▄▄▄▅▅▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>Test Loss</td><td>▁▅▇▇▇▆▇██▇▇█████████████████████████████</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>15.72</td></tr><tr><td>Test Loss</td><td>20.29688</td></tr><tr><td>Train Accuracy</td><td>69.42</td></tr><tr><td>Train Loss</td><td>1.01537</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_031055-hfm8dgbz/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:2 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_031444-ws7fqwm1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1' target=\"_blank\">learning_rate=2 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.6014143745422365, Training Accuracy: 0.98\n",
            "Validation Loss: 4.606205284215842, Validation Accuracy: 0.91\n",
            "[2/150]: Training Loss: 4.605726162719726, Training Accuracy: 0.93\n",
            "Validation Loss: 4.606422876856129, Validation Accuracy: 0.91\n",
            "[3/150]: Training Loss: 4.605443458557129, Training Accuracy: 0.9375\n",
            "Validation Loss: 4.606579792727331, Validation Accuracy: 0.82\n",
            "[4/150]: Training Loss: 4.6053140991210935, Training Accuracy: 1.015\n",
            "Validation Loss: 4.606754381945179, Validation Accuracy: 0.82\n",
            "[5/150]: Training Loss: 4.6052259376525875, Training Accuracy: 1.0275\n",
            "Validation Loss: 4.60688726765335, Validation Accuracy: 0.82\n",
            "[6/150]: Training Loss: 4.605191477966309, Training Accuracy: 1.0175\n",
            "Validation Loss: 4.607019655264107, Validation Accuracy: 0.82\n",
            "[7/150]: Training Loss: 4.6051658882141115, Training Accuracy: 1.015\n",
            "Validation Loss: 4.607097844409335, Validation Accuracy: 0.82\n",
            "[8/150]: Training Loss: 4.605149390411377, Training Accuracy: 0.9775\n",
            "Validation Loss: 4.607179025176224, Validation Accuracy: 0.82\n",
            "[9/150]: Training Loss: 4.605146067810058, Training Accuracy: 0.9425\n",
            "Validation Loss: 4.6072455181437695, Validation Accuracy: 0.82\n",
            "[10/150]: Training Loss: 4.605130741882324, Training Accuracy: 1.02\n",
            "Validation Loss: 4.607305599625703, Validation Accuracy: 0.82\n",
            "[11/150]: Training Loss: 4.605136211395264, Training Accuracy: 0.9625\n",
            "Validation Loss: 4.607342504392005, Validation Accuracy: 0.82\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 4.605385412835771, Test Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▆███▆▇▇▇▇▇▇▇▇▆▇███▇████████████████████</td></tr><tr><td>Test Loss</td><td>█▅▁▁▃▃▅▄▄▄▄▄▄▄▄▅▅▅▅▅▆▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅</td></tr><tr><td>Train Accuracy</td><td>▅▁▂▇█▇▇▄▂▇▃</td></tr><tr><td>Train Loss</td><td>▁██▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.0</td></tr><tr><td>Test Loss</td><td>4.60539</td></tr><tr><td>Train Accuracy</td><td>0.9625</td></tr><tr><td>Train Loss</td><td>4.60514</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=2 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_031444-ws7fqwm1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "\n",
        "for lr in learning_rates:\n",
        "\n",
        "  print('='*50)\n",
        "  print(f'Hyperparameter with lr:{lr} and wd:{wd}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {'learning_rate': lr,\n",
        "                      'weight_decay' : wd\n",
        "                      }\n",
        "  # Load the model\n",
        "  model = LeNet5().to(device)\n",
        "\n",
        "  # Optimizer and scheduler setup\n",
        "  optimizer = LARS(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "  # Training\n",
        "  run_training(num_epochs,\n",
        "                model,\n",
        "                train_loader,\n",
        "                validation_loader,\n",
        "                test_loader,\n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                criterion,\n",
        "                device,\n",
        "                'LARS-HyperParameterTuning',\n",
        "                hyperparameters=hyperparameters,\n",
        "                is_wandb = True,\n",
        "                n_epochs_stop = 10\n",
        "  )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LARS BaseLine B-Size 64 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2bd5fda76227433aa9332e6e48cd415b",
            "fd6480fe59104c7aa15d39bf6ea4a63b",
            "3cb7b5c42e844747ae133750b7d42882",
            "b81cc89707e44dedb51081d13a3ba424",
            "1d74e9350c2c4c61b2e95924ec133012",
            "a5758bfac16a4388a036005a15812d1d",
            "0fdaf43c468043139e5d72397b118371",
            "f63fffe56ff64f1eb2b6e8f14974f011"
          ]
        },
        "id": "czSHzFs7SQug",
        "outputId": "fc513f95-8814-4002-e0d6-2a2eedad7dd7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_005802-t2cjgem5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5' target=\"_blank\">learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1584951/4263216451.py:113: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1578.)\n",
            "  d_p.add_(weight_decay, p.data)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.004822687724667, Training Accuracy: 8.156\n",
            "Validation Loss: 3.6169370329304105, Validation Accuracy: 15.1\n",
            "[2/150]: Training Loss: 3.489779775099986, Training Accuracy: 16.288\n",
            "Validation Loss: 3.195649910884298, Validation Accuracy: 21.71\n",
            "[3/150]: Training Loss: 3.188335987003258, Training Accuracy: 21.346\n",
            "Validation Loss: 2.9737777011409685, Validation Accuracy: 26.15\n",
            "[4/150]: Training Loss: 3.0118914528576006, Training Accuracy: 24.982\n",
            "Validation Loss: 2.7995891707717995, Validation Accuracy: 29.11\n",
            "[5/150]: Training Loss: 2.8688232484071152, Training Accuracy: 28.08\n",
            "Validation Loss: 2.7174989600090464, Validation Accuracy: 30.23\n",
            "[6/150]: Training Loss: 2.7214920926276984, Training Accuracy: 30.662\n",
            "Validation Loss: 2.5305145515757763, Validation Accuracy: 34.82\n",
            "[7/150]: Training Loss: 2.6200312084858983, Training Accuracy: 32.576\n",
            "Validation Loss: 2.474041129373441, Validation Accuracy: 35.97\n",
            "[8/150]: Training Loss: 2.53857664821093, Training Accuracy: 34.358\n",
            "Validation Loss: 2.3864964094890913, Validation Accuracy: 37.73\n",
            "[9/150]: Training Loss: 2.460603771764604, Training Accuracy: 36.126\n",
            "Validation Loss: 2.3855689604570913, Validation Accuracy: 38.2\n",
            "[10/150]: Training Loss: 2.3902963816052507, Training Accuracy: 37.29\n",
            "Validation Loss: 2.301407885399594, Validation Accuracy: 40.45\n",
            "[11/150]: Training Loss: 2.3503475908733087, Training Accuracy: 38.624\n",
            "Validation Loss: 2.3037940666174435, Validation Accuracy: 39.96\n",
            "[12/150]: Training Loss: 2.303130599696313, Training Accuracy: 39.49\n",
            "Validation Loss: 2.2026211046109534, Validation Accuracy: 42.0\n",
            "[13/150]: Training Loss: 2.2486851939459895, Training Accuracy: 40.568\n",
            "Validation Loss: 2.212206517055536, Validation Accuracy: 42.21\n",
            "[14/150]: Training Loss: 2.203081512085312, Training Accuracy: 41.506\n",
            "Validation Loss: 2.1992271386893694, Validation Accuracy: 43.09\n",
            "[15/150]: Training Loss: 2.159660982963679, Training Accuracy: 42.428\n",
            "Validation Loss: 2.1746684677281958, Validation Accuracy: 43.09\n",
            "[16/150]: Training Loss: 2.1303664485511877, Training Accuracy: 43.166\n",
            "Validation Loss: 2.146143364298875, Validation Accuracy: 44.21\n",
            "[17/150]: Training Loss: 2.1123076631589925, Training Accuracy: 43.67\n",
            "Validation Loss: 2.1201481705258605, Validation Accuracy: 44.27\n",
            "[18/150]: Training Loss: 2.0810553551939748, Training Accuracy: 44.61\n",
            "Validation Loss: 2.0423277001472036, Validation Accuracy: 46.15\n",
            "[19/150]: Training Loss: 2.0513661890993338, Training Accuracy: 44.866\n",
            "Validation Loss: 2.091191877225402, Validation Accuracy: 44.81\n",
            "[20/150]: Training Loss: 2.018301057541157, Training Accuracy: 45.826\n",
            "Validation Loss: 2.1546491832490178, Validation Accuracy: 43.92\n",
            "[21/150]: Training Loss: 1.99891439834824, Training Accuracy: 46.082\n",
            "Validation Loss: 2.149173748720983, Validation Accuracy: 44.11\n",
            "[22/150]: Training Loss: 1.9834696277023276, Training Accuracy: 46.51\n",
            "Validation Loss: 2.0435469044241934, Validation Accuracy: 46.15\n",
            "[23/150]: Training Loss: 1.954583641970554, Training Accuracy: 46.984\n",
            "Validation Loss: 2.0593765806999937, Validation Accuracy: 46.4\n",
            "[24/150]: Training Loss: 1.9357828209772134, Training Accuracy: 47.47\n",
            "Validation Loss: 1.9959042148225625, Validation Accuracy: 47.77\n",
            "[25/150]: Training Loss: 1.9152823200311198, Training Accuracy: 48.11\n",
            "Validation Loss: 2.007097185037698, Validation Accuracy: 46.74\n",
            "[26/150]: Training Loss: 1.889640983870572, Training Accuracy: 48.486\n",
            "Validation Loss: 2.0237637371014636, Validation Accuracy: 46.53\n",
            "[27/150]: Training Loss: 1.8898505502954468, Training Accuracy: 48.75\n",
            "Validation Loss: 1.9383376062295998, Validation Accuracy: 48.79\n",
            "[28/150]: Training Loss: 1.8655293333865797, Training Accuracy: 49.182\n",
            "Validation Loss: 2.000895071181522, Validation Accuracy: 47.12\n",
            "[29/150]: Training Loss: 1.843536861564802, Training Accuracy: 49.576\n",
            "Validation Loss: 2.0557514102595627, Validation Accuracy: 46.42\n",
            "[30/150]: Training Loss: 1.8280195388037834, Training Accuracy: 49.938\n",
            "Validation Loss: 1.9708276941518115, Validation Accuracy: 48.17\n",
            "[31/150]: Training Loss: 1.8121484304632982, Training Accuracy: 50.256\n",
            "Validation Loss: 2.0148930929269, Validation Accuracy: 48.07\n",
            "[32/150]: Training Loss: 1.7905213011195287, Training Accuracy: 51.016\n",
            "Validation Loss: 1.9494955805456562, Validation Accuracy: 48.66\n",
            "[33/150]: Training Loss: 1.7819690168513667, Training Accuracy: 51.152\n",
            "Validation Loss: 1.9355155472542829, Validation Accuracy: 49.85\n",
            "[34/150]: Training Loss: 1.7735850943628784, Training Accuracy: 51.054\n",
            "Validation Loss: 1.9679190664534356, Validation Accuracy: 48.93\n",
            "[35/150]: Training Loss: 1.7615117981000934, Training Accuracy: 51.618\n",
            "Validation Loss: 2.019153520559809, Validation Accuracy: 47.96\n",
            "[36/150]: Training Loss: 1.7415476721875809, Training Accuracy: 51.984\n",
            "Validation Loss: 1.9617887530357214, Validation Accuracy: 48.72\n",
            "[37/150]: Training Loss: 1.7273670095007132, Training Accuracy: 52.228\n",
            "Validation Loss: 1.9595575993228111, Validation Accuracy: 49.25\n",
            "[38/150]: Training Loss: 1.7174837630423134, Training Accuracy: 52.45\n",
            "Validation Loss: 1.9162586555359469, Validation Accuracy: 50.15\n",
            "[39/150]: Training Loss: 1.6948718257877222, Training Accuracy: 53.104\n",
            "Validation Loss: 1.9148104600845628, Validation Accuracy: 50.11\n",
            "[40/150]: Training Loss: 1.6948116973537923, Training Accuracy: 53.164\n",
            "Validation Loss: 1.9015048544877653, Validation Accuracy: 50.46\n",
            "[41/150]: Training Loss: 1.669160524292675, Training Accuracy: 53.832\n",
            "Validation Loss: 1.9202428220943282, Validation Accuracy: 49.95\n",
            "[42/150]: Training Loss: 1.6587599679027372, Training Accuracy: 53.99\n",
            "Validation Loss: 1.9989394374713776, Validation Accuracy: 48.53\n",
            "[43/150]: Training Loss: 1.6486307676033596, Training Accuracy: 54.324\n",
            "Validation Loss: 1.9677664428759531, Validation Accuracy: 49.26\n",
            "[44/150]: Training Loss: 1.6284820756034168, Training Accuracy: 54.798\n",
            "Validation Loss: 1.917847274215358, Validation Accuracy: 50.53\n",
            "[45/150]: Training Loss: 1.6195188324774623, Training Accuracy: 54.902\n",
            "Validation Loss: 1.9071525859225327, Validation Accuracy: 50.67\n",
            "[46/150]: Training Loss: 1.6109208017206558, Training Accuracy: 54.96\n",
            "Validation Loss: 1.8967621220145257, Validation Accuracy: 51.23\n",
            "[47/150]: Training Loss: 1.602793920375502, Training Accuracy: 55.188\n",
            "Validation Loss: 1.8559222494720653, Validation Accuracy: 51.43\n",
            "[48/150]: Training Loss: 1.5858894056066528, Training Accuracy: 55.652\n",
            "Validation Loss: 1.9233948349193404, Validation Accuracy: 50.32\n",
            "[49/150]: Training Loss: 1.5650417865694637, Training Accuracy: 56.188\n",
            "Validation Loss: 1.9384330412384811, Validation Accuracy: 49.91\n",
            "[50/150]: Training Loss: 1.5663130182744291, Training Accuracy: 56.228\n",
            "Validation Loss: 1.8889641389725313, Validation Accuracy: 51.35\n",
            "[51/150]: Training Loss: 1.542809939445437, Training Accuracy: 56.846\n",
            "Validation Loss: 1.8398898855136459, Validation Accuracy: 51.84\n",
            "[52/150]: Training Loss: 1.5279571914002108, Training Accuracy: 57.064\n",
            "Validation Loss: 1.9208611204366015, Validation Accuracy: 50.34\n",
            "[53/150]: Training Loss: 1.510901224735143, Training Accuracy: 57.122\n",
            "Validation Loss: 1.9133090471765797, Validation Accuracy: 50.8\n",
            "[54/150]: Training Loss: 1.511483409596831, Training Accuracy: 57.642\n",
            "Validation Loss: 1.8751734176259132, Validation Accuracy: 51.71\n",
            "[55/150]: Training Loss: 1.4843521323960152, Training Accuracy: 58.136\n",
            "Validation Loss: 1.8774340836105832, Validation Accuracy: 51.56\n",
            "[56/150]: Training Loss: 1.4896384542403014, Training Accuracy: 57.742\n",
            "Validation Loss: 1.8484339972210537, Validation Accuracy: 52.7\n",
            "[57/150]: Training Loss: 1.4539345915207778, Training Accuracy: 58.774\n",
            "Validation Loss: 1.8711960125880636, Validation Accuracy: 50.9\n",
            "[58/150]: Training Loss: 1.4413522976591153, Training Accuracy: 59.254\n",
            "Validation Loss: 1.8433482806394055, Validation Accuracy: 52.14\n",
            "[59/150]: Training Loss: 1.435598407407551, Training Accuracy: 59.348\n",
            "Validation Loss: 1.842708926291982, Validation Accuracy: 52.32\n",
            "[60/150]: Training Loss: 1.415708273572995, Training Accuracy: 59.856\n",
            "Validation Loss: 1.8708495941891032, Validation Accuracy: 51.34\n",
            "[61/150]: Training Loss: 1.4131718484489508, Training Accuracy: 59.934\n",
            "Validation Loss: 1.8397565495436359, Validation Accuracy: 52.21\n",
            "[62/150]: Training Loss: 1.3917764421466672, Training Accuracy: 60.466\n",
            "Validation Loss: 1.8553483759521678, Validation Accuracy: 52.56\n",
            "[63/150]: Training Loss: 1.3845181973541485, Training Accuracy: 60.468\n",
            "Validation Loss: 1.8767407699755043, Validation Accuracy: 51.4\n",
            "[64/150]: Training Loss: 1.3718093946156904, Training Accuracy: 60.9\n",
            "Validation Loss: 1.833029270931414, Validation Accuracy: 53.34\n",
            "[65/150]: Training Loss: 1.3485638827771482, Training Accuracy: 61.372\n",
            "Validation Loss: 1.8435825352456159, Validation Accuracy: 52.78\n",
            "[66/150]: Training Loss: 1.3356790865778618, Training Accuracy: 61.914\n",
            "Validation Loss: 1.8920623299422537, Validation Accuracy: 52.03\n",
            "[67/150]: Training Loss: 1.3243823959242047, Training Accuracy: 61.984\n",
            "Validation Loss: 1.8541991725848739, Validation Accuracy: 53.25\n",
            "[68/150]: Training Loss: 1.3088703974128684, Training Accuracy: 62.324\n",
            "Validation Loss: 1.8384279855497323, Validation Accuracy: 52.97\n",
            "[69/150]: Training Loss: 1.2935843141487493, Training Accuracy: 62.726\n",
            "Validation Loss: 1.8648313618010017, Validation Accuracy: 52.35\n",
            "[70/150]: Training Loss: 1.2788333388240747, Training Accuracy: 63.162\n",
            "Validation Loss: 1.8200989673092107, Validation Accuracy: 53.57\n",
            "[71/150]: Training Loss: 1.275286832810058, Training Accuracy: 63.326\n",
            "Validation Loss: 1.8271277934122996, Validation Accuracy: 52.6\n",
            "[72/150]: Training Loss: 1.2548354720063222, Training Accuracy: 63.724\n",
            "Validation Loss: 1.832458599357848, Validation Accuracy: 53.43\n",
            "[73/150]: Training Loss: 1.2409771790589823, Training Accuracy: 64.15\n",
            "Validation Loss: 1.8489187993821066, Validation Accuracy: 53.54\n",
            "[74/150]: Training Loss: 1.2177538118703897, Training Accuracy: 64.682\n",
            "Validation Loss: 1.8377945020699957, Validation Accuracy: 53.86\n",
            "[75/150]: Training Loss: 1.2167601452764039, Training Accuracy: 64.92\n",
            "Validation Loss: 1.856788229031168, Validation Accuracy: 53.15\n",
            "[76/150]: Training Loss: 1.1967681403964987, Training Accuracy: 65.394\n",
            "Validation Loss: 1.8202834759548212, Validation Accuracy: 54.44\n",
            "[77/150]: Training Loss: 1.1781836492021371, Training Accuracy: 65.698\n",
            "Validation Loss: 1.8182067392738002, Validation Accuracy: 54.42\n",
            "[78/150]: Training Loss: 1.1718934528967913, Training Accuracy: 65.94\n",
            "Validation Loss: 1.8361693392893312, Validation Accuracy: 53.34\n",
            "[79/150]: Training Loss: 1.1540917455387847, Training Accuracy: 66.372\n",
            "Validation Loss: 1.8517911365837048, Validation Accuracy: 53.86\n",
            "[80/150]: Training Loss: 1.1260635425215182, Training Accuracy: 67.01\n",
            "Validation Loss: 1.8486420083197819, Validation Accuracy: 53.93\n",
            "[81/150]: Training Loss: 1.1187372058248886, Training Accuracy: 67.426\n",
            "Validation Loss: 1.822678742894701, Validation Accuracy: 54.82\n",
            "[82/150]: Training Loss: 1.1072917601184162, Training Accuracy: 67.646\n",
            "Validation Loss: 1.8640035948935587, Validation Accuracy: 53.57\n",
            "[83/150]: Training Loss: 1.082973983510376, Training Accuracy: 68.116\n",
            "Validation Loss: 1.867675439567323, Validation Accuracy: 53.9\n",
            "[84/150]: Training Loss: 1.0828096682915602, Training Accuracy: 68.366\n",
            "Validation Loss: 1.826197045244229, Validation Accuracy: 54.16\n",
            "[85/150]: Training Loss: 1.0598852286100997, Training Accuracy: 68.856\n",
            "Validation Loss: 1.8198747148938998, Validation Accuracy: 54.4\n",
            "[86/150]: Training Loss: 1.0407975543185572, Training Accuracy: 69.42\n",
            "Validation Loss: 1.8279076829837386, Validation Accuracy: 54.55\n",
            "[87/150]: Training Loss: 1.030418163827618, Training Accuracy: 69.804\n",
            "Validation Loss: 1.854318435784358, Validation Accuracy: 54.88\n",
            "[88/150]: Training Loss: 1.0299987217120807, Training Accuracy: 69.594\n",
            "Validation Loss: 1.8434015595988862, Validation Accuracy: 54.47\n",
            "[89/150]: Training Loss: 1.0026646399741892, Training Accuracy: 70.49\n",
            "Validation Loss: 1.8365007965428055, Validation Accuracy: 54.24\n",
            "[90/150]: Training Loss: 0.9944430979164055, Training Accuracy: 70.764\n",
            "Validation Loss: 1.845389035097353, Validation Accuracy: 54.42\n",
            "[91/150]: Training Loss: 0.9791042178945468, Training Accuracy: 70.958\n",
            "Validation Loss: 1.8592563897940764, Validation Accuracy: 54.71\n",
            "[92/150]: Training Loss: 0.9622275731371491, Training Accuracy: 71.348\n",
            "Validation Loss: 1.8250258181505143, Validation Accuracy: 55.29\n",
            "[93/150]: Training Loss: 0.9501752741349018, Training Accuracy: 71.708\n",
            "Validation Loss: 1.849954966526882, Validation Accuracy: 55.02\n",
            "[94/150]: Training Loss: 0.9297837285358278, Training Accuracy: 72.38\n",
            "Validation Loss: 1.8463909307103248, Validation Accuracy: 55.77\n",
            "[95/150]: Training Loss: 0.9163948694991944, Training Accuracy: 72.898\n",
            "Validation Loss: 1.8669461308011583, Validation Accuracy: 55.06\n",
            "[96/150]: Training Loss: 0.9114100606468938, Training Accuracy: 72.956\n",
            "Validation Loss: 1.8656909784693627, Validation Accuracy: 54.75\n",
            "[97/150]: Training Loss: 0.8974789249165284, Training Accuracy: 73.276\n",
            "Validation Loss: 1.834348332350421, Validation Accuracy: 55.01\n",
            "[98/150]: Training Loss: 0.8797091352360328, Training Accuracy: 73.696\n",
            "Validation Loss: 1.8680614483584264, Validation Accuracy: 55.11\n",
            "[99/150]: Training Loss: 0.8685366097085007, Training Accuracy: 74.026\n",
            "Validation Loss: 1.8696094318559975, Validation Accuracy: 55.43\n",
            "[100/150]: Training Loss: 0.8596895751745804, Training Accuracy: 74.396\n",
            "Validation Loss: 1.8580050965782944, Validation Accuracy: 55.31\n",
            "[101/150]: Training Loss: 0.8450928368531835, Training Accuracy: 74.934\n",
            "Validation Loss: 1.8560257132645626, Validation Accuracy: 55.71\n",
            "[102/150]: Training Loss: 0.834670692567935, Training Accuracy: 75.278\n",
            "Validation Loss: 1.8615986776959366, Validation Accuracy: 55.84\n",
            "[103/150]: Training Loss: 0.8168128573757303, Training Accuracy: 75.56\n",
            "Validation Loss: 1.8920900590100866, Validation Accuracy: 55.38\n",
            "[104/150]: Training Loss: 0.8022570627577165, Training Accuracy: 76.022\n",
            "Validation Loss: 1.9013077168707635, Validation Accuracy: 55.71\n",
            "[105/150]: Training Loss: 0.7898137537414766, Training Accuracy: 76.588\n",
            "Validation Loss: 1.8971044345266501, Validation Accuracy: 55.23\n",
            "[106/150]: Training Loss: 0.7799203321528252, Training Accuracy: 76.854\n",
            "Validation Loss: 1.8587115873956377, Validation Accuracy: 55.33\n",
            "[107/150]: Training Loss: 0.7687876791600377, Training Accuracy: 76.832\n",
            "Validation Loss: 1.9066706402286602, Validation Accuracy: 55.7\n",
            "[108/150]: Training Loss: 0.7502601898234823, Training Accuracy: 77.674\n",
            "Validation Loss: 1.891866291784177, Validation Accuracy: 56.06\n",
            "[109/150]: Training Loss: 0.7453718431236799, Training Accuracy: 77.84\n",
            "Validation Loss: 1.9089836011267012, Validation Accuracy: 55.88\n",
            "[110/150]: Training Loss: 0.7324799866322667, Training Accuracy: 78.2\n",
            "Validation Loss: 1.8889893972949616, Validation Accuracy: 55.7\n",
            "[111/150]: Training Loss: 0.716928729620736, Training Accuracy: 78.53\n",
            "Validation Loss: 1.9158697025791096, Validation Accuracy: 56.07\n",
            "[112/150]: Training Loss: 0.7084723916809882, Training Accuracy: 78.768\n",
            "Validation Loss: 1.9187719480247254, Validation Accuracy: 55.71\n",
            "[113/150]: Training Loss: 0.6928996008146754, Training Accuracy: 79.508\n",
            "Validation Loss: 1.9122050970223299, Validation Accuracy: 56.4\n",
            "[114/150]: Training Loss: 0.686598046775669, Training Accuracy: 79.428\n",
            "Validation Loss: 1.895180571990408, Validation Accuracy: 56.18\n",
            "[115/150]: Training Loss: 0.6837174600881079, Training Accuracy: 79.66\n",
            "Validation Loss: 1.9321321894408792, Validation Accuracy: 55.4\n",
            "[116/150]: Training Loss: 0.6646802525233735, Training Accuracy: 80.106\n",
            "Validation Loss: 1.9081798047776435, Validation Accuracy: 56.21\n",
            "[117/150]: Training Loss: 0.6522155370172638, Training Accuracy: 80.528\n",
            "Validation Loss: 1.9046855428416258, Validation Accuracy: 56.69\n",
            "[118/150]: Training Loss: 0.6456939719064766, Training Accuracy: 80.752\n",
            "Validation Loss: 1.9132253895899294, Validation Accuracy: 56.47\n",
            "[119/150]: Training Loss: 0.6339363064378729, Training Accuracy: 81.286\n",
            "Validation Loss: 1.914469665782467, Validation Accuracy: 56.51\n",
            "[120/150]: Training Loss: 0.6339401570351227, Training Accuracy: 81.134\n",
            "Validation Loss: 1.913045568830648, Validation Accuracy: 55.93\n",
            "[121/150]: Training Loss: 0.6202226441610804, Training Accuracy: 81.422\n",
            "Validation Loss: 1.9112963069016766, Validation Accuracy: 56.58\n",
            "[122/150]: Training Loss: 0.6146376765216403, Training Accuracy: 81.858\n",
            "Validation Loss: 1.916867652516456, Validation Accuracy: 56.28\n",
            "[123/150]: Training Loss: 0.6090771163363591, Training Accuracy: 81.96\n",
            "Validation Loss: 1.9238637059357515, Validation Accuracy: 56.33\n",
            "[124/150]: Training Loss: 0.5977434807497523, Training Accuracy: 82.352\n",
            "Validation Loss: 1.9187599549627607, Validation Accuracy: 56.09\n",
            "[125/150]: Training Loss: 0.5865007763933343, Training Accuracy: 82.728\n",
            "Validation Loss: 1.9290249192031326, Validation Accuracy: 55.85\n",
            "[126/150]: Training Loss: 0.5832711922390686, Training Accuracy: 82.85\n",
            "Validation Loss: 1.925058329560954, Validation Accuracy: 56.54\n",
            "[127/150]: Training Loss: 0.5710540865845692, Training Accuracy: 83.058\n",
            "Validation Loss: 1.9433572846613112, Validation Accuracy: 56.46\n",
            "[128/150]: Training Loss: 0.566251437987208, Training Accuracy: 83.328\n",
            "Validation Loss: 1.9474792632327718, Validation Accuracy: 56.24\n",
            "[129/150]: Training Loss: 0.5592512233787791, Training Accuracy: 83.606\n",
            "Validation Loss: 1.9336790825910628, Validation Accuracy: 56.49\n",
            "[130/150]: Training Loss: 0.5560948127675849, Training Accuracy: 83.726\n",
            "Validation Loss: 1.9358682472994373, Validation Accuracy: 56.75\n",
            "[131/150]: Training Loss: 0.5547005703191623, Training Accuracy: 83.68\n",
            "Validation Loss: 1.941912688647106, Validation Accuracy: 56.32\n",
            "[132/150]: Training Loss: 0.5438491536299591, Training Accuracy: 84.188\n",
            "Validation Loss: 1.9333096700868788, Validation Accuracy: 56.52\n",
            "[133/150]: Training Loss: 0.5359534242421465, Training Accuracy: 84.398\n",
            "Validation Loss: 1.9515662831106004, Validation Accuracy: 56.36\n",
            "[134/150]: Training Loss: 0.5345852662763937, Training Accuracy: 84.37\n",
            "Validation Loss: 1.939547040280263, Validation Accuracy: 56.31\n",
            "[135/150]: Training Loss: 0.5338244077266024, Training Accuracy: 84.354\n",
            "Validation Loss: 1.9529236646214867, Validation Accuracy: 56.4\n",
            "[136/150]: Training Loss: 0.5233329969751256, Training Accuracy: 84.792\n",
            "Validation Loss: 1.9439837556735726, Validation Accuracy: 56.43\n",
            "[137/150]: Training Loss: 0.519332280549247, Training Accuracy: 84.868\n",
            "Validation Loss: 1.9427648835880742, Validation Accuracy: 56.25\n",
            "[138/150]: Training Loss: 0.513342816468395, Training Accuracy: 85.202\n",
            "Validation Loss: 1.9522086123751987, Validation Accuracy: 56.69\n",
            "[139/150]: Training Loss: 0.5097889236515135, Training Accuracy: 85.132\n",
            "Validation Loss: 1.950890618904381, Validation Accuracy: 56.41\n",
            "[140/150]: Training Loss: 0.5080071812319329, Training Accuracy: 85.386\n",
            "Validation Loss: 1.9511753282729227, Validation Accuracy: 56.69\n",
            "[141/150]: Training Loss: 0.5083310039299528, Training Accuracy: 85.398\n",
            "Validation Loss: 1.9493300193434309, Validation Accuracy: 56.5\n",
            "[142/150]: Training Loss: 0.5110094372726157, Training Accuracy: 85.232\n",
            "Validation Loss: 1.9510933920076698, Validation Accuracy: 56.46\n",
            "[143/150]: Training Loss: 0.5043484553161179, Training Accuracy: 85.402\n",
            "Validation Loss: 1.9473720959797027, Validation Accuracy: 56.53\n",
            "[144/150]: Training Loss: 0.5001817758926346, Training Accuracy: 85.59\n",
            "Validation Loss: 1.9499852835752403, Validation Accuracy: 56.64\n",
            "[145/150]: Training Loss: 0.5050505888088584, Training Accuracy: 85.566\n",
            "Validation Loss: 1.9509570135432444, Validation Accuracy: 56.58\n",
            "[146/150]: Training Loss: 0.4944563165230824, Training Accuracy: 85.7\n",
            "Validation Loss: 1.9510046653686814, Validation Accuracy: 56.56\n",
            "[147/150]: Training Loss: 0.4947198845655717, Training Accuracy: 85.914\n",
            "Validation Loss: 1.9500796012817674, Validation Accuracy: 56.62\n",
            "[148/150]: Training Loss: 0.4965038236487857, Training Accuracy: 85.548\n",
            "Validation Loss: 1.9505202922092122, Validation Accuracy: 56.65\n",
            "[149/150]: Training Loss: 0.4984786443964905, Training Accuracy: 85.54\n",
            "Validation Loss: 1.94986033325742, Validation Accuracy: 56.62\n",
            "[150/150]: Training Loss: 0.498654707256333, Training Accuracy: 85.464\n",
            "Validation Loss: 1.9498052581860001, Validation Accuracy: 56.63\n",
            "**********************************************************************\n",
            "Test Loss: 1.9498052581860001, Test Accuracy: 56.63\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▆█▃▄▃▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Test Loss</td><td>▁▃▆▅██▇█▆▇▆▅▆▇█▇▆▇▆▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▆▆▇▆▆▆</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>56.63</td></tr><tr><td>Test Loss</td><td>1.94981</td></tr><tr><td>Train Accuracy</td><td>85.464</td></tr><tr><td>Train Loss</td><td>0.49865</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_005802-t2cjgem5/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 1.5\n",
        "wd = 1e-03\n",
        "\n",
        "hyperparameters = {'learning_rate': lr,\n",
        "                    'weight_decay' : wd\n",
        "                  }\n",
        "\n",
        "# Load the model\n",
        "model = LeNet5().to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer = LARS(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# Training\n",
        "run_training(\n",
        "    num_epochs,\n",
        "    model,\n",
        "    original_train_loader,\n",
        "    original_test_loader,\n",
        "    original_test_loader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    criterion,\n",
        "    device,\n",
        "    optimizer_name='LARS',\n",
        "    hyperparameters=hyperparameters,\n",
        "    is_wandb = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LARS Test Large Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "81408f178c46447f90d36d1d18cdad82",
            "06d3de2006b14793bd57a5ca89d44e4a",
            "804c12a3f13840c7bdb2e6df69e62c10",
            "4a4cfb14c56e477cbfef5a652c05fabc",
            "68d86018628f420e8d7e516ba5827e35",
            "8877fd11846246f989e31835e5d3e7ae",
            "68ff4108835c462b929d0b5c78497555",
            "a63e1d987556448280ca217f17b60b49"
          ]
        },
        "id": "RFWHh4OL50WX",
        "outputId": "099a0039-9168-4d4e-84ec-4d6300dd3498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 512, Learning rate: 4.242640687119286, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:9hoyxk41) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=512 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/9hoyxk41' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/9hoyxk41</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_012745-9hoyxk41/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:9hoyxk41). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_012749-momu4e5u</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u' target=\"_blank\">batch_size=512 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.194531238808924, Training Accuracy: 5.802\n",
            "Validation Loss: 3.8219263911247254, Validation Accuracy: 11.06\n",
            "[2/150]: Training Loss: 3.737521487839368, Training Accuracy: 12.162\n",
            "Validation Loss: 3.5326468467712404, Validation Accuracy: 15.5\n",
            "[3/150]: Training Loss: 3.469568571265863, Training Accuracy: 16.654\n",
            "Validation Loss: 3.2325081706047056, Validation Accuracy: 21.14\n",
            "[4/150]: Training Loss: 3.2623822543085836, Training Accuracy: 20.338\n",
            "Validation Loss: 3.041664385795593, Validation Accuracy: 24.34\n",
            "[5/150]: Training Loss: 3.1202199824002324, Training Accuracy: 23.134\n",
            "Validation Loss: 2.9670259952545166, Validation Accuracy: 26.23\n",
            "[6/150]: Training Loss: 2.973516508024566, Training Accuracy: 25.864\n",
            "Validation Loss: 2.835892844200134, Validation Accuracy: 28.97\n",
            "[7/150]: Training Loss: 2.8618772808386357, Training Accuracy: 27.73\n",
            "Validation Loss: 2.731530475616455, Validation Accuracy: 31.08\n",
            "[8/150]: Training Loss: 2.7622045083921782, Training Accuracy: 29.856\n",
            "Validation Loss: 2.6019123077392576, Validation Accuracy: 33.01\n",
            "[9/150]: Training Loss: 2.719049726213728, Training Accuracy: 30.804\n",
            "Validation Loss: 2.6247976064682006, Validation Accuracy: 33.43\n",
            "[10/150]: Training Loss: 2.6084506000791277, Training Accuracy: 33.166\n",
            "Validation Loss: 2.4777934432029722, Validation Accuracy: 36.28\n",
            "[11/150]: Training Loss: 2.5361861233808556, Training Accuracy: 34.45\n",
            "Validation Loss: 2.3982665181159972, Validation Accuracy: 37.66\n",
            "[12/150]: Training Loss: 2.486152622164512, Training Accuracy: 35.61\n",
            "Validation Loss: 2.4073413372039796, Validation Accuracy: 37.69\n",
            "[13/150]: Training Loss: 2.419785971544227, Training Accuracy: 36.994\n",
            "Validation Loss: 2.3586077094078064, Validation Accuracy: 39.62\n",
            "[14/150]: Training Loss: 2.378681479668131, Training Accuracy: 37.686\n",
            "Validation Loss: 2.2874060750007628, Validation Accuracy: 40.44\n",
            "[15/150]: Training Loss: 2.3229291341742693, Training Accuracy: 39.04\n",
            "Validation Loss: 2.391434836387634, Validation Accuracy: 38.28\n",
            "[16/150]: Training Loss: 2.3006048445798912, Training Accuracy: 39.566\n",
            "Validation Loss: 2.233357620239258, Validation Accuracy: 40.92\n",
            "[17/150]: Training Loss: 2.270021514016755, Training Accuracy: 40.304\n",
            "Validation Loss: 2.2809773087501526, Validation Accuracy: 40.35\n",
            "[18/150]: Training Loss: 2.219994238444737, Training Accuracy: 41.128\n",
            "Validation Loss: 2.208327281475067, Validation Accuracy: 42.26\n",
            "[19/150]: Training Loss: 2.1639412665853697, Training Accuracy: 42.49\n",
            "Validation Loss: 2.1477415204048156, Validation Accuracy: 43.47\n",
            "[20/150]: Training Loss: 2.1379866052647025, Training Accuracy: 43.012\n",
            "Validation Loss: 2.202856254577637, Validation Accuracy: 42.27\n",
            "[21/150]: Training Loss: 2.139409989726787, Training Accuracy: 43.086\n",
            "Validation Loss: 2.1385614931583405, Validation Accuracy: 43.93\n",
            "[22/150]: Training Loss: 2.07097029564332, Training Accuracy: 44.974\n",
            "Validation Loss: 2.128925609588623, Validation Accuracy: 44.07\n",
            "[23/150]: Training Loss: 2.0605698106240253, Training Accuracy: 44.808\n",
            "Validation Loss: 2.15288764834404, Validation Accuracy: 44.03\n",
            "[24/150]: Training Loss: 2.0457090200210106, Training Accuracy: 45.174\n",
            "Validation Loss: 2.1201067209243774, Validation Accuracy: 44.63\n",
            "[25/150]: Training Loss: 2.0101604510326774, Training Accuracy: 45.978\n",
            "Validation Loss: 2.0930452048778534, Validation Accuracy: 45.37\n",
            "[26/150]: Training Loss: 1.9699756795046282, Training Accuracy: 46.9\n",
            "Validation Loss: 2.089737904071808, Validation Accuracy: 45.61\n",
            "[27/150]: Training Loss: 1.967079225851565, Training Accuracy: 47.01\n",
            "Validation Loss: 2.1089664578437803, Validation Accuracy: 44.55\n",
            "[28/150]: Training Loss: 1.9404766535272404, Training Accuracy: 47.568\n",
            "Validation Loss: 2.0624644994735717, Validation Accuracy: 45.57\n",
            "[29/150]: Training Loss: 1.928514483023663, Training Accuracy: 47.712\n",
            "Validation Loss: 2.0849966049194335, Validation Accuracy: 45.11\n",
            "[30/150]: Training Loss: 1.9005685375661265, Training Accuracy: 48.44\n",
            "Validation Loss: 2.015596163272858, Validation Accuracy: 46.79\n",
            "[31/150]: Training Loss: 1.868617719533492, Training Accuracy: 49.064\n",
            "Validation Loss: 2.044619733095169, Validation Accuracy: 45.67\n",
            "[32/150]: Training Loss: 1.8784857465296376, Training Accuracy: 48.794\n",
            "Validation Loss: 2.029283958673477, Validation Accuracy: 46.27\n",
            "[33/150]: Training Loss: 1.8382798968529215, Training Accuracy: 49.868\n",
            "Validation Loss: 1.9727658331394196, Validation Accuracy: 48.14\n",
            "[34/150]: Training Loss: 1.8194199094966965, Training Accuracy: 50.486\n",
            "Validation Loss: 1.9934200942516327, Validation Accuracy: 47.5\n",
            "[35/150]: Training Loss: 1.7988280094399745, Training Accuracy: 50.79\n",
            "Validation Loss: 2.0087626039981843, Validation Accuracy: 46.76\n",
            "[36/150]: Training Loss: 1.8009161900500863, Training Accuracy: 50.804\n",
            "Validation Loss: 1.9823269903659821, Validation Accuracy: 48.14\n",
            "[37/150]: Training Loss: 1.767955198579905, Training Accuracy: 51.498\n",
            "Validation Loss: 1.973670369386673, Validation Accuracy: 48.18\n",
            "[38/150]: Training Loss: 1.761318182458683, Training Accuracy: 51.526\n",
            "Validation Loss: 1.9882086098194123, Validation Accuracy: 47.85\n",
            "[39/150]: Training Loss: 1.7494291188765545, Training Accuracy: 52.066\n",
            "Validation Loss: 1.945462554693222, Validation Accuracy: 48.63\n",
            "[40/150]: Training Loss: 1.7158451688532927, Training Accuracy: 52.788\n",
            "Validation Loss: 1.9138611912727357, Validation Accuracy: 49.24\n",
            "[41/150]: Training Loss: 1.7024361783144426, Training Accuracy: 52.972\n",
            "Validation Loss: 1.9541066110134124, Validation Accuracy: 49.38\n",
            "[42/150]: Training Loss: 1.6934348527266054, Training Accuracy: 53.184\n",
            "Validation Loss: 1.9292299151420593, Validation Accuracy: 49.48\n",
            "[43/150]: Training Loss: 1.6734726489806662, Training Accuracy: 53.69\n",
            "Validation Loss: 1.9281952559947968, Validation Accuracy: 49.57\n",
            "[44/150]: Training Loss: 1.6466177933070125, Training Accuracy: 54.438\n",
            "Validation Loss: 1.92038534283638, Validation Accuracy: 49.7\n",
            "[45/150]: Training Loss: 1.6545339092916371, Training Accuracy: 54.198\n",
            "Validation Loss: 1.9556318461894988, Validation Accuracy: 49.26\n",
            "[46/150]: Training Loss: 1.619915745696243, Training Accuracy: 54.986\n",
            "Validation Loss: 1.899458384513855, Validation Accuracy: 50.32\n",
            "[47/150]: Training Loss: 1.6211930987786274, Training Accuracy: 54.738\n",
            "Validation Loss: 1.9299038767814636, Validation Accuracy: 49.14\n",
            "[48/150]: Training Loss: 1.5995109361045214, Training Accuracy: 55.258\n",
            "Validation Loss: 1.9302596151828766, Validation Accuracy: 49.32\n",
            "[49/150]: Training Loss: 1.5750943154704815, Training Accuracy: 55.898\n",
            "Validation Loss: 1.8993667602539062, Validation Accuracy: 49.91\n",
            "[50/150]: Training Loss: 1.5744633698950008, Training Accuracy: 55.76\n",
            "Validation Loss: 1.9295048713684082, Validation Accuracy: 50.3\n",
            "[51/150]: Training Loss: 1.5583884862004493, Training Accuracy: 56.348\n",
            "Validation Loss: 1.925916600227356, Validation Accuracy: 50.14\n",
            "[52/150]: Training Loss: 1.5439508910081825, Training Accuracy: 56.592\n",
            "Validation Loss: 1.8903591930866241, Validation Accuracy: 50.65\n",
            "[53/150]: Training Loss: 1.5187961057740815, Training Accuracy: 57.12\n",
            "Validation Loss: 1.893898105621338, Validation Accuracy: 51.28\n",
            "[54/150]: Training Loss: 1.500082329827912, Training Accuracy: 57.754\n",
            "Validation Loss: 1.8901280999183654, Validation Accuracy: 50.42\n",
            "[55/150]: Training Loss: 1.5054621915428006, Training Accuracy: 57.632\n",
            "Validation Loss: 1.890578955411911, Validation Accuracy: 51.07\n",
            "[56/150]: Training Loss: 1.4812841756003243, Training Accuracy: 57.986\n",
            "Validation Loss: 1.9068008363246918, Validation Accuracy: 51.44\n",
            "[57/150]: Training Loss: 1.4542514341218131, Training Accuracy: 58.654\n",
            "Validation Loss: 1.8665942788124084, Validation Accuracy: 51.26\n",
            "[58/150]: Training Loss: 1.4304890097404013, Training Accuracy: 59.206\n",
            "Validation Loss: 1.872557681798935, Validation Accuracy: 51.35\n",
            "[59/150]: Training Loss: 1.4232833567930727, Training Accuracy: 59.584\n",
            "Validation Loss: 1.9096780002117157, Validation Accuracy: 50.55\n",
            "[60/150]: Training Loss: 1.4309481491847915, Training Accuracy: 59.184\n",
            "Validation Loss: 1.8791188061237336, Validation Accuracy: 51.2\n",
            "[61/150]: Training Loss: 1.40531452334657, Training Accuracy: 59.96\n",
            "Validation Loss: 1.8862035810947417, Validation Accuracy: 51.85\n",
            "[62/150]: Training Loss: 1.3938273799662688, Training Accuracy: 60.274\n",
            "Validation Loss: 1.8729142725467682, Validation Accuracy: 51.37\n",
            "[63/150]: Training Loss: 1.3863425692733453, Training Accuracy: 60.462\n",
            "Validation Loss: 1.8725131154060364, Validation Accuracy: 52.01\n",
            "[64/150]: Training Loss: 1.366844469187211, Training Accuracy: 60.966\n",
            "Validation Loss: 1.8498412251472474, Validation Accuracy: 51.87\n",
            "[65/150]: Training Loss: 1.3531476259231567, Training Accuracy: 61.048\n",
            "Validation Loss: 1.8571744859218597, Validation Accuracy: 52.36\n",
            "[66/150]: Training Loss: 1.3398733954040372, Training Accuracy: 61.472\n",
            "Validation Loss: 1.869799542427063, Validation Accuracy: 52.02\n",
            "[67/150]: Training Loss: 1.3146201080205488, Training Accuracy: 62.184\n",
            "Validation Loss: 1.85557941198349, Validation Accuracy: 52.16\n",
            "[68/150]: Training Loss: 1.3349930096645743, Training Accuracy: 61.624\n",
            "Validation Loss: 1.899474561214447, Validation Accuracy: 51.8\n",
            "[69/150]: Training Loss: 1.2890492300597989, Training Accuracy: 63.07\n",
            "Validation Loss: 1.8615913569927216, Validation Accuracy: 52.55\n",
            "[70/150]: Training Loss: 1.281143541238746, Training Accuracy: 63.172\n",
            "Validation Loss: 1.8803191304206848, Validation Accuracy: 51.97\n",
            "[71/150]: Training Loss: 1.2725608847579177, Training Accuracy: 63.378\n",
            "Validation Loss: 1.859445983171463, Validation Accuracy: 52.42\n",
            "[72/150]: Training Loss: 1.2559378949963316, Training Accuracy: 63.878\n",
            "Validation Loss: 1.8568916201591492, Validation Accuracy: 52.05\n",
            "[73/150]: Training Loss: 1.236931935865052, Training Accuracy: 64.244\n",
            "Validation Loss: 1.8739505887031556, Validation Accuracy: 52.77\n",
            "[74/150]: Training Loss: 1.2173650824293798, Training Accuracy: 64.762\n",
            "Validation Loss: 1.8348273098468781, Validation Accuracy: 53.4\n",
            "[75/150]: Training Loss: 1.2067504488691991, Training Accuracy: 65.066\n",
            "Validation Loss: 1.8692607581615448, Validation Accuracy: 53.1\n",
            "[76/150]: Training Loss: 1.1903777292796545, Training Accuracy: 65.334\n",
            "Validation Loss: 1.8275393545627594, Validation Accuracy: 54.25\n",
            "[77/150]: Training Loss: 1.1792806843105628, Training Accuracy: 65.694\n",
            "Validation Loss: 1.8515967845916748, Validation Accuracy: 53.52\n",
            "[78/150]: Training Loss: 1.1699374263383904, Training Accuracy: 65.942\n",
            "Validation Loss: 1.8811356365680694, Validation Accuracy: 53.09\n",
            "[79/150]: Training Loss: 1.1490371531369734, Training Accuracy: 66.792\n",
            "Validation Loss: 1.8353333652019501, Validation Accuracy: 53.82\n",
            "[80/150]: Training Loss: 1.13668700383634, Training Accuracy: 66.562\n",
            "Validation Loss: 1.8432445168495177, Validation Accuracy: 53.57\n",
            "[81/150]: Training Loss: 1.1307378818794174, Training Accuracy: 67.21\n",
            "Validation Loss: 1.8481176435947417, Validation Accuracy: 53.14\n",
            "[82/150]: Training Loss: 1.113474543605532, Training Accuracy: 67.312\n",
            "Validation Loss: 1.8410939693450927, Validation Accuracy: 53.83\n",
            "[83/150]: Training Loss: 1.087531100122296, Training Accuracy: 68.23\n",
            "Validation Loss: 1.8605490565299987, Validation Accuracy: 54.45\n",
            "[84/150]: Training Loss: 1.0892518618885352, Training Accuracy: 68.016\n",
            "Validation Loss: 1.8244962751865388, Validation Accuracy: 54.28\n",
            "[85/150]: Training Loss: 1.0597951071602958, Training Accuracy: 68.916\n",
            "Validation Loss: 1.848558533191681, Validation Accuracy: 54.32\n",
            "[86/150]: Training Loss: 1.0610668713949165, Training Accuracy: 69.08\n",
            "Validation Loss: 1.834687203168869, Validation Accuracy: 54.36\n",
            "[87/150]: Training Loss: 1.029247879373784, Training Accuracy: 69.662\n",
            "Validation Loss: 1.8577094554901123, Validation Accuracy: 54.1\n",
            "[88/150]: Training Loss: 1.0305843438420976, Training Accuracy: 69.63\n",
            "Validation Loss: 1.88287433385849, Validation Accuracy: 53.74\n",
            "[89/150]: Training Loss: 1.0086117690923262, Training Accuracy: 70.346\n",
            "Validation Loss: 1.8663456857204437, Validation Accuracy: 53.54\n",
            "[90/150]: Training Loss: 0.9981355539390019, Training Accuracy: 70.802\n",
            "Validation Loss: 1.8623527228832244, Validation Accuracy: 53.93\n",
            "[91/150]: Training Loss: 0.986907957159743, Training Accuracy: 70.956\n",
            "Validation Loss: 1.8775681614875794, Validation Accuracy: 54.56\n",
            "[92/150]: Training Loss: 0.9677880217834395, Training Accuracy: 71.402\n",
            "Validation Loss: 1.886433583498001, Validation Accuracy: 54.06\n",
            "[93/150]: Training Loss: 0.9601449692735866, Training Accuracy: 71.714\n",
            "Validation Loss: 1.848057508468628, Validation Accuracy: 55.12\n",
            "[94/150]: Training Loss: 0.9483164567120221, Training Accuracy: 71.99\n",
            "Validation Loss: 1.8797329545021058, Validation Accuracy: 54.52\n",
            "[95/150]: Training Loss: 0.9349474110165421, Training Accuracy: 72.398\n",
            "Validation Loss: 1.8802269518375396, Validation Accuracy: 54.13\n",
            "[96/150]: Training Loss: 0.9153923319310558, Training Accuracy: 72.582\n",
            "Validation Loss: 1.8897387385368347, Validation Accuracy: 54.28\n",
            "[97/150]: Training Loss: 0.9160392132340646, Training Accuracy: 72.794\n",
            "Validation Loss: 1.9283715963363648, Validation Accuracy: 53.5\n",
            "[98/150]: Training Loss: 0.9007182382807439, Training Accuracy: 73.294\n",
            "Validation Loss: 1.8609421133995057, Validation Accuracy: 54.96\n",
            "[99/150]: Training Loss: 0.8783578246223683, Training Accuracy: 73.912\n",
            "Validation Loss: 1.8637104988098145, Validation Accuracy: 54.56\n",
            "[100/150]: Training Loss: 0.8765367439814976, Training Accuracy: 74.022\n",
            "Validation Loss: 1.8750475943088531, Validation Accuracy: 54.59\n",
            "[101/150]: Training Loss: 0.8546222393610039, Training Accuracy: 74.918\n",
            "Validation Loss: 1.8807278335094453, Validation Accuracy: 55.13\n",
            "[102/150]: Training Loss: 0.8389398102857628, Training Accuracy: 74.972\n",
            "Validation Loss: 1.8944664776325226, Validation Accuracy: 54.78\n",
            "[103/150]: Training Loss: 0.8364474201688961, Training Accuracy: 75.32\n",
            "Validation Loss: 1.8999429523944855, Validation Accuracy: 54.68\n",
            "[104/150]: Training Loss: 0.8233104719191181, Training Accuracy: 75.606\n",
            "Validation Loss: 1.9150757372379303, Validation Accuracy: 54.49\n",
            "[105/150]: Training Loss: 0.8071710479502775, Training Accuracy: 76.094\n",
            "Validation Loss: 1.8923523843288421, Validation Accuracy: 55.54\n",
            "[106/150]: Training Loss: 0.7909371567015745, Training Accuracy: 76.448\n",
            "Validation Loss: 1.883741980791092, Validation Accuracy: 54.92\n",
            "[107/150]: Training Loss: 0.7867257595062256, Training Accuracy: 76.748\n",
            "Validation Loss: 1.8950099110603333, Validation Accuracy: 55.47\n",
            "[108/150]: Training Loss: 0.7650761269793218, Training Accuracy: 77.31\n",
            "Validation Loss: 1.9024061024188996, Validation Accuracy: 54.91\n",
            "[109/150]: Training Loss: 0.7652728691393015, Training Accuracy: 77.178\n",
            "Validation Loss: 1.9312968671321868, Validation Accuracy: 54.57\n",
            "[110/150]: Training Loss: 0.7572217492424712, Training Accuracy: 77.548\n",
            "Validation Loss: 1.8906243860721588, Validation Accuracy: 55.71\n",
            "[111/150]: Training Loss: 0.738684381149253, Training Accuracy: 77.932\n",
            "Validation Loss: 1.945603609085083, Validation Accuracy: 55.18\n",
            "[112/150]: Training Loss: 0.7319968044757843, Training Accuracy: 78.382\n",
            "Validation Loss: 1.9286673545837403, Validation Accuracy: 55.27\n",
            "[113/150]: Training Loss: 0.7233879006638819, Training Accuracy: 78.482\n",
            "Validation Loss: 1.9232488691806793, Validation Accuracy: 55.58\n",
            "[114/150]: Training Loss: 0.7083694521261721, Training Accuracy: 78.814\n",
            "Validation Loss: 1.907062864303589, Validation Accuracy: 55.51\n",
            "[115/150]: Training Loss: 0.7031112666032753, Training Accuracy: 79.204\n",
            "Validation Loss: 1.9117585182189942, Validation Accuracy: 56.02\n",
            "[116/150]: Training Loss: 0.6852655745282465, Training Accuracy: 79.458\n",
            "Validation Loss: 1.9387612223625184, Validation Accuracy: 55.18\n",
            "[117/150]: Training Loss: 0.6826438423322172, Training Accuracy: 79.792\n",
            "Validation Loss: 1.923887985944748, Validation Accuracy: 55.49\n",
            "[118/150]: Training Loss: 0.6681522337757811, Training Accuracy: 80.242\n",
            "Validation Loss: 1.940861666202545, Validation Accuracy: 55.59\n",
            "[119/150]: Training Loss: 0.6691557758924912, Training Accuracy: 80.146\n",
            "Validation Loss: 1.953080016374588, Validation Accuracy: 55.29\n",
            "[120/150]: Training Loss: 0.6531465272514188, Training Accuracy: 80.714\n",
            "Validation Loss: 1.9404853343963624, Validation Accuracy: 55.54\n",
            "[121/150]: Training Loss: 0.6419856098233437, Training Accuracy: 81.21\n",
            "Validation Loss: 1.9526274442672729, Validation Accuracy: 56.17\n",
            "[122/150]: Training Loss: 0.6383003540793244, Training Accuracy: 81.27\n",
            "Validation Loss: 1.9685742020606996, Validation Accuracy: 55.67\n",
            "[123/150]: Training Loss: 0.6263646182357049, Training Accuracy: 81.396\n",
            "Validation Loss: 1.9449464201927185, Validation Accuracy: 56.05\n",
            "[124/150]: Training Loss: 0.62737323982375, Training Accuracy: 81.438\n",
            "Validation Loss: 1.9453241765499114, Validation Accuracy: 56.28\n",
            "[125/150]: Training Loss: 0.6171576502371807, Training Accuracy: 81.866\n",
            "Validation Loss: 1.9553956925868987, Validation Accuracy: 55.63\n",
            "[126/150]: Training Loss: 0.6041063827519514, Training Accuracy: 82.16\n",
            "Validation Loss: 1.9582968175411224, Validation Accuracy: 56.15\n",
            "[127/150]: Training Loss: 0.6006453934372687, Training Accuracy: 82.38\n",
            "Validation Loss: 1.9529106080532075, Validation Accuracy: 56.03\n",
            "[128/150]: Training Loss: 0.6008156434613832, Training Accuracy: 82.348\n",
            "Validation Loss: 1.9448323190212249, Validation Accuracy: 56.3\n",
            "[129/150]: Training Loss: 0.5887871582289131, Training Accuracy: 82.762\n",
            "Validation Loss: 1.9513534665107728, Validation Accuracy: 56.25\n",
            "[130/150]: Training Loss: 0.5888028677020755, Training Accuracy: 82.86\n",
            "Validation Loss: 1.9529175937175751, Validation Accuracy: 56.18\n",
            "[131/150]: Training Loss: 0.5761450562550097, Training Accuracy: 83.106\n",
            "Validation Loss: 1.9645096361637115, Validation Accuracy: 56.02\n",
            "[132/150]: Training Loss: 0.5727416809116092, Training Accuracy: 83.094\n",
            "Validation Loss: 1.963749361038208, Validation Accuracy: 56.38\n",
            "[133/150]: Training Loss: 0.5652356436666177, Training Accuracy: 83.672\n",
            "Validation Loss: 1.9716902256011963, Validation Accuracy: 56.28\n",
            "[134/150]: Training Loss: 0.5695076165150623, Training Accuracy: 83.448\n",
            "Validation Loss: 1.964329320192337, Validation Accuracy: 55.99\n",
            "[135/150]: Training Loss: 0.5608592775403237, Training Accuracy: 83.678\n",
            "Validation Loss: 1.9603359639644622, Validation Accuracy: 56.25\n",
            "[136/150]: Training Loss: 0.5542723040799705, Training Accuracy: 83.854\n",
            "Validation Loss: 1.9673468470573425, Validation Accuracy: 56.04\n",
            "[137/150]: Training Loss: 0.5495593383604166, Training Accuracy: 84.128\n",
            "Validation Loss: 1.9731853008270264, Validation Accuracy: 56.19\n",
            "[138/150]: Training Loss: 0.5444910410715609, Training Accuracy: 84.204\n",
            "Validation Loss: 1.9700454473495483, Validation Accuracy: 56.12\n",
            "[139/150]: Training Loss: 0.5433347140039716, Training Accuracy: 84.306\n",
            "Validation Loss: 1.9688788115978242, Validation Accuracy: 56.13\n",
            "[140/150]: Training Loss: 0.5362506448006144, Training Accuracy: 84.586\n",
            "Validation Loss: 1.966701751947403, Validation Accuracy: 56.24\n",
            "[141/150]: Training Loss: 0.5390761190531205, Training Accuracy: 84.338\n",
            "Validation Loss: 1.9651995241641997, Validation Accuracy: 56.21\n",
            "[142/150]: Training Loss: 0.5341207445884237, Training Accuracy: 84.41\n",
            "Validation Loss: 1.9675312757492065, Validation Accuracy: 56.34\n",
            "[143/150]: Training Loss: 0.5282508870776819, Training Accuracy: 84.872\n",
            "Validation Loss: 1.967752468585968, Validation Accuracy: 56.25\n",
            "[144/150]: Training Loss: 0.5355107656547001, Training Accuracy: 84.532\n",
            "Validation Loss: 1.967381328344345, Validation Accuracy: 56.19\n",
            "[145/150]: Training Loss: 0.5333734960580359, Training Accuracy: 84.51\n",
            "Validation Loss: 1.969217723608017, Validation Accuracy: 56.19\n",
            "[146/150]: Training Loss: 0.5298109002867524, Training Accuracy: 84.75\n",
            "Validation Loss: 1.9694741308689117, Validation Accuracy: 56.12\n",
            "[147/150]: Training Loss: 0.529324583253082, Training Accuracy: 84.722\n",
            "Validation Loss: 1.9707287430763245, Validation Accuracy: 56.17\n",
            "[148/150]: Training Loss: 0.5251490242627203, Training Accuracy: 84.936\n",
            "Validation Loss: 1.9699740409851074, Validation Accuracy: 56.26\n",
            "[149/150]: Training Loss: 0.5242341507454308, Training Accuracy: 84.942\n",
            "Validation Loss: 1.97024707198143, Validation Accuracy: 56.16\n",
            "[150/150]: Training Loss: 0.5341996495821038, Training Accuracy: 84.57\n",
            "Validation Loss: 1.9700913786888123, Validation Accuracy: 56.18\n",
            "**********************************************************************\n",
            "Test Loss: 1.9700913786888123, Test Accuracy: 56.18\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▆▁▅▄▅▃▆▄▃▃▅▅▅▅▅▆▆▆▆</td></tr><tr><td>Test Loss</td><td>▆▆█▄▃▃▄▂▂▃▂▃▃▂▃▃▁▂▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>56.18</td></tr><tr><td>Test Loss</td><td>1.97009</td></tr><tr><td>Train Accuracy</td><td>84.57</td></tr><tr><td>Train Loss</td><td>0.5342</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=512 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_012749-momu4e5u/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 1024, Learning rate: 6.0, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_014707-w80ujlhs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs' target=\"_blank\">batch_size=1024 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.271560571631607, Training Accuracy: 4.982\n",
            "Validation Loss: 3.9391962051391602, Validation Accuracy: 8.84\n",
            "[2/150]: Training Loss: 3.82728639914065, Training Accuracy: 10.948\n",
            "Validation Loss: 3.6827336311340333, Validation Accuracy: 13.37\n",
            "[3/150]: Training Loss: 3.6269987845907408, Training Accuracy: 14.39\n",
            "Validation Loss: 3.4175909042358397, Validation Accuracy: 18.27\n",
            "[4/150]: Training Loss: 3.4358463238696664, Training Accuracy: 17.696\n",
            "Validation Loss: 3.2815504550933836, Validation Accuracy: 20.38\n",
            "[5/150]: Training Loss: 3.3039202203555984, Training Accuracy: 19.798\n",
            "Validation Loss: 3.171510195732117, Validation Accuracy: 23.09\n",
            "[6/150]: Training Loss: 3.1788000671231016, Training Accuracy: 22.106\n",
            "Validation Loss: 3.083156633377075, Validation Accuracy: 24.25\n",
            "[7/150]: Training Loss: 3.0781905553778826, Training Accuracy: 24.04\n",
            "Validation Loss: 2.9470993995666506, Validation Accuracy: 26.99\n",
            "[8/150]: Training Loss: 2.9905799846259917, Training Accuracy: 25.462\n",
            "Validation Loss: 2.8303237915039063, Validation Accuracy: 29.04\n",
            "[9/150]: Training Loss: 2.8789747199233697, Training Accuracy: 28.024\n",
            "Validation Loss: 2.7251497507095337, Validation Accuracy: 31.63\n",
            "[10/150]: Training Loss: 2.752841701312941, Training Accuracy: 30.378\n",
            "Validation Loss: 2.6787135124206545, Validation Accuracy: 32.86\n",
            "[11/150]: Training Loss: 2.664300650966411, Training Accuracy: 32.018\n",
            "Validation Loss: 2.5810091733932494, Validation Accuracy: 34.34\n",
            "[12/150]: Training Loss: 2.6055227445096385, Training Accuracy: 33.34\n",
            "Validation Loss: 2.511804437637329, Validation Accuracy: 35.24\n",
            "[13/150]: Training Loss: 2.569062106463374, Training Accuracy: 34.12\n",
            "Validation Loss: 2.452015995979309, Validation Accuracy: 36.63\n",
            "[14/150]: Training Loss: 2.4965496793085213, Training Accuracy: 35.586\n",
            "Validation Loss: 2.353745174407959, Validation Accuracy: 38.35\n",
            "[15/150]: Training Loss: 2.4792360870205625, Training Accuracy: 35.788\n",
            "Validation Loss: 2.362164545059204, Validation Accuracy: 38.51\n",
            "[16/150]: Training Loss: 2.406777240792099, Training Accuracy: 37.498\n",
            "Validation Loss: 2.322283720970154, Validation Accuracy: 39.52\n",
            "[17/150]: Training Loss: 2.3524391651153564, Training Accuracy: 38.372\n",
            "Validation Loss: 2.2932183027267454, Validation Accuracy: 40.0\n",
            "[18/150]: Training Loss: 2.3162564647441006, Training Accuracy: 39.304\n",
            "Validation Loss: 2.390507221221924, Validation Accuracy: 38.54\n",
            "[19/150]: Training Loss: 2.3010421821049283, Training Accuracy: 39.48\n",
            "Validation Loss: 2.2039053201675416, Validation Accuracy: 42.83\n",
            "[20/150]: Training Loss: 2.2905601968570632, Training Accuracy: 40.014\n",
            "Validation Loss: 2.327000045776367, Validation Accuracy: 39.01\n",
            "[21/150]: Training Loss: 2.2232315151058897, Training Accuracy: 41.472\n",
            "Validation Loss: 2.1848330736160277, Validation Accuracy: 42.37\n",
            "[22/150]: Training Loss: 2.175964046497734, Training Accuracy: 42.316\n",
            "Validation Loss: 2.192854475975037, Validation Accuracy: 42.34\n",
            "[23/150]: Training Loss: 2.12663603315548, Training Accuracy: 43.39\n",
            "Validation Loss: 2.184653973579407, Validation Accuracy: 42.91\n",
            "[24/150]: Training Loss: 2.1151150294712613, Training Accuracy: 43.664\n",
            "Validation Loss: 2.1352186679840086, Validation Accuracy: 43.8\n",
            "[25/150]: Training Loss: 2.0990794507824644, Training Accuracy: 44.256\n",
            "Validation Loss: 2.088001215457916, Validation Accuracy: 44.67\n",
            "[26/150]: Training Loss: 2.0608093349301084, Training Accuracy: 44.852\n",
            "Validation Loss: 2.1832612276077272, Validation Accuracy: 42.92\n",
            "[27/150]: Training Loss: 2.0558675892499028, Training Accuracy: 45.23\n",
            "Validation Loss: 2.176005721092224, Validation Accuracy: 43.08\n",
            "[28/150]: Training Loss: 2.0222370770512796, Training Accuracy: 45.868\n",
            "Validation Loss: 2.1084380388259887, Validation Accuracy: 44.53\n",
            "[29/150]: Training Loss: 1.9681626996215509, Training Accuracy: 47.006\n",
            "Validation Loss: 2.07066353559494, Validation Accuracy: 45.38\n",
            "[30/150]: Training Loss: 1.9436734793137531, Training Accuracy: 47.268\n",
            "Validation Loss: 2.0732940912246702, Validation Accuracy: 45.56\n",
            "[31/150]: Training Loss: 1.9339864083698817, Training Accuracy: 48.042\n",
            "Validation Loss: 2.095534420013428, Validation Accuracy: 45.23\n",
            "[32/150]: Training Loss: 1.9033171717001467, Training Accuracy: 48.32\n",
            "Validation Loss: 2.111021065711975, Validation Accuracy: 45.35\n",
            "[33/150]: Training Loss: 1.904011852887212, Training Accuracy: 48.414\n",
            "Validation Loss: 2.050978398323059, Validation Accuracy: 46.52\n",
            "[34/150]: Training Loss: 1.8750450124545974, Training Accuracy: 48.976\n",
            "Validation Loss: 2.080773401260376, Validation Accuracy: 45.2\n",
            "[35/150]: Training Loss: 1.8805813740710824, Training Accuracy: 48.968\n",
            "Validation Loss: 2.036470913887024, Validation Accuracy: 46.34\n",
            "[36/150]: Training Loss: 1.851276424466347, Training Accuracy: 49.646\n",
            "Validation Loss: 2.1005828619003295, Validation Accuracy: 45.2\n",
            "[37/150]: Training Loss: 1.8493371909978438, Training Accuracy: 49.736\n",
            "Validation Loss: 2.003628730773926, Validation Accuracy: 46.62\n",
            "[38/150]: Training Loss: 1.7895233460835047, Training Accuracy: 50.876\n",
            "Validation Loss: 1.9916603326797486, Validation Accuracy: 47.91\n",
            "[39/150]: Training Loss: 1.783764887829216, Training Accuracy: 51.294\n",
            "Validation Loss: 2.066832160949707, Validation Accuracy: 45.61\n",
            "[40/150]: Training Loss: 1.7760933297021049, Training Accuracy: 51.504\n",
            "Validation Loss: 1.9901643991470337, Validation Accuracy: 47.5\n",
            "[41/150]: Training Loss: 1.7614454274274864, Training Accuracy: 51.798\n",
            "Validation Loss: 2.0182290196418764, Validation Accuracy: 47.1\n",
            "[42/150]: Training Loss: 1.7892101711156416, Training Accuracy: 51.024\n",
            "Validation Loss: 1.9716030478477478, Validation Accuracy: 48.76\n",
            "[43/150]: Training Loss: 1.7074257597631337, Training Accuracy: 52.916\n",
            "Validation Loss: 1.9718806385993957, Validation Accuracy: 48.61\n",
            "[44/150]: Training Loss: 1.714418238523055, Training Accuracy: 52.714\n",
            "Validation Loss: 2.0062466263771057, Validation Accuracy: 48.27\n",
            "[45/150]: Training Loss: 1.6869578483153362, Training Accuracy: 53.334\n",
            "Validation Loss: 1.95201518535614, Validation Accuracy: 49.06\n",
            "[46/150]: Training Loss: 1.6628945092765652, Training Accuracy: 53.922\n",
            "Validation Loss: 2.012867844104767, Validation Accuracy: 47.15\n",
            "[47/150]: Training Loss: 1.6412470778640436, Training Accuracy: 54.254\n",
            "Validation Loss: 1.923640739917755, Validation Accuracy: 49.27\n",
            "[48/150]: Training Loss: 1.6261077705694704, Training Accuracy: 55.032\n",
            "Validation Loss: 1.9601864099502564, Validation Accuracy: 48.98\n",
            "[49/150]: Training Loss: 1.6493362480280351, Training Accuracy: 54.152\n",
            "Validation Loss: 1.9593440890312195, Validation Accuracy: 48.65\n",
            "[50/150]: Training Loss: 1.612999371119908, Training Accuracy: 55.134\n",
            "Validation Loss: 1.9344399094581604, Validation Accuracy: 49.38\n",
            "[51/150]: Training Loss: 1.5939155281806479, Training Accuracy: 55.666\n",
            "Validation Loss: 1.9557547926902772, Validation Accuracy: 48.7\n",
            "[52/150]: Training Loss: 1.6124721181635955, Training Accuracy: 55.024\n",
            "Validation Loss: 1.9627613067626952, Validation Accuracy: 48.75\n",
            "[53/150]: Training Loss: 1.5704505808499394, Training Accuracy: 56.09\n",
            "Validation Loss: 1.9339674592018128, Validation Accuracy: 49.77\n",
            "[54/150]: Training Loss: 1.556705151285444, Training Accuracy: 56.304\n",
            "Validation Loss: 1.910308563709259, Validation Accuracy: 50.74\n",
            "[55/150]: Training Loss: 1.5271518887305746, Training Accuracy: 57.15\n",
            "Validation Loss: 1.8914035081863403, Validation Accuracy: 50.77\n",
            "[56/150]: Training Loss: 1.5157563102488616, Training Accuracy: 57.53\n",
            "Validation Loss: 1.8953699111938476, Validation Accuracy: 50.82\n",
            "[57/150]: Training Loss: 1.4868425179500968, Training Accuracy: 58.284\n",
            "Validation Loss: 1.960522997379303, Validation Accuracy: 49.2\n",
            "[58/150]: Training Loss: 1.5009924586938352, Training Accuracy: 57.544\n",
            "Validation Loss: 1.9257111549377441, Validation Accuracy: 50.88\n",
            "[59/150]: Training Loss: 1.4776659741693614, Training Accuracy: 58.214\n",
            "Validation Loss: 1.9358840227127074, Validation Accuracy: 50.04\n",
            "[60/150]: Training Loss: 1.4657867806298392, Training Accuracy: 58.49\n",
            "Validation Loss: 2.0199026823043824, Validation Accuracy: 48.59\n",
            "[61/150]: Training Loss: 1.4666189709488227, Training Accuracy: 58.868\n",
            "Validation Loss: 1.9441120982170106, Validation Accuracy: 49.79\n",
            "[62/150]: Training Loss: 1.4370595435706937, Training Accuracy: 59.142\n",
            "Validation Loss: 1.8950949430465698, Validation Accuracy: 51.17\n",
            "[63/150]: Training Loss: 1.4119965835493438, Training Accuracy: 59.966\n",
            "Validation Loss: 1.946419107913971, Validation Accuracy: 49.96\n",
            "[64/150]: Training Loss: 1.3992952692265412, Training Accuracy: 60.37\n",
            "Validation Loss: 1.929420042037964, Validation Accuracy: 50.74\n",
            "[65/150]: Training Loss: 1.3843509275086072, Training Accuracy: 60.93\n",
            "Validation Loss: 1.8808708190917969, Validation Accuracy: 51.52\n",
            "[66/150]: Training Loss: 1.3892741276293386, Training Accuracy: 60.538\n",
            "Validation Loss: 1.9178170323371888, Validation Accuracy: 51.05\n",
            "[67/150]: Training Loss: 1.3926233807388617, Training Accuracy: 60.324\n",
            "Validation Loss: 1.879079854488373, Validation Accuracy: 52.14\n",
            "[68/150]: Training Loss: 1.3284603332986638, Training Accuracy: 62.108\n",
            "Validation Loss: 1.8741150736808776, Validation Accuracy: 51.87\n",
            "[69/150]: Training Loss: 1.3297611377677139, Training Accuracy: 62.144\n",
            "Validation Loss: 1.8850895166397095, Validation Accuracy: 51.78\n",
            "[70/150]: Training Loss: 1.3190836857776254, Training Accuracy: 62.086\n",
            "Validation Loss: 1.8964969992637635, Validation Accuracy: 51.72\n",
            "[71/150]: Training Loss: 1.297834805079869, Training Accuracy: 63.002\n",
            "Validation Loss: 1.8922056078910827, Validation Accuracy: 51.94\n",
            "[72/150]: Training Loss: 1.3022558105235198, Training Accuracy: 62.866\n",
            "Validation Loss: 1.8891796469688416, Validation Accuracy: 52.07\n",
            "[73/150]: Training Loss: 1.2681196271156778, Training Accuracy: 63.622\n",
            "Validation Loss: 1.9051270723342895, Validation Accuracy: 52.41\n",
            "[74/150]: Training Loss: 1.2562547800492267, Training Accuracy: 64.092\n",
            "Validation Loss: 1.8912514567375183, Validation Accuracy: 52.28\n",
            "[75/150]: Training Loss: 1.2469108761573324, Training Accuracy: 64.088\n",
            "Validation Loss: 1.9140023827552795, Validation Accuracy: 52.11\n",
            "[76/150]: Training Loss: 1.2424099080416622, Training Accuracy: 64.206\n",
            "Validation Loss: 1.896690595149994, Validation Accuracy: 52.53\n",
            "[77/150]: Training Loss: 1.2172678052162638, Training Accuracy: 64.804\n",
            "Validation Loss: 1.8944836139678956, Validation Accuracy: 52.54\n",
            "[78/150]: Training Loss: 1.198543380717842, Training Accuracy: 65.358\n",
            "Validation Loss: 1.8942208647727967, Validation Accuracy: 52.32\n",
            "[79/150]: Training Loss: 1.198562699921277, Training Accuracy: 65.602\n",
            "Validation Loss: 1.8823209404945374, Validation Accuracy: 52.42\n",
            "[80/150]: Training Loss: 1.1809428224758225, Training Accuracy: 66.04\n",
            "Validation Loss: 1.8809239506721496, Validation Accuracy: 52.57\n",
            "[81/150]: Training Loss: 1.1573098119424314, Training Accuracy: 66.476\n",
            "Validation Loss: 1.904803991317749, Validation Accuracy: 52.64\n",
            "[82/150]: Training Loss: 1.1748385405053898, Training Accuracy: 65.876\n",
            "Validation Loss: 1.8975732803344727, Validation Accuracy: 52.68\n",
            "[83/150]: Training Loss: 1.1381618465696062, Training Accuracy: 67.06\n",
            "Validation Loss: 1.87950758934021, Validation Accuracy: 53.8\n",
            "[84/150]: Training Loss: 1.1169312851769584, Training Accuracy: 67.614\n",
            "Validation Loss: 1.9594017744064331, Validation Accuracy: 51.52\n",
            "[85/150]: Training Loss: 1.1326112601221825, Training Accuracy: 67.212\n",
            "Validation Loss: 1.9392709732055664, Validation Accuracy: 52.51\n",
            "[86/150]: Training Loss: 1.1153452299079116, Training Accuracy: 67.558\n",
            "Validation Loss: 1.8824020862579345, Validation Accuracy: 53.05\n",
            "[87/150]: Training Loss: 1.075961629955136, Training Accuracy: 68.556\n",
            "Validation Loss: 1.8995132923126221, Validation Accuracy: 53.22\n",
            "[88/150]: Training Loss: 1.0561599804430593, Training Accuracy: 69.222\n",
            "Validation Loss: 1.8934579133987426, Validation Accuracy: 53.25\n",
            "[89/150]: Training Loss: 1.0379744488365796, Training Accuracy: 69.548\n",
            "Validation Loss: 1.8907739281654359, Validation Accuracy: 54.05\n",
            "[90/150]: Training Loss: 1.0645462481343015, Training Accuracy: 69.002\n",
            "Validation Loss: 1.9286641478538513, Validation Accuracy: 52.38\n",
            "[91/150]: Training Loss: 1.0510404596523362, Training Accuracy: 69.392\n",
            "Validation Loss: 1.897424077987671, Validation Accuracy: 53.61\n",
            "[92/150]: Training Loss: 1.017185703832276, Training Accuracy: 70.506\n",
            "Validation Loss: 1.9066467523574828, Validation Accuracy: 53.56\n",
            "[93/150]: Training Loss: 0.9912244945156331, Training Accuracy: 70.89\n",
            "Validation Loss: 1.9277787804603577, Validation Accuracy: 53.53\n",
            "[94/150]: Training Loss: 1.0054571652898983, Training Accuracy: 70.53\n",
            "Validation Loss: 1.8809196829795838, Validation Accuracy: 54.1\n",
            "[95/150]: Training Loss: 0.9765667416611497, Training Accuracy: 71.32\n",
            "Validation Loss: 1.8944197297096252, Validation Accuracy: 53.48\n",
            "[96/150]: Training Loss: 0.9578249174721387, Training Accuracy: 71.926\n",
            "Validation Loss: 1.942376160621643, Validation Accuracy: 53.53\n",
            "[97/150]: Training Loss: 0.9575933753227701, Training Accuracy: 71.676\n",
            "Validation Loss: 1.8917027354240417, Validation Accuracy: 53.65\n",
            "[98/150]: Training Loss: 0.944321874453097, Training Accuracy: 72.258\n",
            "Validation Loss: 1.8984474778175353, Validation Accuracy: 54.22\n",
            "[99/150]: Training Loss: 0.9205025398001379, Training Accuracy: 72.942\n",
            "Validation Loss: 1.9325331091880797, Validation Accuracy: 54.27\n",
            "[100/150]: Training Loss: 0.9185542439927861, Training Accuracy: 72.924\n",
            "Validation Loss: 1.9163445711135865, Validation Accuracy: 54.26\n",
            "[101/150]: Training Loss: 0.8990846799344433, Training Accuracy: 73.314\n",
            "Validation Loss: 1.9365061402320862, Validation Accuracy: 54.02\n",
            "[102/150]: Training Loss: 0.8797871519108208, Training Accuracy: 74.004\n",
            "Validation Loss: 1.937075173854828, Validation Accuracy: 54.45\n",
            "[103/150]: Training Loss: 0.8798652291297913, Training Accuracy: 74.044\n",
            "Validation Loss: 1.9165674328804017, Validation Accuracy: 54.24\n",
            "[104/150]: Training Loss: 0.8599669252123151, Training Accuracy: 74.54\n",
            "Validation Loss: 1.9492801547050476, Validation Accuracy: 53.91\n",
            "[105/150]: Training Loss: 0.8473539729507602, Training Accuracy: 74.856\n",
            "Validation Loss: 1.9327858686447144, Validation Accuracy: 53.8\n",
            "[106/150]: Training Loss: 0.8454914895855651, Training Accuracy: 74.948\n",
            "Validation Loss: 1.918694531917572, Validation Accuracy: 54.58\n",
            "[107/150]: Training Loss: 0.8351617601453042, Training Accuracy: 75.38\n",
            "Validation Loss: 1.925265645980835, Validation Accuracy: 54.65\n",
            "[108/150]: Training Loss: 0.8257343720416633, Training Accuracy: 75.846\n",
            "Validation Loss: 1.9380712270736695, Validation Accuracy: 54.61\n",
            "[109/150]: Training Loss: 0.803754199524315, Training Accuracy: 76.216\n",
            "Validation Loss: 1.952760434150696, Validation Accuracy: 54.15\n",
            "[110/150]: Training Loss: 0.8017950021490758, Training Accuracy: 76.334\n",
            "Validation Loss: 1.938999843597412, Validation Accuracy: 54.87\n",
            "[111/150]: Training Loss: 0.7898118264821111, Training Accuracy: 76.776\n",
            "Validation Loss: 1.9549705505371093, Validation Accuracy: 54.67\n",
            "[112/150]: Training Loss: 0.7774463660862981, Training Accuracy: 76.952\n",
            "Validation Loss: 1.9725535273551942, Validation Accuracy: 54.17\n",
            "[113/150]: Training Loss: 0.7767588861134588, Training Accuracy: 76.824\n",
            "Validation Loss: 1.9652800679206848, Validation Accuracy: 54.49\n",
            "[114/150]: Training Loss: 0.7543022997525274, Training Accuracy: 77.61\n",
            "Validation Loss: 1.9479739904403686, Validation Accuracy: 54.85\n",
            "[115/150]: Training Loss: 0.7407014296979321, Training Accuracy: 78.064\n",
            "Validation Loss: 1.974105668067932, Validation Accuracy: 54.73\n",
            "[116/150]: Training Loss: 0.7379214763641357, Training Accuracy: 78.122\n",
            "Validation Loss: 1.9709696292877197, Validation Accuracy: 54.92\n",
            "[117/150]: Training Loss: 0.7280089295640284, Training Accuracy: 78.332\n",
            "Validation Loss: 1.9550812244415283, Validation Accuracy: 54.87\n",
            "[118/150]: Training Loss: 0.719780373330019, Training Accuracy: 78.916\n",
            "Validation Loss: 1.9715718150138855, Validation Accuracy: 54.88\n",
            "[119/150]: Training Loss: 0.7140980338563725, Training Accuracy: 78.938\n",
            "Validation Loss: 1.9744232773780823, Validation Accuracy: 54.88\n",
            "[120/150]: Training Loss: 0.7100381559255172, Training Accuracy: 79.112\n",
            "Validation Loss: 1.9600295305252076, Validation Accuracy: 55.01\n",
            "[121/150]: Training Loss: 0.695380872609664, Training Accuracy: 79.644\n",
            "Validation Loss: 1.9685936331748963, Validation Accuracy: 55.25\n",
            "[122/150]: Training Loss: 0.693044870483632, Training Accuracy: 79.566\n",
            "Validation Loss: 1.973974621295929, Validation Accuracy: 54.99\n",
            "[123/150]: Training Loss: 0.685412128360904, Training Accuracy: 79.762\n",
            "Validation Loss: 1.981511080265045, Validation Accuracy: 55.03\n",
            "[124/150]: Training Loss: 0.6729757055944326, Training Accuracy: 80.388\n",
            "Validation Loss: 1.9723920106887818, Validation Accuracy: 54.88\n",
            "[125/150]: Training Loss: 0.6728029567368177, Training Accuracy: 80.238\n",
            "Validation Loss: 1.972815704345703, Validation Accuracy: 55.06\n",
            "[126/150]: Training Loss: 0.6577607624384821, Training Accuracy: 80.906\n",
            "Validation Loss: 1.9941662311553956, Validation Accuracy: 55.01\n",
            "[127/150]: Training Loss: 0.6531734880135984, Training Accuracy: 80.798\n",
            "Validation Loss: 1.9830293536186219, Validation Accuracy: 55.29\n",
            "[128/150]: Training Loss: 0.6451807484334829, Training Accuracy: 80.998\n",
            "Validation Loss: 1.98456552028656, Validation Accuracy: 55.25\n",
            "[129/150]: Training Loss: 0.6361871045462939, Training Accuracy: 81.438\n",
            "Validation Loss: 1.991374099254608, Validation Accuracy: 55.33\n",
            "[130/150]: Training Loss: 0.6329771852006718, Training Accuracy: 81.326\n",
            "Validation Loss: 1.982979428768158, Validation Accuracy: 55.15\n",
            "[131/150]: Training Loss: 0.6337368731596031, Training Accuracy: 81.498\n",
            "Validation Loss: 1.9814332127571106, Validation Accuracy: 55.25\n",
            "[132/150]: Training Loss: 0.6237865613431347, Training Accuracy: 81.828\n",
            "Validation Loss: 1.9956311464309693, Validation Accuracy: 55.37\n",
            "[133/150]: Training Loss: 0.6184223087466493, Training Accuracy: 81.804\n",
            "Validation Loss: 1.997809612751007, Validation Accuracy: 55.31\n",
            "[134/150]: Training Loss: 0.6162658759525844, Training Accuracy: 82.05\n",
            "Validation Loss: 1.9944164514541627, Validation Accuracy: 55.37\n",
            "[135/150]: Training Loss: 0.6049274242654139, Training Accuracy: 82.308\n",
            "Validation Loss: 2.0007421493530275, Validation Accuracy: 55.18\n",
            "[136/150]: Training Loss: 0.6065428415123297, Training Accuracy: 82.346\n",
            "Validation Loss: 2.002026319503784, Validation Accuracy: 55.35\n",
            "[137/150]: Training Loss: 0.6054561515243686, Training Accuracy: 82.446\n",
            "Validation Loss: 2.0026141166687013, Validation Accuracy: 55.51\n",
            "[138/150]: Training Loss: 0.6016628170499996, Training Accuracy: 82.674\n",
            "Validation Loss: 1.9972286820411682, Validation Accuracy: 55.54\n",
            "[139/150]: Training Loss: 0.599141927397981, Training Accuracy: 82.822\n",
            "Validation Loss: 1.9984585762023925, Validation Accuracy: 55.21\n",
            "[140/150]: Training Loss: 0.5853999877462581, Training Accuracy: 82.924\n",
            "Validation Loss: 2.0003657698631288, Validation Accuracy: 55.39\n",
            "[141/150]: Training Loss: 0.5930902678139356, Training Accuracy: 82.696\n",
            "Validation Loss: 1.9994045495986938, Validation Accuracy: 55.54\n",
            "[142/150]: Training Loss: 0.5897825044028613, Training Accuracy: 83.098\n",
            "Validation Loss: 2.0004444122314453, Validation Accuracy: 55.4\n",
            "[143/150]: Training Loss: 0.5907509253949536, Training Accuracy: 82.836\n",
            "Validation Loss: 1.9981922507286072, Validation Accuracy: 55.3\n",
            "[144/150]: Training Loss: 0.5838782507546094, Training Accuracy: 83.046\n",
            "Validation Loss: 1.9986275434494019, Validation Accuracy: 55.59\n",
            "[145/150]: Training Loss: 0.5819402069461589, Training Accuracy: 83.02\n",
            "Validation Loss: 2.0001697182655334, Validation Accuracy: 55.45\n",
            "[146/150]: Training Loss: 0.5872244786243049, Training Accuracy: 82.898\n",
            "Validation Loss: 2.000009226799011, Validation Accuracy: 55.41\n",
            "[147/150]: Training Loss: 0.5821643848808444, Training Accuracy: 83.186\n",
            "Validation Loss: 2.0006944298744203, Validation Accuracy: 55.35\n",
            "[148/150]: Training Loss: 0.5849178761852031, Training Accuracy: 83.14\n",
            "Validation Loss: 2.000812566280365, Validation Accuracy: 55.33\n",
            "[149/150]: Training Loss: 0.5800890168365167, Training Accuracy: 83.304\n",
            "Validation Loss: 2.000700604915619, Validation Accuracy: 55.4\n",
            "[150/150]: Training Loss: 0.5826626444349483, Training Accuracy: 83.206\n",
            "Validation Loss: 2.000621163845062, Validation Accuracy: 55.35\n",
            "**********************************************************************\n",
            "Test Loss: 2.000621163845062, Test Accuracy: 55.35\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▁▁▃▂▂▅▄▃▄</td></tr><tr><td>Test Loss</td><td>█▂▂▁▄▄▃▄▃▂</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>55.35</td></tr><tr><td>Test Loss</td><td>2.00062</td></tr><tr><td>Train Accuracy</td><td>83.206</td></tr><tr><td>Train Loss</td><td>0.58266</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=1024 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_014707-w80ujlhs/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 2048, Learning rate: 8.485281374238571, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_022321-j8jszxxs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs' target=\"_blank\">batch_size=2048 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.36701738357544, Training Accuracy: 3.886\n",
            "Validation Loss: 4.163058471679688, Validation Accuracy: 5.67\n",
            "[2/150]: Training Loss: 4.113679485321045, Training Accuracy: 6.492\n",
            "Validation Loss: 3.9081267356872558, Validation Accuracy: 9.58\n",
            "[3/150]: Training Loss: 3.8604202461242676, Training Accuracy: 10.016\n",
            "Validation Loss: 3.6436347484588625, Validation Accuracy: 14.06\n",
            "[4/150]: Training Loss: 3.7227595615386964, Training Accuracy: 12.524\n",
            "Validation Loss: 3.58444561958313, Validation Accuracy: 15.49\n",
            "[5/150]: Training Loss: 3.5834127140045164, Training Accuracy: 14.702\n",
            "Validation Loss: 3.4575596332550047, Validation Accuracy: 17.38\n",
            "[6/150]: Training Loss: 3.4569161987304686, Training Accuracy: 16.904\n",
            "Validation Loss: 3.262946367263794, Validation Accuracy: 20.16\n",
            "[7/150]: Training Loss: 3.3158952045440673, Training Accuracy: 19.406\n",
            "Validation Loss: 3.2002731800079345, Validation Accuracy: 22.26\n",
            "[8/150]: Training Loss: 3.1832161426544188, Training Accuracy: 22.068\n",
            "Validation Loss: 3.0290011405944823, Validation Accuracy: 24.81\n",
            "[9/150]: Training Loss: 3.185646390914917, Training Accuracy: 21.936\n",
            "Validation Loss: 3.1018139362335204, Validation Accuracy: 24.11\n",
            "[10/150]: Training Loss: 3.0637048053741456, Training Accuracy: 23.926\n",
            "Validation Loss: 2.9448835372924806, Validation Accuracy: 26.91\n",
            "[11/150]: Training Loss: 2.951322078704834, Training Accuracy: 26.03\n",
            "Validation Loss: 2.8756459236145018, Validation Accuracy: 27.94\n",
            "[12/150]: Training Loss: 2.9279331874847414, Training Accuracy: 26.79\n",
            "Validation Loss: 2.9318973541259767, Validation Accuracy: 27.51\n",
            "[13/150]: Training Loss: 2.828905839920044, Training Accuracy: 28.722\n",
            "Validation Loss: 2.634994125366211, Validation Accuracy: 33.35\n",
            "[14/150]: Training Loss: 2.8078646850585938, Training Accuracy: 29.266\n",
            "Validation Loss: 2.7456551074981688, Validation Accuracy: 30.56\n",
            "[15/150]: Training Loss: 2.712659044265747, Training Accuracy: 30.836\n",
            "Validation Loss: 2.5297746658325195, Validation Accuracy: 34.77\n",
            "[16/150]: Training Loss: 2.651611213684082, Training Accuracy: 32.136\n",
            "Validation Loss: 2.547566270828247, Validation Accuracy: 34.69\n",
            "[17/150]: Training Loss: 2.599648675918579, Training Accuracy: 33.174\n",
            "Validation Loss: 2.5292351245880127, Validation Accuracy: 34.72\n",
            "[18/150]: Training Loss: 2.577428216934204, Training Accuracy: 33.64\n",
            "Validation Loss: 2.5313771247863768, Validation Accuracy: 35.11\n",
            "[19/150]: Training Loss: 2.575561113357544, Training Accuracy: 33.86\n",
            "Validation Loss: 2.50340313911438, Validation Accuracy: 36.18\n",
            "[20/150]: Training Loss: 2.617272434234619, Training Accuracy: 32.958\n",
            "Validation Loss: 2.601680040359497, Validation Accuracy: 34.74\n",
            "[21/150]: Training Loss: 2.503620433807373, Training Accuracy: 35.3\n",
            "Validation Loss: 2.3930795192718506, Validation Accuracy: 38.69\n",
            "[22/150]: Training Loss: 2.4627874088287354, Training Accuracy: 36.662\n",
            "Validation Loss: 2.669213056564331, Validation Accuracy: 33.56\n",
            "[23/150]: Training Loss: 2.548478717803955, Training Accuracy: 34.642\n",
            "Validation Loss: 2.3424915313720702, Validation Accuracy: 39.2\n",
            "[24/150]: Training Loss: 2.3940266799926757, Training Accuracy: 37.736\n",
            "Validation Loss: 2.3067100048065186, Validation Accuracy: 39.53\n",
            "[25/150]: Training Loss: 2.3650291538238526, Training Accuracy: 38.102\n",
            "Validation Loss: 2.4807125091552735, Validation Accuracy: 36.63\n",
            "[26/150]: Training Loss: 2.3525609493255617, Training Accuracy: 38.382\n",
            "Validation Loss: 2.3692517280578613, Validation Accuracy: 39.11\n",
            "[27/150]: Training Loss: 2.2789812183380125, Training Accuracy: 40.074\n",
            "Validation Loss: 2.2349360942840577, Validation Accuracy: 41.75\n",
            "[28/150]: Training Loss: 2.2222459888458252, Training Accuracy: 40.97\n",
            "Validation Loss: 2.19801549911499, Validation Accuracy: 42.61\n",
            "[29/150]: Training Loss: 2.214301881790161, Training Accuracy: 41.694\n",
            "Validation Loss: 2.605668783187866, Validation Accuracy: 35.32\n",
            "[30/150]: Training Loss: 2.338454341888428, Training Accuracy: 38.786\n",
            "Validation Loss: 2.288765287399292, Validation Accuracy: 40.36\n",
            "[31/150]: Training Loss: 2.201904296875, Training Accuracy: 41.99\n",
            "Validation Loss: 2.205613946914673, Validation Accuracy: 42.56\n",
            "[32/150]: Training Loss: 2.1113436079025267, Training Accuracy: 43.934\n",
            "Validation Loss: 2.321266937255859, Validation Accuracy: 41.03\n",
            "[33/150]: Training Loss: 2.1466791677474975, Training Accuracy: 43.006\n",
            "Validation Loss: 2.1349957466125487, Validation Accuracy: 43.8\n",
            "[34/150]: Training Loss: 2.1290991640090944, Training Accuracy: 43.29\n",
            "Validation Loss: 2.217708683013916, Validation Accuracy: 42.79\n",
            "[35/150]: Training Loss: 2.0663461446762086, Training Accuracy: 44.68\n",
            "Validation Loss: 2.3302943229675295, Validation Accuracy: 40.26\n",
            "[36/150]: Training Loss: 2.0297911977767944, Training Accuracy: 45.408\n",
            "Validation Loss: 2.21708025932312, Validation Accuracy: 43.36\n",
            "[37/150]: Training Loss: 2.0129797410964967, Training Accuracy: 45.814\n",
            "Validation Loss: 2.0887409687042235, Validation Accuracy: 45.25\n",
            "[38/150]: Training Loss: 1.937804069519043, Training Accuracy: 47.634\n",
            "Validation Loss: 2.0723501682281493, Validation Accuracy: 45.71\n",
            "[39/150]: Training Loss: 1.943020739555359, Training Accuracy: 47.708\n",
            "Validation Loss: 2.0943463325500487, Validation Accuracy: 44.8\n",
            "[40/150]: Training Loss: 1.9138334608078003, Training Accuracy: 48.346\n",
            "Validation Loss: 2.2366223335266113, Validation Accuracy: 42.21\n",
            "[41/150]: Training Loss: 1.9987104320526123, Training Accuracy: 46.376\n",
            "Validation Loss: 2.12234206199646, Validation Accuracy: 45.35\n",
            "[42/150]: Training Loss: 1.9329777145385743, Training Accuracy: 47.608\n",
            "Validation Loss: 2.097510814666748, Validation Accuracy: 45.23\n",
            "[43/150]: Training Loss: 1.9182713079452514, Training Accuracy: 47.922\n",
            "Validation Loss: 2.085854196548462, Validation Accuracy: 45.54\n",
            "[44/150]: Training Loss: 1.900009379386902, Training Accuracy: 48.732\n",
            "Validation Loss: 2.051387619972229, Validation Accuracy: 46.24\n",
            "[45/150]: Training Loss: 1.8408135080337524, Training Accuracy: 49.714\n",
            "Validation Loss: 2.0023281574249268, Validation Accuracy: 47.57\n",
            "[46/150]: Training Loss: 1.785364203453064, Training Accuracy: 51.326\n",
            "Validation Loss: 2.049905252456665, Validation Accuracy: 46.49\n",
            "[47/150]: Training Loss: 1.8102792501449585, Training Accuracy: 50.858\n",
            "Validation Loss: 2.0309868812561036, Validation Accuracy: 46.64\n",
            "[48/150]: Training Loss: 1.7956236791610718, Training Accuracy: 50.794\n",
            "Validation Loss: 2.0011850118637087, Validation Accuracy: 47.81\n",
            "[49/150]: Training Loss: 1.7439770412445068, Training Accuracy: 52.0\n",
            "Validation Loss: 1.9770531415939332, Validation Accuracy: 48.07\n",
            "[50/150]: Training Loss: 1.7183953332901, Training Accuracy: 52.358\n",
            "Validation Loss: 2.0561673641204834, Validation Accuracy: 46.4\n",
            "[51/150]: Training Loss: 1.717338604927063, Training Accuracy: 53.112\n",
            "Validation Loss: 1.9954840183258056, Validation Accuracy: 47.55\n",
            "[52/150]: Training Loss: 1.7013448238372804, Training Accuracy: 53.098\n",
            "Validation Loss: 2.0024028778076173, Validation Accuracy: 47.72\n",
            "[53/150]: Training Loss: 1.7069044065475465, Training Accuracy: 53.0\n",
            "Validation Loss: 2.08711462020874, Validation Accuracy: 45.49\n",
            "[54/150]: Training Loss: 1.6761716079711915, Training Accuracy: 53.252\n",
            "Validation Loss: 1.9557607173919678, Validation Accuracy: 49.03\n",
            "[55/150]: Training Loss: 1.6440558958053588, Training Accuracy: 54.564\n",
            "Validation Loss: 1.9891844987869263, Validation Accuracy: 48.39\n",
            "[56/150]: Training Loss: 1.6161942625045775, Training Accuracy: 54.926\n",
            "Validation Loss: 2.029493975639343, Validation Accuracy: 47.58\n",
            "[57/150]: Training Loss: 1.6047872447967528, Training Accuracy: 55.192\n",
            "Validation Loss: 1.967372179031372, Validation Accuracy: 48.91\n",
            "[58/150]: Training Loss: 1.713731393814087, Training Accuracy: 52.926\n",
            "Validation Loss: 2.0652761220932008, Validation Accuracy: 46.68\n",
            "[59/150]: Training Loss: 1.6507894086837769, Training Accuracy: 54.226\n",
            "Validation Loss: 1.9929080486297608, Validation Accuracy: 48.66\n",
            "[60/150]: Training Loss: 1.6021452569961547, Training Accuracy: 55.432\n",
            "Validation Loss: 1.9558722257614136, Validation Accuracy: 49.28\n",
            "[61/150]: Training Loss: 1.543088173866272, Training Accuracy: 56.656\n",
            "Validation Loss: 1.9447553634643555, Validation Accuracy: 49.2\n",
            "[62/150]: Training Loss: 1.5494691371917724, Training Accuracy: 56.864\n",
            "Validation Loss: 1.885341501235962, Validation Accuracy: 50.76\n",
            "[63/150]: Training Loss: 1.4937063312530519, Training Accuracy: 58.158\n",
            "Validation Loss: 1.9500130414962769, Validation Accuracy: 49.94\n",
            "[64/150]: Training Loss: 1.4539641427993775, Training Accuracy: 59.004\n",
            "Validation Loss: 1.9285942554473876, Validation Accuracy: 49.98\n",
            "[65/150]: Training Loss: 1.4972302389144898, Training Accuracy: 57.904\n",
            "Validation Loss: 1.9630061149597169, Validation Accuracy: 49.26\n",
            "[66/150]: Training Loss: 1.518665623664856, Training Accuracy: 57.152\n",
            "Validation Loss: 1.9333840608596802, Validation Accuracy: 50.24\n",
            "[67/150]: Training Loss: 1.5036478281021117, Training Accuracy: 57.512\n",
            "Validation Loss: 1.961331272125244, Validation Accuracy: 50.09\n",
            "[68/150]: Training Loss: 1.4539181280136109, Training Accuracy: 59.142\n",
            "Validation Loss: 1.9611144304275512, Validation Accuracy: 49.25\n",
            "[69/150]: Training Loss: 1.46078604221344, Training Accuracy: 58.794\n",
            "Validation Loss: 1.916247057914734, Validation Accuracy: 50.02\n",
            "[70/150]: Training Loss: 1.4112844705581664, Training Accuracy: 59.934\n",
            "Validation Loss: 1.8900265216827392, Validation Accuracy: 51.2\n",
            "[71/150]: Training Loss: 1.3915088891983032, Training Accuracy: 60.376\n",
            "Validation Loss: 1.9043931245803833, Validation Accuracy: 50.87\n",
            "[72/150]: Training Loss: 1.3921622323989868, Training Accuracy: 60.564\n",
            "Validation Loss: 1.9693795204162599, Validation Accuracy: 50.0\n",
            "[73/150]: Training Loss: 1.3862884998321534, Training Accuracy: 60.752\n",
            "Validation Loss: 1.9339972734451294, Validation Accuracy: 50.94\n",
            "[74/150]: Training Loss: 1.358318910598755, Training Accuracy: 61.528\n",
            "Validation Loss: 1.9029748439788818, Validation Accuracy: 51.01\n",
            "[75/150]: Training Loss: 1.3378327751159669, Training Accuracy: 61.9\n",
            "Validation Loss: 1.9114508390426637, Validation Accuracy: 51.04\n",
            "[76/150]: Training Loss: 1.307257251739502, Training Accuracy: 62.684\n",
            "Validation Loss: 1.897865653038025, Validation Accuracy: 51.33\n",
            "[77/150]: Training Loss: 1.307571873664856, Training Accuracy: 62.566\n",
            "Validation Loss: 1.9430776834487915, Validation Accuracy: 50.79\n",
            "[78/150]: Training Loss: 1.2927326250076294, Training Accuracy: 62.904\n",
            "Validation Loss: 1.910500168800354, Validation Accuracy: 52.01\n",
            "[79/150]: Training Loss: 1.2737730932235718, Training Accuracy: 63.4\n",
            "Validation Loss: 1.8831387996673583, Validation Accuracy: 52.08\n",
            "[80/150]: Training Loss: 1.254057068824768, Training Accuracy: 64.394\n",
            "Validation Loss: 1.8864023208618164, Validation Accuracy: 51.51\n",
            "[81/150]: Training Loss: 1.2196114826202393, Training Accuracy: 64.948\n",
            "Validation Loss: 1.9068121194839478, Validation Accuracy: 51.97\n",
            "[82/150]: Training Loss: 1.2375918960571288, Training Accuracy: 64.48\n",
            "Validation Loss: 1.9304296493530273, Validation Accuracy: 51.61\n",
            "[83/150]: Training Loss: 1.2156933164596557, Training Accuracy: 64.806\n",
            "Validation Loss: 1.9103889226913453, Validation Accuracy: 51.82\n",
            "[84/150]: Training Loss: 1.1918401718139648, Training Accuracy: 65.572\n",
            "Validation Loss: 1.9341503858566285, Validation Accuracy: 51.6\n",
            "[85/150]: Training Loss: 1.2064945554733277, Training Accuracy: 65.146\n",
            "Validation Loss: 1.9983864784240724, Validation Accuracy: 50.34\n",
            "[86/150]: Training Loss: 1.201401376724243, Training Accuracy: 65.426\n",
            "Validation Loss: 1.9176611185073853, Validation Accuracy: 51.45\n",
            "[87/150]: Training Loss: 1.1660136127471923, Training Accuracy: 66.176\n",
            "Validation Loss: 1.943800139427185, Validation Accuracy: 51.49\n",
            "[88/150]: Training Loss: 1.1312262058258056, Training Accuracy: 67.438\n",
            "Validation Loss: 1.9152551412582397, Validation Accuracy: 51.68\n",
            "[89/150]: Training Loss: 1.1454276323318482, Training Accuracy: 66.958\n",
            "Validation Loss: 1.9066155195236205, Validation Accuracy: 52.43\n",
            "[90/150]: Training Loss: 1.1536525392532349, Training Accuracy: 66.622\n",
            "Validation Loss: 1.912238073348999, Validation Accuracy: 51.99\n",
            "[91/150]: Training Loss: 1.1152591037750244, Training Accuracy: 67.662\n",
            "Validation Loss: 1.9254616022109985, Validation Accuracy: 52.11\n",
            "[92/150]: Training Loss: 1.0796469306945802, Training Accuracy: 68.538\n",
            "Validation Loss: 1.918501377105713, Validation Accuracy: 52.25\n",
            "[93/150]: Training Loss: 1.0836839008331298, Training Accuracy: 68.43\n",
            "Validation Loss: 1.9036231279373168, Validation Accuracy: 52.52\n",
            "[94/150]: Training Loss: 1.111613211631775, Training Accuracy: 67.66\n",
            "Validation Loss: 1.9749577283859252, Validation Accuracy: 51.0\n",
            "[95/150]: Training Loss: 1.1183206748962402, Training Accuracy: 67.53\n",
            "Validation Loss: 1.9319661378860473, Validation Accuracy: 52.53\n",
            "[96/150]: Training Loss: 1.0428566884994508, Training Accuracy: 69.502\n",
            "Validation Loss: 1.9246442794799805, Validation Accuracy: 52.17\n",
            "[97/150]: Training Loss: 1.029050705432892, Training Accuracy: 70.002\n",
            "Validation Loss: 1.9244839429855347, Validation Accuracy: 52.21\n",
            "[98/150]: Training Loss: 1.0212417674064636, Training Accuracy: 70.272\n",
            "Validation Loss: 1.9145327806472778, Validation Accuracy: 52.78\n",
            "[99/150]: Training Loss: 1.0124672603607179, Training Accuracy: 70.44\n",
            "Validation Loss: 1.9107223749160767, Validation Accuracy: 53.04\n",
            "[100/150]: Training Loss: 0.9880559778213501, Training Accuracy: 71.17\n",
            "Validation Loss: 1.9347419500350953, Validation Accuracy: 52.57\n",
            "[101/150]: Training Loss: 0.9812371230125427, Training Accuracy: 71.144\n",
            "Validation Loss: 1.9186458826065063, Validation Accuracy: 53.22\n",
            "[102/150]: Training Loss: 0.9656218528747559, Training Accuracy: 72.014\n",
            "Validation Loss: 1.9120693922042846, Validation Accuracy: 53.07\n",
            "[103/150]: Training Loss: 0.9567889213562012, Training Accuracy: 71.89\n",
            "Validation Loss: 1.9197413206100464, Validation Accuracy: 52.84\n",
            "[104/150]: Training Loss: 0.9529655456542969, Training Accuracy: 72.086\n",
            "Validation Loss: 1.9144711256027223, Validation Accuracy: 53.0\n",
            "[105/150]: Training Loss: 0.9852134394645691, Training Accuracy: 71.252\n",
            "Validation Loss: 1.9319961071014404, Validation Accuracy: 53.06\n",
            "[106/150]: Training Loss: 0.9273789191246032, Training Accuracy: 72.91\n",
            "Validation Loss: 1.9424444437026978, Validation Accuracy: 52.84\n",
            "[107/150]: Training Loss: 0.9234069514274598, Training Accuracy: 73.084\n",
            "Validation Loss: 1.9234145641326905, Validation Accuracy: 52.94\n",
            "[108/150]: Training Loss: 0.9229924130439758, Training Accuracy: 73.21\n",
            "Validation Loss: 1.9324826002120972, Validation Accuracy: 52.73\n",
            "[109/150]: Training Loss: 0.9058748340606689, Training Accuracy: 73.67\n",
            "Validation Loss: 1.9500421285629272, Validation Accuracy: 53.29\n",
            "[110/150]: Training Loss: 0.8911954641342164, Training Accuracy: 73.872\n",
            "Validation Loss: 1.941013789176941, Validation Accuracy: 53.28\n",
            "[111/150]: Training Loss: 0.8825567650794983, Training Accuracy: 74.012\n",
            "Validation Loss: 1.9361898183822632, Validation Accuracy: 53.09\n",
            "[112/150]: Training Loss: 0.8620765686035157, Training Accuracy: 74.656\n",
            "Validation Loss: 1.9341821908950805, Validation Accuracy: 53.36\n",
            "[113/150]: Training Loss: 0.8601221704483032, Training Accuracy: 74.8\n",
            "Validation Loss: 1.9615466117858886, Validation Accuracy: 53.34\n",
            "[114/150]: Training Loss: 0.8472113418579101, Training Accuracy: 75.216\n",
            "Validation Loss: 1.950052309036255, Validation Accuracy: 53.42\n",
            "[115/150]: Training Loss: 0.84150705575943, Training Accuracy: 75.34\n",
            "Validation Loss: 1.9719900608062744, Validation Accuracy: 53.33\n",
            "[116/150]: Training Loss: 0.8295826292037964, Training Accuracy: 75.764\n",
            "Validation Loss: 1.9504849910736084, Validation Accuracy: 53.65\n",
            "[117/150]: Training Loss: 0.8204774522781372, Training Accuracy: 76.024\n",
            "Validation Loss: 1.9509764194488526, Validation Accuracy: 54.02\n",
            "[118/150]: Training Loss: 0.8103749394416809, Training Accuracy: 76.418\n",
            "Validation Loss: 1.9664921760559082, Validation Accuracy: 53.3\n",
            "[119/150]: Training Loss: 0.8074698400497436, Training Accuracy: 76.44\n",
            "Validation Loss: 1.955758023262024, Validation Accuracy: 53.71\n",
            "[120/150]: Training Loss: 0.8068988251686097, Training Accuracy: 76.406\n",
            "Validation Loss: 1.9637468814849854, Validation Accuracy: 53.55\n",
            "[121/150]: Training Loss: 0.7903911852836609, Training Accuracy: 76.678\n",
            "Validation Loss: 1.9706329584121705, Validation Accuracy: 53.57\n",
            "[122/150]: Training Loss: 0.7799955105781555, Training Accuracy: 77.276\n",
            "Validation Loss: 1.976036286354065, Validation Accuracy: 53.7\n",
            "[123/150]: Training Loss: 0.7765986251831055, Training Accuracy: 77.324\n",
            "Validation Loss: 1.9730368375778198, Validation Accuracy: 53.3\n",
            "[124/150]: Training Loss: 0.7681253123283386, Training Accuracy: 77.54\n",
            "Validation Loss: 1.9741400957107544, Validation Accuracy: 53.63\n",
            "[125/150]: Training Loss: 0.7622663331031799, Training Accuracy: 77.518\n",
            "Validation Loss: 1.9705329179763793, Validation Accuracy: 54.09\n",
            "[126/150]: Training Loss: 0.7574337005615235, Training Accuracy: 78.034\n",
            "Validation Loss: 1.9730172395706176, Validation Accuracy: 53.71\n",
            "[127/150]: Training Loss: 0.7504327082633973, Training Accuracy: 78.258\n",
            "Validation Loss: 1.9689469575881957, Validation Accuracy: 53.87\n",
            "[128/150]: Training Loss: 0.7432142400741577, Training Accuracy: 78.082\n",
            "Validation Loss: 1.9787657976150512, Validation Accuracy: 53.54\n",
            "[129/150]: Training Loss: 0.7408276867866516, Training Accuracy: 78.396\n",
            "Validation Loss: 1.9802783012390137, Validation Accuracy: 53.89\n",
            "[130/150]: Training Loss: 0.7272537612915039, Training Accuracy: 78.856\n",
            "Validation Loss: 1.9677628517150878, Validation Accuracy: 53.76\n",
            "[131/150]: Training Loss: 0.7158604049682618, Training Accuracy: 78.946\n",
            "Validation Loss: 1.97785542011261, Validation Accuracy: 53.9\n",
            "[132/150]: Training Loss: 0.7163805866241455, Training Accuracy: 79.138\n",
            "Validation Loss: 1.976898455619812, Validation Accuracy: 53.7\n",
            "[133/150]: Training Loss: 0.721850438117981, Training Accuracy: 78.816\n",
            "Validation Loss: 1.9874832153320312, Validation Accuracy: 53.66\n",
            "[134/150]: Training Loss: 0.719142906665802, Training Accuracy: 79.046\n",
            "Validation Loss: 1.9858264207839966, Validation Accuracy: 53.92\n",
            "[135/150]: Training Loss: 0.7152685523033142, Training Accuracy: 79.11\n",
            "Validation Loss: 1.9745909452438355, Validation Accuracy: 53.99\n",
            "[136/150]: Training Loss: 0.7077970743179322, Training Accuracy: 79.564\n",
            "Validation Loss: 1.9791152715682983, Validation Accuracy: 53.54\n",
            "[137/150]: Training Loss: 0.7019647455215454, Training Accuracy: 79.626\n",
            "Validation Loss: 1.9777050018310547, Validation Accuracy: 54.06\n",
            "[138/150]: Training Loss: 0.6962298607826233, Training Accuracy: 79.744\n",
            "Validation Loss: 1.9783851623535156, Validation Accuracy: 53.9\n",
            "[139/150]: Training Loss: 0.6945947551727295, Training Accuracy: 79.802\n",
            "Validation Loss: 1.9834458827972412, Validation Accuracy: 53.84\n",
            "[140/150]: Training Loss: 0.6916244769096375, Training Accuracy: 79.876\n",
            "Validation Loss: 1.9782796621322631, Validation Accuracy: 54.08\n",
            "[141/150]: Training Loss: 0.686495201587677, Training Accuracy: 80.022\n",
            "Validation Loss: 1.9823485612869263, Validation Accuracy: 54.07\n",
            "[142/150]: Training Loss: 0.6907814073562623, Training Accuracy: 80.056\n",
            "Validation Loss: 1.9827837467193603, Validation Accuracy: 53.98\n",
            "[143/150]: Training Loss: 0.6871487998962402, Training Accuracy: 80.084\n",
            "Validation Loss: 1.9844423055648803, Validation Accuracy: 54.1\n",
            "[144/150]: Training Loss: 0.6854921150207519, Training Accuracy: 80.314\n",
            "Validation Loss: 1.9852968454360962, Validation Accuracy: 53.98\n",
            "[145/150]: Training Loss: 0.6804221367835999, Training Accuracy: 80.244\n",
            "Validation Loss: 1.9844240427017212, Validation Accuracy: 54.1\n",
            "[146/150]: Training Loss: 0.682576208114624, Training Accuracy: 80.136\n",
            "Validation Loss: 1.9841588735580444, Validation Accuracy: 54.07\n",
            "[147/150]: Training Loss: 0.6786355781555176, Training Accuracy: 80.254\n",
            "Validation Loss: 1.9839539051055908, Validation Accuracy: 54.12\n",
            "[148/150]: Training Loss: 0.6753697633743286, Training Accuracy: 80.27\n",
            "Validation Loss: 1.9842885971069335, Validation Accuracy: 54.16\n",
            "[149/150]: Training Loss: 0.6753653740882873, Training Accuracy: 80.488\n",
            "Validation Loss: 1.9842547655105591, Validation Accuracy: 54.12\n",
            "[150/150]: Training Loss: 0.6740122199058532, Training Accuracy: 80.532\n",
            "Validation Loss: 1.984296441078186, Validation Accuracy: 54.15\n",
            "**********************************************************************\n",
            "Test Loss: 1.984296441078186, Test Accuracy: 54.15\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁█▆▃▇</td></tr><tr><td>Test Loss</td><td>▆▄▆█▁</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>54.15</td></tr><tr><td>Test Loss</td><td>1.9843</td></tr><tr><td>Train Accuracy</td><td>80.532</td></tr><tr><td>Train Loss</td><td>0.67401</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=2048 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_022321-j8jszxxs/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 4096, Learning rate: 12.0, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_024410-fcrci1zx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx' target=\"_blank\">batch_size=4096 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.448218419001653, Training Accuracy: 3.104\n",
            "Validation Loss: 4.19553820292155, Validation Accuracy: 5.83\n",
            "[2/150]: Training Loss: 4.162900081047645, Training Accuracy: 5.992\n",
            "Validation Loss: 4.001244783401489, Validation Accuracy: 8.19\n",
            "[3/150]: Training Loss: 3.9526728116548977, Training Accuracy: 9.02\n",
            "Validation Loss: 3.849735975265503, Validation Accuracy: 10.99\n",
            "[4/150]: Training Loss: 3.9088880098783054, Training Accuracy: 9.748\n",
            "Validation Loss: 3.7720150152842202, Validation Accuracy: 11.82\n",
            "[5/150]: Training Loss: 3.7579474999354434, Training Accuracy: 11.674\n",
            "Validation Loss: 3.6346964836120605, Validation Accuracy: 13.58\n",
            "[6/150]: Training Loss: 3.8317384169651914, Training Accuracy: 10.848\n",
            "Validation Loss: 3.6928911209106445, Validation Accuracy: 13.8\n",
            "[7/150]: Training Loss: 3.6279670825371375, Training Accuracy: 13.764\n",
            "Validation Loss: 3.426232655843099, Validation Accuracy: 17.9\n",
            "[8/150]: Training Loss: 3.559702047934899, Training Accuracy: 15.198\n",
            "Validation Loss: 3.5184563795725503, Validation Accuracy: 15.74\n",
            "[9/150]: Training Loss: 3.619346306874202, Training Accuracy: 14.12\n",
            "Validation Loss: 3.4724766413370767, Validation Accuracy: 16.53\n",
            "[10/150]: Training Loss: 3.4825954070458045, Training Accuracy: 16.518\n",
            "Validation Loss: 3.491668939590454, Validation Accuracy: 17.05\n",
            "[11/150]: Training Loss: 3.5322888447688174, Training Accuracy: 15.738\n",
            "Validation Loss: 3.3563063939412436, Validation Accuracy: 18.53\n",
            "[12/150]: Training Loss: 3.381067386040321, Training Accuracy: 18.274\n",
            "Validation Loss: 3.371330420176188, Validation Accuracy: 18.51\n",
            "[13/150]: Training Loss: 3.3759262011601376, Training Accuracy: 18.364\n",
            "Validation Loss: 3.1718979676564536, Validation Accuracy: 22.38\n",
            "[14/150]: Training Loss: 3.24786586027879, Training Accuracy: 20.88\n",
            "Validation Loss: 3.149214585622152, Validation Accuracy: 22.67\n",
            "[15/150]: Training Loss: 3.155352940926185, Training Accuracy: 22.24\n",
            "Validation Loss: 2.999330917994181, Validation Accuracy: 25.44\n",
            "[16/150]: Training Loss: 2.9997272124657264, Training Accuracy: 25.252\n",
            "Validation Loss: 2.8626222610473633, Validation Accuracy: 28.18\n",
            "[17/150]: Training Loss: 2.952247326190655, Training Accuracy: 26.39\n",
            "Validation Loss: 3.0151708920796714, Validation Accuracy: 25.48\n",
            "[18/150]: Training Loss: 3.035908350577721, Training Accuracy: 24.716\n",
            "Validation Loss: 2.84928830464681, Validation Accuracy: 28.54\n",
            "[19/150]: Training Loss: 2.8744187355041504, Training Accuracy: 27.56\n",
            "Validation Loss: 2.732924222946167, Validation Accuracy: 30.87\n",
            "[20/150]: Training Loss: 2.7774770443256083, Training Accuracy: 29.672\n",
            "Validation Loss: 2.800260861714681, Validation Accuracy: 29.69\n",
            "[21/150]: Training Loss: 2.9696661142202525, Training Accuracy: 26.582\n",
            "Validation Loss: 3.213308095932007, Validation Accuracy: 22.23\n",
            "[22/150]: Training Loss: 3.2055968137887807, Training Accuracy: 22.87\n",
            "Validation Loss: 3.0360422134399414, Validation Accuracy: 25.56\n",
            "[23/150]: Training Loss: 3.0269867456876316, Training Accuracy: 25.102\n",
            "Validation Loss: 2.861661911010742, Validation Accuracy: 29.24\n",
            "[24/150]: Training Loss: 2.8519647121429443, Training Accuracy: 28.248\n",
            "Validation Loss: 2.682717482248942, Validation Accuracy: 31.6\n",
            "[25/150]: Training Loss: 2.705435276031494, Training Accuracy: 31.156\n",
            "Validation Loss: 2.509896198908488, Validation Accuracy: 35.11\n",
            "[26/150]: Training Loss: 2.613956708174485, Training Accuracy: 33.41\n",
            "Validation Loss: 2.505950371424357, Validation Accuracy: 34.77\n",
            "[27/150]: Training Loss: 3.002190589904785, Training Accuracy: 26.466\n",
            "Validation Loss: 2.775864839553833, Validation Accuracy: 29.88\n",
            "[28/150]: Training Loss: 2.742469017322247, Training Accuracy: 30.316\n",
            "Validation Loss: 2.589009443918864, Validation Accuracy: 33.99\n",
            "[29/150]: Training Loss: 2.8341226394359884, Training Accuracy: 29.282\n",
            "Validation Loss: 2.8079089323679605, Validation Accuracy: 29.56\n",
            "[30/150]: Training Loss: 2.745966214400071, Training Accuracy: 30.384\n",
            "Validation Loss: 2.5663379033406577, Validation Accuracy: 34.18\n",
            "[31/150]: Training Loss: 2.5754335476801944, Training Accuracy: 33.944\n",
            "Validation Loss: 2.4243160088857016, Validation Accuracy: 37.33\n",
            "[32/150]: Training Loss: 2.4728290117703953, Training Accuracy: 35.998\n",
            "Validation Loss: 2.4164366722106934, Validation Accuracy: 37.02\n",
            "[33/150]: Training Loss: 2.550288127018855, Training Accuracy: 34.558\n",
            "Validation Loss: 2.4929703871409097, Validation Accuracy: 35.7\n",
            "[34/150]: Training Loss: 2.5172606064723086, Training Accuracy: 35.468\n",
            "Validation Loss: 2.5022354125976562, Validation Accuracy: 35.92\n",
            "[35/150]: Training Loss: 2.5095452528733473, Training Accuracy: 35.052\n",
            "Validation Loss: 2.3538503646850586, Validation Accuracy: 38.43\n",
            "[36/150]: Training Loss: 2.595434335561899, Training Accuracy: 34.342\n",
            "Validation Loss: 2.7459415594736734, Validation Accuracy: 30.95\n",
            "[37/150]: Training Loss: 2.5996641562535214, Training Accuracy: 33.51\n",
            "Validation Loss: 2.4413878122965493, Validation Accuracy: 37.55\n",
            "[38/150]: Training Loss: 2.4308731005742, Training Accuracy: 36.788\n",
            "Validation Loss: 2.3517263730367026, Validation Accuracy: 38.8\n",
            "[39/150]: Training Loss: 2.3385920707996073, Training Accuracy: 39.318\n",
            "Validation Loss: 2.4507851600646973, Validation Accuracy: 37.58\n",
            "[40/150]: Training Loss: 2.4006691529200626, Training Accuracy: 37.842\n",
            "Validation Loss: 2.3381757736206055, Validation Accuracy: 39.0\n",
            "[41/150]: Training Loss: 2.4301048792325535, Training Accuracy: 36.898\n",
            "Validation Loss: 2.3649702866872153, Validation Accuracy: 38.64\n",
            "[42/150]: Training Loss: 2.282798070173997, Training Accuracy: 39.856\n",
            "Validation Loss: 2.2728551228841147, Validation Accuracy: 40.36\n",
            "[43/150]: Training Loss: 2.235896715751061, Training Accuracy: 40.962\n",
            "Validation Loss: 2.2269629637400308, Validation Accuracy: 41.47\n",
            "[44/150]: Training Loss: 2.277051045344426, Training Accuracy: 40.666\n",
            "Validation Loss: 2.393709977467855, Validation Accuracy: 38.68\n",
            "[45/150]: Training Loss: 2.266980061164269, Training Accuracy: 40.186\n",
            "Validation Loss: 2.260339101155599, Validation Accuracy: 41.24\n",
            "[46/150]: Training Loss: 2.2269545518434963, Training Accuracy: 41.5\n",
            "Validation Loss: 2.184856653213501, Validation Accuracy: 42.59\n",
            "[47/150]: Training Loss: 2.1506412762861986, Training Accuracy: 42.82\n",
            "Validation Loss: 2.151843468348185, Validation Accuracy: 43.6\n",
            "[48/150]: Training Loss: 2.0945023756760817, Training Accuracy: 45.142\n",
            "Validation Loss: 2.1851927439371743, Validation Accuracy: 42.94\n",
            "[49/150]: Training Loss: 2.368942279082078, Training Accuracy: 38.482\n",
            "Validation Loss: 2.311251401901245, Validation Accuracy: 39.8\n",
            "[50/150]: Training Loss: 2.1795968275803785, Training Accuracy: 42.274\n",
            "Validation Loss: 2.1368969281514487, Validation Accuracy: 43.53\n",
            "[51/150]: Training Loss: 2.287435614145719, Training Accuracy: 40.578\n",
            "Validation Loss: 2.3101561864217124, Validation Accuracy: 40.31\n",
            "[52/150]: Training Loss: 2.133154667340792, Training Accuracy: 43.412\n",
            "Validation Loss: 2.101039091746012, Validation Accuracy: 44.48\n",
            "[53/150]: Training Loss: 2.009332299232483, Training Accuracy: 46.076\n",
            "Validation Loss: 2.053215821584066, Validation Accuracy: 46.02\n",
            "[54/150]: Training Loss: 2.0092475414276123, Training Accuracy: 46.312\n",
            "Validation Loss: 2.068614880243937, Validation Accuracy: 45.15\n",
            "[55/150]: Training Loss: 2.0747447105554433, Training Accuracy: 44.708\n",
            "Validation Loss: 2.0757156213124595, Validation Accuracy: 45.38\n",
            "[56/150]: Training Loss: 2.0832339341823873, Training Accuracy: 44.52\n",
            "Validation Loss: 2.2822861671447754, Validation Accuracy: 40.59\n",
            "[57/150]: Training Loss: 2.035889350450956, Training Accuracy: 45.234\n",
            "Validation Loss: 2.077279488245646, Validation Accuracy: 45.32\n",
            "[58/150]: Training Loss: 1.9101605140245879, Training Accuracy: 48.098\n",
            "Validation Loss: 2.075277328491211, Validation Accuracy: 45.52\n",
            "[59/150]: Training Loss: 1.930518700526311, Training Accuracy: 47.694\n",
            "Validation Loss: 2.0554560820261636, Validation Accuracy: 45.94\n",
            "[60/150]: Training Loss: 1.9262570784642146, Training Accuracy: 47.71\n",
            "Validation Loss: 1.9856750170389812, Validation Accuracy: 46.93\n",
            "[61/150]: Training Loss: 1.8526782714403593, Training Accuracy: 49.478\n",
            "Validation Loss: 1.9915226300557454, Validation Accuracy: 46.83\n",
            "[62/150]: Training Loss: 1.8636801151128917, Training Accuracy: 49.23\n",
            "Validation Loss: 2.094877322514852, Validation Accuracy: 45.09\n",
            "[63/150]: Training Loss: 1.8440089867665217, Training Accuracy: 49.998\n",
            "Validation Loss: 1.9725687503814697, Validation Accuracy: 47.58\n",
            "[64/150]: Training Loss: 1.8951456546783447, Training Accuracy: 48.798\n",
            "Validation Loss: 2.079833189646403, Validation Accuracy: 45.33\n",
            "[65/150]: Training Loss: 1.9134588700074415, Training Accuracy: 48.274\n",
            "Validation Loss: 2.0578166246414185, Validation Accuracy: 45.69\n",
            "[66/150]: Training Loss: 1.850765347480774, Training Accuracy: 49.748\n",
            "Validation Loss: 2.0475390752156577, Validation Accuracy: 47.14\n",
            "[67/150]: Training Loss: 1.7821540557421172, Training Accuracy: 51.288\n",
            "Validation Loss: 1.9497700134913127, Validation Accuracy: 48.19\n",
            "[68/150]: Training Loss: 1.7319823136696448, Training Accuracy: 52.606\n",
            "Validation Loss: 2.018282691637675, Validation Accuracy: 47.2\n",
            "[69/150]: Training Loss: 1.7186179894667406, Training Accuracy: 52.626\n",
            "Validation Loss: 1.9749605258305867, Validation Accuracy: 48.19\n",
            "[70/150]: Training Loss: 1.6911903069569514, Training Accuracy: 53.452\n",
            "Validation Loss: 1.9826482931772869, Validation Accuracy: 47.74\n",
            "[71/150]: Training Loss: 1.660032529097337, Training Accuracy: 54.11\n",
            "Validation Loss: 1.908194661140442, Validation Accuracy: 49.51\n",
            "[72/150]: Training Loss: 1.6769285110326915, Training Accuracy: 53.518\n",
            "Validation Loss: 1.954678734143575, Validation Accuracy: 48.03\n",
            "[73/150]: Training Loss: 1.6593820590239305, Training Accuracy: 53.96\n",
            "Validation Loss: 1.9272022247314453, Validation Accuracy: 48.63\n",
            "[74/150]: Training Loss: 1.6419056562276988, Training Accuracy: 54.992\n",
            "Validation Loss: 1.992493987083435, Validation Accuracy: 47.79\n",
            "[75/150]: Training Loss: 1.6689472198486328, Training Accuracy: 53.936\n",
            "Validation Loss: 1.9214499394098918, Validation Accuracy: 49.0\n",
            "[76/150]: Training Loss: 1.6087861702992365, Training Accuracy: 55.276\n",
            "Validation Loss: 1.896010160446167, Validation Accuracy: 49.84\n",
            "[77/150]: Training Loss: 1.5652516438410833, Training Accuracy: 56.72\n",
            "Validation Loss: 1.873304804166158, Validation Accuracy: 51.03\n",
            "[78/150]: Training Loss: 1.6746907876088069, Training Accuracy: 54.032\n",
            "Validation Loss: 1.954778750737508, Validation Accuracy: 49.08\n",
            "[79/150]: Training Loss: 1.5788640517454882, Training Accuracy: 55.932\n",
            "Validation Loss: 1.9272086222966511, Validation Accuracy: 50.3\n",
            "[80/150]: Training Loss: 1.6021267634171705, Training Accuracy: 55.644\n",
            "Validation Loss: 2.1279262701670327, Validation Accuracy: 45.34\n",
            "[81/150]: Training Loss: 1.7035195552385771, Training Accuracy: 53.114\n",
            "Validation Loss: 1.9278326431910198, Validation Accuracy: 49.04\n",
            "[82/150]: Training Loss: 1.559159095470722, Training Accuracy: 56.4\n",
            "Validation Loss: 1.9029817183812459, Validation Accuracy: 49.79\n",
            "[83/150]: Training Loss: 1.5267010835500865, Training Accuracy: 57.53\n",
            "Validation Loss: 1.9380817810694377, Validation Accuracy: 48.88\n",
            "[84/150]: Training Loss: 1.5102592706680298, Training Accuracy: 57.916\n",
            "Validation Loss: 1.8722127676010132, Validation Accuracy: 51.08\n",
            "[85/150]: Training Loss: 1.4807829398375292, Training Accuracy: 58.592\n",
            "Validation Loss: 1.9032895962397258, Validation Accuracy: 49.92\n",
            "[86/150]: Training Loss: 1.4740843681188731, Training Accuracy: 58.914\n",
            "Validation Loss: 1.8672935565312703, Validation Accuracy: 50.45\n",
            "[87/150]: Training Loss: 1.515398117212149, Training Accuracy: 57.934\n",
            "Validation Loss: 1.9412325620651245, Validation Accuracy: 49.15\n",
            "[88/150]: Training Loss: 1.576969366807204, Training Accuracy: 55.868\n",
            "Validation Loss: 1.935785969098409, Validation Accuracy: 49.25\n",
            "[89/150]: Training Loss: 1.4918665610826933, Training Accuracy: 58.11\n",
            "Validation Loss: 1.8986582358678181, Validation Accuracy: 51.05\n",
            "[90/150]: Training Loss: 1.4492073059082031, Training Accuracy: 59.446\n",
            "Validation Loss: 1.8367600440979004, Validation Accuracy: 51.57\n",
            "[91/150]: Training Loss: 1.4048503912412202, Training Accuracy: 60.418\n",
            "Validation Loss: 1.837977687517802, Validation Accuracy: 51.68\n",
            "[92/150]: Training Loss: 1.3744991559248705, Training Accuracy: 61.112\n",
            "Validation Loss: 1.8420219818751018, Validation Accuracy: 51.77\n",
            "[93/150]: Training Loss: 1.420577076765207, Training Accuracy: 60.392\n",
            "Validation Loss: 1.8663844267527263, Validation Accuracy: 51.21\n",
            "[94/150]: Training Loss: 1.4505155269916241, Training Accuracy: 59.012\n",
            "Validation Loss: 1.8934478759765625, Validation Accuracy: 50.84\n",
            "[95/150]: Training Loss: 1.4003274349065928, Training Accuracy: 60.672\n",
            "Validation Loss: 1.8551263411839802, Validation Accuracy: 51.69\n",
            "[96/150]: Training Loss: 1.3484498354104848, Training Accuracy: 61.974\n",
            "Validation Loss: 1.8562318483988445, Validation Accuracy: 51.96\n",
            "[97/150]: Training Loss: 1.314302426118117, Training Accuracy: 62.828\n",
            "Validation Loss: 1.8313268423080444, Validation Accuracy: 52.72\n",
            "[98/150]: Training Loss: 1.2976581683525672, Training Accuracy: 63.268\n",
            "Validation Loss: 1.8362118005752563, Validation Accuracy: 52.11\n",
            "[99/150]: Training Loss: 1.317265354670011, Training Accuracy: 62.994\n",
            "Validation Loss: 1.8350664774576824, Validation Accuracy: 52.02\n",
            "[100/150]: Training Loss: 1.2960394254097571, Training Accuracy: 63.262\n",
            "Validation Loss: 1.800865610440572, Validation Accuracy: 53.11\n",
            "[101/150]: Training Loss: 1.298880622937129, Training Accuracy: 62.896\n",
            "Validation Loss: 1.883919596672058, Validation Accuracy: 51.53\n",
            "[102/150]: Training Loss: 1.2974212628144484, Training Accuracy: 62.852\n",
            "Validation Loss: 1.8424270550409954, Validation Accuracy: 52.02\n",
            "[103/150]: Training Loss: 1.256823787322411, Training Accuracy: 63.952\n",
            "Validation Loss: 1.8408455053965251, Validation Accuracy: 52.54\n",
            "[104/150]: Training Loss: 1.252828497153062, Training Accuracy: 64.42\n",
            "Validation Loss: 1.8253253698349, Validation Accuracy: 52.37\n",
            "[105/150]: Training Loss: 1.2491283691846407, Training Accuracy: 64.358\n",
            "Validation Loss: 1.8558521668116252, Validation Accuracy: 52.17\n",
            "[106/150]: Training Loss: 1.229967685846182, Training Accuracy: 64.958\n",
            "Validation Loss: 1.8309193849563599, Validation Accuracy: 52.71\n",
            "[107/150]: Training Loss: 1.2172498794702382, Training Accuracy: 65.282\n",
            "Validation Loss: 1.8394190073013306, Validation Accuracy: 52.48\n",
            "[108/150]: Training Loss: 1.207243589254526, Training Accuracy: 65.544\n",
            "Validation Loss: 1.82985524336497, Validation Accuracy: 52.3\n",
            "[109/150]: Training Loss: 1.1976150732774, Training Accuracy: 65.54\n",
            "Validation Loss: 1.8410567442576091, Validation Accuracy: 52.6\n",
            "[110/150]: Training Loss: 1.1797684522775502, Training Accuracy: 66.22\n",
            "Validation Loss: 1.828009049097697, Validation Accuracy: 53.27\n",
            "[111/150]: Training Loss: 1.1586786141762366, Training Accuracy: 66.72\n",
            "Validation Loss: 1.8170452117919922, Validation Accuracy: 53.66\n",
            "[112/150]: Training Loss: 1.171253332724938, Training Accuracy: 66.59\n",
            "Validation Loss: 1.8729171355565388, Validation Accuracy: 52.55\n",
            "[113/150]: Training Loss: 1.2010775346022387, Training Accuracy: 65.75\n",
            "Validation Loss: 1.8387389580408733, Validation Accuracy: 53.17\n",
            "[114/150]: Training Loss: 1.1887562825129583, Training Accuracy: 66.114\n",
            "Validation Loss: 1.8573131561279297, Validation Accuracy: 52.59\n",
            "[115/150]: Training Loss: 1.1683265062478871, Training Accuracy: 66.588\n",
            "Validation Loss: 1.8474773168563843, Validation Accuracy: 53.17\n",
            "[116/150]: Training Loss: 1.1644465373112605, Training Accuracy: 66.768\n",
            "Validation Loss: 1.8591439326604207, Validation Accuracy: 52.41\n",
            "[117/150]: Training Loss: 1.1479460184390728, Training Accuracy: 67.308\n",
            "Validation Loss: 1.8460925817489624, Validation Accuracy: 53.29\n",
            "[118/150]: Training Loss: 1.1312938928604126, Training Accuracy: 67.496\n",
            "Validation Loss: 1.8282337188720703, Validation Accuracy: 53.62\n",
            "[119/150]: Training Loss: 1.1176557632593007, Training Accuracy: 68.078\n",
            "Validation Loss: 1.8184215625127156, Validation Accuracy: 53.58\n",
            "[120/150]: Training Loss: 1.1109853432728694, Training Accuracy: 68.212\n",
            "Validation Loss: 1.8269612789154053, Validation Accuracy: 53.4\n",
            "[121/150]: Training Loss: 1.092060116621164, Training Accuracy: 68.912\n",
            "Validation Loss: 1.8181384801864624, Validation Accuracy: 53.59\n",
            "[122/150]: Training Loss: 1.0812291502952576, Training Accuracy: 69.18\n",
            "Validation Loss: 1.8252775271733601, Validation Accuracy: 53.6\n",
            "[123/150]: Training Loss: 1.0699465458209698, Training Accuracy: 69.43\n",
            "Validation Loss: 1.834611177444458, Validation Accuracy: 54.01\n",
            "[124/150]: Training Loss: 1.0735741762014537, Training Accuracy: 69.112\n",
            "Validation Loss: 1.8203496138254802, Validation Accuracy: 53.93\n",
            "[125/150]: Training Loss: 1.0661220917334924, Training Accuracy: 69.538\n",
            "Validation Loss: 1.828150749206543, Validation Accuracy: 53.85\n",
            "[126/150]: Training Loss: 1.059658169746399, Training Accuracy: 69.666\n",
            "Validation Loss: 1.8271387418111165, Validation Accuracy: 53.8\n",
            "[127/150]: Training Loss: 1.0502580862778883, Training Accuracy: 70.108\n",
            "Validation Loss: 1.824654181798299, Validation Accuracy: 53.91\n",
            "[128/150]: Training Loss: 1.045825692323538, Training Accuracy: 69.762\n",
            "Validation Loss: 1.828839858373006, Validation Accuracy: 54.18\n",
            "[129/150]: Training Loss: 1.041814657358023, Training Accuracy: 70.04\n",
            "Validation Loss: 1.8183866739273071, Validation Accuracy: 53.89\n",
            "[130/150]: Training Loss: 1.0349373588195214, Training Accuracy: 70.334\n",
            "Validation Loss: 1.817612926165263, Validation Accuracy: 54.08\n",
            "[131/150]: Training Loss: 1.0304582439936125, Training Accuracy: 70.738\n",
            "Validation Loss: 1.8261488676071167, Validation Accuracy: 54.44\n",
            "[132/150]: Training Loss: 1.0328317513832679, Training Accuracy: 70.262\n",
            "Validation Loss: 1.8170503377914429, Validation Accuracy: 54.11\n",
            "[133/150]: Training Loss: 1.0127528584920442, Training Accuracy: 71.006\n",
            "Validation Loss: 1.823221206665039, Validation Accuracy: 54.24\n",
            "[134/150]: Training Loss: 1.0145881175994873, Training Accuracy: 70.876\n",
            "Validation Loss: 1.8177992900212605, Validation Accuracy: 54.42\n",
            "[135/150]: Training Loss: 1.0154722057856047, Training Accuracy: 70.856\n",
            "Validation Loss: 1.8151982227961223, Validation Accuracy: 54.36\n",
            "[136/150]: Training Loss: 1.0053791770568261, Training Accuracy: 70.962\n",
            "Validation Loss: 1.815675139427185, Validation Accuracy: 54.52\n",
            "[137/150]: Training Loss: 1.0002271212064302, Training Accuracy: 71.094\n",
            "Validation Loss: 1.8183471361796062, Validation Accuracy: 54.62\n",
            "[138/150]: Training Loss: 1.003823459148407, Training Accuracy: 71.112\n",
            "Validation Loss: 1.8167388836542766, Validation Accuracy: 54.32\n",
            "[139/150]: Training Loss: 1.0023488723314726, Training Accuracy: 71.28\n",
            "Validation Loss: 1.8161654869715373, Validation Accuracy: 54.5\n",
            "[140/150]: Training Loss: 0.996692391542288, Training Accuracy: 71.326\n",
            "Validation Loss: 1.8168489535649617, Validation Accuracy: 54.36\n",
            "[141/150]: Training Loss: 0.9932475502674396, Training Accuracy: 71.382\n",
            "Validation Loss: 1.8148254950841267, Validation Accuracy: 54.41\n",
            "[142/150]: Training Loss: 0.9950209443385785, Training Accuracy: 71.754\n",
            "Validation Loss: 1.8171526590983074, Validation Accuracy: 54.22\n",
            "[143/150]: Training Loss: 0.9949191717001108, Training Accuracy: 71.676\n",
            "Validation Loss: 1.8166076342264812, Validation Accuracy: 54.39\n",
            "[144/150]: Training Loss: 0.9898046300961421, Training Accuracy: 71.61\n",
            "Validation Loss: 1.8168938557306926, Validation Accuracy: 54.34\n",
            "[145/150]: Training Loss: 0.9879593665783222, Training Accuracy: 71.72\n",
            "Validation Loss: 1.8167452017466228, Validation Accuracy: 54.36\n",
            "[146/150]: Training Loss: 0.9881623249787551, Training Accuracy: 71.81\n",
            "Validation Loss: 1.8165217638015747, Validation Accuracy: 54.38\n",
            "[147/150]: Training Loss: 0.9901151886353126, Training Accuracy: 71.69\n",
            "Validation Loss: 1.81660795211792, Validation Accuracy: 54.48\n",
            "[148/150]: Training Loss: 0.9835631480583777, Training Accuracy: 71.944\n",
            "Validation Loss: 1.8168546358744304, Validation Accuracy: 54.43\n",
            "[149/150]: Training Loss: 0.9820910050318792, Training Accuracy: 72.082\n",
            "Validation Loss: 1.8165034850438435, Validation Accuracy: 54.45\n",
            "[150/150]: Training Loss: 0.9884678217080923, Training Accuracy: 71.694\n",
            "Validation Loss: 1.8165420691172283, Validation Accuracy: 54.48\n",
            "**********************************************************************\n",
            "Test Loss: 1.8165420691172283, Test Accuracy: 54.48\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▂▁█</td></tr><tr><td>Test Loss</td><td>▇█▁</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▃▄▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▇▆▅▅▅▅▄▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>54.48</td></tr><tr><td>Test Loss</td><td>1.81654</td></tr><tr><td>Train Accuracy</td><td>71.694</td></tr><tr><td>Train Loss</td><td>0.98847</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=4096 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_024410-fcrci1zx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 8192, Learning rate: 16.970562748477143, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_030841-573n6uz6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6' target=\"_blank\">batch_size=8192 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.543815356034499, Training Accuracy: 1.934\n",
            "Validation Loss: 4.557299613952637, Validation Accuracy: 4.11\n",
            "[2/150]: Training Loss: 4.4831575613755446, Training Accuracy: 3.478\n",
            "Validation Loss: 4.407679557800293, Validation Accuracy: 3.73\n",
            "[3/150]: Training Loss: 4.383443318880522, Training Accuracy: 4.272\n",
            "Validation Loss: 4.315660317738851, Validation Accuracy: 4.56\n",
            "[4/150]: Training Loss: 4.753879473759578, Training Accuracy: 2.854\n",
            "Validation Loss: 4.617130438486735, Validation Accuracy: 1.55\n",
            "[5/150]: Training Loss: 4.834007776700533, Training Accuracy: 1.152\n",
            "Validation Loss: 4.623517354329427, Validation Accuracy: 1.21\n",
            "[6/150]: Training Loss: 4.748236729548528, Training Accuracy: 1.046\n",
            "Validation Loss: 4.616909344991048, Validation Accuracy: 1.0\n",
            "[7/150]: Training Loss: 4.6143631201524, Training Accuracy: 1.022\n",
            "Validation Loss: 4.6133114496866865, Validation Accuracy: 0.95\n",
            "[8/150]: Training Loss: 86586261001215.23, Training Accuracy: 1.076\n",
            "Validation Loss: 199153685277354.66, Validation Accuracy: 1.08\n",
            "[9/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[10/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[11/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[12/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[13/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[14/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[15/150]: Training Loss: inf, Training Accuracy: 1.006\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[16/150]: Training Loss: inf, Training Accuracy: 1.004\n",
            "Validation Loss: inf, Validation Accuracy: 0.99\n",
            "[17/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[18/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[19/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[20/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[21/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[22/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[23/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[24/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[25/150]: Training Loss: inf, Training Accuracy: 1.014\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[26/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[27/150]: Training Loss: inf, Training Accuracy: 0.998\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[28/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[29/150]: Training Loss: inf, Training Accuracy: 1.004\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[30/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[31/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[32/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[33/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3707833002946235e+34, Validation Accuracy: 1.04\n",
            "[34/150]: Training Loss: 7.178876870446214e+34, Training Accuracy: 1.03\n",
            "Validation Loss: 6.908918765509594e+34, Validation Accuracy: 1.04\n",
            "[35/150]: Training Loss: 7.0300555279696e+34, Training Accuracy: 1.006\n",
            "Validation Loss: 6.850623178707614e+34, Validation Accuracy: 1.04\n",
            "[36/150]: Training Loss: 7.161850738321898e+34, Training Accuracy: 1.024\n",
            "Validation Loss: 6.997600003222513e+34, Validation Accuracy: 1.04\n",
            "[37/150]: Training Loss: 7.295138713909261e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.183176128279674e+34, Validation Accuracy: 1.04\n",
            "[38/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.320277327395843e+34, Validation Accuracy: 1.04\n",
            "[39/150]: Training Loss: 7.652232993118385e+34, Training Accuracy: 1.024\n",
            "Validation Loss: 7.385754947129741e+34, Validation Accuracy: 1.04\n",
            "[40/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.395794805907017e+34, Validation Accuracy: 1.04\n",
            "[41/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.389329127611166e+34, Validation Accuracy: 1.0\n",
            "[42/150]: Training Loss: 7.64618030431646e+34, Training Accuracy: 1.012\n",
            "Validation Loss: 7.36814170130946e+34, Validation Accuracy: 1.04\n",
            "[43/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.351131744934992e+34, Validation Accuracy: 1.04\n",
            "[44/150]: Training Loss: inf, Training Accuracy: 1.026\n",
            "Validation Loss: 7.339262375838323e+34, Validation Accuracy: 1.04\n",
            "[45/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.334463460011365e+34, Validation Accuracy: 1.04\n",
            "[46/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.33473861281743e+34, Validation Accuracy: 1.04\n",
            "[47/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.342335768309189e+34, Validation Accuracy: 1.04\n",
            "[48/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.343183509648092e+34, Validation Accuracy: 1.04\n",
            "[49/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.353242350172637e+34, Validation Accuracy: 1.04\n",
            "[50/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.35132750451987e+34, Validation Accuracy: 1.04\n",
            "[51/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.338507727590375e+34, Validation Accuracy: 1.04\n",
            "[52/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.345233868470492e+34, Validation Accuracy: 1.04\n",
            "[53/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.35075210998961e+34, Validation Accuracy: 1.0\n",
            "[54/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.35572186154199e+34, Validation Accuracy: 1.04\n",
            "[55/150]: Training Loss: 7.620612957278016e+34, Training Accuracy: 1.026\n",
            "Validation Loss: 7.362447672304763e+34, Validation Accuracy: 1.04\n",
            "[56/150]: Training Loss: 7.605070410587243e+34, Training Accuracy: 1.012\n",
            "Validation Loss: 7.363398575313606e+34, Validation Accuracy: 1.04\n",
            "[57/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3600082701926835e+34, Validation Accuracy: 1.04\n",
            "[58/150]: Training Loss: 7.453089464225746e+34, Training Accuracy: 1.02\n",
            "Validation Loss: 7.363201660318024e+34, Validation Accuracy: 1.04\n",
            "[59/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.36800305202506e+34, Validation Accuracy: 1.04\n",
            "[60/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.36741461785972e+34, Validation Accuracy: 1.04\n",
            "[61/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.367097540150991e+34, Validation Accuracy: 1.04\n",
            "[62/150]: Training Loss: inf, Training Accuracy: 1.024\n",
            "Validation Loss: 7.360266091838198e+34, Validation Accuracy: 1.04\n",
            "[63/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.357579927011621e+34, Validation Accuracy: 1.04\n",
            "[64/150]: Training Loss: inf, Training Accuracy: 1.002\n",
            "Validation Loss: 7.3600130568941685e+34, Validation Accuracy: 1.04\n",
            "[65/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.361191245694225e+34, Validation Accuracy: 1.04\n",
            "[66/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.358085336664993e+34, Validation Accuracy: 1.04\n",
            "[67/150]: Training Loss: 7.58769795997563e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.343106922424328e+34, Validation Accuracy: 1.04\n",
            "[68/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.340167062419033e+34, Validation Accuracy: 1.04\n",
            "[69/150]: Training Loss: 7.539564832453689e+34, Training Accuracy: 1.01\n",
            "Validation Loss: 7.3399107263015645e+34, Validation Accuracy: 1.04\n",
            "[70/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.343239299479195e+34, Validation Accuracy: 1.04\n",
            "[71/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.344283460637665e+34, Validation Accuracy: 1.04\n",
            "[72/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3470638739659e+34, Validation Accuracy: 1.04\n",
            "[73/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.337876213111667e+34, Validation Accuracy: 1.04\n",
            "[74/150]: Training Loss: 7.520041527682127e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.339984672586578e+34, Validation Accuracy: 1.04\n",
            "[75/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.341706399593216e+34, Validation Accuracy: 1.04\n",
            "[76/150]: Training Loss: 7.50774223153243e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.344978357646383e+34, Validation Accuracy: 1.04\n",
            "[77/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.34611412636776e+34, Validation Accuracy: 1.04\n",
            "[78/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.34531260145699e+34, Validation Accuracy: 1.04\n",
            "[79/150]: Training Loss: 7.570326728258824e+34, Training Accuracy: 1.008\n",
            "Validation Loss: 7.35118538900336e+34, Validation Accuracy: 1.04\n",
            "[80/150]: Training Loss: 7.392040813047783e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.351906530340912e+34, Validation Accuracy: 1.04\n",
            "[81/150]: Training Loss: 7.55008671334021e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.357009484241519e+34, Validation Accuracy: 1.04\n",
            "[82/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.355688354631593e+34, Validation Accuracy: 1.04\n",
            "[83/150]: Training Loss: inf, Training Accuracy: 1.024\n",
            "Validation Loss: 7.351709120169314e+34, Validation Accuracy: 1.04\n",
            "[84/150]: Training Loss: 7.518704819072938e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.352724726177544e+34, Validation Accuracy: 1.04\n",
            "[85/150]: Training Loss: 7.480363492538042e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.353025628136426e+34, Validation Accuracy: 1.04\n",
            "[86/150]: Training Loss: 7.609761708160119e+34, Training Accuracy: 1.02\n",
            "Validation Loss: 7.355622331162831e+34, Validation Accuracy: 1.04\n",
            "[87/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.351417956672074e+34, Validation Accuracy: 1.04\n",
            "[88/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.3432506885275565e+34, Validation Accuracy: 1.04\n",
            "[89/150]: Training Loss: 7.571858879032369e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.342947970923284e+34, Validation Accuracy: 1.04\n",
            "[90/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.341380243657533e+34, Validation Accuracy: 1.04\n",
            "[91/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3435405315554215e+34, Validation Accuracy: 1.04\n",
            "[92/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.347343978532122e+34, Validation Accuracy: 1.04\n",
            "[93/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.349835869301868e+34, Validation Accuracy: 1.04\n",
            "[94/150]: Training Loss: 7.553363102583877e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.350179851574117e+34, Validation Accuracy: 1.04\n",
            "[95/150]: Training Loss: 7.518843468357338e+34, Training Accuracy: 1.006\n",
            "Validation Loss: 7.349381297719442e+34, Validation Accuracy: 1.04\n",
            "[96/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.350629966572401e+34, Validation Accuracy: 1.04\n",
            "[97/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.350986328245043e+34, Validation Accuracy: 1.04\n",
            "[98/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.349232414797383e+34, Validation Accuracy: 1.04\n",
            "[99/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.3498634341000755e+34, Validation Accuracy: 1.04\n",
            "[100/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.346068900291658e+34, Validation Accuracy: 1.04\n",
            "[101/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.343747350071318e+34, Validation Accuracy: 1.04\n",
            "[102/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.343838462458209e+34, Validation Accuracy: 1.04\n",
            "[103/150]: Training Loss: 7.587162712793105e+34, Training Accuracy: 1.036\n",
            "Validation Loss: 7.34444604342949e+34, Validation Accuracy: 1.04\n",
            "[104/150]: Training Loss: 7.563224723389138e+34, Training Accuracy: 1.024\n",
            "Validation Loss: 7.3440896817568485e+34, Validation Accuracy: 1.04\n",
            "[105/150]: Training Loss: 7.502001541710868e+34, Training Accuracy: 1.012\n",
            "Validation Loss: 7.343770788402729e+34, Validation Accuracy: 1.04\n",
            "[106/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.344186901314601e+34, Validation Accuracy: 1.04\n",
            "[107/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.347785180362123e+34, Validation Accuracy: 1.04\n",
            "[108/150]: Training Loss: 7.541743759284682e+34, Training Accuracy: 1.008\n",
            "Validation Loss: 7.349218714927616e+34, Validation Accuracy: 1.04\n",
            "[109/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.350196357441308e+34, Validation Accuracy: 1.04\n",
            "[110/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.35085609695291e+34, Validation Accuracy: 1.04\n",
            "[111/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.3504860354105e+34, Validation Accuracy: 1.04\n",
            "[112/150]: Training Loss: inf, Training Accuracy: 1.006\n",
            "Validation Loss: 7.350065630973159e+34, Validation Accuracy: 1.04\n",
            "[113/150]: Training Loss: 7.572177010577234e+34, Training Accuracy: 1.014\n",
            "Validation Loss: 7.34878494073785e+34, Validation Accuracy: 1.04\n",
            "[114/150]: Training Loss: 7.602740036076367e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.345721781904642e+34, Validation Accuracy: 1.04\n",
            "[115/150]: Training Loss: 7.637903627666111e+34, Training Accuracy: 1.014\n",
            "Validation Loss: 7.342419287997172e+34, Validation Accuracy: 1.04\n",
            "[116/150]: Training Loss: 7.578869428701e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.340205025913571e+34, Validation Accuracy: 1.04\n",
            "[117/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.33890419852029e+34, Validation Accuracy: 1.04\n",
            "[118/150]: Training Loss: 7.53878203535377e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.3400515213487e+34, Validation Accuracy: 1.04\n",
            "[119/150]: Training Loss: 7.55180291722975e+34, Training Accuracy: 1.006\n",
            "Validation Loss: 7.341673717976179e+34, Validation Accuracy: 1.04\n",
            "[120/150]: Training Loss: 7.52984426063198e+34, Training Accuracy: 1.014\n",
            "Validation Loss: 7.341424314322931e+34, Validation Accuracy: 1.04\n",
            "[121/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.342221382649559e+34, Validation Accuracy: 1.04\n",
            "[122/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.341639550831095e+34, Validation Accuracy: 1.04\n",
            "[123/150]: Training Loss: 7.564221817432473e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.34171366217478e+34, Validation Accuracy: 1.04\n",
            "[124/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.341365883553077e+34, Validation Accuracy: 1.04\n",
            "[125/150]: Training Loss: 7.467476193914916e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.341843068173553e+34, Validation Accuracy: 1.04\n",
            "[126/150]: Training Loss: 7.557965611288089e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.34209494770688e+34, Validation Accuracy: 1.04\n",
            "[127/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.342893501561555e+34, Validation Accuracy: 1.04\n",
            "[128/150]: Training Loss: inf, Training Accuracy: 1.014\n",
            "Validation Loss: 7.343795217086171e+34, Validation Accuracy: 1.04\n",
            "[129/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3440480869715285e+34, Validation Accuracy: 1.04\n",
            "[130/150]: Training Loss: inf, Training Accuracy: 1.026\n",
            "Validation Loss: 7.343806276017188e+34, Validation Accuracy: 1.04\n",
            "[131/150]: Training Loss: 7.511219776509865e+34, Training Accuracy: 1.006\n",
            "Validation Loss: 7.343705425168654e+34, Validation Accuracy: 1.04\n",
            "[132/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.343869988664544e+34, Validation Accuracy: 1.04\n",
            "[133/150]: Training Loss: 7.572846983726496e+34, Training Accuracy: 1.004\n",
            "Validation Loss: 7.34368512295201e+34, Validation Accuracy: 1.04\n",
            "[134/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.3437027842299035e+34, Validation Accuracy: 1.04\n",
            "[135/150]: Training Loss: inf, Training Accuracy: 1.006\n",
            "Validation Loss: 7.34337316206211e+34, Validation Accuracy: 1.04\n",
            "[136/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.343067143284399e+34, Validation Accuracy: 1.04\n",
            "[137/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.343213385267707e+34, Validation Accuracy: 1.04\n",
            "[138/150]: Training Loss: 7.591718865404154e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.343415086964774e+34, Validation Accuracy: 1.04\n",
            "[139/150]: Training Loss: inf, Training Accuracy: 1.024\n",
            "Validation Loss: 7.343418718255556e+34, Validation Accuracy: 1.04\n",
            "[140/150]: Training Loss: 7.5901811534230485e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.343325625164602e+34, Validation Accuracy: 1.04\n",
            "[141/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.343275777445687e+34, Validation Accuracy: 1.04\n",
            "[142/150]: Training Loss: inf, Training Accuracy: 1.014\n",
            "Validation Loss: 7.3432812243818595e+34, Validation Accuracy: 1.04\n",
            "[143/150]: Training Loss: 7.521443789977704e+34, Training Accuracy: 1.008\n",
            "Validation Loss: 7.343387192049222e+34, Validation Accuracy: 1.04\n",
            "[144/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.343500752415492e+34, Validation Accuracy: 1.04\n",
            "[145/150]: Training Loss: 7.489635587251364e+34, Training Accuracy: 1.014\n",
            "Validation Loss: 7.343563144593472e+34, Validation Accuracy: 1.04\n",
            "[146/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.343603253850745e+34, Validation Accuracy: 1.04\n",
            "[147/150]: Training Loss: 7.537443193678667e+34, Training Accuracy: 1.026\n",
            "Validation Loss: 7.343630983707625e+34, Validation Accuracy: 1.04\n",
            "[148/150]: Training Loss: 7.421953062990879e+34, Training Accuracy: 1.024\n",
            "Validation Loss: 7.343639566758564e+34, Validation Accuracy: 1.04\n",
            "[149/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.343644683577394e+34, Validation Accuracy: 1.04\n",
            "[150/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.343644848636065e+34, Validation Accuracy: 1.04\n",
            "**********************************************************************\n",
            "Test Loss: 7.343644848636065e+34, Test Accuracy: 1.04\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁██</td></tr><tr><td>Test Loss</td><td>█▅▁</td></tr><tr><td>Train Accuracy</td><td>█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Loss</td><td>▁▁▁      █ █   █   ████    ██ ██ █  █   </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.04</td></tr><tr><td>Test Loss</td><td>7.343644848636065e+34</td></tr><tr><td>Train Accuracy</td><td>1.01</td></tr><tr><td>Train Loss</td><td>inf</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=8192 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_030841-573n6uz6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 16384, Learning rate: 24.0, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_033254-cneyt8y0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0' target=\"_blank\">batch_size=16384 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.5802256144010105, Training Accuracy: 1.524\n",
            "Validation Loss: 4.505903720855713, Validation Accuracy: 3.24\n",
            "[2/150]: Training Loss: 4.487952305720403, Training Accuracy: 2.782\n",
            "Validation Loss: 4.534458001454671, Validation Accuracy: 1.82\n",
            "[3/150]: Training Loss: 4.498784615443303, Training Accuracy: 2.232\n",
            "Validation Loss: 4.477282365163167, Validation Accuracy: 2.08\n",
            "[4/150]: Training Loss: 4.4941191306481, Training Accuracy: 2.178\n",
            "Validation Loss: 4.612959861755371, Validation Accuracy: 1.29\n",
            "[5/150]: Training Loss: 4.5912819642287035, Training Accuracy: 1.504\n",
            "Validation Loss: 5.598563989003499, Validation Accuracy: 2.19\n",
            "[6/150]: Training Loss: 5.028410141284649, Training Accuracy: 1.632\n",
            "Validation Loss: 7.806542873382568, Validation Accuracy: 1.07\n",
            "[7/150]: Training Loss: 14089.29747926272, Training Accuracy: 1.044\n",
            "Validation Loss: 7.282219409942627, Validation Accuracy: 0.93\n",
            "[8/150]: Training Loss: 461533512.8043683, Training Accuracy: 0.978\n",
            "Validation Loss: 145186464.0, Validation Accuracy: 0.98\n",
            "[9/150]: Training Loss: 1.1580541048823784e+29, Training Accuracy: 0.952\n",
            "Validation Loss: 2.748474842936338e+29, Validation Accuracy: 1.0\n",
            "[10/150]: Training Loss: 2.8978003478935013e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7484746540416787e+29, Validation Accuracy: 1.0\n",
            "[11/150]: Training Loss: 2.828666588105865e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.748474402182133e+29, Validation Accuracy: 1.0\n",
            "[12/150]: Training Loss: 2.8943495766660634e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7484738354981552e+29, Validation Accuracy: 1.0\n",
            "[13/150]: Training Loss: 2.848512485818888e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.748472828059972e+29, Validation Accuracy: 1.0\n",
            "[14/150]: Training Loss: 2.8171856724252214e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.748468672377467e+29, Validation Accuracy: 1.0\n",
            "[15/150]: Training Loss: 2.9237725789099303e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7484595424689335e+29, Validation Accuracy: 1.0\n",
            "[16/150]: Training Loss: 2.8024521069540443e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.748430578621172e+29, Validation Accuracy: 1.0\n",
            "[17/150]: Training Loss: 2.818569224092193e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7483436870778873e+29, Validation Accuracy: 1.0\n",
            "[18/150]: Training Loss: 2.8163175416317275e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7480828865182598e+29, Validation Accuracy: 1.0\n",
            "[19/150]: Training Loss: 2.818046581630586e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7474053213752977e+29, Validation Accuracy: 1.0\n",
            "[20/150]: Training Loss: 2.8566013910414994e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7454263979594298e+29, Validation Accuracy: 1.0\n",
            "[21/150]: Training Loss: 2.8411686828451232e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7395975495271803e+29, Validation Accuracy: 1.0\n",
            "[22/150]: Training Loss: 2.8607326044228643e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.722675988156443e+29, Validation Accuracy: 1.0\n",
            "[23/150]: Training Loss: 2.7987878847615806e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.6741031117411477e+29, Validation Accuracy: 1.0\n",
            "[24/150]: Training Loss: 2.723279453314973e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.535316349808136e+29, Validation Accuracy: 1.0\n",
            "[25/150]: Training Loss: 2.573586814635347e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.1297902431238725e+29, Validation Accuracy: 1.0\n",
            "[26/150]: Training Loss: 2.051805262233517e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 1.0476803956799425e+29, Validation Accuracy: 1.0\n",
            "[27/150]: Training Loss: 8.450376531000046e+28, Training Accuracy: 1.0\n",
            "Validation Loss: 9.264404581011073e+18, Validation Accuracy: 1.0\n",
            "[28/150]: Training Loss: nan, Training Accuracy: 1.044\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[29/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[30/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[31/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[32/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[33/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[34/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[35/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[36/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[37/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[38/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[39/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[40/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[41/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[42/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[43/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[44/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[45/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[46/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[47/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[48/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[49/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[50/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[51/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[52/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[53/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[54/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[55/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[56/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[57/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[58/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[59/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[60/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[61/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[62/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[63/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[64/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[65/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[66/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[67/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[68/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[69/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[70/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[71/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[72/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[73/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[74/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[75/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[76/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[77/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[78/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[79/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[80/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[81/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[82/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[83/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[84/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[85/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[86/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[87/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[88/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[89/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[90/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[91/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[92/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[93/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[94/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[95/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[96/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[97/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[98/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[99/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[100/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[101/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[102/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[103/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[104/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[105/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[106/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[107/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[108/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[109/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[110/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[111/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[112/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[113/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[114/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[115/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[116/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[117/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[118/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[119/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[120/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[121/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[122/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[123/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[124/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[125/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[126/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[127/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[128/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[129/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[130/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[131/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[132/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[133/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[134/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[135/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[136/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[137/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[138/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[139/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[140/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[141/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[142/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[143/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[144/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[145/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[146/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[147/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[148/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[149/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[150/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "**********************************************************************\n",
            "Test Loss: nan, Test Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▃▁</td></tr><tr><td>Train Accuracy</td><td>█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Loss</td><td>▁▁▁████                                 </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.0</td></tr><tr><td>Test Loss</td><td>nan</td></tr><tr><td>Train Accuracy</td><td>1.0</td></tr><tr><td>Train Loss</td><td>nan</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=16384 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_033254-cneyt8y0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 32768, Learning rate: 33.941125496954285, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_035739-zqj7yms0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0' target=\"_blank\">batch_size=32768 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.600700745215783, Training Accuracy: 1.18\n",
            "Validation Loss: 4.5345486005147295, Validation Accuracy: 2.25\n",
            "[2/150]: Training Loss: 4.525542259216309, Training Accuracy: 2.194\n",
            "Validation Loss: 4.480770270029704, Validation Accuracy: 3.63\n",
            "[3/150]: Training Loss: 4.467245468726525, Training Accuracy: 3.348\n",
            "Validation Loss: 4.682004292805989, Validation Accuracy: 2.02\n",
            "[4/150]: Training Loss: 4.584587500645564, Training Accuracy: 2.316\n",
            "Validation Loss: 4.589324951171875, Validation Accuracy: 1.07\n",
            "[5/150]: Training Loss: 4.593533919407771, Training Accuracy: 1.128\n",
            "Validation Loss: 4.565906047821045, Validation Accuracy: 2.24\n",
            "[6/150]: Training Loss: 4.610266098609338, Training Accuracy: 1.65\n",
            "Validation Loss: 5.121236960093181, Validation Accuracy: 1.01\n",
            "[7/150]: Training Loss: 5.261244590465839, Training Accuracy: 1.226\n",
            "Validation Loss: 7.05544392267863, Validation Accuracy: 1.19\n",
            "[8/150]: Training Loss: 6.352289456587571, Training Accuracy: 1.176\n",
            "Validation Loss: 22.820095698038738, Validation Accuracy: 0.83\n",
            "[9/150]: Training Loss: 33.29087433448205, Training Accuracy: 0.906\n",
            "Validation Loss: 54.22376505533854, Validation Accuracy: 1.0\n",
            "[10/150]: Training Loss: 35.99252113929162, Training Accuracy: 0.988\n",
            "Validation Loss: 69.11057027180989, Validation Accuracy: 0.95\n",
            "[11/150]: Training Loss: 76.55324378380409, Training Accuracy: 1.196\n",
            "Validation Loss: 7.983708381652832, Validation Accuracy: 1.06\n",
            "[12/150]: Training Loss: 58.39160236945519, Training Accuracy: 1.032\n",
            "Validation Loss: 164.86418660481772, Validation Accuracy: 0.92\n",
            "[13/150]: Training Loss: 131.58557803814227, Training Accuracy: 0.95\n",
            "Validation Loss: 281.80524190266925, Validation Accuracy: 1.0\n",
            "[14/150]: Training Loss: 178.63248865421002, Training Accuracy: 0.962\n",
            "Validation Loss: 7817948401325397.0, Validation Accuracy: 1.39\n",
            "[15/150]: Training Loss: 4967409836682319.0, Training Accuracy: 1.34\n",
            "Validation Loss: 2.8247563537114726e+17, Validation Accuracy: 1.0\n",
            "[16/150]: Training Loss: 1.7259852552660077e+17, Training Accuracy: 0.982\n",
            "Validation Loss: 7061819945735509.0, Validation Accuracy: 0.96\n",
            "[17/150]: Training Loss: 7.113184439761936e+16, Training Accuracy: 0.924\n",
            "Validation Loss: 64496.6328125, Validation Accuracy: 1.0\n",
            "[18/150]: Training Loss: nan, Training Accuracy: 1.014\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[19/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[20/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[21/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[22/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[23/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[24/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[25/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[26/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[27/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[28/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[29/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[30/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[31/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[32/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[33/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[34/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[35/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[36/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[37/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[38/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[39/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[40/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[41/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[42/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[43/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[44/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[45/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[46/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[47/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[48/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[49/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[50/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[51/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[52/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[53/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[54/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[55/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[56/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[57/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[58/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[59/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[60/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[61/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[62/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[63/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[64/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[65/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[66/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[67/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[68/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[69/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[70/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[71/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[72/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[73/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[74/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[75/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[76/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[77/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[78/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[79/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[80/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[81/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[82/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[83/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[84/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[85/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[86/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[87/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[88/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[89/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[90/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[91/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[92/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[93/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[94/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[95/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[96/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[97/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[98/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[99/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[100/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[101/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[102/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[103/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[104/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[105/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[106/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[107/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[108/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[109/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[110/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[111/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[112/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[113/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[114/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[115/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[116/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[117/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[118/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[119/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[120/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[121/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[122/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[123/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[124/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[125/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[126/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[127/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[128/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[129/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[130/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[131/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[132/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[133/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[134/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[135/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[136/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[137/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[138/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[139/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[140/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[141/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[142/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[143/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[144/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[145/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[146/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[147/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[148/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[149/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[150/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "**********************************************************************\n",
            "Test Loss: nan, Test Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▃▁</td></tr><tr><td>Train Accuracy</td><td>▇█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Loss</td><td>▁▁▁▁█                                   </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.0</td></tr><tr><td>Test Loss</td><td>nan</td></tr><tr><td>Train Accuracy</td><td>1.0</td></tr><tr><td>Train Loss</td><td>nan</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=32768 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_035739-zqj7yms0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# check the lr updates from paper 18\n",
        "\n",
        "num_epochs = 150\n",
        "lr = 1.5\n",
        "wd = 1e-03\n",
        "batch_sizes = [512, 1024, 2048, 4096, 8192, 16384 , 32768]\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "  # Root square scale-up of learning rate\n",
        "  k = (batch_size / 64.0) ** 0.5\n",
        "  print('='*50)\n",
        "  print(f'Batch size: {batch_size}, Learning rate: {lr*k}, Weight decay: {wd}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {\n",
        "    'batch_size': batch_size,\n",
        "    'learning_rate': lr*k,\n",
        "    'weight_decay' : wd\n",
        "  }\n",
        "  if batch_size <= 4096:\n",
        "  \n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= batch_size)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LARS(model.parameters(), lr= lr*k, weight_decay=wd, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LARS_Large_Batches',\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True\n",
        "    )\n",
        "  else:\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= 4096)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LARS(model.parameters(), lr= lr*k, weight_decay=wd, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    accumulation_steps = batch_size // 4096\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LARS_Large_Batches',\n",
        "        accumulation_steps=accumulation_steps,\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LAMB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from typing import Optional, Union, Callable, List\n",
        "\n",
        "class LAMB(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements LAMB (Layer-wise Adaptive Moments) optimizer.\n",
        "\n",
        "    Args:\n",
        "        params (iterable): Iterable of parameters to optimize or dicts defining parameter groups.\n",
        "        learning_rate (Union[float, Callable], optional): The learning rate. Default is 0.001.\n",
        "        beta_1 (float, optional): The exponential decay rate for the 1st moment estimates. Default is 0.9.\n",
        "        beta_2 (float, optional): The exponential decay rate for the 2nd moment estimates. Default is 0.999.\n",
        "        epsilon (float, optional): A small constant for numerical stability. Default is 1e-6.\n",
        "        weight_decay (float, optional): Weight decay. Default is 0.0.\n",
        "        exclude_from_weight_decay (Optional[List[str]], optional): List of regex patterns of variables excluded from weight decay. Variables whose name contain a substring matching the pattern will be excluded. Default is None.\n",
        "        exclude_from_layer_adaptation (Optional[List[str]], optional): List of regex patterns of variables excluded from layer adaptation. Variables whose name contain a substring matching the pattern will be excluded. Default is None.\n",
        "        name (str, optional): Optional name for the operations created when applying gradients. Defaults to \"LAMB\".\n",
        "        **kwargs: Keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip gradients by value, `decay` is included for backward compatibility to allow time inverse decay of learning rate. `lr` is included for backward compatibility, recommended to use `learning_rate` instead.\n",
        "\n",
        "    Note:\n",
        "        - If \"weight_decay_rate\" is found in kwargs, it will be renamed to \"weight_decay\", and will be deprecated in Addons 0.18.\n",
        "        - If exclude_from_layer_adaptation is None, it will be set to exclude_from_weight_decay.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr: Union[float, Callable] = 0.001,\n",
        "        beta_1: float = 0.9,\n",
        "        beta_2: float = 0.999,\n",
        "        epsilon: float = 1e-6,\n",
        "        wd: float = 0.0,\n",
        "        exclude_from_weight_decay: Optional[List[str]] = None,\n",
        "        exclude_from_layer_adaptation: Optional[List[str]] = None,\n",
        "        name: str = \"LAMB\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a new instance of the LAMB optimizer.\n",
        "        \"\"\"\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            betas=(beta_1, beta_2),\n",
        "            eps=epsilon,\n",
        "            wd=wd,\n",
        "            **kwargs)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if\n",
        "        # the arg is None.\n",
        "        if exclude_from_layer_adaptation:\n",
        "            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
        "        else:\n",
        "            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
        "\n",
        "    def _compute_update(self, p, grad, state, group):\n",
        "        \"\"\"\n",
        "        Computes the update for a given parameter.\n",
        "\n",
        "        Args:\n",
        "            p (Tensor): The parameter to be updated.\n",
        "            grad (Tensor): The gradient of the parameter.\n",
        "            state (dict): A dictionary containing information about the optimization state.\n",
        "            group (dict): A dictionary containing the optimization parameters.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The computed update for the parameter.\n",
        "        \"\"\"\n",
        "        # State initialization\n",
        "        if len(state) == 0:\n",
        "            state['step'] = 0\n",
        "            # Exponential moving average of gradient values\n",
        "            state['exp_avg'] = torch.zeros_like(p.data)\n",
        "            # Exponential moving average of squared gradient values\n",
        "            state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "        exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "        beta1, beta2 = group['betas']\n",
        "\n",
        "        state['step'] += 1\n",
        "\n",
        "        # Decay the first and second moment running average coefficient\n",
        "        exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "        exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "        denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "        update = exp_avg / denom\n",
        "\n",
        "        # LAMB layer-wise adaptation\n",
        "        r1 = p.data.pow(2).sum().sqrt()\n",
        "        r2 = update.pow(2).sum().sqrt()\n",
        "        r = torch.where(r1 == 0, torch.zeros_like(r1), r1 / r2)\n",
        "\n",
        "        return r * update\n",
        "\n",
        "    def _update_params(self, p, update, step_size, weight_decay):\n",
        "        \"\"\"\n",
        "        Updates the parameters with the computed update.\n",
        "\n",
        "        Args:\n",
        "            p (Tensor): The parameter to be updated.\n",
        "            update (Tensor): The computed update for the parameter.\n",
        "            step_size (float): The step size for the update.\n",
        "            weight_decay (float): The weight decay factor.\n",
        "        \"\"\"\n",
        "        if weight_decay != 0:\n",
        "            p.data.add_(-weight_decay * step_size, p.data)\n",
        "\n",
        "        p.data.add_(-step_size, update)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model and returns the loss. Default is None.\n",
        "        \"\"\"\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('LAMB does not support sparse gradients.')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                update = self._compute_update(p, grad, state, group)\n",
        "                self._update_params(p, update, group['lr'], group['weight_decay'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LAMB Hyperparameter Tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ftxikW3Lr2ya"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.0009 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_045009-kw5xm0f9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9' target=\"_blank\">learning_rate=0.0009 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.366185345458985, Training Accuracy: 3.775\n",
            "Validation Loss: 4.140061999582182, Validation Accuracy: 6.19\n",
            "[2/150]: Training Loss: 3.9872212955474855, Training Accuracy: 9.2825\n",
            "Validation Loss: 3.9175391182018693, Validation Accuracy: 10.13\n",
            "[3/150]: Training Loss: 3.7751855819702147, Training Accuracy: 12.73\n",
            "Validation Loss: 3.754828929901123, Validation Accuracy: 13.44\n",
            "[4/150]: Training Loss: 3.6031822479248046, Training Accuracy: 15.7675\n",
            "Validation Loss: 3.5609892857302525, Validation Accuracy: 16.33\n",
            "[5/150]: Training Loss: 3.4478089206695555, Training Accuracy: 18.66\n",
            "Validation Loss: 3.4119288389849816, Validation Accuracy: 19.45\n",
            "[6/150]: Training Loss: 3.3203197284698485, Training Accuracy: 20.7075\n",
            "Validation Loss: 3.349654642639646, Validation Accuracy: 20.47\n",
            "[7/150]: Training Loss: 3.20732957611084, Training Accuracy: 22.7475\n",
            "Validation Loss: 3.2386614349996967, Validation Accuracy: 22.79\n",
            "[8/150]: Training Loss: 3.115479539489746, Training Accuracy: 24.5875\n",
            "Validation Loss: 3.1584747019846726, Validation Accuracy: 24.53\n",
            "[9/150]: Training Loss: 3.0234272315979003, Training Accuracy: 26.28\n",
            "Validation Loss: 3.0975672788680737, Validation Accuracy: 24.97\n",
            "[10/150]: Training Loss: 2.9408509815216064, Training Accuracy: 27.665\n",
            "Validation Loss: 3.0618672537955507, Validation Accuracy: 25.33\n",
            "[11/150]: Training Loss: 2.862503829956055, Training Accuracy: 29.3325\n",
            "Validation Loss: 2.9638838965422027, Validation Accuracy: 27.41\n",
            "[12/150]: Training Loss: 2.790407749938965, Training Accuracy: 30.48\n",
            "Validation Loss: 2.937404105617742, Validation Accuracy: 28.32\n",
            "[13/150]: Training Loss: 2.724927402114868, Training Accuracy: 31.9425\n",
            "Validation Loss: 2.9041963838467932, Validation Accuracy: 28.5\n",
            "[14/150]: Training Loss: 2.659171875, Training Accuracy: 33.2275\n",
            "Validation Loss: 2.8527053769227044, Validation Accuracy: 29.6\n",
            "[15/150]: Training Loss: 2.604087335395813, Training Accuracy: 34.1075\n",
            "Validation Loss: 2.8513991179739593, Validation Accuracy: 30.15\n",
            "[16/150]: Training Loss: 2.5427039728164673, Training Accuracy: 35.57\n",
            "Validation Loss: 2.8172519510718668, Validation Accuracy: 30.73\n",
            "[17/150]: Training Loss: 2.4821195234298705, Training Accuracy: 36.73\n",
            "Validation Loss: 2.826693563704278, Validation Accuracy: 31.28\n",
            "[18/150]: Training Loss: 2.4249122804641723, Training Accuracy: 37.97\n",
            "Validation Loss: 2.78142261353268, Validation Accuracy: 32.01\n",
            "[19/150]: Training Loss: 2.366035011482239, Training Accuracy: 39.02\n",
            "Validation Loss: 2.7907780021618884, Validation Accuracy: 31.84\n",
            "[20/150]: Training Loss: 2.3027314464569093, Training Accuracy: 40.4925\n",
            "Validation Loss: 2.740103484718663, Validation Accuracy: 32.92\n",
            "[21/150]: Training Loss: 2.24520486240387, Training Accuracy: 41.56\n",
            "Validation Loss: 2.720843028111063, Validation Accuracy: 33.71\n",
            "[22/150]: Training Loss: 2.194983146095276, Training Accuracy: 42.9375\n",
            "Validation Loss: 2.7611854592705987, Validation Accuracy: 32.67\n",
            "[23/150]: Training Loss: 2.138135533905029, Training Accuracy: 44.1975\n",
            "Validation Loss: 2.7068749218230037, Validation Accuracy: 33.35\n",
            "[24/150]: Training Loss: 2.078735979270935, Training Accuracy: 45.3275\n",
            "Validation Loss: 2.6949741673317686, Validation Accuracy: 34.58\n",
            "[25/150]: Training Loss: 2.0237128454208375, Training Accuracy: 46.48\n",
            "Validation Loss: 2.700145792809262, Validation Accuracy: 33.95\n",
            "[26/150]: Training Loss: 1.9667144546508788, Training Accuracy: 47.8375\n",
            "Validation Loss: 2.749180986623096, Validation Accuracy: 34.02\n",
            "[27/150]: Training Loss: 1.9086554452896118, Training Accuracy: 49.35\n",
            "Validation Loss: 2.719582610828861, Validation Accuracy: 34.45\n",
            "[28/150]: Training Loss: 1.845149391746521, Training Accuracy: 50.64\n",
            "Validation Loss: 2.735584658422288, Validation Accuracy: 34.52\n",
            "[29/150]: Training Loss: 1.7947315074920653, Training Accuracy: 51.665\n",
            "Validation Loss: 2.7848551835224127, Validation Accuracy: 33.98\n",
            "[30/150]: Training Loss: 1.7361212677001954, Training Accuracy: 53.24\n",
            "Validation Loss: 2.7989868426778513, Validation Accuracy: 34.65\n",
            "[31/150]: Training Loss: 1.675447853088379, Training Accuracy: 54.63\n",
            "Validation Loss: 2.805454480420252, Validation Accuracy: 35.16\n",
            "[32/150]: Training Loss: 1.6181456073760987, Training Accuracy: 55.7275\n",
            "Validation Loss: 2.825571666097945, Validation Accuracy: 35.24\n",
            "[33/150]: Training Loss: 1.5572484771728516, Training Accuracy: 57.33\n",
            "Validation Loss: 2.895953078178843, Validation Accuracy: 34.87\n",
            "[34/150]: Training Loss: 1.493730352783203, Training Accuracy: 58.9\n",
            "Validation Loss: 2.8819164500874317, Validation Accuracy: 34.54\n",
            "[35/150]: Training Loss: 1.4407835181236268, Training Accuracy: 60.1325\n",
            "Validation Loss: 2.9298907556351583, Validation Accuracy: 35.13\n",
            "[36/150]: Training Loss: 1.378046295261383, Training Accuracy: 61.6725\n",
            "Validation Loss: 2.9877944615236514, Validation Accuracy: 34.88\n",
            "[37/150]: Training Loss: 1.3214307213783265, Training Accuracy: 63.3825\n",
            "Validation Loss: 3.01296287281498, Validation Accuracy: 34.97\n",
            "[38/150]: Training Loss: 1.2566315541267394, Training Accuracy: 64.46\n",
            "Validation Loss: 3.1075331208052908, Validation Accuracy: 34.96\n",
            "[39/150]: Training Loss: 1.2033088095664979, Training Accuracy: 66.1\n",
            "Validation Loss: 3.1721465040923684, Validation Accuracy: 34.01\n",
            "[40/150]: Training Loss: 1.1419879460334779, Training Accuracy: 67.7475\n",
            "Validation Loss: 3.260179937265481, Validation Accuracy: 34.51\n",
            "[41/150]: Training Loss: 1.0848600325584412, Training Accuracy: 69.2325\n",
            "Validation Loss: 3.281414782165722, Validation Accuracy: 34.07\n",
            "[42/150]: Training Loss: 1.0264957049369812, Training Accuracy: 70.73\n",
            "Validation Loss: 3.4151626255861514, Validation Accuracy: 34.15\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 17.67243060336751, Test Accuracy: 14.49\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▃█▁▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▂▃▃▃▃▃▃▃▂▃▃</td></tr><tr><td>Test Loss</td><td>▆▁▂▂▄▄▆█▇▇▆▇█▇▇▇▆▇▇▇▇▇▆▆▆▆▆▆▆▇▇▆▆▆▆▆▆▆▆▆</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>14.49</td></tr><tr><td>Test Loss</td><td>17.67243</td></tr><tr><td>Train Accuracy</td><td>70.73</td></tr><tr><td>Train Loss</td><td>1.0265</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.0009 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_045009-kw5xm0f9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.00095 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_045608-cwudz2wj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj' target=\"_blank\">learning_rate=0.00095 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.36243955039978, Training Accuracy: 3.8475\n",
            "Validation Loss: 4.152171432592307, Validation Accuracy: 6.23\n",
            "[2/150]: Training Loss: 3.988539717102051, Training Accuracy: 9.035\n",
            "Validation Loss: 3.8899211443153914, Validation Accuracy: 10.42\n",
            "[3/150]: Training Loss: 3.784552033996582, Training Accuracy: 12.44\n",
            "Validation Loss: 3.716481655266634, Validation Accuracy: 13.63\n",
            "[4/150]: Training Loss: 3.6115343948364256, Training Accuracy: 15.38\n",
            "Validation Loss: 3.575136796684022, Validation Accuracy: 15.25\n",
            "[5/150]: Training Loss: 3.464410584640503, Training Accuracy: 18.1625\n",
            "Validation Loss: 3.448981456695848, Validation Accuracy: 17.92\n",
            "[6/150]: Training Loss: 3.340861389923096, Training Accuracy: 20.2875\n",
            "Validation Loss: 3.3443257702384024, Validation Accuracy: 19.79\n",
            "[7/150]: Training Loss: 3.240240854263306, Training Accuracy: 22.0975\n",
            "Validation Loss: 3.2861741910314866, Validation Accuracy: 21.07\n",
            "[8/150]: Training Loss: 3.1493673683166503, Training Accuracy: 23.8725\n",
            "Validation Loss: 3.1970221844448408, Validation Accuracy: 22.88\n",
            "[9/150]: Training Loss: 3.058048994064331, Training Accuracy: 25.4\n",
            "Validation Loss: 3.149918096080707, Validation Accuracy: 23.89\n",
            "[10/150]: Training Loss: 2.980970076370239, Training Accuracy: 26.93\n",
            "Validation Loss: 3.05195216008812, Validation Accuracy: 25.52\n",
            "[11/150]: Training Loss: 2.8978964962005613, Training Accuracy: 28.4275\n",
            "Validation Loss: 3.035321120243923, Validation Accuracy: 26.01\n",
            "[12/150]: Training Loss: 2.828321962738037, Training Accuracy: 29.7825\n",
            "Validation Loss: 3.033455335410537, Validation Accuracy: 26.14\n",
            "[13/150]: Training Loss: 2.764588646316528, Training Accuracy: 31.0525\n",
            "Validation Loss: 2.9178414527018357, Validation Accuracy: 28.5\n",
            "[14/150]: Training Loss: 2.6963481563568115, Training Accuracy: 32.6275\n",
            "Validation Loss: 2.8597835826266342, Validation Accuracy: 29.48\n",
            "[15/150]: Training Loss: 2.6279782932281495, Training Accuracy: 33.5125\n",
            "Validation Loss: 2.820923639710542, Validation Accuracy: 30.76\n",
            "[16/150]: Training Loss: 2.567752400970459, Training Accuracy: 35.0025\n",
            "Validation Loss: 2.8166574232137886, Validation Accuracy: 30.67\n",
            "[17/150]: Training Loss: 2.5122242719650267, Training Accuracy: 36.11\n",
            "Validation Loss: 2.77275866611748, Validation Accuracy: 31.51\n",
            "[18/150]: Training Loss: 2.448093600654602, Training Accuracy: 37.37\n",
            "Validation Loss: 2.7573567454222663, Validation Accuracy: 31.87\n",
            "[19/150]: Training Loss: 2.390553472328186, Training Accuracy: 38.6925\n",
            "Validation Loss: 2.779167422823086, Validation Accuracy: 31.58\n",
            "[20/150]: Training Loss: 2.329650112724304, Training Accuracy: 39.8\n",
            "Validation Loss: 2.7296412940237933, Validation Accuracy: 32.77\n",
            "[21/150]: Training Loss: 2.2680850820541383, Training Accuracy: 41.2075\n",
            "Validation Loss: 2.7711537537301423, Validation Accuracy: 32.31\n",
            "[22/150]: Training Loss: 2.205855764579773, Training Accuracy: 42.4775\n",
            "Validation Loss: 2.710599136959975, Validation Accuracy: 33.08\n",
            "[23/150]: Training Loss: 2.153862490653992, Training Accuracy: 43.845\n",
            "Validation Loss: 2.6846724543601845, Validation Accuracy: 34.22\n",
            "[24/150]: Training Loss: 2.0879069725036623, Training Accuracy: 45.07\n",
            "Validation Loss: 2.6652191763470885, Validation Accuracy: 34.62\n",
            "[25/150]: Training Loss: 2.027932558250427, Training Accuracy: 46.0825\n",
            "Validation Loss: 2.7143203483265674, Validation Accuracy: 34.04\n",
            "[26/150]: Training Loss: 1.9607762811660767, Training Accuracy: 47.975\n",
            "Validation Loss: 2.6897484147624606, Validation Accuracy: 34.96\n",
            "[27/150]: Training Loss: 1.9115407361984253, Training Accuracy: 48.8575\n",
            "Validation Loss: 2.7138490722437574, Validation Accuracy: 34.41\n",
            "[28/150]: Training Loss: 1.8502281684875488, Training Accuracy: 50.4525\n",
            "Validation Loss: 2.699836605673383, Validation Accuracy: 35.02\n",
            "[29/150]: Training Loss: 1.790220089149475, Training Accuracy: 51.585\n",
            "Validation Loss: 2.7410409693505353, Validation Accuracy: 34.77\n",
            "[30/150]: Training Loss: 1.7332203540802003, Training Accuracy: 53.0625\n",
            "Validation Loss: 2.6978999216845083, Validation Accuracy: 35.51\n",
            "[31/150]: Training Loss: 1.6659312999725342, Training Accuracy: 54.48\n",
            "Validation Loss: 2.7799882926758688, Validation Accuracy: 35.39\n",
            "[32/150]: Training Loss: 1.6054936313629151, Training Accuracy: 55.9275\n",
            "Validation Loss: 2.783534232218554, Validation Accuracy: 35.38\n",
            "[33/150]: Training Loss: 1.5524930331230165, Training Accuracy: 57.2425\n",
            "Validation Loss: 2.7876193948612094, Validation Accuracy: 35.43\n",
            "[34/150]: Training Loss: 1.4864114666938781, Training Accuracy: 58.62\n",
            "Validation Loss: 2.8515792378954066, Validation Accuracy: 34.97\n",
            "[35/150]: Training Loss: 1.4250763600349425, Training Accuracy: 60.31\n",
            "Validation Loss: 2.89135210453325, Validation Accuracy: 34.83\n",
            "[36/150]: Training Loss: 1.3677435276031493, Training Accuracy: 61.51\n",
            "Validation Loss: 2.919705820691054, Validation Accuracy: 36.01\n",
            "[37/150]: Training Loss: 1.3075296778678893, Training Accuracy: 63.435\n",
            "Validation Loss: 3.000214212259669, Validation Accuracy: 35.23\n",
            "[38/150]: Training Loss: 1.2455093726158142, Training Accuracy: 64.7925\n",
            "Validation Loss: 3.0685715766469386, Validation Accuracy: 34.47\n",
            "[39/150]: Training Loss: 1.1875538174629212, Training Accuracy: 66.195\n",
            "Validation Loss: 3.0860758040361342, Validation Accuracy: 35.17\n",
            "[40/150]: Training Loss: 1.1235356809616088, Training Accuracy: 67.92\n",
            "Validation Loss: 3.142808488979461, Validation Accuracy: 35.12\n",
            "[41/150]: Training Loss: 1.0638824738502501, Training Accuracy: 69.31\n",
            "Validation Loss: 3.2775286580346954, Validation Accuracy: 35.6\n",
            "[42/150]: Training Loss: 1.0043726112365723, Training Accuracy: 71.1\n",
            "Validation Loss: 3.332795017084498, Validation Accuracy: 34.53\n",
            "[43/150]: Training Loss: 0.9537097842216492, Training Accuracy: 72.4\n",
            "Validation Loss: 3.416984627960594, Validation Accuracy: 34.84\n",
            "[44/150]: Training Loss: 0.8938642192840576, Training Accuracy: 74.145\n",
            "Validation Loss: 3.5402958104564886, Validation Accuracy: 34.94\n",
            "[45/150]: Training Loss: 0.8376890470981598, Training Accuracy: 75.4875\n",
            "Validation Loss: 3.6570517561238285, Validation Accuracy: 34.43\n",
            "[46/150]: Training Loss: 0.7843643071174622, Training Accuracy: 76.875\n",
            "Validation Loss: 3.745219696858886, Validation Accuracy: 34.67\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 24.405659256467395, Test Accuracy: 13.44\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>██▁▂▂▁▁▁▂▃▃▄▃▃▃▄▄▄▅▅▄▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>Test Loss</td><td>▄▁▄▇▆▇█▇▄▂▂▂▄▃▄▃▃▄▃▃▃▂▂▁▂▂▂▃▃▃▃▂▃▃▂▂▂▂▂▂</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>Train Loss</td><td>█▇▇▇▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>13.44</td></tr><tr><td>Test Loss</td><td>24.40566</td></tr><tr><td>Train Accuracy</td><td>76.875</td></tr><tr><td>Train Loss</td><td>0.78436</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.00095 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_045608-cwudz2wj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_050220-wgvfp3rx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx' target=\"_blank\">learning_rate=0.001 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.351185768127442, Training Accuracy: 3.94\n",
            "Validation Loss: 4.125218055810139, Validation Accuracy: 6.94\n",
            "[2/150]: Training Loss: 3.934326037979126, Training Accuracy: 9.675\n",
            "Validation Loss: 3.8352680054439863, Validation Accuracy: 11.53\n",
            "[3/150]: Training Loss: 3.6920593730926514, Training Accuracy: 14.25\n",
            "Validation Loss: 3.6194094821905636, Validation Accuracy: 15.04\n",
            "[4/150]: Training Loss: 3.5119912174224854, Training Accuracy: 17.235\n",
            "Validation Loss: 3.498675950773203, Validation Accuracy: 17.26\n",
            "[5/150]: Training Loss: 3.3595370391845703, Training Accuracy: 19.9025\n",
            "Validation Loss: 3.3320715184424334, Validation Accuracy: 20.65\n",
            "[6/150]: Training Loss: 3.2359037544250486, Training Accuracy: 22.135\n",
            "Validation Loss: 3.2282794827868226, Validation Accuracy: 21.83\n",
            "[7/150]: Training Loss: 3.1236792266845703, Training Accuracy: 24.1875\n",
            "Validation Loss: 3.1484436852157494, Validation Accuracy: 23.69\n",
            "[8/150]: Training Loss: 3.0264555549621583, Training Accuracy: 25.99\n",
            "Validation Loss: 3.0823915505864816, Validation Accuracy: 24.76\n",
            "[9/150]: Training Loss: 2.9406655250549316, Training Accuracy: 27.4325\n",
            "Validation Loss: 3.018990922126041, Validation Accuracy: 25.75\n",
            "[10/150]: Training Loss: 2.8602982753753663, Training Accuracy: 28.8825\n",
            "Validation Loss: 2.982261516486004, Validation Accuracy: 26.83\n",
            "[11/150]: Training Loss: 2.7865394050598145, Training Accuracy: 30.62\n",
            "Validation Loss: 2.881638531472273, Validation Accuracy: 28.86\n",
            "[12/150]: Training Loss: 2.713952407836914, Training Accuracy: 32.0525\n",
            "Validation Loss: 2.8773576226204063, Validation Accuracy: 28.51\n",
            "[13/150]: Training Loss: 2.6428218954086304, Training Accuracy: 33.4175\n",
            "Validation Loss: 2.852285245421586, Validation Accuracy: 29.44\n",
            "[14/150]: Training Loss: 2.5819866678237915, Training Accuracy: 34.2325\n",
            "Validation Loss: 2.8152025581165483, Validation Accuracy: 29.99\n",
            "[15/150]: Training Loss: 2.5168129930496215, Training Accuracy: 36.01\n",
            "Validation Loss: 2.7263220571408606, Validation Accuracy: 32.4\n",
            "[16/150]: Training Loss: 2.456671890640259, Training Accuracy: 37.025\n",
            "Validation Loss: 2.7360152697107596, Validation Accuracy: 31.89\n",
            "[17/150]: Training Loss: 2.394518960952759, Training Accuracy: 38.405\n",
            "Validation Loss: 2.693733840231683, Validation Accuracy: 33.43\n",
            "[18/150]: Training Loss: 2.329904039955139, Training Accuracy: 40.0125\n",
            "Validation Loss: 2.679514690569252, Validation Accuracy: 33.49\n",
            "[19/150]: Training Loss: 2.271217462730408, Training Accuracy: 41.0875\n",
            "Validation Loss: 2.7137394892941615, Validation Accuracy: 32.98\n",
            "[20/150]: Training Loss: 2.2047285720825194, Training Accuracy: 42.575\n",
            "Validation Loss: 2.6841572416815787, Validation Accuracy: 32.96\n",
            "[21/150]: Training Loss: 2.1471473873138427, Training Accuracy: 43.83\n",
            "Validation Loss: 2.6751675947456603, Validation Accuracy: 33.61\n",
            "[22/150]: Training Loss: 2.085570357322693, Training Accuracy: 45.055\n",
            "Validation Loss: 2.6481618797703153, Validation Accuracy: 34.55\n",
            "[23/150]: Training Loss: 2.0214510499954224, Training Accuracy: 46.6625\n",
            "Validation Loss: 2.638173457163914, Validation Accuracy: 35.26\n",
            "[24/150]: Training Loss: 1.9607698831558227, Training Accuracy: 48.02\n",
            "Validation Loss: 2.621039168849872, Validation Accuracy: 35.84\n",
            "[25/150]: Training Loss: 1.8981156831741333, Training Accuracy: 49.45\n",
            "Validation Loss: 2.62651793896013, Validation Accuracy: 36.22\n",
            "[26/150]: Training Loss: 1.8291910402297973, Training Accuracy: 51.235\n",
            "Validation Loss: 2.6563384396255394, Validation Accuracy: 35.62\n",
            "[27/150]: Training Loss: 1.7651165187835693, Training Accuracy: 52.31\n",
            "Validation Loss: 2.639584611935221, Validation Accuracy: 36.55\n",
            "[28/150]: Training Loss: 1.7005936141967772, Training Accuracy: 54.275\n",
            "Validation Loss: 2.7095768307424652, Validation Accuracy: 36.06\n",
            "[29/150]: Training Loss: 1.6383595853805542, Training Accuracy: 55.6225\n",
            "Validation Loss: 2.7259993150735355, Validation Accuracy: 35.64\n",
            "[30/150]: Training Loss: 1.5726151014328003, Training Accuracy: 57.1225\n",
            "Validation Loss: 2.7611397246646274, Validation Accuracy: 36.11\n",
            "[31/150]: Training Loss: 1.508023483467102, Training Accuracy: 58.3025\n",
            "Validation Loss: 2.7436208489594187, Validation Accuracy: 35.66\n",
            "[32/150]: Training Loss: 1.4352122190475465, Training Accuracy: 60.295\n",
            "Validation Loss: 2.8204059456564057, Validation Accuracy: 36.43\n",
            "[33/150]: Training Loss: 1.3806078368186951, Training Accuracy: 61.585\n",
            "Validation Loss: 2.876485233853577, Validation Accuracy: 34.85\n",
            "[34/150]: Training Loss: 1.3117412397384645, Training Accuracy: 63.325\n",
            "Validation Loss: 2.8764415660481544, Validation Accuracy: 36.16\n",
            "[35/150]: Training Loss: 1.2477456188201905, Training Accuracy: 64.815\n",
            "Validation Loss: 2.959255384032134, Validation Accuracy: 35.83\n",
            "[36/150]: Training Loss: 1.178989047718048, Training Accuracy: 66.845\n",
            "Validation Loss: 3.055402055667464, Validation Accuracy: 35.68\n",
            "[37/150]: Training Loss: 1.1195641567230226, Training Accuracy: 68.255\n",
            "Validation Loss: 3.092683092044417, Validation Accuracy: 35.74\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 19.867750665944094, Test Accuracy: 12.24\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>██▂▃▁▁▂▁▁▁▂▂▁▁▂▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>Test Loss</td><td>▂▁▂▃▅▅▆▇▇▆▆▆█▇██▇█▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>12.24</td></tr><tr><td>Test Loss</td><td>19.86775</td></tr><tr><td>Train Accuracy</td><td>68.255</td></tr><tr><td>Train Loss</td><td>1.11956</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_050220-wgvfp3rx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.0015 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_050736-gnfp4sxh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh' target=\"_blank\">learning_rate=0.0015 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.262849729156494, Training Accuracy: 4.9725\n",
            "Validation Loss: 3.9988777105975304, Validation Accuracy: 9.32\n",
            "[2/150]: Training Loss: 3.787733755874634, Training Accuracy: 12.5275\n",
            "Validation Loss: 3.666901140455987, Validation Accuracy: 14.13\n",
            "[3/150]: Training Loss: 3.5222632232666014, Training Accuracy: 16.975\n",
            "Validation Loss: 3.430799526773441, Validation Accuracy: 18.28\n",
            "[4/150]: Training Loss: 3.3261954177856445, Training Accuracy: 20.0275\n",
            "Validation Loss: 3.268884590476941, Validation Accuracy: 20.25\n",
            "[5/150]: Training Loss: 3.1651099575042725, Training Accuracy: 22.825\n",
            "Validation Loss: 3.140599861266507, Validation Accuracy: 23.63\n",
            "[6/150]: Training Loss: 3.0220409996032713, Training Accuracy: 25.57\n",
            "Validation Loss: 3.062152488975768, Validation Accuracy: 24.92\n",
            "[7/150]: Training Loss: 2.9046311878204345, Training Accuracy: 27.855\n",
            "Validation Loss: 2.9628464735237654, Validation Accuracy: 26.95\n",
            "[8/150]: Training Loss: 2.7910710647583006, Training Accuracy: 30.235\n",
            "Validation Loss: 2.8832454301749064, Validation Accuracy: 28.14\n",
            "[9/150]: Training Loss: 2.687530333709717, Training Accuracy: 32.2625\n",
            "Validation Loss: 2.8348786056421367, Validation Accuracy: 30.07\n",
            "[10/150]: Training Loss: 2.590290707015991, Training Accuracy: 34.1175\n",
            "Validation Loss: 2.7776184780582502, Validation Accuracy: 30.52\n",
            "[11/150]: Training Loss: 2.499223603057861, Training Accuracy: 36.12\n",
            "Validation Loss: 2.726046642680077, Validation Accuracy: 31.91\n",
            "[12/150]: Training Loss: 2.4020754039764403, Training Accuracy: 37.9725\n",
            "Validation Loss: 2.7060916757887337, Validation Accuracy: 32.51\n",
            "[13/150]: Training Loss: 2.302425242996216, Training Accuracy: 40.3075\n",
            "Validation Loss: 2.660059646436363, Validation Accuracy: 33.06\n",
            "[14/150]: Training Loss: 2.2182941759109496, Training Accuracy: 42.03\n",
            "Validation Loss: 2.647583461870813, Validation Accuracy: 34.15\n",
            "[15/150]: Training Loss: 2.1231776348114013, Training Accuracy: 43.9025\n",
            "Validation Loss: 2.6469775620539475, Validation Accuracy: 34.32\n",
            "[16/150]: Training Loss: 2.030972394180298, Training Accuracy: 45.9525\n",
            "Validation Loss: 2.7037209963342947, Validation Accuracy: 32.86\n",
            "[17/150]: Training Loss: 1.9431676984786987, Training Accuracy: 47.91\n",
            "Validation Loss: 2.5850457507333937, Validation Accuracy: 36.27\n",
            "[18/150]: Training Loss: 1.8313071418762208, Training Accuracy: 50.255\n",
            "Validation Loss: 2.699979861071155, Validation Accuracy: 35.5\n",
            "[19/150]: Training Loss: 1.7440621503829956, Training Accuracy: 52.4725\n",
            "Validation Loss: 2.661453550028953, Validation Accuracy: 35.96\n",
            "[20/150]: Training Loss: 1.637422308921814, Training Accuracy: 54.8325\n",
            "Validation Loss: 2.7208387426509977, Validation Accuracy: 35.5\n",
            "[21/150]: Training Loss: 1.5425473594665526, Training Accuracy: 57.03\n",
            "Validation Loss: 2.766743406368669, Validation Accuracy: 35.3\n",
            "[22/150]: Training Loss: 1.4425379981994628, Training Accuracy: 59.42\n",
            "Validation Loss: 2.7848968460301684, Validation Accuracy: 36.36\n",
            "[23/150]: Training Loss: 1.346246865272522, Training Accuracy: 61.82\n",
            "Validation Loss: 2.918702983552483, Validation Accuracy: 35.2\n",
            "[24/150]: Training Loss: 1.2415513612747193, Training Accuracy: 64.48\n",
            "Validation Loss: 3.0769596373199657, Validation Accuracy: 34.71\n",
            "[25/150]: Training Loss: 1.1484121152877809, Training Accuracy: 66.9425\n",
            "Validation Loss: 3.111480196570135, Validation Accuracy: 35.56\n",
            "[26/150]: Training Loss: 1.049374456501007, Training Accuracy: 69.2\n",
            "Validation Loss: 3.2441035076311437, Validation Accuracy: 35.12\n",
            "[27/150]: Training Loss: 0.9539197593688965, Training Accuracy: 71.64\n",
            "Validation Loss: 3.3398004808243673, Validation Accuracy: 34.83\n",
            "[28/150]: Training Loss: 0.8686957097053528, Training Accuracy: 73.885\n",
            "Validation Loss: 3.526861637261263, Validation Accuracy: 35.38\n",
            "[29/150]: Training Loss: 0.7777377708911896, Training Accuracy: 76.4\n",
            "Validation Loss: 3.74925019938475, Validation Accuracy: 34.95\n",
            "[30/150]: Training Loss: 0.6971015272140503, Training Accuracy: 78.4225\n",
            "Validation Loss: 3.921683767039305, Validation Accuracy: 34.49\n",
            "[31/150]: Training Loss: 0.6319020359992981, Training Accuracy: 80.37\n",
            "Validation Loss: 4.265976667404175, Validation Accuracy: 33.7\n",
            "[32/150]: Training Loss: 0.5592198015213012, Training Accuracy: 82.41\n",
            "Validation Loss: 4.3743809985507065, Validation Accuracy: 34.21\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 32.802743437943185, Test Accuracy: 15.7\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▅█▂▅▄▂▁▁▁▁▂▃▃▂▂▃▃▃▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▄▄▄▄▄▄</td></tr><tr><td>Test Loss</td><td>▁▄▇▆▇▇███▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>15.7</td></tr><tr><td>Test Loss</td><td>32.80274</td></tr><tr><td>Train Accuracy</td><td>82.41</td></tr><tr><td>Train Loss</td><td>0.55922</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.0015 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_050736-gnfp4sxh/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.002 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_051217-y92gs5m0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0' target=\"_blank\">learning_rate=0.002 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.222224911499024, Training Accuracy: 5.3975\n",
            "Validation Loss: 3.937301506662065, Validation Accuracy: 9.22\n",
            "[2/150]: Training Loss: 3.7261308227539063, Training Accuracy: 13.09\n",
            "Validation Loss: 3.593827167134376, Validation Accuracy: 15.34\n",
            "[3/150]: Training Loss: 3.4419474758148194, Training Accuracy: 18.2775\n",
            "Validation Loss: 3.3379262875599465, Validation Accuracy: 19.89\n",
            "[4/150]: Training Loss: 3.242010182952881, Training Accuracy: 21.865\n",
            "Validation Loss: 3.2397833553848754, Validation Accuracy: 21.54\n",
            "[5/150]: Training Loss: 3.074250465774536, Training Accuracy: 24.8525\n",
            "Validation Loss: 3.2218855049959414, Validation Accuracy: 22.29\n",
            "[6/150]: Training Loss: 2.9360343181610107, Training Accuracy: 27.3175\n",
            "Validation Loss: 2.9865669110778033, Validation Accuracy: 26.78\n",
            "[7/150]: Training Loss: 2.801424619293213, Training Accuracy: 30.0175\n",
            "Validation Loss: 2.881270451150882, Validation Accuracy: 28.82\n",
            "[8/150]: Training Loss: 2.688298546409607, Training Accuracy: 32.05\n",
            "Validation Loss: 2.8182313017025113, Validation Accuracy: 30.07\n",
            "[9/150]: Training Loss: 2.5692677375793456, Training Accuracy: 34.84\n",
            "Validation Loss: 2.7818179950592623, Validation Accuracy: 30.94\n",
            "[10/150]: Training Loss: 2.454072025489807, Training Accuracy: 37.1425\n",
            "Validation Loss: 2.741547807766374, Validation Accuracy: 32.13\n",
            "[11/150]: Training Loss: 2.341491235733032, Training Accuracy: 39.3875\n",
            "Validation Loss: 2.735630521349087, Validation Accuracy: 32.51\n",
            "[12/150]: Training Loss: 2.2293454483032225, Training Accuracy: 41.4725\n",
            "Validation Loss: 2.690371492106444, Validation Accuracy: 33.37\n",
            "[13/150]: Training Loss: 2.1135470266342162, Training Accuracy: 43.935\n",
            "Validation Loss: 2.7198572918108312, Validation Accuracy: 34.01\n",
            "[14/150]: Training Loss: 1.9979984771728516, Training Accuracy: 46.1675\n",
            "Validation Loss: 2.7182075719165195, Validation Accuracy: 34.3\n",
            "[15/150]: Training Loss: 1.8890921760559083, Training Accuracy: 49.265\n",
            "Validation Loss: 2.7245476724235873, Validation Accuracy: 34.8\n",
            "[16/150]: Training Loss: 1.7773400522232055, Training Accuracy: 51.53\n",
            "Validation Loss: 2.749075835677469, Validation Accuracy: 34.12\n",
            "[17/150]: Training Loss: 1.676476739501953, Training Accuracy: 53.69\n",
            "Validation Loss: 2.8863297662917216, Validation Accuracy: 34.72\n",
            "[18/150]: Training Loss: 1.5655937635421753, Training Accuracy: 56.19\n",
            "Validation Loss: 2.8975180911410385, Validation Accuracy: 35.07\n",
            "[19/150]: Training Loss: 1.4489710484504699, Training Accuracy: 59.165\n",
            "Validation Loss: 2.9350054689273715, Validation Accuracy: 34.48\n",
            "[20/150]: Training Loss: 1.3497021337509156, Training Accuracy: 61.3125\n",
            "Validation Loss: 3.0767477864672426, Validation Accuracy: 34.78\n",
            "[21/150]: Training Loss: 1.2329449357032776, Training Accuracy: 64.19\n",
            "Validation Loss: 3.22720639113408, Validation Accuracy: 33.72\n",
            "[22/150]: Training Loss: 1.1406824831962585, Training Accuracy: 66.7375\n",
            "Validation Loss: 3.406786183642734, Validation Accuracy: 33.27\n",
            "[23/150]: Training Loss: 1.044043209552765, Training Accuracy: 69.1625\n",
            "Validation Loss: 3.664752894905722, Validation Accuracy: 32.75\n",
            "[24/150]: Training Loss: 0.9549756371498108, Training Accuracy: 71.1525\n",
            "Validation Loss: 3.7399451838936777, Validation Accuracy: 32.73\n",
            "[25/150]: Training Loss: 0.8772646800518036, Training Accuracy: 73.25\n",
            "Validation Loss: 3.9628275534149946, Validation Accuracy: 33.22\n",
            "[26/150]: Training Loss: 0.7976884460449218, Training Accuracy: 75.4625\n",
            "Validation Loss: 4.143224073823091, Validation Accuracy: 32.26\n",
            "[27/150]: Training Loss: 0.7417024869441986, Training Accuracy: 76.86\n",
            "Validation Loss: 4.497031521645321, Validation Accuracy: 32.38\n",
            "[28/150]: Training Loss: 0.682380113697052, Training Accuracy: 78.7575\n",
            "Validation Loss: 4.677568254956774, Validation Accuracy: 32.16\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 35.528370875461846, Test Accuracy: 11.85\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▅▁▂▁▁▁▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▁▃▆▅▆▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>11.85</td></tr><tr><td>Test Loss</td><td>35.52837</td></tr><tr><td>Train Accuracy</td><td>78.7575</td></tr><tr><td>Train Loss</td><td>0.68238</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.002 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_051217-y92gs5m0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "learning_rates = [9e-04, 95e-05, 1e-03, 15e-04, 2e-03]\n",
        "wd = 4e-04\n",
        "for lr in learning_rates:\n",
        "\n",
        "  print('='*50)\n",
        "  print(f'Hyperparameter with lr:{lr} and wd:{wd}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {'learning_rate': lr,\n",
        "                      'weight_decay' : wd\n",
        "                      }\n",
        "  # Load the model\n",
        "  model = LeNet5().to(device)\n",
        "\n",
        "  # Optimizer and scheduler setup\n",
        "  optimizer = LAMB(model.parameters(), lr=lr, weight_decay=wd)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "  # Training\n",
        "  run_training(num_epochs,\n",
        "                model,\n",
        "                train_loader,\n",
        "                validation_loader,\n",
        "                test_loader,\n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                criterion,\n",
        "                device,\n",
        "                'LAMB-HyperParameterTuning',\n",
        "                hyperparameters=hyperparameters,\n",
        "                is_wandb = True,\n",
        "                n_epochs_stop = 10\n",
        "  )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LAMB BaseLine B-Size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "90b585318084475b9543b3226230d373",
            "5f3a33bc6a334c9e8c5886caa5f015ca",
            "46c9d83bf8af443bb376ed4858ce35f7",
            "2402e482be7e484fa98ae3f60f68d95a",
            "e8040d3a83de41319a2a836def914b1b",
            "0b4b10bb8bff4d40afa8df981440fe43",
            "3b0b3657d30547d7abe702d6c1e7b7c4",
            "17ac4efa9cf148e5be69edbc5b0bdecf",
            "f5fd8db807e144578a2e20762d7369d0",
            "32c759d7fff64701a6ad61b38ac63187",
            "557cf2de74314915b7204d9055fa6987",
            "c5bb24dc0f56483b88a0efbfa2a1c8ee",
            "67aa8f8c2b244c9a9a3bb11caf491247",
            "09b762f5ba784066a9408ffcfdea5b2e",
            "6a345ff7386643eeb7a5b190d69d2e0e",
            "8f4783e2060a4295bb68036a4a7310d9",
            "aa6aa819c1fd4776ac47af588aaaaacd",
            "64224c777f01418e904dcd30ecd7c5ef",
            "c1dbd12595384bc6a0240aaeb16014dc",
            "6aa92cf922724ac08c45385ff8a58447",
            "99929754d3094d03a5f311177ac26cb3",
            "9ea517158b974c5da17899e3fea4fc3b",
            "bb2ee36dd3c04926b10b832928d7d0a0",
            "9d88657b39fd4b169d61b1b8d240e6ff"
          ]
        },
        "id": "7sUsHkiHgURG",
        "outputId": "3829f79a-3d06-48ee-fea5-a00c18f8744b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:uqgfhn52) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁▄▆▇█</td></tr><tr><td>Train Loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>22.668</td></tr><tr><td>Train Loss</td><td>3.18342</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.0015026019100214134 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/uqgfhn52' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/uqgfhn52</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_051725-uqgfhn52/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:uqgfhn52). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_052010-62vz5e00</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00' target=\"_blank\">learning_rate=0.0015 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.170614353226274, Training Accuracy: 6.554\n",
            "Validation Loss: 3.8419871087286883, Validation Accuracy: 11.55\n",
            "[2/150]: Training Loss: 3.73113738697813, Training Accuracy: 13.326\n",
            "Validation Loss: 3.5141605601948536, Validation Accuracy: 17.24\n",
            "[3/150]: Training Loss: 3.4963422099037853, Training Accuracy: 17.148\n",
            "Validation Loss: 3.329459586720558, Validation Accuracy: 21.14\n",
            "[4/150]: Training Loss: 3.32537763838268, Training Accuracy: 20.17\n",
            "Validation Loss: 3.1594036445496188, Validation Accuracy: 23.38\n",
            "[5/150]: Training Loss: 3.1821046982274948, Training Accuracy: 22.724\n",
            "Validation Loss: 3.024734082495331, Validation Accuracy: 25.42\n",
            "[6/150]: Training Loss: 3.0576654736648132, Training Accuracy: 24.838\n",
            "Validation Loss: 2.917888655024729, Validation Accuracy: 27.48\n",
            "[7/150]: Training Loss: 2.9461568444586166, Training Accuracy: 26.996\n",
            "Validation Loss: 2.8047209317517128, Validation Accuracy: 29.77\n",
            "[8/150]: Training Loss: 2.860208951298843, Training Accuracy: 28.704\n",
            "Validation Loss: 2.7236505541831826, Validation Accuracy: 30.98\n",
            "[9/150]: Training Loss: 2.7667418916512023, Training Accuracy: 30.53\n",
            "Validation Loss: 2.6368031137308496, Validation Accuracy: 33.13\n",
            "[10/150]: Training Loss: 2.6879202682343895, Training Accuracy: 31.998\n",
            "Validation Loss: 2.536134794259527, Validation Accuracy: 35.32\n",
            "[11/150]: Training Loss: 2.61040174458033, Training Accuracy: 33.4\n",
            "Validation Loss: 2.4932953764678567, Validation Accuracy: 36.12\n",
            "[12/150]: Training Loss: 2.546117087306879, Training Accuracy: 34.894\n",
            "Validation Loss: 2.4478625689342524, Validation Accuracy: 37.12\n",
            "[13/150]: Training Loss: 2.4885423736803975, Training Accuracy: 35.89\n",
            "Validation Loss: 2.4252931342762745, Validation Accuracy: 37.55\n",
            "[14/150]: Training Loss: 2.4414472836057852, Training Accuracy: 37.026\n",
            "Validation Loss: 2.3621389410298343, Validation Accuracy: 39.2\n",
            "[15/150]: Training Loss: 2.3877986550636, Training Accuracy: 37.84\n",
            "Validation Loss: 2.3034424402151896, Validation Accuracy: 40.21\n",
            "[16/150]: Training Loss: 2.3379326612138382, Training Accuracy: 39.018\n",
            "Validation Loss: 2.272260831419829, Validation Accuracy: 41.24\n",
            "[17/150]: Training Loss: 2.300395258247395, Training Accuracy: 39.968\n",
            "Validation Loss: 2.2585707120834644, Validation Accuracy: 40.6\n",
            "[18/150]: Training Loss: 2.254751528315532, Training Accuracy: 40.976\n",
            "Validation Loss: 2.2186143770339384, Validation Accuracy: 42.07\n",
            "[19/150]: Training Loss: 2.2090765830805843, Training Accuracy: 42.026\n",
            "Validation Loss: 2.1868586600965756, Validation Accuracy: 42.61\n",
            "[20/150]: Training Loss: 2.183374183562101, Training Accuracy: 42.532\n",
            "Validation Loss: 2.184412774007032, Validation Accuracy: 42.58\n",
            "[21/150]: Training Loss: 2.1446489868566507, Training Accuracy: 43.298\n",
            "Validation Loss: 2.144960983543639, Validation Accuracy: 43.98\n",
            "[22/150]: Training Loss: 2.1038296910198144, Training Accuracy: 44.234\n",
            "Validation Loss: 2.14452546493263, Validation Accuracy: 44.01\n",
            "[23/150]: Training Loss: 2.0707620507311026, Training Accuracy: 44.894\n",
            "Validation Loss: 2.098228795513226, Validation Accuracy: 45.23\n",
            "[24/150]: Training Loss: 2.043932166093451, Training Accuracy: 45.518\n",
            "Validation Loss: 2.0813077680624215, Validation Accuracy: 45.38\n",
            "[25/150]: Training Loss: 2.0066164360021994, Training Accuracy: 46.274\n",
            "Validation Loss: 2.075542102953431, Validation Accuracy: 45.85\n",
            "[26/150]: Training Loss: 1.9791695739302184, Training Accuracy: 46.988\n",
            "Validation Loss: 2.0774263060016995, Validation Accuracy: 45.28\n",
            "[27/150]: Training Loss: 1.948626570232079, Training Accuracy: 47.584\n",
            "Validation Loss: 2.0519798697939344, Validation Accuracy: 46.31\n",
            "[28/150]: Training Loss: 1.9167836584398508, Training Accuracy: 48.236\n",
            "Validation Loss: 2.0201504807563344, Validation Accuracy: 46.88\n",
            "[29/150]: Training Loss: 1.8909891124271676, Training Accuracy: 48.752\n",
            "Validation Loss: 2.040525470569635, Validation Accuracy: 46.57\n",
            "[30/150]: Training Loss: 1.8726914470153087, Training Accuracy: 49.618\n",
            "Validation Loss: 2.0113253760489687, Validation Accuracy: 47.44\n",
            "[31/150]: Training Loss: 1.8444365990131408, Training Accuracy: 50.096\n",
            "Validation Loss: 2.0178289990516225, Validation Accuracy: 47.23\n",
            "[32/150]: Training Loss: 1.8157384923047117, Training Accuracy: 50.462\n",
            "Validation Loss: 1.9997166759648901, Validation Accuracy: 47.55\n",
            "[33/150]: Training Loss: 1.7931588938474046, Training Accuracy: 51.29\n",
            "Validation Loss: 1.9950028824958073, Validation Accuracy: 47.93\n",
            "[34/150]: Training Loss: 1.770590110355631, Training Accuracy: 51.824\n",
            "Validation Loss: 1.993164770162789, Validation Accuracy: 48.12\n",
            "[35/150]: Training Loss: 1.7467985225607976, Training Accuracy: 52.03\n",
            "Validation Loss: 1.9869080242837311, Validation Accuracy: 47.9\n",
            "[36/150]: Training Loss: 1.7260596163742377, Training Accuracy: 52.556\n",
            "Validation Loss: 1.9724081488931255, Validation Accuracy: 48.61\n",
            "[37/150]: Training Loss: 1.7053224419998696, Training Accuracy: 53.142\n",
            "Validation Loss: 1.9826935643603087, Validation Accuracy: 48.29\n",
            "[38/150]: Training Loss: 1.67785135681367, Training Accuracy: 53.82\n",
            "Validation Loss: 1.9470138079041888, Validation Accuracy: 49.88\n",
            "[39/150]: Training Loss: 1.6623152171254463, Training Accuracy: 54.074\n",
            "Validation Loss: 1.9769204634769706, Validation Accuracy: 48.87\n",
            "[40/150]: Training Loss: 1.6364040380853522, Training Accuracy: 54.652\n",
            "Validation Loss: 1.9554897744184847, Validation Accuracy: 49.57\n",
            "[41/150]: Training Loss: 1.6296177715291758, Training Accuracy: 54.87\n",
            "Validation Loss: 1.9755018660976629, Validation Accuracy: 49.42\n",
            "[42/150]: Training Loss: 1.6172699540319955, Training Accuracy: 55.052\n",
            "Validation Loss: 1.9689236112460968, Validation Accuracy: 49.6\n",
            "[43/150]: Training Loss: 1.5901159763793506, Training Accuracy: 55.706\n",
            "Validation Loss: 1.9709602108426914, Validation Accuracy: 49.39\n",
            "[44/150]: Training Loss: 1.5688080241917954, Training Accuracy: 56.494\n",
            "Validation Loss: 1.944002225140857, Validation Accuracy: 50.24\n",
            "[45/150]: Training Loss: 1.5468424783490808, Training Accuracy: 57.02\n",
            "Validation Loss: 1.9809717410688947, Validation Accuracy: 49.68\n",
            "[46/150]: Training Loss: 1.5288271441331605, Training Accuracy: 57.372\n",
            "Validation Loss: 1.9420389149599016, Validation Accuracy: 50.41\n",
            "[47/150]: Training Loss: 1.527083154453341, Training Accuracy: 57.472\n",
            "Validation Loss: 1.9570772792123685, Validation Accuracy: 49.71\n",
            "[48/150]: Training Loss: 1.5049020071773578, Training Accuracy: 57.818\n",
            "Validation Loss: 1.954159548328181, Validation Accuracy: 50.38\n",
            "[49/150]: Training Loss: 1.48557850992893, Training Accuracy: 58.67\n",
            "Validation Loss: 1.934345255232161, Validation Accuracy: 50.33\n",
            "[50/150]: Training Loss: 1.461377340052134, Training Accuracy: 58.866\n",
            "Validation Loss: 1.9670750221629052, Validation Accuracy: 50.09\n",
            "[51/150]: Training Loss: 1.4531892761397545, Training Accuracy: 59.148\n",
            "Validation Loss: 1.9439144878630426, Validation Accuracy: 50.57\n",
            "[52/150]: Training Loss: 1.4365854823528348, Training Accuracy: 59.4\n",
            "Validation Loss: 1.9623823484797387, Validation Accuracy: 50.49\n",
            "[53/150]: Training Loss: 1.4231898410393453, Training Accuracy: 59.886\n",
            "Validation Loss: 1.9759472274476555, Validation Accuracy: 50.14\n",
            "[54/150]: Training Loss: 1.408926703664653, Training Accuracy: 60.114\n",
            "Validation Loss: 1.924273559242297, Validation Accuracy: 51.21\n",
            "[55/150]: Training Loss: 1.3876726690613095, Training Accuracy: 60.616\n",
            "Validation Loss: 1.9458329692767684, Validation Accuracy: 50.63\n",
            "[56/150]: Training Loss: 1.375196378478004, Training Accuracy: 61.12\n",
            "Validation Loss: 1.9410056468028172, Validation Accuracy: 51.4\n",
            "[57/150]: Training Loss: 1.3586041162081082, Training Accuracy: 61.634\n",
            "Validation Loss: 1.9649552509283563, Validation Accuracy: 50.61\n",
            "[58/150]: Training Loss: 1.3581253530271828, Training Accuracy: 61.412\n",
            "Validation Loss: 1.9388997919240576, Validation Accuracy: 51.27\n",
            "[59/150]: Training Loss: 1.3380669855400729, Training Accuracy: 61.932\n",
            "Validation Loss: 1.9778220661126884, Validation Accuracy: 50.91\n",
            "[60/150]: Training Loss: 1.3257979125622898, Training Accuracy: 62.056\n",
            "Validation Loss: 1.9304483665782175, Validation Accuracy: 51.44\n",
            "[61/150]: Training Loss: 1.316665162896866, Training Accuracy: 62.458\n",
            "Validation Loss: 1.9412351808730204, Validation Accuracy: 51.61\n",
            "[62/150]: Training Loss: 1.2979835793947625, Training Accuracy: 62.832\n",
            "Validation Loss: 1.9522758213577756, Validation Accuracy: 51.6\n",
            "[63/150]: Training Loss: 1.2900130991130838, Training Accuracy: 63.092\n",
            "Validation Loss: 1.9639577113898696, Validation Accuracy: 51.23\n",
            "[64/150]: Training Loss: 1.2804708784955847, Training Accuracy: 63.436\n",
            "Validation Loss: 1.9787959422275518, Validation Accuracy: 51.42\n",
            "[65/150]: Training Loss: 1.255796139959789, Training Accuracy: 63.948\n",
            "Validation Loss: 1.9797948439409778, Validation Accuracy: 51.63\n",
            "[66/150]: Training Loss: 1.25389591042343, Training Accuracy: 64.23\n",
            "Validation Loss: 1.97708031554131, Validation Accuracy: 52.19\n",
            "[67/150]: Training Loss: 1.2357215599330795, Training Accuracy: 64.366\n",
            "Validation Loss: 1.9641305322100402, Validation Accuracy: 52.22\n",
            "[68/150]: Training Loss: 1.2240538022402303, Training Accuracy: 64.862\n",
            "Validation Loss: 1.9815536661512534, Validation Accuracy: 51.47\n",
            "[69/150]: Training Loss: 1.2087419308969736, Training Accuracy: 65.136\n",
            "Validation Loss: 1.979879951021474, Validation Accuracy: 51.77\n",
            "[70/150]: Training Loss: 1.1984577734604516, Training Accuracy: 65.552\n",
            "Validation Loss: 2.005889118856685, Validation Accuracy: 51.82\n",
            "[71/150]: Training Loss: 1.184933677506264, Training Accuracy: 66.026\n",
            "Validation Loss: 1.9805337274150483, Validation Accuracy: 51.89\n",
            "[72/150]: Training Loss: 1.1769923466398282, Training Accuracy: 65.964\n",
            "Validation Loss: 2.0164508561419834, Validation Accuracy: 51.97\n",
            "[73/150]: Training Loss: 1.1639809905720488, Training Accuracy: 66.116\n",
            "Validation Loss: 2.0093113024523306, Validation Accuracy: 51.75\n",
            "[74/150]: Training Loss: 1.1529584921077085, Training Accuracy: 66.55\n",
            "Validation Loss: 2.017468781987573, Validation Accuracy: 51.53\n",
            "[75/150]: Training Loss: 1.1453676276347216, Training Accuracy: 66.758\n",
            "Validation Loss: 2.011715882902692, Validation Accuracy: 51.48\n",
            "[76/150]: Training Loss: 1.1254654065574832, Training Accuracy: 67.396\n",
            "Validation Loss: 2.0016076625532406, Validation Accuracy: 51.93\n",
            "[77/150]: Training Loss: 1.1255306048161537, Training Accuracy: 67.168\n",
            "Validation Loss: 2.037281950567938, Validation Accuracy: 51.54\n",
            "[78/150]: Training Loss: 1.107588133391212, Training Accuracy: 67.898\n",
            "Validation Loss: 2.0207215995545598, Validation Accuracy: 52.25\n",
            "[79/150]: Training Loss: 1.0974091778478354, Training Accuracy: 67.988\n",
            "Validation Loss: 2.0399385941256383, Validation Accuracy: 52.27\n",
            "[80/150]: Training Loss: 1.0841643994726489, Training Accuracy: 68.482\n",
            "Validation Loss: 2.0396120669735467, Validation Accuracy: 51.88\n",
            "[81/150]: Training Loss: 1.086065025128367, Training Accuracy: 68.472\n",
            "Validation Loss: 2.0602660406926634, Validation Accuracy: 52.23\n",
            "[82/150]: Training Loss: 1.068733287741766, Training Accuracy: 68.792\n",
            "Validation Loss: 2.038099098357425, Validation Accuracy: 52.32\n",
            "[83/150]: Training Loss: 1.0567113834116466, Training Accuracy: 69.072\n",
            "Validation Loss: 2.0566319333519907, Validation Accuracy: 52.0\n",
            "[84/150]: Training Loss: 1.0568410671123154, Training Accuracy: 69.164\n",
            "Validation Loss: 2.031417142054078, Validation Accuracy: 52.23\n",
            "[85/150]: Training Loss: 1.0462324290019471, Training Accuracy: 69.518\n",
            "Validation Loss: 2.039376434247205, Validation Accuracy: 52.34\n",
            "[86/150]: Training Loss: 1.0314530159353905, Training Accuracy: 69.866\n",
            "Validation Loss: 2.0698205103540115, Validation Accuracy: 52.02\n",
            "[87/150]: Training Loss: 1.0182559825956363, Training Accuracy: 70.136\n",
            "Validation Loss: 2.063336102825821, Validation Accuracy: 52.83\n",
            "[88/150]: Training Loss: 1.010449574350396, Training Accuracy: 70.25\n",
            "Validation Loss: 2.077486888618226, Validation Accuracy: 52.3\n",
            "[89/150]: Training Loss: 1.0035706778316547, Training Accuracy: 70.786\n",
            "Validation Loss: 2.095621291998845, Validation Accuracy: 52.44\n",
            "[90/150]: Training Loss: 0.9991849294251494, Training Accuracy: 70.812\n",
            "Validation Loss: 2.069785719464539, Validation Accuracy: 52.4\n",
            "[91/150]: Training Loss: 0.993351522995078, Training Accuracy: 70.858\n",
            "Validation Loss: 2.1024031069627993, Validation Accuracy: 52.29\n",
            "[92/150]: Training Loss: 0.9726321048215222, Training Accuracy: 71.618\n",
            "Validation Loss: 2.084312925672835, Validation Accuracy: 52.53\n",
            "[93/150]: Training Loss: 0.97342309279515, Training Accuracy: 71.502\n",
            "Validation Loss: 2.0946866744642803, Validation Accuracy: 52.66\n",
            "[94/150]: Training Loss: 0.963803626920866, Training Accuracy: 71.718\n",
            "Validation Loss: 2.110511908105984, Validation Accuracy: 52.0\n",
            "[95/150]: Training Loss: 0.9580129440635672, Training Accuracy: 71.984\n",
            "Validation Loss: 2.111149025570815, Validation Accuracy: 52.13\n",
            "[96/150]: Training Loss: 0.9513878270869365, Training Accuracy: 72.204\n",
            "Validation Loss: 2.1115560136782894, Validation Accuracy: 52.52\n",
            "[97/150]: Training Loss: 0.938226883673607, Training Accuracy: 72.542\n",
            "Validation Loss: 2.1242388023692333, Validation Accuracy: 52.65\n",
            "[98/150]: Training Loss: 0.938509788811969, Training Accuracy: 72.476\n",
            "Validation Loss: 2.1135271635784467, Validation Accuracy: 52.82\n",
            "[99/150]: Training Loss: 0.9283110121326983, Training Accuracy: 72.674\n",
            "Validation Loss: 2.125285923860635, Validation Accuracy: 52.06\n",
            "[100/150]: Training Loss: 0.9165082442790956, Training Accuracy: 73.184\n",
            "Validation Loss: 2.1202975777304096, Validation Accuracy: 52.38\n",
            "[101/150]: Training Loss: 0.9109463012584335, Training Accuracy: 72.998\n",
            "Validation Loss: 2.136450658937928, Validation Accuracy: 52.42\n",
            "[102/150]: Training Loss: 0.9098046744418571, Training Accuracy: 73.26\n",
            "Validation Loss: 2.1267288725846893, Validation Accuracy: 52.86\n",
            "[103/150]: Training Loss: 0.8908002294237961, Training Accuracy: 73.838\n",
            "Validation Loss: 2.125086438883642, Validation Accuracy: 52.34\n",
            "[104/150]: Training Loss: 0.8947355315432219, Training Accuracy: 73.496\n",
            "Validation Loss: 2.13778414771815, Validation Accuracy: 52.62\n",
            "[105/150]: Training Loss: 0.8788276913830692, Training Accuracy: 74.106\n",
            "Validation Loss: 2.153025217876313, Validation Accuracy: 52.42\n",
            "[106/150]: Training Loss: 0.8798901442524112, Training Accuracy: 74.082\n",
            "Validation Loss: 2.1569059441803367, Validation Accuracy: 52.12\n",
            "[107/150]: Training Loss: 0.8666944346388282, Training Accuracy: 74.51\n",
            "Validation Loss: 2.1608687851839004, Validation Accuracy: 52.57\n",
            "[108/150]: Training Loss: 0.863055142485882, Training Accuracy: 74.528\n",
            "Validation Loss: 2.152739950805713, Validation Accuracy: 52.64\n",
            "[109/150]: Training Loss: 0.8662346141874943, Training Accuracy: 74.474\n",
            "Validation Loss: 2.14941197823567, Validation Accuracy: 52.71\n",
            "[110/150]: Training Loss: 0.8472307874342365, Training Accuracy: 75.102\n",
            "Validation Loss: 2.1724644390640746, Validation Accuracy: 52.69\n",
            "[111/150]: Training Loss: 0.8462071400469221, Training Accuracy: 75.26\n",
            "Validation Loss: 2.166171987345264, Validation Accuracy: 52.5\n",
            "[112/150]: Training Loss: 0.8393054546221442, Training Accuracy: 75.156\n",
            "Validation Loss: 2.16822886315121, Validation Accuracy: 52.79\n",
            "[113/150]: Training Loss: 0.835232794132379, Training Accuracy: 75.426\n",
            "Validation Loss: 2.1755759412316, Validation Accuracy: 52.8\n",
            "[114/150]: Training Loss: 0.8176318519484357, Training Accuracy: 75.594\n",
            "Validation Loss: 2.1797922325741714, Validation Accuracy: 52.63\n",
            "[115/150]: Training Loss: 0.8166115129618998, Training Accuracy: 75.654\n",
            "Validation Loss: 2.1856938577761316, Validation Accuracy: 52.44\n",
            "[116/150]: Training Loss: 0.8185412460733252, Training Accuracy: 75.768\n",
            "Validation Loss: 2.178807595732865, Validation Accuracy: 52.51\n",
            "[117/150]: Training Loss: 0.819738648203023, Training Accuracy: 75.68\n",
            "Validation Loss: 2.196637267519714, Validation Accuracy: 52.53\n",
            "[118/150]: Training Loss: 0.8139445048463924, Training Accuracy: 76.114\n",
            "Validation Loss: 2.1727140124436395, Validation Accuracy: 52.14\n",
            "[119/150]: Training Loss: 0.804041659359432, Training Accuracy: 76.238\n",
            "Validation Loss: 2.189874280789855, Validation Accuracy: 52.54\n",
            "[120/150]: Training Loss: 0.8070289515473349, Training Accuracy: 75.962\n",
            "Validation Loss: 2.1908302922157725, Validation Accuracy: 52.74\n",
            "[121/150]: Training Loss: 0.7972686002626443, Training Accuracy: 76.462\n",
            "Validation Loss: 2.183206063167305, Validation Accuracy: 52.9\n",
            "[122/150]: Training Loss: 0.7931420375090426, Training Accuracy: 76.7\n",
            "Validation Loss: 2.1991693654637428, Validation Accuracy: 52.88\n",
            "[123/150]: Training Loss: 0.7942516611284002, Training Accuracy: 76.382\n",
            "Validation Loss: 2.1899024916302627, Validation Accuracy: 52.96\n",
            "[124/150]: Training Loss: 0.7901588624243236, Training Accuracy: 76.912\n",
            "Validation Loss: 2.1976229795225106, Validation Accuracy: 52.76\n",
            "[125/150]: Training Loss: 0.784064800881059, Training Accuracy: 76.852\n",
            "Validation Loss: 2.197301295152895, Validation Accuracy: 52.94\n",
            "[126/150]: Training Loss: 0.7810554346236427, Training Accuracy: 77.018\n",
            "Validation Loss: 2.1978831883448704, Validation Accuracy: 53.05\n",
            "[127/150]: Training Loss: 0.7694210947855659, Training Accuracy: 77.242\n",
            "Validation Loss: 2.2037578973041216, Validation Accuracy: 52.8\n",
            "[128/150]: Training Loss: 0.771505500425768, Training Accuracy: 77.276\n",
            "Validation Loss: 2.204603326548437, Validation Accuracy: 52.61\n",
            "[129/150]: Training Loss: 0.7755528422039183, Training Accuracy: 77.05\n",
            "Validation Loss: 2.2066474340523885, Validation Accuracy: 52.48\n",
            "[130/150]: Training Loss: 0.7633897851190299, Training Accuracy: 77.462\n",
            "Validation Loss: 2.20657627294018, Validation Accuracy: 52.87\n",
            "[131/150]: Training Loss: 0.7619557766734487, Training Accuracy: 77.74\n",
            "Validation Loss: 2.2119038287241746, Validation Accuracy: 52.87\n",
            "[132/150]: Training Loss: 0.7639070795015301, Training Accuracy: 77.628\n",
            "Validation Loss: 2.214540675946861, Validation Accuracy: 52.93\n",
            "[133/150]: Training Loss: 0.7627752876800039, Training Accuracy: 77.624\n",
            "Validation Loss: 2.2126305232382126, Validation Accuracy: 52.92\n",
            "[134/150]: Training Loss: 0.762916150681503, Training Accuracy: 77.468\n",
            "Validation Loss: 2.213455740813237, Validation Accuracy: 52.87\n",
            "[135/150]: Training Loss: 0.7556408758434798, Training Accuracy: 77.858\n",
            "Validation Loss: 2.2093218823147427, Validation Accuracy: 52.78\n",
            "[136/150]: Training Loss: 0.7546808309567249, Training Accuracy: 77.614\n",
            "Validation Loss: 2.215512813276546, Validation Accuracy: 52.82\n",
            "[137/150]: Training Loss: 0.7569507613130237, Training Accuracy: 77.856\n",
            "Validation Loss: 2.214977786799145, Validation Accuracy: 52.94\n",
            "[138/150]: Training Loss: 0.7538443226414873, Training Accuracy: 77.938\n",
            "Validation Loss: 2.215168529255375, Validation Accuracy: 52.96\n",
            "[139/150]: Training Loss: 0.7538767649084711, Training Accuracy: 77.666\n",
            "Validation Loss: 2.215025094664021, Validation Accuracy: 52.95\n",
            "[140/150]: Training Loss: 0.7476015420216123, Training Accuracy: 77.98\n",
            "Validation Loss: 2.2151690637989407, Validation Accuracy: 53.11\n",
            "[141/150]: Training Loss: 0.7429782649135346, Training Accuracy: 78.256\n",
            "Validation Loss: 2.214695118794775, Validation Accuracy: 52.91\n",
            "[142/150]: Training Loss: 0.7340887488077974, Training Accuracy: 78.538\n",
            "Validation Loss: 2.216404147968171, Validation Accuracy: 53.08\n",
            "[143/150]: Training Loss: 0.7499228092029576, Training Accuracy: 78.16\n",
            "Validation Loss: 2.216663356799229, Validation Accuracy: 52.99\n",
            "[144/150]: Training Loss: 0.739901978646398, Training Accuracy: 78.208\n",
            "Validation Loss: 2.216373413231722, Validation Accuracy: 53.03\n",
            "[145/150]: Training Loss: 0.7356343204560487, Training Accuracy: 78.332\n",
            "Validation Loss: 2.21599667087482, Validation Accuracy: 52.82\n",
            "[146/150]: Training Loss: 0.7417128108575216, Training Accuracy: 78.1\n",
            "Validation Loss: 2.216853774277268, Validation Accuracy: 53.06\n",
            "[147/150]: Training Loss: 0.7406589664385447, Training Accuracy: 78.444\n",
            "Validation Loss: 2.2168655562552675, Validation Accuracy: 52.95\n",
            "[148/150]: Training Loss: 0.7310213162694745, Training Accuracy: 78.62\n",
            "Validation Loss: 2.216719139913085, Validation Accuracy: 52.99\n",
            "[149/150]: Training Loss: 0.7304428781923431, Training Accuracy: 78.478\n",
            "Validation Loss: 2.216782635943905, Validation Accuracy: 53.01\n",
            "[150/150]: Training Loss: 0.7461780087493569, Training Accuracy: 78.148\n",
            "Validation Loss: 2.2168066478838586, Validation Accuracy: 53.01\n",
            "**********************************************************************\n",
            "Test Loss: 2.2168066478838586, Test Accuracy: 53.01\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▆█▁▁▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▂▁█▆▇▅▆▇▄▅▅▅▅▅▆▅▅▅▅▆▆▆▅▅▅▅▆▆▅▅▅▅▅▅▄▅▅▅▄▄</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>53.01</td></tr><tr><td>Test Loss</td><td>2.21681</td></tr><tr><td>Train Accuracy</td><td>78.148</td></tr><tr><td>Train Loss</td><td>0.74618</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.0015 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_052010-62vz5e00/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 4.8/(2**5 *1e02) # approximately 15e-04\n",
        "wd = 4e-04\n",
        "\n",
        "hyperparameters = {'learning_rate': lr,\n",
        "                    'weight_decay' : wd\n",
        "                  }\n",
        "\n",
        "# Load the model\n",
        "model = LeNet5().to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer = LAMB(model.parameters(), learning_rate=lr, weight_decay=wd)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# Training\n",
        "run_training(num_epochs, model, original_train_loader, original_test_loader, original_test_loader, optimizer, scheduler, criterion, device, optimizer_name='LAMB', hyperparameters=hyperparameters, is_wandb = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LAMB Test Large Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 512\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_030434-cxzhvadl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/cxzhvadl' target=\"_blank\">batch_size=512 learning_rate=0.012 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/cxzhvadl' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/cxzhvadl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.548103619594963, Training Accuracy: 2.248\n",
            "Validation Loss: 4.408652305603027, Validation Accuracy: 4.64\n",
            "[2/150]: Training Loss: 4.2417654796522495, Training Accuracy: 5.812\n",
            "Validation Loss: 4.078771162033081, Validation Accuracy: 7.52\n",
            "[3/150]: Training Loss: 4.044979589326041, Training Accuracy: 8.312\n",
            "Validation Loss: 3.9360867619514464, Validation Accuracy: 10.3\n",
            "[4/150]: Training Loss: 3.919014852874133, Training Accuracy: 10.582\n",
            "Validation Loss: 3.8062320232391356, Validation Accuracy: 12.69\n",
            "[5/150]: Training Loss: 3.802256394405754, Training Accuracy: 12.248\n",
            "Validation Loss: 3.6829858899116514, Validation Accuracy: 14.81\n",
            "[6/150]: Training Loss: 3.7044574873788014, Training Accuracy: 13.958\n",
            "Validation Loss: 3.598350536823273, Validation Accuracy: 16.05\n",
            "[7/150]: Training Loss: 3.619959531998148, Training Accuracy: 15.254\n",
            "Validation Loss: 3.504871332645416, Validation Accuracy: 17.62\n",
            "[8/150]: Training Loss: 3.5404407418504054, Training Accuracy: 16.608\n",
            "Validation Loss: 3.430806314945221, Validation Accuracy: 19.07\n",
            "[9/150]: Training Loss: 3.4704261118051956, Training Accuracy: 17.92\n",
            "Validation Loss: 3.355990946292877, Validation Accuracy: 20.39\n",
            "[10/150]: Training Loss: 3.4050637994493758, Training Accuracy: 18.696\n",
            "Validation Loss: 3.2953290343284607, Validation Accuracy: 21.2\n",
            "[11/150]: Training Loss: 3.347317642095138, Training Accuracy: 20.048\n",
            "Validation Loss: 3.2452304482460024, Validation Accuracy: 21.79\n",
            "[12/150]: Training Loss: 3.297038664623183, Training Accuracy: 20.922\n",
            "Validation Loss: 3.19344140291214, Validation Accuracy: 23.07\n",
            "[13/150]: Training Loss: 3.252234322684152, Training Accuracy: 21.52\n",
            "Validation Loss: 3.141191840171814, Validation Accuracy: 24.41\n",
            "[14/150]: Training Loss: 3.2010493010890726, Training Accuracy: 22.678\n",
            "Validation Loss: 3.0985478281974794, Validation Accuracy: 25.22\n",
            "[15/150]: Training Loss: 3.157341334284568, Training Accuracy: 23.544\n",
            "Validation Loss: 3.0466434955596924, Validation Accuracy: 25.34\n",
            "[16/150]: Training Loss: 3.119534066745213, Training Accuracy: 24.012\n",
            "Validation Loss: 3.001296067237854, Validation Accuracy: 26.5\n",
            "[17/150]: Training Loss: 3.072014643221485, Training Accuracy: 25.244\n",
            "Validation Loss: 2.9735819339752196, Validation Accuracy: 27.2\n",
            "[18/150]: Training Loss: 3.036018361850661, Training Accuracy: 25.664\n",
            "Validation Loss: 2.9088571310043334, Validation Accuracy: 28.33\n",
            "[19/150]: Training Loss: 2.998151776741962, Training Accuracy: 26.456\n",
            "Validation Loss: 2.87474707365036, Validation Accuracy: 29.39\n",
            "[20/150]: Training Loss: 2.954772486978648, Training Accuracy: 27.418\n",
            "Validation Loss: 2.8360928535461425, Validation Accuracy: 30.02\n",
            "[21/150]: Training Loss: 2.9239249545700696, Training Accuracy: 27.92\n",
            "Validation Loss: 2.8242129921913146, Validation Accuracy: 30.11\n",
            "[22/150]: Training Loss: 2.895316936531845, Training Accuracy: 28.508\n",
            "Validation Loss: 2.774155652523041, Validation Accuracy: 31.66\n",
            "[23/150]: Training Loss: 2.859412969375143, Training Accuracy: 29.08\n",
            "Validation Loss: 2.744171452522278, Validation Accuracy: 31.68\n",
            "[24/150]: Training Loss: 2.8252378531864712, Training Accuracy: 29.67\n",
            "Validation Loss: 2.714933896064758, Validation Accuracy: 32.09\n",
            "[25/150]: Training Loss: 2.7994687800504723, Training Accuracy: 30.014\n",
            "Validation Loss: 2.677684450149536, Validation Accuracy: 32.99\n",
            "[26/150]: Training Loss: 2.765696126587537, Training Accuracy: 31.108\n",
            "Validation Loss: 2.6599324345588684, Validation Accuracy: 33.18\n",
            "[27/150]: Training Loss: 2.740787510969201, Training Accuracy: 31.346\n",
            "Validation Loss: 2.6425792574882507, Validation Accuracy: 34.19\n",
            "[28/150]: Training Loss: 2.708871919281629, Training Accuracy: 31.902\n",
            "Validation Loss: 2.595020318031311, Validation Accuracy: 34.92\n",
            "[29/150]: Training Loss: 2.685730184827532, Training Accuracy: 32.37\n",
            "Validation Loss: 2.5777265787124635, Validation Accuracy: 35.0\n",
            "[30/150]: Training Loss: 2.65871207324826, Training Accuracy: 32.814\n",
            "Validation Loss: 2.5629518866539, Validation Accuracy: 35.52\n",
            "[31/150]: Training Loss: 2.6434246131352017, Training Accuracy: 33.334\n",
            "Validation Loss: 2.536830258369446, Validation Accuracy: 36.09\n",
            "[32/150]: Training Loss: 2.6128752523539016, Training Accuracy: 33.888\n",
            "Validation Loss: 2.5102296233177186, Validation Accuracy: 36.3\n",
            "[33/150]: Training Loss: 2.5891901619580326, Training Accuracy: 34.278\n",
            "Validation Loss: 2.5139454007148743, Validation Accuracy: 35.79\n",
            "[34/150]: Training Loss: 2.5790800391411293, Training Accuracy: 34.66\n",
            "Validation Loss: 2.4993165135383606, Validation Accuracy: 36.84\n",
            "[35/150]: Training Loss: 2.5574506521224976, Training Accuracy: 35.194\n",
            "Validation Loss: 2.4735957622528075, Validation Accuracy: 37.42\n",
            "[36/150]: Training Loss: 2.5401810237339566, Training Accuracy: 35.368\n",
            "Validation Loss: 2.4526307821273803, Validation Accuracy: 37.83\n",
            "[37/150]: Training Loss: 2.525405545623935, Training Accuracy: 35.55\n",
            "Validation Loss: 2.4341293573379517, Validation Accuracy: 38.12\n",
            "[38/150]: Training Loss: 2.511008401306308, Training Accuracy: 36.018\n",
            "Validation Loss: 2.4294227361679077, Validation Accuracy: 38.29\n",
            "[39/150]: Training Loss: 2.485745899531306, Training Accuracy: 36.542\n",
            "Validation Loss: 2.424438309669495, Validation Accuracy: 38.25\n",
            "[40/150]: Training Loss: 2.4709256230568397, Training Accuracy: 37.07\n",
            "Validation Loss: 2.392498195171356, Validation Accuracy: 39.18\n",
            "[41/150]: Training Loss: 2.453389508383615, Training Accuracy: 37.188\n",
            "Validation Loss: 2.3836654901504515, Validation Accuracy: 39.34\n",
            "[42/150]: Training Loss: 2.436603519381309, Training Accuracy: 37.772\n",
            "Validation Loss: 2.366767430305481, Validation Accuracy: 39.42\n",
            "[43/150]: Training Loss: 2.4228299369617385, Training Accuracy: 37.662\n",
            "Validation Loss: 2.376271140575409, Validation Accuracy: 39.35\n",
            "[44/150]: Training Loss: 2.410697212024611, Training Accuracy: 38.148\n",
            "Validation Loss: 2.346989369392395, Validation Accuracy: 39.91\n",
            "[45/150]: Training Loss: 2.3892664276823705, Training Accuracy: 38.722\n",
            "Validation Loss: 2.343613922595978, Validation Accuracy: 40.27\n",
            "[46/150]: Training Loss: 2.3732383518802878, Training Accuracy: 38.978\n",
            "Validation Loss: 2.3365381956100464, Validation Accuracy: 40.04\n",
            "[47/150]: Training Loss: 2.365978574266239, Training Accuracy: 39.05\n",
            "Validation Loss: 2.327448809146881, Validation Accuracy: 40.27\n",
            "[48/150]: Training Loss: 2.3588604537808164, Training Accuracy: 39.06\n",
            "Validation Loss: 2.306267297267914, Validation Accuracy: 40.56\n",
            "[49/150]: Training Loss: 2.3424654785467656, Training Accuracy: 39.404\n",
            "Validation Loss: 2.3065796971321104, Validation Accuracy: 40.84\n",
            "[50/150]: Training Loss: 2.3214932753115285, Training Accuracy: 39.88\n",
            "Validation Loss: 2.291364300251007, Validation Accuracy: 40.85\n",
            "[51/150]: Training Loss: 2.319426546291429, Training Accuracy: 40.212\n",
            "Validation Loss: 2.2994829177856446, Validation Accuracy: 40.73\n",
            "[52/150]: Training Loss: 2.3011135446782016, Training Accuracy: 40.49\n",
            "Validation Loss: 2.272173082828522, Validation Accuracy: 41.48\n",
            "[53/150]: Training Loss: 2.291705601069392, Training Accuracy: 40.65\n",
            "Validation Loss: 2.266998326778412, Validation Accuracy: 41.39\n",
            "[54/150]: Training Loss: 2.2740079359132417, Training Accuracy: 40.958\n",
            "Validation Loss: 2.262247622013092, Validation Accuracy: 41.51\n",
            "[55/150]: Training Loss: 2.2702779356314213, Training Accuracy: 41.14\n",
            "Validation Loss: 2.247134339809418, Validation Accuracy: 42.18\n",
            "[56/150]: Training Loss: 2.2560372790511773, Training Accuracy: 41.404\n",
            "Validation Loss: 2.2417269587516784, Validation Accuracy: 42.2\n",
            "[57/150]: Training Loss: 2.2483739342008318, Training Accuracy: 41.564\n",
            "Validation Loss: 2.2450056076049805, Validation Accuracy: 41.86\n",
            "[58/150]: Training Loss: 2.2375453880855014, Training Accuracy: 41.784\n",
            "Validation Loss: 2.240702271461487, Validation Accuracy: 42.3\n",
            "[59/150]: Training Loss: 2.2237761117974104, Training Accuracy: 42.04\n",
            "Validation Loss: 2.2335373759269714, Validation Accuracy: 42.3\n",
            "[60/150]: Training Loss: 2.205965436234766, Training Accuracy: 42.788\n",
            "Validation Loss: 2.2246406435966493, Validation Accuracy: 42.3\n",
            "[61/150]: Training Loss: 2.209139476017076, Training Accuracy: 42.374\n",
            "Validation Loss: 2.2219802379608153, Validation Accuracy: 42.67\n",
            "[62/150]: Training Loss: 2.1963447551338042, Training Accuracy: 42.81\n",
            "Validation Loss: 2.2133118152618407, Validation Accuracy: 43.14\n",
            "[63/150]: Training Loss: 2.1860338960375105, Training Accuracy: 42.898\n",
            "Validation Loss: 2.1990301489830015, Validation Accuracy: 43.06\n",
            "[64/150]: Training Loss: 2.1734118291309903, Training Accuracy: 43.52\n",
            "Validation Loss: 2.184101331233978, Validation Accuracy: 43.49\n",
            "[65/150]: Training Loss: 2.1634396545741024, Training Accuracy: 43.672\n",
            "Validation Loss: 2.184304988384247, Validation Accuracy: 43.2\n",
            "[66/150]: Training Loss: 2.1568861445602105, Training Accuracy: 43.418\n",
            "Validation Loss: 2.185279929637909, Validation Accuracy: 43.78\n",
            "[67/150]: Training Loss: 2.148503156340852, Training Accuracy: 43.668\n",
            "Validation Loss: 2.167687404155731, Validation Accuracy: 43.87\n",
            "[68/150]: Training Loss: 2.1439749002456665, Training Accuracy: 44.14\n",
            "Validation Loss: 2.1670034646987917, Validation Accuracy: 43.46\n",
            "[69/150]: Training Loss: 2.1294457790802936, Training Accuracy: 44.21\n",
            "Validation Loss: 2.16845703125, Validation Accuracy: 43.74\n",
            "[70/150]: Training Loss: 2.124348041962604, Training Accuracy: 44.398\n",
            "Validation Loss: 2.1625195741653442, Validation Accuracy: 43.96\n",
            "[71/150]: Training Loss: 2.1167217590370955, Training Accuracy: 44.5\n",
            "Validation Loss: 2.156108921766281, Validation Accuracy: 44.18\n",
            "[72/150]: Training Loss: 2.110376476025095, Training Accuracy: 44.732\n",
            "Validation Loss: 2.163306188583374, Validation Accuracy: 44.44\n",
            "[73/150]: Training Loss: 2.0973582632687626, Training Accuracy: 45.182\n",
            "Validation Loss: 2.1542032241821287, Validation Accuracy: 43.94\n",
            "[74/150]: Training Loss: 2.0909664728203596, Training Accuracy: 44.946\n",
            "Validation Loss: 2.1355461835861207, Validation Accuracy: 44.96\n",
            "[75/150]: Training Loss: 2.0853602423959847, Training Accuracy: 45.484\n",
            "Validation Loss: 2.133738082647324, Validation Accuracy: 44.85\n",
            "[76/150]: Training Loss: 2.0740136200067947, Training Accuracy: 45.472\n",
            "Validation Loss: 2.133003461360931, Validation Accuracy: 44.84\n",
            "[77/150]: Training Loss: 2.0717564541466382, Training Accuracy: 45.53\n",
            "Validation Loss: 2.1258037745952607, Validation Accuracy: 44.82\n",
            "[78/150]: Training Loss: 2.0652840003675346, Training Accuracy: 45.638\n",
            "Validation Loss: 2.128880649805069, Validation Accuracy: 45.05\n",
            "[79/150]: Training Loss: 2.058366948244523, Training Accuracy: 45.926\n",
            "Validation Loss: 2.1184136271476746, Validation Accuracy: 45.13\n",
            "[80/150]: Training Loss: 2.0537973958618787, Training Accuracy: 46.032\n",
            "Validation Loss: 2.1158532202243805, Validation Accuracy: 45.41\n",
            "[81/150]: Training Loss: 2.045546445311332, Training Accuracy: 46.22\n",
            "Validation Loss: 2.1055331349372866, Validation Accuracy: 45.66\n",
            "[82/150]: Training Loss: 2.0298990595097446, Training Accuracy: 46.356\n",
            "Validation Loss: 2.1113824367523195, Validation Accuracy: 44.91\n",
            "[83/150]: Training Loss: 2.0376479844657744, Training Accuracy: 46.254\n",
            "Validation Loss: 2.107465136051178, Validation Accuracy: 45.22\n",
            "[84/150]: Training Loss: 2.024351028763518, Training Accuracy: 46.772\n",
            "Validation Loss: 2.111053156852722, Validation Accuracy: 45.25\n",
            "[85/150]: Training Loss: 2.0180776496322785, Training Accuracy: 46.708\n",
            "Validation Loss: 2.1116692423820496, Validation Accuracy: 45.4\n",
            "[86/150]: Training Loss: 2.0185614410711796, Training Accuracy: 46.768\n",
            "Validation Loss: 2.0962437868118284, Validation Accuracy: 45.53\n",
            "[87/150]: Training Loss: 2.008148268777497, Training Accuracy: 47.038\n",
            "Validation Loss: 2.1018231213092804, Validation Accuracy: 45.87\n",
            "[88/150]: Training Loss: 2.0017905855665403, Training Accuracy: 47.142\n",
            "Validation Loss: 2.0960277736186983, Validation Accuracy: 45.43\n",
            "[89/150]: Training Loss: 1.997242003071065, Training Accuracy: 47.234\n",
            "Validation Loss: 2.0983319759368895, Validation Accuracy: 45.81\n",
            "[90/150]: Training Loss: 1.990279420297973, Training Accuracy: 47.292\n",
            "Validation Loss: 2.0858993887901307, Validation Accuracy: 45.95\n",
            "[91/150]: Training Loss: 1.9810005718347978, Training Accuracy: 47.574\n",
            "Validation Loss: 2.084125190973282, Validation Accuracy: 45.77\n",
            "[92/150]: Training Loss: 1.9804937997642829, Training Accuracy: 47.424\n",
            "Validation Loss: 2.082464426755905, Validation Accuracy: 46.53\n",
            "[93/150]: Training Loss: 1.9814110872696857, Training Accuracy: 47.53\n",
            "Validation Loss: 2.0802278578281403, Validation Accuracy: 46.11\n",
            "[94/150]: Training Loss: 1.9699082763827578, Training Accuracy: 47.858\n",
            "Validation Loss: 2.0711540818214416, Validation Accuracy: 46.49\n",
            "[95/150]: Training Loss: 1.963157915339178, Training Accuracy: 47.982\n",
            "Validation Loss: 2.0740585386753083, Validation Accuracy: 46.44\n",
            "[96/150]: Training Loss: 1.958235819729007, Training Accuracy: 48.274\n",
            "Validation Loss: 2.07954381108284, Validation Accuracy: 46.17\n",
            "[97/150]: Training Loss: 1.954954169234451, Training Accuracy: 48.15\n",
            "Validation Loss: 2.074056273698807, Validation Accuracy: 46.2\n",
            "[98/150]: Training Loss: 1.9549918174743652, Training Accuracy: 48.224\n",
            "Validation Loss: 2.076428145170212, Validation Accuracy: 46.58\n",
            "[99/150]: Training Loss: 1.9507698878950002, Training Accuracy: 48.31\n",
            "Validation Loss: 2.0744490921497345, Validation Accuracy: 46.39\n",
            "[100/150]: Training Loss: 1.9442955656927459, Training Accuracy: 48.386\n",
            "Validation Loss: 2.070361965894699, Validation Accuracy: 46.32\n",
            "[101/150]: Training Loss: 1.9397471109215094, Training Accuracy: 48.69\n",
            "Validation Loss: 2.0684597194194794, Validation Accuracy: 46.38\n",
            "[102/150]: Training Loss: 1.9359576568311574, Training Accuracy: 48.714\n",
            "Validation Loss: 2.0585030019283295, Validation Accuracy: 46.51\n",
            "[103/150]: Training Loss: 1.9329783028485823, Training Accuracy: 48.922\n",
            "Validation Loss: 2.0640245974063873, Validation Accuracy: 46.74\n",
            "[104/150]: Training Loss: 1.9296102134548887, Training Accuracy: 48.876\n",
            "Validation Loss: 2.0570806205272674, Validation Accuracy: 46.91\n",
            "[105/150]: Training Loss: 1.9274067817902079, Training Accuracy: 48.836\n",
            "Validation Loss: 2.0609201788902283, Validation Accuracy: 46.8\n",
            "[106/150]: Training Loss: 1.9235176504874716, Training Accuracy: 48.79\n",
            "Validation Loss: 2.0552598774433135, Validation Accuracy: 46.81\n",
            "[107/150]: Training Loss: 1.9150361941785228, Training Accuracy: 48.932\n",
            "Validation Loss: 2.0611153185367583, Validation Accuracy: 46.99\n",
            "[108/150]: Training Loss: 1.9190492228585847, Training Accuracy: 49.058\n",
            "Validation Loss: 2.0567619800567627, Validation Accuracy: 46.75\n",
            "[109/150]: Training Loss: 1.913926816716486, Training Accuracy: 49.228\n",
            "Validation Loss: 2.050940066576004, Validation Accuracy: 47.03\n",
            "[110/150]: Training Loss: 1.9050643358911787, Training Accuracy: 49.476\n",
            "Validation Loss: 2.0542674362659454, Validation Accuracy: 47.17\n",
            "[111/150]: Training Loss: 1.9050180559255638, Training Accuracy: 49.556\n",
            "Validation Loss: 2.0528355121612547, Validation Accuracy: 46.76\n",
            "[112/150]: Training Loss: 1.9088959620923411, Training Accuracy: 49.342\n",
            "Validation Loss: 2.0460824489593508, Validation Accuracy: 47.29\n",
            "[113/150]: Training Loss: 1.9044355105380624, Training Accuracy: 49.372\n",
            "Validation Loss: 2.0464745998382567, Validation Accuracy: 47.08\n",
            "[114/150]: Training Loss: 1.8996793262812557, Training Accuracy: 49.554\n",
            "Validation Loss: 2.0468083798885344, Validation Accuracy: 46.91\n",
            "[115/150]: Training Loss: 1.8977701590985667, Training Accuracy: 49.58\n",
            "Validation Loss: 2.0489026606082916, Validation Accuracy: 47.02\n",
            "[116/150]: Training Loss: 1.9024124863196392, Training Accuracy: 49.632\n",
            "Validation Loss: 2.045408546924591, Validation Accuracy: 47.0\n",
            "[117/150]: Training Loss: 1.896902701076196, Training Accuracy: 49.664\n",
            "Validation Loss: 2.0449475407600404, Validation Accuracy: 47.28\n",
            "[118/150]: Training Loss: 1.887433451049182, Training Accuracy: 49.708\n",
            "Validation Loss: 2.0426290810108183, Validation Accuracy: 47.38\n",
            "[119/150]: Training Loss: 1.890810448296216, Training Accuracy: 49.792\n",
            "Validation Loss: 2.0400512278079987, Validation Accuracy: 47.37\n",
            "[120/150]: Training Loss: 1.8811560346155751, Training Accuracy: 49.924\n",
            "Validation Loss: 2.0446768164634705, Validation Accuracy: 47.22\n",
            "[121/150]: Training Loss: 1.8865068250772905, Training Accuracy: 49.69\n",
            "Validation Loss: 2.043155688047409, Validation Accuracy: 47.38\n",
            "[122/150]: Training Loss: 1.8877364591676362, Training Accuracy: 49.846\n",
            "Validation Loss: 2.0397587597370146, Validation Accuracy: 47.32\n",
            "[123/150]: Training Loss: 1.8801838646129685, Training Accuracy: 50.076\n",
            "Validation Loss: 2.0414818286895753, Validation Accuracy: 47.09\n",
            "[124/150]: Training Loss: 1.885428019932338, Training Accuracy: 49.846\n",
            "Validation Loss: 2.039280617237091, Validation Accuracy: 47.25\n",
            "[125/150]: Training Loss: 1.8782189658709936, Training Accuracy: 49.912\n",
            "Validation Loss: 2.041011613607407, Validation Accuracy: 47.37\n",
            "[126/150]: Training Loss: 1.879096979997596, Training Accuracy: 50.194\n",
            "Validation Loss: 2.039641273021698, Validation Accuracy: 47.31\n",
            "[127/150]: Training Loss: 1.8750834902938531, Training Accuracy: 50.116\n",
            "Validation Loss: 2.0357660591602325, Validation Accuracy: 47.24\n",
            "[128/150]: Training Loss: 1.8704910095857115, Training Accuracy: 50.072\n",
            "Validation Loss: 2.036173564195633, Validation Accuracy: 47.35\n",
            "[129/150]: Training Loss: 1.877774753132645, Training Accuracy: 49.962\n",
            "Validation Loss: 2.036165338754654, Validation Accuracy: 47.39\n",
            "[130/150]: Training Loss: 1.873654185509195, Training Accuracy: 50.22\n",
            "Validation Loss: 2.0360648393630982, Validation Accuracy: 47.45\n",
            "[131/150]: Training Loss: 1.8686100913553823, Training Accuracy: 50.242\n",
            "Validation Loss: 2.0372694969177245, Validation Accuracy: 47.45\n",
            "[132/150]: Training Loss: 1.8713702170216306, Training Accuracy: 50.18\n",
            "Validation Loss: 2.0337652981281282, Validation Accuracy: 47.49\n",
            "[133/150]: Training Loss: 1.8645332942203598, Training Accuracy: 50.246\n",
            "Validation Loss: 2.0328580677509307, Validation Accuracy: 47.49\n",
            "[134/150]: Training Loss: 1.8704569595200675, Training Accuracy: 50.214\n",
            "Validation Loss: 2.034824085235596, Validation Accuracy: 47.51\n",
            "[135/150]: Training Loss: 1.8728308142447958, Training Accuracy: 50.154\n",
            "Validation Loss: 2.0340331494808197, Validation Accuracy: 47.43\n",
            "[136/150]: Training Loss: 1.8635819201566735, Training Accuracy: 50.458\n",
            "Validation Loss: 2.0333786249160766, Validation Accuracy: 47.49\n",
            "[137/150]: Training Loss: 1.8670503034883617, Training Accuracy: 50.31\n",
            "Validation Loss: 2.0328627586364747, Validation Accuracy: 47.54\n",
            "[138/150]: Training Loss: 1.8680279534690234, Training Accuracy: 50.094\n",
            "Validation Loss: 2.033160853385925, Validation Accuracy: 47.58\n",
            "[139/150]: Training Loss: 1.8607495062205257, Training Accuracy: 50.598\n",
            "Validation Loss: 2.0337081253528595, Validation Accuracy: 47.43\n",
            "[140/150]: Training Loss: 1.8683336133859596, Training Accuracy: 50.274\n",
            "Validation Loss: 2.0336008071899414, Validation Accuracy: 47.43\n",
            "[141/150]: Training Loss: 1.8690763091554448, Training Accuracy: 50.138\n",
            "Validation Loss: 2.0335811972618103, Validation Accuracy: 47.35\n",
            "[142/150]: Training Loss: 1.8656006175644544, Training Accuracy: 50.446\n",
            "Validation Loss: 2.03268141746521, Validation Accuracy: 47.33\n",
            "[143/150]: Training Loss: 1.8692044360297067, Training Accuracy: 50.272\n",
            "Validation Loss: 2.0328011751174926, Validation Accuracy: 47.47\n",
            "[144/150]: Training Loss: 1.8644137017580928, Training Accuracy: 50.484\n",
            "Validation Loss: 2.0326188147068023, Validation Accuracy: 47.35\n",
            "[145/150]: Training Loss: 1.8648517861658214, Training Accuracy: 50.596\n",
            "Validation Loss: 2.0326371788978577, Validation Accuracy: 47.46\n",
            "[146/150]: Training Loss: 1.8628962891442435, Training Accuracy: 50.572\n",
            "Validation Loss: 2.0327242374420167, Validation Accuracy: 47.44\n",
            "[147/150]: Training Loss: 1.8659434099586643, Training Accuracy: 50.224\n",
            "Validation Loss: 2.0326199173927306, Validation Accuracy: 47.48\n",
            "[148/150]: Training Loss: 1.8660591877236659, Training Accuracy: 50.486\n",
            "Validation Loss: 2.0326066315174103, Validation Accuracy: 47.47\n",
            "[149/150]: Training Loss: 1.8515786005526174, Training Accuracy: 50.66\n",
            "Validation Loss: 2.0325814962387083, Validation Accuracy: 47.47\n",
            "[150/150]: Training Loss: 1.8628681095278994, Training Accuracy: 50.284\n",
            "Validation Loss: 2.032588768005371, Validation Accuracy: 47.49\n",
            "**********************************************************************\n",
            "Test Loss: 2.032588768005371, Test Accuracy: 47.49\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▅▄▁▃▅▄▄▅▆▆▆▆▇▇▇▇███▇</td></tr><tr><td>Test Loss</td><td>██▇▃▃▃▃▁▁▂▂▁▂▁▂▂▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>47.49</td></tr><tr><td>Test Loss</td><td>2.03259</td></tr><tr><td>Train Accuracy</td><td>50.284</td></tr><tr><td>Train Loss</td><td>1.86287</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=512 learning_rate=0.012 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/cxzhvadl' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/cxzhvadl</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_030434-cxzhvadl/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 1024\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_032944-gya1gfdd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/gya1gfdd' target=\"_blank\">batch_size=1024 learning_rate=0.024 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/gya1gfdd' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/gya1gfdd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.590565778771225, Training Accuracy: 2.506\n",
            "Validation Loss: 4.557540512084961, Validation Accuracy: 3.82\n",
            "[2/150]: Training Loss: 4.493393129231978, Training Accuracy: 3.93\n",
            "Validation Loss: 4.3973547458648685, Validation Accuracy: 4.08\n",
            "[3/150]: Training Loss: 4.306708112054942, Training Accuracy: 4.95\n",
            "Validation Loss: 4.188614082336426, Validation Accuracy: 7.08\n",
            "[4/150]: Training Loss: 4.141291871362803, Training Accuracy: 7.376\n",
            "Validation Loss: 4.052172231674194, Validation Accuracy: 9.03\n",
            "[5/150]: Training Loss: 4.041586535317557, Training Accuracy: 8.738\n",
            "Validation Loss: 3.958093047142029, Validation Accuracy: 10.53\n",
            "[6/150]: Training Loss: 3.964396841672002, Training Accuracy: 9.848\n",
            "Validation Loss: 3.877572846412659, Validation Accuracy: 11.97\n",
            "[7/150]: Training Loss: 3.884106631181678, Training Accuracy: 11.158\n",
            "Validation Loss: 3.7878554344177244, Validation Accuracy: 13.45\n",
            "[8/150]: Training Loss: 3.8150518037834944, Training Accuracy: 12.512\n",
            "Validation Loss: 3.717964506149292, Validation Accuracy: 14.81\n",
            "[9/150]: Training Loss: 3.7486333409134223, Training Accuracy: 13.622\n",
            "Validation Loss: 3.6480979919433594, Validation Accuracy: 15.64\n",
            "[10/150]: Training Loss: 3.6901821408952986, Training Accuracy: 14.558\n",
            "Validation Loss: 3.596638226509094, Validation Accuracy: 17.04\n",
            "[11/150]: Training Loss: 3.6437562047218788, Training Accuracy: 15.138\n",
            "Validation Loss: 3.5412201166152952, Validation Accuracy: 17.54\n",
            "[12/150]: Training Loss: 3.5933454912536, Training Accuracy: 16.05\n",
            "Validation Loss: 3.4941962718963624, Validation Accuracy: 18.44\n",
            "[13/150]: Training Loss: 3.5444304748457305, Training Accuracy: 16.992\n",
            "Validation Loss: 3.4401322841644286, Validation Accuracy: 19.71\n",
            "[14/150]: Training Loss: 3.4985976121863542, Training Accuracy: 17.586\n",
            "Validation Loss: 3.395567035675049, Validation Accuracy: 20.55\n",
            "[15/150]: Training Loss: 3.453222994901696, Training Accuracy: 18.504\n",
            "Validation Loss: 3.345482683181763, Validation Accuracy: 21.11\n",
            "[16/150]: Training Loss: 3.409157923289708, Training Accuracy: 19.202\n",
            "Validation Loss: 3.309169292449951, Validation Accuracy: 21.17\n",
            "[17/150]: Training Loss: 3.362671049273744, Training Accuracy: 19.936\n",
            "Validation Loss: 3.2603885412216185, Validation Accuracy: 22.74\n",
            "[18/150]: Training Loss: 3.325447690730192, Training Accuracy: 20.694\n",
            "Validation Loss: 3.2284779787063598, Validation Accuracy: 22.9\n",
            "[19/150]: Training Loss: 3.2953158884632345, Training Accuracy: 21.242\n",
            "Validation Loss: 3.1922855138778687, Validation Accuracy: 23.83\n",
            "[20/150]: Training Loss: 3.2581767743947556, Training Accuracy: 21.716\n",
            "Validation Loss: 3.1589023590087892, Validation Accuracy: 24.16\n",
            "[21/150]: Training Loss: 3.2280877463671627, Training Accuracy: 22.26\n",
            "Validation Loss: 3.120241904258728, Validation Accuracy: 25.39\n",
            "[22/150]: Training Loss: 3.197068000326351, Training Accuracy: 22.91\n",
            "Validation Loss: 3.1016393423080446, Validation Accuracy: 25.24\n",
            "[23/150]: Training Loss: 3.168019095245673, Training Accuracy: 23.574\n",
            "Validation Loss: 3.070971941947937, Validation Accuracy: 25.89\n",
            "[24/150]: Training Loss: 3.1401676158515777, Training Accuracy: 23.778\n",
            "Validation Loss: 3.043724036216736, Validation Accuracy: 26.42\n",
            "[25/150]: Training Loss: 3.1160317245794804, Training Accuracy: 24.456\n",
            "Validation Loss: 3.0154256582260133, Validation Accuracy: 27.1\n",
            "[26/150]: Training Loss: 3.0870673218551947, Training Accuracy: 24.898\n",
            "Validation Loss: 2.9828964948654173, Validation Accuracy: 27.83\n",
            "[27/150]: Training Loss: 3.063414077369534, Training Accuracy: 25.346\n",
            "Validation Loss: 2.9679323196411134, Validation Accuracy: 27.77\n",
            "[28/150]: Training Loss: 3.0381831246979383, Training Accuracy: 25.734\n",
            "Validation Loss: 2.9393006801605224, Validation Accuracy: 27.88\n",
            "[29/150]: Training Loss: 3.0093668480308686, Training Accuracy: 26.254\n",
            "Validation Loss: 2.9122467756271364, Validation Accuracy: 28.61\n",
            "[30/150]: Training Loss: 2.993134167729592, Training Accuracy: 26.534\n",
            "Validation Loss: 2.8969119787216187, Validation Accuracy: 29.41\n",
            "[31/150]: Training Loss: 2.96768881836716, Training Accuracy: 27.274\n",
            "Validation Loss: 2.8658586025238035, Validation Accuracy: 29.67\n",
            "[32/150]: Training Loss: 2.9414292695570965, Training Accuracy: 27.478\n",
            "Validation Loss: 2.862342190742493, Validation Accuracy: 29.54\n",
            "[33/150]: Training Loss: 2.9305670018098793, Training Accuracy: 27.892\n",
            "Validation Loss: 2.822964382171631, Validation Accuracy: 30.79\n",
            "[34/150]: Training Loss: 2.9122587028814824, Training Accuracy: 28.452\n",
            "Validation Loss: 2.827067565917969, Validation Accuracy: 30.5\n",
            "[35/150]: Training Loss: 2.8878965669748733, Training Accuracy: 28.686\n",
            "Validation Loss: 2.7999042510986327, Validation Accuracy: 31.18\n",
            "[36/150]: Training Loss: 2.877296729963653, Training Accuracy: 29.068\n",
            "Validation Loss: 2.779293489456177, Validation Accuracy: 31.43\n",
            "[37/150]: Training Loss: 2.8623253374683615, Training Accuracy: 29.252\n",
            "Validation Loss: 2.771078372001648, Validation Accuracy: 31.46\n",
            "[38/150]: Training Loss: 2.845758058586899, Training Accuracy: 29.536\n",
            "Validation Loss: 2.7440542459487913, Validation Accuracy: 32.21\n",
            "[39/150]: Training Loss: 2.826752618867524, Training Accuracy: 30.078\n",
            "Validation Loss: 2.732813310623169, Validation Accuracy: 32.48\n",
            "[40/150]: Training Loss: 2.8119893901202144, Training Accuracy: 30.44\n",
            "Validation Loss: 2.7104941606521606, Validation Accuracy: 32.62\n",
            "[41/150]: Training Loss: 2.798588991165161, Training Accuracy: 30.704\n",
            "Validation Loss: 2.71542112827301, Validation Accuracy: 32.54\n",
            "[42/150]: Training Loss: 2.789037441720768, Training Accuracy: 30.904\n",
            "Validation Loss: 2.699553608894348, Validation Accuracy: 32.91\n",
            "[43/150]: Training Loss: 2.7693806959658254, Training Accuracy: 31.302\n",
            "Validation Loss: 2.6739187002182008, Validation Accuracy: 33.55\n",
            "[44/150]: Training Loss: 2.759038647826837, Training Accuracy: 31.434\n",
            "Validation Loss: 2.6662899017333985, Validation Accuracy: 33.41\n",
            "[45/150]: Training Loss: 2.741479440611236, Training Accuracy: 31.38\n",
            "Validation Loss: 2.668094348907471, Validation Accuracy: 33.54\n",
            "[46/150]: Training Loss: 2.7325507524062176, Training Accuracy: 31.794\n",
            "Validation Loss: 2.642213749885559, Validation Accuracy: 33.91\n",
            "[47/150]: Training Loss: 2.713993953198803, Training Accuracy: 32.126\n",
            "Validation Loss: 2.638446068763733, Validation Accuracy: 33.93\n",
            "[48/150]: Training Loss: 2.7059789968996633, Training Accuracy: 32.314\n",
            "Validation Loss: 2.625475454330444, Validation Accuracy: 34.3\n",
            "[49/150]: Training Loss: 2.6864885018796336, Training Accuracy: 32.77\n",
            "Validation Loss: 2.609480929374695, Validation Accuracy: 34.55\n",
            "[50/150]: Training Loss: 2.672593793090509, Training Accuracy: 32.9\n",
            "Validation Loss: 2.6012288331985474, Validation Accuracy: 34.62\n",
            "[51/150]: Training Loss: 2.6611643907975178, Training Accuracy: 33.552\n",
            "Validation Loss: 2.5788160800933837, Validation Accuracy: 35.31\n",
            "[52/150]: Training Loss: 2.6511389333374646, Training Accuracy: 33.566\n",
            "Validation Loss: 2.5761229038238525, Validation Accuracy: 35.32\n",
            "[53/150]: Training Loss: 2.6390009899528657, Training Accuracy: 33.73\n",
            "Validation Loss: 2.5618453502655028, Validation Accuracy: 35.69\n",
            "[54/150]: Training Loss: 2.6149472947023353, Training Accuracy: 34.264\n",
            "Validation Loss: 2.5448814392089845, Validation Accuracy: 36.26\n",
            "[55/150]: Training Loss: 2.6032056467873708, Training Accuracy: 34.406\n",
            "Validation Loss: 2.540690517425537, Validation Accuracy: 36.11\n",
            "[56/150]: Training Loss: 2.5915229174555563, Training Accuracy: 34.672\n",
            "Validation Loss: 2.5265331268310547, Validation Accuracy: 36.11\n",
            "[57/150]: Training Loss: 2.5826624850837554, Training Accuracy: 34.72\n",
            "Validation Loss: 2.508814811706543, Validation Accuracy: 36.63\n",
            "[58/150]: Training Loss: 2.5663072041102817, Training Accuracy: 35.1\n",
            "Validation Loss: 2.5033692836761476, Validation Accuracy: 36.62\n",
            "[59/150]: Training Loss: 2.5570305464219074, Training Accuracy: 35.432\n",
            "Validation Loss: 2.5039987564086914, Validation Accuracy: 36.86\n",
            "[60/150]: Training Loss: 2.5427569759135342, Training Accuracy: 35.544\n",
            "Validation Loss: 2.480252814292908, Validation Accuracy: 36.95\n",
            "[61/150]: Training Loss: 2.5343954806425133, Training Accuracy: 35.818\n",
            "Validation Loss: 2.4878724813461304, Validation Accuracy: 36.76\n",
            "[62/150]: Training Loss: 2.525669209811152, Training Accuracy: 36.168\n",
            "Validation Loss: 2.463840389251709, Validation Accuracy: 37.41\n",
            "[63/150]: Training Loss: 2.5151758339940287, Training Accuracy: 36.232\n",
            "Validation Loss: 2.4627907991409304, Validation Accuracy: 37.57\n",
            "[64/150]: Training Loss: 2.500540592232529, Training Accuracy: 36.386\n",
            "Validation Loss: 2.436687207221985, Validation Accuracy: 37.62\n",
            "[65/150]: Training Loss: 2.495049014383433, Training Accuracy: 36.56\n",
            "Validation Loss: 2.4480030298233033, Validation Accuracy: 37.49\n",
            "[66/150]: Training Loss: 2.4877421953240217, Training Accuracy: 36.736\n",
            "Validation Loss: 2.438114333152771, Validation Accuracy: 37.91\n",
            "[67/150]: Training Loss: 2.473500212844537, Training Accuracy: 37.184\n",
            "Validation Loss: 2.4267699241638185, Validation Accuracy: 38.53\n",
            "[68/150]: Training Loss: 2.4613005725704893, Training Accuracy: 37.244\n",
            "Validation Loss: 2.4308807611465455, Validation Accuracy: 38.0\n",
            "[69/150]: Training Loss: 2.4629757258356832, Training Accuracy: 37.416\n",
            "Validation Loss: 2.4256237030029295, Validation Accuracy: 38.22\n",
            "[70/150]: Training Loss: 2.4537137041286545, Training Accuracy: 37.484\n",
            "Validation Loss: 2.4168110609054567, Validation Accuracy: 38.54\n",
            "[71/150]: Training Loss: 2.438401460647583, Training Accuracy: 37.964\n",
            "Validation Loss: 2.401189923286438, Validation Accuracy: 38.86\n",
            "[72/150]: Training Loss: 2.431730114683813, Training Accuracy: 37.806\n",
            "Validation Loss: 2.3951660871505736, Validation Accuracy: 38.77\n",
            "[73/150]: Training Loss: 2.425863484947049, Training Accuracy: 38.066\n",
            "Validation Loss: 2.3934099197387697, Validation Accuracy: 38.84\n",
            "[74/150]: Training Loss: 2.418164759266133, Training Accuracy: 38.102\n",
            "Validation Loss: 2.3899000883102417, Validation Accuracy: 39.18\n",
            "[75/150]: Training Loss: 2.4137614357228183, Training Accuracy: 38.604\n",
            "Validation Loss: 2.38698296546936, Validation Accuracy: 38.79\n",
            "[76/150]: Training Loss: 2.4025228948009256, Training Accuracy: 38.79\n",
            "Validation Loss: 2.379944348335266, Validation Accuracy: 39.32\n",
            "[77/150]: Training Loss: 2.402056786478782, Training Accuracy: 38.59\n",
            "Validation Loss: 2.367121386528015, Validation Accuracy: 39.44\n",
            "[78/150]: Training Loss: 2.39358198886015, Training Accuracy: 38.706\n",
            "Validation Loss: 2.360290360450745, Validation Accuracy: 39.64\n",
            "[79/150]: Training Loss: 2.385896707067684, Training Accuracy: 38.97\n",
            "Validation Loss: 2.362123727798462, Validation Accuracy: 39.74\n",
            "[80/150]: Training Loss: 2.3822805248961156, Training Accuracy: 38.96\n",
            "Validation Loss: 2.355213189125061, Validation Accuracy: 39.86\n",
            "[81/150]: Training Loss: 2.3766152566793015, Training Accuracy: 39.064\n",
            "Validation Loss: 2.354436731338501, Validation Accuracy: 39.82\n",
            "[82/150]: Training Loss: 2.3680763390599466, Training Accuracy: 39.428\n",
            "Validation Loss: 2.3397852420806884, Validation Accuracy: 40.27\n",
            "[83/150]: Training Loss: 2.3645690363280627, Training Accuracy: 39.386\n",
            "Validation Loss: 2.340263819694519, Validation Accuracy: 40.03\n",
            "[84/150]: Training Loss: 2.3626577708185934, Training Accuracy: 39.516\n",
            "Validation Loss: 2.3479089021682737, Validation Accuracy: 39.5\n",
            "[85/150]: Training Loss: 2.356617353400406, Training Accuracy: 39.298\n",
            "Validation Loss: 2.3384116172790526, Validation Accuracy: 39.85\n",
            "[86/150]: Training Loss: 2.350528507816548, Training Accuracy: 39.64\n",
            "Validation Loss: 2.3308178663253782, Validation Accuracy: 40.42\n",
            "[87/150]: Training Loss: 2.3518636421281465, Training Accuracy: 39.588\n",
            "Validation Loss: 2.332779359817505, Validation Accuracy: 40.17\n",
            "[88/150]: Training Loss: 2.339253683479465, Training Accuracy: 39.942\n",
            "Validation Loss: 2.3242568492889406, Validation Accuracy: 40.35\n",
            "[89/150]: Training Loss: 2.331887250043908, Training Accuracy: 39.994\n",
            "Validation Loss: 2.3254419326782227, Validation Accuracy: 40.36\n",
            "[90/150]: Training Loss: 2.330076134934717, Training Accuracy: 40.046\n",
            "Validation Loss: 2.3118756771087647, Validation Accuracy: 40.79\n",
            "[91/150]: Training Loss: 2.3228837275991636, Training Accuracy: 40.382\n",
            "Validation Loss: 2.308647394180298, Validation Accuracy: 40.81\n",
            "[92/150]: Training Loss: 2.3223668945078946, Training Accuracy: 40.278\n",
            "Validation Loss: 2.3109630823135374, Validation Accuracy: 40.91\n",
            "[93/150]: Training Loss: 2.3131886890956332, Training Accuracy: 40.396\n",
            "Validation Loss: 2.3050234794616697, Validation Accuracy: 40.87\n",
            "[94/150]: Training Loss: 2.3118463730325503, Training Accuracy: 40.442\n",
            "Validation Loss: 2.309488868713379, Validation Accuracy: 40.79\n",
            "[95/150]: Training Loss: 2.31477233341762, Training Accuracy: 40.426\n",
            "Validation Loss: 2.3041553497314453, Validation Accuracy: 41.03\n",
            "[96/150]: Training Loss: 2.3052495450389627, Training Accuracy: 40.74\n",
            "Validation Loss: 2.2959155321121214, Validation Accuracy: 41.33\n",
            "[97/150]: Training Loss: 2.2986154750901826, Training Accuracy: 41.036\n",
            "Validation Loss: 2.291999101638794, Validation Accuracy: 41.16\n",
            "[98/150]: Training Loss: 2.296130083045181, Training Accuracy: 41.05\n",
            "Validation Loss: 2.296833896636963, Validation Accuracy: 40.89\n",
            "[99/150]: Training Loss: 2.293913335216289, Training Accuracy: 40.574\n",
            "Validation Loss: 2.2895299196243286, Validation Accuracy: 41.51\n",
            "[100/150]: Training Loss: 2.289002457443549, Training Accuracy: 40.874\n",
            "Validation Loss: 2.296149969100952, Validation Accuracy: 41.25\n",
            "[101/150]: Training Loss: 2.28773240653836, Training Accuracy: 40.816\n",
            "Validation Loss: 2.2874104261398314, Validation Accuracy: 41.22\n",
            "[102/150]: Training Loss: 2.2810704367501393, Training Accuracy: 41.238\n",
            "Validation Loss: 2.2846805810928346, Validation Accuracy: 41.36\n",
            "[103/150]: Training Loss: 2.2808507607907664, Training Accuracy: 41.062\n",
            "Validation Loss: 2.2863134384155273, Validation Accuracy: 41.45\n",
            "[104/150]: Training Loss: 2.281994581222534, Training Accuracy: 41.224\n",
            "Validation Loss: 2.2800295829772947, Validation Accuracy: 41.63\n",
            "[105/150]: Training Loss: 2.2791298846809234, Training Accuracy: 41.27\n",
            "Validation Loss: 2.276352620124817, Validation Accuracy: 41.69\n",
            "[106/150]: Training Loss: 2.274389768133358, Training Accuracy: 41.284\n",
            "Validation Loss: 2.277020573616028, Validation Accuracy: 41.82\n",
            "[107/150]: Training Loss: 2.266129561832973, Training Accuracy: 41.632\n",
            "Validation Loss: 2.2738038301467896, Validation Accuracy: 41.79\n",
            "[108/150]: Training Loss: 2.264082616689254, Training Accuracy: 41.726\n",
            "Validation Loss: 2.272693967819214, Validation Accuracy: 41.83\n",
            "[109/150]: Training Loss: 2.266490250217671, Training Accuracy: 41.69\n",
            "Validation Loss: 2.269743800163269, Validation Accuracy: 41.79\n",
            "[110/150]: Training Loss: 2.264426902848847, Training Accuracy: 41.53\n",
            "Validation Loss: 2.2705984592437742, Validation Accuracy: 41.75\n",
            "[111/150]: Training Loss: 2.259202057001542, Training Accuracy: 41.662\n",
            "Validation Loss: 2.266038990020752, Validation Accuracy: 41.82\n",
            "[112/150]: Training Loss: 2.25746505114497, Training Accuracy: 41.906\n",
            "Validation Loss: 2.265566897392273, Validation Accuracy: 41.83\n",
            "[113/150]: Training Loss: 2.253900381983543, Training Accuracy: 41.718\n",
            "Validation Loss: 2.268652153015137, Validation Accuracy: 41.67\n",
            "[114/150]: Training Loss: 2.258889456184543, Training Accuracy: 41.682\n",
            "Validation Loss: 2.264122176170349, Validation Accuracy: 41.95\n",
            "[115/150]: Training Loss: 2.2516084106601015, Training Accuracy: 41.906\n",
            "Validation Loss: 2.26185884475708, Validation Accuracy: 42.08\n",
            "[116/150]: Training Loss: 2.244454252476595, Training Accuracy: 41.716\n",
            "Validation Loss: 2.25951509475708, Validation Accuracy: 42.01\n",
            "[117/150]: Training Loss: 2.2538246329949825, Training Accuracy: 41.758\n",
            "Validation Loss: 2.257939171791077, Validation Accuracy: 42.16\n",
            "[118/150]: Training Loss: 2.2463515972604555, Training Accuracy: 41.924\n",
            "Validation Loss: 2.256657099723816, Validation Accuracy: 42.13\n",
            "[119/150]: Training Loss: 2.245320835892035, Training Accuracy: 41.94\n",
            "Validation Loss: 2.255589818954468, Validation Accuracy: 41.97\n",
            "[120/150]: Training Loss: 2.250691910179294, Training Accuracy: 42.04\n",
            "Validation Loss: 2.259244465827942, Validation Accuracy: 42.07\n",
            "[121/150]: Training Loss: 2.237687383379255, Training Accuracy: 42.29\n",
            "Validation Loss: 2.2565879344940187, Validation Accuracy: 42.27\n",
            "[122/150]: Training Loss: 2.2319457725602754, Training Accuracy: 42.16\n",
            "Validation Loss: 2.2570300817489626, Validation Accuracy: 42.15\n",
            "[123/150]: Training Loss: 2.2342230300514068, Training Accuracy: 42.226\n",
            "Validation Loss: 2.255305027961731, Validation Accuracy: 42.29\n",
            "[124/150]: Training Loss: 2.239549919050567, Training Accuracy: 42.174\n",
            "Validation Loss: 2.256370449066162, Validation Accuracy: 42.23\n",
            "[125/150]: Training Loss: 2.237409543017952, Training Accuracy: 42.23\n",
            "Validation Loss: 2.2512456655502318, Validation Accuracy: 42.3\n",
            "[126/150]: Training Loss: 2.2329092658295924, Training Accuracy: 42.402\n",
            "Validation Loss: 2.251137447357178, Validation Accuracy: 42.43\n",
            "[127/150]: Training Loss: 2.2352922595277125, Training Accuracy: 42.266\n",
            "Validation Loss: 2.2498720407485964, Validation Accuracy: 42.34\n",
            "[128/150]: Training Loss: 2.2299188545772006, Training Accuracy: 42.322\n",
            "Validation Loss: 2.248763990402222, Validation Accuracy: 42.27\n",
            "[129/150]: Training Loss: 2.2254644364726786, Training Accuracy: 42.516\n",
            "Validation Loss: 2.247484731674194, Validation Accuracy: 42.49\n",
            "[130/150]: Training Loss: 2.229749392489998, Training Accuracy: 42.22\n",
            "Validation Loss: 2.249401307106018, Validation Accuracy: 42.39\n",
            "[131/150]: Training Loss: 2.2227475886442223, Training Accuracy: 42.588\n",
            "Validation Loss: 2.248521161079407, Validation Accuracy: 42.35\n",
            "[132/150]: Training Loss: 2.2250091251061885, Training Accuracy: 42.428\n",
            "Validation Loss: 2.2475123167037965, Validation Accuracy: 42.33\n",
            "[133/150]: Training Loss: 2.2305113004178416, Training Accuracy: 42.204\n",
            "Validation Loss: 2.248475170135498, Validation Accuracy: 42.25\n",
            "[134/150]: Training Loss: 2.227376563208444, Training Accuracy: 42.406\n",
            "Validation Loss: 2.248474097251892, Validation Accuracy: 42.42\n",
            "[135/150]: Training Loss: 2.228417683620842, Training Accuracy: 42.378\n",
            "Validation Loss: 2.24667911529541, Validation Accuracy: 42.48\n",
            "[136/150]: Training Loss: 2.2207712056685467, Training Accuracy: 42.538\n",
            "Validation Loss: 2.2469127416610717, Validation Accuracy: 42.44\n",
            "[137/150]: Training Loss: 2.2226368310500164, Training Accuracy: 42.52\n",
            "Validation Loss: 2.246177053451538, Validation Accuracy: 42.41\n",
            "[138/150]: Training Loss: 2.2275044090893803, Training Accuracy: 42.468\n",
            "Validation Loss: 2.246805429458618, Validation Accuracy: 42.4\n",
            "[139/150]: Training Loss: 2.223565267056835, Training Accuracy: 42.634\n",
            "Validation Loss: 2.246001434326172, Validation Accuracy: 42.32\n",
            "[140/150]: Training Loss: 2.224436273380202, Training Accuracy: 42.37\n",
            "Validation Loss: 2.2460646629333496, Validation Accuracy: 42.42\n",
            "[141/150]: Training Loss: 2.228755454627835, Training Accuracy: 42.58\n",
            "Validation Loss: 2.2457078218460085, Validation Accuracy: 42.42\n",
            "[142/150]: Training Loss: 2.2257018916460933, Training Accuracy: 42.484\n",
            "Validation Loss: 2.24568395614624, Validation Accuracy: 42.43\n",
            "[143/150]: Training Loss: 2.224288507383697, Training Accuracy: 42.534\n",
            "Validation Loss: 2.245313835144043, Validation Accuracy: 42.47\n",
            "[144/150]: Training Loss: 2.2213949767910703, Training Accuracy: 42.712\n",
            "Validation Loss: 2.245689034461975, Validation Accuracy: 42.39\n",
            "[145/150]: Training Loss: 2.2182185795842386, Training Accuracy: 42.612\n",
            "Validation Loss: 2.2455665349960325, Validation Accuracy: 42.39\n",
            "[146/150]: Training Loss: 2.2254402540167986, Training Accuracy: 42.488\n",
            "Validation Loss: 2.245345687866211, Validation Accuracy: 42.39\n",
            "[147/150]: Training Loss: 2.2218092509678433, Training Accuracy: 42.64\n",
            "Validation Loss: 2.2454842805862425, Validation Accuracy: 42.42\n",
            "[148/150]: Training Loss: 2.2220640085181413, Training Accuracy: 42.318\n",
            "Validation Loss: 2.2455402135849, Validation Accuracy: 42.39\n",
            "[149/150]: Training Loss: 2.2183865965629113, Training Accuracy: 42.668\n",
            "Validation Loss: 2.2455331802368166, Validation Accuracy: 42.42\n",
            "[150/150]: Training Loss: 2.218842954051738, Training Accuracy: 42.57\n",
            "Validation Loss: 2.2455447912216187, Validation Accuracy: 42.42\n",
            "**********************************************************************\n",
            "Test Loss: 2.2455447912216187, Test Accuracy: 42.42\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▃▃▄▂▃▂▁▁▂</td></tr><tr><td>Test Loss</td><td>█▄▂▁▃▂▃▄▃▂</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>42.42</td></tr><tr><td>Test Loss</td><td>2.24554</td></tr><tr><td>Train Accuracy</td><td>42.57</td></tr><tr><td>Train Loss</td><td>2.21884</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=1024 learning_rate=0.024 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/gya1gfdd' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/gya1gfdd</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_032944-gya1gfdd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 2048\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_035348-1r088cdb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/1r088cdb' target=\"_blank\">batch_size=2048 learning_rate=0.048 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/1r088cdb' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/1r088cdb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.606891899108887, Training Accuracy: 0.96\n",
            "Validation Loss: 4.60687141418457, Validation Accuracy: 1.1\n",
            "[2/150]: Training Loss: 4.600610694885254, Training Accuracy: 1.736\n",
            "Validation Loss: 4.590463447570801, Validation Accuracy: 2.39\n",
            "[3/150]: Training Loss: 4.575713977813721, Training Accuracy: 2.896\n",
            "Validation Loss: 4.549876403808594, Validation Accuracy: 3.48\n",
            "[4/150]: Training Loss: 4.521224956512452, Training Accuracy: 3.332\n",
            "Validation Loss: 4.476098251342774, Validation Accuracy: 3.51\n",
            "[5/150]: Training Loss: 4.436832466125488, Training Accuracy: 3.676\n",
            "Validation Loss: 4.373760604858399, Validation Accuracy: 4.44\n",
            "[6/150]: Training Loss: 4.333190250396728, Training Accuracy: 4.484\n",
            "Validation Loss: 4.2619599342346195, Validation Accuracy: 5.74\n",
            "[7/150]: Training Loss: 4.228750267028809, Training Accuracy: 6.184\n",
            "Validation Loss: 4.158302211761475, Validation Accuracy: 7.0\n",
            "[8/150]: Training Loss: 4.14489444732666, Training Accuracy: 7.23\n",
            "Validation Loss: 4.080980014801026, Validation Accuracy: 8.09\n",
            "[9/150]: Training Loss: 4.086068439483642, Training Accuracy: 8.046\n",
            "Validation Loss: 4.022517251968384, Validation Accuracy: 9.23\n",
            "[10/150]: Training Loss: 4.038323402404785, Training Accuracy: 8.906\n",
            "Validation Loss: 3.973035955429077, Validation Accuracy: 10.09\n",
            "[11/150]: Training Loss: 3.9903813552856446, Training Accuracy: 9.57\n",
            "Validation Loss: 3.9220180988311766, Validation Accuracy: 11.14\n",
            "[12/150]: Training Loss: 3.9483608531951906, Training Accuracy: 10.134\n",
            "Validation Loss: 3.8770122051239015, Validation Accuracy: 11.85\n",
            "[13/150]: Training Loss: 3.906474552154541, Training Accuracy: 11.124\n",
            "Validation Loss: 3.830677843093872, Validation Accuracy: 12.24\n",
            "[14/150]: Training Loss: 3.8657492351531983, Training Accuracy: 11.712\n",
            "Validation Loss: 3.784984064102173, Validation Accuracy: 13.01\n",
            "[15/150]: Training Loss: 3.827094488143921, Training Accuracy: 12.232\n",
            "Validation Loss: 3.747487115859985, Validation Accuracy: 13.69\n",
            "[16/150]: Training Loss: 3.7882041549682617, Training Accuracy: 12.894\n",
            "Validation Loss: 3.706352472305298, Validation Accuracy: 14.75\n",
            "[17/150]: Training Loss: 3.7569018363952638, Training Accuracy: 13.466\n",
            "Validation Loss: 3.6668768405914305, Validation Accuracy: 15.0\n",
            "[18/150]: Training Loss: 3.724953260421753, Training Accuracy: 13.934\n",
            "Validation Loss: 3.6342330455780028, Validation Accuracy: 15.84\n",
            "[19/150]: Training Loss: 3.6888757514953614, Training Accuracy: 14.426\n",
            "Validation Loss: 3.602110433578491, Validation Accuracy: 16.34\n",
            "[20/150]: Training Loss: 3.6559412384033205, Training Accuracy: 15.034\n",
            "Validation Loss: 3.5667267799377442, Validation Accuracy: 16.84\n",
            "[21/150]: Training Loss: 3.621606273651123, Training Accuracy: 15.512\n",
            "Validation Loss: 3.532243776321411, Validation Accuracy: 17.51\n",
            "[22/150]: Training Loss: 3.586877536773682, Training Accuracy: 16.244\n",
            "Validation Loss: 3.498146724700928, Validation Accuracy: 17.93\n",
            "[23/150]: Training Loss: 3.5530471801757812, Training Accuracy: 16.71\n",
            "Validation Loss: 3.469740056991577, Validation Accuracy: 18.54\n",
            "[24/150]: Training Loss: 3.523145809173584, Training Accuracy: 17.322\n",
            "Validation Loss: 3.429482364654541, Validation Accuracy: 19.66\n",
            "[25/150]: Training Loss: 3.492525005340576, Training Accuracy: 17.878\n",
            "Validation Loss: 3.4018688201904297, Validation Accuracy: 19.37\n",
            "[26/150]: Training Loss: 3.462628803253174, Training Accuracy: 18.3\n",
            "Validation Loss: 3.3740827560424806, Validation Accuracy: 20.22\n",
            "[27/150]: Training Loss: 3.4355313301086428, Training Accuracy: 18.766\n",
            "Validation Loss: 3.345688581466675, Validation Accuracy: 20.63\n",
            "[28/150]: Training Loss: 3.4118361473083496, Training Accuracy: 19.086\n",
            "Validation Loss: 3.3152890682220457, Validation Accuracy: 21.47\n",
            "[29/150]: Training Loss: 3.3853207206726075, Training Accuracy: 19.684\n",
            "Validation Loss: 3.288516616821289, Validation Accuracy: 21.77\n",
            "[30/150]: Training Loss: 3.357349433898926, Training Accuracy: 20.182\n",
            "Validation Loss: 3.272135925292969, Validation Accuracy: 21.96\n",
            "[31/150]: Training Loss: 3.33790997505188, Training Accuracy: 20.556\n",
            "Validation Loss: 3.2512518882751467, Validation Accuracy: 22.86\n",
            "[32/150]: Training Loss: 3.3162687015533447, Training Accuracy: 20.618\n",
            "Validation Loss: 3.2230645179748536, Validation Accuracy: 22.77\n",
            "[33/150]: Training Loss: 3.2954384326934814, Training Accuracy: 21.236\n",
            "Validation Loss: 3.209771013259888, Validation Accuracy: 23.22\n",
            "[34/150]: Training Loss: 3.2834206104278563, Training Accuracy: 21.342\n",
            "Validation Loss: 3.1870476245880126, Validation Accuracy: 23.97\n",
            "[35/150]: Training Loss: 3.255247573852539, Training Accuracy: 21.83\n",
            "Validation Loss: 3.1760802268981934, Validation Accuracy: 24.2\n",
            "[36/150]: Training Loss: 3.244738836288452, Training Accuracy: 22.18\n",
            "Validation Loss: 3.149981164932251, Validation Accuracy: 24.63\n",
            "[37/150]: Training Loss: 3.2233246517181398, Training Accuracy: 22.646\n",
            "Validation Loss: 3.1303664207458497, Validation Accuracy: 24.92\n",
            "[38/150]: Training Loss: 3.2000336933135984, Training Accuracy: 23.05\n",
            "Validation Loss: 3.1102230072021486, Validation Accuracy: 25.24\n",
            "[39/150]: Training Loss: 3.1809381294250487, Training Accuracy: 23.32\n",
            "Validation Loss: 3.0988600730895994, Validation Accuracy: 25.57\n",
            "[40/150]: Training Loss: 3.1758554553985596, Training Accuracy: 23.448\n",
            "Validation Loss: 3.0785787105560303, Validation Accuracy: 25.66\n",
            "[41/150]: Training Loss: 3.1560576248168943, Training Accuracy: 23.69\n",
            "Validation Loss: 3.056659126281738, Validation Accuracy: 26.3\n",
            "[42/150]: Training Loss: 3.136617965698242, Training Accuracy: 24.108\n",
            "Validation Loss: 3.044490671157837, Validation Accuracy: 26.49\n",
            "[43/150]: Training Loss: 3.1227911949157714, Training Accuracy: 24.27\n",
            "Validation Loss: 3.029969644546509, Validation Accuracy: 26.68\n",
            "[44/150]: Training Loss: 3.102838659286499, Training Accuracy: 24.826\n",
            "Validation Loss: 3.0146406173706053, Validation Accuracy: 26.97\n",
            "[45/150]: Training Loss: 3.094334297180176, Training Accuracy: 24.89\n",
            "Validation Loss: 2.998156213760376, Validation Accuracy: 27.54\n",
            "[46/150]: Training Loss: 3.075647096633911, Training Accuracy: 25.27\n",
            "Validation Loss: 2.9830405712127686, Validation Accuracy: 27.7\n",
            "[47/150]: Training Loss: 3.0576992225646973, Training Accuracy: 25.456\n",
            "Validation Loss: 2.972663164138794, Validation Accuracy: 27.56\n",
            "[48/150]: Training Loss: 3.0457812404632567, Training Accuracy: 25.736\n",
            "Validation Loss: 2.957260274887085, Validation Accuracy: 28.21\n",
            "[49/150]: Training Loss: 3.0373900318145752, Training Accuracy: 25.894\n",
            "Validation Loss: 2.9418825626373293, Validation Accuracy: 28.5\n",
            "[50/150]: Training Loss: 3.020056085586548, Training Accuracy: 26.326\n",
            "Validation Loss: 2.929175853729248, Validation Accuracy: 28.88\n",
            "[51/150]: Training Loss: 3.0056448650360106, Training Accuracy: 26.46\n",
            "Validation Loss: 2.9070215702056883, Validation Accuracy: 29.27\n",
            "[52/150]: Training Loss: 2.9939701843261717, Training Accuracy: 26.762\n",
            "Validation Loss: 2.8945343494415283, Validation Accuracy: 29.48\n",
            "[53/150]: Training Loss: 2.9771877002716063, Training Accuracy: 27.252\n",
            "Validation Loss: 2.885859823226929, Validation Accuracy: 29.39\n",
            "[54/150]: Training Loss: 2.9624980735778808, Training Accuracy: 27.476\n",
            "Validation Loss: 2.87581090927124, Validation Accuracy: 29.49\n",
            "[55/150]: Training Loss: 2.9567051315307618, Training Accuracy: 27.696\n",
            "Validation Loss: 2.8627774238586428, Validation Accuracy: 29.88\n",
            "[56/150]: Training Loss: 2.945523920059204, Training Accuracy: 27.744\n",
            "Validation Loss: 2.855295705795288, Validation Accuracy: 29.62\n",
            "[57/150]: Training Loss: 2.93146315574646, Training Accuracy: 27.892\n",
            "Validation Loss: 2.8434480667114257, Validation Accuracy: 30.07\n",
            "[58/150]: Training Loss: 2.924803171157837, Training Accuracy: 27.968\n",
            "Validation Loss: 2.823704719543457, Validation Accuracy: 30.61\n",
            "[59/150]: Training Loss: 2.9088122272491455, Training Accuracy: 28.24\n",
            "Validation Loss: 2.8173577785491943, Validation Accuracy: 30.69\n",
            "[60/150]: Training Loss: 2.8987307167053222, Training Accuracy: 28.58\n",
            "Validation Loss: 2.809761810302734, Validation Accuracy: 30.93\n",
            "[61/150]: Training Loss: 2.8892831802368164, Training Accuracy: 28.828\n",
            "Validation Loss: 2.7993114948272706, Validation Accuracy: 31.22\n",
            "[62/150]: Training Loss: 2.874851484298706, Training Accuracy: 29.142\n",
            "Validation Loss: 2.7836734771728517, Validation Accuracy: 31.41\n",
            "[63/150]: Training Loss: 2.869437532424927, Training Accuracy: 29.33\n",
            "Validation Loss: 2.7782646656036376, Validation Accuracy: 31.29\n",
            "[64/150]: Training Loss: 2.8525683212280275, Training Accuracy: 29.672\n",
            "Validation Loss: 2.766908121109009, Validation Accuracy: 31.82\n",
            "[65/150]: Training Loss: 2.847346153259277, Training Accuracy: 29.614\n",
            "Validation Loss: 2.7592673778533934, Validation Accuracy: 31.8\n",
            "[66/150]: Training Loss: 2.838597478866577, Training Accuracy: 29.832\n",
            "Validation Loss: 2.746263265609741, Validation Accuracy: 31.86\n",
            "[67/150]: Training Loss: 2.8289484119415285, Training Accuracy: 29.956\n",
            "Validation Loss: 2.7383673667907713, Validation Accuracy: 32.15\n",
            "[68/150]: Training Loss: 2.818249168395996, Training Accuracy: 29.954\n",
            "Validation Loss: 2.723827075958252, Validation Accuracy: 32.35\n",
            "[69/150]: Training Loss: 2.8115413284301756, Training Accuracy: 30.47\n",
            "Validation Loss: 2.7157896995544433, Validation Accuracy: 33.16\n",
            "[70/150]: Training Loss: 2.796581230163574, Training Accuracy: 30.382\n",
            "Validation Loss: 2.7060704708099363, Validation Accuracy: 33.03\n",
            "[71/150]: Training Loss: 2.788346128463745, Training Accuracy: 30.692\n",
            "Validation Loss: 2.6914602279663087, Validation Accuracy: 33.27\n",
            "[72/150]: Training Loss: 2.779642858505249, Training Accuracy: 30.64\n",
            "Validation Loss: 2.691270637512207, Validation Accuracy: 33.74\n",
            "[73/150]: Training Loss: 2.773522415161133, Training Accuracy: 30.952\n",
            "Validation Loss: 2.6753844261169433, Validation Accuracy: 33.72\n",
            "[74/150]: Training Loss: 2.751913080215454, Training Accuracy: 31.466\n",
            "Validation Loss: 2.662951850891113, Validation Accuracy: 33.7\n",
            "[75/150]: Training Loss: 2.745556564331055, Training Accuracy: 31.714\n",
            "Validation Loss: 2.6605172634124754, Validation Accuracy: 33.77\n",
            "[76/150]: Training Loss: 2.7446984481811523, Training Accuracy: 31.672\n",
            "Validation Loss: 2.657077407836914, Validation Accuracy: 33.73\n",
            "[77/150]: Training Loss: 2.7300216293334962, Training Accuracy: 31.946\n",
            "Validation Loss: 2.643053579330444, Validation Accuracy: 34.25\n",
            "[78/150]: Training Loss: 2.7262183570861818, Training Accuracy: 31.986\n",
            "Validation Loss: 2.6381571292877197, Validation Accuracy: 34.28\n",
            "[79/150]: Training Loss: 2.7191726398468017, Training Accuracy: 32.246\n",
            "Validation Loss: 2.627451467514038, Validation Accuracy: 34.45\n",
            "[80/150]: Training Loss: 2.7006086349487304, Training Accuracy: 32.402\n",
            "Validation Loss: 2.6252679347991945, Validation Accuracy: 34.61\n",
            "[81/150]: Training Loss: 2.7075089263916015, Training Accuracy: 32.438\n",
            "Validation Loss: 2.6192337036132813, Validation Accuracy: 34.21\n",
            "[82/150]: Training Loss: 2.702421941757202, Training Accuracy: 32.624\n",
            "Validation Loss: 2.6089219570159914, Validation Accuracy: 34.72\n",
            "[83/150]: Training Loss: 2.685064287185669, Training Accuracy: 32.76\n",
            "Validation Loss: 2.601409578323364, Validation Accuracy: 34.82\n",
            "[84/150]: Training Loss: 2.6799155330657958, Training Accuracy: 32.842\n",
            "Validation Loss: 2.59562029838562, Validation Accuracy: 35.02\n",
            "[85/150]: Training Loss: 2.6843585395812988, Training Accuracy: 32.746\n",
            "Validation Loss: 2.594114303588867, Validation Accuracy: 35.06\n",
            "[86/150]: Training Loss: 2.6704106616973875, Training Accuracy: 33.22\n",
            "Validation Loss: 2.593081569671631, Validation Accuracy: 35.13\n",
            "[87/150]: Training Loss: 2.668564338684082, Training Accuracy: 33.186\n",
            "Validation Loss: 2.5836416244506837, Validation Accuracy: 35.4\n",
            "[88/150]: Training Loss: 2.662708683013916, Training Accuracy: 33.212\n",
            "Validation Loss: 2.5814727306365968, Validation Accuracy: 35.44\n",
            "[89/150]: Training Loss: 2.6585529041290283, Training Accuracy: 33.444\n",
            "Validation Loss: 2.5715579986572266, Validation Accuracy: 35.67\n",
            "[90/150]: Training Loss: 2.6515896797180174, Training Accuracy: 33.474\n",
            "Validation Loss: 2.5656859397888185, Validation Accuracy: 35.65\n",
            "[91/150]: Training Loss: 2.6523977279663087, Training Accuracy: 33.45\n",
            "Validation Loss: 2.566520643234253, Validation Accuracy: 35.62\n",
            "[92/150]: Training Loss: 2.6442916107177736, Training Accuracy: 33.936\n",
            "Validation Loss: 2.5638641357421874, Validation Accuracy: 35.49\n",
            "[93/150]: Training Loss: 2.6416356277465822, Training Accuracy: 33.602\n",
            "Validation Loss: 2.562070035934448, Validation Accuracy: 35.91\n",
            "[94/150]: Training Loss: 2.628076629638672, Training Accuracy: 33.744\n",
            "Validation Loss: 2.5538381576538085, Validation Accuracy: 35.99\n",
            "[95/150]: Training Loss: 2.6320537185668944, Training Accuracy: 34.032\n",
            "Validation Loss: 2.5495237827301027, Validation Accuracy: 35.75\n",
            "[96/150]: Training Loss: 2.628206911087036, Training Accuracy: 34.152\n",
            "Validation Loss: 2.545219898223877, Validation Accuracy: 36.1\n",
            "[97/150]: Training Loss: 2.6205778026580813, Training Accuracy: 34.186\n",
            "Validation Loss: 2.542753982543945, Validation Accuracy: 36.02\n",
            "[98/150]: Training Loss: 2.617592248916626, Training Accuracy: 34.176\n",
            "Validation Loss: 2.542101335525513, Validation Accuracy: 35.98\n",
            "[99/150]: Training Loss: 2.6197222805023195, Training Accuracy: 34.094\n",
            "Validation Loss: 2.538325071334839, Validation Accuracy: 36.14\n",
            "[100/150]: Training Loss: 2.6128591823577882, Training Accuracy: 34.212\n",
            "Validation Loss: 2.535270118713379, Validation Accuracy: 36.06\n",
            "[101/150]: Training Loss: 2.609950532913208, Training Accuracy: 34.26\n",
            "Validation Loss: 2.5293672561645506, Validation Accuracy: 36.23\n",
            "[102/150]: Training Loss: 2.6042222595214843, Training Accuracy: 34.486\n",
            "Validation Loss: 2.5314455032348633, Validation Accuracy: 36.4\n",
            "[103/150]: Training Loss: 2.60733717918396, Training Accuracy: 34.668\n",
            "Validation Loss: 2.5286536693572996, Validation Accuracy: 36.22\n",
            "[104/150]: Training Loss: 2.600928258895874, Training Accuracy: 34.68\n",
            "Validation Loss: 2.5272301197052003, Validation Accuracy: 36.12\n",
            "[105/150]: Training Loss: 2.5978956508636473, Training Accuracy: 34.822\n",
            "Validation Loss: 2.521611976623535, Validation Accuracy: 36.39\n",
            "[106/150]: Training Loss: 2.5903251361846924, Training Accuracy: 34.832\n",
            "Validation Loss: 2.5163689136505125, Validation Accuracy: 36.51\n",
            "[107/150]: Training Loss: 2.594358024597168, Training Accuracy: 34.894\n",
            "Validation Loss: 2.5197267055511476, Validation Accuracy: 36.45\n",
            "[108/150]: Training Loss: 2.590383281707764, Training Accuracy: 34.85\n",
            "Validation Loss: 2.515224838256836, Validation Accuracy: 36.61\n",
            "[109/150]: Training Loss: 2.58540961265564, Training Accuracy: 34.91\n",
            "Validation Loss: 2.5137331008911135, Validation Accuracy: 36.72\n",
            "[110/150]: Training Loss: 2.5731867122650147, Training Accuracy: 35.144\n",
            "Validation Loss: 2.5086013793945314, Validation Accuracy: 36.91\n",
            "[111/150]: Training Loss: 2.5846965503692627, Training Accuracy: 34.988\n",
            "Validation Loss: 2.51100435256958, Validation Accuracy: 36.72\n",
            "[112/150]: Training Loss: 2.5743587589263917, Training Accuracy: 34.872\n",
            "Validation Loss: 2.5043835639953613, Validation Accuracy: 36.84\n",
            "[113/150]: Training Loss: 2.577282676696777, Training Accuracy: 35.01\n",
            "Validation Loss: 2.507812738418579, Validation Accuracy: 36.84\n",
            "[114/150]: Training Loss: 2.5752318000793455, Training Accuracy: 35.168\n",
            "Validation Loss: 2.5039757251739503, Validation Accuracy: 36.75\n",
            "[115/150]: Training Loss: 2.5756383037567137, Training Accuracy: 35.106\n",
            "Validation Loss: 2.5018725395202637, Validation Accuracy: 36.89\n",
            "[116/150]: Training Loss: 2.573223829269409, Training Accuracy: 35.25\n",
            "Validation Loss: 2.5003193855285644, Validation Accuracy: 37.2\n",
            "[117/150]: Training Loss: 2.5703605079650877, Training Accuracy: 35.122\n",
            "Validation Loss: 2.49833083152771, Validation Accuracy: 37.18\n",
            "[118/150]: Training Loss: 2.56836067199707, Training Accuracy: 35.334\n",
            "Validation Loss: 2.4982669830322264, Validation Accuracy: 36.95\n",
            "[119/150]: Training Loss: 2.568598871231079, Training Accuracy: 35.246\n",
            "Validation Loss: 2.4962736129760743, Validation Accuracy: 36.98\n",
            "[120/150]: Training Loss: 2.565032138824463, Training Accuracy: 35.226\n",
            "Validation Loss: 2.4972612857818604, Validation Accuracy: 37.05\n",
            "[121/150]: Training Loss: 2.564799222946167, Training Accuracy: 35.332\n",
            "Validation Loss: 2.494395542144775, Validation Accuracy: 37.02\n",
            "[122/150]: Training Loss: 2.5654745578765867, Training Accuracy: 35.408\n",
            "Validation Loss: 2.4940038681030274, Validation Accuracy: 37.03\n",
            "[123/150]: Training Loss: 2.567827205657959, Training Accuracy: 35.318\n",
            "Validation Loss: 2.4918994426727297, Validation Accuracy: 37.44\n",
            "[124/150]: Training Loss: 2.564124135971069, Training Accuracy: 35.482\n",
            "Validation Loss: 2.4921056747436525, Validation Accuracy: 37.23\n",
            "[125/150]: Training Loss: 2.5578205394744873, Training Accuracy: 35.414\n",
            "Validation Loss: 2.4925122261047363, Validation Accuracy: 37.1\n",
            "[126/150]: Training Loss: 2.553278617858887, Training Accuracy: 35.388\n",
            "Validation Loss: 2.491460990905762, Validation Accuracy: 37.3\n",
            "[127/150]: Training Loss: 2.556161298751831, Training Accuracy: 35.484\n",
            "Validation Loss: 2.4900985240936278, Validation Accuracy: 37.34\n",
            "[128/150]: Training Loss: 2.5557092952728273, Training Accuracy: 35.646\n",
            "Validation Loss: 2.487586164474487, Validation Accuracy: 37.36\n",
            "[129/150]: Training Loss: 2.560698127746582, Training Accuracy: 35.54\n",
            "Validation Loss: 2.4893627643585203, Validation Accuracy: 37.25\n",
            "[130/150]: Training Loss: 2.5523008346557616, Training Accuracy: 35.798\n",
            "Validation Loss: 2.4873109817504884, Validation Accuracy: 37.43\n",
            "[131/150]: Training Loss: 2.547942838668823, Training Accuracy: 35.822\n",
            "Validation Loss: 2.486782455444336, Validation Accuracy: 37.36\n",
            "[132/150]: Training Loss: 2.5490105152130127, Training Accuracy: 35.614\n",
            "Validation Loss: 2.4866137981414793, Validation Accuracy: 37.34\n",
            "[133/150]: Training Loss: 2.554526605606079, Training Accuracy: 35.53\n",
            "Validation Loss: 2.48647084236145, Validation Accuracy: 37.39\n",
            "[134/150]: Training Loss: 2.549386672973633, Training Accuracy: 35.682\n",
            "Validation Loss: 2.4857484340667724, Validation Accuracy: 37.31\n",
            "[135/150]: Training Loss: 2.5567483425140383, Training Accuracy: 35.614\n",
            "Validation Loss: 2.485041952133179, Validation Accuracy: 37.4\n",
            "[136/150]: Training Loss: 2.548656129837036, Training Accuracy: 35.698\n",
            "Validation Loss: 2.485407066345215, Validation Accuracy: 37.43\n",
            "[137/150]: Training Loss: 2.542435131072998, Training Accuracy: 35.692\n",
            "Validation Loss: 2.485075092315674, Validation Accuracy: 37.25\n",
            "[138/150]: Training Loss: 2.55237006187439, Training Accuracy: 35.72\n",
            "Validation Loss: 2.484128713607788, Validation Accuracy: 37.39\n",
            "[139/150]: Training Loss: 2.5439516735076904, Training Accuracy: 35.732\n",
            "Validation Loss: 2.4843750953674317, Validation Accuracy: 37.4\n",
            "[140/150]: Training Loss: 2.546170501708984, Training Accuracy: 35.786\n",
            "Validation Loss: 2.4839417934417725, Validation Accuracy: 37.32\n",
            "[141/150]: Training Loss: 2.546264410018921, Training Accuracy: 35.748\n",
            "Validation Loss: 2.4843177318573, Validation Accuracy: 37.35\n",
            "[142/150]: Training Loss: 2.546672296524048, Training Accuracy: 35.486\n",
            "Validation Loss: 2.48406925201416, Validation Accuracy: 37.43\n",
            "[143/150]: Training Loss: 2.543995885848999, Training Accuracy: 35.828\n",
            "Validation Loss: 2.4839701652526855, Validation Accuracy: 37.31\n",
            "[144/150]: Training Loss: 2.54662841796875, Training Accuracy: 35.866\n",
            "Validation Loss: 2.4839941024780274, Validation Accuracy: 37.33\n",
            "[145/150]: Training Loss: 2.55161771774292, Training Accuracy: 35.778\n",
            "Validation Loss: 2.483786201477051, Validation Accuracy: 37.34\n",
            "[146/150]: Training Loss: 2.551975059509277, Training Accuracy: 35.694\n",
            "Validation Loss: 2.4838438034057617, Validation Accuracy: 37.34\n",
            "[147/150]: Training Loss: 2.5511072635650636, Training Accuracy: 35.556\n",
            "Validation Loss: 2.4838553428649903, Validation Accuracy: 37.33\n",
            "[148/150]: Training Loss: 2.54339656829834, Training Accuracy: 35.836\n",
            "Validation Loss: 2.4838125705718994, Validation Accuracy: 37.35\n",
            "[149/150]: Training Loss: 2.541078643798828, Training Accuracy: 35.776\n",
            "Validation Loss: 2.483800745010376, Validation Accuracy: 37.32\n",
            "[150/150]: Training Loss: 2.5504231452941895, Training Accuracy: 35.95\n",
            "Validation Loss: 2.4837899684906004, Validation Accuracy: 37.34\n",
            "**********************************************************************\n",
            "Test Loss: 2.4837899684906004, Test Accuracy: 37.34\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▆▇█▁▃</td></tr><tr><td>Test Loss</td><td>█▁▁▅▂</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▃▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Train Loss</td><td>██▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>37.34</td></tr><tr><td>Test Loss</td><td>2.48379</td></tr><tr><td>Train Accuracy</td><td>35.95</td></tr><tr><td>Train Loss</td><td>2.55042</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=2048 learning_rate=0.048 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/1r088cdb' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/1r088cdb</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_035348-1r088cdb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 4096\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_041810-9zk0s9ui</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/9zk0s9ui' target=\"_blank\">batch_size=4096 learning_rate=0.096 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/9zk0s9ui' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/9zk0s9ui</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.606667481935942, Training Accuracy: 1.056\n",
            "Validation Loss: 4.606671492258708, Validation Accuracy: 1.19\n",
            "[2/150]: Training Loss: 4.605730753678542, Training Accuracy: 1.08\n",
            "Validation Loss: 4.6042531331380205, Validation Accuracy: 1.28\n",
            "[3/150]: Training Loss: 4.602307979877178, Training Accuracy: 1.302\n",
            "Validation Loss: 4.599176406860352, Validation Accuracy: 1.68\n",
            "[4/150]: Training Loss: 4.5949787726769085, Training Accuracy: 1.758\n",
            "Validation Loss: 4.587771574656169, Validation Accuracy: 2.21\n",
            "[5/150]: Training Loss: 4.581083957965557, Training Accuracy: 2.472\n",
            "Validation Loss: 4.569830258687337, Validation Accuracy: 2.95\n",
            "[6/150]: Training Loss: 4.561565582568829, Training Accuracy: 2.844\n",
            "Validation Loss: 4.543959299723308, Validation Accuracy: 3.23\n",
            "[7/150]: Training Loss: 4.530556715451754, Training Accuracy: 3.226\n",
            "Validation Loss: 4.509588241577148, Validation Accuracy: 3.39\n",
            "[8/150]: Training Loss: 4.495710409604586, Training Accuracy: 3.406\n",
            "Validation Loss: 4.4675553639729815, Validation Accuracy: 3.84\n",
            "[9/150]: Training Loss: 4.451528292435866, Training Accuracy: 3.844\n",
            "Validation Loss: 4.417957146962483, Validation Accuracy: 4.41\n",
            "[10/150]: Training Loss: 4.401393670302171, Training Accuracy: 4.492\n",
            "Validation Loss: 4.361032485961914, Validation Accuracy: 5.12\n",
            "[11/150]: Training Loss: 4.34503434254573, Training Accuracy: 5.242\n",
            "Validation Loss: 4.297556241353353, Validation Accuracy: 6.11\n",
            "[12/150]: Training Loss: 4.284046503213736, Training Accuracy: 6.066\n",
            "Validation Loss: 4.233114242553711, Validation Accuracy: 7.08\n",
            "[13/150]: Training Loss: 4.224347151242769, Training Accuracy: 6.906\n",
            "Validation Loss: 4.170989990234375, Validation Accuracy: 7.67\n",
            "[14/150]: Training Loss: 4.16760961826031, Training Accuracy: 7.38\n",
            "Validation Loss: 4.115687370300293, Validation Accuracy: 8.27\n",
            "[15/150]: Training Loss: 4.123450425954966, Training Accuracy: 7.966\n",
            "Validation Loss: 4.068022092183431, Validation Accuracy: 8.87\n",
            "[16/150]: Training Loss: 4.084108205942007, Training Accuracy: 8.202\n",
            "Validation Loss: 4.029605388641357, Validation Accuracy: 9.54\n",
            "[17/150]: Training Loss: 4.048959145179162, Training Accuracy: 8.832\n",
            "Validation Loss: 3.9942894776662192, Validation Accuracy: 10.03\n",
            "[18/150]: Training Loss: 4.015094628700843, Training Accuracy: 9.204\n",
            "Validation Loss: 3.961328665415446, Validation Accuracy: 10.5\n",
            "[19/150]: Training Loss: 3.9947778628422665, Training Accuracy: 9.404\n",
            "Validation Loss: 3.9311509132385254, Validation Accuracy: 10.79\n",
            "[20/150]: Training Loss: 3.967010424687312, Training Accuracy: 10.074\n",
            "Validation Loss: 3.904151280721029, Validation Accuracy: 11.22\n",
            "[21/150]: Training Loss: 3.940983533859253, Training Accuracy: 10.394\n",
            "Validation Loss: 3.8733415603637695, Validation Accuracy: 12.06\n",
            "[22/150]: Training Loss: 3.9169222758366513, Training Accuracy: 10.968\n",
            "Validation Loss: 3.849616289138794, Validation Accuracy: 12.37\n",
            "[23/150]: Training Loss: 3.891700506210327, Training Accuracy: 11.278\n",
            "Validation Loss: 3.8228482405344644, Validation Accuracy: 12.65\n",
            "[24/150]: Training Loss: 3.8753539415506215, Training Accuracy: 11.308\n",
            "Validation Loss: 3.7991214593251548, Validation Accuracy: 13.44\n",
            "[25/150]: Training Loss: 3.8501394345210147, Training Accuracy: 11.864\n",
            "Validation Loss: 3.7803680896759033, Validation Accuracy: 13.28\n",
            "[26/150]: Training Loss: 3.8272896363185, Training Accuracy: 12.248\n",
            "Validation Loss: 3.7514286041259766, Validation Accuracy: 13.69\n",
            "[27/150]: Training Loss: 3.8028186651376576, Training Accuracy: 12.736\n",
            "Validation Loss: 3.7285123666127524, Validation Accuracy: 14.23\n",
            "[28/150]: Training Loss: 3.784411540398231, Training Accuracy: 12.956\n",
            "Validation Loss: 3.7080076535542807, Validation Accuracy: 14.9\n",
            "[29/150]: Training Loss: 3.769466730264517, Training Accuracy: 13.47\n",
            "Validation Loss: 3.685424486796061, Validation Accuracy: 15.03\n",
            "[30/150]: Training Loss: 3.7380496722001295, Training Accuracy: 13.826\n",
            "Validation Loss: 3.6666505336761475, Validation Accuracy: 15.27\n",
            "[31/150]: Training Loss: 3.72714613034175, Training Accuracy: 14.114\n",
            "Validation Loss: 3.647770563761393, Validation Accuracy: 15.3\n",
            "[32/150]: Training Loss: 3.704881869829618, Training Accuracy: 14.22\n",
            "Validation Loss: 3.6258915265401206, Validation Accuracy: 15.94\n",
            "[33/150]: Training Loss: 3.6916760481320896, Training Accuracy: 14.512\n",
            "Validation Loss: 3.605382760365804, Validation Accuracy: 16.3\n",
            "[34/150]: Training Loss: 3.669787131823026, Training Accuracy: 15.118\n",
            "Validation Loss: 3.585141976674398, Validation Accuracy: 16.42\n",
            "[35/150]: Training Loss: 3.6509481026576114, Training Accuracy: 15.094\n",
            "Validation Loss: 3.565377871195475, Validation Accuracy: 16.71\n",
            "[36/150]: Training Loss: 3.627447036596445, Training Accuracy: 15.684\n",
            "Validation Loss: 3.5500471591949463, Validation Accuracy: 17.2\n",
            "[37/150]: Training Loss: 3.6156828036675086, Training Accuracy: 15.97\n",
            "Validation Loss: 3.5334328015645347, Validation Accuracy: 17.48\n",
            "[38/150]: Training Loss: 3.60913295012254, Training Accuracy: 15.936\n",
            "Validation Loss: 3.518000523249308, Validation Accuracy: 17.68\n",
            "[39/150]: Training Loss: 3.588004075563871, Training Accuracy: 16.056\n",
            "Validation Loss: 3.5036707719167075, Validation Accuracy: 17.79\n",
            "[40/150]: Training Loss: 3.572803992491502, Training Accuracy: 16.404\n",
            "Validation Loss: 3.4900471369425454, Validation Accuracy: 18.34\n",
            "[41/150]: Training Loss: 3.553792586693397, Training Accuracy: 16.82\n",
            "Validation Loss: 3.4707635243733725, Validation Accuracy: 18.5\n",
            "[42/150]: Training Loss: 3.5353387685922475, Training Accuracy: 17.062\n",
            "Validation Loss: 3.451948642730713, Validation Accuracy: 19.1\n",
            "[43/150]: Training Loss: 3.5242697642399716, Training Accuracy: 17.448\n",
            "Validation Loss: 3.440772612889608, Validation Accuracy: 18.73\n",
            "[44/150]: Training Loss: 3.508471268873948, Training Accuracy: 17.51\n",
            "Validation Loss: 3.4259134928385415, Validation Accuracy: 19.32\n",
            "[45/150]: Training Loss: 3.495282558294443, Training Accuracy: 17.764\n",
            "Validation Loss: 3.4083244800567627, Validation Accuracy: 19.93\n",
            "[46/150]: Training Loss: 3.4788230932675877, Training Accuracy: 17.988\n",
            "Validation Loss: 3.3957042694091797, Validation Accuracy: 20.13\n",
            "[47/150]: Training Loss: 3.4677468629983754, Training Accuracy: 18.18\n",
            "Validation Loss: 3.381241957346598, Validation Accuracy: 20.12\n",
            "[48/150]: Training Loss: 3.4516031558697042, Training Accuracy: 18.396\n",
            "Validation Loss: 3.3734585444132485, Validation Accuracy: 20.41\n",
            "[49/150]: Training Loss: 3.444180488586426, Training Accuracy: 18.762\n",
            "Validation Loss: 3.355231444040934, Validation Accuracy: 20.56\n",
            "[50/150]: Training Loss: 3.431383829850417, Training Accuracy: 18.72\n",
            "Validation Loss: 3.3463191191355386, Validation Accuracy: 21.08\n",
            "[51/150]: Training Loss: 3.4191522414867697, Training Accuracy: 18.926\n",
            "Validation Loss: 3.3322107791900635, Validation Accuracy: 21.28\n",
            "[52/150]: Training Loss: 3.4109142010028544, Training Accuracy: 19.188\n",
            "Validation Loss: 3.3198678493499756, Validation Accuracy: 21.38\n",
            "[53/150]: Training Loss: 3.389728619502141, Training Accuracy: 19.502\n",
            "Validation Loss: 3.3070958455403647, Validation Accuracy: 21.67\n",
            "[54/150]: Training Loss: 3.3762773000277004, Training Accuracy: 19.762\n",
            "Validation Loss: 3.298886696497599, Validation Accuracy: 21.9\n",
            "[55/150]: Training Loss: 3.3614331025343676, Training Accuracy: 20.024\n",
            "Validation Loss: 3.2792559464772544, Validation Accuracy: 22.34\n",
            "[56/150]: Training Loss: 3.3627426624298096, Training Accuracy: 19.906\n",
            "Validation Loss: 3.2689783573150635, Validation Accuracy: 22.21\n",
            "[57/150]: Training Loss: 3.3459411401015062, Training Accuracy: 20.326\n",
            "Validation Loss: 3.262605905532837, Validation Accuracy: 22.4\n",
            "[58/150]: Training Loss: 3.328116068473229, Training Accuracy: 20.592\n",
            "Validation Loss: 3.2577860355377197, Validation Accuracy: 22.57\n",
            "[59/150]: Training Loss: 3.319796947332529, Training Accuracy: 20.718\n",
            "Validation Loss: 3.244077761967977, Validation Accuracy: 22.72\n",
            "[60/150]: Training Loss: 3.3160109519958496, Training Accuracy: 20.926\n",
            "Validation Loss: 3.232391436894735, Validation Accuracy: 23.21\n",
            "[61/150]: Training Loss: 3.3058263888725867, Training Accuracy: 20.962\n",
            "Validation Loss: 3.2259323596954346, Validation Accuracy: 23.38\n",
            "[62/150]: Training Loss: 3.2940543431502123, Training Accuracy: 21.21\n",
            "Validation Loss: 3.2195600668589273, Validation Accuracy: 23.22\n",
            "[63/150]: Training Loss: 3.2899109033437877, Training Accuracy: 21.408\n",
            "Validation Loss: 3.20307191212972, Validation Accuracy: 23.73\n",
            "[64/150]: Training Loss: 3.2760523832761326, Training Accuracy: 21.614\n",
            "Validation Loss: 3.19364595413208, Validation Accuracy: 23.82\n",
            "[65/150]: Training Loss: 3.2687179492070126, Training Accuracy: 21.762\n",
            "Validation Loss: 3.188945452372233, Validation Accuracy: 23.98\n",
            "[66/150]: Training Loss: 3.263365232027494, Training Accuracy: 21.714\n",
            "Validation Loss: 3.180399020512899, Validation Accuracy: 24.13\n",
            "[67/150]: Training Loss: 3.2552486749795766, Training Accuracy: 21.902\n",
            "Validation Loss: 3.169262409210205, Validation Accuracy: 24.32\n",
            "[68/150]: Training Loss: 3.2468105462881236, Training Accuracy: 22.02\n",
            "Validation Loss: 3.1592602729797363, Validation Accuracy: 24.7\n",
            "[69/150]: Training Loss: 3.236006791775043, Training Accuracy: 22.332\n",
            "Validation Loss: 3.1546688079833984, Validation Accuracy: 24.36\n",
            "[70/150]: Training Loss: 3.2279206055861254, Training Accuracy: 22.532\n",
            "Validation Loss: 3.1467626094818115, Validation Accuracy: 24.62\n",
            "[71/150]: Training Loss: 3.2204081278580885, Training Accuracy: 22.422\n",
            "Validation Loss: 3.142202456792196, Validation Accuracy: 25.23\n",
            "[72/150]: Training Loss: 3.2053526181441088, Training Accuracy: 22.976\n",
            "Validation Loss: 3.1304725805918374, Validation Accuracy: 25.06\n",
            "[73/150]: Training Loss: 3.2087103036733775, Training Accuracy: 22.836\n",
            "Validation Loss: 3.121542453765869, Validation Accuracy: 25.43\n",
            "[74/150]: Training Loss: 3.21028942328233, Training Accuracy: 22.914\n",
            "Validation Loss: 3.1152079900105796, Validation Accuracy: 25.46\n",
            "[75/150]: Training Loss: 3.1951534748077393, Training Accuracy: 23.172\n",
            "Validation Loss: 3.1097386678059897, Validation Accuracy: 25.91\n",
            "[76/150]: Training Loss: 3.187066591702975, Training Accuracy: 23.242\n",
            "Validation Loss: 3.1020663579305015, Validation Accuracy: 25.53\n",
            "[77/150]: Training Loss: 3.181211379858164, Training Accuracy: 23.288\n",
            "Validation Loss: 3.0956215858459473, Validation Accuracy: 26.07\n",
            "[78/150]: Training Loss: 3.1798458099365234, Training Accuracy: 23.354\n",
            "Validation Loss: 3.092714786529541, Validation Accuracy: 25.77\n",
            "[79/150]: Training Loss: 3.1642683102534366, Training Accuracy: 23.748\n",
            "Validation Loss: 3.0828441778818765, Validation Accuracy: 26.06\n",
            "[80/150]: Training Loss: 3.159773588180542, Training Accuracy: 23.54\n",
            "Validation Loss: 3.081282138824463, Validation Accuracy: 26.08\n",
            "[81/150]: Training Loss: 3.1536663678976207, Training Accuracy: 23.836\n",
            "Validation Loss: 3.0736547311147056, Validation Accuracy: 26.57\n",
            "[82/150]: Training Loss: 3.152631429525522, Training Accuracy: 23.908\n",
            "Validation Loss: 3.0681962966918945, Validation Accuracy: 26.27\n",
            "[83/150]: Training Loss: 3.148173992450421, Training Accuracy: 24.21\n",
            "Validation Loss: 3.060869057973226, Validation Accuracy: 26.52\n",
            "[84/150]: Training Loss: 3.144138354521531, Training Accuracy: 24.266\n",
            "Validation Loss: 3.057458480199178, Validation Accuracy: 26.17\n",
            "[85/150]: Training Loss: 3.1297646302443285, Training Accuracy: 24.25\n",
            "Validation Loss: 3.051652987798055, Validation Accuracy: 26.52\n",
            "[86/150]: Training Loss: 3.1335733120258036, Training Accuracy: 24.256\n",
            "Validation Loss: 3.049645980199178, Validation Accuracy: 26.69\n",
            "[87/150]: Training Loss: 3.1256027221679688, Training Accuracy: 24.65\n",
            "Validation Loss: 3.0414721171061196, Validation Accuracy: 26.59\n",
            "[88/150]: Training Loss: 3.1209965302393985, Training Accuracy: 24.62\n",
            "Validation Loss: 3.038153092066447, Validation Accuracy: 26.63\n",
            "[89/150]: Training Loss: 3.1104712302868185, Training Accuracy: 24.64\n",
            "Validation Loss: 3.0309388637542725, Validation Accuracy: 27.11\n",
            "[90/150]: Training Loss: 3.1125464806189904, Training Accuracy: 24.67\n",
            "Validation Loss: 3.0235498746236167, Validation Accuracy: 27.1\n",
            "[91/150]: Training Loss: 3.1025814093076267, Training Accuracy: 24.932\n",
            "Validation Loss: 3.0206593672434487, Validation Accuracy: 26.91\n",
            "[92/150]: Training Loss: 3.0976960842426005, Training Accuracy: 25.086\n",
            "Validation Loss: 3.0188937981923423, Validation Accuracy: 27.18\n",
            "[93/150]: Training Loss: 3.1000913106478176, Training Accuracy: 25.098\n",
            "Validation Loss: 3.0101601282755532, Validation Accuracy: 27.45\n",
            "[94/150]: Training Loss: 3.0842209595900316, Training Accuracy: 25.102\n",
            "Validation Loss: 3.0081886450449624, Validation Accuracy: 27.32\n",
            "[95/150]: Training Loss: 3.089082112679115, Training Accuracy: 25.238\n",
            "Validation Loss: 3.004807790120443, Validation Accuracy: 27.51\n",
            "[96/150]: Training Loss: 3.0870832113119273, Training Accuracy: 25.588\n",
            "Validation Loss: 2.9993834495544434, Validation Accuracy: 27.42\n",
            "[97/150]: Training Loss: 3.0780730064098654, Training Accuracy: 25.422\n",
            "Validation Loss: 2.9940427939097085, Validation Accuracy: 27.54\n",
            "[98/150]: Training Loss: 3.0749858892880955, Training Accuracy: 25.39\n",
            "Validation Loss: 2.992240826288859, Validation Accuracy: 27.52\n",
            "[99/150]: Training Loss: 3.0701116231771617, Training Accuracy: 25.484\n",
            "Validation Loss: 2.991929848988851, Validation Accuracy: 27.38\n",
            "[100/150]: Training Loss: 3.075368826205914, Training Accuracy: 25.532\n",
            "Validation Loss: 2.985892375310262, Validation Accuracy: 27.66\n",
            "[101/150]: Training Loss: 3.0686124104719896, Training Accuracy: 25.752\n",
            "Validation Loss: 2.9830666383107505, Validation Accuracy: 27.62\n",
            "[102/150]: Training Loss: 3.065825737439669, Training Accuracy: 25.708\n",
            "Validation Loss: 2.9795896212259927, Validation Accuracy: 27.75\n",
            "[103/150]: Training Loss: 3.0559082948244534, Training Accuracy: 25.714\n",
            "Validation Loss: 2.9758129914601645, Validation Accuracy: 27.99\n",
            "[104/150]: Training Loss: 3.055809112695547, Training Accuracy: 25.83\n",
            "Validation Loss: 2.9747377236684165, Validation Accuracy: 28.08\n",
            "[105/150]: Training Loss: 3.051142399127667, Training Accuracy: 26.058\n",
            "Validation Loss: 2.9694248040517173, Validation Accuracy: 28.1\n",
            "[106/150]: Training Loss: 3.052405449060293, Training Accuracy: 25.962\n",
            "Validation Loss: 2.967036406199137, Validation Accuracy: 28.21\n",
            "[107/150]: Training Loss: 3.050378047502958, Training Accuracy: 26.184\n",
            "Validation Loss: 2.9633421103159585, Validation Accuracy: 28.25\n",
            "[108/150]: Training Loss: 3.049203927700336, Training Accuracy: 25.912\n",
            "Validation Loss: 2.960350831349691, Validation Accuracy: 28.27\n",
            "[109/150]: Training Loss: 3.039809043590839, Training Accuracy: 26.25\n",
            "Validation Loss: 2.9594175815582275, Validation Accuracy: 28.54\n",
            "[110/150]: Training Loss: 3.0399123521951528, Training Accuracy: 26.228\n",
            "Validation Loss: 2.956749359766642, Validation Accuracy: 28.35\n",
            "[111/150]: Training Loss: 3.035701183172373, Training Accuracy: 26.196\n",
            "Validation Loss: 2.953205664952596, Validation Accuracy: 28.27\n",
            "[112/150]: Training Loss: 3.033349422308115, Training Accuracy: 26.232\n",
            "Validation Loss: 2.9511603514353433, Validation Accuracy: 28.56\n",
            "[113/150]: Training Loss: 3.0366667050581713, Training Accuracy: 26.474\n",
            "Validation Loss: 2.9514257113138833, Validation Accuracy: 28.41\n",
            "[114/150]: Training Loss: 3.028848886489868, Training Accuracy: 26.362\n",
            "Validation Loss: 2.948322137196859, Validation Accuracy: 28.45\n",
            "[115/150]: Training Loss: 3.028550863265991, Training Accuracy: 26.532\n",
            "Validation Loss: 2.9465808073679605, Validation Accuracy: 28.55\n",
            "[116/150]: Training Loss: 3.0293847230764537, Training Accuracy: 26.382\n",
            "Validation Loss: 2.943213860193888, Validation Accuracy: 28.71\n",
            "[117/150]: Training Loss: 3.0208603785588193, Training Accuracy: 26.396\n",
            "Validation Loss: 2.944130261739095, Validation Accuracy: 28.47\n",
            "[118/150]: Training Loss: 3.027190428513747, Training Accuracy: 26.268\n",
            "Validation Loss: 2.9417835076649985, Validation Accuracy: 28.72\n",
            "[119/150]: Training Loss: 3.018063325148362, Training Accuracy: 26.58\n",
            "Validation Loss: 2.9406763712565103, Validation Accuracy: 28.57\n",
            "[120/150]: Training Loss: 3.0236737177922177, Training Accuracy: 26.436\n",
            "Validation Loss: 2.938833475112915, Validation Accuracy: 28.71\n",
            "[121/150]: Training Loss: 3.025139331817627, Training Accuracy: 26.506\n",
            "Validation Loss: 2.9386914571126304, Validation Accuracy: 28.59\n",
            "[122/150]: Training Loss: 3.019059492991521, Training Accuracy: 26.644\n",
            "Validation Loss: 2.9362939993540444, Validation Accuracy: 28.92\n",
            "[123/150]: Training Loss: 3.026591025865995, Training Accuracy: 26.702\n",
            "Validation Loss: 2.93481437365214, Validation Accuracy: 28.77\n",
            "[124/150]: Training Loss: 3.0196935580326962, Training Accuracy: 26.838\n",
            "Validation Loss: 2.9332067171732583, Validation Accuracy: 28.81\n",
            "[125/150]: Training Loss: 3.011383111660297, Training Accuracy: 26.388\n",
            "Validation Loss: 2.9336490631103516, Validation Accuracy: 28.87\n",
            "[126/150]: Training Loss: 3.0112004463489237, Training Accuracy: 26.728\n",
            "Validation Loss: 2.932124614715576, Validation Accuracy: 28.88\n",
            "[127/150]: Training Loss: 3.0043031985943136, Training Accuracy: 26.746\n",
            "Validation Loss: 2.9308346112569175, Validation Accuracy: 28.92\n",
            "[128/150]: Training Loss: 3.0122800973745494, Training Accuracy: 26.708\n",
            "Validation Loss: 2.9301002820332847, Validation Accuracy: 28.88\n",
            "[129/150]: Training Loss: 3.0128011336693397, Training Accuracy: 26.618\n",
            "Validation Loss: 2.9301366806030273, Validation Accuracy: 28.94\n",
            "[130/150]: Training Loss: 3.021453068806575, Training Accuracy: 26.574\n",
            "Validation Loss: 2.9286012649536133, Validation Accuracy: 28.95\n",
            "[131/150]: Training Loss: 3.0087369772104116, Training Accuracy: 26.808\n",
            "Validation Loss: 2.9288355509440103, Validation Accuracy: 28.86\n",
            "[132/150]: Training Loss: 3.009090882081252, Training Accuracy: 26.898\n",
            "Validation Loss: 2.927795886993408, Validation Accuracy: 28.9\n",
            "[133/150]: Training Loss: 3.0040540328392615, Training Accuracy: 26.832\n",
            "Validation Loss: 2.926760117212931, Validation Accuracy: 28.94\n",
            "[134/150]: Training Loss: 3.0087718413426328, Training Accuracy: 26.818\n",
            "Validation Loss: 2.927534898122152, Validation Accuracy: 28.89\n",
            "[135/150]: Training Loss: 3.0036952312176046, Training Accuracy: 26.822\n",
            "Validation Loss: 2.926776965459188, Validation Accuracy: 28.95\n",
            "[136/150]: Training Loss: 3.006104597678551, Training Accuracy: 26.948\n",
            "Validation Loss: 2.925653616587321, Validation Accuracy: 29.02\n",
            "[137/150]: Training Loss: 3.005603166726919, Training Accuracy: 26.876\n",
            "Validation Loss: 2.925398667653402, Validation Accuracy: 28.93\n",
            "[138/150]: Training Loss: 3.008038960970365, Training Accuracy: 26.988\n",
            "Validation Loss: 2.925060510635376, Validation Accuracy: 28.97\n",
            "[139/150]: Training Loss: 3.006507231638982, Training Accuracy: 26.72\n",
            "Validation Loss: 2.9248908360799155, Validation Accuracy: 28.91\n",
            "[140/150]: Training Loss: 3.0022271413069506, Training Accuracy: 26.81\n",
            "Validation Loss: 2.924588123957316, Validation Accuracy: 29.03\n",
            "[141/150]: Training Loss: 3.011791944503784, Training Accuracy: 26.896\n",
            "Validation Loss: 2.925060828526815, Validation Accuracy: 28.91\n",
            "[142/150]: Training Loss: 3.007263256953313, Training Accuracy: 26.982\n",
            "Validation Loss: 2.924833297729492, Validation Accuracy: 28.9\n",
            "[143/150]: Training Loss: 3.0038411617279053, Training Accuracy: 26.82\n",
            "Validation Loss: 2.924312194188436, Validation Accuracy: 29.02\n",
            "[144/150]: Training Loss: 3.0007228667919454, Training Accuracy: 26.974\n",
            "Validation Loss: 2.9242995580037436, Validation Accuracy: 28.92\n",
            "[145/150]: Training Loss: 3.0014155277839074, Training Accuracy: 26.706\n",
            "Validation Loss: 2.92417041460673, Validation Accuracy: 28.95\n",
            "[146/150]: Training Loss: 3.0123214538280783, Training Accuracy: 26.89\n",
            "Validation Loss: 2.924126386642456, Validation Accuracy: 28.91\n",
            "[147/150]: Training Loss: 3.008448545749371, Training Accuracy: 26.812\n",
            "Validation Loss: 2.9240718682607016, Validation Accuracy: 28.92\n",
            "[148/150]: Training Loss: 3.0003258815178504, Training Accuracy: 26.644\n",
            "Validation Loss: 2.9240763187408447, Validation Accuracy: 28.91\n",
            "[149/150]: Training Loss: 3.0013261208167443, Training Accuracy: 26.96\n",
            "Validation Loss: 2.924076795578003, Validation Accuracy: 28.9\n",
            "[150/150]: Training Loss: 3.001100833599384, Training Accuracy: 26.888\n",
            "Validation Loss: 2.9240763187408447, Validation Accuracy: 28.9\n",
            "**********************************************************************\n",
            "Test Loss: 2.9240763187408447, Test Accuracy: 28.9\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▁▇</td></tr><tr><td>Test Loss</td><td>▁█▂</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Train Loss</td><td>███▇▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>28.9</td></tr><tr><td>Test Loss</td><td>2.92408</td></tr><tr><td>Train Accuracy</td><td>26.888</td></tr><tr><td>Train Loss</td><td>3.0011</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=4096 learning_rate=0.096 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/9zk0s9ui' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/9zk0s9ui</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_041810-9zk0s9ui/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 8192\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_044434-2i4ynggq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/2i4ynggq' target=\"_blank\">batch_size=8192 learning_rate=0.192 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/2i4ynggq' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/2i4ynggq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.606760025024414, Training Accuracy: 1.154\n",
            "Validation Loss: 4.6062242190043134, Validation Accuracy: 1.16\n",
            "[2/150]: Training Loss: 4.606538149026724, Training Accuracy: 1.112\n",
            "Validation Loss: 4.605688412984212, Validation Accuracy: 1.25\n",
            "[3/150]: Training Loss: 4.605847468742957, Training Accuracy: 1.244\n",
            "Validation Loss: 4.6046427090962725, Validation Accuracy: 1.37\n",
            "[4/150]: Training Loss: 4.604880149547871, Training Accuracy: 1.37\n",
            "Validation Loss: 4.603097438812256, Validation Accuracy: 1.75\n",
            "[5/150]: Training Loss: 4.602994515345647, Training Accuracy: 1.682\n",
            "Validation Loss: 4.600901285807292, Validation Accuracy: 2.09\n",
            "[6/150]: Training Loss: 4.600499519935021, Training Accuracy: 1.924\n",
            "Validation Loss: 4.597598870595296, Validation Accuracy: 2.29\n",
            "[7/150]: Training Loss: 4.596665969261756, Training Accuracy: 2.098\n",
            "Validation Loss: 4.592558860778809, Validation Accuracy: 2.64\n",
            "[8/150]: Training Loss: 4.591101573063777, Training Accuracy: 2.486\n",
            "Validation Loss: 4.584865570068359, Validation Accuracy: 3.16\n",
            "[9/150]: Training Loss: 4.5825383479778585, Training Accuracy: 3.076\n",
            "Validation Loss: 4.575003465016683, Validation Accuracy: 3.84\n",
            "[10/150]: Training Loss: 4.571999696584848, Training Accuracy: 3.336\n",
            "Validation Loss: 4.562810103098552, Validation Accuracy: 3.58\n",
            "[11/150]: Training Loss: 4.559818267822266, Training Accuracy: 3.298\n",
            "Validation Loss: 4.548019091288249, Validation Accuracy: 3.45\n",
            "[12/150]: Training Loss: 4.544124970069299, Training Accuracy: 3.284\n",
            "Validation Loss: 4.530599912007649, Validation Accuracy: 3.53\n",
            "[13/150]: Training Loss: 4.526562433976394, Training Accuracy: 3.516\n",
            "Validation Loss: 4.510464509328206, Validation Accuracy: 3.87\n",
            "[14/150]: Training Loss: 4.505583102886494, Training Accuracy: 3.706\n",
            "Validation Loss: 4.487647533416748, Validation Accuracy: 3.96\n",
            "[15/150]: Training Loss: 4.4825364626370945, Training Accuracy: 3.97\n",
            "Validation Loss: 4.462220668792725, Validation Accuracy: 4.06\n",
            "[16/150]: Training Loss: 4.456124305725098, Training Accuracy: 4.04\n",
            "Validation Loss: 4.434208234151204, Validation Accuracy: 4.26\n",
            "[17/150]: Training Loss: 4.428849366995005, Training Accuracy: 4.238\n",
            "Validation Loss: 4.404198328653972, Validation Accuracy: 4.68\n",
            "[18/150]: Training Loss: 4.398712635040283, Training Accuracy: 4.382\n",
            "Validation Loss: 4.372480710347493, Validation Accuracy: 5.08\n",
            "[19/150]: Training Loss: 4.369481490208552, Training Accuracy: 4.73\n",
            "Validation Loss: 4.339550971984863, Validation Accuracy: 5.32\n",
            "[20/150]: Training Loss: 4.3372171842134914, Training Accuracy: 5.082\n",
            "Validation Loss: 4.306232770284017, Validation Accuracy: 5.64\n",
            "[21/150]: Training Loss: 4.306336256173941, Training Accuracy: 5.376\n",
            "Validation Loss: 4.272884527842204, Validation Accuracy: 5.85\n",
            "[22/150]: Training Loss: 4.27412722660945, Training Accuracy: 5.544\n",
            "Validation Loss: 4.240369955698649, Validation Accuracy: 6.19\n",
            "[23/150]: Training Loss: 4.245465902181772, Training Accuracy: 6.004\n",
            "Validation Loss: 4.209603627522786, Validation Accuracy: 6.6\n",
            "[24/150]: Training Loss: 4.2155220325176535, Training Accuracy: 6.35\n",
            "Validation Loss: 4.179777940114339, Validation Accuracy: 6.96\n",
            "[25/150]: Training Loss: 4.185919578258808, Training Accuracy: 6.624\n",
            "Validation Loss: 4.151111761728923, Validation Accuracy: 7.2\n",
            "[26/150]: Training Loss: 4.160709711221548, Training Accuracy: 6.94\n",
            "Validation Loss: 4.125725905100505, Validation Accuracy: 7.66\n",
            "[27/150]: Training Loss: 4.137477397918701, Training Accuracy: 7.344\n",
            "Validation Loss: 4.100640455881755, Validation Accuracy: 8.21\n",
            "[28/150]: Training Loss: 4.1160736450782185, Training Accuracy: 7.532\n",
            "Validation Loss: 4.079355557759603, Validation Accuracy: 8.25\n",
            "[29/150]: Training Loss: 4.098053125234751, Training Accuracy: 7.884\n",
            "Validation Loss: 4.058371702829997, Validation Accuracy: 8.55\n",
            "[30/150]: Training Loss: 4.086406047527607, Training Accuracy: 7.96\n",
            "Validation Loss: 4.0379958152771, Validation Accuracy: 9.04\n",
            "[31/150]: Training Loss: 4.063176705287053, Training Accuracy: 8.412\n",
            "Validation Loss: 4.020632902781169, Validation Accuracy: 9.28\n",
            "[32/150]: Training Loss: 4.054692195012019, Training Accuracy: 8.59\n",
            "Validation Loss: 4.0030783812205, Validation Accuracy: 9.63\n",
            "[33/150]: Training Loss: 4.032094441927397, Training Accuracy: 8.758\n",
            "Validation Loss: 3.987109978993734, Validation Accuracy: 9.71\n",
            "[34/150]: Training Loss: 4.022676577934852, Training Accuracy: 8.996\n",
            "Validation Loss: 3.9713465372721353, Validation Accuracy: 9.76\n",
            "[35/150]: Training Loss: 4.000812567197359, Training Accuracy: 9.384\n",
            "Validation Loss: 3.9538060824076333, Validation Accuracy: 10.19\n",
            "[36/150]: Training Loss: 3.9866452400500956, Training Accuracy: 9.608\n",
            "Validation Loss: 3.93888529141744, Validation Accuracy: 10.44\n",
            "[37/150]: Training Loss: 3.9776483315687914, Training Accuracy: 9.874\n",
            "Validation Loss: 3.922476847966512, Validation Accuracy: 10.56\n",
            "[38/150]: Training Loss: 3.9651166108938365, Training Accuracy: 9.862\n",
            "Validation Loss: 3.90744686126709, Validation Accuracy: 10.81\n",
            "[39/150]: Training Loss: 3.948905064509465, Training Accuracy: 10.218\n",
            "Validation Loss: 3.89294163386027, Validation Accuracy: 11.35\n",
            "[40/150]: Training Loss: 3.937441715827355, Training Accuracy: 10.372\n",
            "Validation Loss: 3.8783599535624185, Validation Accuracy: 11.22\n",
            "[41/150]: Training Loss: 3.9230183821458082, Training Accuracy: 10.612\n",
            "Validation Loss: 3.8631796836853027, Validation Accuracy: 11.66\n",
            "[42/150]: Training Loss: 3.912040582069984, Training Accuracy: 10.784\n",
            "Validation Loss: 3.8491604328155518, Validation Accuracy: 11.86\n",
            "[43/150]: Training Loss: 3.8964080076951246, Training Accuracy: 11.06\n",
            "Validation Loss: 3.8363358974456787, Validation Accuracy: 12.08\n",
            "[44/150]: Training Loss: 3.8831426180326023, Training Accuracy: 11.18\n",
            "Validation Loss: 3.825343132019043, Validation Accuracy: 12.27\n",
            "[45/150]: Training Loss: 3.8707955250373254, Training Accuracy: 11.322\n",
            "Validation Loss: 3.811360756556193, Validation Accuracy: 12.62\n",
            "[46/150]: Training Loss: 3.860361337661743, Training Accuracy: 11.54\n",
            "Validation Loss: 3.795260190963745, Validation Accuracy: 12.84\n",
            "[47/150]: Training Loss: 3.8526153014256406, Training Accuracy: 11.646\n",
            "Validation Loss: 3.784214735031128, Validation Accuracy: 13.13\n",
            "[48/150]: Training Loss: 3.8345638788663425, Training Accuracy: 11.768\n",
            "Validation Loss: 3.7713340123494468, Validation Accuracy: 13.34\n",
            "[49/150]: Training Loss: 3.8285101377047024, Training Accuracy: 12.096\n",
            "Validation Loss: 3.76099960009257, Validation Accuracy: 13.47\n",
            "[50/150]: Training Loss: 3.816150261805608, Training Accuracy: 12.372\n",
            "Validation Loss: 3.7498886585235596, Validation Accuracy: 13.5\n",
            "[51/150]: Training Loss: 3.8035758091853213, Training Accuracy: 12.228\n",
            "Validation Loss: 3.7394703229268393, Validation Accuracy: 13.84\n",
            "[52/150]: Training Loss: 3.7984210527860203, Training Accuracy: 12.59\n",
            "Validation Loss: 3.726769765218099, Validation Accuracy: 14.28\n",
            "[53/150]: Training Loss: 3.786878989293025, Training Accuracy: 12.82\n",
            "Validation Loss: 3.719003756841024, Validation Accuracy: 14.37\n",
            "[54/150]: Training Loss: 3.774993529686561, Training Accuracy: 12.926\n",
            "Validation Loss: 3.707072099049886, Validation Accuracy: 14.61\n",
            "[55/150]: Training Loss: 3.765552227313702, Training Accuracy: 13.176\n",
            "Validation Loss: 3.698981682459513, Validation Accuracy: 14.82\n",
            "[56/150]: Training Loss: 3.759232466037457, Training Accuracy: 13.316\n",
            "Validation Loss: 3.6923623085021973, Validation Accuracy: 14.6\n",
            "[57/150]: Training Loss: 3.746529505803035, Training Accuracy: 13.31\n",
            "Validation Loss: 3.6810197035471597, Validation Accuracy: 14.95\n",
            "[58/150]: Training Loss: 3.739341607460609, Training Accuracy: 13.734\n",
            "Validation Loss: 3.673978885014852, Validation Accuracy: 14.96\n",
            "[59/150]: Training Loss: 3.734126567840576, Training Accuracy: 13.616\n",
            "Validation Loss: 3.6645074685414634, Validation Accuracy: 15.33\n",
            "[60/150]: Training Loss: 3.724345353933481, Training Accuracy: 13.872\n",
            "Validation Loss: 3.654491583506266, Validation Accuracy: 15.44\n",
            "[61/150]: Training Loss: 3.717476936487051, Training Accuracy: 13.9\n",
            "Validation Loss: 3.645918528238932, Validation Accuracy: 15.77\n",
            "[62/150]: Training Loss: 3.7088359869443455, Training Accuracy: 14.286\n",
            "Validation Loss: 3.638837734858195, Validation Accuracy: 15.73\n",
            "[63/150]: Training Loss: 3.7012051068819485, Training Accuracy: 14.23\n",
            "Validation Loss: 3.629307826360067, Validation Accuracy: 16.17\n",
            "[64/150]: Training Loss: 3.6978440284729004, Training Accuracy: 14.368\n",
            "Validation Loss: 3.6212432384490967, Validation Accuracy: 16.28\n",
            "[65/150]: Training Loss: 3.685521070773785, Training Accuracy: 14.538\n",
            "Validation Loss: 3.6163320541381836, Validation Accuracy: 16.03\n",
            "[66/150]: Training Loss: 3.681316999288706, Training Accuracy: 14.432\n",
            "Validation Loss: 3.606062889099121, Validation Accuracy: 16.41\n",
            "[67/150]: Training Loss: 3.6727612752180834, Training Accuracy: 14.784\n",
            "Validation Loss: 3.5995655059814453, Validation Accuracy: 16.47\n",
            "[68/150]: Training Loss: 3.670240182142991, Training Accuracy: 14.814\n",
            "Validation Loss: 3.5927998224894204, Validation Accuracy: 16.65\n",
            "[69/150]: Training Loss: 3.655353234364436, Training Accuracy: 14.904\n",
            "Validation Loss: 3.5843842029571533, Validation Accuracy: 16.92\n",
            "[70/150]: Training Loss: 3.6477879744309645, Training Accuracy: 15.132\n",
            "Validation Loss: 3.5797649224599204, Validation Accuracy: 16.89\n",
            "[71/150]: Training Loss: 3.6427008555485654, Training Accuracy: 15.29\n",
            "Validation Loss: 3.5710333983103433, Validation Accuracy: 17.1\n",
            "[72/150]: Training Loss: 3.6349054116469164, Training Accuracy: 15.326\n",
            "Validation Loss: 3.564692815144857, Validation Accuracy: 17.25\n",
            "[73/150]: Training Loss: 3.631576318007249, Training Accuracy: 15.41\n",
            "Validation Loss: 3.557377735773722, Validation Accuracy: 17.21\n",
            "[74/150]: Training Loss: 3.625403587634747, Training Accuracy: 15.382\n",
            "Validation Loss: 3.5509705543518066, Validation Accuracy: 17.52\n",
            "[75/150]: Training Loss: 3.615908696101262, Training Accuracy: 15.832\n",
            "Validation Loss: 3.54543407758077, Validation Accuracy: 17.53\n",
            "[76/150]: Training Loss: 3.617959187580989, Training Accuracy: 15.9\n",
            "Validation Loss: 3.5409313837687173, Validation Accuracy: 17.63\n",
            "[77/150]: Training Loss: 3.61479036624615, Training Accuracy: 15.924\n",
            "Validation Loss: 3.5348545710245767, Validation Accuracy: 17.74\n",
            "[78/150]: Training Loss: 3.599782026731051, Training Accuracy: 15.856\n",
            "Validation Loss: 3.52974534034729, Validation Accuracy: 17.95\n",
            "[79/150]: Training Loss: 3.5990573993095984, Training Accuracy: 16.238\n",
            "Validation Loss: 3.5212018489837646, Validation Accuracy: 17.94\n",
            "[80/150]: Training Loss: 3.592515175159161, Training Accuracy: 16.154\n",
            "Validation Loss: 3.5165327390034995, Validation Accuracy: 18.21\n",
            "[81/150]: Training Loss: 3.58927504832928, Training Accuracy: 16.29\n",
            "Validation Loss: 3.5114110310872397, Validation Accuracy: 18.24\n",
            "[82/150]: Training Loss: 3.5821184745201697, Training Accuracy: 16.33\n",
            "Validation Loss: 3.5056587855021157, Validation Accuracy: 18.24\n",
            "[83/150]: Training Loss: 3.571472461407001, Training Accuracy: 16.528\n",
            "Validation Loss: 3.4996766249338784, Validation Accuracy: 18.35\n",
            "[84/150]: Training Loss: 3.5723203145540676, Training Accuracy: 16.702\n",
            "Validation Loss: 3.4941263993581138, Validation Accuracy: 18.52\n",
            "[85/150]: Training Loss: 3.5644964621617246, Training Accuracy: 16.56\n",
            "Validation Loss: 3.4899566968282065, Validation Accuracy: 18.69\n",
            "[86/150]: Training Loss: 3.5610409883352427, Training Accuracy: 16.728\n",
            "Validation Loss: 3.486067454020182, Validation Accuracy: 18.62\n",
            "[87/150]: Training Loss: 3.5533294677734375, Training Accuracy: 16.754\n",
            "Validation Loss: 3.478351672490438, Validation Accuracy: 18.82\n",
            "[88/150]: Training Loss: 3.5531911116379957, Training Accuracy: 16.91\n",
            "Validation Loss: 3.476896047592163, Validation Accuracy: 19.08\n",
            "[89/150]: Training Loss: 3.5495745952312765, Training Accuracy: 16.96\n",
            "Validation Loss: 3.4712963104248047, Validation Accuracy: 18.83\n",
            "[90/150]: Training Loss: 3.54185702250554, Training Accuracy: 16.884\n",
            "Validation Loss: 3.4682395458221436, Validation Accuracy: 19.06\n",
            "[91/150]: Training Loss: 3.5386377297914944, Training Accuracy: 17.084\n",
            "Validation Loss: 3.4618451595306396, Validation Accuracy: 19.09\n",
            "[92/150]: Training Loss: 3.5369215928591213, Training Accuracy: 17.188\n",
            "Validation Loss: 3.45944881439209, Validation Accuracy: 19.09\n",
            "[93/150]: Training Loss: 3.534542725636409, Training Accuracy: 17.522\n",
            "Validation Loss: 3.4543633460998535, Validation Accuracy: 19.27\n",
            "[94/150]: Training Loss: 3.527037657224215, Training Accuracy: 17.302\n",
            "Validation Loss: 3.4519523779551187, Validation Accuracy: 19.39\n",
            "[95/150]: Training Loss: 3.5207606095534105, Training Accuracy: 17.612\n",
            "Validation Loss: 3.4471476078033447, Validation Accuracy: 19.3\n",
            "[96/150]: Training Loss: 3.523169627556434, Training Accuracy: 17.366\n",
            "Validation Loss: 3.44489852587382, Validation Accuracy: 19.46\n",
            "[97/150]: Training Loss: 3.514947872895461, Training Accuracy: 17.576\n",
            "Validation Loss: 3.440441687901815, Validation Accuracy: 19.53\n",
            "[98/150]: Training Loss: 3.516437218739436, Training Accuracy: 17.636\n",
            "Validation Loss: 3.4375240008036294, Validation Accuracy: 19.7\n",
            "[99/150]: Training Loss: 3.515314450630775, Training Accuracy: 17.722\n",
            "Validation Loss: 3.4350737730662027, Validation Accuracy: 19.64\n",
            "[100/150]: Training Loss: 3.5090638674222507, Training Accuracy: 17.904\n",
            "Validation Loss: 3.430509169896444, Validation Accuracy: 19.79\n",
            "[101/150]: Training Loss: 3.504866893474872, Training Accuracy: 17.752\n",
            "Validation Loss: 3.4284051259358725, Validation Accuracy: 19.74\n",
            "[102/150]: Training Loss: 3.4917823534745436, Training Accuracy: 17.848\n",
            "Validation Loss: 3.425445636113485, Validation Accuracy: 19.83\n",
            "[103/150]: Training Loss: 3.5007056456345778, Training Accuracy: 18.006\n",
            "Validation Loss: 3.4235105514526367, Validation Accuracy: 19.77\n",
            "[104/150]: Training Loss: 3.488600785915668, Training Accuracy: 18.088\n",
            "Validation Loss: 3.419638713200887, Validation Accuracy: 19.93\n",
            "[105/150]: Training Loss: 3.4892408664409933, Training Accuracy: 17.904\n",
            "Validation Loss: 3.4154659112294516, Validation Accuracy: 20.02\n",
            "[106/150]: Training Loss: 3.483979060099675, Training Accuracy: 18.014\n",
            "Validation Loss: 3.413444995880127, Validation Accuracy: 20.12\n",
            "[107/150]: Training Loss: 3.4866304030785193, Training Accuracy: 18.084\n",
            "Validation Loss: 3.4109641710917153, Validation Accuracy: 20.04\n",
            "[108/150]: Training Loss: 3.486282256933359, Training Accuracy: 18.144\n",
            "Validation Loss: 3.4091246922810874, Validation Accuracy: 20.08\n",
            "[109/150]: Training Loss: 3.481999470637395, Training Accuracy: 17.968\n",
            "Validation Loss: 3.4066150983174643, Validation Accuracy: 20.18\n",
            "[110/150]: Training Loss: 3.4790745148291955, Training Accuracy: 18.248\n",
            "Validation Loss: 3.4039930502573648, Validation Accuracy: 20.22\n",
            "[111/150]: Training Loss: 3.476212684924786, Training Accuracy: 18.252\n",
            "Validation Loss: 3.402389129002889, Validation Accuracy: 20.29\n",
            "[112/150]: Training Loss: 3.474406755887545, Training Accuracy: 18.33\n",
            "Validation Loss: 3.400217294692993, Validation Accuracy: 20.3\n",
            "[113/150]: Training Loss: 3.4745553456819973, Training Accuracy: 18.042\n",
            "Validation Loss: 3.397853215535482, Validation Accuracy: 20.22\n",
            "[114/150]: Training Loss: 3.474104697887714, Training Accuracy: 18.246\n",
            "Validation Loss: 3.3972776730855307, Validation Accuracy: 20.24\n",
            "[115/150]: Training Loss: 3.4714555190159726, Training Accuracy: 18.372\n",
            "Validation Loss: 3.394416252772013, Validation Accuracy: 20.37\n",
            "[116/150]: Training Loss: 3.4606630985553446, Training Accuracy: 18.416\n",
            "Validation Loss: 3.3930967648824057, Validation Accuracy: 20.55\n",
            "[117/150]: Training Loss: 3.4698875684004564, Training Accuracy: 18.464\n",
            "Validation Loss: 3.3917219638824463, Validation Accuracy: 20.42\n",
            "[118/150]: Training Loss: 3.46042590874892, Training Accuracy: 18.584\n",
            "Validation Loss: 3.3900171915690103, Validation Accuracy: 20.36\n",
            "[119/150]: Training Loss: 3.459270037137545, Training Accuracy: 18.616\n",
            "Validation Loss: 3.38869571685791, Validation Accuracy: 20.44\n",
            "[120/150]: Training Loss: 3.4700822096604567, Training Accuracy: 18.456\n",
            "Validation Loss: 3.3867367108662925, Validation Accuracy: 20.59\n",
            "[121/150]: Training Loss: 3.4599712445185733, Training Accuracy: 18.566\n",
            "Validation Loss: 3.385423421859741, Validation Accuracy: 20.63\n",
            "[122/150]: Training Loss: 3.461164199388944, Training Accuracy: 18.588\n",
            "Validation Loss: 3.3854024410247803, Validation Accuracy: 20.65\n",
            "[123/150]: Training Loss: 3.4520862102508545, Training Accuracy: 18.478\n",
            "Validation Loss: 3.38422425587972, Validation Accuracy: 20.61\n",
            "[124/150]: Training Loss: 3.4461356676541843, Training Accuracy: 18.644\n",
            "Validation Loss: 3.3832034269968667, Validation Accuracy: 20.59\n",
            "[125/150]: Training Loss: 3.4624485419346738, Training Accuracy: 18.532\n",
            "Validation Loss: 3.3818772633870444, Validation Accuracy: 20.62\n",
            "[126/150]: Training Loss: 3.4651179497058573, Training Accuracy: 18.372\n",
            "Validation Loss: 3.381105343500773, Validation Accuracy: 20.62\n",
            "[127/150]: Training Loss: 3.4483938950758715, Training Accuracy: 18.59\n",
            "Validation Loss: 3.37996514638265, Validation Accuracy: 20.77\n",
            "[128/150]: Training Loss: 3.443562305890597, Training Accuracy: 18.652\n",
            "Validation Loss: 3.379426956176758, Validation Accuracy: 20.72\n",
            "[129/150]: Training Loss: 3.4467421678396373, Training Accuracy: 18.606\n",
            "Validation Loss: 3.3791234493255615, Validation Accuracy: 20.52\n",
            "[130/150]: Training Loss: 3.4486422538757324, Training Accuracy: 18.642\n",
            "Validation Loss: 3.3781088987986245, Validation Accuracy: 20.66\n",
            "[131/150]: Training Loss: 3.4504753993107724, Training Accuracy: 18.628\n",
            "Validation Loss: 3.3772608439127603, Validation Accuracy: 20.72\n",
            "[132/150]: Training Loss: 3.456728091606727, Training Accuracy: 18.674\n",
            "Validation Loss: 3.3773043950398765, Validation Accuracy: 20.71\n",
            "[133/150]: Training Loss: 3.44931200834421, Training Accuracy: 18.624\n",
            "Validation Loss: 3.3772084712982178, Validation Accuracy: 20.7\n",
            "[134/150]: Training Loss: 3.449618541277372, Training Accuracy: 18.704\n",
            "Validation Loss: 3.376373608907064, Validation Accuracy: 20.79\n",
            "[135/150]: Training Loss: 3.4503632875589223, Training Accuracy: 18.66\n",
            "Validation Loss: 3.3759363492329917, Validation Accuracy: 20.83\n",
            "[136/150]: Training Loss: 3.449677210587722, Training Accuracy: 18.596\n",
            "Validation Loss: 3.375778595606486, Validation Accuracy: 20.83\n",
            "[137/150]: Training Loss: 3.449537864098182, Training Accuracy: 18.88\n",
            "Validation Loss: 3.375666777292887, Validation Accuracy: 20.86\n",
            "[138/150]: Training Loss: 3.450434923171997, Training Accuracy: 18.604\n",
            "Validation Loss: 3.3753153483072915, Validation Accuracy: 20.78\n",
            "[139/150]: Training Loss: 3.4475225118490367, Training Accuracy: 18.636\n",
            "Validation Loss: 3.37508487701416, Validation Accuracy: 20.75\n",
            "[140/150]: Training Loss: 3.4522607143108663, Training Accuracy: 18.838\n",
            "Validation Loss: 3.3748606046040854, Validation Accuracy: 20.76\n",
            "[141/150]: Training Loss: 3.4529799681443434, Training Accuracy: 18.474\n",
            "Validation Loss: 3.3746429284413657, Validation Accuracy: 20.81\n",
            "[142/150]: Training Loss: 3.4617789341853213, Training Accuracy: 18.554\n",
            "Validation Loss: 3.374579668045044, Validation Accuracy: 20.83\n",
            "[143/150]: Training Loss: 3.4485555245326114, Training Accuracy: 18.682\n",
            "Validation Loss: 3.374601682027181, Validation Accuracy: 20.8\n",
            "[144/150]: Training Loss: 3.444190428807185, Training Accuracy: 18.658\n",
            "Validation Loss: 3.3745707670847573, Validation Accuracy: 20.8\n",
            "[145/150]: Training Loss: 3.4463418263655443, Training Accuracy: 18.744\n",
            "Validation Loss: 3.3744784196217856, Validation Accuracy: 20.77\n",
            "[146/150]: Training Loss: 3.4481177146618185, Training Accuracy: 18.63\n",
            "Validation Loss: 3.3744141260782876, Validation Accuracy: 20.8\n",
            "[147/150]: Training Loss: 3.45231503706712, Training Accuracy: 18.596\n",
            "Validation Loss: 3.3743797143300376, Validation Accuracy: 20.78\n",
            "[148/150]: Training Loss: 3.4454290316655087, Training Accuracy: 18.72\n",
            "Validation Loss: 3.3743703365325928, Validation Accuracy: 20.79\n",
            "[149/150]: Training Loss: 3.4413316616645226, Training Accuracy: 18.828\n",
            "Validation Loss: 3.3743655681610107, Validation Accuracy: 20.78\n",
            "[150/150]: Training Loss: 3.446342101463905, Training Accuracy: 18.644\n",
            "Validation Loss: 3.374370892842611, Validation Accuracy: 20.78\n",
            "**********************************************************************\n",
            "Test Loss: 3.374370892842611, Test Accuracy: 20.78\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▃▁█</td></tr><tr><td>Test Loss</td><td>▃█▁</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>Train Loss</td><td>████▇▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>20.78</td></tr><tr><td>Test Loss</td><td>3.37437</td></tr><tr><td>Train Accuracy</td><td>18.644</td></tr><tr><td>Train Loss</td><td>3.44634</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=8192 learning_rate=0.192 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/2i4ynggq' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/2i4ynggq</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_044434-2i4ynggq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 16384\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_051107-0sspuo81</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/0sspuo81' target=\"_blank\">batch_size=16384 learning_rate=0.384 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/0sspuo81' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/0sspuo81</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.606681016775278, Training Accuracy: 0.952\n",
            "Validation Loss: 4.606822967529297, Validation Accuracy: 1.1\n",
            "[2/150]: Training Loss: 4.606707572937012, Training Accuracy: 0.922\n",
            "Validation Loss: 4.606650352478027, Validation Accuracy: 1.1\n",
            "[3/150]: Training Loss: 4.606582201444185, Training Accuracy: 0.856\n",
            "Validation Loss: 4.606306711832683, Validation Accuracy: 1.12\n",
            "[4/150]: Training Loss: 4.606221565833459, Training Accuracy: 0.884\n",
            "Validation Loss: 4.605796655019124, Validation Accuracy: 1.13\n",
            "[5/150]: Training Loss: 4.605512875777024, Training Accuracy: 0.93\n",
            "Validation Loss: 4.605127016703288, Validation Accuracy: 1.15\n",
            "[6/150]: Training Loss: 4.605027785668006, Training Accuracy: 1.008\n",
            "Validation Loss: 4.604293505350749, Validation Accuracy: 1.14\n",
            "[7/150]: Training Loss: 4.604158364809477, Training Accuracy: 1.026\n",
            "Validation Loss: 4.603291670481364, Validation Accuracy: 1.15\n",
            "[8/150]: Training Loss: 4.602965281559871, Training Accuracy: 1.028\n",
            "Validation Loss: 4.602102597554524, Validation Accuracy: 1.17\n",
            "[9/150]: Training Loss: 4.601937660804162, Training Accuracy: 1.122\n",
            "Validation Loss: 4.600688298543294, Validation Accuracy: 1.27\n",
            "[10/150]: Training Loss: 4.6005659103393555, Training Accuracy: 1.248\n",
            "Validation Loss: 4.598995208740234, Validation Accuracy: 1.39\n",
            "[11/150]: Training Loss: 4.598669895758996, Training Accuracy: 1.406\n",
            "Validation Loss: 4.596963723500569, Validation Accuracy: 1.69\n",
            "[12/150]: Training Loss: 4.596671361189622, Training Accuracy: 1.644\n",
            "Validation Loss: 4.594494024912517, Validation Accuracy: 1.97\n",
            "[13/150]: Training Loss: 4.5939992024348335, Training Accuracy: 1.998\n",
            "Validation Loss: 4.591485818227132, Validation Accuracy: 2.25\n",
            "[14/150]: Training Loss: 4.590727952810434, Training Accuracy: 2.41\n",
            "Validation Loss: 4.587808132171631, Validation Accuracy: 2.67\n",
            "[15/150]: Training Loss: 4.587152261000413, Training Accuracy: 2.794\n",
            "Validation Loss: 4.583325386047363, Validation Accuracy: 2.81\n",
            "[16/150]: Training Loss: 4.582096283252422, Training Accuracy: 2.964\n",
            "Validation Loss: 4.577853520711263, Validation Accuracy: 2.97\n",
            "[17/150]: Training Loss: 4.577282648820144, Training Accuracy: 3.076\n",
            "Validation Loss: 4.571693579355876, Validation Accuracy: 3.11\n",
            "[18/150]: Training Loss: 4.570924282073975, Training Accuracy: 3.242\n",
            "Validation Loss: 4.564822355906169, Validation Accuracy: 3.27\n",
            "[19/150]: Training Loss: 4.563632451570951, Training Accuracy: 3.34\n",
            "Validation Loss: 4.5572309494018555, Validation Accuracy: 3.35\n",
            "[20/150]: Training Loss: 4.555748756115253, Training Accuracy: 3.342\n",
            "Validation Loss: 4.5488667488098145, Validation Accuracy: 3.36\n",
            "[21/150]: Training Loss: 4.5476059180039625, Training Accuracy: 3.322\n",
            "Validation Loss: 4.539740244547526, Validation Accuracy: 3.35\n",
            "[22/150]: Training Loss: 4.538436192732591, Training Accuracy: 3.37\n",
            "Validation Loss: 4.529847621917725, Validation Accuracy: 3.39\n",
            "[23/150]: Training Loss: 4.528222891000601, Training Accuracy: 3.476\n",
            "Validation Loss: 4.519181410471599, Validation Accuracy: 3.46\n",
            "[24/150]: Training Loss: 4.518055218916673, Training Accuracy: 3.4\n",
            "Validation Loss: 4.5077924728393555, Validation Accuracy: 3.55\n",
            "[25/150]: Training Loss: 4.507573017707238, Training Accuracy: 3.426\n",
            "Validation Loss: 4.4956404368082685, Validation Accuracy: 3.58\n",
            "[26/150]: Training Loss: 4.493582468766433, Training Accuracy: 3.424\n",
            "Validation Loss: 4.482788721720378, Validation Accuracy: 3.57\n",
            "[27/150]: Training Loss: 4.482110867133508, Training Accuracy: 3.468\n",
            "Validation Loss: 4.469227155049642, Validation Accuracy: 3.49\n",
            "[28/150]: Training Loss: 4.468239380763127, Training Accuracy: 3.518\n",
            "Validation Loss: 4.454997380574544, Validation Accuracy: 3.61\n",
            "[29/150]: Training Loss: 4.454548689035269, Training Accuracy: 3.53\n",
            "Validation Loss: 4.440215110778809, Validation Accuracy: 3.63\n",
            "[30/150]: Training Loss: 4.440642576951247, Training Accuracy: 3.57\n",
            "Validation Loss: 4.424936135609944, Validation Accuracy: 3.75\n",
            "[31/150]: Training Loss: 4.425176070286677, Training Accuracy: 3.614\n",
            "Validation Loss: 4.409231026967366, Validation Accuracy: 3.93\n",
            "[32/150]: Training Loss: 4.408273770258977, Training Accuracy: 3.716\n",
            "Validation Loss: 4.393052260080974, Validation Accuracy: 3.92\n",
            "[33/150]: Training Loss: 4.392657866844764, Training Accuracy: 3.734\n",
            "Validation Loss: 4.376562436421712, Validation Accuracy: 3.98\n",
            "[34/150]: Training Loss: 4.376563952519343, Training Accuracy: 3.866\n",
            "Validation Loss: 4.3598737716674805, Validation Accuracy: 4.08\n",
            "[35/150]: Training Loss: 4.3609442344078655, Training Accuracy: 3.982\n",
            "Validation Loss: 4.343031565348308, Validation Accuracy: 4.25\n",
            "[36/150]: Training Loss: 4.346464450542744, Training Accuracy: 4.026\n",
            "Validation Loss: 4.326065540313721, Validation Accuracy: 4.41\n",
            "[37/150]: Training Loss: 4.329667274768536, Training Accuracy: 4.286\n",
            "Validation Loss: 4.308992862701416, Validation Accuracy: 4.58\n",
            "[38/150]: Training Loss: 4.311468894665058, Training Accuracy: 4.522\n",
            "Validation Loss: 4.291934331258138, Validation Accuracy: 4.63\n",
            "[39/150]: Training Loss: 4.297071236830491, Training Accuracy: 4.664\n",
            "Validation Loss: 4.274834314982097, Validation Accuracy: 4.93\n",
            "[40/150]: Training Loss: 4.27970842214731, Training Accuracy: 4.878\n",
            "Validation Loss: 4.257642904917399, Validation Accuracy: 5.28\n",
            "[41/150]: Training Loss: 4.261493022625263, Training Accuracy: 5.156\n",
            "Validation Loss: 4.240853945414226, Validation Accuracy: 5.64\n",
            "[42/150]: Training Loss: 4.25109129685622, Training Accuracy: 5.306\n",
            "Validation Loss: 4.224237600962321, Validation Accuracy: 5.99\n",
            "[43/150]: Training Loss: 4.231884186084454, Training Accuracy: 5.772\n",
            "Validation Loss: 4.207854111989339, Validation Accuracy: 6.25\n",
            "[44/150]: Training Loss: 4.219960396106426, Training Accuracy: 6.004\n",
            "Validation Loss: 4.1916913986206055, Validation Accuracy: 6.58\n",
            "[45/150]: Training Loss: 4.202443342942458, Training Accuracy: 6.26\n",
            "Validation Loss: 4.17600154876709, Validation Accuracy: 6.89\n",
            "[46/150]: Training Loss: 4.190515664907602, Training Accuracy: 6.364\n",
            "Validation Loss: 4.161019643147786, Validation Accuracy: 7.02\n",
            "[47/150]: Training Loss: 4.178160373981182, Training Accuracy: 6.606\n",
            "Validation Loss: 4.146643161773682, Validation Accuracy: 7.07\n",
            "[48/150]: Training Loss: 4.163040858048659, Training Accuracy: 6.732\n",
            "Validation Loss: 4.132482687632243, Validation Accuracy: 7.23\n",
            "[49/150]: Training Loss: 4.155646397517278, Training Accuracy: 6.992\n",
            "Validation Loss: 4.119060039520264, Validation Accuracy: 7.49\n",
            "[50/150]: Training Loss: 4.138869872459998, Training Accuracy: 7.174\n",
            "Validation Loss: 4.106416384379069, Validation Accuracy: 7.62\n",
            "[51/150]: Training Loss: 4.130591539236216, Training Accuracy: 7.38\n",
            "Validation Loss: 4.094014008839925, Validation Accuracy: 7.87\n",
            "[52/150]: Training Loss: 4.117375740638146, Training Accuracy: 7.462\n",
            "Validation Loss: 4.0823713938395185, Validation Accuracy: 8.04\n",
            "[53/150]: Training Loss: 4.105965027442346, Training Accuracy: 7.604\n",
            "Validation Loss: 4.07077423731486, Validation Accuracy: 8.32\n",
            "[54/150]: Training Loss: 4.097295357630803, Training Accuracy: 7.75\n",
            "Validation Loss: 4.059831301371257, Validation Accuracy: 8.44\n",
            "[55/150]: Training Loss: 4.088907241821289, Training Accuracy: 7.992\n",
            "Validation Loss: 4.04987112681071, Validation Accuracy: 8.46\n",
            "[56/150]: Training Loss: 4.084290797893818, Training Accuracy: 8.136\n",
            "Validation Loss: 4.039998849232991, Validation Accuracy: 8.65\n",
            "[57/150]: Training Loss: 4.0709899022029, Training Accuracy: 8.344\n",
            "Validation Loss: 4.030025164286296, Validation Accuracy: 8.9\n",
            "[58/150]: Training Loss: 4.060490058018611, Training Accuracy: 8.506\n",
            "Validation Loss: 4.020981788635254, Validation Accuracy: 9.02\n",
            "[59/150]: Training Loss: 4.054420214432937, Training Accuracy: 8.518\n",
            "Validation Loss: 4.012075821558635, Validation Accuracy: 9.21\n",
            "[60/150]: Training Loss: 4.045688995948205, Training Accuracy: 8.586\n",
            "Validation Loss: 4.003910303115845, Validation Accuracy: 9.58\n",
            "[61/150]: Training Loss: 4.043613470517672, Training Accuracy: 8.84\n",
            "Validation Loss: 3.9957558314005532, Validation Accuracy: 9.69\n",
            "[62/150]: Training Loss: 4.030606673314021, Training Accuracy: 8.906\n",
            "Validation Loss: 3.9876187642415366, Validation Accuracy: 9.77\n",
            "[63/150]: Training Loss: 4.024190425872803, Training Accuracy: 9.034\n",
            "Validation Loss: 3.979825814565023, Validation Accuracy: 9.9\n",
            "[64/150]: Training Loss: 4.017007460960975, Training Accuracy: 9.224\n",
            "Validation Loss: 3.9720873832702637, Validation Accuracy: 10.1\n",
            "[65/150]: Training Loss: 4.008704735682561, Training Accuracy: 9.386\n",
            "Validation Loss: 3.964705228805542, Validation Accuracy: 10.18\n",
            "[66/150]: Training Loss: 4.002646042750432, Training Accuracy: 9.524\n",
            "Validation Loss: 3.9574963251749673, Validation Accuracy: 10.36\n",
            "[67/150]: Training Loss: 3.9969605115743785, Training Accuracy: 9.504\n",
            "Validation Loss: 3.9497090180714927, Validation Accuracy: 10.69\n",
            "[68/150]: Training Loss: 3.9883175996633677, Training Accuracy: 9.578\n",
            "Validation Loss: 3.943034569422404, Validation Accuracy: 10.55\n",
            "[69/150]: Training Loss: 3.9852346090170054, Training Accuracy: 9.846\n",
            "Validation Loss: 3.9364415804545083, Validation Accuracy: 10.77\n",
            "[70/150]: Training Loss: 3.975684587772076, Training Accuracy: 9.944\n",
            "Validation Loss: 3.929186741511027, Validation Accuracy: 11.05\n",
            "[71/150]: Training Loss: 3.971948128480178, Training Accuracy: 10.058\n",
            "Validation Loss: 3.9228974978129068, Validation Accuracy: 11.05\n",
            "[72/150]: Training Loss: 3.971257521555974, Training Accuracy: 9.956\n",
            "Validation Loss: 3.916752497355143, Validation Accuracy: 11.14\n",
            "[73/150]: Training Loss: 3.958688259124756, Training Accuracy: 10.334\n",
            "Validation Loss: 3.9100093046824136, Validation Accuracy: 11.16\n",
            "[74/150]: Training Loss: 3.951256807033832, Training Accuracy: 10.262\n",
            "Validation Loss: 3.903977711995443, Validation Accuracy: 11.43\n",
            "[75/150]: Training Loss: 3.952374531672551, Training Accuracy: 10.376\n",
            "Validation Loss: 3.8979154427846274, Validation Accuracy: 11.6\n",
            "[76/150]: Training Loss: 3.9451453318962684, Training Accuracy: 10.552\n",
            "Validation Loss: 3.8917051951090493, Validation Accuracy: 11.8\n",
            "[77/150]: Training Loss: 3.9374708212338962, Training Accuracy: 10.576\n",
            "Validation Loss: 3.8862176736195884, Validation Accuracy: 11.91\n",
            "[78/150]: Training Loss: 3.9311011204352746, Training Accuracy: 10.524\n",
            "Validation Loss: 3.8805812199910483, Validation Accuracy: 11.98\n",
            "[79/150]: Training Loss: 3.9276427305661716, Training Accuracy: 10.858\n",
            "Validation Loss: 3.874942938486735, Validation Accuracy: 12.24\n",
            "[80/150]: Training Loss: 3.9206688770881066, Training Accuracy: 10.978\n",
            "Validation Loss: 3.8699731826782227, Validation Accuracy: 12.27\n",
            "[81/150]: Training Loss: 3.915351335818951, Training Accuracy: 10.92\n",
            "Validation Loss: 3.8641488552093506, Validation Accuracy: 12.18\n",
            "[82/150]: Training Loss: 3.916545317723201, Training Accuracy: 11.03\n",
            "Validation Loss: 3.8589042027791343, Validation Accuracy: 12.32\n",
            "[83/150]: Training Loss: 3.9109069384061375, Training Accuracy: 11.152\n",
            "Validation Loss: 3.8542784055074057, Validation Accuracy: 12.36\n",
            "[84/150]: Training Loss: 3.90489517725431, Training Accuracy: 11.19\n",
            "Validation Loss: 3.849609613418579, Validation Accuracy: 12.51\n",
            "[85/150]: Training Loss: 3.899632453918457, Training Accuracy: 11.386\n",
            "Validation Loss: 3.844412406285604, Validation Accuracy: 12.58\n",
            "[86/150]: Training Loss: 3.891819275342501, Training Accuracy: 11.476\n",
            "Validation Loss: 3.83998433748881, Validation Accuracy: 12.58\n",
            "[87/150]: Training Loss: 3.8915958404541016, Training Accuracy: 11.318\n",
            "Validation Loss: 3.8357551097869873, Validation Accuracy: 12.7\n",
            "[88/150]: Training Loss: 3.8887247305649977, Training Accuracy: 11.456\n",
            "Validation Loss: 3.8317208290100098, Validation Accuracy: 12.82\n",
            "[89/150]: Training Loss: 3.8861560087937574, Training Accuracy: 11.436\n",
            "Validation Loss: 3.8272467454274497, Validation Accuracy: 12.95\n",
            "[90/150]: Training Loss: 3.882909444662241, Training Accuracy: 11.526\n",
            "Validation Loss: 3.8237024943033853, Validation Accuracy: 12.93\n",
            "[91/150]: Training Loss: 3.8792625757364125, Training Accuracy: 11.726\n",
            "Validation Loss: 3.8198357423146567, Validation Accuracy: 12.98\n",
            "[92/150]: Training Loss: 3.8738321707798886, Training Accuracy: 11.788\n",
            "Validation Loss: 3.816159645716349, Validation Accuracy: 13.03\n",
            "[93/150]: Training Loss: 3.8671187254098744, Training Accuracy: 11.858\n",
            "Validation Loss: 3.81238055229187, Validation Accuracy: 13.17\n",
            "[94/150]: Training Loss: 3.863940477371216, Training Accuracy: 11.886\n",
            "Validation Loss: 3.80879545211792, Validation Accuracy: 13.17\n",
            "[95/150]: Training Loss: 3.861557960510254, Training Accuracy: 11.898\n",
            "Validation Loss: 3.8057108720143638, Validation Accuracy: 13.01\n",
            "[96/150]: Training Loss: 3.8610231692974386, Training Accuracy: 11.998\n",
            "Validation Loss: 3.80320143699646, Validation Accuracy: 13.06\n",
            "[97/150]: Training Loss: 3.8616612874544582, Training Accuracy: 11.81\n",
            "Validation Loss: 3.8002119859059653, Validation Accuracy: 13.2\n",
            "[98/150]: Training Loss: 3.8542607930990367, Training Accuracy: 11.916\n",
            "Validation Loss: 3.7972320715586343, Validation Accuracy: 13.3\n",
            "[99/150]: Training Loss: 3.8545856842627892, Training Accuracy: 12.306\n",
            "Validation Loss: 3.7939964135487876, Validation Accuracy: 13.34\n",
            "[100/150]: Training Loss: 3.8537971056424656, Training Accuracy: 12.136\n",
            "Validation Loss: 3.791252930959066, Validation Accuracy: 13.37\n",
            "[101/150]: Training Loss: 3.845060165111835, Training Accuracy: 12.294\n",
            "Validation Loss: 3.788733164469401, Validation Accuracy: 13.44\n",
            "[102/150]: Training Loss: 3.8459858160752516, Training Accuracy: 12.318\n",
            "Validation Loss: 3.7863372961680093, Validation Accuracy: 13.55\n",
            "[103/150]: Training Loss: 3.845681208830613, Training Accuracy: 12.304\n",
            "Validation Loss: 3.7838097413380942, Validation Accuracy: 13.5\n",
            "[104/150]: Training Loss: 3.8407657329852762, Training Accuracy: 12.284\n",
            "Validation Loss: 3.7814003626505532, Validation Accuracy: 13.47\n",
            "[105/150]: Training Loss: 3.843270485217755, Training Accuracy: 12.246\n",
            "Validation Loss: 3.7793458302815757, Validation Accuracy: 13.54\n",
            "[106/150]: Training Loss: 3.8365319875570445, Training Accuracy: 12.352\n",
            "Validation Loss: 3.7774136066436768, Validation Accuracy: 13.68\n",
            "[107/150]: Training Loss: 3.8353553185096154, Training Accuracy: 12.4\n",
            "Validation Loss: 3.775336424509684, Validation Accuracy: 13.7\n",
            "[108/150]: Training Loss: 3.8264750884129453, Training Accuracy: 12.538\n",
            "Validation Loss: 3.773082176844279, Validation Accuracy: 13.78\n",
            "[109/150]: Training Loss: 3.828341557429387, Training Accuracy: 12.494\n",
            "Validation Loss: 3.7709336280822754, Validation Accuracy: 13.82\n",
            "[110/150]: Training Loss: 3.8278093338012695, Training Accuracy: 12.49\n",
            "Validation Loss: 3.769374450047811, Validation Accuracy: 13.73\n",
            "[111/150]: Training Loss: 3.8284831047058105, Training Accuracy: 12.502\n",
            "Validation Loss: 3.76780104637146, Validation Accuracy: 13.69\n",
            "[112/150]: Training Loss: 3.826379500902616, Training Accuracy: 12.588\n",
            "Validation Loss: 3.7659263610839844, Validation Accuracy: 13.75\n",
            "[113/150]: Training Loss: 3.824379774240347, Training Accuracy: 12.478\n",
            "Validation Loss: 3.7643912633260093, Validation Accuracy: 13.82\n",
            "[114/150]: Training Loss: 3.8296838907095103, Training Accuracy: 12.514\n",
            "Validation Loss: 3.763111432393392, Validation Accuracy: 13.91\n",
            "[115/150]: Training Loss: 3.8198436223543606, Training Accuracy: 12.7\n",
            "Validation Loss: 3.761921485265096, Validation Accuracy: 13.98\n",
            "[116/150]: Training Loss: 3.825025430092445, Training Accuracy: 12.724\n",
            "Validation Loss: 3.7602592309316, Validation Accuracy: 14.05\n",
            "[117/150]: Training Loss: 3.8199995847848744, Training Accuracy: 12.67\n",
            "Validation Loss: 3.7588558991750083, Validation Accuracy: 13.97\n",
            "[118/150]: Training Loss: 3.8171204236837535, Training Accuracy: 12.626\n",
            "Validation Loss: 3.757905960083008, Validation Accuracy: 13.89\n",
            "[119/150]: Training Loss: 3.8168038588303785, Training Accuracy: 12.574\n",
            "Validation Loss: 3.7570317586263022, Validation Accuracy: 13.95\n",
            "[120/150]: Training Loss: 3.8181789104755106, Training Accuracy: 12.786\n",
            "Validation Loss: 3.7561088403066, Validation Accuracy: 14.05\n",
            "[121/150]: Training Loss: 3.814372722919171, Training Accuracy: 12.866\n",
            "Validation Loss: 3.7551366488138833, Validation Accuracy: 14.12\n",
            "[122/150]: Training Loss: 3.8122683671804576, Training Accuracy: 12.62\n",
            "Validation Loss: 3.754225571950277, Validation Accuracy: 14.09\n",
            "[123/150]: Training Loss: 3.8116220327524037, Training Accuracy: 12.65\n",
            "Validation Loss: 3.7533694903055825, Validation Accuracy: 14.1\n",
            "[124/150]: Training Loss: 3.8165716391343336, Training Accuracy: 12.532\n",
            "Validation Loss: 3.7525957425435386, Validation Accuracy: 14.11\n",
            "[125/150]: Training Loss: 3.8071923439319315, Training Accuracy: 12.776\n",
            "Validation Loss: 3.7518486976623535, Validation Accuracy: 14.13\n",
            "[126/150]: Training Loss: 3.804902773637038, Training Accuracy: 12.66\n",
            "Validation Loss: 3.751216491063436, Validation Accuracy: 14.18\n",
            "[127/150]: Training Loss: 3.812517001078679, Training Accuracy: 12.756\n",
            "Validation Loss: 3.7504872481028237, Validation Accuracy: 14.06\n",
            "[128/150]: Training Loss: 3.808407214971689, Training Accuracy: 12.782\n",
            "Validation Loss: 3.749931971232096, Validation Accuracy: 14.08\n",
            "[129/150]: Training Loss: 3.809405345183152, Training Accuracy: 12.982\n",
            "Validation Loss: 3.7494645913441977, Validation Accuracy: 14.12\n",
            "[130/150]: Training Loss: 3.8093962852771464, Training Accuracy: 12.79\n",
            "Validation Loss: 3.749009291330973, Validation Accuracy: 14.19\n",
            "[131/150]: Training Loss: 3.8160287050100474, Training Accuracy: 12.826\n",
            "Validation Loss: 3.748585859934489, Validation Accuracy: 14.15\n",
            "[132/150]: Training Loss: 3.805815549997183, Training Accuracy: 12.954\n",
            "Validation Loss: 3.7482031981150308, Validation Accuracy: 14.18\n",
            "[133/150]: Training Loss: 3.8074949154487023, Training Accuracy: 13.01\n",
            "Validation Loss: 3.7478596369425454, Validation Accuracy: 14.27\n",
            "[134/150]: Training Loss: 3.8079946224506083, Training Accuracy: 13.006\n",
            "Validation Loss: 3.7474911212921143, Validation Accuracy: 14.29\n",
            "[135/150]: Training Loss: 3.803782499753512, Training Accuracy: 13.028\n",
            "Validation Loss: 3.7471730709075928, Validation Accuracy: 14.26\n",
            "[136/150]: Training Loss: 3.8073516809023342, Training Accuracy: 12.738\n",
            "Validation Loss: 3.746917645136515, Validation Accuracy: 14.27\n",
            "[137/150]: Training Loss: 3.808643799561721, Training Accuracy: 13.072\n",
            "Validation Loss: 3.746670722961426, Validation Accuracy: 14.26\n",
            "[138/150]: Training Loss: 3.8111466811253476, Training Accuracy: 12.808\n",
            "Validation Loss: 3.7464906374613443, Validation Accuracy: 14.26\n",
            "[139/150]: Training Loss: 3.800331610899705, Training Accuracy: 12.784\n",
            "Validation Loss: 3.7463111877441406, Validation Accuracy: 14.25\n",
            "[140/150]: Training Loss: 3.8085801968207726, Training Accuracy: 13.02\n",
            "Validation Loss: 3.7461959520975747, Validation Accuracy: 14.24\n",
            "[141/150]: Training Loss: 3.8026637114011326, Training Accuracy: 13.04\n",
            "Validation Loss: 3.746067841847738, Validation Accuracy: 14.24\n",
            "[142/150]: Training Loss: 3.8049483666053185, Training Accuracy: 13.014\n",
            "Validation Loss: 3.745962699254354, Validation Accuracy: 14.26\n",
            "[143/150]: Training Loss: 3.802163234123817, Training Accuracy: 12.892\n",
            "Validation Loss: 3.745898723602295, Validation Accuracy: 14.26\n",
            "[144/150]: Training Loss: 3.80464847271259, Training Accuracy: 13.098\n",
            "Validation Loss: 3.7458243370056152, Validation Accuracy: 14.23\n",
            "[145/150]: Training Loss: 3.8023996353149414, Training Accuracy: 12.958\n",
            "Validation Loss: 3.7457852363586426, Validation Accuracy: 14.24\n",
            "[146/150]: Training Loss: 3.808238928134625, Training Accuracy: 12.754\n",
            "Validation Loss: 3.74574605623881, Validation Accuracy: 14.25\n",
            "[147/150]: Training Loss: 3.8094704701350284, Training Accuracy: 12.834\n",
            "Validation Loss: 3.7457223733266196, Validation Accuracy: 14.24\n",
            "[148/150]: Training Loss: 3.810224698140071, Training Accuracy: 12.922\n",
            "Validation Loss: 3.7457141081492105, Validation Accuracy: 14.24\n",
            "[149/150]: Training Loss: 3.808670997619629, Training Accuracy: 12.826\n",
            "Validation Loss: 3.745707352956136, Validation Accuracy: 14.22\n",
            "[150/150]: Training Loss: 3.8055135103372426, Training Accuracy: 12.94\n",
            "Validation Loss: 3.745703140894572, Validation Accuracy: 14.2\n",
            "**********************************************************************\n",
            "Test Loss: 3.745703140894572, Test Accuracy: 14.2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▂▁█</td></tr><tr><td>Test Loss</td><td>▁█▄</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▁▂▂▂▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇█████████████</td></tr><tr><td>Train Loss</td><td>██████▇▇▆▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>14.2</td></tr><tr><td>Test Loss</td><td>3.7457</td></tr><tr><td>Train Accuracy</td><td>12.94</td></tr><tr><td>Train Loss</td><td>3.80551</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=16384 learning_rate=0.384 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/0sspuo81' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/0sspuo81</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_051107-0sspuo81/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 32768\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_053723-99jlxrtf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/99jlxrtf' target=\"_blank\">batch_size=32768 learning_rate=0.768 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/99jlxrtf' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/99jlxrtf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.606651562910813, Training Accuracy: 1.002\n",
            "Validation Loss: 4.6062649091084795, Validation Accuracy: 1.0\n",
            "[2/150]: Training Loss: 4.6064779208256645, Training Accuracy: 1.0\n",
            "Validation Loss: 4.606228033701579, Validation Accuracy: 1.0\n",
            "[3/150]: Training Loss: 4.606321224799523, Training Accuracy: 1.0\n",
            "Validation Loss: 4.606154441833496, Validation Accuracy: 1.0\n",
            "[4/150]: Training Loss: 4.606344589820275, Training Accuracy: 1.0\n",
            "Validation Loss: 4.606043815612793, Validation Accuracy: 1.0\n",
            "[5/150]: Training Loss: 4.606256888462947, Training Accuracy: 1.006\n",
            "Validation Loss: 4.605896631876628, Validation Accuracy: 1.0\n",
            "[6/150]: Training Loss: 4.606011354006254, Training Accuracy: 1.002\n",
            "Validation Loss: 4.605713526407878, Validation Accuracy: 1.0\n",
            "[7/150]: Training Loss: 4.605957177969126, Training Accuracy: 1.0\n",
            "Validation Loss: 4.605494976043701, Validation Accuracy: 1.0\n",
            "[8/150]: Training Loss: 4.60562016413762, Training Accuracy: 1.004\n",
            "Validation Loss: 4.6052422523498535, Validation Accuracy: 1.0\n",
            "[9/150]: Training Loss: 4.605379141294039, Training Accuracy: 1.0\n",
            "Validation Loss: 4.6049544016520185, Validation Accuracy: 1.0\n",
            "[10/150]: Training Loss: 4.605238987849309, Training Accuracy: 0.998\n",
            "Validation Loss: 4.6046319007873535, Validation Accuracy: 1.0\n",
            "[11/150]: Training Loss: 4.604823736044077, Training Accuracy: 0.992\n",
            "Validation Loss: 4.6042758623758955, Validation Accuracy: 1.0\n",
            "[12/150]: Training Loss: 4.6045523790212775, Training Accuracy: 1.002\n",
            "Validation Loss: 4.603885491689046, Validation Accuracy: 1.01\n",
            "[13/150]: Training Loss: 4.6041412720313435, Training Accuracy: 1.006\n",
            "Validation Loss: 4.603460311889648, Validation Accuracy: 1.01\n",
            "[14/150]: Training Loss: 4.603790209843562, Training Accuracy: 1.002\n",
            "Validation Loss: 4.60299825668335, Validation Accuracy: 1.01\n",
            "[15/150]: Training Loss: 4.603167937352107, Training Accuracy: 1.0\n",
            "Validation Loss: 4.60249678293864, Validation Accuracy: 1.01\n",
            "[16/150]: Training Loss: 4.602945987994854, Training Accuracy: 1.002\n",
            "Validation Loss: 4.601947625478108, Validation Accuracy: 1.01\n",
            "[17/150]: Training Loss: 4.602274197798509, Training Accuracy: 0.998\n",
            "Validation Loss: 4.601348876953125, Validation Accuracy: 1.01\n",
            "[18/150]: Training Loss: 4.601670815394475, Training Accuracy: 0.998\n",
            "Validation Loss: 4.600697199503581, Validation Accuracy: 1.02\n",
            "[19/150]: Training Loss: 4.600979328155518, Training Accuracy: 1.014\n",
            "Validation Loss: 4.599982261657715, Validation Accuracy: 1.02\n",
            "[20/150]: Training Loss: 4.600179158724272, Training Accuracy: 1.006\n",
            "Validation Loss: 4.59920072555542, Validation Accuracy: 1.03\n",
            "[21/150]: Training Loss: 4.599789472726675, Training Accuracy: 1.014\n",
            "Validation Loss: 4.5983381271362305, Validation Accuracy: 1.04\n",
            "[22/150]: Training Loss: 4.5985127962552586, Training Accuracy: 1.016\n",
            "Validation Loss: 4.5973836580912275, Validation Accuracy: 1.04\n",
            "[23/150]: Training Loss: 4.597778356992281, Training Accuracy: 1.012\n",
            "Validation Loss: 4.596320788065593, Validation Accuracy: 1.04\n",
            "[24/150]: Training Loss: 4.596712589263916, Training Accuracy: 1.01\n",
            "Validation Loss: 4.595138390858968, Validation Accuracy: 1.04\n",
            "[25/150]: Training Loss: 4.595335960388184, Training Accuracy: 1.016\n",
            "Validation Loss: 4.593827406565349, Validation Accuracy: 1.04\n",
            "[26/150]: Training Loss: 4.5942942545964165, Training Accuracy: 1.026\n",
            "Validation Loss: 4.592372576395671, Validation Accuracy: 1.04\n",
            "[27/150]: Training Loss: 4.5928042851961575, Training Accuracy: 1.04\n",
            "Validation Loss: 4.590754985809326, Validation Accuracy: 1.05\n",
            "[28/150]: Training Loss: 4.591340908637414, Training Accuracy: 1.034\n",
            "Validation Loss: 4.588959534962972, Validation Accuracy: 1.08\n",
            "[29/150]: Training Loss: 4.589395119593694, Training Accuracy: 1.06\n",
            "Validation Loss: 4.586968739827474, Validation Accuracy: 1.1\n",
            "[30/150]: Training Loss: 4.587504900418795, Training Accuracy: 1.074\n",
            "Validation Loss: 4.5847554206848145, Validation Accuracy: 1.13\n",
            "[31/150]: Training Loss: 4.585566887488732, Training Accuracy: 1.084\n",
            "Validation Loss: 4.582296371459961, Validation Accuracy: 1.16\n",
            "[32/150]: Training Loss: 4.58308194233821, Training Accuracy: 1.166\n",
            "Validation Loss: 4.579681237538655, Validation Accuracy: 1.29\n",
            "[33/150]: Training Loss: 4.580507901998667, Training Accuracy: 1.31\n",
            "Validation Loss: 4.576923529307048, Validation Accuracy: 1.44\n",
            "[34/150]: Training Loss: 4.577524295220008, Training Accuracy: 1.46\n",
            "Validation Loss: 4.574008464813232, Validation Accuracy: 1.63\n",
            "[35/150]: Training Loss: 4.574791504786565, Training Accuracy: 1.666\n",
            "Validation Loss: 4.570941766103108, Validation Accuracy: 1.84\n",
            "[36/150]: Training Loss: 4.571814023531401, Training Accuracy: 1.898\n",
            "Validation Loss: 4.5677235921223955, Validation Accuracy: 1.95\n",
            "[37/150]: Training Loss: 4.56871957045335, Training Accuracy: 2.106\n",
            "Validation Loss: 4.564344724019368, Validation Accuracy: 2.12\n",
            "[38/150]: Training Loss: 4.564909861637996, Training Accuracy: 2.244\n",
            "Validation Loss: 4.560825665791829, Validation Accuracy: 2.26\n",
            "[39/150]: Training Loss: 4.562269687652588, Training Accuracy: 2.412\n",
            "Validation Loss: 4.5571645100911455, Validation Accuracy: 2.47\n",
            "[40/150]: Training Loss: 4.558965609623836, Training Accuracy: 2.522\n",
            "Validation Loss: 4.553352991739909, Validation Accuracy: 2.64\n",
            "[41/150]: Training Loss: 4.554804985339825, Training Accuracy: 2.674\n",
            "Validation Loss: 4.549396355946858, Validation Accuracy: 2.8\n",
            "[42/150]: Training Loss: 4.550996633676382, Training Accuracy: 2.828\n",
            "Validation Loss: 4.545290470123291, Validation Accuracy: 2.91\n",
            "[43/150]: Training Loss: 4.5466168110187235, Training Accuracy: 2.882\n",
            "Validation Loss: 4.541029135386149, Validation Accuracy: 3.07\n",
            "[44/150]: Training Loss: 4.542607417473426, Training Accuracy: 3.024\n",
            "Validation Loss: 4.536633332570394, Validation Accuracy: 3.24\n",
            "[45/150]: Training Loss: 4.538647064795861, Training Accuracy: 3.174\n",
            "Validation Loss: 4.532093207041423, Validation Accuracy: 3.27\n",
            "[46/150]: Training Loss: 4.534129582918608, Training Accuracy: 3.226\n",
            "Validation Loss: 4.527428309122722, Validation Accuracy: 3.35\n",
            "[47/150]: Training Loss: 4.530724268693191, Training Accuracy: 3.344\n",
            "Validation Loss: 4.522626082102458, Validation Accuracy: 3.42\n",
            "[48/150]: Training Loss: 4.524971155019907, Training Accuracy: 3.418\n",
            "Validation Loss: 4.517709096272786, Validation Accuracy: 3.53\n",
            "[49/150]: Training Loss: 4.521018285017747, Training Accuracy: 3.492\n",
            "Validation Loss: 4.5126776695251465, Validation Accuracy: 3.61\n",
            "[50/150]: Training Loss: 4.516248776362493, Training Accuracy: 3.598\n",
            "Validation Loss: 4.507528463999431, Validation Accuracy: 3.69\n",
            "[51/150]: Training Loss: 4.51058134665856, Training Accuracy: 3.602\n",
            "Validation Loss: 4.50228230158488, Validation Accuracy: 3.74\n",
            "[52/150]: Training Loss: 4.506296891432542, Training Accuracy: 3.646\n",
            "Validation Loss: 4.496960957845052, Validation Accuracy: 3.76\n",
            "[53/150]: Training Loss: 4.500542383927566, Training Accuracy: 3.72\n",
            "Validation Loss: 4.491568565368652, Validation Accuracy: 3.77\n",
            "[54/150]: Training Loss: 4.4958164141728325, Training Accuracy: 3.792\n",
            "Validation Loss: 4.4861117998758955, Validation Accuracy: 3.79\n",
            "[55/150]: Training Loss: 4.4898162988516, Training Accuracy: 3.77\n",
            "Validation Loss: 4.480595429738362, Validation Accuracy: 3.81\n",
            "[56/150]: Training Loss: 4.48429430448092, Training Accuracy: 3.86\n",
            "Validation Loss: 4.475037892659505, Validation Accuracy: 3.9\n",
            "[57/150]: Training Loss: 4.480384533221905, Training Accuracy: 3.908\n",
            "Validation Loss: 4.4694546063741045, Validation Accuracy: 3.92\n",
            "[58/150]: Training Loss: 4.474462802593525, Training Accuracy: 3.888\n",
            "Validation Loss: 4.463854630788167, Validation Accuracy: 4.05\n",
            "[59/150]: Training Loss: 4.468657420231746, Training Accuracy: 3.994\n",
            "Validation Loss: 4.458210150400798, Validation Accuracy: 4.21\n",
            "[60/150]: Training Loss: 4.4632906180161696, Training Accuracy: 4.054\n",
            "Validation Loss: 4.452545960744222, Validation Accuracy: 4.31\n",
            "[61/150]: Training Loss: 4.457904448876014, Training Accuracy: 4.174\n",
            "Validation Loss: 4.446877797444661, Validation Accuracy: 4.34\n",
            "[62/150]: Training Loss: 4.45219505750216, Training Accuracy: 4.154\n",
            "Validation Loss: 4.441228866577148, Validation Accuracy: 4.39\n",
            "[63/150]: Training Loss: 4.448141024662898, Training Accuracy: 4.228\n",
            "Validation Loss: 4.4355998039245605, Validation Accuracy: 4.51\n",
            "[64/150]: Training Loss: 4.440924974588247, Training Accuracy: 4.254\n",
            "Validation Loss: 4.430009365081787, Validation Accuracy: 4.58\n",
            "[65/150]: Training Loss: 4.436653247246375, Training Accuracy: 4.264\n",
            "Validation Loss: 4.424450397491455, Validation Accuracy: 4.56\n",
            "[66/150]: Training Loss: 4.432394211108868, Training Accuracy: 4.304\n",
            "Validation Loss: 4.418941179911296, Validation Accuracy: 4.6\n",
            "[67/150]: Training Loss: 4.426660941197322, Training Accuracy: 4.248\n",
            "Validation Loss: 4.4134782155354815, Validation Accuracy: 4.69\n",
            "[68/150]: Training Loss: 4.423159122467041, Training Accuracy: 4.292\n",
            "Validation Loss: 4.408063252766927, Validation Accuracy: 4.71\n",
            "[69/150]: Training Loss: 4.415358873514029, Training Accuracy: 4.38\n",
            "Validation Loss: 4.4027144114176435, Validation Accuracy: 4.76\n",
            "[70/150]: Training Loss: 4.4111648339491625, Training Accuracy: 4.352\n",
            "Validation Loss: 4.397424379984538, Validation Accuracy: 4.8\n",
            "[71/150]: Training Loss: 4.404525976914626, Training Accuracy: 4.32\n",
            "Validation Loss: 4.392204602559407, Validation Accuracy: 4.8\n",
            "[72/150]: Training Loss: 4.402150594271147, Training Accuracy: 4.372\n",
            "Validation Loss: 4.3870619138081866, Validation Accuracy: 4.87\n",
            "[73/150]: Training Loss: 4.396322653843806, Training Accuracy: 4.44\n",
            "Validation Loss: 4.381992657979329, Validation Accuracy: 4.89\n",
            "[74/150]: Training Loss: 4.389365893143874, Training Accuracy: 4.388\n",
            "Validation Loss: 4.377000013987224, Validation Accuracy: 4.93\n",
            "[75/150]: Training Loss: 4.385019522446853, Training Accuracy: 4.56\n",
            "Validation Loss: 4.372077624003093, Validation Accuracy: 4.92\n",
            "[76/150]: Training Loss: 4.38299454175509, Training Accuracy: 4.474\n",
            "Validation Loss: 4.367240905761719, Validation Accuracy: 4.89\n",
            "[77/150]: Training Loss: 4.375784213726337, Training Accuracy: 4.58\n",
            "Validation Loss: 4.362499078114827, Validation Accuracy: 4.87\n",
            "[78/150]: Training Loss: 4.372459705059345, Training Accuracy: 4.58\n",
            "Validation Loss: 4.357850233713786, Validation Accuracy: 4.88\n",
            "[79/150]: Training Loss: 4.368399766775278, Training Accuracy: 4.494\n",
            "Validation Loss: 4.3532694180806475, Validation Accuracy: 4.87\n",
            "[80/150]: Training Loss: 4.363157015580398, Training Accuracy: 4.656\n",
            "Validation Loss: 4.348786989847819, Validation Accuracy: 4.92\n",
            "[81/150]: Training Loss: 4.360122277186467, Training Accuracy: 4.664\n",
            "Validation Loss: 4.344403266906738, Validation Accuracy: 4.94\n",
            "[82/150]: Training Loss: 4.353952591235821, Training Accuracy: 4.588\n",
            "Validation Loss: 4.340079625447591, Validation Accuracy: 4.99\n",
            "[83/150]: Training Loss: 4.351973606989934, Training Accuracy: 4.668\n",
            "Validation Loss: 4.335822423299153, Validation Accuracy: 5.03\n",
            "[84/150]: Training Loss: 4.347680421975943, Training Accuracy: 4.772\n",
            "Validation Loss: 4.331654230753581, Validation Accuracy: 5.01\n",
            "[85/150]: Training Loss: 4.344626720135029, Training Accuracy: 4.73\n",
            "Validation Loss: 4.327584743499756, Validation Accuracy: 5.01\n",
            "[86/150]: Training Loss: 4.340798157912034, Training Accuracy: 4.81\n",
            "Validation Loss: 4.323618253072103, Validation Accuracy: 5.05\n",
            "[87/150]: Training Loss: 4.33451535151555, Training Accuracy: 4.846\n",
            "Validation Loss: 4.319734573364258, Validation Accuracy: 5.03\n",
            "[88/150]: Training Loss: 4.335264719449556, Training Accuracy: 4.872\n",
            "Validation Loss: 4.315944194793701, Validation Accuracy: 5.07\n",
            "[89/150]: Training Loss: 4.326850634354812, Training Accuracy: 4.928\n",
            "Validation Loss: 4.312259356180827, Validation Accuracy: 5.16\n",
            "[90/150]: Training Loss: 4.325683997227595, Training Accuracy: 4.968\n",
            "Validation Loss: 4.308659394582112, Validation Accuracy: 5.22\n",
            "[91/150]: Training Loss: 4.320460429558387, Training Accuracy: 5.014\n",
            "Validation Loss: 4.3051651318868, Validation Accuracy: 5.26\n",
            "[92/150]: Training Loss: 4.319362420302171, Training Accuracy: 5.026\n",
            "Validation Loss: 4.301756381988525, Validation Accuracy: 5.29\n",
            "[93/150]: Training Loss: 4.314967118776762, Training Accuracy: 5.114\n",
            "Validation Loss: 4.298442999521892, Validation Accuracy: 5.37\n",
            "[94/150]: Training Loss: 4.312812475057749, Training Accuracy: 5.194\n",
            "Validation Loss: 4.295233090718587, Validation Accuracy: 5.44\n",
            "[95/150]: Training Loss: 4.307969496800349, Training Accuracy: 5.212\n",
            "Validation Loss: 4.292148113250732, Validation Accuracy: 5.49\n",
            "[96/150]: Training Loss: 4.304759869208703, Training Accuracy: 5.198\n",
            "Validation Loss: 4.2891364097595215, Validation Accuracy: 5.49\n",
            "[97/150]: Training Loss: 4.303560513716477, Training Accuracy: 5.25\n",
            "Validation Loss: 4.286212921142578, Validation Accuracy: 5.56\n",
            "[98/150]: Training Loss: 4.299245760991023, Training Accuracy: 5.218\n",
            "Validation Loss: 4.283364772796631, Validation Accuracy: 5.6\n",
            "[99/150]: Training Loss: 4.29710762317364, Training Accuracy: 5.426\n",
            "Validation Loss: 4.2806094487508135, Validation Accuracy: 5.61\n",
            "[100/150]: Training Loss: 4.293303159567026, Training Accuracy: 5.38\n",
            "Validation Loss: 4.277939637502034, Validation Accuracy: 5.67\n",
            "[101/150]: Training Loss: 4.291012140420767, Training Accuracy: 5.472\n",
            "Validation Loss: 4.2753651936848955, Validation Accuracy: 5.72\n",
            "[102/150]: Training Loss: 4.290883467747615, Training Accuracy: 5.438\n",
            "Validation Loss: 4.272902647654216, Validation Accuracy: 5.72\n",
            "[103/150]: Training Loss: 4.289033926450289, Training Accuracy: 5.448\n",
            "Validation Loss: 4.2705206871032715, Validation Accuracy: 5.73\n",
            "[104/150]: Training Loss: 4.286309095529409, Training Accuracy: 5.464\n",
            "Validation Loss: 4.268236955006917, Validation Accuracy: 5.77\n",
            "[105/150]: Training Loss: 4.283603448134202, Training Accuracy: 5.542\n",
            "Validation Loss: 4.266055901845296, Validation Accuracy: 5.79\n",
            "[106/150]: Training Loss: 4.281673284677359, Training Accuracy: 5.566\n",
            "Validation Loss: 4.263958930969238, Validation Accuracy: 5.78\n",
            "[107/150]: Training Loss: 4.2781767478356, Training Accuracy: 5.594\n",
            "Validation Loss: 4.261946201324463, Validation Accuracy: 5.8\n",
            "[108/150]: Training Loss: 4.275523039010855, Training Accuracy: 5.586\n",
            "Validation Loss: 4.2600124677022295, Validation Accuracy: 5.78\n",
            "[109/150]: Training Loss: 4.274957400101882, Training Accuracy: 5.594\n",
            "Validation Loss: 4.258159160614014, Validation Accuracy: 5.79\n",
            "[110/150]: Training Loss: 4.2723074692946215, Training Accuracy: 5.732\n",
            "Validation Loss: 4.2563802401224775, Validation Accuracy: 5.82\n",
            "[111/150]: Training Loss: 4.270333436819223, Training Accuracy: 5.656\n",
            "Validation Loss: 4.254659652709961, Validation Accuracy: 5.83\n",
            "[112/150]: Training Loss: 4.269529599409837, Training Accuracy: 5.63\n",
            "Validation Loss: 4.253021240234375, Validation Accuracy: 5.84\n",
            "[113/150]: Training Loss: 4.26841218654926, Training Accuracy: 5.734\n",
            "Validation Loss: 4.2514543533325195, Validation Accuracy: 5.88\n",
            "[114/150]: Training Loss: 4.264081184680645, Training Accuracy: 5.674\n",
            "Validation Loss: 4.249970277150472, Validation Accuracy: 5.89\n",
            "[115/150]: Training Loss: 4.264317292433518, Training Accuracy: 5.816\n",
            "Validation Loss: 4.248543898264567, Validation Accuracy: 5.9\n",
            "[116/150]: Training Loss: 4.265165035541241, Training Accuracy: 5.812\n",
            "Validation Loss: 4.24720033009847, Validation Accuracy: 5.93\n",
            "[117/150]: Training Loss: 4.264072968409612, Training Accuracy: 5.836\n",
            "Validation Loss: 4.245920976003011, Validation Accuracy: 5.97\n",
            "[118/150]: Training Loss: 4.263286517216609, Training Accuracy: 5.73\n",
            "Validation Loss: 4.244728883107503, Validation Accuracy: 5.98\n",
            "[119/150]: Training Loss: 4.258808466104361, Training Accuracy: 5.826\n",
            "Validation Loss: 4.243605295817058, Validation Accuracy: 6.01\n",
            "[120/150]: Training Loss: 4.259444530193623, Training Accuracy: 5.79\n",
            "Validation Loss: 4.242557366689046, Validation Accuracy: 6.03\n",
            "[121/150]: Training Loss: 4.259942898383508, Training Accuracy: 5.87\n",
            "Validation Loss: 4.241562366485596, Validation Accuracy: 6.04\n",
            "[122/150]: Training Loss: 4.256737085489126, Training Accuracy: 5.956\n",
            "Validation Loss: 4.240633487701416, Validation Accuracy: 6.08\n",
            "[123/150]: Training Loss: 4.25673587505634, Training Accuracy: 5.888\n",
            "Validation Loss: 4.23976484934489, Validation Accuracy: 6.08\n",
            "[124/150]: Training Loss: 4.256068853231577, Training Accuracy: 5.852\n",
            "Validation Loss: 4.23895804087321, Validation Accuracy: 6.09\n",
            "[125/150]: Training Loss: 4.252622494330773, Training Accuracy: 5.926\n",
            "Validation Loss: 4.238210360209147, Validation Accuracy: 6.12\n",
            "[126/150]: Training Loss: 4.253502772404597, Training Accuracy: 5.974\n",
            "Validation Loss: 4.237521489461263, Validation Accuracy: 6.15\n",
            "[127/150]: Training Loss: 4.255204897660476, Training Accuracy: 5.94\n",
            "Validation Loss: 4.236883481343587, Validation Accuracy: 6.16\n",
            "[128/150]: Training Loss: 4.254409569960374, Training Accuracy: 5.962\n",
            "Validation Loss: 4.236288070678711, Validation Accuracy: 6.16\n",
            "[129/150]: Training Loss: 4.2498477055476265, Training Accuracy: 5.95\n",
            "Validation Loss: 4.235741297403972, Validation Accuracy: 6.16\n",
            "[130/150]: Training Loss: 4.249825147482065, Training Accuracy: 5.97\n",
            "Validation Loss: 4.235249042510986, Validation Accuracy: 6.17\n",
            "[131/150]: Training Loss: 4.253135497753437, Training Accuracy: 5.892\n",
            "Validation Loss: 4.234799226125081, Validation Accuracy: 6.19\n",
            "[132/150]: Training Loss: 4.2529898423414965, Training Accuracy: 5.85\n",
            "Validation Loss: 4.234389940897624, Validation Accuracy: 6.2\n",
            "[133/150]: Training Loss: 4.250357114351713, Training Accuracy: 5.972\n",
            "Validation Loss: 4.234029134114583, Validation Accuracy: 6.19\n",
            "[134/150]: Training Loss: 4.250753989586463, Training Accuracy: 5.952\n",
            "Validation Loss: 4.233706633249919, Validation Accuracy: 6.22\n",
            "[135/150]: Training Loss: 4.249126434326172, Training Accuracy: 5.988\n",
            "Validation Loss: 4.23341433207194, Validation Accuracy: 6.23\n",
            "[136/150]: Training Loss: 4.252225215618427, Training Accuracy: 5.98\n",
            "Validation Loss: 4.2331617673238116, Validation Accuracy: 6.2\n",
            "[137/150]: Training Loss: 4.2500433921813965, Training Accuracy: 5.95\n",
            "Validation Loss: 4.232938925425212, Validation Accuracy: 6.21\n",
            "[138/150]: Training Loss: 4.24816370010376, Training Accuracy: 5.874\n",
            "Validation Loss: 4.232744057973226, Validation Accuracy: 6.21\n",
            "[139/150]: Training Loss: 4.250017312856821, Training Accuracy: 5.968\n",
            "Validation Loss: 4.232572714487712, Validation Accuracy: 6.21\n",
            "[140/150]: Training Loss: 4.248001502110408, Training Accuracy: 5.962\n",
            "Validation Loss: 4.232438087463379, Validation Accuracy: 6.21\n",
            "[141/150]: Training Loss: 4.248642994807317, Training Accuracy: 5.936\n",
            "Validation Loss: 4.232326825459798, Validation Accuracy: 6.21\n",
            "[142/150]: Training Loss: 4.247634997734656, Training Accuracy: 6.008\n",
            "Validation Loss: 4.232233047485352, Validation Accuracy: 6.21\n",
            "[143/150]: Training Loss: 4.247423832233135, Training Accuracy: 5.942\n",
            "Validation Loss: 4.232162952423096, Validation Accuracy: 6.21\n",
            "[144/150]: Training Loss: 4.247767558464637, Training Accuracy: 5.964\n",
            "Validation Loss: 4.232110182444255, Validation Accuracy: 6.21\n",
            "[145/150]: Training Loss: 4.250305909376878, Training Accuracy: 5.908\n",
            "Validation Loss: 4.2320709228515625, Validation Accuracy: 6.21\n",
            "[146/150]: Training Loss: 4.250130616701567, Training Accuracy: 6.024\n",
            "Validation Loss: 4.232042153676351, Validation Accuracy: 6.21\n",
            "[147/150]: Training Loss: 4.247591238755446, Training Accuracy: 6.032\n",
            "Validation Loss: 4.232023398081462, Validation Accuracy: 6.21\n",
            "[148/150]: Training Loss: 4.248689578129695, Training Accuracy: 6.046\n",
            "Validation Loss: 4.232014338175456, Validation Accuracy: 6.21\n",
            "[149/150]: Training Loss: 4.2455423428462105, Training Accuracy: 5.968\n",
            "Validation Loss: 4.232010682423909, Validation Accuracy: 6.21\n",
            "[150/150]: Training Loss: 4.251231266902043, Training Accuracy: 5.994\n",
            "Validation Loss: 4.23201052347819, Validation Accuracy: 6.21\n",
            "**********************************************************************\n",
            "Test Loss: 4.23201052347819, Test Accuracy: 6.21\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▁▅</td></tr><tr><td>Test Loss</td><td>▁█▅</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██████████</td></tr><tr><td>Train Loss</td><td>█████████▇▇▇▇▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>6.21</td></tr><tr><td>Test Loss</td><td>4.23201</td></tr><tr><td>Train Accuracy</td><td>5.994</td></tr><tr><td>Train Loss</td><td>4.25123</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=32768 learning_rate=0.768 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/99jlxrtf' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/99jlxrtf</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_053723-99jlxrtf/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# with warmup\n",
        "num_epochs = 150\n",
        "wd = 1e-04\n",
        "batch_sizes = [512, 1024, 2048, 4096, 8192, 16384 , 32768]\n",
        "lr = 4.8/(2**5 *1e02) # approximately 15e-04\n",
        "learning_rates = [lr * batch_size / 64.0 for batch_size in batch_sizes] # linear scale-up of learning rate\n",
        "warmup_ratio = [1/320, 1/160, 1/80, 1/40, 1/20, 1/10, 1/5]\n",
        "\n",
        "for i, batch_size in enumerate(batch_sizes):\n",
        "  print('='*50)\n",
        "  print(f'Batch size: {batch_size}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {\n",
        "    'batch_size': batch_size,\n",
        "    'learning_rate': learning_rates[i],\n",
        "    'weight_decay' : wd\n",
        "  }\n",
        "\n",
        "  if batch_size <= 4096:\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= batch_size)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LAMB(model.parameters(), learning_rate=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LAMB_Large_Batches',\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True,\n",
        "        warmup_ratio=warmup_ratio[i]\n",
        "    )\n",
        "  else:\n",
        "    accumulation_steps = batch_size // 4096\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= 4096)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LAMB(model.parameters(), learning_rate=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LAMB_Large_Batches',\n",
        "        accumulation_steps=accumulation_steps,\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True,\n",
        "        warmup_ratio=warmup_ratio[i]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# without warmup\n",
        "num_epochs = 150\n",
        "wd = 1e-04\n",
        "batch_sizes = [512, 1024, 2048, 4096, 8192, 16384 , 32768]\n",
        "lr = 4.8/(2**5 *1e02) # approximately 15e-04\n",
        "learning_rates = [lr * batch_size / 64.0 for batch_size in batch_sizes] # linear scale-up of learning rate\n",
        "# warmup_ratio = [1/320, 1/160, 1/80, 1/40, 1/20, 1/10, 1/5]\n",
        "\n",
        "for i, batch_size in enumerate(batch_sizes):\n",
        "  print('='*50)\n",
        "  print(f'Batch size: {batch_size}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {\n",
        "    'batch_size': batch_size,\n",
        "    'learning_rate': learning_rates[i],\n",
        "    'weight_decay' : wd\n",
        "  }\n",
        "\n",
        "  if batch_size <= 4096:\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= batch_size)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LAMB(model.parameters(), learning_rate=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LAMB_Large_Batches_without_warmup',\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True\n",
        "    )\n",
        "  else:\n",
        "    accumulation_steps = batch_size // 4096\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= 4096)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LAMB(model.parameters(), learning_rate=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LAMB_Large_Batches',\n",
        "        accumulation_steps=accumulation_steps,\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Distributed Approaches**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hRd9SL7iRiKl"
      },
      "outputs": [],
      "source": [
        "# Initialize a model and save its initial parameters\n",
        "initial_model = LeNet5()\n",
        "initial_state_dict = initial_model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GXZaFXruZI_"
      },
      "source": [
        "### Local SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I8hvzRbQW32s"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class LocalSGDOptimizer(Optimizer):\n",
        "    def __init__(self, global_model, lr=0.01):\n",
        "        self.global_model = global_model\n",
        "        self.lr = lr\n",
        "        params = list(global_model.parameters())\n",
        "        super(LocalSGDOptimizer, self).__init__(params, {'lr': lr})\n",
        "\n",
        "    def step(self, local_models):\n",
        "        # Get global model parameters\n",
        "        global_params = list(self.global_model.parameters())\n",
        "        \n",
        "        # Initialize deltas for each parameter, same size as global parameters\n",
        "        deltas = [torch.zeros_like(param) for param in global_params]\n",
        "        \n",
        "        # Sum up differences between global model and local models\n",
        "        for local_model in local_models:\n",
        "            local_params = list(local_model.parameters())\n",
        "            for i, param in enumerate(local_params):\n",
        "                deltas[i] += (global_params[i] - param)\n",
        "        \n",
        "        # Average the delta over the number of local models\n",
        "        num_models = len(local_models)\n",
        "        for i, delta in enumerate(deltas):\n",
        "            deltas[i] /= self.lr\n",
        "            deltas[i] /= num_models\n",
        "        \n",
        "        # Update global model parameters\n",
        "        with torch.no_grad():\n",
        "            for i, param in enumerate(global_params):\n",
        "                param.copy_(param - self.lr * deltas[i])\n",
        "        \n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def local_SGD(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, is_wandb=False):\n",
        "      \n",
        "  total_start_time = time.time()\n",
        "\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "  \n",
        "  global_optimizer = LocalSGDOptimizer(global_model, lr)\n",
        "  \n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "  \n",
        "  checkpoint = load_checkpoint('local_sgd', 64, {'k': k, 'j': j})\n",
        "  if checkpoint is not None:\n",
        "      global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      global_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      \n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn)\n",
        "      print(f'Global Update: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      \n",
        "      return None\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for loca_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    global_optimizer.step(local_models)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    for local_optimizer in local_optimizers:\n",
        "        local_optimizer.param_groups[0]['lr'] = global_optimizer.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "    for local_model in local_models:\n",
        "      local_model.load_state_dict(global_model.state_dict())\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Global Update {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "  \n",
        "  total_end_time = time.time()\n",
        "  \n",
        "  # Save checkpoint\n",
        "  save_checkpoint({\n",
        "              'epoch': num_epochs,\n",
        "              'model_state_dict': global_model.state_dict(),\n",
        "              'optimizer_state_dict': global_optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss': loss_fn\n",
        "              }, 149, 64, 'local_sgd', {'k': k, 'j': j})\n",
        " \n",
        "\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for local_SGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:4\n",
            "==================================================\n",
            "Global Update: Test Loss: 1.891312885, Test Accuracy: 56.100\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:8\n",
            "==================================================\n",
            "Worker 1, [01/08]: Training Loss: 4.310751363, Training Accuracy: 4.664\n",
            "Worker 1, [02/08]: Training Loss: 3.835420891, Training Accuracy: 10.256\n",
            "Worker 1, [03/08]: Training Loss: 3.567942598, Training Accuracy: 14.744\n",
            "Worker 1, [04/08]: Training Loss: 3.360835533, Training Accuracy: 18.288\n",
            "Worker 1, [05/08]: Training Loss: 3.195537841, Training Accuracy: 21.504\n",
            "Worker 1, [06/08]: Training Loss: 3.044762489, Training Accuracy: 24.432\n",
            "Worker 1, [07/08]: Training Loss: 2.920546254, Training Accuracy: 26.296\n",
            "Worker 1, [08/08]: Training Loss: 2.812243348, Training Accuracy: 28.528\n",
            "Time taken for training worker 1: 0:01:25.810651\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 4.291843493, Training Accuracy: 4.808\n",
            "Worker 2, [02/08]: Training Loss: 3.816796027, Training Accuracy: 10.988\n",
            "Worker 2, [03/08]: Training Loss: 3.555709440, Training Accuracy: 15.324\n",
            "Worker 2, [04/08]: Training Loss: 3.353708510, Training Accuracy: 18.552\n",
            "Worker 2, [05/08]: Training Loss: 3.177699926, Training Accuracy: 21.872\n",
            "Worker 2, [06/08]: Training Loss: 3.034588182, Training Accuracy: 24.616\n",
            "Worker 2, [07/08]: Training Loss: 2.896238709, Training Accuracy: 27.168\n",
            "Worker 2, [08/08]: Training Loss: 2.794929221, Training Accuracy: 29.068\n",
            "Time taken for training worker 2: 0:01:30.325812\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001658\n",
            "Global Update 01: Test Loss: 2.960003367, Test Accuracy: 30.390\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.868156375, Training Accuracy: 27.672\n",
            "Worker 1, [02/08]: Training Loss: 2.720451442, Training Accuracy: 30.568\n",
            "Worker 1, [03/08]: Training Loss: 2.634408314, Training Accuracy: 32.504\n",
            "Worker 1, [04/08]: Training Loss: 2.557181688, Training Accuracy: 33.592\n",
            "Worker 1, [05/08]: Training Loss: 2.483751769, Training Accuracy: 35.204\n",
            "Worker 1, [06/08]: Training Loss: 2.433566610, Training Accuracy: 36.204\n",
            "Worker 1, [07/08]: Training Loss: 2.359093584, Training Accuracy: 37.976\n",
            "Worker 1, [08/08]: Training Loss: 2.301370069, Training Accuracy: 39.284\n",
            "Time taken for training worker 1: 0:01:31.876399\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.845321472, Training Accuracy: 28.100\n",
            "Worker 2, [02/08]: Training Loss: 2.728380026, Training Accuracy: 30.736\n",
            "Worker 2, [03/08]: Training Loss: 2.630975062, Training Accuracy: 32.244\n",
            "Worker 2, [04/08]: Training Loss: 2.560422528, Training Accuracy: 33.664\n",
            "Worker 2, [05/08]: Training Loss: 2.490493207, Training Accuracy: 35.268\n",
            "Worker 2, [06/08]: Training Loss: 2.416221427, Training Accuracy: 36.940\n",
            "Worker 2, [07/08]: Training Loss: 2.363720608, Training Accuracy: 38.216\n",
            "Worker 2, [08/08]: Training Loss: 2.316010459, Training Accuracy: 39.060\n",
            "Time taken for training worker 2: 0:01:33.879052\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001722\n",
            "Global Update 02: Test Loss: 2.214037673, Test Accuracy: 41.990\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.363322169, Training Accuracy: 38.032\n",
            "Worker 1, [02/08]: Training Loss: 2.285857722, Training Accuracy: 39.984\n",
            "Worker 1, [03/08]: Training Loss: 2.234148098, Training Accuracy: 40.268\n"
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [2, 4, 8]\n",
        "J = [4, 8, 16, 32, 64]\n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    local_SGD(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SlowMo Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "class SlowMoOptimizer(Optimizer):\n",
        "    def __init__(self, global_model, momentum, lr=0.01, beta=0.5, alpha=1.0):\n",
        "        self.global_model = global_model\n",
        "        self.lr = lr\n",
        "        self.beta = beta\n",
        "        self.alpha = alpha\n",
        "        self.momentum = momentum\n",
        "        params = list(global_model.parameters())\n",
        "        super(SlowMoOptimizer, self).__init__(params, {'lr': lr})\n",
        "        \n",
        "    def step(self, local_models):\n",
        "        # Calculate exact average of local models\n",
        "        avg_state_dict = {key: torch.zeros_like(value) for key, value in local_models[0].state_dict().items()}\n",
        "        for model in local_models:\n",
        "            for key, param in model.state_dict().items():\n",
        "                avg_state_dict[key] += param\n",
        "        \n",
        "        # Averaging the models\n",
        "        for key in avg_state_dict:\n",
        "            avg_state_dict[key] /= len(local_models)\n",
        "        \n",
        "        # Perform SlowMo momentum update\n",
        "        for key in self.global_model.state_dict().keys():\n",
        "            self.momentum[key] = self.beta * self.momentum[key] + (1.0 / self.lr) * (self.global_model.state_dict()[key] - avg_state_dict[key])\n",
        "        \n",
        "        # Update global model parameters with outer update\n",
        "        with torch.no_grad():\n",
        "            for key, param in self.global_model.state_dict().items():\n",
        "                param.copy_(param - self.alpha * self.lr * self.momentum[key])\n",
        "\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def local_SGD_SlowMo(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, beta, alpha):\n",
        "\n",
        "  total_start_time = time.time()\n",
        "\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "  \n",
        "  momentum = {key: torch.zeros_like(value) for key, value in global_model.state_dict().items()}\n",
        "  \n",
        "  global_optimizer = SlowMoOptimizer(global_model, momentum , lr, beta, alpha)\n",
        "  \n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "\n",
        "  checkpoint = load_checkpoint('slowmo', 64, {'k': k, 'j': j})\n",
        "  if checkpoint is not None:\n",
        "      global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      global_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      \n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn)\n",
        "      print(f'Global Update: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      \n",
        "      return None\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for loca_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    global_optimizer.step(local_models)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    for local_optimizer in local_optimizers:\n",
        "        local_optimizer.param_groups[0]['lr'] = global_optimizer.param_groups[0]['lr']    \n",
        "\n",
        "    for local_model in local_models:\n",
        "       local_model.load_state_dict(global_model.state_dict())\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Global Update {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "  # Save checkpoint\n",
        "  save_checkpoint({\n",
        "              'epoch': num_epochs,\n",
        "              'model_state_dict': global_model.state_dict(),\n",
        "              'optimizer_state_dict': global_optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss': loss_fn\n",
        "              }, 149, 64, 'slowmo', {'k': k, 'j': j})\n",
        "\n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for local_SGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:4\n",
            "==================================================\n",
            "Global Update: Test Loss: 1.981766467, Test Accuracy: 56.000\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:8\n",
            "==================================================\n",
            "Global Update: Test Loss: 2.081315363, Test Accuracy: 54.610\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:16\n",
            "==================================================\n",
            "Global Update: Test Loss: 2.200608175, Test Accuracy: 53.200\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:32\n",
            "==================================================\n",
            "Worker 1, [01/32]: Training Loss: 4.318718161, Training Accuracy: 4.468\n",
            "Worker 1, [02/32]: Training Loss: 3.877831652, Training Accuracy: 9.696\n",
            "Worker 1, [03/32]: Training Loss: 3.640464294, Training Accuracy: 13.312\n",
            "Worker 1, [04/32]: Training Loss: 3.422830634, Training Accuracy: 17.076\n",
            "Worker 1, [05/32]: Training Loss: 3.255500807, Training Accuracy: 20.200\n",
            "Worker 1, [06/32]: Training Loss: 3.113154846, Training Accuracy: 22.964\n",
            "Worker 1, [07/32]: Training Loss: 2.981484269, Training Accuracy: 25.292\n",
            "Worker 1, [08/32]: Training Loss: 2.862554001, Training Accuracy: 27.732\n",
            "Worker 1, [09/32]: Training Loss: 2.785575069, Training Accuracy: 28.984\n",
            "Worker 1, [10/32]: Training Loss: 2.674609156, Training Accuracy: 31.456\n",
            "Worker 1, [11/32]: Training Loss: 2.623153888, Training Accuracy: 32.472\n",
            "Worker 1, [12/32]: Training Loss: 2.542067225, Training Accuracy: 34.320\n",
            "Worker 1, [13/32]: Training Loss: 2.483203181, Training Accuracy: 35.324\n",
            "Worker 1, [14/32]: Training Loss: 2.426890549, Training Accuracy: 36.576\n",
            "Worker 1, [15/32]: Training Loss: 2.368320735, Training Accuracy: 37.844\n",
            "Worker 1, [16/32]: Training Loss: 2.323266455, Training Accuracy: 38.640\n",
            "Worker 1, [17/32]: Training Loss: 2.302392234, Training Accuracy: 39.204\n",
            "Worker 1, [18/32]: Training Loss: 2.256805244, Training Accuracy: 40.208\n",
            "Worker 1, [19/32]: Training Loss: 2.186803485, Training Accuracy: 41.744\n",
            "Worker 1, [20/32]: Training Loss: 2.168537518, Training Accuracy: 41.952\n",
            "Worker 1, [21/32]: Training Loss: 2.147296406, Training Accuracy: 42.520\n",
            "Worker 1, [22/32]: Training Loss: 2.110411103, Training Accuracy: 43.536\n",
            "Worker 1, [23/32]: Training Loss: 2.060665249, Training Accuracy: 44.736\n",
            "Worker 1, [24/32]: Training Loss: 2.045895326, Training Accuracy: 44.552\n",
            "Worker 1, [25/32]: Training Loss: 2.011924494, Training Accuracy: 45.612\n",
            "Worker 1, [26/32]: Training Loss: 1.994966724, Training Accuracy: 45.844\n",
            "Worker 1, [27/32]: Training Loss: 1.966037908, Training Accuracy: 46.796\n",
            "Worker 1, [28/32]: Training Loss: 1.946620011, Training Accuracy: 46.976\n",
            "Worker 1, [29/32]: Training Loss: 1.925417707, Training Accuracy: 47.468\n",
            "Worker 1, [30/32]: Training Loss: 1.915749983, Training Accuracy: 47.860\n",
            "Worker 1, [31/32]: Training Loss: 1.886966591, Training Accuracy: 48.584\n",
            "Worker 1, [32/32]: Training Loss: 1.870698990, Training Accuracy: 48.736\n",
            "Time taken for training worker 1: 0:07:01.674109\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 4.330166889, Training Accuracy: 4.452\n",
            "Worker 2, [02/32]: Training Loss: 3.880449830, Training Accuracy: 9.832\n",
            "Worker 2, [03/32]: Training Loss: 3.629018317, Training Accuracy: 13.916\n",
            "Worker 2, [04/32]: Training Loss: 3.419030047, Training Accuracy: 17.536\n",
            "Worker 2, [05/32]: Training Loss: 3.244625512, Training Accuracy: 20.468\n",
            "Worker 2, [06/32]: Training Loss: 3.090387887, Training Accuracy: 23.392\n",
            "Worker 2, [07/32]: Training Loss: 2.957504899, Training Accuracy: 25.860\n",
            "Worker 2, [08/32]: Training Loss: 2.853302778, Training Accuracy: 27.272\n",
            "Worker 2, [09/32]: Training Loss: 2.753476272, Training Accuracy: 30.140\n",
            "Worker 2, [10/32]: Training Loss: 2.658668277, Training Accuracy: 31.484\n",
            "Worker 2, [11/32]: Training Loss: 2.598476278, Training Accuracy: 32.940\n",
            "Worker 2, [12/32]: Training Loss: 2.524171440, Training Accuracy: 34.240\n",
            "Worker 2, [13/32]: Training Loss: 2.476561191, Training Accuracy: 35.580\n",
            "Worker 2, [14/32]: Training Loss: 2.418727922, Training Accuracy: 36.348\n",
            "Worker 2, [15/32]: Training Loss: 2.372902742, Training Accuracy: 37.520\n",
            "Worker 2, [16/32]: Training Loss: 2.310814691, Training Accuracy: 38.632\n",
            "Worker 2, [17/32]: Training Loss: 2.288344751, Training Accuracy: 39.232\n",
            "Worker 2, [18/32]: Training Loss: 2.244608941, Training Accuracy: 40.524\n",
            "Worker 2, [19/32]: Training Loss: 2.218673080, Training Accuracy: 41.044\n",
            "Worker 2, [20/32]: Training Loss: 2.154305523, Training Accuracy: 42.432\n",
            "Worker 2, [21/32]: Training Loss: 2.155362810, Training Accuracy: 42.144\n",
            "Worker 2, [22/32]: Training Loss: 2.108835120, Training Accuracy: 42.956\n",
            "Worker 2, [23/32]: Training Loss: 2.079338666, Training Accuracy: 43.860\n",
            "Worker 2, [24/32]: Training Loss: 2.056498876, Training Accuracy: 44.484\n",
            "Worker 2, [25/32]: Training Loss: 2.031588143, Training Accuracy: 45.152\n",
            "Worker 2, [26/32]: Training Loss: 2.005695744, Training Accuracy: 45.408\n",
            "Worker 2, [27/32]: Training Loss: 1.976559590, Training Accuracy: 46.288\n",
            "Worker 2, [28/32]: Training Loss: 1.953573124, Training Accuracy: 46.704\n",
            "Worker 2, [29/32]: Training Loss: 1.943700608, Training Accuracy: 46.732\n",
            "Worker 2, [30/32]: Training Loss: 1.913658696, Training Accuracy: 47.540\n",
            "Worker 2, [31/32]: Training Loss: 1.885051278, Training Accuracy: 48.292\n",
            "Worker 2, [32/32]: Training Loss: 1.874550997, Training Accuracy: 48.588\n",
            "Time taken for training worker 2: 0:07:10.711274\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002508\n",
            "Global Update 01: Test Loss: 3.168802974, Test Accuracy: 31.770\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.431942189, Training Accuracy: 36.572\n",
            "Worker 1, [02/32]: Training Loss: 2.221474648, Training Accuracy: 41.172\n",
            "Worker 1, [03/32]: Training Loss: 2.112546628, Training Accuracy: 43.304\n",
            "Worker 1, [04/32]: Training Loss: 2.036472640, Training Accuracy: 44.896\n",
            "Worker 1, [05/32]: Training Loss: 1.979697591, Training Accuracy: 46.564\n",
            "Worker 1, [06/32]: Training Loss: 1.924758551, Training Accuracy: 47.644\n",
            "Worker 1, [07/32]: Training Loss: 1.884784204, Training Accuracy: 48.404\n",
            "Worker 1, [08/32]: Training Loss: 1.863711866, Training Accuracy: 48.948\n",
            "Worker 1, [09/32]: Training Loss: 1.823090219, Training Accuracy: 49.920\n",
            "Worker 1, [10/32]: Training Loss: 1.779462171, Training Accuracy: 50.984\n",
            "Worker 1, [11/32]: Training Loss: 1.787315282, Training Accuracy: 50.900\n",
            "Worker 1, [12/32]: Training Loss: 1.726943334, Training Accuracy: 52.248\n",
            "Worker 1, [13/32]: Training Loss: 1.716133309, Training Accuracy: 52.428\n",
            "Worker 1, [14/32]: Training Loss: 1.698230388, Training Accuracy: 53.132\n",
            "Worker 1, [15/32]: Training Loss: 1.668448001, Training Accuracy: 53.360\n",
            "Worker 1, [16/32]: Training Loss: 1.653698195, Training Accuracy: 53.596\n",
            "Worker 1, [17/32]: Training Loss: 1.638230688, Training Accuracy: 53.900\n",
            "Worker 1, [18/32]: Training Loss: 1.599241915, Training Accuracy: 55.280\n",
            "Worker 1, [19/32]: Training Loss: 1.594455496, Training Accuracy: 55.332\n",
            "Worker 1, [20/32]: Training Loss: 1.576093735, Training Accuracy: 55.792\n",
            "Worker 1, [21/32]: Training Loss: 1.571796291, Training Accuracy: 55.892\n",
            "Worker 1, [22/32]: Training Loss: 1.551219460, Training Accuracy: 56.328\n",
            "Worker 1, [23/32]: Training Loss: 1.532245905, Training Accuracy: 56.332\n",
            "Worker 1, [24/32]: Training Loss: 1.512378622, Training Accuracy: 56.872\n",
            "Worker 1, [25/32]: Training Loss: 1.514010190, Training Accuracy: 57.228\n",
            "Worker 1, [26/32]: Training Loss: 1.492566506, Training Accuracy: 57.616\n",
            "Worker 1, [27/32]: Training Loss: 1.466586415, Training Accuracy: 58.212\n",
            "Worker 1, [28/32]: Training Loss: 1.468159144, Training Accuracy: 58.292\n",
            "Worker 1, [29/32]: Training Loss: 1.457779737, Training Accuracy: 58.756\n",
            "Worker 1, [30/32]: Training Loss: 1.452110873, Training Accuracy: 58.756\n",
            "Worker 1, [31/32]: Training Loss: 1.443674989, Training Accuracy: 58.592\n",
            "Worker 1, [32/32]: Training Loss: 1.444204823, Training Accuracy: 59.144\n",
            "Time taken for training worker 1: 0:07:04.094740\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.426688716, Training Accuracy: 36.432\n",
            "Worker 2, [02/32]: Training Loss: 2.219134543, Training Accuracy: 41.052\n",
            "Worker 2, [03/32]: Training Loss: 2.118452910, Training Accuracy: 42.868\n",
            "Worker 2, [04/32]: Training Loss: 2.045770933, Training Accuracy: 44.904\n",
            "Worker 2, [05/32]: Training Loss: 1.983618726, Training Accuracy: 46.328\n",
            "Worker 2, [06/32]: Training Loss: 1.938860019, Training Accuracy: 47.112\n",
            "Worker 2, [07/32]: Training Loss: 1.888205119, Training Accuracy: 48.336\n",
            "Worker 2, [08/32]: Training Loss: 1.859154945, Training Accuracy: 48.872\n",
            "Worker 2, [09/32]: Training Loss: 1.836106800, Training Accuracy: 49.456\n",
            "Worker 2, [10/32]: Training Loss: 1.803494883, Training Accuracy: 50.396\n",
            "Worker 2, [11/32]: Training Loss: 1.773971741, Training Accuracy: 50.960\n",
            "Worker 2, [12/32]: Training Loss: 1.746322549, Training Accuracy: 51.716\n",
            "Worker 2, [13/32]: Training Loss: 1.720684752, Training Accuracy: 52.128\n",
            "Worker 2, [14/32]: Training Loss: 1.702822019, Training Accuracy: 52.176\n",
            "Worker 2, [15/32]: Training Loss: 1.682480454, Training Accuracy: 52.600\n",
            "Worker 2, [16/32]: Training Loss: 1.660389918, Training Accuracy: 53.452\n",
            "Worker 2, [17/32]: Training Loss: 1.641048950, Training Accuracy: 53.728\n",
            "Worker 2, [18/32]: Training Loss: 1.629200956, Training Accuracy: 54.140\n",
            "Worker 2, [19/32]: Training Loss: 1.617610219, Training Accuracy: 54.348\n",
            "Worker 2, [20/32]: Training Loss: 1.591941869, Training Accuracy: 54.928\n",
            "Worker 2, [21/32]: Training Loss: 1.563827653, Training Accuracy: 55.948\n",
            "Worker 2, [22/32]: Training Loss: 1.561260635, Training Accuracy: 55.868\n",
            "Worker 2, [23/32]: Training Loss: 1.553112802, Training Accuracy: 56.184\n",
            "Worker 2, [24/32]: Training Loss: 1.537609920, Training Accuracy: 56.504\n",
            "Worker 2, [25/32]: Training Loss: 1.518253313, Training Accuracy: 56.824\n",
            "Worker 2, [26/32]: Training Loss: 1.504978355, Training Accuracy: 57.348\n",
            "Worker 2, [27/32]: Training Loss: 1.494689944, Training Accuracy: 57.128\n",
            "Worker 2, [28/32]: Training Loss: 1.491332804, Training Accuracy: 57.880\n",
            "Worker 2, [29/32]: Training Loss: 1.475501822, Training Accuracy: 57.824\n",
            "Worker 2, [30/32]: Training Loss: 1.470937283, Training Accuracy: 58.072\n",
            "Worker 2, [31/32]: Training Loss: 1.442255861, Training Accuracy: 58.704\n",
            "Worker 2, [32/32]: Training Loss: 1.453605708, Training Accuracy: 58.488\n",
            "Time taken for training worker 2: 0:07:05.757619\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002746\n",
            "Global Update 02: Test Loss: 3.135097251, Test Accuracy: 42.940\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 1.980078971, Training Accuracy: 47.400\n",
            "Worker 1, [02/32]: Training Loss: 1.697830314, Training Accuracy: 53.116\n",
            "Worker 1, [03/32]: Training Loss: 1.579131570, Training Accuracy: 56.048\n",
            "Worker 1, [04/32]: Training Loss: 1.487182164, Training Accuracy: 57.856\n",
            "Worker 1, [05/32]: Training Loss: 1.419155437, Training Accuracy: 59.404\n",
            "Worker 1, [06/32]: Training Loss: 1.369853203, Training Accuracy: 60.732\n",
            "Worker 1, [07/32]: Training Loss: 1.314139842, Training Accuracy: 62.096\n",
            "Worker 1, [08/32]: Training Loss: 1.274146968, Training Accuracy: 63.072\n",
            "Worker 1, [09/32]: Training Loss: 1.244520141, Training Accuracy: 63.740\n",
            "Worker 1, [10/32]: Training Loss: 1.230145811, Training Accuracy: 64.172\n",
            "Worker 1, [11/32]: Training Loss: 1.210686334, Training Accuracy: 64.756\n",
            "Worker 1, [12/32]: Training Loss: 1.180613544, Training Accuracy: 65.412\n",
            "Worker 1, [13/32]: Training Loss: 1.164052683, Training Accuracy: 66.216\n",
            "Worker 1, [14/32]: Training Loss: 1.147282432, Training Accuracy: 65.888\n",
            "Worker 1, [15/32]: Training Loss: 1.127191315, Training Accuracy: 66.808\n",
            "Worker 1, [16/32]: Training Loss: 1.116851261, Training Accuracy: 67.196\n",
            "Worker 1, [17/32]: Training Loss: 1.109903869, Training Accuracy: 67.204\n",
            "Worker 1, [18/32]: Training Loss: 1.075635809, Training Accuracy: 68.224\n",
            "Worker 1, [19/32]: Training Loss: 1.089094044, Training Accuracy: 68.116\n",
            "Worker 1, [20/32]: Training Loss: 1.058600476, Training Accuracy: 68.568\n",
            "Worker 1, [21/32]: Training Loss: 1.057428440, Training Accuracy: 68.788\n",
            "Worker 1, [22/32]: Training Loss: 1.058184624, Training Accuracy: 68.556\n",
            "Worker 1, [23/32]: Training Loss: 1.034256281, Training Accuracy: 69.384\n",
            "Worker 1, [24/32]: Training Loss: 1.035068005, Training Accuracy: 69.184\n",
            "Worker 1, [25/32]: Training Loss: 1.026634756, Training Accuracy: 69.540\n",
            "Worker 1, [26/32]: Training Loss: 1.005820150, Training Accuracy: 69.732\n",
            "Worker 1, [27/32]: Training Loss: 1.010234685, Training Accuracy: 69.740\n",
            "Worker 1, [28/32]: Training Loss: 0.997065117, Training Accuracy: 70.316\n",
            "Worker 1, [29/32]: Training Loss: 0.980279296, Training Accuracy: 70.480\n",
            "Worker 1, [30/32]: Training Loss: 0.973897799, Training Accuracy: 70.744\n",
            "Worker 1, [31/32]: Training Loss: 0.974686578, Training Accuracy: 70.868\n",
            "Worker 1, [32/32]: Training Loss: 0.967300577, Training Accuracy: 71.132\n",
            "Time taken for training worker 1: 0:06:51.598127\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 1.988821987, Training Accuracy: 47.200\n",
            "Worker 2, [02/32]: Training Loss: 1.710079246, Training Accuracy: 52.460\n",
            "Worker 2, [03/32]: Training Loss: 1.581874226, Training Accuracy: 55.460\n",
            "Worker 2, [04/32]: Training Loss: 1.473874557, Training Accuracy: 58.008\n",
            "Worker 2, [05/32]: Training Loss: 1.407046488, Training Accuracy: 60.112\n",
            "Worker 2, [06/32]: Training Loss: 1.380836536, Training Accuracy: 60.504\n",
            "Worker 2, [07/32]: Training Loss: 1.333867752, Training Accuracy: 61.492\n",
            "Worker 2, [08/32]: Training Loss: 1.298045832, Training Accuracy: 62.764\n",
            "Worker 2, [09/32]: Training Loss: 1.263387536, Training Accuracy: 63.352\n",
            "Worker 2, [10/32]: Training Loss: 1.239844682, Training Accuracy: 63.784\n",
            "Worker 2, [11/32]: Training Loss: 1.214696186, Training Accuracy: 64.404\n",
            "Worker 2, [12/32]: Training Loss: 1.196188717, Training Accuracy: 64.984\n",
            "Worker 2, [13/32]: Training Loss: 1.161462075, Training Accuracy: 65.860\n",
            "Worker 2, [14/32]: Training Loss: 1.158984197, Training Accuracy: 65.932\n",
            "Worker 2, [15/32]: Training Loss: 1.134659190, Training Accuracy: 66.532\n",
            "Worker 2, [16/32]: Training Loss: 1.129572284, Training Accuracy: 66.836\n",
            "Worker 2, [17/32]: Training Loss: 1.118534968, Training Accuracy: 66.804\n",
            "Worker 2, [18/32]: Training Loss: 1.108500758, Training Accuracy: 67.208\n",
            "Worker 2, [19/32]: Training Loss: 1.091338238, Training Accuracy: 67.560\n",
            "Worker 2, [20/32]: Training Loss: 1.083002095, Training Accuracy: 68.012\n",
            "Worker 2, [21/32]: Training Loss: 1.067469343, Training Accuracy: 68.256\n",
            "Worker 2, [22/32]: Training Loss: 1.043980923, Training Accuracy: 68.772\n",
            "Worker 2, [23/32]: Training Loss: 1.045400076, Training Accuracy: 68.656\n",
            "Worker 2, [24/32]: Training Loss: 1.031529064, Training Accuracy: 69.216\n",
            "Worker 2, [25/32]: Training Loss: 1.029114117, Training Accuracy: 69.036\n",
            "Worker 2, [26/32]: Training Loss: 1.022031960, Training Accuracy: 69.416\n",
            "Worker 2, [27/32]: Training Loss: 1.009300071, Training Accuracy: 69.912\n",
            "Worker 2, [28/32]: Training Loss: 0.995044861, Training Accuracy: 70.236\n",
            "Worker 2, [29/32]: Training Loss: 1.000965547, Training Accuracy: 69.764\n",
            "Worker 2, [30/32]: Training Loss: 0.979732665, Training Accuracy: 70.608\n",
            "Worker 2, [31/32]: Training Loss: 0.990817274, Training Accuracy: 70.148\n",
            "Worker 2, [32/32]: Training Loss: 0.968331316, Training Accuracy: 70.908\n",
            "Time taken for training worker 2: 0:06:31.093693\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002289\n",
            "Global Update 03: Test Loss: 3.201050324, Test Accuracy: 48.660\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 1.561305115, Training Accuracy: 58.348\n",
            "Worker 1, [02/32]: Training Loss: 1.289966601, Training Accuracy: 62.740\n",
            "Worker 1, [03/32]: Training Loss: 1.183878956, Training Accuracy: 65.248\n",
            "Worker 1, [04/32]: Training Loss: 1.096486104, Training Accuracy: 67.724\n",
            "Worker 1, [05/32]: Training Loss: 1.027644278, Training Accuracy: 69.664\n",
            "Worker 1, [06/32]: Training Loss: 0.987390538, Training Accuracy: 70.660\n",
            "Worker 1, [07/32]: Training Loss: 0.930439596, Training Accuracy: 71.876\n",
            "Worker 1, [08/32]: Training Loss: 0.892744874, Training Accuracy: 73.268\n",
            "Worker 1, [09/32]: Training Loss: 0.863512769, Training Accuracy: 74.160\n",
            "Worker 1, [10/32]: Training Loss: 0.831226963, Training Accuracy: 75.020\n",
            "Worker 1, [11/32]: Training Loss: 0.811114610, Training Accuracy: 75.496\n",
            "Worker 1, [12/32]: Training Loss: 0.783176349, Training Accuracy: 76.720\n",
            "Worker 1, [13/32]: Training Loss: 0.743474483, Training Accuracy: 77.648\n",
            "Worker 1, [14/32]: Training Loss: 0.740630962, Training Accuracy: 77.772\n",
            "Worker 1, [15/32]: Training Loss: 0.719623807, Training Accuracy: 78.380\n",
            "Worker 1, [16/32]: Training Loss: 0.693646719, Training Accuracy: 79.028\n",
            "Worker 1, [17/32]: Training Loss: 0.667601857, Training Accuracy: 80.084\n",
            "Worker 1, [18/32]: Training Loss: 0.673892643, Training Accuracy: 79.528\n",
            "Worker 1, [19/32]: Training Loss: 0.658143706, Training Accuracy: 80.056\n",
            "Worker 1, [20/32]: Training Loss: 0.641769447, Training Accuracy: 80.524\n",
            "Worker 1, [21/32]: Training Loss: 0.623737109, Training Accuracy: 81.304\n",
            "Worker 1, [22/32]: Training Loss: 0.616546291, Training Accuracy: 81.100\n",
            "Worker 1, [23/32]: Training Loss: 0.599317777, Training Accuracy: 81.904\n",
            "Worker 1, [24/32]: Training Loss: 0.589819826, Training Accuracy: 82.160\n",
            "Worker 1, [25/32]: Training Loss: 0.582639263, Training Accuracy: 82.520\n",
            "Worker 1, [26/32]: Training Loss: 0.571638629, Training Accuracy: 82.816\n",
            "Worker 1, [27/32]: Training Loss: 0.568312916, Training Accuracy: 82.740\n",
            "Worker 1, [28/32]: Training Loss: 0.551600126, Training Accuracy: 83.328\n",
            "Worker 1, [29/32]: Training Loss: 0.551059080, Training Accuracy: 83.416\n",
            "Worker 1, [30/32]: Training Loss: 0.549861383, Training Accuracy: 83.468\n",
            "Worker 1, [31/32]: Training Loss: 0.536587891, Training Accuracy: 83.892\n",
            "Worker 1, [32/32]: Training Loss: 0.535061059, Training Accuracy: 83.992\n",
            "Time taken for training worker 1: 0:06:29.101162\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 1.578471987, Training Accuracy: 57.784\n",
            "Worker 2, [02/32]: Training Loss: 1.307528218, Training Accuracy: 62.308\n",
            "Worker 2, [03/32]: Training Loss: 1.195335220, Training Accuracy: 65.456\n",
            "Worker 2, [04/32]: Training Loss: 1.111984892, Training Accuracy: 67.372\n",
            "Worker 2, [05/32]: Training Loss: 1.045327596, Training Accuracy: 69.064\n",
            "Worker 2, [06/32]: Training Loss: 1.002142515, Training Accuracy: 70.224\n",
            "Worker 2, [07/32]: Training Loss: 0.940573336, Training Accuracy: 72.252\n",
            "Worker 2, [08/32]: Training Loss: 0.898860240, Training Accuracy: 72.948\n",
            "Worker 2, [09/32]: Training Loss: 0.863007753, Training Accuracy: 74.100\n",
            "Worker 2, [10/32]: Training Loss: 0.833589261, Training Accuracy: 75.200\n",
            "Worker 2, [11/32]: Training Loss: 0.804185679, Training Accuracy: 76.036\n",
            "Worker 2, [12/32]: Training Loss: 0.781669486, Training Accuracy: 76.420\n",
            "Worker 2, [13/32]: Training Loss: 0.760185619, Training Accuracy: 76.904\n",
            "Worker 2, [14/32]: Training Loss: 0.727681662, Training Accuracy: 78.176\n",
            "Worker 2, [15/32]: Training Loss: 0.721320595, Training Accuracy: 78.164\n",
            "Worker 2, [16/32]: Training Loss: 0.700029959, Training Accuracy: 78.656\n",
            "Worker 2, [17/32]: Training Loss: 0.673168594, Training Accuracy: 79.616\n",
            "Worker 2, [18/32]: Training Loss: 0.678237658, Training Accuracy: 79.640\n",
            "Worker 2, [19/32]: Training Loss: 0.666839988, Training Accuracy: 79.800\n",
            "Worker 2, [20/32]: Training Loss: 0.630215681, Training Accuracy: 81.080\n",
            "Worker 2, [21/32]: Training Loss: 0.625397056, Training Accuracy: 81.048\n",
            "Worker 2, [22/32]: Training Loss: 0.620291678, Training Accuracy: 81.176\n",
            "Worker 2, [23/32]: Training Loss: 0.614874962, Training Accuracy: 81.064\n",
            "Worker 2, [24/32]: Training Loss: 0.601968008, Training Accuracy: 81.660\n",
            "Worker 2, [25/32]: Training Loss: 0.593597234, Training Accuracy: 81.992\n",
            "Worker 2, [26/32]: Training Loss: 0.584562037, Training Accuracy: 82.320\n",
            "Worker 2, [27/32]: Training Loss: 0.584070700, Training Accuracy: 82.212\n",
            "Worker 2, [28/32]: Training Loss: 0.560004755, Training Accuracy: 83.096\n",
            "Worker 2, [29/32]: Training Loss: 0.562012568, Training Accuracy: 82.900\n",
            "Worker 2, [30/32]: Training Loss: 0.544357420, Training Accuracy: 83.580\n",
            "Worker 2, [31/32]: Training Loss: 0.549934416, Training Accuracy: 83.088\n",
            "Worker 2, [32/32]: Training Loss: 0.540437037, Training Accuracy: 83.840\n",
            "Time taken for training worker 2: 0:06:21.438946\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002182\n",
            "Global Update 04: Test Loss: 2.824673044, Test Accuracy: 48.240\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:54:40.258473\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:64\n",
            "==================================================\n",
            "Worker 1, [01/64]: Training Loss: 4.317410567, Training Accuracy: 4.348\n",
            "Worker 1, [02/64]: Training Loss: 3.865632965, Training Accuracy: 9.828\n",
            "Worker 1, [03/64]: Training Loss: 3.608775807, Training Accuracy: 14.088\n",
            "Worker 1, [04/64]: Training Loss: 3.400542918, Training Accuracy: 17.532\n",
            "Worker 1, [05/64]: Training Loss: 3.235713992, Training Accuracy: 20.716\n",
            "Worker 1, [06/64]: Training Loss: 3.095553910, Training Accuracy: 23.308\n",
            "Worker 1, [07/64]: Training Loss: 2.962388220, Training Accuracy: 25.792\n",
            "Worker 1, [08/64]: Training Loss: 2.838548064, Training Accuracy: 28.288\n",
            "Worker 1, [09/64]: Training Loss: 2.743956449, Training Accuracy: 30.340\n",
            "Worker 1, [10/64]: Training Loss: 2.665711515, Training Accuracy: 31.372\n",
            "Worker 1, [11/64]: Training Loss: 2.597885691, Training Accuracy: 32.896\n",
            "Worker 1, [12/64]: Training Loss: 2.550057019, Training Accuracy: 33.956\n",
            "Worker 1, [13/64]: Training Loss: 2.482825580, Training Accuracy: 35.332\n",
            "Worker 1, [14/64]: Training Loss: 2.428326385, Training Accuracy: 36.352\n",
            "Worker 1, [15/64]: Training Loss: 2.367351174, Training Accuracy: 37.980\n",
            "Worker 1, [16/64]: Training Loss: 2.333726091, Training Accuracy: 38.500\n",
            "Worker 1, [17/64]: Training Loss: 2.278588459, Training Accuracy: 39.872\n",
            "Worker 1, [18/64]: Training Loss: 2.232725452, Training Accuracy: 40.412\n",
            "Worker 1, [19/64]: Training Loss: 2.206172033, Training Accuracy: 41.128\n",
            "Worker 1, [20/64]: Training Loss: 2.158014825, Training Accuracy: 42.556\n",
            "Worker 1, [21/64]: Training Loss: 2.137102863, Training Accuracy: 42.820\n",
            "Worker 1, [22/64]: Training Loss: 2.088616492, Training Accuracy: 43.744\n",
            "Worker 1, [23/64]: Training Loss: 2.065491297, Training Accuracy: 44.480\n",
            "Worker 1, [24/64]: Training Loss: 2.030434675, Training Accuracy: 45.212\n",
            "Worker 1, [25/64]: Training Loss: 2.011324461, Training Accuracy: 45.440\n",
            "Worker 1, [26/64]: Training Loss: 1.979280221, Training Accuracy: 46.316\n",
            "Worker 1, [27/64]: Training Loss: 1.974996601, Training Accuracy: 46.380\n",
            "Worker 1, [28/64]: Training Loss: 1.919743788, Training Accuracy: 47.740\n",
            "Worker 1, [29/64]: Training Loss: 1.917120255, Training Accuracy: 47.664\n",
            "Worker 1, [30/64]: Training Loss: 1.893121806, Training Accuracy: 48.212\n",
            "Worker 1, [31/64]: Training Loss: 1.890167378, Training Accuracy: 48.144\n",
            "Worker 1, [32/64]: Training Loss: 1.844495233, Training Accuracy: 48.712\n",
            "Worker 1, [33/64]: Training Loss: 1.841043082, Training Accuracy: 49.164\n",
            "Worker 1, [34/64]: Training Loss: 1.822387205, Training Accuracy: 50.176\n",
            "Worker 1, [35/64]: Training Loss: 1.821244784, Training Accuracy: 49.960\n",
            "Worker 1, [36/64]: Training Loss: 1.792483372, Training Accuracy: 50.860\n",
            "Worker 1, [37/64]: Training Loss: 1.765942500, Training Accuracy: 51.200\n",
            "Worker 1, [38/64]: Training Loss: 1.760168040, Training Accuracy: 51.632\n",
            "Worker 1, [39/64]: Training Loss: 1.736625870, Training Accuracy: 51.608\n",
            "Worker 1, [40/64]: Training Loss: 1.740168512, Training Accuracy: 51.916\n",
            "Worker 1, [41/64]: Training Loss: 1.702151386, Training Accuracy: 53.088\n",
            "Worker 1, [42/64]: Training Loss: 1.695686265, Training Accuracy: 52.916\n",
            "Worker 1, [43/64]: Training Loss: 1.691259821, Training Accuracy: 52.868\n",
            "Worker 1, [44/64]: Training Loss: 1.678518673, Training Accuracy: 53.020\n",
            "Worker 1, [45/64]: Training Loss: 1.648180343, Training Accuracy: 54.048\n",
            "Worker 1, [46/64]: Training Loss: 1.669432455, Training Accuracy: 53.172\n",
            "Worker 1, [47/64]: Training Loss: 1.634276824, Training Accuracy: 54.472\n",
            "Worker 1, [48/64]: Training Loss: 1.637031405, Training Accuracy: 54.156\n",
            "Worker 1, [49/64]: Training Loss: 1.628701935, Training Accuracy: 54.936\n",
            "Worker 1, [50/64]: Training Loss: 1.620891346, Training Accuracy: 54.780\n",
            "Worker 1, [51/64]: Training Loss: 1.612359169, Training Accuracy: 54.748\n",
            "Worker 1, [52/64]: Training Loss: 1.580162849, Training Accuracy: 55.476\n",
            "Worker 1, [53/64]: Training Loss: 1.588994144, Training Accuracy: 55.616\n",
            "Worker 1, [54/64]: Training Loss: 1.569151880, Training Accuracy: 55.912\n",
            "Worker 1, [55/64]: Training Loss: 1.577281423, Training Accuracy: 56.012\n",
            "Worker 1, [56/64]: Training Loss: 1.570978406, Training Accuracy: 55.568\n",
            "Worker 1, [57/64]: Training Loss: 1.549650087, Training Accuracy: 56.320\n",
            "Worker 1, [58/64]: Training Loss: 1.538901028, Training Accuracy: 56.496\n",
            "Worker 1, [59/64]: Training Loss: 1.535626119, Training Accuracy: 56.552\n",
            "Worker 1, [60/64]: Training Loss: 1.525627572, Training Accuracy: 56.808\n",
            "Worker 1, [61/64]: Training Loss: 1.520457820, Training Accuracy: 56.952\n",
            "Worker 1, [62/64]: Training Loss: 1.504890394, Training Accuracy: 57.448\n",
            "Worker 1, [63/64]: Training Loss: 1.516819410, Training Accuracy: 57.112\n",
            "Worker 1, [64/64]: Training Loss: 1.506464673, Training Accuracy: 57.224\n",
            "Time taken for training worker 1: 0:12:40.512914\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/64]: Training Loss: 4.325444266, Training Accuracy: 4.312\n",
            "Worker 2, [02/64]: Training Loss: 3.869848942, Training Accuracy: 10.096\n",
            "Worker 2, [03/64]: Training Loss: 3.617584224, Training Accuracy: 13.832\n",
            "Worker 2, [04/64]: Training Loss: 3.390181194, Training Accuracy: 17.708\n",
            "Worker 2, [05/64]: Training Loss: 3.234620202, Training Accuracy: 20.680\n",
            "Worker 2, [06/64]: Training Loss: 3.091413206, Training Accuracy: 23.008\n",
            "Worker 2, [07/64]: Training Loss: 2.946567927, Training Accuracy: 26.216\n",
            "Worker 2, [08/64]: Training Loss: 2.837860748, Training Accuracy: 28.344\n",
            "Worker 2, [09/64]: Training Loss: 2.746902301, Training Accuracy: 29.744\n",
            "Worker 2, [10/64]: Training Loss: 2.676349336, Training Accuracy: 31.280\n",
            "Worker 2, [11/64]: Training Loss: 2.609872618, Training Accuracy: 32.348\n",
            "Worker 2, [12/64]: Training Loss: 2.538668179, Training Accuracy: 34.240\n",
            "Worker 2, [13/64]: Training Loss: 2.490932571, Training Accuracy: 35.544\n",
            "Worker 2, [14/64]: Training Loss: 2.445910472, Training Accuracy: 36.464\n",
            "Worker 2, [15/64]: Training Loss: 2.383241032, Training Accuracy: 37.760\n",
            "Worker 2, [16/64]: Training Loss: 2.347417551, Training Accuracy: 37.996\n",
            "Worker 2, [17/64]: Training Loss: 2.290514057, Training Accuracy: 39.300\n",
            "Worker 2, [18/64]: Training Loss: 2.283440644, Training Accuracy: 39.600\n",
            "Worker 2, [19/64]: Training Loss: 2.225367778, Training Accuracy: 40.672\n",
            "Worker 2, [20/64]: Training Loss: 2.186667262, Training Accuracy: 41.884\n",
            "Worker 2, [21/64]: Training Loss: 2.157678906, Training Accuracy: 42.396\n",
            "Worker 2, [22/64]: Training Loss: 2.120819521, Training Accuracy: 43.136\n",
            "Worker 2, [23/64]: Training Loss: 2.106534614, Training Accuracy: 43.360\n",
            "Worker 2, [24/64]: Training Loss: 2.058813704, Training Accuracy: 44.568\n",
            "Worker 2, [25/64]: Training Loss: 2.052094729, Training Accuracy: 44.452\n",
            "Worker 2, [26/64]: Training Loss: 2.009570019, Training Accuracy: 45.484\n",
            "Worker 2, [27/64]: Training Loss: 1.983343442, Training Accuracy: 46.432\n",
            "Worker 2, [28/64]: Training Loss: 1.968831277, Training Accuracy: 46.392\n",
            "Worker 2, [29/64]: Training Loss: 1.929981878, Training Accuracy: 47.648\n",
            "Worker 2, [30/64]: Training Loss: 1.924923791, Training Accuracy: 47.828\n",
            "Worker 2, [31/64]: Training Loss: 1.895451873, Training Accuracy: 48.408\n",
            "Worker 2, [32/64]: Training Loss: 1.873072530, Training Accuracy: 48.440\n",
            "Worker 2, [33/64]: Training Loss: 1.865749771, Training Accuracy: 49.052\n",
            "Worker 2, [34/64]: Training Loss: 1.855472740, Training Accuracy: 49.228\n",
            "Worker 2, [35/64]: Training Loss: 1.831377150, Training Accuracy: 49.740\n",
            "Worker 2, [36/64]: Training Loss: 1.816469485, Training Accuracy: 49.932\n",
            "Worker 2, [37/64]: Training Loss: 1.790904156, Training Accuracy: 50.628\n",
            "Worker 2, [38/64]: Training Loss: 1.789091389, Training Accuracy: 50.732\n",
            "Worker 2, [39/64]: Training Loss: 1.776141408, Training Accuracy: 50.696\n",
            "Worker 2, [40/64]: Training Loss: 1.749856599, Training Accuracy: 51.176\n",
            "Worker 2, [41/64]: Training Loss: 1.728195107, Training Accuracy: 51.972\n",
            "Worker 2, [42/64]: Training Loss: 1.726276912, Training Accuracy: 51.808\n",
            "Worker 2, [43/64]: Training Loss: 1.708534455, Training Accuracy: 52.464\n",
            "Worker 2, [44/64]: Training Loss: 1.692141042, Training Accuracy: 52.488\n",
            "Worker 2, [45/64]: Training Loss: 1.692141269, Training Accuracy: 53.012\n",
            "Worker 2, [46/64]: Training Loss: 1.690535465, Training Accuracy: 52.968\n",
            "Worker 2, [47/64]: Training Loss: 1.671161644, Training Accuracy: 53.660\n",
            "Worker 2, [48/64]: Training Loss: 1.656512389, Training Accuracy: 53.540\n",
            "Worker 2, [49/64]: Training Loss: 1.647585141, Training Accuracy: 53.480\n",
            "Worker 2, [50/64]: Training Loss: 1.657025066, Training Accuracy: 53.308\n",
            "Worker 2, [51/64]: Training Loss: 1.609596756, Training Accuracy: 54.976\n",
            "Worker 2, [52/64]: Training Loss: 1.628193522, Training Accuracy: 54.496\n",
            "Worker 2, [53/64]: Training Loss: 1.620185853, Training Accuracy: 54.348\n",
            "Worker 2, [54/64]: Training Loss: 1.609862087, Training Accuracy: 54.888\n",
            "Worker 2, [55/64]: Training Loss: 1.599316073, Training Accuracy: 55.344\n",
            "Worker 2, [56/64]: Training Loss: 1.581903169, Training Accuracy: 55.444\n",
            "Worker 2, [57/64]: Training Loss: 1.566258287, Training Accuracy: 55.880\n",
            "Worker 2, [58/64]: Training Loss: 1.558803084, Training Accuracy: 55.892\n",
            "Worker 2, [59/64]: Training Loss: 1.561825912, Training Accuracy: 56.096\n",
            "Worker 2, [60/64]: Training Loss: 1.565093259, Training Accuracy: 55.856\n",
            "Worker 2, [61/64]: Training Loss: 1.553477334, Training Accuracy: 55.956\n",
            "Worker 2, [62/64]: Training Loss: 1.541997157, Training Accuracy: 56.508\n",
            "Worker 2, [63/64]: Training Loss: 1.543747470, Training Accuracy: 56.016\n",
            "Worker 2, [64/64]: Training Loss: 1.508671811, Training Accuracy: 57.444\n",
            "Time taken for training worker 2: 0:12:49.306743\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002191\n",
            "Global Update 01: Test Loss: 3.193186727, Test Accuracy: 30.500\n",
            "**************************************************\n",
            "Worker 1, [01/64]: Training Loss: 2.327212813, Training Accuracy: 38.632\n",
            "Worker 1, [02/64]: Training Loss: 1.996860252, Training Accuracy: 46.212\n",
            "Worker 1, [03/64]: Training Loss: 1.861062275, Training Accuracy: 48.748\n",
            "Worker 1, [04/64]: Training Loss: 1.744274811, Training Accuracy: 51.724\n",
            "Worker 1, [05/64]: Training Loss: 1.669049008, Training Accuracy: 53.428\n",
            "Worker 1, [06/64]: Training Loss: 1.589439724, Training Accuracy: 55.572\n",
            "Worker 1, [07/64]: Training Loss: 1.549108128, Training Accuracy: 56.212\n",
            "Worker 1, [08/64]: Training Loss: 1.487470274, Training Accuracy: 57.852\n",
            "Worker 1, [09/64]: Training Loss: 1.451968465, Training Accuracy: 58.544\n",
            "Worker 1, [10/64]: Training Loss: 1.411195104, Training Accuracy: 59.840\n",
            "Worker 1, [11/64]: Training Loss: 1.380201386, Training Accuracy: 60.304\n",
            "Worker 1, [12/64]: Training Loss: 1.341018404, Training Accuracy: 61.872\n",
            "Worker 1, [13/64]: Training Loss: 1.324556111, Training Accuracy: 61.744\n",
            "Worker 1, [14/64]: Training Loss: 1.300796855, Training Accuracy: 62.492\n",
            "Worker 1, [15/64]: Training Loss: 1.267663553, Training Accuracy: 63.248\n",
            "Worker 1, [16/64]: Training Loss: 1.248861675, Training Accuracy: 63.784\n",
            "Worker 1, [17/64]: Training Loss: 1.218669556, Training Accuracy: 64.056\n",
            "Worker 1, [18/64]: Training Loss: 1.208487179, Training Accuracy: 64.772\n",
            "Worker 1, [19/64]: Training Loss: 1.180657266, Training Accuracy: 65.540\n",
            "Worker 1, [20/64]: Training Loss: 1.171744696, Training Accuracy: 65.732\n",
            "Worker 1, [21/64]: Training Loss: 1.154653857, Training Accuracy: 66.216\n",
            "Worker 1, [22/64]: Training Loss: 1.148758291, Training Accuracy: 66.168\n",
            "Worker 1, [23/64]: Training Loss: 1.125536958, Training Accuracy: 66.820\n",
            "Worker 1, [24/64]: Training Loss: 1.114460817, Training Accuracy: 67.164\n",
            "Worker 1, [25/64]: Training Loss: 1.107482329, Training Accuracy: 67.220\n",
            "Worker 1, [26/64]: Training Loss: 1.076145363, Training Accuracy: 67.936\n",
            "Worker 1, [27/64]: Training Loss: 1.075704022, Training Accuracy: 68.308\n",
            "Worker 1, [28/64]: Training Loss: 1.052574315, Training Accuracy: 68.712\n",
            "Worker 1, [29/64]: Training Loss: 1.050994643, Training Accuracy: 68.804\n",
            "Worker 1, [30/64]: Training Loss: 1.031597618, Training Accuracy: 69.084\n",
            "Worker 1, [31/64]: Training Loss: 1.021467071, Training Accuracy: 69.644\n",
            "Worker 1, [32/64]: Training Loss: 1.019344013, Training Accuracy: 69.496\n",
            "Worker 1, [33/64]: Training Loss: 1.017172079, Training Accuracy: 69.636\n",
            "Worker 1, [34/64]: Training Loss: 0.999807970, Training Accuracy: 70.064\n",
            "Worker 1, [35/64]: Training Loss: 0.991049041, Training Accuracy: 70.244\n",
            "Worker 1, [36/64]: Training Loss: 0.994231696, Training Accuracy: 70.252\n",
            "Worker 1, [37/64]: Training Loss: 0.991596794, Training Accuracy: 70.376\n",
            "Worker 1, [38/64]: Training Loss: 0.965629597, Training Accuracy: 71.124\n",
            "Worker 1, [39/64]: Training Loss: 0.960007020, Training Accuracy: 71.148\n",
            "Worker 1, [40/64]: Training Loss: 0.968748972, Training Accuracy: 70.984\n",
            "Worker 1, [41/64]: Training Loss: 0.944605238, Training Accuracy: 71.756\n",
            "Worker 1, [42/64]: Training Loss: 0.943149190, Training Accuracy: 71.816\n",
            "Worker 1, [43/64]: Training Loss: 0.913789538, Training Accuracy: 72.620\n",
            "Worker 1, [44/64]: Training Loss: 0.936647113, Training Accuracy: 71.692\n",
            "Worker 1, [45/64]: Training Loss: 0.917671679, Training Accuracy: 72.480\n",
            "Worker 1, [46/64]: Training Loss: 0.928336479, Training Accuracy: 72.344\n",
            "Worker 1, [47/64]: Training Loss: 0.903138520, Training Accuracy: 72.948\n",
            "Worker 1, [48/64]: Training Loss: 0.920957278, Training Accuracy: 72.288\n",
            "Worker 1, [49/64]: Training Loss: 0.894169753, Training Accuracy: 73.012\n",
            "Worker 1, [50/64]: Training Loss: 0.916814154, Training Accuracy: 71.948\n",
            "Worker 1, [51/64]: Training Loss: 0.882164282, Training Accuracy: 73.680\n",
            "Worker 1, [52/64]: Training Loss: 0.886630713, Training Accuracy: 73.168\n",
            "Worker 1, [53/64]: Training Loss: 0.891080125, Training Accuracy: 73.204\n",
            "Worker 1, [54/64]: Training Loss: 0.876817425, Training Accuracy: 73.584\n",
            "Worker 1, [55/64]: Training Loss: 0.872406620, Training Accuracy: 73.816\n",
            "Worker 1, [56/64]: Training Loss: 0.872856950, Training Accuracy: 73.692\n",
            "Worker 1, [57/64]: Training Loss: 0.853323670, Training Accuracy: 74.276\n",
            "Worker 1, [58/64]: Training Loss: 0.860147733, Training Accuracy: 73.864\n",
            "Worker 1, [59/64]: Training Loss: 0.848391610, Training Accuracy: 74.168\n",
            "Worker 1, [60/64]: Training Loss: 0.858533604, Training Accuracy: 73.748\n",
            "Worker 1, [61/64]: Training Loss: 0.838071900, Training Accuracy: 74.436\n",
            "Worker 1, [62/64]: Training Loss: 0.846886647, Training Accuracy: 74.512\n",
            "Worker 1, [63/64]: Training Loss: 0.827392305, Training Accuracy: 74.964\n",
            "Worker 1, [64/64]: Training Loss: 0.831430899, Training Accuracy: 75.072\n",
            "Time taken for training worker 1: 0:12:39.140584\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/64]: Training Loss: 2.335462913, Training Accuracy: 38.944\n",
            "Worker 2, [02/64]: Training Loss: 2.015535970, Training Accuracy: 45.172\n",
            "Worker 2, [03/64]: Training Loss: 1.865137018, Training Accuracy: 48.476\n",
            "Worker 2, [04/64]: Training Loss: 1.767070862, Training Accuracy: 50.964\n",
            "Worker 2, [05/64]: Training Loss: 1.696375634, Training Accuracy: 52.832\n",
            "Worker 2, [06/64]: Training Loss: 1.621465589, Training Accuracy: 54.900\n",
            "Worker 2, [07/64]: Training Loss: 1.569770893, Training Accuracy: 55.816\n",
            "Worker 2, [08/64]: Training Loss: 1.521843766, Training Accuracy: 56.768\n",
            "Worker 2, [09/64]: Training Loss: 1.490330937, Training Accuracy: 57.436\n",
            "Worker 2, [10/64]: Training Loss: 1.442480695, Training Accuracy: 58.852\n",
            "Worker 2, [11/64]: Training Loss: 1.403573062, Training Accuracy: 59.748\n",
            "Worker 2, [12/64]: Training Loss: 1.386309685, Training Accuracy: 60.280\n",
            "Worker 2, [13/64]: Training Loss: 1.343379433, Training Accuracy: 61.164\n",
            "Worker 2, [14/64]: Training Loss: 1.330520587, Training Accuracy: 61.364\n",
            "Worker 2, [15/64]: Training Loss: 1.291827791, Training Accuracy: 62.352\n",
            "Worker 2, [16/64]: Training Loss: 1.270419512, Training Accuracy: 63.092\n",
            "Worker 2, [17/64]: Training Loss: 1.247462027, Training Accuracy: 63.432\n",
            "Worker 2, [18/64]: Training Loss: 1.221496288, Training Accuracy: 64.232\n",
            "Worker 2, [19/64]: Training Loss: 1.216571556, Training Accuracy: 64.192\n",
            "Worker 2, [20/64]: Training Loss: 1.197484293, Training Accuracy: 65.028\n",
            "Worker 2, [21/64]: Training Loss: 1.197375170, Training Accuracy: 64.912\n",
            "Worker 2, [22/64]: Training Loss: 1.181317477, Training Accuracy: 65.004\n",
            "Worker 2, [23/64]: Training Loss: 1.157909917, Training Accuracy: 65.992\n",
            "Worker 2, [24/64]: Training Loss: 1.140771229, Training Accuracy: 66.808\n",
            "Worker 2, [25/64]: Training Loss: 1.122840229, Training Accuracy: 66.340\n",
            "Worker 2, [26/64]: Training Loss: 1.113415346, Training Accuracy: 67.172\n",
            "Worker 2, [27/64]: Training Loss: 1.106863858, Training Accuracy: 67.268\n",
            "Worker 2, [28/64]: Training Loss: 1.070009869, Training Accuracy: 68.108\n",
            "Worker 2, [29/64]: Training Loss: 1.078725293, Training Accuracy: 67.944\n",
            "Worker 2, [30/64]: Training Loss: 1.059249114, Training Accuracy: 68.548\n",
            "Worker 2, [31/64]: Training Loss: 1.050607636, Training Accuracy: 68.904\n",
            "Worker 2, [32/64]: Training Loss: 1.044813645, Training Accuracy: 68.876\n",
            "Worker 2, [33/64]: Training Loss: 1.051862196, Training Accuracy: 68.400\n",
            "Worker 2, [34/64]: Training Loss: 1.027530330, Training Accuracy: 69.216\n",
            "Worker 2, [35/64]: Training Loss: 1.014177961, Training Accuracy: 69.800\n",
            "Worker 2, [36/64]: Training Loss: 1.001334269, Training Accuracy: 70.032\n",
            "Worker 2, [37/64]: Training Loss: 1.005032173, Training Accuracy: 69.860\n",
            "Worker 2, [38/64]: Training Loss: 0.976934654, Training Accuracy: 70.720\n",
            "Worker 2, [39/64]: Training Loss: 0.995310730, Training Accuracy: 70.240\n",
            "Worker 2, [40/64]: Training Loss: 0.971123807, Training Accuracy: 71.080\n",
            "Worker 2, [41/64]: Training Loss: 0.969890099, Training Accuracy: 70.832\n",
            "Worker 2, [42/64]: Training Loss: 0.952134435, Training Accuracy: 71.700\n",
            "Worker 2, [43/64]: Training Loss: 0.963881349, Training Accuracy: 71.068\n",
            "Worker 2, [44/64]: Training Loss: 0.940566280, Training Accuracy: 71.408\n",
            "Worker 2, [45/64]: Training Loss: 0.931798653, Training Accuracy: 71.876\n",
            "Worker 2, [46/64]: Training Loss: 0.939964106, Training Accuracy: 71.468\n",
            "Worker 2, [47/64]: Training Loss: 0.929810183, Training Accuracy: 71.736\n",
            "Worker 2, [48/64]: Training Loss: 0.934834943, Training Accuracy: 71.548\n",
            "Worker 2, [49/64]: Training Loss: 0.927580272, Training Accuracy: 72.180\n",
            "Worker 2, [50/64]: Training Loss: 0.923290317, Training Accuracy: 71.880\n",
            "Worker 2, [51/64]: Training Loss: 0.912607527, Training Accuracy: 72.588\n",
            "Worker 2, [52/64]: Training Loss: 0.905860386, Training Accuracy: 72.276\n",
            "Worker 2, [53/64]: Training Loss: 0.898504711, Training Accuracy: 72.880\n",
            "Worker 2, [54/64]: Training Loss: 0.896857255, Training Accuracy: 72.980\n",
            "Worker 2, [55/64]: Training Loss: 0.894453185, Training Accuracy: 72.724\n",
            "Worker 2, [56/64]: Training Loss: 0.874282558, Training Accuracy: 73.732\n",
            "Worker 2, [57/64]: Training Loss: 0.867876786, Training Accuracy: 73.664\n",
            "Worker 2, [58/64]: Training Loss: 0.872511329, Training Accuracy: 73.272\n",
            "Worker 2, [59/64]: Training Loss: 0.872985501, Training Accuracy: 73.344\n",
            "Worker 2, [60/64]: Training Loss: 0.868676765, Training Accuracy: 73.776\n",
            "Worker 2, [61/64]: Training Loss: 0.865143298, Training Accuracy: 73.876\n",
            "Worker 2, [62/64]: Training Loss: 0.850894063, Training Accuracy: 74.176\n",
            "Worker 2, [63/64]: Training Loss: 0.847600570, Training Accuracy: 74.076\n",
            "Worker 2, [64/64]: Training Loss: 0.856136582, Training Accuracy: 73.856\n",
            "Time taken for training worker 2: 0:12:34.641302\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002262\n",
            "Global Update 02: Test Loss: 3.877965771, Test Accuracy: 40.550\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:50:45.697478\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:4\n",
            "==================================================\n",
            "Worker 1, [01/04]: Training Loss: 4.511293387, Training Accuracy: 2.496\n",
            "Worker 1, [02/04]: Training Loss: 4.122969507, Training Accuracy: 6.208\n",
            "Worker 1, [03/04]: Training Loss: 3.902740841, Training Accuracy: 9.408\n",
            "Worker 1, [04/04]: Training Loss: 3.755545620, Training Accuracy: 11.640\n",
            "Time taken for training worker 1: 0:00:23.994971\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.521250584, Training Accuracy: 2.680\n",
            "Worker 2, [02/04]: Training Loss: 4.140838539, Training Accuracy: 6.112\n",
            "Worker 2, [03/04]: Training Loss: 3.931404569, Training Accuracy: 9.656\n",
            "Worker 2, [04/04]: Training Loss: 3.781499854, Training Accuracy: 11.056\n",
            "Time taken for training worker 2: 0:00:24.168833\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 4.485296897, Training Accuracy: 2.840\n",
            "Worker 3, [02/04]: Training Loss: 4.101342575, Training Accuracy: 6.568\n",
            "Worker 3, [03/04]: Training Loss: 3.895576725, Training Accuracy: 9.280\n",
            "Worker 3, [04/04]: Training Loss: 3.739871767, Training Accuracy: 11.640\n",
            "Time taken for training worker 3: 0:00:23.873622\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 4.511548198, Training Accuracy: 2.728\n",
            "Worker 4, [02/04]: Training Loss: 4.150943512, Training Accuracy: 6.112\n",
            "Worker 4, [03/04]: Training Loss: 3.945854477, Training Accuracy: 9.112\n",
            "Worker 4, [04/04]: Training Loss: 3.780508229, Training Accuracy: 11.720\n",
            "Time taken for training worker 4: 0:00:23.409349\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002766\n",
            "Global Update 01: Test Loss: 3.696462783, Test Accuracy: 14.630\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.688281832, Training Accuracy: 12.688\n",
            "Worker 1, [02/04]: Training Loss: 3.540656940, Training Accuracy: 15.184\n",
            "Worker 1, [03/04]: Training Loss: 3.398501998, Training Accuracy: 17.440\n",
            "Worker 1, [04/04]: Training Loss: 3.299772381, Training Accuracy: 18.912\n",
            "Time taken for training worker 1: 0:00:23.714827\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.695585054, Training Accuracy: 12.864\n",
            "Worker 2, [02/04]: Training Loss: 3.556077964, Training Accuracy: 15.408\n",
            "Worker 2, [03/04]: Training Loss: 3.418120310, Training Accuracy: 17.432\n",
            "Worker 2, [04/04]: Training Loss: 3.316870622, Training Accuracy: 18.944\n",
            "Time taken for training worker 2: 0:00:24.395143\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.679664664, Training Accuracy: 12.888\n",
            "Worker 3, [02/04]: Training Loss: 3.539603823, Training Accuracy: 15.000\n",
            "Worker 3, [03/04]: Training Loss: 3.420334028, Training Accuracy: 16.968\n",
            "Worker 3, [04/04]: Training Loss: 3.288639066, Training Accuracy: 19.208\n",
            "Time taken for training worker 3: 0:00:24.875085\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.701856439, Training Accuracy: 12.552\n",
            "Worker 4, [02/04]: Training Loss: 3.572880240, Training Accuracy: 14.952\n",
            "Worker 4, [03/04]: Training Loss: 3.427427229, Training Accuracy: 16.848\n",
            "Worker 4, [04/04]: Training Loss: 3.305319666, Training Accuracy: 19.432\n",
            "Time taken for training worker 4: 0:00:23.851488\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002784\n",
            "Global Update 02: Test Loss: 3.720433823, Test Accuracy: 21.910\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.290224676, Training Accuracy: 19.648\n",
            "Worker 1, [02/04]: Training Loss: 3.129142828, Training Accuracy: 22.152\n",
            "Worker 1, [03/04]: Training Loss: 3.050476737, Training Accuracy: 23.792\n",
            "Worker 1, [04/04]: Training Loss: 2.945007193, Training Accuracy: 25.896\n",
            "Time taken for training worker 1: 0:00:23.844815\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.292074454, Training Accuracy: 20.184\n",
            "Worker 2, [02/04]: Training Loss: 3.170386410, Training Accuracy: 22.184\n",
            "Worker 2, [03/04]: Training Loss: 3.075869930, Training Accuracy: 23.392\n",
            "Worker 2, [04/04]: Training Loss: 2.986795320, Training Accuracy: 25.576\n",
            "Time taken for training worker 2: 0:00:23.879030\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.279369843, Training Accuracy: 19.760\n",
            "Worker 3, [02/04]: Training Loss: 3.143225436, Training Accuracy: 22.128\n",
            "Worker 3, [03/04]: Training Loss: 3.060106434, Training Accuracy: 23.544\n",
            "Worker 3, [04/04]: Training Loss: 2.979492802, Training Accuracy: 25.704\n",
            "Time taken for training worker 3: 0:00:23.623551\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.297771353, Training Accuracy: 19.600\n",
            "Worker 4, [02/04]: Training Loss: 3.157225276, Training Accuracy: 22.104\n",
            "Worker 4, [03/04]: Training Loss: 3.068414846, Training Accuracy: 23.704\n",
            "Worker 4, [04/04]: Training Loss: 3.008129223, Training Accuracy: 24.536\n",
            "Time taken for training worker 4: 0:00:24.065691\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002921\n",
            "Global Update 03: Test Loss: 3.245370055, Test Accuracy: 28.480\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.975222534, Training Accuracy: 25.808\n",
            "Worker 1, [02/04]: Training Loss: 2.847781400, Training Accuracy: 27.920\n",
            "Worker 1, [03/04]: Training Loss: 2.774807754, Training Accuracy: 29.264\n",
            "Worker 1, [04/04]: Training Loss: 2.701485760, Training Accuracy: 30.168\n",
            "Time taken for training worker 1: 0:00:23.772316\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.030973711, Training Accuracy: 25.192\n",
            "Worker 2, [02/04]: Training Loss: 2.861173040, Training Accuracy: 27.880\n",
            "Worker 2, [03/04]: Training Loss: 2.784219624, Training Accuracy: 29.040\n",
            "Worker 2, [04/04]: Training Loss: 2.700868725, Training Accuracy: 30.904\n",
            "Time taken for training worker 2: 0:00:23.929405\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.982020704, Training Accuracy: 25.656\n",
            "Worker 3, [02/04]: Training Loss: 2.851823596, Training Accuracy: 27.656\n",
            "Worker 3, [03/04]: Training Loss: 2.786060903, Training Accuracy: 28.936\n",
            "Worker 3, [04/04]: Training Loss: 2.689125803, Training Accuracy: 30.392\n",
            "Time taken for training worker 3: 0:00:24.194455\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.006082967, Training Accuracy: 25.536\n",
            "Worker 4, [02/04]: Training Loss: 2.890029721, Training Accuracy: 27.280\n",
            "Worker 4, [03/04]: Training Loss: 2.799316253, Training Accuracy: 29.248\n",
            "Worker 4, [04/04]: Training Loss: 2.723788748, Training Accuracy: 29.752\n",
            "Time taken for training worker 4: 0:00:23.316813\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002904\n",
            "Global Update 04: Test Loss: 2.688422308, Test Accuracy: 33.580\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.719434102, Training Accuracy: 31.320\n",
            "Worker 1, [02/04]: Training Loss: 2.631038938, Training Accuracy: 32.200\n",
            "Worker 1, [03/04]: Training Loss: 2.547687078, Training Accuracy: 33.848\n",
            "Worker 1, [04/04]: Training Loss: 2.473703163, Training Accuracy: 34.376\n",
            "Time taken for training worker 1: 0:00:24.670380\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.732672455, Training Accuracy: 30.832\n",
            "Worker 2, [02/04]: Training Loss: 2.630216639, Training Accuracy: 32.288\n",
            "Worker 2, [03/04]: Training Loss: 2.548775451, Training Accuracy: 33.528\n",
            "Worker 2, [04/04]: Training Loss: 2.493879495, Training Accuracy: 34.800\n",
            "Time taken for training worker 2: 0:00:24.004055\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.748717952, Training Accuracy: 29.416\n",
            "Worker 3, [02/04]: Training Loss: 2.643166083, Training Accuracy: 32.504\n",
            "Worker 3, [03/04]: Training Loss: 2.559583146, Training Accuracy: 33.288\n",
            "Worker 3, [04/04]: Training Loss: 2.510498620, Training Accuracy: 34.680\n",
            "Time taken for training worker 3: 0:00:23.812332\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.756737792, Training Accuracy: 29.576\n",
            "Worker 4, [02/04]: Training Loss: 2.641719535, Training Accuracy: 32.320\n",
            "Worker 4, [03/04]: Training Loss: 2.579334002, Training Accuracy: 33.432\n",
            "Worker 4, [04/04]: Training Loss: 2.515544565, Training Accuracy: 34.480\n",
            "Time taken for training worker 4: 0:00:24.394948\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002811\n",
            "Global Update 05: Test Loss: 2.435333992, Test Accuracy: 38.090\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.521876240, Training Accuracy: 34.792\n",
            "Worker 1, [02/04]: Training Loss: 2.435656479, Training Accuracy: 36.112\n",
            "Worker 1, [03/04]: Training Loss: 2.385261384, Training Accuracy: 36.944\n",
            "Worker 1, [04/04]: Training Loss: 2.295130566, Training Accuracy: 39.304\n",
            "Time taken for training worker 1: 0:00:23.713091\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.550402494, Training Accuracy: 34.336\n",
            "Worker 2, [02/04]: Training Loss: 2.457418301, Training Accuracy: 36.080\n",
            "Worker 2, [03/04]: Training Loss: 2.383892210, Training Accuracy: 37.080\n",
            "Worker 2, [04/04]: Training Loss: 2.315068240, Training Accuracy: 38.312\n",
            "Time taken for training worker 2: 0:00:24.949555\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.525895673, Training Accuracy: 35.160\n",
            "Worker 3, [02/04]: Training Loss: 2.453560773, Training Accuracy: 36.200\n",
            "Worker 3, [03/04]: Training Loss: 2.373883463, Training Accuracy: 37.232\n",
            "Worker 3, [04/04]: Training Loss: 2.313524481, Training Accuracy: 38.152\n",
            "Time taken for training worker 3: 0:00:23.855530\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.564613453, Training Accuracy: 34.136\n",
            "Worker 4, [02/04]: Training Loss: 2.472880859, Training Accuracy: 35.912\n",
            "Worker 4, [03/04]: Training Loss: 2.389974970, Training Accuracy: 37.432\n",
            "Worker 4, [04/04]: Training Loss: 2.340885542, Training Accuracy: 37.904\n",
            "Time taken for training worker 4: 0:00:23.956717\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002877\n",
            "Global Update 06: Test Loss: 2.277359154, Test Accuracy: 40.470\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.381440394, Training Accuracy: 37.856\n",
            "Worker 1, [02/04]: Training Loss: 2.268267071, Training Accuracy: 39.152\n",
            "Worker 1, [03/04]: Training Loss: 2.210404320, Training Accuracy: 40.920\n",
            "Worker 1, [04/04]: Training Loss: 2.169146883, Training Accuracy: 41.808\n",
            "Time taken for training worker 1: 0:00:23.153037\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.389428445, Training Accuracy: 37.632\n",
            "Worker 2, [02/04]: Training Loss: 2.303059860, Training Accuracy: 38.864\n",
            "Worker 2, [03/04]: Training Loss: 2.248599912, Training Accuracy: 40.104\n",
            "Worker 2, [04/04]: Training Loss: 2.169328637, Training Accuracy: 42.048\n",
            "Time taken for training worker 2: 0:00:24.082687\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.407607696, Training Accuracy: 36.648\n",
            "Worker 3, [02/04]: Training Loss: 2.318839256, Training Accuracy: 38.376\n",
            "Worker 3, [03/04]: Training Loss: 2.233901605, Training Accuracy: 39.920\n",
            "Worker 3, [04/04]: Training Loss: 2.198354398, Training Accuracy: 40.936\n",
            "Time taken for training worker 3: 0:00:23.668463\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.427954709, Training Accuracy: 36.544\n",
            "Worker 4, [02/04]: Training Loss: 2.326971306, Training Accuracy: 38.832\n",
            "Worker 4, [03/04]: Training Loss: 2.258080888, Training Accuracy: 39.408\n",
            "Worker 4, [04/04]: Training Loss: 2.199426714, Training Accuracy: 41.224\n",
            "Time taken for training worker 4: 0:00:22.997923\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002988\n",
            "Global Update 07: Test Loss: 2.168670306, Test Accuracy: 42.690\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.262200731, Training Accuracy: 40.144\n",
            "Worker 1, [02/04]: Training Loss: 2.159431062, Training Accuracy: 41.960\n",
            "Worker 1, [03/04]: Training Loss: 2.101929759, Training Accuracy: 42.848\n",
            "Worker 1, [04/04]: Training Loss: 2.053954373, Training Accuracy: 44.072\n",
            "Time taken for training worker 1: 0:00:25.299413\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.263589597, Training Accuracy: 39.872\n",
            "Worker 2, [02/04]: Training Loss: 2.188327366, Training Accuracy: 41.640\n",
            "Worker 2, [03/04]: Training Loss: 2.123987600, Training Accuracy: 42.920\n",
            "Worker 2, [04/04]: Training Loss: 2.049118817, Training Accuracy: 44.200\n",
            "Time taken for training worker 2: 0:00:23.717178\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.274098315, Training Accuracy: 39.792\n",
            "Worker 3, [02/04]: Training Loss: 2.181837009, Training Accuracy: 41.480\n",
            "Worker 3, [03/04]: Training Loss: 2.111632514, Training Accuracy: 42.720\n",
            "Worker 3, [04/04]: Training Loss: 2.062138056, Training Accuracy: 43.936\n",
            "Time taken for training worker 3: 0:00:24.149626\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.292972750, Training Accuracy: 39.328\n",
            "Worker 4, [02/04]: Training Loss: 2.218571092, Training Accuracy: 40.680\n",
            "Worker 4, [03/04]: Training Loss: 2.129211864, Training Accuracy: 42.784\n",
            "Worker 4, [04/04]: Training Loss: 2.098513010, Training Accuracy: 43.464\n",
            "Time taken for training worker 4: 0:00:23.295249\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002750\n",
            "Global Update 08: Test Loss: 2.105655338, Test Accuracy: 44.770\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.155948324, Training Accuracy: 41.768\n",
            "Worker 1, [02/04]: Training Loss: 2.054511547, Training Accuracy: 44.184\n",
            "Worker 1, [03/04]: Training Loss: 1.983251979, Training Accuracy: 46.008\n",
            "Worker 1, [04/04]: Training Loss: 1.971204199, Training Accuracy: 46.608\n",
            "Time taken for training worker 1: 0:00:24.327180\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.147916295, Training Accuracy: 42.320\n",
            "Worker 2, [02/04]: Training Loss: 2.068803939, Training Accuracy: 44.248\n",
            "Worker 2, [03/04]: Training Loss: 1.979855812, Training Accuracy: 45.064\n",
            "Worker 2, [04/04]: Training Loss: 1.940021351, Training Accuracy: 47.152\n",
            "Time taken for training worker 2: 0:00:24.300013\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.183759723, Training Accuracy: 41.616\n",
            "Worker 3, [02/04]: Training Loss: 2.086578600, Training Accuracy: 43.576\n",
            "Worker 3, [03/04]: Training Loss: 2.017119889, Training Accuracy: 45.120\n",
            "Worker 3, [04/04]: Training Loss: 1.946772816, Training Accuracy: 46.680\n",
            "Time taken for training worker 3: 0:00:23.773735\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.192847304, Training Accuracy: 41.376\n",
            "Worker 4, [02/04]: Training Loss: 2.068699981, Training Accuracy: 44.352\n",
            "Worker 4, [03/04]: Training Loss: 2.048117678, Training Accuracy: 44.120\n",
            "Worker 4, [04/04]: Training Loss: 1.981776341, Training Accuracy: 46.040\n",
            "Time taken for training worker 4: 0:00:23.667886\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002816\n",
            "Global Update 09: Test Loss: 2.039186740, Test Accuracy: 46.570\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.052845262, Training Accuracy: 43.976\n",
            "Worker 1, [02/04]: Training Loss: 1.957221683, Training Accuracy: 47.000\n",
            "Worker 1, [03/04]: Training Loss: 1.911605533, Training Accuracy: 47.648\n",
            "Worker 1, [04/04]: Training Loss: 1.835435805, Training Accuracy: 49.032\n",
            "Time taken for training worker 1: 0:00:23.954035\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.050901342, Training Accuracy: 44.568\n",
            "Worker 2, [02/04]: Training Loss: 1.958779325, Training Accuracy: 46.776\n",
            "Worker 2, [03/04]: Training Loss: 1.912345814, Training Accuracy: 47.400\n",
            "Worker 2, [04/04]: Training Loss: 1.834674177, Training Accuracy: 48.776\n",
            "Time taken for training worker 2: 0:00:23.691712\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.066713441, Training Accuracy: 44.600\n",
            "Worker 3, [02/04]: Training Loss: 1.970684447, Training Accuracy: 46.584\n",
            "Worker 3, [03/04]: Training Loss: 1.906475846, Training Accuracy: 47.800\n",
            "Worker 3, [04/04]: Training Loss: 1.853737864, Training Accuracy: 48.656\n",
            "Time taken for training worker 3: 0:00:23.348395\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.071376528, Training Accuracy: 44.352\n",
            "Worker 4, [02/04]: Training Loss: 1.992709097, Training Accuracy: 45.816\n",
            "Worker 4, [03/04]: Training Loss: 1.945728949, Training Accuracy: 46.992\n",
            "Worker 4, [04/04]: Training Loss: 1.859981986, Training Accuracy: 49.256\n",
            "Time taken for training worker 4: 0:00:23.026362\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002759\n",
            "Global Update 10: Test Loss: 1.994311087, Test Accuracy: 47.660\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.990595743, Training Accuracy: 46.272\n",
            "Worker 1, [02/04]: Training Loss: 1.890585553, Training Accuracy: 48.288\n",
            "Worker 1, [03/04]: Training Loss: 1.821255987, Training Accuracy: 49.288\n",
            "Worker 1, [04/04]: Training Loss: 1.752824280, Training Accuracy: 51.144\n",
            "Time taken for training worker 1: 0:00:23.905958\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.970392824, Training Accuracy: 46.352\n",
            "Worker 2, [02/04]: Training Loss: 1.879037465, Training Accuracy: 48.656\n",
            "Worker 2, [03/04]: Training Loss: 1.802758715, Training Accuracy: 49.536\n",
            "Worker 2, [04/04]: Training Loss: 1.757116459, Training Accuracy: 50.608\n",
            "Time taken for training worker 2: 0:00:24.017758\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.989225684, Training Accuracy: 46.496\n",
            "Worker 3, [02/04]: Training Loss: 1.905309573, Training Accuracy: 48.032\n",
            "Worker 3, [03/04]: Training Loss: 1.817064510, Training Accuracy: 49.472\n",
            "Worker 3, [04/04]: Training Loss: 1.762257459, Training Accuracy: 51.064\n",
            "Time taken for training worker 3: 0:00:23.569958\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.022632837, Training Accuracy: 44.952\n",
            "Worker 4, [02/04]: Training Loss: 1.926379585, Training Accuracy: 47.120\n",
            "Worker 4, [03/04]: Training Loss: 1.841037504, Training Accuracy: 49.080\n",
            "Worker 4, [04/04]: Training Loss: 1.785557119, Training Accuracy: 50.776\n",
            "Time taken for training worker 4: 0:00:23.633590\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002779\n",
            "Global Update 11: Test Loss: 1.983800122, Test Accuracy: 48.330\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.902632720, Training Accuracy: 48.216\n",
            "Worker 1, [02/04]: Training Loss: 1.784535601, Training Accuracy: 51.120\n",
            "Worker 1, [03/04]: Training Loss: 1.718167300, Training Accuracy: 52.344\n",
            "Worker 1, [04/04]: Training Loss: 1.662251095, Training Accuracy: 53.288\n",
            "Time taken for training worker 1: 0:00:23.618853\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.893292336, Training Accuracy: 47.984\n",
            "Worker 2, [02/04]: Training Loss: 1.785537156, Training Accuracy: 51.016\n",
            "Worker 2, [03/04]: Training Loss: 1.717838745, Training Accuracy: 51.688\n",
            "Worker 2, [04/04]: Training Loss: 1.676341581, Training Accuracy: 53.416\n",
            "Time taken for training worker 2: 0:00:24.616626\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.924651432, Training Accuracy: 47.808\n",
            "Worker 3, [02/04]: Training Loss: 1.813797637, Training Accuracy: 50.264\n",
            "Worker 3, [03/04]: Training Loss: 1.731387783, Training Accuracy: 51.344\n",
            "Worker 3, [04/04]: Training Loss: 1.681195373, Training Accuracy: 52.880\n",
            "Time taken for training worker 3: 0:00:23.892714\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.917196524, Training Accuracy: 47.760\n",
            "Worker 4, [02/04]: Training Loss: 1.813832702, Training Accuracy: 50.056\n",
            "Worker 4, [03/04]: Training Loss: 1.771765401, Training Accuracy: 50.680\n",
            "Worker 4, [04/04]: Training Loss: 1.703075059, Training Accuracy: 52.344\n",
            "Time taken for training worker 4: 0:00:24.008990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002803\n",
            "Global Update 12: Test Loss: 1.944131745, Test Accuracy: 49.030\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.831820972, Training Accuracy: 49.648\n",
            "Worker 1, [02/04]: Training Loss: 1.730504026, Training Accuracy: 51.456\n",
            "Worker 1, [03/04]: Training Loss: 1.665902289, Training Accuracy: 52.816\n",
            "Worker 1, [04/04]: Training Loss: 1.583371612, Training Accuracy: 55.192\n",
            "Time taken for training worker 1: 0:00:23.932721\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.831401826, Training Accuracy: 49.936\n",
            "Worker 2, [02/04]: Training Loss: 1.719685039, Training Accuracy: 52.056\n",
            "Worker 2, [03/04]: Training Loss: 1.654178282, Training Accuracy: 53.800\n",
            "Worker 2, [04/04]: Training Loss: 1.594130193, Training Accuracy: 55.040\n",
            "Time taken for training worker 2: 0:00:23.849576\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.831163505, Training Accuracy: 49.688\n",
            "Worker 3, [02/04]: Training Loss: 1.737060319, Training Accuracy: 51.816\n",
            "Worker 3, [03/04]: Training Loss: 1.660886082, Training Accuracy: 53.200\n",
            "Worker 3, [04/04]: Training Loss: 1.627551845, Training Accuracy: 54.080\n",
            "Time taken for training worker 3: 0:00:23.722755\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.865321928, Training Accuracy: 49.016\n",
            "Worker 4, [02/04]: Training Loss: 1.758124488, Training Accuracy: 51.360\n",
            "Worker 4, [03/04]: Training Loss: 1.675844134, Training Accuracy: 53.152\n",
            "Worker 4, [04/04]: Training Loss: 1.642312994, Training Accuracy: 53.728\n",
            "Time taken for training worker 4: 0:00:23.469273\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002777\n",
            "Global Update 13: Test Loss: 1.923239665, Test Accuracy: 49.630\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.767039682, Training Accuracy: 51.136\n",
            "Worker 1, [02/04]: Training Loss: 1.647180292, Training Accuracy: 53.544\n",
            "Worker 1, [03/04]: Training Loss: 1.576402450, Training Accuracy: 54.984\n",
            "Worker 1, [04/04]: Training Loss: 1.514890830, Training Accuracy: 56.776\n",
            "Time taken for training worker 1: 0:00:23.373663\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.752215052, Training Accuracy: 51.456\n",
            "Worker 2, [02/04]: Training Loss: 1.648628074, Training Accuracy: 53.816\n",
            "Worker 2, [03/04]: Training Loss: 1.596849810, Training Accuracy: 55.448\n",
            "Worker 2, [04/04]: Training Loss: 1.529302530, Training Accuracy: 56.752\n",
            "Time taken for training worker 2: 0:00:23.123954\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.751054458, Training Accuracy: 51.472\n",
            "Worker 3, [02/04]: Training Loss: 1.672979939, Training Accuracy: 53.104\n",
            "Worker 3, [03/04]: Training Loss: 1.599202494, Training Accuracy: 54.480\n",
            "Worker 3, [04/04]: Training Loss: 1.520932117, Training Accuracy: 57.168\n",
            "Time taken for training worker 3: 0:00:22.971770\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.794023055, Training Accuracy: 50.312\n",
            "Worker 4, [02/04]: Training Loss: 1.679787722, Training Accuracy: 53.656\n",
            "Worker 4, [03/04]: Training Loss: 1.613811133, Training Accuracy: 54.984\n",
            "Worker 4, [04/04]: Training Loss: 1.567511164, Training Accuracy: 55.520\n",
            "Time taken for training worker 4: 0:00:22.585997\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002760\n",
            "Global Update 14: Test Loss: 1.905498492, Test Accuracy: 50.050\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.689990119, Training Accuracy: 52.792\n",
            "Worker 1, [02/04]: Training Loss: 1.588721611, Training Accuracy: 55.336\n",
            "Worker 1, [03/04]: Training Loss: 1.539350259, Training Accuracy: 56.728\n",
            "Worker 1, [04/04]: Training Loss: 1.454510237, Training Accuracy: 58.752\n",
            "Time taken for training worker 1: 0:00:24.533144\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.685464470, Training Accuracy: 52.960\n",
            "Worker 2, [02/04]: Training Loss: 1.587525882, Training Accuracy: 55.272\n",
            "Worker 2, [03/04]: Training Loss: 1.539554663, Training Accuracy: 56.472\n",
            "Worker 2, [04/04]: Training Loss: 1.458669776, Training Accuracy: 58.160\n",
            "Time taken for training worker 2: 0:00:23.948204\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.701018088, Training Accuracy: 52.368\n",
            "Worker 3, [02/04]: Training Loss: 1.605633376, Training Accuracy: 55.144\n",
            "Worker 3, [03/04]: Training Loss: 1.525399152, Training Accuracy: 56.544\n",
            "Worker 3, [04/04]: Training Loss: 1.460450387, Training Accuracy: 58.376\n",
            "Time taken for training worker 3: 0:00:23.854284\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.713739536, Training Accuracy: 52.456\n",
            "Worker 4, [02/04]: Training Loss: 1.630085595, Training Accuracy: 54.360\n",
            "Worker 4, [03/04]: Training Loss: 1.539151114, Training Accuracy: 56.096\n",
            "Worker 4, [04/04]: Training Loss: 1.473351445, Training Accuracy: 58.016\n",
            "Time taken for training worker 4: 0:00:23.280697\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002830\n",
            "Global Update 15: Test Loss: 1.913802157, Test Accuracy: 50.490\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.639074375, Training Accuracy: 54.856\n",
            "Worker 1, [02/04]: Training Loss: 1.510915420, Training Accuracy: 56.992\n",
            "Worker 1, [03/04]: Training Loss: 1.450782215, Training Accuracy: 58.568\n",
            "Worker 1, [04/04]: Training Loss: 1.387594409, Training Accuracy: 59.920\n",
            "Time taken for training worker 1: 0:00:23.378722\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.635909842, Training Accuracy: 54.576\n",
            "Worker 2, [02/04]: Training Loss: 1.520160424, Training Accuracy: 56.944\n",
            "Worker 2, [03/04]: Training Loss: 1.457796339, Training Accuracy: 58.968\n",
            "Worker 2, [04/04]: Training Loss: 1.395222872, Training Accuracy: 59.944\n",
            "Time taken for training worker 2: 0:00:23.118693\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.641077729, Training Accuracy: 53.952\n",
            "Worker 3, [02/04]: Training Loss: 1.538268199, Training Accuracy: 56.760\n",
            "Worker 3, [03/04]: Training Loss: 1.463671357, Training Accuracy: 58.264\n",
            "Worker 3, [04/04]: Training Loss: 1.407783616, Training Accuracy: 59.472\n",
            "Time taken for training worker 3: 0:00:23.160181\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.656276374, Training Accuracy: 53.928\n",
            "Worker 4, [02/04]: Training Loss: 1.561855467, Training Accuracy: 56.120\n",
            "Worker 4, [03/04]: Training Loss: 1.481435888, Training Accuracy: 58.248\n",
            "Worker 4, [04/04]: Training Loss: 1.445626658, Training Accuracy: 58.496\n",
            "Time taken for training worker 4: 0:00:24.128020\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002967\n",
            "Global Update 16: Test Loss: 1.913364640, Test Accuracy: 50.600\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.593449610, Training Accuracy: 55.072\n",
            "Worker 1, [02/04]: Training Loss: 1.459396960, Training Accuracy: 58.080\n",
            "Worker 1, [03/04]: Training Loss: 1.399039881, Training Accuracy: 59.552\n",
            "Worker 1, [04/04]: Training Loss: 1.345799212, Training Accuracy: 61.400\n",
            "Time taken for training worker 1: 0:00:23.591640\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.579370586, Training Accuracy: 55.368\n",
            "Worker 2, [02/04]: Training Loss: 1.461518950, Training Accuracy: 58.312\n",
            "Worker 2, [03/04]: Training Loss: 1.403339946, Training Accuracy: 59.568\n",
            "Worker 2, [04/04]: Training Loss: 1.344082707, Training Accuracy: 61.280\n",
            "Time taken for training worker 2: 0:00:22.954365\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.593354616, Training Accuracy: 55.152\n",
            "Worker 3, [02/04]: Training Loss: 1.493243347, Training Accuracy: 57.152\n",
            "Worker 3, [03/04]: Training Loss: 1.406804704, Training Accuracy: 59.288\n",
            "Worker 3, [04/04]: Training Loss: 1.343491973, Training Accuracy: 60.848\n",
            "Time taken for training worker 3: 0:00:22.968425\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.593927765, Training Accuracy: 54.984\n",
            "Worker 4, [02/04]: Training Loss: 1.484954217, Training Accuracy: 58.064\n",
            "Worker 4, [03/04]: Training Loss: 1.415947573, Training Accuracy: 59.112\n",
            "Worker 4, [04/04]: Training Loss: 1.364964371, Training Accuracy: 60.944\n",
            "Time taken for training worker 4: 0:00:22.487444\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002732\n",
            "Global Update 17: Test Loss: 1.891649322, Test Accuracy: 51.340\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.528630746, Training Accuracy: 56.344\n",
            "Worker 1, [02/04]: Training Loss: 1.407458565, Training Accuracy: 59.592\n",
            "Worker 1, [03/04]: Training Loss: 1.330240762, Training Accuracy: 61.840\n",
            "Worker 1, [04/04]: Training Loss: 1.272164860, Training Accuracy: 63.056\n",
            "Time taken for training worker 1: 0:00:23.797783\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.519945788, Training Accuracy: 57.392\n",
            "Worker 2, [02/04]: Training Loss: 1.405879362, Training Accuracy: 59.624\n",
            "Worker 2, [03/04]: Training Loss: 1.320572110, Training Accuracy: 61.680\n",
            "Worker 2, [04/04]: Training Loss: 1.254465732, Training Accuracy: 63.224\n",
            "Time taken for training worker 2: 0:00:24.859069\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.526760621, Training Accuracy: 56.648\n",
            "Worker 3, [02/04]: Training Loss: 1.412239577, Training Accuracy: 59.472\n",
            "Worker 3, [03/04]: Training Loss: 1.348509573, Training Accuracy: 61.504\n",
            "Worker 3, [04/04]: Training Loss: 1.279197407, Training Accuracy: 62.616\n",
            "Time taken for training worker 3: 0:00:23.067440\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.572549094, Training Accuracy: 55.760\n",
            "Worker 4, [02/04]: Training Loss: 1.407321403, Training Accuracy: 59.696\n",
            "Worker 4, [03/04]: Training Loss: 1.339766007, Training Accuracy: 61.368\n",
            "Worker 4, [04/04]: Training Loss: 1.300241130, Training Accuracy: 62.216\n",
            "Time taken for training worker 4: 0:00:23.062018\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002706\n",
            "Global Update 18: Test Loss: 1.914002889, Test Accuracy: 51.000\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.466635930, Training Accuracy: 58.216\n",
            "Worker 1, [02/04]: Training Loss: 1.338658139, Training Accuracy: 61.504\n",
            "Worker 1, [03/04]: Training Loss: 1.266417041, Training Accuracy: 63.136\n",
            "Worker 1, [04/04]: Training Loss: 1.209091147, Training Accuracy: 65.232\n",
            "Time taken for training worker 1: 0:00:23.037227\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.454455069, Training Accuracy: 59.032\n",
            "Worker 2, [02/04]: Training Loss: 1.336240757, Training Accuracy: 61.376\n",
            "Worker 2, [03/04]: Training Loss: 1.261289533, Training Accuracy: 63.064\n",
            "Worker 2, [04/04]: Training Loss: 1.210106553, Training Accuracy: 64.696\n",
            "Time taken for training worker 2: 0:00:22.871477\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.475471391, Training Accuracy: 57.832\n",
            "Worker 3, [02/04]: Training Loss: 1.345666084, Training Accuracy: 61.640\n",
            "Worker 3, [03/04]: Training Loss: 1.270294414, Training Accuracy: 63.384\n",
            "Worker 3, [04/04]: Training Loss: 1.215056064, Training Accuracy: 64.608\n",
            "Time taken for training worker 3: 0:00:24.407320\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.505834475, Training Accuracy: 56.888\n",
            "Worker 4, [02/04]: Training Loss: 1.396704870, Training Accuracy: 60.120\n",
            "Worker 4, [03/04]: Training Loss: 1.296812037, Training Accuracy: 62.936\n",
            "Worker 4, [04/04]: Training Loss: 1.245712367, Training Accuracy: 63.728\n",
            "Time taken for training worker 4: 0:00:23.233744\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003511\n",
            "Global Update 19: Test Loss: 1.909148401, Test Accuracy: 51.730\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.411742540, Training Accuracy: 59.776\n",
            "Worker 1, [02/04]: Training Loss: 1.280566150, Training Accuracy: 62.912\n",
            "Worker 1, [03/04]: Training Loss: 1.230531328, Training Accuracy: 64.200\n",
            "Worker 1, [04/04]: Training Loss: 1.146285546, Training Accuracy: 66.216\n",
            "Time taken for training worker 1: 0:00:23.106812\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.403804112, Training Accuracy: 59.800\n",
            "Worker 2, [02/04]: Training Loss: 1.280729548, Training Accuracy: 62.632\n",
            "Worker 2, [03/04]: Training Loss: 1.220878686, Training Accuracy: 64.400\n",
            "Worker 2, [04/04]: Training Loss: 1.164257092, Training Accuracy: 65.552\n",
            "Time taken for training worker 2: 0:00:24.610668\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.432228795, Training Accuracy: 58.984\n",
            "Worker 3, [02/04]: Training Loss: 1.291876228, Training Accuracy: 62.544\n",
            "Worker 3, [03/04]: Training Loss: 1.215732885, Training Accuracy: 64.096\n",
            "Worker 3, [04/04]: Training Loss: 1.171436888, Training Accuracy: 65.624\n",
            "Time taken for training worker 3: 0:00:24.906079\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.439024494, Training Accuracy: 58.784\n",
            "Worker 4, [02/04]: Training Loss: 1.312031679, Training Accuracy: 62.248\n",
            "Worker 4, [03/04]: Training Loss: 1.224171136, Training Accuracy: 64.688\n",
            "Worker 4, [04/04]: Training Loss: 1.193850559, Training Accuracy: 65.080\n",
            "Time taken for training worker 4: 0:00:23.578205\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002896\n",
            "Global Update 20: Test Loss: 1.937788507, Test Accuracy: 51.350\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.360387396, Training Accuracy: 60.744\n",
            "Worker 1, [02/04]: Training Loss: 1.227714709, Training Accuracy: 64.544\n",
            "Worker 1, [03/04]: Training Loss: 1.154496280, Training Accuracy: 66.816\n",
            "Worker 1, [04/04]: Training Loss: 1.099950336, Training Accuracy: 67.416\n",
            "Time taken for training worker 1: 0:00:23.465294\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.389065322, Training Accuracy: 60.248\n",
            "Worker 2, [02/04]: Training Loss: 1.231182065, Training Accuracy: 64.200\n",
            "Worker 2, [03/04]: Training Loss: 1.167472375, Training Accuracy: 65.776\n",
            "Worker 2, [04/04]: Training Loss: 1.100839192, Training Accuracy: 67.680\n",
            "Time taken for training worker 2: 0:00:22.696205\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.353139959, Training Accuracy: 60.752\n",
            "Worker 3, [02/04]: Training Loss: 1.234931510, Training Accuracy: 63.808\n",
            "Worker 3, [03/04]: Training Loss: 1.170730779, Training Accuracy: 66.000\n",
            "Worker 3, [04/04]: Training Loss: 1.078030202, Training Accuracy: 68.424\n",
            "Time taken for training worker 3: 0:00:24.033389\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.407827837, Training Accuracy: 59.440\n",
            "Worker 4, [02/04]: Training Loss: 1.259173286, Training Accuracy: 63.656\n",
            "Worker 4, [03/04]: Training Loss: 1.157841615, Training Accuracy: 66.256\n",
            "Worker 4, [04/04]: Training Loss: 1.114931479, Training Accuracy: 67.368\n",
            "Time taken for training worker 4: 0:00:24.107918\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002815\n",
            "Global Update 21: Test Loss: 1.923347235, Test Accuracy: 52.130\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.296229463, Training Accuracy: 62.600\n",
            "Worker 1, [02/04]: Training Loss: 1.174855794, Training Accuracy: 65.784\n",
            "Worker 1, [03/04]: Training Loss: 1.099607092, Training Accuracy: 67.856\n",
            "Worker 1, [04/04]: Training Loss: 1.050860071, Training Accuracy: 68.592\n",
            "Time taken for training worker 1: 0:00:23.507113\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.325782238, Training Accuracy: 61.896\n",
            "Worker 2, [02/04]: Training Loss: 1.159294058, Training Accuracy: 66.432\n",
            "Worker 2, [03/04]: Training Loss: 1.094450783, Training Accuracy: 67.464\n",
            "Worker 2, [04/04]: Training Loss: 1.025259104, Training Accuracy: 69.768\n",
            "Time taken for training worker 2: 0:00:23.148324\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.311046591, Training Accuracy: 62.320\n",
            "Worker 3, [02/04]: Training Loss: 1.178170669, Training Accuracy: 65.704\n",
            "Worker 3, [03/04]: Training Loss: 1.097091164, Training Accuracy: 68.192\n",
            "Worker 3, [04/04]: Training Loss: 1.053677883, Training Accuracy: 68.944\n",
            "Time taken for training worker 3: 0:00:23.107925\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.328023549, Training Accuracy: 61.640\n",
            "Worker 4, [02/04]: Training Loss: 1.207811735, Training Accuracy: 65.256\n",
            "Worker 4, [03/04]: Training Loss: 1.120080097, Training Accuracy: 66.832\n",
            "Worker 4, [04/04]: Training Loss: 1.082262229, Training Accuracy: 68.448\n",
            "Time taken for training worker 4: 0:00:24.342818\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002687\n",
            "Global Update 22: Test Loss: 1.924444557, Test Accuracy: 52.770\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.251815738, Training Accuracy: 63.888\n",
            "Worker 1, [02/04]: Training Loss: 1.142118167, Training Accuracy: 66.616\n",
            "Worker 1, [03/04]: Training Loss: 1.047252633, Training Accuracy: 68.944\n",
            "Worker 1, [04/04]: Training Loss: 0.998160264, Training Accuracy: 70.560\n",
            "Time taken for training worker 1: 0:00:24.397360\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.253666139, Training Accuracy: 63.640\n",
            "Worker 2, [02/04]: Training Loss: 1.124043926, Training Accuracy: 67.520\n",
            "Worker 2, [03/04]: Training Loss: 1.040004231, Training Accuracy: 69.120\n",
            "Worker 2, [04/04]: Training Loss: 0.982882547, Training Accuracy: 70.888\n",
            "Time taken for training worker 2: 0:00:24.321721\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.244183142, Training Accuracy: 63.728\n",
            "Worker 3, [02/04]: Training Loss: 1.133365232, Training Accuracy: 67.168\n",
            "Worker 3, [03/04]: Training Loss: 1.048340287, Training Accuracy: 68.904\n",
            "Worker 3, [04/04]: Training Loss: 0.973958891, Training Accuracy: 71.216\n",
            "Time taken for training worker 3: 0:00:24.080961\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.265803670, Training Accuracy: 63.152\n",
            "Worker 4, [02/04]: Training Loss: 1.164196262, Training Accuracy: 66.016\n",
            "Worker 4, [03/04]: Training Loss: 1.078869115, Training Accuracy: 68.072\n",
            "Worker 4, [04/04]: Training Loss: 0.998141823, Training Accuracy: 70.384\n",
            "Time taken for training worker 4: 0:00:24.179799\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002756\n",
            "Global Update 23: Test Loss: 1.958589837, Test Accuracy: 52.570\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.205042151, Training Accuracy: 64.768\n",
            "Worker 1, [02/04]: Training Loss: 1.107982361, Training Accuracy: 67.672\n",
            "Worker 1, [03/04]: Training Loss: 0.992561025, Training Accuracy: 70.808\n",
            "Worker 1, [04/04]: Training Loss: 0.935746012, Training Accuracy: 72.280\n",
            "Time taken for training worker 1: 0:00:23.558818\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.185590191, Training Accuracy: 65.160\n",
            "Worker 2, [02/04]: Training Loss: 1.069619259, Training Accuracy: 68.176\n",
            "Worker 2, [03/04]: Training Loss: 0.976920236, Training Accuracy: 71.040\n",
            "Worker 2, [04/04]: Training Loss: 0.941213187, Training Accuracy: 72.104\n",
            "Time taken for training worker 2: 0:00:24.034476\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.205366285, Training Accuracy: 64.848\n",
            "Worker 3, [02/04]: Training Loss: 1.041598134, Training Accuracy: 69.144\n",
            "Worker 3, [03/04]: Training Loss: 0.987714352, Training Accuracy: 71.016\n",
            "Worker 3, [04/04]: Training Loss: 0.942817963, Training Accuracy: 71.704\n",
            "Time taken for training worker 3: 0:00:23.382348\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.215137329, Training Accuracy: 64.232\n",
            "Worker 4, [02/04]: Training Loss: 1.105608610, Training Accuracy: 68.024\n",
            "Worker 4, [03/04]: Training Loss: 1.017662546, Training Accuracy: 70.408\n",
            "Worker 4, [04/04]: Training Loss: 0.968059324, Training Accuracy: 71.880\n",
            "Time taken for training worker 4: 0:00:23.677994\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002712\n",
            "Global Update 24: Test Loss: 1.954029294, Test Accuracy: 52.670\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.137782874, Training Accuracy: 66.168\n",
            "Worker 1, [02/04]: Training Loss: 1.025409265, Training Accuracy: 70.024\n",
            "Worker 1, [03/04]: Training Loss: 0.963769187, Training Accuracy: 71.400\n",
            "Worker 1, [04/04]: Training Loss: 0.890385076, Training Accuracy: 73.240\n",
            "Time taken for training worker 1: 0:00:23.190289\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.144028644, Training Accuracy: 66.624\n",
            "Worker 2, [02/04]: Training Loss: 1.036720386, Training Accuracy: 69.664\n",
            "Worker 2, [03/04]: Training Loss: 0.962655129, Training Accuracy: 71.192\n",
            "Worker 2, [04/04]: Training Loss: 0.880195599, Training Accuracy: 73.792\n",
            "Time taken for training worker 2: 0:00:26.768736\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.137592919, Training Accuracy: 66.352\n",
            "Worker 3, [02/04]: Training Loss: 1.020408227, Training Accuracy: 69.784\n",
            "Worker 3, [03/04]: Training Loss: 0.948360621, Training Accuracy: 72.112\n",
            "Worker 3, [04/04]: Training Loss: 0.884480707, Training Accuracy: 73.680\n",
            "Time taken for training worker 3: 0:00:25.650924\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.185249538, Training Accuracy: 65.016\n",
            "Worker 4, [02/04]: Training Loss: 1.039073533, Training Accuracy: 69.360\n",
            "Worker 4, [03/04]: Training Loss: 0.942130713, Training Accuracy: 72.192\n",
            "Worker 4, [04/04]: Training Loss: 0.906839865, Training Accuracy: 72.816\n",
            "Time taken for training worker 4: 0:00:24.220962\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004056\n",
            "Global Update 25: Test Loss: 1.957063160, Test Accuracy: 52.920\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.098078116, Training Accuracy: 67.600\n",
            "Worker 1, [02/04]: Training Loss: 0.965766023, Training Accuracy: 71.128\n",
            "Worker 1, [03/04]: Training Loss: 0.905771346, Training Accuracy: 73.616\n",
            "Worker 1, [04/04]: Training Loss: 0.841781742, Training Accuracy: 74.832\n",
            "Time taken for training worker 1: 0:00:24.143721\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.094633176, Training Accuracy: 68.016\n",
            "Worker 2, [02/04]: Training Loss: 0.971954802, Training Accuracy: 71.560\n",
            "Worker 2, [03/04]: Training Loss: 0.903699122, Training Accuracy: 73.432\n",
            "Worker 2, [04/04]: Training Loss: 0.818849618, Training Accuracy: 75.968\n",
            "Time taken for training worker 2: 0:00:22.892829\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.093702990, Training Accuracy: 67.504\n",
            "Worker 3, [02/04]: Training Loss: 0.968753362, Training Accuracy: 71.088\n",
            "Worker 3, [03/04]: Training Loss: 0.920538418, Training Accuracy: 72.600\n",
            "Worker 3, [04/04]: Training Loss: 0.848193793, Training Accuracy: 74.840\n",
            "Time taken for training worker 3: 0:00:23.741034\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.116215990, Training Accuracy: 66.808\n",
            "Worker 4, [02/04]: Training Loss: 0.988667140, Training Accuracy: 70.912\n",
            "Worker 4, [03/04]: Training Loss: 0.921023871, Training Accuracy: 72.488\n",
            "Worker 4, [04/04]: Training Loss: 0.874900509, Training Accuracy: 74.064\n",
            "Time taken for training worker 4: 0:00:24.704463\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002787\n",
            "Global Update 26: Test Loss: 1.981694066, Test Accuracy: 53.100\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.039276514, Training Accuracy: 69.088\n",
            "Worker 1, [02/04]: Training Loss: 0.924804568, Training Accuracy: 72.392\n",
            "Worker 1, [03/04]: Training Loss: 0.851745485, Training Accuracy: 74.560\n",
            "Worker 1, [04/04]: Training Loss: 0.801014239, Training Accuracy: 76.136\n",
            "Time taken for training worker 1: 0:00:23.185963\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.052287539, Training Accuracy: 68.656\n",
            "Worker 2, [02/04]: Training Loss: 0.927486953, Training Accuracy: 72.584\n",
            "Worker 2, [03/04]: Training Loss: 0.857787708, Training Accuracy: 74.760\n",
            "Worker 2, [04/04]: Training Loss: 0.803833842, Training Accuracy: 76.216\n",
            "Time taken for training worker 2: 0:00:23.069485\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.050847446, Training Accuracy: 68.944\n",
            "Worker 3, [02/04]: Training Loss: 0.930290807, Training Accuracy: 72.528\n",
            "Worker 3, [03/04]: Training Loss: 0.856354186, Training Accuracy: 74.600\n",
            "Worker 3, [04/04]: Training Loss: 0.788415025, Training Accuracy: 76.584\n",
            "Time taken for training worker 3: 0:00:23.279083\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.054702957, Training Accuracy: 68.776\n",
            "Worker 4, [02/04]: Training Loss: 0.959470115, Training Accuracy: 71.672\n",
            "Worker 4, [03/04]: Training Loss: 0.897553713, Training Accuracy: 73.848\n",
            "Worker 4, [04/04]: Training Loss: 0.823108485, Training Accuracy: 75.552\n",
            "Time taken for training worker 4: 0:00:23.896793\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002816\n",
            "Global Update 27: Test Loss: 1.997530334, Test Accuracy: 53.270\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.979769662, Training Accuracy: 70.976\n",
            "Worker 1, [02/04]: Training Loss: 0.883452922, Training Accuracy: 73.464\n",
            "Worker 1, [03/04]: Training Loss: 0.826510881, Training Accuracy: 75.768\n",
            "Worker 1, [04/04]: Training Loss: 0.775289976, Training Accuracy: 77.096\n",
            "Time taken for training worker 1: 0:00:24.412721\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.006188405, Training Accuracy: 70.064\n",
            "Worker 2, [02/04]: Training Loss: 0.871275717, Training Accuracy: 74.144\n",
            "Worker 2, [03/04]: Training Loss: 0.813687828, Training Accuracy: 75.688\n",
            "Worker 2, [04/04]: Training Loss: 0.759855914, Training Accuracy: 77.248\n",
            "Time taken for training worker 2: 0:00:23.804591\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.003535893, Training Accuracy: 70.136\n",
            "Worker 3, [02/04]: Training Loss: 0.879792007, Training Accuracy: 73.952\n",
            "Worker 3, [03/04]: Training Loss: 0.816553668, Training Accuracy: 75.616\n",
            "Worker 3, [04/04]: Training Loss: 0.776547503, Training Accuracy: 76.888\n",
            "Time taken for training worker 3: 0:00:23.278217\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.017369883, Training Accuracy: 69.624\n",
            "Worker 4, [02/04]: Training Loss: 0.893350542, Training Accuracy: 73.144\n",
            "Worker 4, [03/04]: Training Loss: 0.839333603, Training Accuracy: 75.272\n",
            "Worker 4, [04/04]: Training Loss: 0.792876041, Training Accuracy: 76.504\n",
            "Time taken for training worker 4: 0:00:23.736267\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002913\n",
            "Global Update 28: Test Loss: 2.007110953, Test Accuracy: 53.210\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.943125702, Training Accuracy: 71.880\n",
            "Worker 1, [02/04]: Training Loss: 0.860762780, Training Accuracy: 74.520\n",
            "Worker 1, [03/04]: Training Loss: 0.793460060, Training Accuracy: 76.704\n",
            "Worker 1, [04/04]: Training Loss: 0.737961989, Training Accuracy: 78.440\n",
            "Time taken for training worker 1: 0:00:24.037255\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.951322391, Training Accuracy: 71.216\n",
            "Worker 2, [02/04]: Training Loss: 0.833149054, Training Accuracy: 75.192\n",
            "Worker 2, [03/04]: Training Loss: 0.784660703, Training Accuracy: 76.880\n",
            "Worker 2, [04/04]: Training Loss: 0.737718657, Training Accuracy: 78.280\n",
            "Time taken for training worker 2: 0:00:23.294524\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.969561508, Training Accuracy: 71.320\n",
            "Worker 3, [02/04]: Training Loss: 0.843490469, Training Accuracy: 75.152\n",
            "Worker 3, [03/04]: Training Loss: 0.787301101, Training Accuracy: 76.640\n",
            "Worker 3, [04/04]: Training Loss: 0.733786521, Training Accuracy: 78.320\n",
            "Time taken for training worker 3: 0:00:22.983938\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.962132635, Training Accuracy: 71.424\n",
            "Worker 4, [02/04]: Training Loss: 0.868100300, Training Accuracy: 74.488\n",
            "Worker 4, [03/04]: Training Loss: 0.803858528, Training Accuracy: 76.312\n",
            "Worker 4, [04/04]: Training Loss: 0.754467056, Training Accuracy: 77.608\n",
            "Time taken for training worker 4: 0:00:25.431731\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002990\n",
            "Global Update 29: Test Loss: 2.012511499, Test Accuracy: 53.140\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.910187713, Training Accuracy: 72.784\n",
            "Worker 1, [02/04]: Training Loss: 0.820203183, Training Accuracy: 75.976\n",
            "Worker 1, [03/04]: Training Loss: 0.769368769, Training Accuracy: 77.416\n",
            "Worker 1, [04/04]: Training Loss: 0.719661258, Training Accuracy: 78.952\n",
            "Time taken for training worker 1: 0:00:24.421337\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.920093552, Training Accuracy: 72.720\n",
            "Worker 2, [02/04]: Training Loss: 0.813759307, Training Accuracy: 75.976\n",
            "Worker 2, [03/04]: Training Loss: 0.748139294, Training Accuracy: 78.264\n",
            "Worker 2, [04/04]: Training Loss: 0.707992330, Training Accuracy: 79.208\n",
            "Time taken for training worker 2: 0:00:23.530914\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.900987592, Training Accuracy: 72.464\n",
            "Worker 3, [02/04]: Training Loss: 0.807432508, Training Accuracy: 76.240\n",
            "Worker 3, [03/04]: Training Loss: 0.744426522, Training Accuracy: 78.104\n",
            "Worker 3, [04/04]: Training Loss: 0.702900043, Training Accuracy: 79.408\n",
            "Time taken for training worker 3: 0:00:24.748930\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.923948731, Training Accuracy: 72.544\n",
            "Worker 4, [02/04]: Training Loss: 0.831858867, Training Accuracy: 75.360\n",
            "Worker 4, [03/04]: Training Loss: 0.772503782, Training Accuracy: 77.672\n",
            "Worker 4, [04/04]: Training Loss: 0.716357559, Training Accuracy: 78.944\n",
            "Time taken for training worker 4: 0:00:23.746196\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002653\n",
            "Global Update 30: Test Loss: 2.025719949, Test Accuracy: 53.350\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.890983201, Training Accuracy: 73.656\n",
            "Worker 1, [02/04]: Training Loss: 0.778402660, Training Accuracy: 76.680\n",
            "Worker 1, [03/04]: Training Loss: 0.729290587, Training Accuracy: 78.624\n",
            "Worker 1, [04/04]: Training Loss: 0.692448082, Training Accuracy: 79.736\n",
            "Time taken for training worker 1: 0:00:23.667651\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.874997545, Training Accuracy: 73.872\n",
            "Worker 2, [02/04]: Training Loss: 0.794463120, Training Accuracy: 76.360\n",
            "Worker 2, [03/04]: Training Loss: 0.734780303, Training Accuracy: 78.224\n",
            "Worker 2, [04/04]: Training Loss: 0.688632990, Training Accuracy: 79.760\n",
            "Time taken for training worker 2: 0:00:24.626628\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.858728381, Training Accuracy: 74.248\n",
            "Worker 3, [02/04]: Training Loss: 0.790486524, Training Accuracy: 76.544\n",
            "Worker 3, [03/04]: Training Loss: 0.728470448, Training Accuracy: 78.656\n",
            "Worker 3, [04/04]: Training Loss: 0.701093947, Training Accuracy: 79.528\n",
            "Time taken for training worker 3: 0:00:23.241624\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.889505567, Training Accuracy: 73.480\n",
            "Worker 4, [02/04]: Training Loss: 0.801719898, Training Accuracy: 76.432\n",
            "Worker 4, [03/04]: Training Loss: 0.760588500, Training Accuracy: 77.432\n",
            "Worker 4, [04/04]: Training Loss: 0.714444450, Training Accuracy: 78.936\n",
            "Time taken for training worker 4: 0:00:24.107400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002770\n",
            "Global Update 31: Test Loss: 2.041311168, Test Accuracy: 53.390\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.841956100, Training Accuracy: 74.744\n",
            "Worker 1, [02/04]: Training Loss: 0.769453051, Training Accuracy: 76.992\n",
            "Worker 1, [03/04]: Training Loss: 0.714397489, Training Accuracy: 78.848\n",
            "Worker 1, [04/04]: Training Loss: 0.675568876, Training Accuracy: 79.800\n",
            "Time taken for training worker 1: 0:00:23.173772\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.844751365, Training Accuracy: 74.424\n",
            "Worker 2, [02/04]: Training Loss: 0.757720049, Training Accuracy: 77.992\n",
            "Worker 2, [03/04]: Training Loss: 0.716993831, Training Accuracy: 78.568\n",
            "Worker 2, [04/04]: Training Loss: 0.682892591, Training Accuracy: 79.784\n",
            "Time taken for training worker 2: 0:00:23.737406\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.842170295, Training Accuracy: 75.088\n",
            "Worker 3, [02/04]: Training Loss: 0.764010907, Training Accuracy: 77.456\n",
            "Worker 3, [03/04]: Training Loss: 0.714076965, Training Accuracy: 78.936\n",
            "Worker 3, [04/04]: Training Loss: 0.672808326, Training Accuracy: 80.200\n",
            "Time taken for training worker 3: 0:00:24.210983\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.864102680, Training Accuracy: 74.144\n",
            "Worker 4, [02/04]: Training Loss: 0.775836599, Training Accuracy: 77.024\n",
            "Worker 4, [03/04]: Training Loss: 0.734479772, Training Accuracy: 78.504\n",
            "Worker 4, [04/04]: Training Loss: 0.705760568, Training Accuracy: 79.488\n",
            "Time taken for training worker 4: 0:00:23.733512\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002754\n",
            "Global Update 32: Test Loss: 2.044501897, Test Accuracy: 53.270\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.806514620, Training Accuracy: 75.800\n",
            "Worker 1, [02/04]: Training Loss: 0.750078852, Training Accuracy: 78.120\n",
            "Worker 1, [03/04]: Training Loss: 0.714091123, Training Accuracy: 78.768\n",
            "Worker 1, [04/04]: Training Loss: 0.675434210, Training Accuracy: 79.992\n",
            "Time taken for training worker 1: 0:00:23.797539\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.825538980, Training Accuracy: 75.496\n",
            "Worker 2, [02/04]: Training Loss: 0.760305185, Training Accuracy: 77.688\n",
            "Worker 2, [03/04]: Training Loss: 0.710889909, Training Accuracy: 78.656\n",
            "Worker 2, [04/04]: Training Loss: 0.679573423, Training Accuracy: 80.088\n",
            "Time taken for training worker 2: 0:00:23.041017\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.814428247, Training Accuracy: 75.840\n",
            "Worker 3, [02/04]: Training Loss: 0.747832959, Training Accuracy: 78.432\n",
            "Worker 3, [03/04]: Training Loss: 0.707893385, Training Accuracy: 79.408\n",
            "Worker 3, [04/04]: Training Loss: 0.680415201, Training Accuracy: 80.328\n",
            "Time taken for training worker 3: 0:00:24.464509\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.829461107, Training Accuracy: 74.648\n",
            "Worker 4, [02/04]: Training Loss: 0.768683412, Training Accuracy: 77.360\n",
            "Worker 4, [03/04]: Training Loss: 0.730723357, Training Accuracy: 78.864\n",
            "Worker 4, [04/04]: Training Loss: 0.686746596, Training Accuracy: 79.896\n",
            "Time taken for training worker 4: 0:00:24.080211\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002688\n",
            "Global Update 33: Test Loss: 2.047413269, Test Accuracy: 53.070\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.789824454, Training Accuracy: 76.424\n",
            "Worker 1, [02/04]: Training Loss: 0.752474710, Training Accuracy: 77.712\n",
            "Worker 1, [03/04]: Training Loss: 0.715547609, Training Accuracy: 78.744\n",
            "Worker 1, [04/04]: Training Loss: 0.682506255, Training Accuracy: 79.936\n",
            "Time taken for training worker 1: 0:00:24.208135\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.795529299, Training Accuracy: 76.328\n",
            "Worker 2, [02/04]: Training Loss: 0.750777169, Training Accuracy: 78.088\n",
            "Worker 2, [03/04]: Training Loss: 0.704998734, Training Accuracy: 79.344\n",
            "Worker 2, [04/04]: Training Loss: 0.687316010, Training Accuracy: 80.080\n",
            "Time taken for training worker 2: 0:00:23.738891\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.779642270, Training Accuracy: 76.880\n",
            "Worker 3, [02/04]: Training Loss: 0.736660357, Training Accuracy: 78.120\n",
            "Worker 3, [03/04]: Training Loss: 0.711631455, Training Accuracy: 79.184\n",
            "Worker 3, [04/04]: Training Loss: 0.685488784, Training Accuracy: 79.856\n",
            "Time taken for training worker 3: 0:00:23.632084\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.826984031, Training Accuracy: 75.760\n",
            "Worker 4, [02/04]: Training Loss: 0.765615551, Training Accuracy: 77.416\n",
            "Worker 4, [03/04]: Training Loss: 0.730714665, Training Accuracy: 78.616\n",
            "Worker 4, [04/04]: Training Loss: 0.695973206, Training Accuracy: 79.296\n",
            "Time taken for training worker 4: 0:00:23.067832\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002706\n",
            "Global Update 34: Test Loss: 2.045266377, Test Accuracy: 53.210\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.782302569, Training Accuracy: 76.632\n",
            "Worker 1, [02/04]: Training Loss: 0.733473404, Training Accuracy: 78.168\n",
            "Worker 1, [03/04]: Training Loss: 0.715788711, Training Accuracy: 79.080\n",
            "Worker 1, [04/04]: Training Loss: 0.701333601, Training Accuracy: 79.176\n",
            "Time taken for training worker 1: 0:00:23.822675\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.783078007, Training Accuracy: 76.160\n",
            "Worker 2, [02/04]: Training Loss: 0.737937500, Training Accuracy: 78.024\n",
            "Worker 2, [03/04]: Training Loss: 0.715797837, Training Accuracy: 78.672\n",
            "Worker 2, [04/04]: Training Loss: 0.703420478, Training Accuracy: 79.344\n",
            "Time taken for training worker 2: 0:00:25.182632\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.765816892, Training Accuracy: 77.008\n",
            "Worker 3, [02/04]: Training Loss: 0.738495297, Training Accuracy: 78.008\n",
            "Worker 3, [03/04]: Training Loss: 0.712609242, Training Accuracy: 78.864\n",
            "Worker 3, [04/04]: Training Loss: 0.696936133, Training Accuracy: 79.520\n",
            "Time taken for training worker 3: 0:00:24.804943\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.787904560, Training Accuracy: 76.800\n",
            "Worker 4, [02/04]: Training Loss: 0.759366334, Training Accuracy: 77.392\n",
            "Worker 4, [03/04]: Training Loss: 0.733471225, Training Accuracy: 78.472\n",
            "Worker 4, [04/04]: Training Loss: 0.716137901, Training Accuracy: 79.344\n",
            "Time taken for training worker 4: 0:00:23.387478\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002765\n",
            "Global Update 35: Test Loss: 2.042450649, Test Accuracy: 53.370\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.779047377, Training Accuracy: 76.576\n",
            "Worker 1, [02/04]: Training Loss: 0.749891728, Training Accuracy: 77.704\n",
            "Worker 1, [03/04]: Training Loss: 0.752276730, Training Accuracy: 77.360\n",
            "Worker 1, [04/04]: Training Loss: 0.710243961, Training Accuracy: 79.328\n",
            "Time taken for training worker 1: 0:00:24.293374\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.770214025, Training Accuracy: 76.888\n",
            "Worker 2, [02/04]: Training Loss: 0.760268468, Training Accuracy: 77.176\n",
            "Worker 2, [03/04]: Training Loss: 0.741280524, Training Accuracy: 78.200\n",
            "Worker 2, [04/04]: Training Loss: 0.724930594, Training Accuracy: 78.688\n",
            "Time taken for training worker 2: 0:00:24.473873\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.776036801, Training Accuracy: 76.976\n",
            "Worker 3, [02/04]: Training Loss: 0.756685367, Training Accuracy: 77.304\n",
            "Worker 3, [03/04]: Training Loss: 0.728201840, Training Accuracy: 78.560\n",
            "Worker 3, [04/04]: Training Loss: 0.721654306, Training Accuracy: 78.792\n",
            "Time taken for training worker 3: 0:00:23.391799\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.773184324, Training Accuracy: 77.176\n",
            "Worker 4, [02/04]: Training Loss: 0.770358016, Training Accuracy: 77.248\n",
            "Worker 4, [03/04]: Training Loss: 0.756299998, Training Accuracy: 77.816\n",
            "Worker 4, [04/04]: Training Loss: 0.742689414, Training Accuracy: 78.128\n",
            "Time taken for training worker 4: 0:00:23.400284\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002825\n",
            "Global Update 36: Test Loss: 2.037922569, Test Accuracy: 53.490\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.777338371, Training Accuracy: 76.400\n",
            "Worker 1, [02/04]: Training Loss: 0.770355742, Training Accuracy: 77.088\n",
            "Worker 1, [03/04]: Training Loss: 0.755827069, Training Accuracy: 77.368\n",
            "Worker 1, [04/04]: Training Loss: 0.749247462, Training Accuracy: 77.712\n",
            "Time taken for training worker 1: 0:00:23.260692\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.787196034, Training Accuracy: 76.520\n",
            "Worker 2, [02/04]: Training Loss: 0.759122457, Training Accuracy: 77.104\n",
            "Worker 2, [03/04]: Training Loss: 0.767475786, Training Accuracy: 76.808\n",
            "Worker 2, [04/04]: Training Loss: 0.758121041, Training Accuracy: 77.776\n",
            "Time taken for training worker 2: 0:00:23.667996\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.764283545, Training Accuracy: 76.864\n",
            "Worker 3, [02/04]: Training Loss: 0.759098878, Training Accuracy: 77.272\n",
            "Worker 3, [03/04]: Training Loss: 0.750911631, Training Accuracy: 77.704\n",
            "Worker 3, [04/04]: Training Loss: 0.755554128, Training Accuracy: 77.728\n",
            "Time taken for training worker 3: 0:00:23.617296\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.786175550, Training Accuracy: 76.712\n",
            "Worker 4, [02/04]: Training Loss: 0.779537572, Training Accuracy: 76.624\n",
            "Worker 4, [03/04]: Training Loss: 0.784835255, Training Accuracy: 76.608\n",
            "Worker 4, [04/04]: Training Loss: 0.761647977, Training Accuracy: 77.464\n",
            "Time taken for training worker 4: 0:00:23.020496\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002765\n",
            "Global Update 37: Test Loss: 2.035068179, Test Accuracy: 53.420\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:59:23.626823\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:8\n",
            "==================================================\n",
            "Worker 1, [01/08]: Training Loss: 4.506814485, Training Accuracy: 2.616\n",
            "Worker 1, [02/08]: Training Loss: 4.128333266, Training Accuracy: 6.448\n",
            "Worker 1, [03/08]: Training Loss: 3.911654553, Training Accuracy: 9.288\n",
            "Worker 1, [04/08]: Training Loss: 3.763191217, Training Accuracy: 11.808\n",
            "Worker 1, [05/08]: Training Loss: 3.626793979, Training Accuracy: 13.512\n",
            "Worker 1, [06/08]: Training Loss: 3.486398231, Training Accuracy: 15.808\n",
            "Worker 1, [07/08]: Training Loss: 3.380997351, Training Accuracy: 17.432\n",
            "Worker 1, [08/08]: Training Loss: 3.272324844, Training Accuracy: 18.864\n",
            "Time taken for training worker 1: 0:00:48.224757\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 4.523780003, Training Accuracy: 2.040\n",
            "Worker 2, [02/08]: Training Loss: 4.149567752, Training Accuracy: 5.920\n",
            "Worker 2, [03/08]: Training Loss: 3.946294865, Training Accuracy: 8.696\n",
            "Worker 2, [04/08]: Training Loss: 3.787770564, Training Accuracy: 11.280\n",
            "Worker 2, [05/08]: Training Loss: 3.663095495, Training Accuracy: 12.960\n",
            "Worker 2, [06/08]: Training Loss: 3.548093431, Training Accuracy: 15.376\n",
            "Worker 2, [07/08]: Training Loss: 3.420888031, Training Accuracy: 16.944\n",
            "Worker 2, [08/08]: Training Loss: 3.313168953, Training Accuracy: 18.640\n",
            "Time taken for training worker 2: 0:00:49.299328\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 4.494056814, Training Accuracy: 2.768\n",
            "Worker 3, [02/08]: Training Loss: 4.112356940, Training Accuracy: 6.384\n",
            "Worker 3, [03/08]: Training Loss: 3.907486125, Training Accuracy: 9.208\n",
            "Worker 3, [04/08]: Training Loss: 3.758911602, Training Accuracy: 11.840\n",
            "Worker 3, [05/08]: Training Loss: 3.610646817, Training Accuracy: 13.800\n",
            "Worker 3, [06/08]: Training Loss: 3.491013124, Training Accuracy: 15.936\n",
            "Worker 3, [07/08]: Training Loss: 3.408713439, Training Accuracy: 17.288\n",
            "Worker 3, [08/08]: Training Loss: 3.282718671, Training Accuracy: 19.672\n",
            "Time taken for training worker 3: 0:00:47.542157\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 4.516010501, Training Accuracy: 2.128\n",
            "Worker 4, [02/08]: Training Loss: 4.151936884, Training Accuracy: 6.240\n",
            "Worker 4, [03/08]: Training Loss: 3.941152803, Training Accuracy: 9.176\n",
            "Worker 4, [04/08]: Training Loss: 3.798557603, Training Accuracy: 10.920\n",
            "Worker 4, [05/08]: Training Loss: 3.651759168, Training Accuracy: 13.544\n",
            "Worker 4, [06/08]: Training Loss: 3.516759782, Training Accuracy: 15.672\n",
            "Worker 4, [07/08]: Training Loss: 3.417406721, Training Accuracy: 17.128\n",
            "Worker 4, [08/08]: Training Loss: 3.295923006, Training Accuracy: 19.544\n",
            "Time taken for training worker 4: 0:00:47.589399\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002722\n",
            "Global Update 01: Test Loss: 3.441592499, Test Accuracy: 22.120\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 3.361782489, Training Accuracy: 17.632\n",
            "Worker 1, [02/08]: Training Loss: 3.217561088, Training Accuracy: 20.976\n",
            "Worker 1, [03/08]: Training Loss: 3.105512361, Training Accuracy: 23.112\n",
            "Worker 1, [04/08]: Training Loss: 2.988452059, Training Accuracy: 24.664\n",
            "Worker 1, [05/08]: Training Loss: 2.938881125, Training Accuracy: 25.808\n",
            "Worker 1, [06/08]: Training Loss: 2.824502509, Training Accuracy: 28.136\n",
            "Worker 1, [07/08]: Training Loss: 2.748914328, Training Accuracy: 28.856\n",
            "Worker 1, [08/08]: Training Loss: 2.697429819, Training Accuracy: 30.344\n",
            "Time taken for training worker 1: 0:00:46.599314\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 3.360742078, Training Accuracy: 18.384\n",
            "Worker 2, [02/08]: Training Loss: 3.237645535, Training Accuracy: 20.704\n",
            "Worker 2, [03/08]: Training Loss: 3.124072290, Training Accuracy: 23.016\n",
            "Worker 2, [04/08]: Training Loss: 3.023968388, Training Accuracy: 24.488\n",
            "Worker 2, [05/08]: Training Loss: 2.938535248, Training Accuracy: 26.080\n",
            "Worker 2, [06/08]: Training Loss: 2.867012042, Training Accuracy: 27.376\n",
            "Worker 2, [07/08]: Training Loss: 2.812906689, Training Accuracy: 28.664\n",
            "Worker 2, [08/08]: Training Loss: 2.736332778, Training Accuracy: 30.112\n",
            "Time taken for training worker 2: 0:00:48.100032\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 3.355510184, Training Accuracy: 17.920\n",
            "Worker 3, [02/08]: Training Loss: 3.201315768, Training Accuracy: 21.120\n",
            "Worker 3, [03/08]: Training Loss: 3.122860406, Training Accuracy: 22.336\n",
            "Worker 3, [04/08]: Training Loss: 3.016773670, Training Accuracy: 24.424\n",
            "Worker 3, [05/08]: Training Loss: 2.931631271, Training Accuracy: 25.256\n",
            "Worker 3, [06/08]: Training Loss: 2.847871082, Training Accuracy: 27.392\n",
            "Worker 3, [07/08]: Training Loss: 2.757302208, Training Accuracy: 29.528\n",
            "Worker 3, [08/08]: Training Loss: 2.717469495, Training Accuracy: 30.160\n",
            "Time taken for training worker 3: 0:00:47.915138\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 3.374296224, Training Accuracy: 18.224\n",
            "Worker 4, [02/08]: Training Loss: 3.237646233, Training Accuracy: 20.224\n",
            "Worker 4, [03/08]: Training Loss: 3.123673531, Training Accuracy: 22.984\n",
            "Worker 4, [04/08]: Training Loss: 3.026578240, Training Accuracy: 24.192\n",
            "Worker 4, [05/08]: Training Loss: 2.925045832, Training Accuracy: 26.552\n",
            "Worker 4, [06/08]: Training Loss: 2.864462309, Training Accuracy: 27.272\n",
            "Worker 4, [07/08]: Training Loss: 2.782377089, Training Accuracy: 28.928\n",
            "Worker 4, [08/08]: Training Loss: 2.721504014, Training Accuracy: 29.704\n",
            "Time taken for training worker 4: 0:00:48.102668\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002777\n",
            "Global Update 02: Test Loss: 3.360292058, Test Accuracy: 31.310\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.832522469, Training Accuracy: 28.736\n",
            "Worker 1, [02/08]: Training Loss: 2.699827965, Training Accuracy: 30.672\n",
            "Worker 1, [03/08]: Training Loss: 2.603385955, Training Accuracy: 32.552\n",
            "Worker 1, [04/08]: Training Loss: 2.539712046, Training Accuracy: 33.408\n",
            "Worker 1, [05/08]: Training Loss: 2.468421099, Training Accuracy: 35.144\n",
            "Worker 1, [06/08]: Training Loss: 2.423451508, Training Accuracy: 36.032\n",
            "Worker 1, [07/08]: Training Loss: 2.362377832, Training Accuracy: 37.488\n",
            "Worker 1, [08/08]: Training Loss: 2.315974152, Training Accuracy: 38.408\n",
            "Time taken for training worker 1: 0:00:50.363101\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.859317374, Training Accuracy: 28.296\n",
            "Worker 2, [02/08]: Training Loss: 2.692487522, Training Accuracy: 31.232\n",
            "Worker 2, [03/08]: Training Loss: 2.619804607, Training Accuracy: 32.168\n",
            "Worker 2, [04/08]: Training Loss: 2.546921609, Training Accuracy: 33.784\n",
            "Worker 2, [05/08]: Training Loss: 2.481487590, Training Accuracy: 35.256\n",
            "Worker 2, [06/08]: Training Loss: 2.424876151, Training Accuracy: 36.072\n",
            "Worker 2, [07/08]: Training Loss: 2.357974742, Training Accuracy: 37.160\n",
            "Worker 2, [08/08]: Training Loss: 2.314265470, Training Accuracy: 38.128\n",
            "Time taken for training worker 2: 0:00:47.038615\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.873192224, Training Accuracy: 27.904\n",
            "Worker 3, [02/08]: Training Loss: 2.699574166, Training Accuracy: 30.616\n",
            "Worker 3, [03/08]: Training Loss: 2.617778004, Training Accuracy: 31.872\n",
            "Worker 3, [04/08]: Training Loss: 2.563038962, Training Accuracy: 33.304\n",
            "Worker 3, [05/08]: Training Loss: 2.482169153, Training Accuracy: 34.832\n",
            "Worker 3, [06/08]: Training Loss: 2.397051674, Training Accuracy: 36.472\n",
            "Worker 3, [07/08]: Training Loss: 2.380573174, Training Accuracy: 36.928\n",
            "Worker 3, [08/08]: Training Loss: 2.309069751, Training Accuracy: 38.232\n",
            "Time taken for training worker 3: 0:00:46.737770\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.886385431, Training Accuracy: 28.216\n",
            "Worker 4, [02/08]: Training Loss: 2.720269453, Training Accuracy: 30.440\n",
            "Worker 4, [03/08]: Training Loss: 2.627129391, Training Accuracy: 32.224\n",
            "Worker 4, [04/08]: Training Loss: 2.577834287, Training Accuracy: 32.984\n",
            "Worker 4, [05/08]: Training Loss: 2.513843681, Training Accuracy: 34.032\n",
            "Worker 4, [06/08]: Training Loss: 2.443665951, Training Accuracy: 35.464\n",
            "Worker 4, [07/08]: Training Loss: 2.374638253, Training Accuracy: 36.792\n",
            "Worker 4, [08/08]: Training Loss: 2.346143203, Training Accuracy: 37.912\n",
            "Time taken for training worker 4: 0:00:48.076338\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002829\n",
            "Global Update 03: Test Loss: 2.937053199, Test Accuracy: 39.470\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.553237893, Training Accuracy: 34.976\n",
            "Worker 1, [02/08]: Training Loss: 2.385738662, Training Accuracy: 37.488\n",
            "Worker 1, [03/08]: Training Loss: 2.291242215, Training Accuracy: 39.600\n",
            "Worker 1, [04/08]: Training Loss: 2.225310553, Training Accuracy: 40.624\n",
            "Worker 1, [05/08]: Training Loss: 2.167520134, Training Accuracy: 41.840\n",
            "Worker 1, [06/08]: Training Loss: 2.112002321, Training Accuracy: 42.808\n",
            "Worker 1, [07/08]: Training Loss: 2.090505866, Training Accuracy: 43.120\n",
            "Worker 1, [08/08]: Training Loss: 2.011946690, Training Accuracy: 45.216\n",
            "Time taken for training worker 1: 0:00:48.682865\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.568028452, Training Accuracy: 34.160\n",
            "Worker 2, [02/08]: Training Loss: 2.387630150, Training Accuracy: 37.192\n",
            "Worker 2, [03/08]: Training Loss: 2.322705722, Training Accuracy: 38.648\n",
            "Worker 2, [04/08]: Training Loss: 2.241716018, Training Accuracy: 40.336\n",
            "Worker 2, [05/08]: Training Loss: 2.179473367, Training Accuracy: 40.904\n",
            "Worker 2, [06/08]: Training Loss: 2.104519748, Training Accuracy: 43.088\n",
            "Worker 2, [07/08]: Training Loss: 2.062923366, Training Accuracy: 44.200\n",
            "Worker 2, [08/08]: Training Loss: 2.046907906, Training Accuracy: 44.072\n",
            "Time taken for training worker 2: 0:00:48.114344\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.557817244, Training Accuracy: 34.480\n",
            "Worker 3, [02/08]: Training Loss: 2.399353541, Training Accuracy: 36.720\n",
            "Worker 3, [03/08]: Training Loss: 2.304478517, Training Accuracy: 39.064\n",
            "Worker 3, [04/08]: Training Loss: 2.219154954, Training Accuracy: 40.120\n",
            "Worker 3, [05/08]: Training Loss: 2.183654426, Training Accuracy: 41.368\n",
            "Worker 3, [06/08]: Training Loss: 2.116281830, Training Accuracy: 42.200\n",
            "Worker 3, [07/08]: Training Loss: 2.083695407, Training Accuracy: 43.632\n",
            "Worker 3, [08/08]: Training Loss: 2.012868671, Training Accuracy: 44.408\n",
            "Time taken for training worker 3: 0:00:47.304291\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.587263914, Training Accuracy: 34.640\n",
            "Worker 4, [02/08]: Training Loss: 2.412140452, Training Accuracy: 36.928\n",
            "Worker 4, [03/08]: Training Loss: 2.314052820, Training Accuracy: 38.488\n",
            "Worker 4, [04/08]: Training Loss: 2.261344428, Training Accuracy: 39.928\n",
            "Worker 4, [05/08]: Training Loss: 2.213672264, Training Accuracy: 40.368\n",
            "Worker 4, [06/08]: Training Loss: 2.142168737, Training Accuracy: 42.128\n",
            "Worker 4, [07/08]: Training Loss: 2.075983868, Training Accuracy: 44.000\n",
            "Worker 4, [08/08]: Training Loss: 2.073336098, Training Accuracy: 44.080\n",
            "Time taken for training worker 4: 0:00:48.069896\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003021\n",
            "Global Update 04: Test Loss: 2.292192144, Test Accuracy: 42.780\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.275140298, Training Accuracy: 40.256\n",
            "Worker 1, [02/08]: Training Loss: 2.158841466, Training Accuracy: 42.576\n",
            "Worker 1, [03/08]: Training Loss: 2.056628618, Training Accuracy: 44.768\n",
            "Worker 1, [04/08]: Training Loss: 1.989525321, Training Accuracy: 45.832\n",
            "Worker 1, [05/08]: Training Loss: 1.922212530, Training Accuracy: 47.680\n",
            "Worker 1, [06/08]: Training Loss: 1.896607505, Training Accuracy: 47.888\n",
            "Worker 1, [07/08]: Training Loss: 1.831123501, Training Accuracy: 49.160\n",
            "Worker 1, [08/08]: Training Loss: 1.807156610, Training Accuracy: 49.224\n",
            "Time taken for training worker 1: 0:00:48.042185\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.280165678, Training Accuracy: 40.184\n",
            "Worker 2, [02/08]: Training Loss: 2.132426294, Training Accuracy: 42.888\n",
            "Worker 2, [03/08]: Training Loss: 2.066524070, Training Accuracy: 43.896\n",
            "Worker 2, [04/08]: Training Loss: 2.002684159, Training Accuracy: 45.736\n",
            "Worker 2, [05/08]: Training Loss: 1.916952042, Training Accuracy: 47.624\n",
            "Worker 2, [06/08]: Training Loss: 1.889753102, Training Accuracy: 47.832\n",
            "Worker 2, [07/08]: Training Loss: 1.863592325, Training Accuracy: 48.104\n",
            "Worker 2, [08/08]: Training Loss: 1.797384894, Training Accuracy: 49.608\n",
            "Time taken for training worker 2: 0:00:48.635470\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.279936699, Training Accuracy: 39.976\n",
            "Worker 3, [02/08]: Training Loss: 2.158288171, Training Accuracy: 42.216\n",
            "Worker 3, [03/08]: Training Loss: 2.072211532, Training Accuracy: 43.568\n",
            "Worker 3, [04/08]: Training Loss: 1.988356855, Training Accuracy: 45.592\n",
            "Worker 3, [05/08]: Training Loss: 1.948323783, Training Accuracy: 46.880\n",
            "Worker 3, [06/08]: Training Loss: 1.886214382, Training Accuracy: 48.008\n",
            "Worker 3, [07/08]: Training Loss: 1.835820069, Training Accuracy: 49.384\n",
            "Worker 3, [08/08]: Training Loss: 1.800261946, Training Accuracy: 49.720\n",
            "Time taken for training worker 3: 0:00:47.899698\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.286708115, Training Accuracy: 40.176\n",
            "Worker 4, [02/08]: Training Loss: 2.154369340, Training Accuracy: 42.616\n",
            "Worker 4, [03/08]: Training Loss: 2.072301674, Training Accuracy: 44.576\n",
            "Worker 4, [04/08]: Training Loss: 2.011824152, Training Accuracy: 45.072\n",
            "Worker 4, [05/08]: Training Loss: 1.963161855, Training Accuracy: 46.488\n",
            "Worker 4, [06/08]: Training Loss: 1.908389368, Training Accuracy: 47.696\n",
            "Worker 4, [07/08]: Training Loss: 1.886874039, Training Accuracy: 47.856\n",
            "Worker 4, [08/08]: Training Loss: 1.826133215, Training Accuracy: 49.336\n",
            "Time taken for training worker 4: 0:00:47.496340\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002985\n",
            "Global Update 05: Test Loss: 2.111277289, Test Accuracy: 45.270\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.063313074, Training Accuracy: 44.944\n",
            "Worker 1, [02/08]: Training Loss: 1.932481600, Training Accuracy: 47.056\n",
            "Worker 1, [03/08]: Training Loss: 1.835241343, Training Accuracy: 49.752\n",
            "Worker 1, [04/08]: Training Loss: 1.792136541, Training Accuracy: 50.520\n",
            "Worker 1, [05/08]: Training Loss: 1.736313855, Training Accuracy: 51.544\n",
            "Worker 1, [06/08]: Training Loss: 1.693708790, Training Accuracy: 52.512\n",
            "Worker 1, [07/08]: Training Loss: 1.688517328, Training Accuracy: 52.336\n",
            "Worker 1, [08/08]: Training Loss: 1.611411708, Training Accuracy: 54.736\n",
            "Time taken for training worker 1: 0:00:47.362839\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.076728277, Training Accuracy: 44.400\n",
            "Worker 2, [02/08]: Training Loss: 1.936588371, Training Accuracy: 46.776\n",
            "Worker 2, [03/08]: Training Loss: 1.859443379, Training Accuracy: 48.976\n",
            "Worker 2, [04/08]: Training Loss: 1.791951835, Training Accuracy: 50.136\n",
            "Worker 2, [05/08]: Training Loss: 1.741160348, Training Accuracy: 51.208\n",
            "Worker 2, [06/08]: Training Loss: 1.672637077, Training Accuracy: 53.168\n",
            "Worker 2, [07/08]: Training Loss: 1.654972344, Training Accuracy: 53.664\n",
            "Worker 2, [08/08]: Training Loss: 1.619753040, Training Accuracy: 53.920\n",
            "Time taken for training worker 2: 0:00:49.450713\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.078827298, Training Accuracy: 44.040\n",
            "Worker 3, [02/08]: Training Loss: 1.960957739, Training Accuracy: 46.752\n",
            "Worker 3, [03/08]: Training Loss: 1.843951177, Training Accuracy: 49.184\n",
            "Worker 3, [04/08]: Training Loss: 1.794024360, Training Accuracy: 50.344\n",
            "Worker 3, [05/08]: Training Loss: 1.736829699, Training Accuracy: 51.536\n",
            "Worker 3, [06/08]: Training Loss: 1.707130913, Training Accuracy: 51.840\n",
            "Worker 3, [07/08]: Training Loss: 1.647555846, Training Accuracy: 53.552\n",
            "Worker 3, [08/08]: Training Loss: 1.615279740, Training Accuracy: 54.664\n",
            "Time taken for training worker 3: 0:00:48.014710\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.085058136, Training Accuracy: 43.920\n",
            "Worker 4, [02/08]: Training Loss: 1.968245852, Training Accuracy: 47.248\n",
            "Worker 4, [03/08]: Training Loss: 1.899453763, Training Accuracy: 48.000\n",
            "Worker 4, [04/08]: Training Loss: 1.795204852, Training Accuracy: 50.512\n",
            "Worker 4, [05/08]: Training Loss: 1.765643570, Training Accuracy: 50.928\n",
            "Worker 4, [06/08]: Training Loss: 1.725246468, Training Accuracy: 52.024\n",
            "Worker 4, [07/08]: Training Loss: 1.681185903, Training Accuracy: 52.936\n",
            "Worker 4, [08/08]: Training Loss: 1.638885176, Training Accuracy: 54.016\n",
            "Time taken for training worker 4: 0:00:48.432397\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002786\n",
            "Global Update 06: Test Loss: 2.033866616, Test Accuracy: 46.780\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.905161628, Training Accuracy: 48.000\n",
            "Worker 1, [02/08]: Training Loss: 1.754055178, Training Accuracy: 51.640\n",
            "Worker 1, [03/08]: Training Loss: 1.669088621, Training Accuracy: 53.456\n",
            "Worker 1, [04/08]: Training Loss: 1.629734461, Training Accuracy: 54.848\n",
            "Worker 1, [05/08]: Training Loss: 1.567375388, Training Accuracy: 55.472\n",
            "Worker 1, [06/08]: Training Loss: 1.554255137, Training Accuracy: 55.984\n",
            "Worker 1, [07/08]: Training Loss: 1.471130061, Training Accuracy: 58.728\n",
            "Worker 1, [08/08]: Training Loss: 1.456165888, Training Accuracy: 57.992\n",
            "Time taken for training worker 1: 0:00:48.386738\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.891394949, Training Accuracy: 48.032\n",
            "Worker 2, [02/08]: Training Loss: 1.815428413, Training Accuracy: 49.528\n",
            "Worker 2, [03/08]: Training Loss: 1.678970074, Training Accuracy: 52.856\n",
            "Worker 2, [04/08]: Training Loss: 1.616695170, Training Accuracy: 54.344\n",
            "Worker 2, [05/08]: Training Loss: 1.570561657, Training Accuracy: 55.464\n",
            "Worker 2, [06/08]: Training Loss: 1.510922797, Training Accuracy: 56.720\n",
            "Worker 2, [07/08]: Training Loss: 1.499030567, Training Accuracy: 57.616\n",
            "Worker 2, [08/08]: Training Loss: 1.452094562, Training Accuracy: 58.848\n",
            "Time taken for training worker 2: 0:00:47.627944\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.912215806, Training Accuracy: 48.592\n",
            "Worker 3, [02/08]: Training Loss: 1.785426776, Training Accuracy: 50.248\n",
            "Worker 3, [03/08]: Training Loss: 1.685257865, Training Accuracy: 53.192\n",
            "Worker 3, [04/08]: Training Loss: 1.622089359, Training Accuracy: 54.352\n",
            "Worker 3, [05/08]: Training Loss: 1.573977849, Training Accuracy: 55.152\n",
            "Worker 3, [06/08]: Training Loss: 1.538203065, Training Accuracy: 55.912\n",
            "Worker 3, [07/08]: Training Loss: 1.499908147, Training Accuracy: 57.224\n",
            "Worker 3, [08/08]: Training Loss: 1.460161328, Training Accuracy: 58.120\n",
            "Time taken for training worker 3: 0:00:48.476764\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.934889022, Training Accuracy: 47.592\n",
            "Worker 4, [02/08]: Training Loss: 1.810763000, Training Accuracy: 50.104\n",
            "Worker 4, [03/08]: Training Loss: 1.731714916, Training Accuracy: 52.328\n",
            "Worker 4, [04/08]: Training Loss: 1.661018499, Training Accuracy: 53.472\n",
            "Worker 4, [05/08]: Training Loss: 1.606686660, Training Accuracy: 54.816\n",
            "Worker 4, [06/08]: Training Loss: 1.538260744, Training Accuracy: 56.024\n",
            "Worker 4, [07/08]: Training Loss: 1.512856281, Training Accuracy: 57.464\n",
            "Worker 4, [08/08]: Training Loss: 1.470256347, Training Accuracy: 57.920\n",
            "Time taken for training worker 4: 0:00:47.368495\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002927\n",
            "Global Update 07: Test Loss: 2.055331183, Test Accuracy: 47.940\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.788310482, Training Accuracy: 50.816\n",
            "Worker 1, [02/08]: Training Loss: 1.663630505, Training Accuracy: 53.592\n",
            "Worker 1, [03/08]: Training Loss: 1.541671211, Training Accuracy: 56.040\n",
            "Worker 1, [04/08]: Training Loss: 1.477056322, Training Accuracy: 58.216\n",
            "Worker 1, [05/08]: Training Loss: 1.420858826, Training Accuracy: 58.904\n",
            "Worker 1, [06/08]: Training Loss: 1.369452469, Training Accuracy: 60.856\n",
            "Worker 1, [07/08]: Training Loss: 1.350384820, Training Accuracy: 61.112\n",
            "Worker 1, [08/08]: Training Loss: 1.292452365, Training Accuracy: 62.304\n",
            "Time taken for training worker 1: 0:00:47.672790\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.771139931, Training Accuracy: 50.984\n",
            "Worker 2, [02/08]: Training Loss: 1.627856525, Training Accuracy: 54.336\n",
            "Worker 2, [03/08]: Training Loss: 1.551546574, Training Accuracy: 56.120\n",
            "Worker 2, [04/08]: Training Loss: 1.464565118, Training Accuracy: 58.360\n",
            "Worker 2, [05/08]: Training Loss: 1.425848673, Training Accuracy: 59.208\n",
            "Worker 2, [06/08]: Training Loss: 1.367185072, Training Accuracy: 60.936\n",
            "Worker 2, [07/08]: Training Loss: 1.355867999, Training Accuracy: 61.008\n",
            "Worker 2, [08/08]: Training Loss: 1.291893869, Training Accuracy: 62.472\n",
            "Time taken for training worker 2: 0:00:46.869661\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.782696345, Training Accuracy: 51.128\n",
            "Worker 3, [02/08]: Training Loss: 1.620344045, Training Accuracy: 54.176\n",
            "Worker 3, [03/08]: Training Loss: 1.547059717, Training Accuracy: 55.776\n",
            "Worker 3, [04/08]: Training Loss: 1.472053499, Training Accuracy: 57.696\n",
            "Worker 3, [05/08]: Training Loss: 1.407048145, Training Accuracy: 59.400\n",
            "Worker 3, [06/08]: Training Loss: 1.396737277, Training Accuracy: 59.920\n",
            "Worker 3, [07/08]: Training Loss: 1.356520276, Training Accuracy: 60.856\n",
            "Worker 3, [08/08]: Training Loss: 1.305846677, Training Accuracy: 61.864\n",
            "Time taken for training worker 3: 0:00:46.996475\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.809144234, Training Accuracy: 50.072\n",
            "Worker 4, [02/08]: Training Loss: 1.650839601, Training Accuracy: 53.504\n",
            "Worker 4, [03/08]: Training Loss: 1.581603600, Training Accuracy: 55.808\n",
            "Worker 4, [04/08]: Training Loss: 1.511647674, Training Accuracy: 57.048\n",
            "Worker 4, [05/08]: Training Loss: 1.452665284, Training Accuracy: 58.552\n",
            "Worker 4, [06/08]: Training Loss: 1.410251041, Training Accuracy: 59.680\n",
            "Worker 4, [07/08]: Training Loss: 1.346322380, Training Accuracy: 61.568\n",
            "Worker 4, [08/08]: Training Loss: 1.325447142, Training Accuracy: 62.392\n",
            "Time taken for training worker 4: 0:00:46.968705\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003117\n",
            "Global Update 08: Test Loss: 2.065372804, Test Accuracy: 48.990\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.648266458, Training Accuracy: 53.648\n",
            "Worker 1, [02/08]: Training Loss: 1.514244463, Training Accuracy: 57.096\n",
            "Worker 1, [03/08]: Training Loss: 1.404581279, Training Accuracy: 59.552\n",
            "Worker 1, [04/08]: Training Loss: 1.342240466, Training Accuracy: 61.624\n",
            "Worker 1, [05/08]: Training Loss: 1.269623042, Training Accuracy: 62.896\n",
            "Worker 1, [06/08]: Training Loss: 1.226799306, Training Accuracy: 64.024\n",
            "Worker 1, [07/08]: Training Loss: 1.187908555, Training Accuracy: 65.192\n",
            "Worker 1, [08/08]: Training Loss: 1.154175307, Training Accuracy: 66.008\n",
            "Time taken for training worker 1: 0:00:46.319080\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.661505599, Training Accuracy: 53.800\n",
            "Worker 2, [02/08]: Training Loss: 1.500690306, Training Accuracy: 57.544\n",
            "Worker 2, [03/08]: Training Loss: 1.408396114, Training Accuracy: 59.944\n",
            "Worker 2, [04/08]: Training Loss: 1.348210299, Training Accuracy: 61.168\n",
            "Worker 2, [05/08]: Training Loss: 1.282649391, Training Accuracy: 62.656\n",
            "Worker 2, [06/08]: Training Loss: 1.222234047, Training Accuracy: 64.728\n",
            "Worker 2, [07/08]: Training Loss: 1.206467871, Training Accuracy: 64.320\n",
            "Worker 2, [08/08]: Training Loss: 1.150658387, Training Accuracy: 65.808\n",
            "Time taken for training worker 2: 0:00:48.404837\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.643321588, Training Accuracy: 54.184\n",
            "Worker 3, [02/08]: Training Loss: 1.507595803, Training Accuracy: 57.344\n",
            "Worker 3, [03/08]: Training Loss: 1.401765922, Training Accuracy: 59.912\n",
            "Worker 3, [04/08]: Training Loss: 1.343422697, Training Accuracy: 61.200\n",
            "Worker 3, [05/08]: Training Loss: 1.278136244, Training Accuracy: 62.888\n",
            "Worker 3, [06/08]: Training Loss: 1.235017395, Training Accuracy: 64.632\n",
            "Worker 3, [07/08]: Training Loss: 1.187139262, Training Accuracy: 65.800\n",
            "Worker 3, [08/08]: Training Loss: 1.166585144, Training Accuracy: 65.936\n",
            "Time taken for training worker 3: 0:00:47.701633\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.674160430, Training Accuracy: 53.544\n",
            "Worker 4, [02/08]: Training Loss: 1.531715343, Training Accuracy: 57.232\n",
            "Worker 4, [03/08]: Training Loss: 1.436217382, Training Accuracy: 59.416\n",
            "Worker 4, [04/08]: Training Loss: 1.366224026, Training Accuracy: 61.280\n",
            "Worker 4, [05/08]: Training Loss: 1.324197747, Training Accuracy: 62.088\n",
            "Worker 4, [06/08]: Training Loss: 1.265251842, Training Accuracy: 63.376\n",
            "Worker 4, [07/08]: Training Loss: 1.233784938, Training Accuracy: 64.280\n",
            "Worker 4, [08/08]: Training Loss: 1.183853347, Training Accuracy: 65.168\n",
            "Time taken for training worker 4: 0:00:48.151463\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002844\n",
            "Global Update 09: Test Loss: 2.032400449, Test Accuracy: 50.070\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.539754632, Training Accuracy: 56.528\n",
            "Worker 1, [02/08]: Training Loss: 1.368194527, Training Accuracy: 60.840\n",
            "Worker 1, [03/08]: Training Loss: 1.279448683, Training Accuracy: 63.024\n",
            "Worker 1, [04/08]: Training Loss: 1.210876754, Training Accuracy: 64.656\n",
            "Worker 1, [05/08]: Training Loss: 1.165504168, Training Accuracy: 66.120\n",
            "Worker 1, [06/08]: Training Loss: 1.085313167, Training Accuracy: 67.904\n",
            "Worker 1, [07/08]: Training Loss: 1.080755708, Training Accuracy: 67.704\n",
            "Worker 1, [08/08]: Training Loss: 1.027412090, Training Accuracy: 69.552\n",
            "Time taken for training worker 1: 0:00:47.020982\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.524112325, Training Accuracy: 56.528\n",
            "Worker 2, [02/08]: Training Loss: 1.369790504, Training Accuracy: 60.880\n",
            "Worker 2, [03/08]: Training Loss: 1.278794579, Training Accuracy: 62.760\n",
            "Worker 2, [04/08]: Training Loss: 1.230721220, Training Accuracy: 63.840\n",
            "Worker 2, [05/08]: Training Loss: 1.148089323, Training Accuracy: 66.232\n",
            "Worker 2, [06/08]: Training Loss: 1.099070418, Training Accuracy: 67.488\n",
            "Worker 2, [07/08]: Training Loss: 1.059588900, Training Accuracy: 68.760\n",
            "Worker 2, [08/08]: Training Loss: 1.014701338, Training Accuracy: 69.880\n",
            "Time taken for training worker 2: 0:00:46.646683\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.547418407, Training Accuracy: 56.288\n",
            "Worker 3, [02/08]: Training Loss: 1.389550419, Training Accuracy: 60.248\n",
            "Worker 3, [03/08]: Training Loss: 1.296057641, Training Accuracy: 61.984\n",
            "Worker 3, [04/08]: Training Loss: 1.238746738, Training Accuracy: 63.848\n",
            "Worker 3, [05/08]: Training Loss: 1.153628431, Training Accuracy: 66.120\n",
            "Worker 3, [06/08]: Training Loss: 1.102549109, Training Accuracy: 67.392\n",
            "Worker 3, [07/08]: Training Loss: 1.074445112, Training Accuracy: 68.040\n",
            "Worker 3, [08/08]: Training Loss: 1.051439065, Training Accuracy: 69.144\n",
            "Time taken for training worker 3: 0:00:47.514517\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.592820498, Training Accuracy: 55.608\n",
            "Worker 4, [02/08]: Training Loss: 1.390446823, Training Accuracy: 60.080\n",
            "Worker 4, [03/08]: Training Loss: 1.310787189, Training Accuracy: 62.000\n",
            "Worker 4, [04/08]: Training Loss: 1.243692317, Training Accuracy: 64.000\n",
            "Worker 4, [05/08]: Training Loss: 1.200076663, Training Accuracy: 64.912\n",
            "Worker 4, [06/08]: Training Loss: 1.134553035, Training Accuracy: 66.480\n",
            "Worker 4, [07/08]: Training Loss: 1.094181659, Training Accuracy: 67.752\n",
            "Worker 4, [08/08]: Training Loss: 1.051351028, Training Accuracy: 69.192\n",
            "Time taken for training worker 4: 0:00:47.639973\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002995\n",
            "Global Update 10: Test Loss: 2.017816723, Test Accuracy: 50.510\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.410463560, Training Accuracy: 59.680\n",
            "Worker 1, [02/08]: Training Loss: 1.250131812, Training Accuracy: 63.816\n",
            "Worker 1, [03/08]: Training Loss: 1.160570985, Training Accuracy: 65.792\n",
            "Worker 1, [04/08]: Training Loss: 1.106125671, Training Accuracy: 67.440\n",
            "Worker 1, [05/08]: Training Loss: 1.031077494, Training Accuracy: 70.040\n",
            "Worker 1, [06/08]: Training Loss: 0.974179695, Training Accuracy: 71.632\n",
            "Worker 1, [07/08]: Training Loss: 0.941775366, Training Accuracy: 71.568\n",
            "Worker 1, [08/08]: Training Loss: 0.910037426, Training Accuracy: 72.992\n",
            "Time taken for training worker 1: 0:00:47.917810\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.440739239, Training Accuracy: 59.096\n",
            "Worker 2, [02/08]: Training Loss: 1.253419957, Training Accuracy: 63.584\n",
            "Worker 2, [03/08]: Training Loss: 1.157282160, Training Accuracy: 65.968\n",
            "Worker 2, [04/08]: Training Loss: 1.106560978, Training Accuracy: 67.536\n",
            "Worker 2, [05/08]: Training Loss: 1.023061708, Training Accuracy: 69.560\n",
            "Worker 2, [06/08]: Training Loss: 0.999023832, Training Accuracy: 70.448\n",
            "Worker 2, [07/08]: Training Loss: 0.948850305, Training Accuracy: 71.656\n",
            "Worker 2, [08/08]: Training Loss: 0.898572981, Training Accuracy: 73.104\n",
            "Time taken for training worker 2: 0:00:47.507604\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.431511814, Training Accuracy: 59.152\n",
            "Worker 3, [02/08]: Training Loss: 1.271512904, Training Accuracy: 63.192\n",
            "Worker 3, [03/08]: Training Loss: 1.153977811, Training Accuracy: 65.776\n",
            "Worker 3, [04/08]: Training Loss: 1.091982690, Training Accuracy: 67.848\n",
            "Worker 3, [05/08]: Training Loss: 1.034973548, Training Accuracy: 69.168\n",
            "Worker 3, [06/08]: Training Loss: 1.006525294, Training Accuracy: 70.072\n",
            "Worker 3, [07/08]: Training Loss: 0.944457454, Training Accuracy: 72.032\n",
            "Worker 3, [08/08]: Training Loss: 0.936619527, Training Accuracy: 71.768\n",
            "Time taken for training worker 3: 0:00:46.908558\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.454519541, Training Accuracy: 58.648\n",
            "Worker 4, [02/08]: Training Loss: 1.271582254, Training Accuracy: 63.592\n",
            "Worker 4, [03/08]: Training Loss: 1.189293221, Training Accuracy: 65.208\n",
            "Worker 4, [04/08]: Training Loss: 1.129046565, Training Accuracy: 66.872\n",
            "Worker 4, [05/08]: Training Loss: 1.062418213, Training Accuracy: 68.920\n",
            "Worker 4, [06/08]: Training Loss: 1.004884352, Training Accuracy: 69.928\n",
            "Worker 4, [07/08]: Training Loss: 0.972060780, Training Accuracy: 71.104\n",
            "Worker 4, [08/08]: Training Loss: 0.917099424, Training Accuracy: 73.024\n",
            "Time taken for training worker 4: 0:00:46.422304\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002736\n",
            "Global Update 11: Test Loss: 2.069022323, Test Accuracy: 50.620\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.319765084, Training Accuracy: 62.176\n",
            "Worker 1, [02/08]: Training Loss: 1.146113923, Training Accuracy: 66.344\n",
            "Worker 1, [03/08]: Training Loss: 1.057336864, Training Accuracy: 68.560\n",
            "Worker 1, [04/08]: Training Loss: 0.988899577, Training Accuracy: 70.496\n",
            "Worker 1, [05/08]: Training Loss: 0.941802129, Training Accuracy: 71.872\n",
            "Worker 1, [06/08]: Training Loss: 0.860754332, Training Accuracy: 73.960\n",
            "Worker 1, [07/08]: Training Loss: 0.840686776, Training Accuracy: 74.736\n",
            "Worker 1, [08/08]: Training Loss: 0.812766727, Training Accuracy: 75.312\n",
            "Time taken for training worker 1: 0:00:47.554445\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.312701678, Training Accuracy: 61.904\n",
            "Worker 2, [02/08]: Training Loss: 1.142491661, Training Accuracy: 66.384\n",
            "Worker 2, [03/08]: Training Loss: 1.046290767, Training Accuracy: 68.832\n",
            "Worker 2, [04/08]: Training Loss: 0.984261500, Training Accuracy: 70.704\n",
            "Worker 2, [05/08]: Training Loss: 0.916833159, Training Accuracy: 72.528\n",
            "Worker 2, [06/08]: Training Loss: 0.893527181, Training Accuracy: 73.272\n",
            "Worker 2, [07/08]: Training Loss: 0.824604165, Training Accuracy: 75.048\n",
            "Worker 2, [08/08]: Training Loss: 0.787964110, Training Accuracy: 76.200\n",
            "Time taken for training worker 2: 0:00:47.769754\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.320884867, Training Accuracy: 61.480\n",
            "Worker 3, [02/08]: Training Loss: 1.147732015, Training Accuracy: 65.968\n",
            "Worker 3, [03/08]: Training Loss: 1.052733749, Training Accuracy: 69.368\n",
            "Worker 3, [04/08]: Training Loss: 0.995286676, Training Accuracy: 70.496\n",
            "Worker 3, [05/08]: Training Loss: 0.925529083, Training Accuracy: 72.112\n",
            "Worker 3, [06/08]: Training Loss: 0.873428071, Training Accuracy: 74.032\n",
            "Worker 3, [07/08]: Training Loss: 0.845647552, Training Accuracy: 75.056\n",
            "Worker 3, [08/08]: Training Loss: 0.797555288, Training Accuracy: 75.888\n",
            "Time taken for training worker 3: 0:00:49.910165\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.341147920, Training Accuracy: 61.616\n",
            "Worker 4, [02/08]: Training Loss: 1.161238141, Training Accuracy: 66.152\n",
            "Worker 4, [03/08]: Training Loss: 1.057291827, Training Accuracy: 69.192\n",
            "Worker 4, [04/08]: Training Loss: 0.994649466, Training Accuracy: 70.768\n",
            "Worker 4, [05/08]: Training Loss: 0.940540424, Training Accuracy: 72.304\n",
            "Worker 4, [06/08]: Training Loss: 0.898302737, Training Accuracy: 73.520\n",
            "Worker 4, [07/08]: Training Loss: 0.842604297, Training Accuracy: 74.944\n",
            "Worker 4, [08/08]: Training Loss: 0.814789559, Training Accuracy: 76.168\n",
            "Time taken for training worker 4: 0:00:50.328134\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002940\n",
            "Global Update 12: Test Loss: 2.117997085, Test Accuracy: 51.480\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.218808126, Training Accuracy: 64.272\n",
            "Worker 1, [02/08]: Training Loss: 1.029764115, Training Accuracy: 69.352\n",
            "Worker 1, [03/08]: Training Loss: 0.941454293, Training Accuracy: 71.656\n",
            "Worker 1, [04/08]: Training Loss: 0.869237733, Training Accuracy: 73.800\n",
            "Worker 1, [05/08]: Training Loss: 0.833846183, Training Accuracy: 75.368\n",
            "Worker 1, [06/08]: Training Loss: 0.785724570, Training Accuracy: 76.064\n",
            "Worker 1, [07/08]: Training Loss: 0.766697391, Training Accuracy: 77.072\n",
            "Worker 1, [08/08]: Training Loss: 0.708287231, Training Accuracy: 78.888\n",
            "Time taken for training worker 1: 0:00:50.763214\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.209191762, Training Accuracy: 64.640\n",
            "Worker 2, [02/08]: Training Loss: 1.038794595, Training Accuracy: 68.992\n",
            "Worker 2, [03/08]: Training Loss: 0.967029483, Training Accuracy: 71.744\n",
            "Worker 2, [04/08]: Training Loss: 0.889973435, Training Accuracy: 73.440\n",
            "Worker 2, [05/08]: Training Loss: 0.828773875, Training Accuracy: 75.192\n",
            "Worker 2, [06/08]: Training Loss: 0.784165374, Training Accuracy: 76.584\n",
            "Worker 2, [07/08]: Training Loss: 0.748685326, Training Accuracy: 77.744\n",
            "Worker 2, [08/08]: Training Loss: 0.727102524, Training Accuracy: 78.048\n",
            "Time taken for training worker 2: 0:00:49.123199\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.223611043, Training Accuracy: 64.624\n",
            "Worker 3, [02/08]: Training Loss: 1.034225855, Training Accuracy: 69.448\n",
            "Worker 3, [03/08]: Training Loss: 0.954945381, Training Accuracy: 71.824\n",
            "Worker 3, [04/08]: Training Loss: 0.883309801, Training Accuracy: 73.912\n",
            "Worker 3, [05/08]: Training Loss: 0.828614620, Training Accuracy: 75.496\n",
            "Worker 3, [06/08]: Training Loss: 0.778679150, Training Accuracy: 76.888\n",
            "Worker 3, [07/08]: Training Loss: 0.754342003, Training Accuracy: 77.192\n",
            "Worker 3, [08/08]: Training Loss: 0.713055792, Training Accuracy: 78.968\n",
            "Time taken for training worker 3: 0:00:49.616096\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.236073731, Training Accuracy: 63.744\n",
            "Worker 4, [02/08]: Training Loss: 1.074085909, Training Accuracy: 68.624\n",
            "Worker 4, [03/08]: Training Loss: 0.981265174, Training Accuracy: 71.000\n",
            "Worker 4, [04/08]: Training Loss: 0.911307847, Training Accuracy: 73.584\n",
            "Worker 4, [05/08]: Training Loss: 0.852655183, Training Accuracy: 74.968\n",
            "Worker 4, [06/08]: Training Loss: 0.789053665, Training Accuracy: 76.688\n",
            "Worker 4, [07/08]: Training Loss: 0.769636306, Training Accuracy: 77.224\n",
            "Worker 4, [08/08]: Training Loss: 0.723042081, Training Accuracy: 78.448\n",
            "Time taken for training worker 4: 0:00:49.606481\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003294\n",
            "Global Update 13: Test Loss: 2.151479647, Test Accuracy: 51.980\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.126552711, Training Accuracy: 67.024\n",
            "Worker 1, [02/08]: Training Loss: 0.960944480, Training Accuracy: 71.672\n",
            "Worker 1, [03/08]: Training Loss: 0.871424437, Training Accuracy: 73.536\n",
            "Worker 1, [04/08]: Training Loss: 0.806315090, Training Accuracy: 76.152\n",
            "Worker 1, [05/08]: Training Loss: 0.755872795, Training Accuracy: 77.544\n",
            "Worker 1, [06/08]: Training Loss: 0.728492065, Training Accuracy: 78.144\n",
            "Worker 1, [07/08]: Training Loss: 0.681280332, Training Accuracy: 79.400\n",
            "Worker 1, [08/08]: Training Loss: 0.645385089, Training Accuracy: 80.816\n",
            "Time taken for training worker 1: 0:00:50.401894\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.108758409, Training Accuracy: 66.920\n",
            "Worker 2, [02/08]: Training Loss: 0.936480500, Training Accuracy: 72.176\n",
            "Worker 2, [03/08]: Training Loss: 0.866535095, Training Accuracy: 74.464\n",
            "Worker 2, [04/08]: Training Loss: 0.798303263, Training Accuracy: 76.112\n",
            "Worker 2, [05/08]: Training Loss: 0.746913337, Training Accuracy: 77.832\n",
            "Worker 2, [06/08]: Training Loss: 0.717840011, Training Accuracy: 78.568\n",
            "Worker 2, [07/08]: Training Loss: 0.672151746, Training Accuracy: 80.024\n",
            "Worker 2, [08/08]: Training Loss: 0.651186867, Training Accuracy: 80.432\n",
            "Time taken for training worker 2: 0:00:50.125914\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.106501791, Training Accuracy: 67.160\n",
            "Worker 3, [02/08]: Training Loss: 0.958732106, Training Accuracy: 71.152\n",
            "Worker 3, [03/08]: Training Loss: 0.850525663, Training Accuracy: 74.464\n",
            "Worker 3, [04/08]: Training Loss: 0.796682097, Training Accuracy: 76.384\n",
            "Worker 3, [05/08]: Training Loss: 0.760995293, Training Accuracy: 77.256\n",
            "Worker 3, [06/08]: Training Loss: 0.712671475, Training Accuracy: 79.072\n",
            "Worker 3, [07/08]: Training Loss: 0.683838741, Training Accuracy: 79.432\n",
            "Worker 3, [08/08]: Training Loss: 0.649272193, Training Accuracy: 80.208\n",
            "Time taken for training worker 3: 0:00:48.457386\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.135815072, Training Accuracy: 66.416\n",
            "Worker 4, [02/08]: Training Loss: 0.977182559, Training Accuracy: 70.648\n",
            "Worker 4, [03/08]: Training Loss: 0.880133452, Training Accuracy: 73.792\n",
            "Worker 4, [04/08]: Training Loss: 0.804596448, Training Accuracy: 76.216\n",
            "Worker 4, [05/08]: Training Loss: 0.781094543, Training Accuracy: 77.256\n",
            "Worker 4, [06/08]: Training Loss: 0.718452141, Training Accuracy: 78.880\n",
            "Worker 4, [07/08]: Training Loss: 0.676394983, Training Accuracy: 79.800\n",
            "Worker 4, [08/08]: Training Loss: 0.650312281, Training Accuracy: 80.848\n",
            "Time taken for training worker 4: 0:00:48.672802\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002912\n",
            "Global Update 14: Test Loss: 2.169095349, Test Accuracy: 52.070\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.036879061, Training Accuracy: 68.544\n",
            "Worker 1, [02/08]: Training Loss: 0.897342347, Training Accuracy: 72.864\n",
            "Worker 1, [03/08]: Training Loss: 0.800770997, Training Accuracy: 75.896\n",
            "Worker 1, [04/08]: Training Loss: 0.750136028, Training Accuracy: 77.928\n",
            "Worker 1, [05/08]: Training Loss: 0.705399443, Training Accuracy: 79.112\n",
            "Worker 1, [06/08]: Training Loss: 0.656146299, Training Accuracy: 80.696\n",
            "Worker 1, [07/08]: Training Loss: 0.644218995, Training Accuracy: 80.488\n",
            "Worker 1, [08/08]: Training Loss: 0.606889286, Training Accuracy: 82.352\n",
            "Time taken for training worker 1: 0:00:49.659024\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.015720679, Training Accuracy: 69.992\n",
            "Worker 2, [02/08]: Training Loss: 0.872244081, Training Accuracy: 74.304\n",
            "Worker 2, [03/08]: Training Loss: 0.803827915, Training Accuracy: 76.256\n",
            "Worker 2, [04/08]: Training Loss: 0.733491092, Training Accuracy: 78.056\n",
            "Worker 2, [05/08]: Training Loss: 0.697815338, Training Accuracy: 78.936\n",
            "Worker 2, [06/08]: Training Loss: 0.666097757, Training Accuracy: 80.288\n",
            "Worker 2, [07/08]: Training Loss: 0.630005996, Training Accuracy: 81.352\n",
            "Worker 2, [08/08]: Training Loss: 0.598364992, Training Accuracy: 82.608\n",
            "Time taken for training worker 2: 0:00:49.710696\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.018389107, Training Accuracy: 69.336\n",
            "Worker 3, [02/08]: Training Loss: 0.881554787, Training Accuracy: 73.160\n",
            "Worker 3, [03/08]: Training Loss: 0.805808091, Training Accuracy: 75.496\n",
            "Worker 3, [04/08]: Training Loss: 0.746658780, Training Accuracy: 78.000\n",
            "Worker 3, [05/08]: Training Loss: 0.701973969, Training Accuracy: 78.816\n",
            "Worker 3, [06/08]: Training Loss: 0.652030639, Training Accuracy: 80.656\n",
            "Worker 3, [07/08]: Training Loss: 0.647655231, Training Accuracy: 81.056\n",
            "Worker 3, [08/08]: Training Loss: 0.606374370, Training Accuracy: 82.176\n",
            "Time taken for training worker 3: 0:00:49.028990\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.021786616, Training Accuracy: 69.768\n",
            "Worker 4, [02/08]: Training Loss: 0.910252952, Training Accuracy: 73.240\n",
            "Worker 4, [03/08]: Training Loss: 0.814494507, Training Accuracy: 76.016\n",
            "Worker 4, [04/08]: Training Loss: 0.763443671, Training Accuracy: 77.744\n",
            "Worker 4, [05/08]: Training Loss: 0.709000520, Training Accuracy: 78.832\n",
            "Worker 4, [06/08]: Training Loss: 0.673643318, Training Accuracy: 80.528\n",
            "Worker 4, [07/08]: Training Loss: 0.645669126, Training Accuracy: 80.944\n",
            "Worker 4, [08/08]: Training Loss: 0.614832814, Training Accuracy: 81.848\n",
            "Time taken for training worker 4: 0:00:50.309091\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002764\n",
            "Global Update 15: Test Loss: 2.179636636, Test Accuracy: 52.110\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 0.957393908, Training Accuracy: 71.232\n",
            "Worker 1, [02/08]: Training Loss: 0.834499354, Training Accuracy: 74.784\n",
            "Worker 1, [03/08]: Training Loss: 0.773316334, Training Accuracy: 76.512\n",
            "Worker 1, [04/08]: Training Loss: 0.717538637, Training Accuracy: 78.696\n",
            "Worker 1, [05/08]: Training Loss: 0.686093134, Training Accuracy: 79.776\n",
            "Worker 1, [06/08]: Training Loss: 0.646957696, Training Accuracy: 81.128\n",
            "Worker 1, [07/08]: Training Loss: 0.637275421, Training Accuracy: 81.256\n",
            "Worker 1, [08/08]: Training Loss: 0.599095495, Training Accuracy: 82.392\n",
            "Time taken for training worker 1: 0:00:48.983535\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 0.946676445, Training Accuracy: 71.456\n",
            "Worker 2, [02/08]: Training Loss: 0.839814097, Training Accuracy: 74.808\n",
            "Worker 2, [03/08]: Training Loss: 0.769693214, Training Accuracy: 77.552\n",
            "Worker 2, [04/08]: Training Loss: 0.710761201, Training Accuracy: 78.904\n",
            "Worker 2, [05/08]: Training Loss: 0.665628801, Training Accuracy: 80.424\n",
            "Worker 2, [06/08]: Training Loss: 0.640330674, Training Accuracy: 81.448\n",
            "Worker 2, [07/08]: Training Loss: 0.620893889, Training Accuracy: 82.016\n",
            "Worker 2, [08/08]: Training Loss: 0.593534987, Training Accuracy: 83.064\n",
            "Time taken for training worker 2: 0:00:50.426928\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 0.924030832, Training Accuracy: 72.000\n",
            "Worker 3, [02/08]: Training Loss: 0.829730885, Training Accuracy: 74.712\n",
            "Worker 3, [03/08]: Training Loss: 0.762997418, Training Accuracy: 77.152\n",
            "Worker 3, [04/08]: Training Loss: 0.716439071, Training Accuracy: 78.720\n",
            "Worker 3, [05/08]: Training Loss: 0.691159646, Training Accuracy: 79.928\n",
            "Worker 3, [06/08]: Training Loss: 0.640371788, Training Accuracy: 81.320\n",
            "Worker 3, [07/08]: Training Loss: 0.624109906, Training Accuracy: 81.600\n",
            "Worker 3, [08/08]: Training Loss: 0.612372605, Training Accuracy: 82.176\n",
            "Time taken for training worker 3: 0:00:48.679428\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 0.963600793, Training Accuracy: 70.920\n",
            "Worker 4, [02/08]: Training Loss: 0.842735414, Training Accuracy: 74.864\n",
            "Worker 4, [03/08]: Training Loss: 0.783282277, Training Accuracy: 76.552\n",
            "Worker 4, [04/08]: Training Loss: 0.725977320, Training Accuracy: 78.592\n",
            "Worker 4, [05/08]: Training Loss: 0.703184986, Training Accuracy: 79.224\n",
            "Worker 4, [06/08]: Training Loss: 0.662191036, Training Accuracy: 80.736\n",
            "Worker 4, [07/08]: Training Loss: 0.643163691, Training Accuracy: 81.032\n",
            "Worker 4, [08/08]: Training Loss: 0.606826032, Training Accuracy: 82.728\n",
            "Time taken for training worker 4: 0:00:49.935448\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002681\n",
            "Global Update 16: Test Loss: 2.193939955, Test Accuracy: 52.210\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 0.900336780, Training Accuracy: 72.720\n",
            "Worker 1, [02/08]: Training Loss: 0.820921571, Training Accuracy: 75.248\n",
            "Worker 1, [03/08]: Training Loss: 0.758504078, Training Accuracy: 77.456\n",
            "Worker 1, [04/08]: Training Loss: 0.733532198, Training Accuracy: 77.776\n",
            "Worker 1, [05/08]: Training Loss: 0.718309448, Training Accuracy: 78.824\n",
            "Worker 1, [06/08]: Training Loss: 0.684497532, Training Accuracy: 79.624\n",
            "Worker 1, [07/08]: Training Loss: 0.656347159, Training Accuracy: 81.152\n",
            "Worker 1, [08/08]: Training Loss: 0.660406287, Training Accuracy: 80.864\n",
            "Time taken for training worker 1: 0:00:47.897235\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 0.888441131, Training Accuracy: 72.928\n",
            "Worker 2, [02/08]: Training Loss: 0.807933753, Training Accuracy: 75.448\n",
            "Worker 2, [03/08]: Training Loss: 0.767271258, Training Accuracy: 77.040\n",
            "Worker 2, [04/08]: Training Loss: 0.722992712, Training Accuracy: 78.696\n",
            "Worker 2, [05/08]: Training Loss: 0.690186301, Training Accuracy: 79.568\n",
            "Worker 2, [06/08]: Training Loss: 0.672903558, Training Accuracy: 79.744\n",
            "Worker 2, [07/08]: Training Loss: 0.644656620, Training Accuracy: 80.752\n",
            "Worker 2, [08/08]: Training Loss: 0.634650603, Training Accuracy: 81.496\n",
            "Time taken for training worker 2: 0:00:47.539226\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 0.877881405, Training Accuracy: 72.976\n",
            "Worker 3, [02/08]: Training Loss: 0.810822014, Training Accuracy: 75.416\n",
            "Worker 3, [03/08]: Training Loss: 0.768927575, Training Accuracy: 77.208\n",
            "Worker 3, [04/08]: Training Loss: 0.730638583, Training Accuracy: 77.968\n",
            "Worker 3, [05/08]: Training Loss: 0.707855483, Training Accuracy: 78.720\n",
            "Worker 3, [06/08]: Training Loss: 0.691424284, Training Accuracy: 79.640\n",
            "Worker 3, [07/08]: Training Loss: 0.660248196, Training Accuracy: 80.472\n",
            "Worker 3, [08/08]: Training Loss: 0.657995236, Training Accuracy: 81.152\n",
            "Time taken for training worker 3: 0:00:48.507442\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 0.915358860, Training Accuracy: 72.184\n",
            "Worker 4, [02/08]: Training Loss: 0.818242636, Training Accuracy: 75.408\n",
            "Worker 4, [03/08]: Training Loss: 0.786377055, Training Accuracy: 76.744\n",
            "Worker 4, [04/08]: Training Loss: 0.752011818, Training Accuracy: 77.616\n",
            "Worker 4, [05/08]: Training Loss: 0.719986139, Training Accuracy: 78.424\n",
            "Worker 4, [06/08]: Training Loss: 0.700891556, Training Accuracy: 79.352\n",
            "Worker 4, [07/08]: Training Loss: 0.669576556, Training Accuracy: 80.880\n",
            "Worker 4, [08/08]: Training Loss: 0.642542295, Training Accuracy: 81.368\n",
            "Time taken for training worker 4: 0:00:47.126892\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003105\n",
            "Global Update 17: Test Loss: 2.173971456, Test Accuracy: 52.280\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 0.855846967, Training Accuracy: 73.536\n",
            "Worker 1, [02/08]: Training Loss: 0.830718429, Training Accuracy: 74.784\n",
            "Worker 1, [03/08]: Training Loss: 0.811341458, Training Accuracy: 75.520\n",
            "Worker 1, [04/08]: Training Loss: 0.785673867, Training Accuracy: 76.272\n",
            "Worker 1, [05/08]: Training Loss: 0.780091899, Training Accuracy: 76.592\n",
            "Worker 1, [06/08]: Training Loss: 0.745709632, Training Accuracy: 77.456\n",
            "Worker 1, [07/08]: Training Loss: 0.745630964, Training Accuracy: 77.688\n",
            "Worker 1, [08/08]: Training Loss: 0.737499183, Training Accuracy: 78.160\n",
            "Time taken for training worker 1: 0:00:47.772302\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 0.849496863, Training Accuracy: 74.056\n",
            "Worker 2, [02/08]: Training Loss: 0.829927530, Training Accuracy: 74.744\n",
            "Worker 2, [03/08]: Training Loss: 0.794983524, Training Accuracy: 76.192\n",
            "Worker 2, [04/08]: Training Loss: 0.787361604, Training Accuracy: 76.400\n",
            "Worker 2, [05/08]: Training Loss: 0.759833959, Training Accuracy: 77.480\n",
            "Worker 2, [06/08]: Training Loss: 0.747266683, Training Accuracy: 77.816\n",
            "Worker 2, [07/08]: Training Loss: 0.733786980, Training Accuracy: 77.736\n",
            "Worker 2, [08/08]: Training Loss: 0.724136432, Training Accuracy: 78.520\n",
            "Time taken for training worker 2: 0:00:48.712947\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 0.866047069, Training Accuracy: 73.592\n",
            "Worker 3, [02/08]: Training Loss: 0.824754281, Training Accuracy: 74.832\n",
            "Worker 3, [03/08]: Training Loss: 0.801131578, Training Accuracy: 75.880\n",
            "Worker 3, [04/08]: Training Loss: 0.784150533, Training Accuracy: 76.104\n",
            "Worker 3, [05/08]: Training Loss: 0.770643163, Training Accuracy: 76.592\n",
            "Worker 3, [06/08]: Training Loss: 0.760707230, Training Accuracy: 77.048\n",
            "Worker 3, [07/08]: Training Loss: 0.745956226, Training Accuracy: 77.752\n",
            "Worker 3, [08/08]: Training Loss: 0.724623112, Training Accuracy: 78.336\n",
            "Time taken for training worker 3: 0:00:47.360786\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 0.874529954, Training Accuracy: 73.704\n",
            "Worker 4, [02/08]: Training Loss: 0.836037538, Training Accuracy: 75.160\n",
            "Worker 4, [03/08]: Training Loss: 0.814570678, Training Accuracy: 75.848\n",
            "Worker 4, [04/08]: Training Loss: 0.790369679, Training Accuracy: 76.488\n",
            "Worker 4, [05/08]: Training Loss: 0.793081823, Training Accuracy: 76.432\n",
            "Worker 4, [06/08]: Training Loss: 0.773529392, Training Accuracy: 76.984\n",
            "Worker 4, [07/08]: Training Loss: 0.759549118, Training Accuracy: 77.624\n",
            "Worker 4, [08/08]: Training Loss: 0.744017617, Training Accuracy: 78.160\n",
            "Time taken for training worker 4: 0:00:46.887169\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002826\n",
            "Global Update 18: Test Loss: 2.146553544, Test Accuracy: 52.300\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:58:12.033631\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:16\n",
            "==================================================\n",
            "Worker 1, [01/16]: Training Loss: 4.515100450, Training Accuracy: 2.432\n",
            "Worker 1, [02/16]: Training Loss: 4.138045192, Training Accuracy: 6.120\n",
            "Worker 1, [03/16]: Training Loss: 3.905501489, Training Accuracy: 9.304\n",
            "Worker 1, [04/16]: Training Loss: 3.759096282, Training Accuracy: 11.360\n",
            "Worker 1, [05/16]: Training Loss: 3.630032786, Training Accuracy: 13.936\n",
            "Worker 1, [06/16]: Training Loss: 3.497638573, Training Accuracy: 15.736\n",
            "Worker 1, [07/16]: Training Loss: 3.400458597, Training Accuracy: 17.640\n",
            "Worker 1, [08/16]: Training Loss: 3.267892357, Training Accuracy: 19.544\n",
            "Worker 1, [09/16]: Training Loss: 3.171802500, Training Accuracy: 21.240\n",
            "Worker 1, [10/16]: Training Loss: 3.101246830, Training Accuracy: 22.824\n",
            "Worker 1, [11/16]: Training Loss: 3.020206567, Training Accuracy: 24.112\n",
            "Worker 1, [12/16]: Training Loss: 2.923245390, Training Accuracy: 26.016\n",
            "Worker 1, [13/16]: Training Loss: 2.865706141, Training Accuracy: 27.192\n",
            "Worker 1, [14/16]: Training Loss: 2.773618586, Training Accuracy: 28.520\n",
            "Worker 1, [15/16]: Training Loss: 2.724931843, Training Accuracy: 30.112\n",
            "Worker 1, [16/16]: Training Loss: 2.654804938, Training Accuracy: 31.544\n",
            "Time taken for training worker 1: 0:01:36.008238\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 4.517025254, Training Accuracy: 2.712\n",
            "Worker 2, [02/16]: Training Loss: 4.141691817, Training Accuracy: 5.848\n",
            "Worker 2, [03/16]: Training Loss: 3.938128037, Training Accuracy: 8.608\n",
            "Worker 2, [04/16]: Training Loss: 3.773209809, Training Accuracy: 11.552\n",
            "Worker 2, [05/16]: Training Loss: 3.661839961, Training Accuracy: 13.528\n",
            "Worker 2, [06/16]: Training Loss: 3.519094556, Training Accuracy: 15.496\n",
            "Worker 2, [07/16]: Training Loss: 3.414940919, Training Accuracy: 17.552\n",
            "Worker 2, [08/16]: Training Loss: 3.323070133, Training Accuracy: 18.992\n",
            "Worker 2, [09/16]: Training Loss: 3.198266604, Training Accuracy: 21.432\n",
            "Worker 2, [10/16]: Training Loss: 3.131929261, Training Accuracy: 22.992\n",
            "Worker 2, [11/16]: Training Loss: 3.043516441, Training Accuracy: 24.264\n",
            "Worker 2, [12/16]: Training Loss: 2.963027510, Training Accuracy: 24.936\n",
            "Worker 2, [13/16]: Training Loss: 2.892047028, Training Accuracy: 26.888\n",
            "Worker 2, [14/16]: Training Loss: 2.791543019, Training Accuracy: 28.688\n",
            "Worker 2, [15/16]: Training Loss: 2.757351986, Training Accuracy: 29.392\n",
            "Worker 2, [16/16]: Training Loss: 2.682258029, Training Accuracy: 31.120\n",
            "Time taken for training worker 2: 0:01:35.831148\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 4.498190863, Training Accuracy: 2.496\n",
            "Worker 3, [02/16]: Training Loss: 4.108099768, Training Accuracy: 6.496\n",
            "Worker 3, [03/16]: Training Loss: 3.903321571, Training Accuracy: 9.560\n",
            "Worker 3, [04/16]: Training Loss: 3.751326399, Training Accuracy: 11.536\n",
            "Worker 3, [05/16]: Training Loss: 3.618436890, Training Accuracy: 13.792\n",
            "Worker 3, [06/16]: Training Loss: 3.502571146, Training Accuracy: 15.432\n",
            "Worker 3, [07/16]: Training Loss: 3.374946953, Training Accuracy: 17.600\n",
            "Worker 3, [08/16]: Training Loss: 3.289187707, Training Accuracy: 19.504\n",
            "Worker 3, [09/16]: Training Loss: 3.168589181, Training Accuracy: 21.504\n",
            "Worker 3, [10/16]: Training Loss: 3.089494508, Training Accuracy: 22.904\n",
            "Worker 3, [11/16]: Training Loss: 3.017748019, Training Accuracy: 24.360\n",
            "Worker 3, [12/16]: Training Loss: 2.927226574, Training Accuracy: 25.464\n",
            "Worker 3, [13/16]: Training Loss: 2.842790544, Training Accuracy: 27.464\n",
            "Worker 3, [14/16]: Training Loss: 2.760330689, Training Accuracy: 28.680\n",
            "Worker 3, [15/16]: Training Loss: 2.711588245, Training Accuracy: 30.176\n",
            "Worker 3, [16/16]: Training Loss: 2.637951831, Training Accuracy: 31.088\n",
            "Time taken for training worker 3: 0:01:35.354220\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 4.516673151, Training Accuracy: 2.552\n",
            "Worker 4, [02/16]: Training Loss: 4.148673677, Training Accuracy: 6.040\n",
            "Worker 4, [03/16]: Training Loss: 3.948153409, Training Accuracy: 8.840\n",
            "Worker 4, [04/16]: Training Loss: 3.805949673, Training Accuracy: 11.040\n",
            "Worker 4, [05/16]: Training Loss: 3.657375940, Training Accuracy: 13.072\n",
            "Worker 4, [06/16]: Training Loss: 3.530066808, Training Accuracy: 15.232\n",
            "Worker 4, [07/16]: Training Loss: 3.428969484, Training Accuracy: 16.744\n",
            "Worker 4, [08/16]: Training Loss: 3.309923880, Training Accuracy: 19.224\n",
            "Worker 4, [09/16]: Training Loss: 3.207281827, Training Accuracy: 21.208\n",
            "Worker 4, [10/16]: Training Loss: 3.137901294, Training Accuracy: 22.152\n",
            "Worker 4, [11/16]: Training Loss: 3.042815567, Training Accuracy: 23.976\n",
            "Worker 4, [12/16]: Training Loss: 2.971354167, Training Accuracy: 25.392\n",
            "Worker 4, [13/16]: Training Loss: 2.872465415, Training Accuracy: 27.584\n",
            "Worker 4, [14/16]: Training Loss: 2.825682246, Training Accuracy: 27.696\n",
            "Worker 4, [15/16]: Training Loss: 2.734202320, Training Accuracy: 30.232\n",
            "Worker 4, [16/16]: Training Loss: 2.674732127, Training Accuracy: 30.864\n",
            "Time taken for training worker 4: 0:01:34.831957\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002677\n",
            "Global Update 01: Test Loss: 3.318129497, Test Accuracy: 27.920\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.979706691, Training Accuracy: 25.208\n",
            "Worker 1, [02/16]: Training Loss: 2.814577752, Training Accuracy: 27.952\n",
            "Worker 1, [03/16]: Training Loss: 2.722234360, Training Accuracy: 29.632\n",
            "Worker 1, [04/16]: Training Loss: 2.645820677, Training Accuracy: 31.000\n",
            "Worker 1, [05/16]: Training Loss: 2.551539788, Training Accuracy: 33.544\n",
            "Worker 1, [06/16]: Training Loss: 2.494181514, Training Accuracy: 34.584\n",
            "Worker 1, [07/16]: Training Loss: 2.449592103, Training Accuracy: 35.408\n",
            "Worker 1, [08/16]: Training Loss: 2.373512002, Training Accuracy: 36.608\n",
            "Worker 1, [09/16]: Training Loss: 2.337790133, Training Accuracy: 37.552\n",
            "Worker 1, [10/16]: Training Loss: 2.253082698, Training Accuracy: 40.096\n",
            "Worker 1, [11/16]: Training Loss: 2.220059579, Training Accuracy: 40.352\n",
            "Worker 1, [12/16]: Training Loss: 2.187740755, Training Accuracy: 40.768\n",
            "Worker 1, [13/16]: Training Loss: 2.130221879, Training Accuracy: 42.080\n",
            "Worker 1, [14/16]: Training Loss: 2.089371273, Training Accuracy: 43.136\n",
            "Worker 1, [15/16]: Training Loss: 2.059505036, Training Accuracy: 43.720\n",
            "Worker 1, [16/16]: Training Loss: 1.995705113, Training Accuracy: 45.504\n",
            "Time taken for training worker 1: 0:01:35.389997\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.989827160, Training Accuracy: 25.200\n",
            "Worker 2, [02/16]: Training Loss: 2.821643058, Training Accuracy: 27.608\n",
            "Worker 2, [03/16]: Training Loss: 2.724192117, Training Accuracy: 30.072\n",
            "Worker 2, [04/16]: Training Loss: 2.637904590, Training Accuracy: 31.600\n",
            "Worker 2, [05/16]: Training Loss: 2.592141884, Training Accuracy: 32.456\n",
            "Worker 2, [06/16]: Training Loss: 2.500863152, Training Accuracy: 34.224\n",
            "Worker 2, [07/16]: Training Loss: 2.456696786, Training Accuracy: 34.776\n",
            "Worker 2, [08/16]: Training Loss: 2.378140915, Training Accuracy: 36.672\n",
            "Worker 2, [09/16]: Training Loss: 2.333475977, Training Accuracy: 37.920\n",
            "Worker 2, [10/16]: Training Loss: 2.272079960, Training Accuracy: 38.888\n",
            "Worker 2, [11/16]: Training Loss: 2.225153919, Training Accuracy: 40.032\n",
            "Worker 2, [12/16]: Training Loss: 2.180751316, Training Accuracy: 40.856\n",
            "Worker 2, [13/16]: Training Loss: 2.133895006, Training Accuracy: 42.032\n",
            "Worker 2, [14/16]: Training Loss: 2.086982303, Training Accuracy: 42.760\n",
            "Worker 2, [15/16]: Training Loss: 2.049912368, Training Accuracy: 43.672\n",
            "Worker 2, [16/16]: Training Loss: 2.006452753, Training Accuracy: 44.704\n",
            "Time taken for training worker 2: 0:01:37.172053\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.970883575, Training Accuracy: 25.672\n",
            "Worker 3, [02/16]: Training Loss: 2.806555668, Training Accuracy: 28.312\n",
            "Worker 3, [03/16]: Training Loss: 2.719775752, Training Accuracy: 30.080\n",
            "Worker 3, [04/16]: Training Loss: 2.646287686, Training Accuracy: 31.464\n",
            "Worker 3, [05/16]: Training Loss: 2.580688061, Training Accuracy: 32.760\n",
            "Worker 3, [06/16]: Training Loss: 2.489035277, Training Accuracy: 35.232\n",
            "Worker 3, [07/16]: Training Loss: 2.432459430, Training Accuracy: 36.080\n",
            "Worker 3, [08/16]: Training Loss: 2.397940076, Training Accuracy: 36.672\n",
            "Worker 3, [09/16]: Training Loss: 2.324827555, Training Accuracy: 38.512\n",
            "Worker 3, [10/16]: Training Loss: 2.272809042, Training Accuracy: 38.704\n",
            "Worker 3, [11/16]: Training Loss: 2.237572059, Training Accuracy: 40.240\n",
            "Worker 3, [12/16]: Training Loss: 2.194655209, Training Accuracy: 41.136\n",
            "Worker 3, [13/16]: Training Loss: 2.135534522, Training Accuracy: 41.760\n",
            "Worker 3, [14/16]: Training Loss: 2.097368319, Training Accuracy: 42.880\n",
            "Worker 3, [15/16]: Training Loss: 2.058082970, Training Accuracy: 44.032\n",
            "Worker 3, [16/16]: Training Loss: 2.025957205, Training Accuracy: 44.360\n",
            "Time taken for training worker 3: 0:01:37.364751\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.977692591, Training Accuracy: 25.760\n",
            "Worker 4, [02/16]: Training Loss: 2.845064270, Training Accuracy: 28.000\n",
            "Worker 4, [03/16]: Training Loss: 2.749414208, Training Accuracy: 30.000\n",
            "Worker 4, [04/16]: Training Loss: 2.668791756, Training Accuracy: 30.848\n",
            "Worker 4, [05/16]: Training Loss: 2.592929489, Training Accuracy: 32.488\n",
            "Worker 4, [06/16]: Training Loss: 2.523450219, Training Accuracy: 34.856\n",
            "Worker 4, [07/16]: Training Loss: 2.487749531, Training Accuracy: 35.032\n",
            "Worker 4, [08/16]: Training Loss: 2.404124852, Training Accuracy: 36.832\n",
            "Worker 4, [09/16]: Training Loss: 2.369158061, Training Accuracy: 37.600\n",
            "Worker 4, [10/16]: Training Loss: 2.292381499, Training Accuracy: 38.848\n",
            "Worker 4, [11/16]: Training Loss: 2.235817148, Training Accuracy: 40.336\n",
            "Worker 4, [12/16]: Training Loss: 2.201654295, Training Accuracy: 40.600\n",
            "Worker 4, [13/16]: Training Loss: 2.158353025, Training Accuracy: 42.016\n",
            "Worker 4, [14/16]: Training Loss: 2.105120147, Training Accuracy: 42.992\n",
            "Worker 4, [15/16]: Training Loss: 2.080113285, Training Accuracy: 43.320\n",
            "Worker 4, [16/16]: Training Loss: 2.030387824, Training Accuracy: 44.232\n",
            "Time taken for training worker 4: 0:01:37.162413\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002969\n",
            "Global Update 02: Test Loss: 3.053596197, Test Accuracy: 38.790\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.496446611, Training Accuracy: 35.616\n",
            "Worker 1, [02/16]: Training Loss: 2.266082657, Training Accuracy: 40.296\n",
            "Worker 1, [03/16]: Training Loss: 2.166760392, Training Accuracy: 41.344\n",
            "Worker 1, [04/16]: Training Loss: 2.114273149, Training Accuracy: 43.352\n",
            "Worker 1, [05/16]: Training Loss: 2.022252245, Training Accuracy: 45.096\n",
            "Worker 1, [06/16]: Training Loss: 1.964810638, Training Accuracy: 46.632\n",
            "Worker 1, [07/16]: Training Loss: 1.921207615, Training Accuracy: 46.688\n",
            "Worker 1, [08/16]: Training Loss: 1.872405222, Training Accuracy: 48.008\n",
            "Worker 1, [09/16]: Training Loss: 1.837305576, Training Accuracy: 48.944\n",
            "Worker 1, [10/16]: Training Loss: 1.805935127, Training Accuracy: 49.640\n",
            "Worker 1, [11/16]: Training Loss: 1.754448697, Training Accuracy: 51.168\n",
            "Worker 1, [12/16]: Training Loss: 1.716151517, Training Accuracy: 51.920\n",
            "Worker 1, [13/16]: Training Loss: 1.678615401, Training Accuracy: 52.480\n",
            "Worker 1, [14/16]: Training Loss: 1.634793366, Training Accuracy: 53.672\n",
            "Worker 1, [15/16]: Training Loss: 1.620760872, Training Accuracy: 54.376\n",
            "Worker 1, [16/16]: Training Loss: 1.572984688, Training Accuracy: 55.704\n",
            "Time taken for training worker 1: 0:01:35.241334\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.511248483, Training Accuracy: 35.800\n",
            "Worker 2, [02/16]: Training Loss: 2.297724116, Training Accuracy: 39.160\n",
            "Worker 2, [03/16]: Training Loss: 2.162059215, Training Accuracy: 41.768\n",
            "Worker 2, [04/16]: Training Loss: 2.098027749, Training Accuracy: 42.936\n",
            "Worker 2, [05/16]: Training Loss: 2.022955462, Training Accuracy: 45.160\n",
            "Worker 2, [06/16]: Training Loss: 1.964977276, Training Accuracy: 45.776\n",
            "Worker 2, [07/16]: Training Loss: 1.926616753, Training Accuracy: 47.096\n",
            "Worker 2, [08/16]: Training Loss: 1.894355423, Training Accuracy: 47.456\n",
            "Worker 2, [09/16]: Training Loss: 1.838204345, Training Accuracy: 48.536\n",
            "Worker 2, [10/16]: Training Loss: 1.800009291, Training Accuracy: 49.704\n",
            "Worker 2, [11/16]: Training Loss: 1.782973723, Training Accuracy: 49.792\n",
            "Worker 2, [12/16]: Training Loss: 1.732462837, Training Accuracy: 51.032\n",
            "Worker 2, [13/16]: Training Loss: 1.697649824, Training Accuracy: 52.448\n",
            "Worker 2, [14/16]: Training Loss: 1.631127169, Training Accuracy: 53.784\n",
            "Worker 2, [15/16]: Training Loss: 1.626216539, Training Accuracy: 53.464\n",
            "Worker 2, [16/16]: Training Loss: 1.595285753, Training Accuracy: 54.736\n",
            "Time taken for training worker 2: 0:01:35.787422\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.511319828, Training Accuracy: 36.184\n",
            "Worker 3, [02/16]: Training Loss: 2.281384470, Training Accuracy: 39.408\n",
            "Worker 3, [03/16]: Training Loss: 2.194996824, Training Accuracy: 41.144\n",
            "Worker 3, [04/16]: Training Loss: 2.104593515, Training Accuracy: 43.376\n",
            "Worker 3, [05/16]: Training Loss: 2.051167911, Training Accuracy: 44.432\n",
            "Worker 3, [06/16]: Training Loss: 1.967901846, Training Accuracy: 45.576\n",
            "Worker 3, [07/16]: Training Loss: 1.937155177, Training Accuracy: 46.312\n",
            "Worker 3, [08/16]: Training Loss: 1.877021705, Training Accuracy: 47.864\n",
            "Worker 3, [09/16]: Training Loss: 1.825271442, Training Accuracy: 48.840\n",
            "Worker 3, [10/16]: Training Loss: 1.795087262, Training Accuracy: 49.744\n",
            "Worker 3, [11/16]: Training Loss: 1.755100907, Training Accuracy: 51.296\n",
            "Worker 3, [12/16]: Training Loss: 1.714686299, Training Accuracy: 51.672\n",
            "Worker 3, [13/16]: Training Loss: 1.687566634, Training Accuracy: 52.640\n",
            "Worker 3, [14/16]: Training Loss: 1.653569825, Training Accuracy: 53.384\n",
            "Worker 3, [15/16]: Training Loss: 1.596350868, Training Accuracy: 54.856\n",
            "Worker 3, [16/16]: Training Loss: 1.596589543, Training Accuracy: 54.888\n",
            "Time taken for training worker 3: 0:01:35.433540\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.509253383, Training Accuracy: 35.504\n",
            "Worker 4, [02/16]: Training Loss: 2.305850266, Training Accuracy: 39.632\n",
            "Worker 4, [03/16]: Training Loss: 2.187361803, Training Accuracy: 41.592\n",
            "Worker 4, [04/16]: Training Loss: 2.124428973, Training Accuracy: 42.928\n",
            "Worker 4, [05/16]: Training Loss: 2.053652502, Training Accuracy: 44.632\n",
            "Worker 4, [06/16]: Training Loss: 1.977096095, Training Accuracy: 45.232\n",
            "Worker 4, [07/16]: Training Loss: 1.948265055, Training Accuracy: 46.024\n",
            "Worker 4, [08/16]: Training Loss: 1.902183175, Training Accuracy: 47.512\n",
            "Worker 4, [09/16]: Training Loss: 1.844111596, Training Accuracy: 49.544\n",
            "Worker 4, [10/16]: Training Loss: 1.815678297, Training Accuracy: 49.312\n",
            "Worker 4, [11/16]: Training Loss: 1.758988755, Training Accuracy: 51.200\n",
            "Worker 4, [12/16]: Training Loss: 1.736022965, Training Accuracy: 51.632\n",
            "Worker 4, [13/16]: Training Loss: 1.704434045, Training Accuracy: 52.280\n",
            "Worker 4, [14/16]: Training Loss: 1.657633272, Training Accuracy: 53.368\n",
            "Worker 4, [15/16]: Training Loss: 1.628359312, Training Accuracy: 53.752\n",
            "Worker 4, [16/16]: Training Loss: 1.594501217, Training Accuracy: 55.320\n",
            "Time taken for training worker 4: 0:01:34.583169\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002857\n",
            "Global Update 03: Test Loss: 3.105949314, Test Accuracy: 43.770\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.212996429, Training Accuracy: 42.248\n",
            "Worker 1, [02/16]: Training Loss: 1.952891556, Training Accuracy: 46.872\n",
            "Worker 1, [03/16]: Training Loss: 1.812460052, Training Accuracy: 49.952\n",
            "Worker 1, [04/16]: Training Loss: 1.710130584, Training Accuracy: 52.312\n",
            "Worker 1, [05/16]: Training Loss: 1.665482870, Training Accuracy: 53.472\n",
            "Worker 1, [06/16]: Training Loss: 1.613641923, Training Accuracy: 54.152\n",
            "Worker 1, [07/16]: Training Loss: 1.553722376, Training Accuracy: 56.424\n",
            "Worker 1, [08/16]: Training Loss: 1.488476560, Training Accuracy: 57.408\n",
            "Worker 1, [09/16]: Training Loss: 1.475653431, Training Accuracy: 57.904\n",
            "Worker 1, [10/16]: Training Loss: 1.411542047, Training Accuracy: 59.232\n",
            "Worker 1, [11/16]: Training Loss: 1.384477227, Training Accuracy: 60.016\n",
            "Worker 1, [12/16]: Training Loss: 1.381459969, Training Accuracy: 59.904\n",
            "Worker 1, [13/16]: Training Loss: 1.339930808, Training Accuracy: 61.472\n",
            "Worker 1, [14/16]: Training Loss: 1.301334817, Training Accuracy: 61.760\n",
            "Worker 1, [15/16]: Training Loss: 1.282367450, Training Accuracy: 62.808\n",
            "Worker 1, [16/16]: Training Loss: 1.242718873, Training Accuracy: 63.712\n",
            "Time taken for training worker 1: 0:01:40.988987\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.231131042, Training Accuracy: 41.552\n",
            "Worker 2, [02/16]: Training Loss: 1.950595878, Training Accuracy: 47.176\n",
            "Worker 2, [03/16]: Training Loss: 1.839404930, Training Accuracy: 48.944\n",
            "Worker 2, [04/16]: Training Loss: 1.737025947, Training Accuracy: 51.544\n",
            "Worker 2, [05/16]: Training Loss: 1.677167782, Training Accuracy: 53.384\n",
            "Worker 2, [06/16]: Training Loss: 1.591439521, Training Accuracy: 54.784\n",
            "Worker 2, [07/16]: Training Loss: 1.526958284, Training Accuracy: 56.728\n",
            "Worker 2, [08/16]: Training Loss: 1.515490522, Training Accuracy: 56.576\n",
            "Worker 2, [09/16]: Training Loss: 1.493683016, Training Accuracy: 56.720\n",
            "Worker 2, [10/16]: Training Loss: 1.446834672, Training Accuracy: 58.384\n",
            "Worker 2, [11/16]: Training Loss: 1.430275583, Training Accuracy: 58.704\n",
            "Worker 2, [12/16]: Training Loss: 1.372105768, Training Accuracy: 60.456\n",
            "Worker 2, [13/16]: Training Loss: 1.357038765, Training Accuracy: 60.528\n",
            "Worker 2, [14/16]: Training Loss: 1.327346480, Training Accuracy: 61.128\n",
            "Worker 2, [15/16]: Training Loss: 1.280144641, Training Accuracy: 63.032\n",
            "Worker 2, [16/16]: Training Loss: 1.271007583, Training Accuracy: 62.992\n",
            "Time taken for training worker 2: 0:01:37.040627\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.249519188, Training Accuracy: 41.512\n",
            "Worker 3, [02/16]: Training Loss: 1.963339042, Training Accuracy: 47.056\n",
            "Worker 3, [03/16]: Training Loss: 1.816231116, Training Accuracy: 50.104\n",
            "Worker 3, [04/16]: Training Loss: 1.738635782, Training Accuracy: 51.720\n",
            "Worker 3, [05/16]: Training Loss: 1.675622518, Training Accuracy: 52.800\n",
            "Worker 3, [06/16]: Training Loss: 1.602112492, Training Accuracy: 54.904\n",
            "Worker 3, [07/16]: Training Loss: 1.548185494, Training Accuracy: 56.264\n",
            "Worker 3, [08/16]: Training Loss: 1.497738493, Training Accuracy: 57.064\n",
            "Worker 3, [09/16]: Training Loss: 1.459687015, Training Accuracy: 57.912\n",
            "Worker 3, [10/16]: Training Loss: 1.435605633, Training Accuracy: 58.480\n",
            "Worker 3, [11/16]: Training Loss: 1.422520229, Training Accuracy: 59.168\n",
            "Worker 3, [12/16]: Training Loss: 1.366807364, Training Accuracy: 60.520\n",
            "Worker 3, [13/16]: Training Loss: 1.313617021, Training Accuracy: 61.736\n",
            "Worker 3, [14/16]: Training Loss: 1.318246189, Training Accuracy: 60.944\n",
            "Worker 3, [15/16]: Training Loss: 1.267063599, Training Accuracy: 63.320\n",
            "Worker 3, [16/16]: Training Loss: 1.272226578, Training Accuracy: 62.776\n",
            "Time taken for training worker 3: 0:01:36.494954\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.255173691, Training Accuracy: 41.472\n",
            "Worker 4, [02/16]: Training Loss: 1.977310209, Training Accuracy: 46.800\n",
            "Worker 4, [03/16]: Training Loss: 1.850481895, Training Accuracy: 49.848\n",
            "Worker 4, [04/16]: Training Loss: 1.747067849, Training Accuracy: 51.712\n",
            "Worker 4, [05/16]: Training Loss: 1.680237012, Training Accuracy: 53.464\n",
            "Worker 4, [06/16]: Training Loss: 1.636957746, Training Accuracy: 54.336\n",
            "Worker 4, [07/16]: Training Loss: 1.580732663, Training Accuracy: 55.720\n",
            "Worker 4, [08/16]: Training Loss: 1.527575317, Training Accuracy: 55.968\n",
            "Worker 4, [09/16]: Training Loss: 1.483943842, Training Accuracy: 57.336\n",
            "Worker 4, [10/16]: Training Loss: 1.448113608, Training Accuracy: 58.520\n",
            "Worker 4, [11/16]: Training Loss: 1.430038874, Training Accuracy: 59.384\n",
            "Worker 4, [12/16]: Training Loss: 1.372633804, Training Accuracy: 60.552\n",
            "Worker 4, [13/16]: Training Loss: 1.344435719, Training Accuracy: 61.160\n",
            "Worker 4, [14/16]: Training Loss: 1.315204890, Training Accuracy: 62.192\n",
            "Worker 4, [15/16]: Training Loss: 1.293556112, Training Accuracy: 62.384\n",
            "Worker 4, [16/16]: Training Loss: 1.277413907, Training Accuracy: 62.928\n",
            "Time taken for training worker 4: 0:01:36.592537\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003390\n",
            "Global Update 04: Test Loss: 2.374432668, Test Accuracy: 46.180\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.866345091, Training Accuracy: 49.296\n",
            "Worker 1, [02/16]: Training Loss: 1.644436800, Training Accuracy: 54.504\n",
            "Worker 1, [03/16]: Training Loss: 1.507060224, Training Accuracy: 57.056\n",
            "Worker 1, [04/16]: Training Loss: 1.431408282, Training Accuracy: 59.264\n",
            "Worker 1, [05/16]: Training Loss: 1.338078620, Training Accuracy: 61.696\n",
            "Worker 1, [06/16]: Training Loss: 1.284182154, Training Accuracy: 62.280\n",
            "Worker 1, [07/16]: Training Loss: 1.244606705, Training Accuracy: 63.704\n",
            "Worker 1, [08/16]: Training Loss: 1.195666956, Training Accuracy: 64.864\n",
            "Worker 1, [09/16]: Training Loss: 1.147762573, Training Accuracy: 66.728\n",
            "Worker 1, [10/16]: Training Loss: 1.109893451, Training Accuracy: 67.528\n",
            "Worker 1, [11/16]: Training Loss: 1.096038054, Training Accuracy: 67.424\n",
            "Worker 1, [12/16]: Training Loss: 1.086874829, Training Accuracy: 68.016\n",
            "Worker 1, [13/16]: Training Loss: 1.058723953, Training Accuracy: 68.472\n",
            "Worker 1, [14/16]: Training Loss: 1.018042156, Training Accuracy: 69.840\n",
            "Worker 1, [15/16]: Training Loss: 0.965523449, Training Accuracy: 71.112\n",
            "Worker 1, [16/16]: Training Loss: 0.999754163, Training Accuracy: 70.624\n",
            "Time taken for training worker 1: 0:01:42.805140\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.891055197, Training Accuracy: 48.792\n",
            "Worker 2, [02/16]: Training Loss: 1.663958459, Training Accuracy: 53.424\n",
            "Worker 2, [03/16]: Training Loss: 1.527177119, Training Accuracy: 56.288\n",
            "Worker 2, [04/16]: Training Loss: 1.431080136, Training Accuracy: 58.656\n",
            "Worker 2, [05/16]: Training Loss: 1.357905270, Training Accuracy: 60.776\n",
            "Worker 2, [06/16]: Training Loss: 1.282758485, Training Accuracy: 62.368\n",
            "Worker 2, [07/16]: Training Loss: 1.258747093, Training Accuracy: 62.824\n",
            "Worker 2, [08/16]: Training Loss: 1.191890661, Training Accuracy: 64.976\n",
            "Worker 2, [09/16]: Training Loss: 1.165537848, Training Accuracy: 65.472\n",
            "Worker 2, [10/16]: Training Loss: 1.131643699, Training Accuracy: 66.536\n",
            "Worker 2, [11/16]: Training Loss: 1.129878803, Training Accuracy: 66.864\n",
            "Worker 2, [12/16]: Training Loss: 1.069002142, Training Accuracy: 68.048\n",
            "Worker 2, [13/16]: Training Loss: 1.055286047, Training Accuracy: 68.960\n",
            "Worker 2, [14/16]: Training Loss: 1.020196817, Training Accuracy: 68.984\n",
            "Worker 2, [15/16]: Training Loss: 1.009941707, Training Accuracy: 69.832\n",
            "Worker 2, [16/16]: Training Loss: 0.974676394, Training Accuracy: 71.336\n",
            "Time taken for training worker 2: 0:01:34.334209\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.882427818, Training Accuracy: 48.952\n",
            "Worker 3, [02/16]: Training Loss: 1.650471549, Training Accuracy: 54.304\n",
            "Worker 3, [03/16]: Training Loss: 1.516225502, Training Accuracy: 57.056\n",
            "Worker 3, [04/16]: Training Loss: 1.418682657, Training Accuracy: 59.912\n",
            "Worker 3, [05/16]: Training Loss: 1.349990682, Training Accuracy: 60.736\n",
            "Worker 3, [06/16]: Training Loss: 1.319647276, Training Accuracy: 61.984\n",
            "Worker 3, [07/16]: Training Loss: 1.249107455, Training Accuracy: 64.152\n",
            "Worker 3, [08/16]: Training Loss: 1.228653879, Training Accuracy: 64.072\n",
            "Worker 3, [09/16]: Training Loss: 1.149167854, Training Accuracy: 66.368\n",
            "Worker 3, [10/16]: Training Loss: 1.145548325, Training Accuracy: 66.232\n",
            "Worker 3, [11/16]: Training Loss: 1.090755975, Training Accuracy: 67.952\n",
            "Worker 3, [12/16]: Training Loss: 1.053947270, Training Accuracy: 68.984\n",
            "Worker 3, [13/16]: Training Loss: 1.039553294, Training Accuracy: 69.032\n",
            "Worker 3, [14/16]: Training Loss: 1.019111695, Training Accuracy: 69.728\n",
            "Worker 3, [15/16]: Training Loss: 1.016376736, Training Accuracy: 69.192\n",
            "Worker 3, [16/16]: Training Loss: 0.980451152, Training Accuracy: 71.208\n",
            "Time taken for training worker 3: 0:01:34.265194\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.887193207, Training Accuracy: 49.304\n",
            "Worker 4, [02/16]: Training Loss: 1.668657790, Training Accuracy: 53.464\n",
            "Worker 4, [03/16]: Training Loss: 1.530050176, Training Accuracy: 56.504\n",
            "Worker 4, [04/16]: Training Loss: 1.455763118, Training Accuracy: 58.640\n",
            "Worker 4, [05/16]: Training Loss: 1.370653193, Training Accuracy: 60.720\n",
            "Worker 4, [06/16]: Training Loss: 1.310472302, Training Accuracy: 62.128\n",
            "Worker 4, [07/16]: Training Loss: 1.264838875, Training Accuracy: 63.320\n",
            "Worker 4, [08/16]: Training Loss: 1.212511424, Training Accuracy: 64.872\n",
            "Worker 4, [09/16]: Training Loss: 1.176245171, Training Accuracy: 65.784\n",
            "Worker 4, [10/16]: Training Loss: 1.153054988, Training Accuracy: 66.304\n",
            "Worker 4, [11/16]: Training Loss: 1.101007154, Training Accuracy: 67.456\n",
            "Worker 4, [12/16]: Training Loss: 1.099664211, Training Accuracy: 67.424\n",
            "Worker 4, [13/16]: Training Loss: 1.060680706, Training Accuracy: 69.008\n",
            "Worker 4, [14/16]: Training Loss: 1.027070426, Training Accuracy: 69.616\n",
            "Worker 4, [15/16]: Training Loss: 1.014578121, Training Accuracy: 69.776\n",
            "Worker 4, [16/16]: Training Loss: 1.007185830, Training Accuracy: 70.176\n",
            "Time taken for training worker 4: 0:01:34.655959\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002895\n",
            "Global Update 05: Test Loss: 2.174225692, Test Accuracy: 47.160\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.613142382, Training Accuracy: 54.576\n",
            "Worker 1, [02/16]: Training Loss: 1.390187594, Training Accuracy: 60.168\n",
            "Worker 1, [03/16]: Training Loss: 1.256482682, Training Accuracy: 63.640\n",
            "Worker 1, [04/16]: Training Loss: 1.166027901, Training Accuracy: 65.720\n",
            "Worker 1, [05/16]: Training Loss: 1.079142585, Training Accuracy: 68.040\n",
            "Worker 1, [06/16]: Training Loss: 1.039752989, Training Accuracy: 69.008\n",
            "Worker 1, [07/16]: Training Loss: 0.987742401, Training Accuracy: 70.648\n",
            "Worker 1, [08/16]: Training Loss: 0.941480828, Training Accuracy: 71.576\n",
            "Worker 1, [09/16]: Training Loss: 0.918664857, Training Accuracy: 72.600\n",
            "Worker 1, [10/16]: Training Loss: 0.859755663, Training Accuracy: 73.976\n",
            "Worker 1, [11/16]: Training Loss: 0.846142582, Training Accuracy: 74.632\n",
            "Worker 1, [12/16]: Training Loss: 0.805001934, Training Accuracy: 75.360\n",
            "Worker 1, [13/16]: Training Loss: 0.789842526, Training Accuracy: 76.064\n",
            "Worker 1, [14/16]: Training Loss: 0.759914046, Training Accuracy: 76.968\n",
            "Worker 1, [15/16]: Training Loss: 0.737367640, Training Accuracy: 77.864\n",
            "Worker 1, [16/16]: Training Loss: 0.724351641, Training Accuracy: 78.040\n",
            "Time taken for training worker 1: 0:01:34.844789\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.619340394, Training Accuracy: 54.944\n",
            "Worker 2, [02/16]: Training Loss: 1.399776683, Training Accuracy: 59.352\n",
            "Worker 2, [03/16]: Training Loss: 1.282309997, Training Accuracy: 62.672\n",
            "Worker 2, [04/16]: Training Loss: 1.183442041, Training Accuracy: 65.224\n",
            "Worker 2, [05/16]: Training Loss: 1.116652117, Training Accuracy: 67.344\n",
            "Worker 2, [06/16]: Training Loss: 1.052167486, Training Accuracy: 68.952\n",
            "Worker 2, [07/16]: Training Loss: 1.005458939, Training Accuracy: 69.880\n",
            "Worker 2, [08/16]: Training Loss: 0.968749344, Training Accuracy: 71.320\n",
            "Worker 2, [09/16]: Training Loss: 0.928393705, Training Accuracy: 72.152\n",
            "Worker 2, [10/16]: Training Loss: 0.903893633, Training Accuracy: 72.680\n",
            "Worker 2, [11/16]: Training Loss: 0.860570752, Training Accuracy: 73.856\n",
            "Worker 2, [12/16]: Training Loss: 0.844882785, Training Accuracy: 74.592\n",
            "Worker 2, [13/16]: Training Loss: 0.799907499, Training Accuracy: 75.504\n",
            "Worker 2, [14/16]: Training Loss: 0.766843248, Training Accuracy: 76.944\n",
            "Worker 2, [15/16]: Training Loss: 0.735801074, Training Accuracy: 77.504\n",
            "Worker 2, [16/16]: Training Loss: 0.751475413, Training Accuracy: 76.960\n",
            "Time taken for training worker 2: 0:01:35.399798\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.608278453, Training Accuracy: 55.024\n",
            "Worker 3, [02/16]: Training Loss: 1.399152987, Training Accuracy: 59.528\n",
            "Worker 3, [03/16]: Training Loss: 1.246729684, Training Accuracy: 63.504\n",
            "Worker 3, [04/16]: Training Loss: 1.169392576, Training Accuracy: 65.888\n",
            "Worker 3, [05/16]: Training Loss: 1.107524100, Training Accuracy: 67.512\n",
            "Worker 3, [06/16]: Training Loss: 1.065038247, Training Accuracy: 68.376\n",
            "Worker 3, [07/16]: Training Loss: 0.994576271, Training Accuracy: 70.616\n",
            "Worker 3, [08/16]: Training Loss: 0.948596109, Training Accuracy: 71.800\n",
            "Worker 3, [09/16]: Training Loss: 0.913890749, Training Accuracy: 72.768\n",
            "Worker 3, [10/16]: Training Loss: 0.887253308, Training Accuracy: 73.592\n",
            "Worker 3, [11/16]: Training Loss: 0.847335019, Training Accuracy: 74.336\n",
            "Worker 3, [12/16]: Training Loss: 0.824772438, Training Accuracy: 74.856\n",
            "Worker 3, [13/16]: Training Loss: 0.807207628, Training Accuracy: 75.600\n",
            "Worker 3, [14/16]: Training Loss: 0.787871162, Training Accuracy: 76.488\n",
            "Worker 3, [15/16]: Training Loss: 0.731602148, Training Accuracy: 78.048\n",
            "Worker 3, [16/16]: Training Loss: 0.744170871, Training Accuracy: 77.224\n",
            "Time taken for training worker 3: 0:01:35.217889\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.614105772, Training Accuracy: 54.336\n",
            "Worker 4, [02/16]: Training Loss: 1.415679524, Training Accuracy: 59.528\n",
            "Worker 4, [03/16]: Training Loss: 1.273814392, Training Accuracy: 62.856\n",
            "Worker 4, [04/16]: Training Loss: 1.214010150, Training Accuracy: 65.104\n",
            "Worker 4, [05/16]: Training Loss: 1.129028005, Training Accuracy: 66.896\n",
            "Worker 4, [06/16]: Training Loss: 1.059409562, Training Accuracy: 68.960\n",
            "Worker 4, [07/16]: Training Loss: 1.005207594, Training Accuracy: 70.368\n",
            "Worker 4, [08/16]: Training Loss: 0.975240065, Training Accuracy: 71.192\n",
            "Worker 4, [09/16]: Training Loss: 0.939035246, Training Accuracy: 71.448\n",
            "Worker 4, [10/16]: Training Loss: 0.885844876, Training Accuracy: 73.688\n",
            "Worker 4, [11/16]: Training Loss: 0.856867977, Training Accuracy: 74.224\n",
            "Worker 4, [12/16]: Training Loss: 0.854070965, Training Accuracy: 74.344\n",
            "Worker 4, [13/16]: Training Loss: 0.808662902, Training Accuracy: 75.528\n",
            "Worker 4, [14/16]: Training Loss: 0.788622952, Training Accuracy: 75.968\n",
            "Worker 4, [15/16]: Training Loss: 0.781309659, Training Accuracy: 76.240\n",
            "Worker 4, [16/16]: Training Loss: 0.758286689, Training Accuracy: 76.984\n",
            "Time taken for training worker 4: 0:01:34.038313\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002708\n",
            "Global Update 06: Test Loss: 2.265587199, Test Accuracy: 48.440\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.400757923, Training Accuracy: 59.496\n",
            "Worker 1, [02/16]: Training Loss: 1.209670053, Training Accuracy: 64.896\n",
            "Worker 1, [03/16]: Training Loss: 1.073710550, Training Accuracy: 68.352\n",
            "Worker 1, [04/16]: Training Loss: 0.975772876, Training Accuracy: 71.048\n",
            "Worker 1, [05/16]: Training Loss: 0.916269312, Training Accuracy: 72.776\n",
            "Worker 1, [06/16]: Training Loss: 0.861714667, Training Accuracy: 74.504\n",
            "Worker 1, [07/16]: Training Loss: 0.804045987, Training Accuracy: 75.792\n",
            "Worker 1, [08/16]: Training Loss: 0.758081594, Training Accuracy: 76.912\n",
            "Worker 1, [09/16]: Training Loss: 0.733550211, Training Accuracy: 77.472\n",
            "Worker 1, [10/16]: Training Loss: 0.702384333, Training Accuracy: 78.688\n",
            "Worker 1, [11/16]: Training Loss: 0.644090803, Training Accuracy: 80.984\n",
            "Worker 1, [12/16]: Training Loss: 0.653585955, Training Accuracy: 80.392\n",
            "Worker 1, [13/16]: Training Loss: 0.610852322, Training Accuracy: 81.896\n",
            "Worker 1, [14/16]: Training Loss: 0.596290239, Training Accuracy: 82.600\n",
            "Worker 1, [15/16]: Training Loss: 0.576005185, Training Accuracy: 82.864\n",
            "Worker 1, [16/16]: Training Loss: 0.560321537, Training Accuracy: 82.936\n",
            "Time taken for training worker 1: 0:01:36.432327\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.442074666, Training Accuracy: 58.888\n",
            "Worker 2, [02/16]: Training Loss: 1.217817446, Training Accuracy: 64.608\n",
            "Worker 2, [03/16]: Training Loss: 1.079392284, Training Accuracy: 67.720\n",
            "Worker 2, [04/16]: Training Loss: 1.007583517, Training Accuracy: 70.608\n",
            "Worker 2, [05/16]: Training Loss: 0.927563532, Training Accuracy: 72.088\n",
            "Worker 2, [06/16]: Training Loss: 0.869249714, Training Accuracy: 74.112\n",
            "Worker 2, [07/16]: Training Loss: 0.818739011, Training Accuracy: 75.984\n",
            "Worker 2, [08/16]: Training Loss: 0.757868826, Training Accuracy: 77.832\n",
            "Worker 2, [09/16]: Training Loss: 0.733570408, Training Accuracy: 78.056\n",
            "Worker 2, [10/16]: Training Loss: 0.698291549, Training Accuracy: 79.176\n",
            "Worker 2, [11/16]: Training Loss: 0.677736501, Training Accuracy: 79.576\n",
            "Worker 2, [12/16]: Training Loss: 0.654292645, Training Accuracy: 80.352\n",
            "Worker 2, [13/16]: Training Loss: 0.630750079, Training Accuracy: 81.440\n",
            "Worker 2, [14/16]: Training Loss: 0.603212235, Training Accuracy: 81.528\n",
            "Worker 2, [15/16]: Training Loss: 0.561385926, Training Accuracy: 83.088\n",
            "Worker 2, [16/16]: Training Loss: 0.555513445, Training Accuracy: 83.344\n",
            "Time taken for training worker 2: 0:01:35.902515\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.416742524, Training Accuracy: 59.232\n",
            "Worker 3, [02/16]: Training Loss: 1.173660850, Training Accuracy: 65.352\n",
            "Worker 3, [03/16]: Training Loss: 1.078532580, Training Accuracy: 68.296\n",
            "Worker 3, [04/16]: Training Loss: 0.973710225, Training Accuracy: 71.816\n",
            "Worker 3, [05/16]: Training Loss: 0.902269966, Training Accuracy: 73.016\n",
            "Worker 3, [06/16]: Training Loss: 0.850574808, Training Accuracy: 74.512\n",
            "Worker 3, [07/16]: Training Loss: 0.801863913, Training Accuracy: 76.176\n",
            "Worker 3, [08/16]: Training Loss: 0.753504645, Training Accuracy: 77.368\n",
            "Worker 3, [09/16]: Training Loss: 0.726520122, Training Accuracy: 78.464\n",
            "Worker 3, [10/16]: Training Loss: 0.692243629, Training Accuracy: 79.416\n",
            "Worker 3, [11/16]: Training Loss: 0.653963404, Training Accuracy: 80.224\n",
            "Worker 3, [12/16]: Training Loss: 0.659634084, Training Accuracy: 80.232\n",
            "Worker 3, [13/16]: Training Loss: 0.622221792, Training Accuracy: 81.200\n",
            "Worker 3, [14/16]: Training Loss: 0.588727154, Training Accuracy: 82.312\n",
            "Worker 3, [15/16]: Training Loss: 0.573657243, Training Accuracy: 82.848\n",
            "Worker 3, [16/16]: Training Loss: 0.559967137, Training Accuracy: 83.112\n",
            "Time taken for training worker 3: 0:01:36.518046\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.434989727, Training Accuracy: 58.864\n",
            "Worker 4, [02/16]: Training Loss: 1.207501223, Training Accuracy: 64.760\n",
            "Worker 4, [03/16]: Training Loss: 1.081597235, Training Accuracy: 68.232\n",
            "Worker 4, [04/16]: Training Loss: 0.993373623, Training Accuracy: 70.944\n",
            "Worker 4, [05/16]: Training Loss: 0.919830069, Training Accuracy: 72.560\n",
            "Worker 4, [06/16]: Training Loss: 0.890655999, Training Accuracy: 73.704\n",
            "Worker 4, [07/16]: Training Loss: 0.831186989, Training Accuracy: 75.440\n",
            "Worker 4, [08/16]: Training Loss: 0.779025855, Training Accuracy: 76.688\n",
            "Worker 4, [09/16]: Training Loss: 0.743364739, Training Accuracy: 77.976\n",
            "Worker 4, [10/16]: Training Loss: 0.710167896, Training Accuracy: 79.352\n",
            "Worker 4, [11/16]: Training Loss: 0.699929234, Training Accuracy: 78.888\n",
            "Worker 4, [12/16]: Training Loss: 0.647488987, Training Accuracy: 80.616\n",
            "Worker 4, [13/16]: Training Loss: 0.630262991, Training Accuracy: 81.128\n",
            "Worker 4, [14/16]: Training Loss: 0.611234673, Training Accuracy: 81.312\n",
            "Worker 4, [15/16]: Training Loss: 0.590865772, Training Accuracy: 82.072\n",
            "Worker 4, [16/16]: Training Loss: 0.573225207, Training Accuracy: 82.720\n",
            "Time taken for training worker 4: 0:01:35.234635\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002784\n",
            "Global Update 07: Test Loss: 2.500175361, Test Accuracy: 49.640\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.257754092, Training Accuracy: 63.400\n",
            "Worker 1, [02/16]: Training Loss: 1.032132050, Training Accuracy: 68.912\n",
            "Worker 1, [03/16]: Training Loss: 0.941084561, Training Accuracy: 71.704\n",
            "Worker 1, [04/16]: Training Loss: 0.873177601, Training Accuracy: 74.272\n",
            "Worker 1, [05/16]: Training Loss: 0.803310267, Training Accuracy: 75.704\n",
            "Worker 1, [06/16]: Training Loss: 0.752461481, Training Accuracy: 77.656\n",
            "Worker 1, [07/16]: Training Loss: 0.719479334, Training Accuracy: 78.280\n",
            "Worker 1, [08/16]: Training Loss: 0.675867033, Training Accuracy: 79.400\n",
            "Worker 1, [09/16]: Training Loss: 0.641698168, Training Accuracy: 80.984\n",
            "Worker 1, [10/16]: Training Loss: 0.606012712, Training Accuracy: 81.752\n",
            "Worker 1, [11/16]: Training Loss: 0.595963802, Training Accuracy: 82.648\n",
            "Worker 1, [12/16]: Training Loss: 0.551323821, Training Accuracy: 84.072\n",
            "Worker 1, [13/16]: Training Loss: 0.551496694, Training Accuracy: 83.792\n",
            "Worker 1, [14/16]: Training Loss: 0.526918623, Training Accuracy: 84.424\n",
            "Worker 1, [15/16]: Training Loss: 0.508217712, Training Accuracy: 84.992\n",
            "Worker 1, [16/16]: Training Loss: 0.497686180, Training Accuracy: 85.048\n",
            "Time taken for training worker 1: 0:01:34.812648\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.261153795, Training Accuracy: 63.168\n",
            "Worker 2, [02/16]: Training Loss: 1.061029732, Training Accuracy: 68.152\n",
            "Worker 2, [03/16]: Training Loss: 0.953986137, Training Accuracy: 71.224\n",
            "Worker 2, [04/16]: Training Loss: 0.893372997, Training Accuracy: 73.696\n",
            "Worker 2, [05/16]: Training Loss: 0.830864548, Training Accuracy: 75.664\n",
            "Worker 2, [06/16]: Training Loss: 0.763499875, Training Accuracy: 77.640\n",
            "Worker 2, [07/16]: Training Loss: 0.711286212, Training Accuracy: 78.552\n",
            "Worker 2, [08/16]: Training Loss: 0.691739914, Training Accuracy: 79.600\n",
            "Worker 2, [09/16]: Training Loss: 0.661548119, Training Accuracy: 80.504\n",
            "Worker 2, [10/16]: Training Loss: 0.635558767, Training Accuracy: 81.528\n",
            "Worker 2, [11/16]: Training Loss: 0.596835313, Training Accuracy: 82.376\n",
            "Worker 2, [12/16]: Training Loss: 0.579022007, Training Accuracy: 82.736\n",
            "Worker 2, [13/16]: Training Loss: 0.556511564, Training Accuracy: 83.736\n",
            "Worker 2, [14/16]: Training Loss: 0.539019090, Training Accuracy: 84.456\n",
            "Worker 2, [15/16]: Training Loss: 0.508944834, Training Accuracy: 85.424\n",
            "Worker 2, [16/16]: Training Loss: 0.492399679, Training Accuracy: 85.504\n",
            "Time taken for training worker 2: 0:01:36.719293\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.274303662, Training Accuracy: 63.256\n",
            "Worker 3, [02/16]: Training Loss: 1.049765058, Training Accuracy: 68.264\n",
            "Worker 3, [03/16]: Training Loss: 0.961353743, Training Accuracy: 71.544\n",
            "Worker 3, [04/16]: Training Loss: 0.861894879, Training Accuracy: 74.352\n",
            "Worker 3, [05/16]: Training Loss: 0.805156912, Training Accuracy: 76.152\n",
            "Worker 3, [06/16]: Training Loss: 0.754817811, Training Accuracy: 77.424\n",
            "Worker 3, [07/16]: Training Loss: 0.709839041, Training Accuracy: 78.256\n",
            "Worker 3, [08/16]: Training Loss: 0.671694299, Training Accuracy: 79.720\n",
            "Worker 3, [09/16]: Training Loss: 0.633932567, Training Accuracy: 81.208\n",
            "Worker 3, [10/16]: Training Loss: 0.611932307, Training Accuracy: 81.896\n",
            "Worker 3, [11/16]: Training Loss: 0.588948758, Training Accuracy: 82.264\n",
            "Worker 3, [12/16]: Training Loss: 0.565545120, Training Accuracy: 82.976\n",
            "Worker 3, [13/16]: Training Loss: 0.541000506, Training Accuracy: 83.992\n",
            "Worker 3, [14/16]: Training Loss: 0.535175833, Training Accuracy: 84.208\n",
            "Worker 3, [15/16]: Training Loss: 0.512572104, Training Accuracy: 85.168\n",
            "Worker 3, [16/16]: Training Loss: 0.495290650, Training Accuracy: 85.776\n",
            "Time taken for training worker 3: 0:01:34.263373\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.263594335, Training Accuracy: 63.480\n",
            "Worker 4, [02/16]: Training Loss: 1.080197329, Training Accuracy: 67.856\n",
            "Worker 4, [03/16]: Training Loss: 0.959506261, Training Accuracy: 71.288\n",
            "Worker 4, [04/16]: Training Loss: 0.892978576, Training Accuracy: 73.696\n",
            "Worker 4, [05/16]: Training Loss: 0.814243574, Training Accuracy: 75.768\n",
            "Worker 4, [06/16]: Training Loss: 0.763711345, Training Accuracy: 77.088\n",
            "Worker 4, [07/16]: Training Loss: 0.728280559, Training Accuracy: 78.416\n",
            "Worker 4, [08/16]: Training Loss: 0.686893494, Training Accuracy: 79.632\n",
            "Worker 4, [09/16]: Training Loss: 0.649495952, Training Accuracy: 80.872\n",
            "Worker 4, [10/16]: Training Loss: 0.619523403, Training Accuracy: 81.360\n",
            "Worker 4, [11/16]: Training Loss: 0.604726377, Training Accuracy: 82.400\n",
            "Worker 4, [12/16]: Training Loss: 0.583112254, Training Accuracy: 82.952\n",
            "Worker 4, [13/16]: Training Loss: 0.564735064, Training Accuracy: 83.432\n",
            "Worker 4, [14/16]: Training Loss: 0.543265626, Training Accuracy: 84.280\n",
            "Worker 4, [15/16]: Training Loss: 0.529739501, Training Accuracy: 84.248\n",
            "Worker 4, [16/16]: Training Loss: 0.513703448, Training Accuracy: 84.616\n",
            "Time taken for training worker 4: 0:01:34.342366\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002710\n",
            "Global Update 08: Test Loss: 2.551319402, Test Accuracy: 49.860\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.133956998, Training Accuracy: 66.744\n",
            "Worker 1, [02/16]: Training Loss: 0.992296170, Training Accuracy: 70.464\n",
            "Worker 1, [03/16]: Training Loss: 0.928487936, Training Accuracy: 72.080\n",
            "Worker 1, [04/16]: Training Loss: 0.869149724, Training Accuracy: 73.728\n",
            "Worker 1, [05/16]: Training Loss: 0.838591979, Training Accuracy: 74.560\n",
            "Worker 1, [06/16]: Training Loss: 0.805917342, Training Accuracy: 75.544\n",
            "Worker 1, [07/16]: Training Loss: 0.771752154, Training Accuracy: 76.744\n",
            "Worker 1, [08/16]: Training Loss: 0.747617330, Training Accuracy: 77.680\n",
            "Worker 1, [09/16]: Training Loss: 0.713036406, Training Accuracy: 78.800\n",
            "Worker 1, [10/16]: Training Loss: 0.709031648, Training Accuracy: 78.776\n",
            "Worker 1, [11/16]: Training Loss: 0.676129011, Training Accuracy: 79.640\n",
            "Worker 1, [12/16]: Training Loss: 0.660847412, Training Accuracy: 80.360\n",
            "Worker 1, [13/16]: Training Loss: 0.645232373, Training Accuracy: 80.608\n",
            "Worker 1, [14/16]: Training Loss: 0.637300500, Training Accuracy: 81.064\n",
            "Worker 1, [15/16]: Training Loss: 0.599248767, Training Accuracy: 82.616\n",
            "Worker 1, [16/16]: Training Loss: 0.609486589, Training Accuracy: 82.168\n",
            "Time taken for training worker 1: 0:01:37.343176\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.161716908, Training Accuracy: 66.176\n",
            "Worker 2, [02/16]: Training Loss: 1.035970245, Training Accuracy: 68.936\n",
            "Worker 2, [03/16]: Training Loss: 0.945736605, Training Accuracy: 71.520\n",
            "Worker 2, [04/16]: Training Loss: 0.891232038, Training Accuracy: 73.096\n",
            "Worker 2, [05/16]: Training Loss: 0.862304368, Training Accuracy: 73.984\n",
            "Worker 2, [06/16]: Training Loss: 0.819444620, Training Accuracy: 75.600\n",
            "Worker 2, [07/16]: Training Loss: 0.783152532, Training Accuracy: 76.544\n",
            "Worker 2, [08/16]: Training Loss: 0.753944568, Training Accuracy: 77.496\n",
            "Worker 2, [09/16]: Training Loss: 0.732985062, Training Accuracy: 78.424\n",
            "Worker 2, [10/16]: Training Loss: 0.711190581, Training Accuracy: 79.056\n",
            "Worker 2, [11/16]: Training Loss: 0.701910148, Training Accuracy: 78.992\n",
            "Worker 2, [12/16]: Training Loss: 0.674161896, Training Accuracy: 79.760\n",
            "Worker 2, [13/16]: Training Loss: 0.651420128, Training Accuracy: 80.248\n",
            "Worker 2, [14/16]: Training Loss: 0.652224595, Training Accuracy: 80.824\n",
            "Worker 2, [15/16]: Training Loss: 0.638534706, Training Accuracy: 81.464\n",
            "Worker 2, [16/16]: Training Loss: 0.605682568, Training Accuracy: 82.336\n",
            "Time taken for training worker 2: 0:01:34.247437\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.156143902, Training Accuracy: 66.040\n",
            "Worker 3, [02/16]: Training Loss: 1.000571254, Training Accuracy: 69.336\n",
            "Worker 3, [03/16]: Training Loss: 0.927806303, Training Accuracy: 72.088\n",
            "Worker 3, [04/16]: Training Loss: 0.883949074, Training Accuracy: 72.960\n",
            "Worker 3, [05/16]: Training Loss: 0.849403151, Training Accuracy: 74.032\n",
            "Worker 3, [06/16]: Training Loss: 0.807645197, Training Accuracy: 75.264\n",
            "Worker 3, [07/16]: Training Loss: 0.787407956, Training Accuracy: 76.712\n",
            "Worker 3, [08/16]: Training Loss: 0.732952843, Training Accuracy: 78.016\n",
            "Worker 3, [09/16]: Training Loss: 0.726034945, Training Accuracy: 78.056\n",
            "Worker 3, [10/16]: Training Loss: 0.701192015, Training Accuracy: 78.864\n",
            "Worker 3, [11/16]: Training Loss: 0.693591330, Training Accuracy: 79.392\n",
            "Worker 3, [12/16]: Training Loss: 0.666001099, Training Accuracy: 80.008\n",
            "Worker 3, [13/16]: Training Loss: 0.657878707, Training Accuracy: 80.344\n",
            "Worker 3, [14/16]: Training Loss: 0.641142299, Training Accuracy: 81.248\n",
            "Worker 3, [15/16]: Training Loss: 0.628279519, Training Accuracy: 81.624\n",
            "Worker 3, [16/16]: Training Loss: 0.602656266, Training Accuracy: 82.448\n",
            "Time taken for training worker 3: 0:01:34.902431\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.156558171, Training Accuracy: 66.296\n",
            "Worker 4, [02/16]: Training Loss: 1.025467789, Training Accuracy: 69.448\n",
            "Worker 4, [03/16]: Training Loss: 0.967837838, Training Accuracy: 71.224\n",
            "Worker 4, [04/16]: Training Loss: 0.894378899, Training Accuracy: 73.368\n",
            "Worker 4, [05/16]: Training Loss: 0.848324676, Training Accuracy: 74.640\n",
            "Worker 4, [06/16]: Training Loss: 0.818695796, Training Accuracy: 75.216\n",
            "Worker 4, [07/16]: Training Loss: 0.797635540, Training Accuracy: 76.528\n",
            "Worker 4, [08/16]: Training Loss: 0.753208598, Training Accuracy: 77.184\n",
            "Worker 4, [09/16]: Training Loss: 0.740245110, Training Accuracy: 77.880\n",
            "Worker 4, [10/16]: Training Loss: 0.712646914, Training Accuracy: 78.680\n",
            "Worker 4, [11/16]: Training Loss: 0.693243378, Training Accuracy: 79.376\n",
            "Worker 4, [12/16]: Training Loss: 0.679681767, Training Accuracy: 79.784\n",
            "Worker 4, [13/16]: Training Loss: 0.652697012, Training Accuracy: 80.864\n",
            "Worker 4, [14/16]: Training Loss: 0.652953958, Training Accuracy: 80.640\n",
            "Worker 4, [15/16]: Training Loss: 0.641967221, Training Accuracy: 81.448\n",
            "Worker 4, [16/16]: Training Loss: 0.629765237, Training Accuracy: 81.656\n",
            "Time taken for training worker 4: 0:01:34.849054\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003002\n",
            "Global Update 09: Test Loss: 2.383769921, Test Accuracy: 49.880\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:57:42.275915\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:32\n",
            "==================================================\n",
            "Worker 1, [01/32]: Training Loss: 4.505800812, Training Accuracy: 2.768\n",
            "Worker 1, [02/32]: Training Loss: 4.126186834, Training Accuracy: 6.128\n",
            "Worker 1, [03/32]: Training Loss: 3.912663000, Training Accuracy: 9.632\n",
            "Worker 1, [04/32]: Training Loss: 3.752745288, Training Accuracy: 11.704\n",
            "Worker 1, [05/32]: Training Loss: 3.626770652, Training Accuracy: 13.496\n",
            "Worker 1, [06/32]: Training Loss: 3.488760460, Training Accuracy: 15.600\n",
            "Worker 1, [07/32]: Training Loss: 3.369803570, Training Accuracy: 17.928\n",
            "Worker 1, [08/32]: Training Loss: 3.248465173, Training Accuracy: 20.080\n",
            "Worker 1, [09/32]: Training Loss: 3.156691859, Training Accuracy: 22.064\n",
            "Worker 1, [10/32]: Training Loss: 3.070610450, Training Accuracy: 23.448\n",
            "Worker 1, [11/32]: Training Loss: 2.970672738, Training Accuracy: 25.592\n",
            "Worker 1, [12/32]: Training Loss: 2.906819996, Training Accuracy: 26.232\n",
            "Worker 1, [13/32]: Training Loss: 2.843393055, Training Accuracy: 28.048\n",
            "Worker 1, [14/32]: Training Loss: 2.749845032, Training Accuracy: 29.680\n",
            "Worker 1, [15/32]: Training Loss: 2.684614930, Training Accuracy: 30.736\n",
            "Worker 1, [16/32]: Training Loss: 2.639351020, Training Accuracy: 31.760\n",
            "Worker 1, [17/32]: Training Loss: 2.561442794, Training Accuracy: 33.120\n",
            "Worker 1, [18/32]: Training Loss: 2.526152253, Training Accuracy: 33.864\n",
            "Worker 1, [19/32]: Training Loss: 2.463537217, Training Accuracy: 35.368\n",
            "Worker 1, [20/32]: Training Loss: 2.390733978, Training Accuracy: 36.320\n",
            "Worker 1, [21/32]: Training Loss: 2.340289348, Training Accuracy: 37.512\n",
            "Worker 1, [22/32]: Training Loss: 2.307347989, Training Accuracy: 37.808\n",
            "Worker 1, [23/32]: Training Loss: 2.272312871, Training Accuracy: 39.264\n",
            "Worker 1, [24/32]: Training Loss: 2.238755587, Training Accuracy: 39.640\n",
            "Worker 1, [25/32]: Training Loss: 2.168080032, Training Accuracy: 41.528\n",
            "Worker 1, [26/32]: Training Loss: 2.128023803, Training Accuracy: 42.208\n",
            "Worker 1, [27/32]: Training Loss: 2.097854102, Training Accuracy: 43.240\n",
            "Worker 1, [28/32]: Training Loss: 2.063496903, Training Accuracy: 43.432\n",
            "Worker 1, [29/32]: Training Loss: 2.033833766, Training Accuracy: 44.456\n",
            "Worker 1, [30/32]: Training Loss: 1.987331410, Training Accuracy: 45.312\n",
            "Worker 1, [31/32]: Training Loss: 1.972856034, Training Accuracy: 45.896\n",
            "Worker 1, [32/32]: Training Loss: 1.944531362, Training Accuracy: 46.600\n",
            "Time taken for training worker 1: 0:03:13.196452\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 4.526799309, Training Accuracy: 2.176\n",
            "Worker 2, [02/32]: Training Loss: 4.137365935, Training Accuracy: 6.144\n",
            "Worker 2, [03/32]: Training Loss: 3.933072329, Training Accuracy: 9.016\n",
            "Worker 2, [04/32]: Training Loss: 3.797619832, Training Accuracy: 11.008\n",
            "Worker 2, [05/32]: Training Loss: 3.653089913, Training Accuracy: 13.376\n",
            "Worker 2, [06/32]: Training Loss: 3.518445784, Training Accuracy: 15.520\n",
            "Worker 2, [07/32]: Training Loss: 3.407428086, Training Accuracy: 17.456\n",
            "Worker 2, [08/32]: Training Loss: 3.323142789, Training Accuracy: 19.008\n",
            "Worker 2, [09/32]: Training Loss: 3.246192696, Training Accuracy: 20.832\n",
            "Worker 2, [10/32]: Training Loss: 3.134716059, Training Accuracy: 21.832\n",
            "Worker 2, [11/32]: Training Loss: 3.037197975, Training Accuracy: 23.944\n",
            "Worker 2, [12/32]: Training Loss: 2.976101952, Training Accuracy: 25.272\n",
            "Worker 2, [13/32]: Training Loss: 2.909477298, Training Accuracy: 26.240\n",
            "Worker 2, [14/32]: Training Loss: 2.832003924, Training Accuracy: 27.368\n",
            "Worker 2, [15/32]: Training Loss: 2.744459577, Training Accuracy: 29.536\n",
            "Worker 2, [16/32]: Training Loss: 2.698097078, Training Accuracy: 30.248\n",
            "Worker 2, [17/32]: Training Loss: 2.621972391, Training Accuracy: 31.520\n",
            "Worker 2, [18/32]: Training Loss: 2.552094152, Training Accuracy: 33.584\n",
            "Worker 2, [19/32]: Training Loss: 2.505396669, Training Accuracy: 33.928\n",
            "Worker 2, [20/32]: Training Loss: 2.447880785, Training Accuracy: 35.112\n",
            "Worker 2, [21/32]: Training Loss: 2.410582372, Training Accuracy: 36.344\n",
            "Worker 2, [22/32]: Training Loss: 2.366856017, Training Accuracy: 36.672\n",
            "Worker 2, [23/32]: Training Loss: 2.290933451, Training Accuracy: 38.800\n",
            "Worker 2, [24/32]: Training Loss: 2.264048929, Training Accuracy: 39.752\n",
            "Worker 2, [25/32]: Training Loss: 2.207242842, Training Accuracy: 40.336\n",
            "Worker 2, [26/32]: Training Loss: 2.201712824, Training Accuracy: 40.496\n",
            "Worker 2, [27/32]: Training Loss: 2.140698741, Training Accuracy: 41.728\n",
            "Worker 2, [28/32]: Training Loss: 2.093747942, Training Accuracy: 43.152\n",
            "Worker 2, [29/32]: Training Loss: 2.072782282, Training Accuracy: 43.008\n",
            "Worker 2, [30/32]: Training Loss: 2.015594043, Training Accuracy: 44.760\n",
            "Worker 2, [31/32]: Training Loss: 2.007027826, Training Accuracy: 44.520\n",
            "Worker 2, [32/32]: Training Loss: 1.971656781, Training Accuracy: 46.056\n",
            "Time taken for training worker 2: 0:03:13.119722\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 4.492927999, Training Accuracy: 2.720\n",
            "Worker 3, [02/32]: Training Loss: 4.102155103, Training Accuracy: 6.512\n",
            "Worker 3, [03/32]: Training Loss: 3.909064082, Training Accuracy: 9.048\n",
            "Worker 3, [04/32]: Training Loss: 3.756218433, Training Accuracy: 11.296\n",
            "Worker 3, [05/32]: Training Loss: 3.625012454, Training Accuracy: 13.600\n",
            "Worker 3, [06/32]: Training Loss: 3.512378284, Training Accuracy: 15.784\n",
            "Worker 3, [07/32]: Training Loss: 3.409841823, Training Accuracy: 17.000\n",
            "Worker 3, [08/32]: Training Loss: 3.289416908, Training Accuracy: 19.168\n",
            "Worker 3, [09/32]: Training Loss: 3.192517891, Training Accuracy: 21.280\n",
            "Worker 3, [10/32]: Training Loss: 3.125245813, Training Accuracy: 22.272\n",
            "Worker 3, [11/32]: Training Loss: 3.034403811, Training Accuracy: 24.216\n",
            "Worker 3, [12/32]: Training Loss: 2.947622360, Training Accuracy: 25.528\n",
            "Worker 3, [13/32]: Training Loss: 2.880659630, Training Accuracy: 26.776\n",
            "Worker 3, [14/32]: Training Loss: 2.817048024, Training Accuracy: 27.960\n",
            "Worker 3, [15/32]: Training Loss: 2.762573454, Training Accuracy: 29.240\n",
            "Worker 3, [16/32]: Training Loss: 2.676083072, Training Accuracy: 30.440\n",
            "Worker 3, [17/32]: Training Loss: 2.609974088, Training Accuracy: 32.368\n",
            "Worker 3, [18/32]: Training Loss: 2.563731829, Training Accuracy: 33.184\n",
            "Worker 3, [19/32]: Training Loss: 2.497195213, Training Accuracy: 34.480\n",
            "Worker 3, [20/32]: Training Loss: 2.455252421, Training Accuracy: 35.552\n",
            "Worker 3, [21/32]: Training Loss: 2.377914977, Training Accuracy: 36.648\n",
            "Worker 3, [22/32]: Training Loss: 2.351014714, Training Accuracy: 37.256\n",
            "Worker 3, [23/32]: Training Loss: 2.297402154, Training Accuracy: 38.248\n",
            "Worker 3, [24/32]: Training Loss: 2.257986196, Training Accuracy: 39.232\n",
            "Worker 3, [25/32]: Training Loss: 2.198926749, Training Accuracy: 40.600\n",
            "Worker 3, [26/32]: Training Loss: 2.196734405, Training Accuracy: 41.000\n",
            "Worker 3, [27/32]: Training Loss: 2.131196417, Training Accuracy: 42.096\n",
            "Worker 3, [28/32]: Training Loss: 2.087087432, Training Accuracy: 43.160\n",
            "Worker 3, [29/32]: Training Loss: 2.053262417, Training Accuracy: 44.072\n",
            "Worker 3, [30/32]: Training Loss: 2.019627320, Training Accuracy: 44.936\n",
            "Worker 3, [31/32]: Training Loss: 1.988706682, Training Accuracy: 45.208\n",
            "Worker 3, [32/32]: Training Loss: 1.939175472, Training Accuracy: 46.448\n",
            "Time taken for training worker 3: 0:03:14.119859\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 4.519110147, Training Accuracy: 2.424\n",
            "Worker 4, [02/32]: Training Loss: 4.145226890, Training Accuracy: 6.288\n",
            "Worker 4, [03/32]: Training Loss: 3.943852684, Training Accuracy: 8.944\n",
            "Worker 4, [04/32]: Training Loss: 3.770779932, Training Accuracy: 11.560\n",
            "Worker 4, [05/32]: Training Loss: 3.644060772, Training Accuracy: 13.568\n",
            "Worker 4, [06/32]: Training Loss: 3.531723430, Training Accuracy: 15.208\n",
            "Worker 4, [07/32]: Training Loss: 3.396855482, Training Accuracy: 17.560\n",
            "Worker 4, [08/32]: Training Loss: 3.300179238, Training Accuracy: 19.128\n",
            "Worker 4, [09/32]: Training Loss: 3.202610608, Training Accuracy: 21.392\n",
            "Worker 4, [10/32]: Training Loss: 3.109354952, Training Accuracy: 22.560\n",
            "Worker 4, [11/32]: Training Loss: 3.035025653, Training Accuracy: 24.144\n",
            "Worker 4, [12/32]: Training Loss: 2.956434854, Training Accuracy: 25.488\n",
            "Worker 4, [13/32]: Training Loss: 2.892739722, Training Accuracy: 26.768\n",
            "Worker 4, [14/32]: Training Loss: 2.801307430, Training Accuracy: 28.336\n",
            "Worker 4, [15/32]: Training Loss: 2.749898456, Training Accuracy: 29.280\n",
            "Worker 4, [16/32]: Training Loss: 2.673673211, Training Accuracy: 30.992\n",
            "Worker 4, [17/32]: Training Loss: 2.598650756, Training Accuracy: 32.064\n",
            "Worker 4, [18/32]: Training Loss: 2.540753247, Training Accuracy: 33.712\n",
            "Worker 4, [19/32]: Training Loss: 2.507197123, Training Accuracy: 34.528\n",
            "Worker 4, [20/32]: Training Loss: 2.438724877, Training Accuracy: 35.544\n",
            "Worker 4, [21/32]: Training Loss: 2.414177216, Training Accuracy: 36.096\n",
            "Worker 4, [22/32]: Training Loss: 2.342874396, Training Accuracy: 37.592\n",
            "Worker 4, [23/32]: Training Loss: 2.308266030, Training Accuracy: 38.520\n",
            "Worker 4, [24/32]: Training Loss: 2.272331354, Training Accuracy: 39.200\n",
            "Worker 4, [25/32]: Training Loss: 2.216839849, Training Accuracy: 40.088\n",
            "Worker 4, [26/32]: Training Loss: 2.186496368, Training Accuracy: 40.680\n",
            "Worker 4, [27/32]: Training Loss: 2.165248966, Training Accuracy: 41.536\n",
            "Worker 4, [28/32]: Training Loss: 2.114180412, Training Accuracy: 42.432\n",
            "Worker 4, [29/32]: Training Loss: 2.077042147, Training Accuracy: 43.432\n",
            "Worker 4, [30/32]: Training Loss: 2.030701756, Training Accuracy: 44.656\n",
            "Worker 4, [31/32]: Training Loss: 1.995200666, Training Accuracy: 45.384\n",
            "Worker 4, [32/32]: Training Loss: 1.958521711, Training Accuracy: 46.520\n",
            "Time taken for training worker 4: 0:03:13.439486\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002847\n",
            "Global Update 01: Test Loss: 3.482495709, Test Accuracy: 26.130\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.827330455, Training Accuracy: 28.728\n",
            "Worker 1, [02/32]: Training Loss: 2.582446814, Training Accuracy: 33.240\n",
            "Worker 1, [03/32]: Training Loss: 2.432340513, Training Accuracy: 36.064\n",
            "Worker 1, [04/32]: Training Loss: 2.366042826, Training Accuracy: 37.544\n",
            "Worker 1, [05/32]: Training Loss: 2.278870188, Training Accuracy: 38.968\n",
            "Worker 1, [06/32]: Training Loss: 2.193379759, Training Accuracy: 41.072\n",
            "Worker 1, [07/32]: Training Loss: 2.129513733, Training Accuracy: 41.848\n",
            "Worker 1, [08/32]: Training Loss: 2.088149673, Training Accuracy: 42.856\n",
            "Worker 1, [09/32]: Training Loss: 2.024514258, Training Accuracy: 44.352\n",
            "Worker 1, [10/32]: Training Loss: 1.987338699, Training Accuracy: 45.008\n",
            "Worker 1, [11/32]: Training Loss: 1.920592329, Training Accuracy: 47.152\n",
            "Worker 1, [12/32]: Training Loss: 1.880394948, Training Accuracy: 47.752\n",
            "Worker 1, [13/32]: Training Loss: 1.831848121, Training Accuracy: 49.184\n",
            "Worker 1, [14/32]: Training Loss: 1.784934600, Training Accuracy: 50.144\n",
            "Worker 1, [15/32]: Training Loss: 1.782229667, Training Accuracy: 49.656\n",
            "Worker 1, [16/32]: Training Loss: 1.714752028, Training Accuracy: 51.880\n",
            "Worker 1, [17/32]: Training Loss: 1.685819792, Training Accuracy: 52.216\n",
            "Worker 1, [18/32]: Training Loss: 1.635334832, Training Accuracy: 54.128\n",
            "Worker 1, [19/32]: Training Loss: 1.601951120, Training Accuracy: 54.552\n",
            "Worker 1, [20/32]: Training Loss: 1.564689394, Training Accuracy: 55.304\n",
            "Worker 1, [21/32]: Training Loss: 1.536881874, Training Accuracy: 56.040\n",
            "Worker 1, [22/32]: Training Loss: 1.533498682, Training Accuracy: 56.208\n",
            "Worker 1, [23/32]: Training Loss: 1.486862015, Training Accuracy: 57.360\n",
            "Worker 1, [24/32]: Training Loss: 1.450408933, Training Accuracy: 58.472\n",
            "Worker 1, [25/32]: Training Loss: 1.428680481, Training Accuracy: 58.512\n",
            "Worker 1, [26/32]: Training Loss: 1.421870710, Training Accuracy: 58.872\n",
            "Worker 1, [27/32]: Training Loss: 1.379179258, Training Accuracy: 60.248\n",
            "Worker 1, [28/32]: Training Loss: 1.376843795, Training Accuracy: 60.136\n",
            "Worker 1, [29/32]: Training Loss: 1.321468031, Training Accuracy: 60.888\n",
            "Worker 1, [30/32]: Training Loss: 1.320009377, Training Accuracy: 61.440\n",
            "Worker 1, [31/32]: Training Loss: 1.310025676, Training Accuracy: 61.880\n",
            "Worker 1, [32/32]: Training Loss: 1.292646908, Training Accuracy: 62.232\n",
            "Time taken for training worker 1: 0:03:13.543606\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.818340337, Training Accuracy: 28.672\n",
            "Worker 2, [02/32]: Training Loss: 2.566228431, Training Accuracy: 33.136\n",
            "Worker 2, [03/32]: Training Loss: 2.453637408, Training Accuracy: 35.320\n",
            "Worker 2, [04/32]: Training Loss: 2.357956583, Training Accuracy: 37.384\n",
            "Worker 2, [05/32]: Training Loss: 2.288843779, Training Accuracy: 38.816\n",
            "Worker 2, [06/32]: Training Loss: 2.199261489, Training Accuracy: 41.080\n",
            "Worker 2, [07/32]: Training Loss: 2.129854618, Training Accuracy: 42.480\n",
            "Worker 2, [08/32]: Training Loss: 2.095806554, Training Accuracy: 43.400\n",
            "Worker 2, [09/32]: Training Loss: 2.028222476, Training Accuracy: 45.000\n",
            "Worker 2, [10/32]: Training Loss: 1.981911530, Training Accuracy: 45.680\n",
            "Worker 2, [11/32]: Training Loss: 1.939970987, Training Accuracy: 46.000\n",
            "Worker 2, [12/32]: Training Loss: 1.881684536, Training Accuracy: 47.664\n",
            "Worker 2, [13/32]: Training Loss: 1.862129762, Training Accuracy: 48.536\n",
            "Worker 2, [14/32]: Training Loss: 1.787180898, Training Accuracy: 50.016\n",
            "Worker 2, [15/32]: Training Loss: 1.777730009, Training Accuracy: 49.768\n",
            "Worker 2, [16/32]: Training Loss: 1.747199586, Training Accuracy: 50.408\n",
            "Worker 2, [17/32]: Training Loss: 1.688268183, Training Accuracy: 52.392\n",
            "Worker 2, [18/32]: Training Loss: 1.669401186, Training Accuracy: 52.672\n",
            "Worker 2, [19/32]: Training Loss: 1.621602775, Training Accuracy: 54.096\n",
            "Worker 2, [20/32]: Training Loss: 1.592844554, Training Accuracy: 54.224\n",
            "Worker 2, [21/32]: Training Loss: 1.579599118, Training Accuracy: 54.992\n",
            "Worker 2, [22/32]: Training Loss: 1.512216097, Training Accuracy: 56.808\n",
            "Worker 2, [23/32]: Training Loss: 1.497061219, Training Accuracy: 57.144\n",
            "Worker 2, [24/32]: Training Loss: 1.457500050, Training Accuracy: 57.984\n",
            "Worker 2, [25/32]: Training Loss: 1.476344187, Training Accuracy: 57.680\n",
            "Worker 2, [26/32]: Training Loss: 1.436303893, Training Accuracy: 58.992\n",
            "Worker 2, [27/32]: Training Loss: 1.405022135, Training Accuracy: 59.176\n",
            "Worker 2, [28/32]: Training Loss: 1.392721398, Training Accuracy: 60.016\n",
            "Worker 2, [29/32]: Training Loss: 1.356738882, Training Accuracy: 61.024\n",
            "Worker 2, [30/32]: Training Loss: 1.335516182, Training Accuracy: 61.272\n",
            "Worker 2, [31/32]: Training Loss: 1.318475644, Training Accuracy: 61.520\n",
            "Worker 2, [32/32]: Training Loss: 1.296217031, Training Accuracy: 61.856\n",
            "Time taken for training worker 2: 0:03:13.564527\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 2.830506944, Training Accuracy: 28.312\n",
            "Worker 3, [02/32]: Training Loss: 2.568851276, Training Accuracy: 33.456\n",
            "Worker 3, [03/32]: Training Loss: 2.458434418, Training Accuracy: 34.824\n",
            "Worker 3, [04/32]: Training Loss: 2.345020881, Training Accuracy: 37.976\n",
            "Worker 3, [05/32]: Training Loss: 2.286097427, Training Accuracy: 38.952\n",
            "Worker 3, [06/32]: Training Loss: 2.199838477, Training Accuracy: 40.608\n",
            "Worker 3, [07/32]: Training Loss: 2.155540362, Training Accuracy: 42.248\n",
            "Worker 3, [08/32]: Training Loss: 2.091060591, Training Accuracy: 43.408\n",
            "Worker 3, [09/32]: Training Loss: 2.039692787, Training Accuracy: 44.672\n",
            "Worker 3, [10/32]: Training Loss: 2.001765231, Training Accuracy: 44.928\n",
            "Worker 3, [11/32]: Training Loss: 1.935916303, Training Accuracy: 46.400\n",
            "Worker 3, [12/32]: Training Loss: 1.870304388, Training Accuracy: 48.592\n",
            "Worker 3, [13/32]: Training Loss: 1.844495643, Training Accuracy: 49.296\n",
            "Worker 3, [14/32]: Training Loss: 1.809193630, Training Accuracy: 49.448\n",
            "Worker 3, [15/32]: Training Loss: 1.780438486, Training Accuracy: 50.256\n",
            "Worker 3, [16/32]: Training Loss: 1.727237642, Training Accuracy: 51.304\n",
            "Worker 3, [17/32]: Training Loss: 1.679369032, Training Accuracy: 52.192\n",
            "Worker 3, [18/32]: Training Loss: 1.657918029, Training Accuracy: 53.392\n",
            "Worker 3, [19/32]: Training Loss: 1.611364000, Training Accuracy: 53.984\n",
            "Worker 3, [20/32]: Training Loss: 1.580121598, Training Accuracy: 55.208\n",
            "Worker 3, [21/32]: Training Loss: 1.581605004, Training Accuracy: 54.888\n",
            "Worker 3, [22/32]: Training Loss: 1.547411084, Training Accuracy: 56.112\n",
            "Worker 3, [23/32]: Training Loss: 1.503122062, Training Accuracy: 56.848\n",
            "Worker 3, [24/32]: Training Loss: 1.481995067, Training Accuracy: 57.664\n",
            "Worker 3, [25/32]: Training Loss: 1.452718082, Training Accuracy: 58.360\n",
            "Worker 3, [26/32]: Training Loss: 1.428156444, Training Accuracy: 58.600\n",
            "Worker 3, [27/32]: Training Loss: 1.392103897, Training Accuracy: 60.216\n",
            "Worker 3, [28/32]: Training Loss: 1.354193811, Training Accuracy: 61.048\n",
            "Worker 3, [29/32]: Training Loss: 1.338358767, Training Accuracy: 61.424\n",
            "Worker 3, [30/32]: Training Loss: 1.336102606, Training Accuracy: 61.408\n",
            "Worker 3, [31/32]: Training Loss: 1.295681013, Training Accuracy: 62.208\n",
            "Worker 3, [32/32]: Training Loss: 1.272367258, Training Accuracy: 63.096\n",
            "Time taken for training worker 3: 0:03:15.007596\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 2.830073587, Training Accuracy: 28.688\n",
            "Worker 4, [02/32]: Training Loss: 2.570843009, Training Accuracy: 33.128\n",
            "Worker 4, [03/32]: Training Loss: 2.460937130, Training Accuracy: 35.960\n",
            "Worker 4, [04/32]: Training Loss: 2.384227714, Training Accuracy: 37.464\n",
            "Worker 4, [05/32]: Training Loss: 2.308337383, Training Accuracy: 38.496\n",
            "Worker 4, [06/32]: Training Loss: 2.230291257, Training Accuracy: 40.208\n",
            "Worker 4, [07/32]: Training Loss: 2.176203633, Training Accuracy: 41.808\n",
            "Worker 4, [08/32]: Training Loss: 2.090600408, Training Accuracy: 43.336\n",
            "Worker 4, [09/32]: Training Loss: 2.065557013, Training Accuracy: 43.952\n",
            "Worker 4, [10/32]: Training Loss: 2.008072151, Training Accuracy: 45.280\n",
            "Worker 4, [11/32]: Training Loss: 1.952992072, Training Accuracy: 46.384\n",
            "Worker 4, [12/32]: Training Loss: 1.912315926, Training Accuracy: 47.704\n",
            "Worker 4, [13/32]: Training Loss: 1.888530012, Training Accuracy: 47.552\n",
            "Worker 4, [14/32]: Training Loss: 1.852521757, Training Accuracy: 48.288\n",
            "Worker 4, [15/32]: Training Loss: 1.796255053, Training Accuracy: 50.544\n",
            "Worker 4, [16/32]: Training Loss: 1.758939137, Training Accuracy: 50.464\n",
            "Worker 4, [17/32]: Training Loss: 1.710377826, Training Accuracy: 51.760\n",
            "Worker 4, [18/32]: Training Loss: 1.676011346, Training Accuracy: 52.328\n",
            "Worker 4, [19/32]: Training Loss: 1.635407174, Training Accuracy: 54.000\n",
            "Worker 4, [20/32]: Training Loss: 1.620606374, Training Accuracy: 54.128\n",
            "Worker 4, [21/32]: Training Loss: 1.595787006, Training Accuracy: 54.432\n",
            "Worker 4, [22/32]: Training Loss: 1.584492192, Training Accuracy: 54.544\n",
            "Worker 4, [23/32]: Training Loss: 1.529217704, Training Accuracy: 56.040\n",
            "Worker 4, [24/32]: Training Loss: 1.483559437, Training Accuracy: 57.280\n",
            "Worker 4, [25/32]: Training Loss: 1.450286313, Training Accuracy: 58.352\n",
            "Worker 4, [26/32]: Training Loss: 1.436884221, Training Accuracy: 58.648\n",
            "Worker 4, [27/32]: Training Loss: 1.406430989, Training Accuracy: 59.832\n",
            "Worker 4, [28/32]: Training Loss: 1.388149009, Training Accuracy: 60.128\n",
            "Worker 4, [29/32]: Training Loss: 1.368244138, Training Accuracy: 60.728\n",
            "Worker 4, [30/32]: Training Loss: 1.385648123, Training Accuracy: 60.192\n",
            "Worker 4, [31/32]: Training Loss: 1.333845898, Training Accuracy: 61.192\n",
            "Worker 4, [32/32]: Training Loss: 1.315710407, Training Accuracy: 61.768\n",
            "Time taken for training worker 4: 0:03:15.912873\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004827\n",
            "Global Update 02: Test Loss: 3.259520367, Test Accuracy: 41.010\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.225811037, Training Accuracy: 42.152\n",
            "Worker 1, [02/32]: Training Loss: 1.907335834, Training Accuracy: 47.984\n",
            "Worker 1, [03/32]: Training Loss: 1.749791968, Training Accuracy: 51.992\n",
            "Worker 1, [04/32]: Training Loss: 1.626008462, Training Accuracy: 54.480\n",
            "Worker 1, [05/32]: Training Loss: 1.530581457, Training Accuracy: 56.832\n",
            "Worker 1, [06/32]: Training Loss: 1.454323030, Training Accuracy: 57.968\n",
            "Worker 1, [07/32]: Training Loss: 1.383989291, Training Accuracy: 60.208\n",
            "Worker 1, [08/32]: Training Loss: 1.342031799, Training Accuracy: 61.328\n",
            "Worker 1, [09/32]: Training Loss: 1.244184000, Training Accuracy: 63.048\n",
            "Worker 1, [10/32]: Training Loss: 1.238991717, Training Accuracy: 63.496\n",
            "Worker 1, [11/32]: Training Loss: 1.185016535, Training Accuracy: 65.368\n",
            "Worker 1, [12/32]: Training Loss: 1.145514953, Training Accuracy: 66.112\n",
            "Worker 1, [13/32]: Training Loss: 1.113469907, Training Accuracy: 66.480\n",
            "Worker 1, [14/32]: Training Loss: 1.099857052, Training Accuracy: 67.688\n",
            "Worker 1, [15/32]: Training Loss: 1.031439415, Training Accuracy: 69.136\n",
            "Worker 1, [16/32]: Training Loss: 1.005143000, Training Accuracy: 70.120\n",
            "Worker 1, [17/32]: Training Loss: 0.986001745, Training Accuracy: 70.408\n",
            "Worker 1, [18/32]: Training Loss: 0.968616958, Training Accuracy: 70.968\n",
            "Worker 1, [19/32]: Training Loss: 0.936464796, Training Accuracy: 72.240\n",
            "Worker 1, [20/32]: Training Loss: 0.892624159, Training Accuracy: 72.800\n",
            "Worker 1, [21/32]: Training Loss: 0.905724376, Training Accuracy: 72.816\n",
            "Worker 1, [22/32]: Training Loss: 0.879492349, Training Accuracy: 73.096\n",
            "Worker 1, [23/32]: Training Loss: 0.843873111, Training Accuracy: 74.464\n",
            "Worker 1, [24/32]: Training Loss: 0.852334543, Training Accuracy: 74.280\n",
            "Worker 1, [25/32]: Training Loss: 0.827154759, Training Accuracy: 74.992\n",
            "Worker 1, [26/32]: Training Loss: 0.801729068, Training Accuracy: 75.704\n",
            "Worker 1, [27/32]: Training Loss: 0.797182559, Training Accuracy: 75.856\n",
            "Worker 1, [28/32]: Training Loss: 0.797905928, Training Accuracy: 75.728\n",
            "Worker 1, [29/32]: Training Loss: 0.770805502, Training Accuracy: 75.792\n",
            "Worker 1, [30/32]: Training Loss: 0.745785252, Training Accuracy: 77.344\n",
            "Worker 1, [31/32]: Training Loss: 0.722103297, Training Accuracy: 77.632\n",
            "Worker 1, [32/32]: Training Loss: 0.712929268, Training Accuracy: 78.688\n",
            "Time taken for training worker 1: 0:03:11.652462\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.250232653, Training Accuracy: 42.112\n",
            "Worker 2, [02/32]: Training Loss: 1.928272874, Training Accuracy: 46.880\n",
            "Worker 2, [03/32]: Training Loss: 1.786566864, Training Accuracy: 50.856\n",
            "Worker 2, [04/32]: Training Loss: 1.654544909, Training Accuracy: 53.784\n",
            "Worker 2, [05/32]: Training Loss: 1.563446552, Training Accuracy: 55.688\n",
            "Worker 2, [06/32]: Training Loss: 1.472988560, Training Accuracy: 58.032\n",
            "Worker 2, [07/32]: Training Loss: 1.396781357, Training Accuracy: 59.896\n",
            "Worker 2, [08/32]: Training Loss: 1.362223227, Training Accuracy: 60.992\n",
            "Worker 2, [09/32]: Training Loss: 1.310387232, Training Accuracy: 61.560\n",
            "Worker 2, [10/32]: Training Loss: 1.239920674, Training Accuracy: 63.560\n",
            "Worker 2, [11/32]: Training Loss: 1.206296067, Training Accuracy: 64.920\n",
            "Worker 2, [12/32]: Training Loss: 1.184075421, Training Accuracy: 65.488\n",
            "Worker 2, [13/32]: Training Loss: 1.132617917, Training Accuracy: 66.320\n",
            "Worker 2, [14/32]: Training Loss: 1.102921771, Training Accuracy: 67.040\n",
            "Worker 2, [15/32]: Training Loss: 1.060216709, Training Accuracy: 68.704\n",
            "Worker 2, [16/32]: Training Loss: 1.052628948, Training Accuracy: 68.704\n",
            "Worker 2, [17/32]: Training Loss: 1.008023008, Training Accuracy: 70.232\n",
            "Worker 2, [18/32]: Training Loss: 0.977927281, Training Accuracy: 70.672\n",
            "Worker 2, [19/32]: Training Loss: 0.942544503, Training Accuracy: 71.952\n",
            "Worker 2, [20/32]: Training Loss: 0.923799784, Training Accuracy: 71.704\n",
            "Worker 2, [21/32]: Training Loss: 0.921017469, Training Accuracy: 72.328\n",
            "Worker 2, [22/32]: Training Loss: 0.881249473, Training Accuracy: 73.608\n",
            "Worker 2, [23/32]: Training Loss: 0.879074930, Training Accuracy: 73.280\n",
            "Worker 2, [24/32]: Training Loss: 0.859200942, Training Accuracy: 74.040\n",
            "Worker 2, [25/32]: Training Loss: 0.848518720, Training Accuracy: 74.256\n",
            "Worker 2, [26/32]: Training Loss: 0.810348713, Training Accuracy: 74.968\n",
            "Worker 2, [27/32]: Training Loss: 0.779606214, Training Accuracy: 76.600\n",
            "Worker 2, [28/32]: Training Loss: 0.785456568, Training Accuracy: 76.400\n",
            "Worker 2, [29/32]: Training Loss: 0.765616666, Training Accuracy: 76.496\n",
            "Worker 2, [30/32]: Training Loss: 0.765006985, Training Accuracy: 76.984\n",
            "Worker 2, [31/32]: Training Loss: 0.727770637, Training Accuracy: 77.280\n",
            "Worker 2, [32/32]: Training Loss: 0.740687075, Training Accuracy: 77.344\n",
            "Time taken for training worker 2: 0:03:15.379873\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 2.230946371, Training Accuracy: 42.096\n",
            "Worker 3, [02/32]: Training Loss: 1.927492748, Training Accuracy: 48.240\n",
            "Worker 3, [03/32]: Training Loss: 1.771193583, Training Accuracy: 51.000\n",
            "Worker 3, [04/32]: Training Loss: 1.681013239, Training Accuracy: 53.456\n",
            "Worker 3, [05/32]: Training Loss: 1.569821512, Training Accuracy: 55.008\n",
            "Worker 3, [06/32]: Training Loss: 1.481697831, Training Accuracy: 58.152\n",
            "Worker 3, [07/32]: Training Loss: 1.422056090, Training Accuracy: 59.160\n",
            "Worker 3, [08/32]: Training Loss: 1.357073317, Training Accuracy: 61.256\n",
            "Worker 3, [09/32]: Training Loss: 1.291289833, Training Accuracy: 62.520\n",
            "Worker 3, [10/32]: Training Loss: 1.245355193, Training Accuracy: 63.320\n",
            "Worker 3, [11/32]: Training Loss: 1.196912780, Training Accuracy: 64.688\n",
            "Worker 3, [12/32]: Training Loss: 1.157820978, Training Accuracy: 66.136\n",
            "Worker 3, [13/32]: Training Loss: 1.120249239, Training Accuracy: 67.392\n",
            "Worker 3, [14/32]: Training Loss: 1.087220267, Training Accuracy: 67.720\n",
            "Worker 3, [15/32]: Training Loss: 1.043063328, Training Accuracy: 69.336\n",
            "Worker 3, [16/32]: Training Loss: 1.018562678, Training Accuracy: 69.928\n",
            "Worker 3, [17/32]: Training Loss: 1.001159039, Training Accuracy: 69.928\n",
            "Worker 3, [18/32]: Training Loss: 0.969311137, Training Accuracy: 71.088\n",
            "Worker 3, [19/32]: Training Loss: 0.949028140, Training Accuracy: 71.392\n",
            "Worker 3, [20/32]: Training Loss: 0.926581764, Training Accuracy: 72.200\n",
            "Worker 3, [21/32]: Training Loss: 0.905518381, Training Accuracy: 73.128\n",
            "Worker 3, [22/32]: Training Loss: 0.879952116, Training Accuracy: 73.768\n",
            "Worker 3, [23/32]: Training Loss: 0.856795979, Training Accuracy: 74.080\n",
            "Worker 3, [24/32]: Training Loss: 0.834567390, Training Accuracy: 74.616\n",
            "Worker 3, [25/32]: Training Loss: 0.809934381, Training Accuracy: 75.312\n",
            "Worker 3, [26/32]: Training Loss: 0.809078129, Training Accuracy: 75.408\n",
            "Worker 3, [27/32]: Training Loss: 0.788979982, Training Accuracy: 75.944\n",
            "Worker 3, [28/32]: Training Loss: 0.779509463, Training Accuracy: 76.248\n",
            "Worker 3, [29/32]: Training Loss: 0.760220014, Training Accuracy: 76.832\n",
            "Worker 3, [30/32]: Training Loss: 0.763718242, Training Accuracy: 77.328\n",
            "Worker 3, [31/32]: Training Loss: 0.755097404, Training Accuracy: 77.072\n",
            "Worker 3, [32/32]: Training Loss: 0.721598608, Training Accuracy: 78.096\n",
            "Time taken for training worker 3: 0:03:13.179401\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 2.251760677, Training Accuracy: 42.120\n",
            "Worker 4, [02/32]: Training Loss: 1.946700278, Training Accuracy: 47.648\n",
            "Worker 4, [03/32]: Training Loss: 1.782367113, Training Accuracy: 50.688\n",
            "Worker 4, [04/32]: Training Loss: 1.672420717, Training Accuracy: 53.752\n",
            "Worker 4, [05/32]: Training Loss: 1.568022584, Training Accuracy: 55.728\n",
            "Worker 4, [06/32]: Training Loss: 1.495534495, Training Accuracy: 57.688\n",
            "Worker 4, [07/32]: Training Loss: 1.427679808, Training Accuracy: 59.320\n",
            "Worker 4, [08/32]: Training Loss: 1.353129542, Training Accuracy: 61.136\n",
            "Worker 4, [09/32]: Training Loss: 1.300418209, Training Accuracy: 62.032\n",
            "Worker 4, [10/32]: Training Loss: 1.255561975, Training Accuracy: 63.544\n",
            "Worker 4, [11/32]: Training Loss: 1.209211427, Training Accuracy: 65.088\n",
            "Worker 4, [12/32]: Training Loss: 1.172889573, Training Accuracy: 65.480\n",
            "Worker 4, [13/32]: Training Loss: 1.129940052, Training Accuracy: 66.528\n",
            "Worker 4, [14/32]: Training Loss: 1.093043825, Training Accuracy: 67.832\n",
            "Worker 4, [15/32]: Training Loss: 1.050755547, Training Accuracy: 69.200\n",
            "Worker 4, [16/32]: Training Loss: 1.045946659, Training Accuracy: 68.552\n",
            "Worker 4, [17/32]: Training Loss: 1.001697989, Training Accuracy: 69.776\n",
            "Worker 4, [18/32]: Training Loss: 0.967386953, Training Accuracy: 70.936\n",
            "Worker 4, [19/32]: Training Loss: 0.969614319, Training Accuracy: 70.720\n",
            "Worker 4, [20/32]: Training Loss: 0.947704897, Training Accuracy: 71.888\n",
            "Worker 4, [21/32]: Training Loss: 0.919330946, Training Accuracy: 72.928\n",
            "Worker 4, [22/32]: Training Loss: 0.898156057, Training Accuracy: 72.816\n",
            "Worker 4, [23/32]: Training Loss: 0.886185370, Training Accuracy: 73.064\n",
            "Worker 4, [24/32]: Training Loss: 0.861654047, Training Accuracy: 74.264\n",
            "Worker 4, [25/32]: Training Loss: 0.848737402, Training Accuracy: 74.256\n",
            "Worker 4, [26/32]: Training Loss: 0.812640196, Training Accuracy: 75.320\n",
            "Worker 4, [27/32]: Training Loss: 0.798090104, Training Accuracy: 76.272\n",
            "Worker 4, [28/32]: Training Loss: 0.803924677, Training Accuracy: 75.568\n",
            "Worker 4, [29/32]: Training Loss: 0.785800540, Training Accuracy: 76.064\n",
            "Worker 4, [30/32]: Training Loss: 0.749902585, Training Accuracy: 77.024\n",
            "Worker 4, [31/32]: Training Loss: 0.739930134, Training Accuracy: 77.488\n",
            "Worker 4, [32/32]: Training Loss: 0.750577522, Training Accuracy: 76.608\n",
            "Time taken for training worker 4: 0:03:14.308080\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002828\n",
            "Global Update 03: Test Loss: 4.291873566, Test Accuracy: 47.060\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.091998535, Training Accuracy: 51.040\n",
            "Worker 1, [02/32]: Training Loss: 1.593561620, Training Accuracy: 56.408\n",
            "Worker 1, [03/32]: Training Loss: 1.455751206, Training Accuracy: 59.488\n",
            "Worker 1, [04/32]: Training Loss: 1.346966678, Training Accuracy: 61.448\n",
            "Worker 1, [05/32]: Training Loss: 1.233261867, Training Accuracy: 64.344\n",
            "Worker 1, [06/32]: Training Loss: 1.169027797, Training Accuracy: 65.912\n",
            "Worker 1, [07/32]: Training Loss: 1.096309743, Training Accuracy: 67.688\n",
            "Worker 1, [08/32]: Training Loss: 1.029801721, Training Accuracy: 69.448\n",
            "Worker 1, [09/32]: Training Loss: 0.967804916, Training Accuracy: 71.024\n",
            "Worker 1, [10/32]: Training Loss: 0.915828226, Training Accuracy: 72.776\n",
            "Worker 1, [11/32]: Training Loss: 0.873093239, Training Accuracy: 74.056\n",
            "Worker 1, [12/32]: Training Loss: 0.829678099, Training Accuracy: 75.040\n",
            "Worker 1, [13/32]: Training Loss: 0.792349648, Training Accuracy: 76.104\n",
            "Worker 1, [14/32]: Training Loss: 0.762587830, Training Accuracy: 77.448\n",
            "Worker 1, [15/32]: Training Loss: 0.728917834, Training Accuracy: 77.776\n",
            "Worker 1, [16/32]: Training Loss: 0.707927287, Training Accuracy: 78.576\n",
            "Worker 1, [17/32]: Training Loss: 0.664656234, Training Accuracy: 79.992\n",
            "Worker 1, [18/32]: Training Loss: 0.644636159, Training Accuracy: 80.784\n",
            "Worker 1, [19/32]: Training Loss: 0.618799998, Training Accuracy: 81.048\n",
            "Worker 1, [20/32]: Training Loss: 0.579373409, Training Accuracy: 82.488\n",
            "Worker 1, [21/32]: Training Loss: 0.584232825, Training Accuracy: 82.392\n",
            "Worker 1, [22/32]: Training Loss: 0.560432723, Training Accuracy: 83.096\n",
            "Worker 1, [23/32]: Training Loss: 0.545655506, Training Accuracy: 83.960\n",
            "Worker 1, [24/32]: Training Loss: 0.530858487, Training Accuracy: 84.544\n",
            "Worker 1, [25/32]: Training Loss: 0.500857097, Training Accuracy: 85.240\n",
            "Worker 1, [26/32]: Training Loss: 0.483366165, Training Accuracy: 85.648\n",
            "Worker 1, [27/32]: Training Loss: 0.479115835, Training Accuracy: 85.640\n",
            "Worker 1, [28/32]: Training Loss: 0.459107388, Training Accuracy: 86.568\n",
            "Worker 1, [29/32]: Training Loss: 0.461093420, Training Accuracy: 86.488\n",
            "Worker 1, [30/32]: Training Loss: 0.447917067, Training Accuracy: 86.528\n",
            "Worker 1, [31/32]: Training Loss: 0.431873797, Training Accuracy: 87.208\n",
            "Worker 1, [32/32]: Training Loss: 0.411104812, Training Accuracy: 87.776\n",
            "Time taken for training worker 1: 0:03:15.513570\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.120878599, Training Accuracy: 50.376\n",
            "Worker 2, [02/32]: Training Loss: 1.615686536, Training Accuracy: 55.672\n",
            "Worker 2, [03/32]: Training Loss: 1.462052894, Training Accuracy: 59.256\n",
            "Worker 2, [04/32]: Training Loss: 1.354384867, Training Accuracy: 61.536\n",
            "Worker 2, [05/32]: Training Loss: 1.248875554, Training Accuracy: 63.872\n",
            "Worker 2, [06/32]: Training Loss: 1.172904217, Training Accuracy: 66.192\n",
            "Worker 2, [07/32]: Training Loss: 1.119715383, Training Accuracy: 66.792\n",
            "Worker 2, [08/32]: Training Loss: 1.055056866, Training Accuracy: 68.720\n",
            "Worker 2, [09/32]: Training Loss: 0.999001792, Training Accuracy: 70.256\n",
            "Worker 2, [10/32]: Training Loss: 0.937145689, Training Accuracy: 72.400\n",
            "Worker 2, [11/32]: Training Loss: 0.907893356, Training Accuracy: 73.040\n",
            "Worker 2, [12/32]: Training Loss: 0.859550252, Training Accuracy: 74.328\n",
            "Worker 2, [13/32]: Training Loss: 0.816401584, Training Accuracy: 75.888\n",
            "Worker 2, [14/32]: Training Loss: 0.770398984, Training Accuracy: 76.992\n",
            "Worker 2, [15/32]: Training Loss: 0.759036279, Training Accuracy: 77.328\n",
            "Worker 2, [16/32]: Training Loss: 0.725744233, Training Accuracy: 78.568\n",
            "Worker 2, [17/32]: Training Loss: 0.693653089, Training Accuracy: 78.848\n",
            "Worker 2, [18/32]: Training Loss: 0.664292950, Training Accuracy: 80.240\n",
            "Worker 2, [19/32]: Training Loss: 0.646617459, Training Accuracy: 80.680\n",
            "Worker 2, [20/32]: Training Loss: 0.611155730, Training Accuracy: 81.632\n",
            "Worker 2, [21/32]: Training Loss: 0.586854487, Training Accuracy: 82.536\n",
            "Worker 2, [22/32]: Training Loss: 0.566599664, Training Accuracy: 82.992\n",
            "Worker 2, [23/32]: Training Loss: 0.551961640, Training Accuracy: 83.528\n",
            "Worker 2, [24/32]: Training Loss: 0.535438029, Training Accuracy: 83.688\n",
            "Worker 2, [25/32]: Training Loss: 0.530077416, Training Accuracy: 84.240\n",
            "Worker 2, [26/32]: Training Loss: 0.504483940, Training Accuracy: 84.752\n",
            "Worker 2, [27/32]: Training Loss: 0.497674490, Training Accuracy: 84.672\n",
            "Worker 2, [28/32]: Training Loss: 0.471022324, Training Accuracy: 86.120\n",
            "Worker 2, [29/32]: Training Loss: 0.458576599, Training Accuracy: 86.280\n",
            "Worker 2, [30/32]: Training Loss: 0.438431827, Training Accuracy: 87.048\n",
            "Worker 2, [31/32]: Training Loss: 0.421396643, Training Accuracy: 87.408\n",
            "Worker 2, [32/32]: Training Loss: 0.429344776, Training Accuracy: 86.760\n",
            "Time taken for training worker 2: 0:03:15.725753\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 2.120545487, Training Accuracy: 50.472\n",
            "Worker 3, [02/32]: Training Loss: 1.611640637, Training Accuracy: 56.008\n",
            "Worker 3, [03/32]: Training Loss: 1.457240488, Training Accuracy: 59.448\n",
            "Worker 3, [04/32]: Training Loss: 1.345610605, Training Accuracy: 61.568\n",
            "Worker 3, [05/32]: Training Loss: 1.253197606, Training Accuracy: 64.048\n",
            "Worker 3, [06/32]: Training Loss: 1.170759984, Training Accuracy: 66.000\n",
            "Worker 3, [07/32]: Training Loss: 1.087115001, Training Accuracy: 68.560\n",
            "Worker 3, [08/32]: Training Loss: 1.042369342, Training Accuracy: 69.552\n",
            "Worker 3, [09/32]: Training Loss: 0.981747090, Training Accuracy: 71.048\n",
            "Worker 3, [10/32]: Training Loss: 0.917020302, Training Accuracy: 73.080\n",
            "Worker 3, [11/32]: Training Loss: 0.867084472, Training Accuracy: 74.520\n",
            "Worker 3, [12/32]: Training Loss: 0.847539869, Training Accuracy: 74.848\n",
            "Worker 3, [13/32]: Training Loss: 0.806291173, Training Accuracy: 76.176\n",
            "Worker 3, [14/32]: Training Loss: 0.767824448, Training Accuracy: 77.240\n",
            "Worker 3, [15/32]: Training Loss: 0.733581512, Training Accuracy: 78.416\n",
            "Worker 3, [16/32]: Training Loss: 0.697454601, Training Accuracy: 79.128\n",
            "Worker 3, [17/32]: Training Loss: 0.662382829, Training Accuracy: 80.528\n",
            "Worker 3, [18/32]: Training Loss: 0.648118197, Training Accuracy: 80.496\n",
            "Worker 3, [19/32]: Training Loss: 0.640411890, Training Accuracy: 80.752\n",
            "Worker 3, [20/32]: Training Loss: 0.600713936, Training Accuracy: 81.952\n",
            "Worker 3, [21/32]: Training Loss: 0.568472159, Training Accuracy: 82.928\n",
            "Worker 3, [22/32]: Training Loss: 0.551179719, Training Accuracy: 83.232\n",
            "Worker 3, [23/32]: Training Loss: 0.552240496, Training Accuracy: 83.608\n",
            "Worker 3, [24/32]: Training Loss: 0.523490623, Training Accuracy: 84.768\n",
            "Worker 3, [25/32]: Training Loss: 0.515481372, Training Accuracy: 84.600\n",
            "Worker 3, [26/32]: Training Loss: 0.497937192, Training Accuracy: 85.216\n",
            "Worker 3, [27/32]: Training Loss: 0.485588830, Training Accuracy: 85.672\n",
            "Worker 3, [28/32]: Training Loss: 0.468899655, Training Accuracy: 85.808\n",
            "Worker 3, [29/32]: Training Loss: 0.448808352, Training Accuracy: 86.864\n",
            "Worker 3, [30/32]: Training Loss: 0.435470440, Training Accuracy: 87.096\n",
            "Worker 3, [31/32]: Training Loss: 0.437323705, Training Accuracy: 86.928\n",
            "Worker 3, [32/32]: Training Loss: 0.409561120, Training Accuracy: 88.056\n",
            "Time taken for training worker 3: 0:03:14.860476\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 2.133727452, Training Accuracy: 50.056\n",
            "Worker 4, [02/32]: Training Loss: 1.638657085, Training Accuracy: 55.360\n",
            "Worker 4, [03/32]: Training Loss: 1.477151978, Training Accuracy: 58.320\n",
            "Worker 4, [04/32]: Training Loss: 1.375326597, Training Accuracy: 60.600\n",
            "Worker 4, [05/32]: Training Loss: 1.269358196, Training Accuracy: 63.808\n",
            "Worker 4, [06/32]: Training Loss: 1.188154561, Training Accuracy: 66.184\n",
            "Worker 4, [07/32]: Training Loss: 1.132182906, Training Accuracy: 67.264\n",
            "Worker 4, [08/32]: Training Loss: 1.050292573, Training Accuracy: 69.256\n",
            "Worker 4, [09/32]: Training Loss: 0.990346496, Training Accuracy: 70.736\n",
            "Worker 4, [10/32]: Training Loss: 0.949018398, Training Accuracy: 72.176\n",
            "Worker 4, [11/32]: Training Loss: 0.923198634, Training Accuracy: 73.312\n",
            "Worker 4, [12/32]: Training Loss: 0.839323838, Training Accuracy: 75.232\n",
            "Worker 4, [13/32]: Training Loss: 0.810652174, Training Accuracy: 75.872\n",
            "Worker 4, [14/32]: Training Loss: 0.775565413, Training Accuracy: 76.560\n",
            "Worker 4, [15/32]: Training Loss: 0.746771005, Training Accuracy: 77.328\n",
            "Worker 4, [16/32]: Training Loss: 0.716176645, Training Accuracy: 79.008\n",
            "Worker 4, [17/32]: Training Loss: 0.674366213, Training Accuracy: 79.792\n",
            "Worker 4, [18/32]: Training Loss: 0.648344496, Training Accuracy: 80.808\n",
            "Worker 4, [19/32]: Training Loss: 0.646821829, Training Accuracy: 80.784\n",
            "Worker 4, [20/32]: Training Loss: 0.614164462, Training Accuracy: 81.464\n",
            "Worker 4, [21/32]: Training Loss: 0.590370885, Training Accuracy: 82.656\n",
            "Worker 4, [22/32]: Training Loss: 0.573395350, Training Accuracy: 82.968\n",
            "Worker 4, [23/32]: Training Loss: 0.558644678, Training Accuracy: 83.040\n",
            "Worker 4, [24/32]: Training Loss: 0.533434940, Training Accuracy: 84.128\n",
            "Worker 4, [25/32]: Training Loss: 0.513792807, Training Accuracy: 84.328\n",
            "Worker 4, [26/32]: Training Loss: 0.501344887, Training Accuracy: 84.888\n",
            "Worker 4, [27/32]: Training Loss: 0.484533554, Training Accuracy: 85.336\n",
            "Worker 4, [28/32]: Training Loss: 0.477603619, Training Accuracy: 85.728\n",
            "Worker 4, [29/32]: Training Loss: 0.455179609, Training Accuracy: 86.320\n",
            "Worker 4, [30/32]: Training Loss: 0.440291379, Training Accuracy: 86.936\n",
            "Worker 4, [31/32]: Training Loss: 0.438150583, Training Accuracy: 86.816\n",
            "Worker 4, [32/32]: Training Loss: 0.422983697, Training Accuracy: 87.616\n",
            "Time taken for training worker 4: 0:03:18.413601\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003197\n",
            "Global Update 04: Test Loss: 3.623166016, Test Accuracy: 47.160\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:51:55.257700\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:64\n",
            "==================================================\n",
            "Worker 1, [01/64]: Training Loss: 4.503808187, Training Accuracy: 2.656\n",
            "Worker 1, [02/64]: Training Loss: 4.126072403, Training Accuracy: 6.424\n",
            "Worker 1, [03/64]: Training Loss: 3.919452514, Training Accuracy: 9.216\n",
            "Worker 1, [04/64]: Training Loss: 3.771016655, Training Accuracy: 10.976\n",
            "Worker 1, [05/64]: Training Loss: 3.616328985, Training Accuracy: 13.768\n",
            "Worker 1, [06/64]: Training Loss: 3.490143259, Training Accuracy: 15.712\n",
            "Worker 1, [07/64]: Training Loss: 3.360723045, Training Accuracy: 17.760\n",
            "Worker 1, [08/64]: Training Loss: 3.277538197, Training Accuracy: 19.296\n",
            "Worker 1, [09/64]: Training Loss: 3.181208426, Training Accuracy: 21.352\n",
            "Worker 1, [10/64]: Training Loss: 3.068116447, Training Accuracy: 23.288\n",
            "Worker 1, [11/64]: Training Loss: 2.996314461, Training Accuracy: 24.800\n",
            "Worker 1, [12/64]: Training Loss: 2.909334657, Training Accuracy: 26.280\n",
            "Worker 1, [13/64]: Training Loss: 2.837545390, Training Accuracy: 27.880\n",
            "Worker 1, [14/64]: Training Loss: 2.777426239, Training Accuracy: 28.712\n",
            "Worker 1, [15/64]: Training Loss: 2.699383533, Training Accuracy: 30.144\n",
            "Worker 1, [16/64]: Training Loss: 2.628586364, Training Accuracy: 31.808\n",
            "Worker 1, [17/64]: Training Loss: 2.599845122, Training Accuracy: 31.840\n",
            "Worker 1, [18/64]: Training Loss: 2.512570890, Training Accuracy: 34.184\n",
            "Worker 1, [19/64]: Training Loss: 2.456941144, Training Accuracy: 34.976\n",
            "Worker 1, [20/64]: Training Loss: 2.436093740, Training Accuracy: 35.752\n",
            "Worker 1, [21/64]: Training Loss: 2.377632376, Training Accuracy: 36.576\n",
            "Worker 1, [22/64]: Training Loss: 2.344656335, Training Accuracy: 37.240\n",
            "Worker 1, [23/64]: Training Loss: 2.264586124, Training Accuracy: 39.408\n",
            "Worker 1, [24/64]: Training Loss: 2.218714540, Training Accuracy: 40.304\n",
            "Worker 1, [25/64]: Training Loss: 2.185681211, Training Accuracy: 41.096\n",
            "Worker 1, [26/64]: Training Loss: 2.157896875, Training Accuracy: 41.696\n",
            "Worker 1, [27/64]: Training Loss: 2.105941128, Training Accuracy: 42.680\n",
            "Worker 1, [28/64]: Training Loss: 2.076829919, Training Accuracy: 43.680\n",
            "Worker 1, [29/64]: Training Loss: 2.015256182, Training Accuracy: 44.496\n",
            "Worker 1, [30/64]: Training Loss: 1.997641356, Training Accuracy: 44.944\n",
            "Worker 1, [31/64]: Training Loss: 1.961466481, Training Accuracy: 46.440\n",
            "Worker 1, [32/64]: Training Loss: 1.926142597, Training Accuracy: 46.928\n",
            "Worker 1, [33/64]: Training Loss: 1.890902312, Training Accuracy: 47.696\n",
            "Worker 1, [34/64]: Training Loss: 1.857520256, Training Accuracy: 48.576\n",
            "Worker 1, [35/64]: Training Loss: 1.863928193, Training Accuracy: 48.240\n",
            "Worker 1, [36/64]: Training Loss: 1.816132030, Training Accuracy: 49.560\n",
            "Worker 1, [37/64]: Training Loss: 1.792308435, Training Accuracy: 50.096\n",
            "Worker 1, [38/64]: Training Loss: 1.727092869, Training Accuracy: 51.568\n",
            "Worker 1, [39/64]: Training Loss: 1.720546663, Training Accuracy: 51.896\n",
            "Worker 1, [40/64]: Training Loss: 1.716878279, Training Accuracy: 51.472\n",
            "Worker 1, [41/64]: Training Loss: 1.662594401, Training Accuracy: 53.696\n",
            "Worker 1, [42/64]: Training Loss: 1.642973855, Training Accuracy: 53.536\n",
            "Worker 1, [43/64]: Training Loss: 1.636917423, Training Accuracy: 53.408\n",
            "Worker 1, [44/64]: Training Loss: 1.623908309, Training Accuracy: 54.176\n",
            "Worker 1, [45/64]: Training Loss: 1.590728418, Training Accuracy: 54.696\n",
            "Worker 1, [46/64]: Training Loss: 1.563621772, Training Accuracy: 55.288\n",
            "Worker 1, [47/64]: Training Loss: 1.529788362, Training Accuracy: 55.800\n",
            "Worker 1, [48/64]: Training Loss: 1.515173073, Training Accuracy: 56.720\n",
            "Worker 1, [49/64]: Training Loss: 1.551099282, Training Accuracy: 56.088\n",
            "Worker 1, [50/64]: Training Loss: 1.512904311, Training Accuracy: 56.920\n",
            "Worker 1, [51/64]: Training Loss: 1.492160122, Training Accuracy: 57.832\n",
            "Worker 1, [52/64]: Training Loss: 1.435080870, Training Accuracy: 58.584\n",
            "Worker 1, [53/64]: Training Loss: 1.428302793, Training Accuracy: 59.024\n",
            "Worker 1, [54/64]: Training Loss: 1.405343508, Training Accuracy: 59.336\n",
            "Worker 1, [55/64]: Training Loss: 1.415254895, Training Accuracy: 59.184\n",
            "Worker 1, [56/64]: Training Loss: 1.403102589, Training Accuracy: 59.512\n",
            "Worker 1, [57/64]: Training Loss: 1.383178393, Training Accuracy: 59.808\n",
            "Worker 1, [58/64]: Training Loss: 1.358469264, Training Accuracy: 60.840\n",
            "Worker 1, [59/64]: Training Loss: 1.311491767, Training Accuracy: 61.944\n",
            "Worker 1, [60/64]: Training Loss: 1.355438183, Training Accuracy: 60.800\n",
            "Worker 1, [61/64]: Training Loss: 1.328740995, Training Accuracy: 61.752\n",
            "Worker 1, [62/64]: Training Loss: 1.295944395, Training Accuracy: 62.104\n",
            "Worker 1, [63/64]: Training Loss: 1.309198192, Training Accuracy: 61.992\n",
            "Worker 1, [64/64]: Training Loss: 1.301043746, Training Accuracy: 62.560\n",
            "Time taken for training worker 1: 0:06:32.657812\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/64]: Training Loss: 4.523070005, Training Accuracy: 2.368\n",
            "Worker 2, [02/64]: Training Loss: 4.142935429, Training Accuracy: 5.984\n",
            "Worker 2, [03/64]: Training Loss: 3.941487916, Training Accuracy: 8.968\n",
            "Worker 2, [04/64]: Training Loss: 3.774281009, Training Accuracy: 11.120\n",
            "Worker 2, [05/64]: Training Loss: 3.642314303, Training Accuracy: 13.712\n",
            "Worker 2, [06/64]: Training Loss: 3.520512538, Training Accuracy: 15.280\n",
            "Worker 2, [07/64]: Training Loss: 3.393892342, Training Accuracy: 17.352\n",
            "Worker 2, [08/64]: Training Loss: 3.286495394, Training Accuracy: 20.000\n",
            "Worker 2, [09/64]: Training Loss: 3.198552036, Training Accuracy: 21.480\n",
            "Worker 2, [10/64]: Training Loss: 3.097992665, Training Accuracy: 22.832\n",
            "Worker 2, [11/64]: Training Loss: 3.039080980, Training Accuracy: 23.872\n",
            "Worker 2, [12/64]: Training Loss: 2.973010954, Training Accuracy: 25.440\n",
            "Worker 2, [13/64]: Training Loss: 2.881050171, Training Accuracy: 26.736\n",
            "Worker 2, [14/64]: Training Loss: 2.803545929, Training Accuracy: 28.472\n",
            "Worker 2, [15/64]: Training Loss: 2.715018227, Training Accuracy: 30.312\n",
            "Worker 2, [16/64]: Training Loss: 2.669638052, Training Accuracy: 31.176\n",
            "Worker 2, [17/64]: Training Loss: 2.587413183, Training Accuracy: 32.528\n",
            "Worker 2, [18/64]: Training Loss: 2.541988425, Training Accuracy: 32.920\n",
            "Worker 2, [19/64]: Training Loss: 2.484802419, Training Accuracy: 35.216\n",
            "Worker 2, [20/64]: Training Loss: 2.442353416, Training Accuracy: 36.448\n",
            "Worker 2, [21/64]: Training Loss: 2.373987698, Training Accuracy: 36.960\n",
            "Worker 2, [22/64]: Training Loss: 2.317213513, Training Accuracy: 38.560\n",
            "Worker 2, [23/64]: Training Loss: 2.284633740, Training Accuracy: 38.848\n",
            "Worker 2, [24/64]: Training Loss: 2.240301799, Training Accuracy: 39.768\n",
            "Worker 2, [25/64]: Training Loss: 2.212706488, Training Accuracy: 40.328\n",
            "Worker 2, [26/64]: Training Loss: 2.165723650, Training Accuracy: 40.976\n",
            "Worker 2, [27/64]: Training Loss: 2.080346773, Training Accuracy: 43.232\n",
            "Worker 2, [28/64]: Training Loss: 2.076329064, Training Accuracy: 43.184\n",
            "Worker 2, [29/64]: Training Loss: 2.042502349, Training Accuracy: 43.936\n",
            "Worker 2, [30/64]: Training Loss: 1.978875806, Training Accuracy: 45.200\n",
            "Worker 2, [31/64]: Training Loss: 1.961367416, Training Accuracy: 45.584\n",
            "Worker 2, [32/64]: Training Loss: 1.937972754, Training Accuracy: 46.568\n",
            "Worker 2, [33/64]: Training Loss: 1.905852906, Training Accuracy: 47.256\n",
            "Worker 2, [34/64]: Training Loss: 1.871903991, Training Accuracy: 48.680\n",
            "Worker 2, [35/64]: Training Loss: 1.842776286, Training Accuracy: 48.712\n",
            "Worker 2, [36/64]: Training Loss: 1.799536068, Training Accuracy: 48.792\n",
            "Worker 2, [37/64]: Training Loss: 1.773984338, Training Accuracy: 50.672\n",
            "Worker 2, [38/64]: Training Loss: 1.753628360, Training Accuracy: 50.776\n",
            "Worker 2, [39/64]: Training Loss: 1.725132774, Training Accuracy: 51.616\n",
            "Worker 2, [40/64]: Training Loss: 1.709083794, Training Accuracy: 51.984\n",
            "Worker 2, [41/64]: Training Loss: 1.657313080, Training Accuracy: 52.744\n",
            "Worker 2, [42/64]: Training Loss: 1.655073322, Training Accuracy: 52.384\n",
            "Worker 2, [43/64]: Training Loss: 1.644808907, Training Accuracy: 53.216\n",
            "Worker 2, [44/64]: Training Loss: 1.611257569, Training Accuracy: 54.072\n",
            "Worker 2, [45/64]: Training Loss: 1.584951315, Training Accuracy: 54.632\n",
            "Worker 2, [46/64]: Training Loss: 1.569387026, Training Accuracy: 55.056\n",
            "Worker 2, [47/64]: Training Loss: 1.514769190, Training Accuracy: 56.848\n",
            "Worker 2, [48/64]: Training Loss: 1.512312149, Training Accuracy: 57.104\n",
            "Worker 2, [49/64]: Training Loss: 1.511110911, Training Accuracy: 56.400\n",
            "Worker 2, [50/64]: Training Loss: 1.497086451, Training Accuracy: 56.952\n",
            "Worker 2, [51/64]: Training Loss: 1.469193781, Training Accuracy: 57.936\n",
            "Worker 2, [52/64]: Training Loss: 1.470887423, Training Accuracy: 57.824\n",
            "Worker 2, [53/64]: Training Loss: 1.458520014, Training Accuracy: 58.072\n",
            "Worker 2, [54/64]: Training Loss: 1.400317343, Training Accuracy: 59.608\n",
            "Worker 2, [55/64]: Training Loss: 1.409264037, Training Accuracy: 58.696\n",
            "Worker 2, [56/64]: Training Loss: 1.366304808, Training Accuracy: 60.688\n",
            "Worker 2, [57/64]: Training Loss: 1.377733185, Training Accuracy: 60.368\n",
            "Worker 2, [58/64]: Training Loss: 1.352303227, Training Accuracy: 60.848\n",
            "Worker 2, [59/64]: Training Loss: 1.315346170, Training Accuracy: 61.760\n",
            "Worker 2, [60/64]: Training Loss: 1.293790676, Training Accuracy: 62.272\n",
            "Worker 2, [61/64]: Training Loss: 1.292832716, Training Accuracy: 62.528\n",
            "Worker 2, [62/64]: Training Loss: 1.337704459, Training Accuracy: 61.552\n",
            "Worker 2, [63/64]: Training Loss: 1.299695329, Training Accuracy: 62.296\n",
            "Worker 2, [64/64]: Training Loss: 1.283331011, Training Accuracy: 62.672\n",
            "Time taken for training worker 2: 0:06:27.302072\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/64]: Training Loss: 4.486740297, Training Accuracy: 2.904\n",
            "Worker 3, [02/64]: Training Loss: 4.091939474, Training Accuracy: 7.272\n",
            "Worker 3, [03/64]: Training Loss: 3.898320020, Training Accuracy: 9.192\n",
            "Worker 3, [04/64]: Training Loss: 3.750246132, Training Accuracy: 11.544\n",
            "Worker 3, [05/64]: Training Loss: 3.627973154, Training Accuracy: 13.360\n",
            "Worker 3, [06/64]: Training Loss: 3.501926046, Training Accuracy: 15.808\n",
            "Worker 3, [07/64]: Training Loss: 3.399292267, Training Accuracy: 17.296\n",
            "Worker 3, [08/64]: Training Loss: 3.291217657, Training Accuracy: 19.320\n",
            "Worker 3, [09/64]: Training Loss: 3.187261366, Training Accuracy: 21.528\n",
            "Worker 3, [10/64]: Training Loss: 3.092077424, Training Accuracy: 22.856\n",
            "Worker 3, [11/64]: Training Loss: 3.005675784, Training Accuracy: 24.440\n",
            "Worker 3, [12/64]: Training Loss: 2.944973320, Training Accuracy: 25.808\n",
            "Worker 3, [13/64]: Training Loss: 2.837739240, Training Accuracy: 27.816\n",
            "Worker 3, [14/64]: Training Loss: 2.771810260, Training Accuracy: 29.000\n",
            "Worker 3, [15/64]: Training Loss: 2.700903922, Training Accuracy: 30.592\n",
            "Worker 3, [16/64]: Training Loss: 2.653451206, Training Accuracy: 31.648\n",
            "Worker 3, [17/64]: Training Loss: 2.574049669, Training Accuracy: 32.800\n",
            "Worker 3, [18/64]: Training Loss: 2.525520354, Training Accuracy: 34.024\n",
            "Worker 3, [19/64]: Training Loss: 2.471975848, Training Accuracy: 35.304\n",
            "Worker 3, [20/64]: Training Loss: 2.432609574, Training Accuracy: 36.152\n",
            "Worker 3, [21/64]: Training Loss: 2.358928228, Training Accuracy: 37.456\n",
            "Worker 3, [22/64]: Training Loss: 2.303504074, Training Accuracy: 38.624\n",
            "Worker 3, [23/64]: Training Loss: 2.271116510, Training Accuracy: 38.952\n",
            "Worker 3, [24/64]: Training Loss: 2.228135120, Training Accuracy: 40.080\n",
            "Worker 3, [25/64]: Training Loss: 2.187517595, Training Accuracy: 40.904\n",
            "Worker 3, [26/64]: Training Loss: 2.155232914, Training Accuracy: 41.384\n",
            "Worker 3, [27/64]: Training Loss: 2.128121535, Training Accuracy: 42.752\n",
            "Worker 3, [28/64]: Training Loss: 2.082730174, Training Accuracy: 43.000\n",
            "Worker 3, [29/64]: Training Loss: 2.049975527, Training Accuracy: 44.568\n",
            "Worker 3, [30/64]: Training Loss: 1.998134428, Training Accuracy: 44.848\n",
            "Worker 3, [31/64]: Training Loss: 1.972247710, Training Accuracy: 45.600\n",
            "Worker 3, [32/64]: Training Loss: 1.919891135, Training Accuracy: 46.320\n",
            "Worker 3, [33/64]: Training Loss: 1.897805585, Training Accuracy: 47.024\n",
            "Worker 3, [34/64]: Training Loss: 1.889016628, Training Accuracy: 47.048\n",
            "Worker 3, [35/64]: Training Loss: 1.882978137, Training Accuracy: 47.672\n",
            "Worker 3, [36/64]: Training Loss: 1.809599887, Training Accuracy: 49.192\n",
            "Worker 3, [37/64]: Training Loss: 1.772607567, Training Accuracy: 49.816\n",
            "Worker 3, [38/64]: Training Loss: 1.741490213, Training Accuracy: 51.296\n",
            "Worker 3, [39/64]: Training Loss: 1.751122045, Training Accuracy: 50.792\n",
            "Worker 3, [40/64]: Training Loss: 1.706492117, Training Accuracy: 52.192\n",
            "Worker 3, [41/64]: Training Loss: 1.667308091, Training Accuracy: 52.984\n",
            "Worker 3, [42/64]: Training Loss: 1.648957600, Training Accuracy: 53.608\n",
            "Worker 3, [43/64]: Training Loss: 1.652609073, Training Accuracy: 53.136\n",
            "Worker 3, [44/64]: Training Loss: 1.617388397, Training Accuracy: 54.128\n",
            "Worker 3, [45/64]: Training Loss: 1.558565988, Training Accuracy: 55.752\n",
            "Worker 3, [46/64]: Training Loss: 1.565446338, Training Accuracy: 55.512\n",
            "Worker 3, [47/64]: Training Loss: 1.545126086, Training Accuracy: 56.136\n",
            "Worker 3, [48/64]: Training Loss: 1.527261842, Training Accuracy: 55.968\n",
            "Worker 3, [49/64]: Training Loss: 1.484279270, Training Accuracy: 57.624\n",
            "Worker 3, [50/64]: Training Loss: 1.487892784, Training Accuracy: 57.600\n",
            "Worker 3, [51/64]: Training Loss: 1.458980139, Training Accuracy: 57.280\n",
            "Worker 3, [52/64]: Training Loss: 1.461180936, Training Accuracy: 57.984\n",
            "Worker 3, [53/64]: Training Loss: 1.434653492, Training Accuracy: 58.288\n",
            "Worker 3, [54/64]: Training Loss: 1.408762635, Training Accuracy: 59.552\n",
            "Worker 3, [55/64]: Training Loss: 1.427989589, Training Accuracy: 58.968\n",
            "Worker 3, [56/64]: Training Loss: 1.405155979, Training Accuracy: 59.784\n",
            "Worker 3, [57/64]: Training Loss: 1.380382223, Training Accuracy: 60.568\n",
            "Worker 3, [58/64]: Training Loss: 1.322073219, Training Accuracy: 61.640\n",
            "Worker 3, [59/64]: Training Loss: 1.356559037, Training Accuracy: 60.760\n",
            "Worker 3, [60/64]: Training Loss: 1.325558845, Training Accuracy: 61.240\n",
            "Worker 3, [61/64]: Training Loss: 1.320041373, Training Accuracy: 61.432\n",
            "Worker 3, [62/64]: Training Loss: 1.292285073, Training Accuracy: 62.496\n",
            "Worker 3, [63/64]: Training Loss: 1.289803507, Training Accuracy: 62.560\n",
            "Worker 3, [64/64]: Training Loss: 1.283479447, Training Accuracy: 62.832\n",
            "Time taken for training worker 3: 0:06:27.702759\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/64]: Training Loss: 4.513327294, Training Accuracy: 2.552\n",
            "Worker 4, [02/64]: Training Loss: 4.153224249, Training Accuracy: 6.232\n",
            "Worker 4, [03/64]: Training Loss: 3.955879148, Training Accuracy: 8.624\n",
            "Worker 4, [04/64]: Training Loss: 3.789221749, Training Accuracy: 11.496\n",
            "Worker 4, [05/64]: Training Loss: 3.677155924, Training Accuracy: 12.736\n",
            "Worker 4, [06/64]: Training Loss: 3.524272449, Training Accuracy: 15.144\n",
            "Worker 4, [07/64]: Training Loss: 3.409851777, Training Accuracy: 17.384\n",
            "Worker 4, [08/64]: Training Loss: 3.298445904, Training Accuracy: 19.296\n",
            "Worker 4, [09/64]: Training Loss: 3.218615888, Training Accuracy: 20.544\n",
            "Worker 4, [10/64]: Training Loss: 3.129101915, Training Accuracy: 22.152\n",
            "Worker 4, [11/64]: Training Loss: 3.054605949, Training Accuracy: 23.680\n",
            "Worker 4, [12/64]: Training Loss: 2.959330573, Training Accuracy: 25.496\n",
            "Worker 4, [13/64]: Training Loss: 2.909437872, Training Accuracy: 26.400\n",
            "Worker 4, [14/64]: Training Loss: 2.834067382, Training Accuracy: 28.144\n",
            "Worker 4, [15/64]: Training Loss: 2.754455337, Training Accuracy: 29.368\n",
            "Worker 4, [16/64]: Training Loss: 2.690097673, Training Accuracy: 31.064\n",
            "Worker 4, [17/64]: Training Loss: 2.651451549, Training Accuracy: 31.424\n",
            "Worker 4, [18/64]: Training Loss: 2.581033046, Training Accuracy: 33.040\n",
            "Worker 4, [19/64]: Training Loss: 2.522837072, Training Accuracy: 33.832\n",
            "Worker 4, [20/64]: Training Loss: 2.467564316, Training Accuracy: 35.512\n",
            "Worker 4, [21/64]: Training Loss: 2.409358629, Training Accuracy: 36.096\n",
            "Worker 4, [22/64]: Training Loss: 2.363107756, Training Accuracy: 37.104\n",
            "Worker 4, [23/64]: Training Loss: 2.322300534, Training Accuracy: 38.080\n",
            "Worker 4, [24/64]: Training Loss: 2.271068351, Training Accuracy: 39.200\n",
            "Worker 4, [25/64]: Training Loss: 2.236064652, Training Accuracy: 39.496\n",
            "Worker 4, [26/64]: Training Loss: 2.195093244, Training Accuracy: 40.528\n",
            "Worker 4, [27/64]: Training Loss: 2.151825541, Training Accuracy: 42.112\n",
            "Worker 4, [28/64]: Training Loss: 2.138922703, Training Accuracy: 42.088\n",
            "Worker 4, [29/64]: Training Loss: 2.069763215, Training Accuracy: 43.656\n",
            "Worker 4, [30/64]: Training Loss: 2.044929875, Training Accuracy: 44.072\n",
            "Worker 4, [31/64]: Training Loss: 2.010846289, Training Accuracy: 44.664\n",
            "Worker 4, [32/64]: Training Loss: 1.976604987, Training Accuracy: 45.512\n",
            "Worker 4, [33/64]: Training Loss: 1.934023532, Training Accuracy: 46.696\n",
            "Worker 4, [34/64]: Training Loss: 1.897785476, Training Accuracy: 47.432\n",
            "Worker 4, [35/64]: Training Loss: 1.871699781, Training Accuracy: 48.128\n",
            "Worker 4, [36/64]: Training Loss: 1.825380901, Training Accuracy: 49.552\n",
            "Worker 4, [37/64]: Training Loss: 1.809978294, Training Accuracy: 50.000\n",
            "Worker 4, [38/64]: Training Loss: 1.774009922, Training Accuracy: 50.296\n",
            "Worker 4, [39/64]: Training Loss: 1.780364512, Training Accuracy: 50.408\n",
            "Worker 4, [40/64]: Training Loss: 1.741926741, Training Accuracy: 51.392\n",
            "Worker 4, [41/64]: Training Loss: 1.697270711, Training Accuracy: 52.352\n",
            "Worker 4, [42/64]: Training Loss: 1.688264091, Training Accuracy: 52.616\n",
            "Worker 4, [43/64]: Training Loss: 1.683377201, Training Accuracy: 52.120\n",
            "Worker 4, [44/64]: Training Loss: 1.626949309, Training Accuracy: 53.512\n",
            "Worker 4, [45/64]: Training Loss: 1.616105494, Training Accuracy: 53.800\n",
            "Worker 4, [46/64]: Training Loss: 1.599475648, Training Accuracy: 54.384\n",
            "Worker 4, [47/64]: Training Loss: 1.588296559, Training Accuracy: 55.056\n",
            "Worker 4, [48/64]: Training Loss: 1.543068754, Training Accuracy: 56.592\n",
            "Worker 4, [49/64]: Training Loss: 1.536374943, Training Accuracy: 56.104\n",
            "Worker 4, [50/64]: Training Loss: 1.508619284, Training Accuracy: 56.336\n",
            "Worker 4, [51/64]: Training Loss: 1.500365715, Training Accuracy: 57.008\n",
            "Worker 4, [52/64]: Training Loss: 1.488771343, Training Accuracy: 57.168\n",
            "Worker 4, [53/64]: Training Loss: 1.468485472, Training Accuracy: 58.064\n",
            "Worker 4, [54/64]: Training Loss: 1.457338544, Training Accuracy: 58.128\n",
            "Worker 4, [55/64]: Training Loss: 1.447882730, Training Accuracy: 58.200\n",
            "Worker 4, [56/64]: Training Loss: 1.419853204, Training Accuracy: 59.072\n",
            "Worker 4, [57/64]: Training Loss: 1.423435194, Training Accuracy: 59.408\n",
            "Worker 4, [58/64]: Training Loss: 1.349568727, Training Accuracy: 60.856\n",
            "Worker 4, [59/64]: Training Loss: 1.380439395, Training Accuracy: 60.032\n",
            "Worker 4, [60/64]: Training Loss: 1.372948990, Training Accuracy: 60.720\n",
            "Worker 4, [61/64]: Training Loss: 1.350427594, Training Accuracy: 61.560\n",
            "Worker 4, [62/64]: Training Loss: 1.343145432, Training Accuracy: 61.352\n",
            "Worker 4, [63/64]: Training Loss: 1.364408972, Training Accuracy: 60.824\n",
            "Worker 4, [64/64]: Training Loss: 1.306420743, Training Accuracy: 62.632\n",
            "Time taken for training worker 4: 0:06:29.215902\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002660\n",
            "Global Update 01: Test Loss: 3.794462335, Test Accuracy: 21.370\n",
            "**************************************************\n",
            "Worker 1, [01/64]: Training Loss: 2.924747910, Training Accuracy: 27.424\n",
            "Worker 1, [02/64]: Training Loss: 2.458897731, Training Accuracy: 35.632\n",
            "Worker 1, [03/64]: Training Loss: 2.317101598, Training Accuracy: 38.384\n",
            "Worker 1, [04/64]: Training Loss: 2.186875141, Training Accuracy: 40.984\n",
            "Worker 1, [05/64]: Training Loss: 2.075676433, Training Accuracy: 43.920\n",
            "Worker 1, [06/64]: Training Loss: 2.003817839, Training Accuracy: 44.848\n",
            "Worker 1, [07/64]: Training Loss: 1.921322902, Training Accuracy: 47.232\n",
            "Worker 1, [08/64]: Training Loss: 1.858197294, Training Accuracy: 48.320\n",
            "Worker 1, [09/64]: Training Loss: 1.782505024, Training Accuracy: 50.280\n",
            "Worker 1, [10/64]: Training Loss: 1.736241786, Training Accuracy: 51.096\n",
            "Worker 1, [11/64]: Training Loss: 1.660146751, Training Accuracy: 53.160\n",
            "Worker 1, [12/64]: Training Loss: 1.606175080, Training Accuracy: 54.056\n",
            "Worker 1, [13/64]: Training Loss: 1.573554136, Training Accuracy: 55.504\n",
            "Worker 1, [14/64]: Training Loss: 1.514940730, Training Accuracy: 56.720\n",
            "Worker 1, [15/64]: Training Loss: 1.478221182, Training Accuracy: 56.816\n",
            "Worker 1, [16/64]: Training Loss: 1.431696461, Training Accuracy: 58.696\n",
            "Worker 1, [17/64]: Training Loss: 1.408537436, Training Accuracy: 59.160\n",
            "Worker 1, [18/64]: Training Loss: 1.349545566, Training Accuracy: 60.904\n",
            "Worker 1, [19/64]: Training Loss: 1.347600417, Training Accuracy: 60.808\n",
            "Worker 1, [20/64]: Training Loss: 1.271427078, Training Accuracy: 62.736\n",
            "Worker 1, [21/64]: Training Loss: 1.238119942, Training Accuracy: 63.400\n",
            "Worker 1, [22/64]: Training Loss: 1.230068325, Training Accuracy: 64.040\n",
            "Worker 1, [23/64]: Training Loss: 1.186801213, Training Accuracy: 65.064\n",
            "Worker 1, [24/64]: Training Loss: 1.155767642, Training Accuracy: 65.464\n",
            "Worker 1, [25/64]: Training Loss: 1.106451444, Training Accuracy: 67.272\n",
            "Worker 1, [26/64]: Training Loss: 1.116703714, Training Accuracy: 67.392\n",
            "Worker 1, [27/64]: Training Loss: 1.045255091, Training Accuracy: 68.792\n",
            "Worker 1, [28/64]: Training Loss: 1.048476287, Training Accuracy: 68.560\n",
            "Worker 1, [29/64]: Training Loss: 1.022404144, Training Accuracy: 69.520\n",
            "Worker 1, [30/64]: Training Loss: 0.991539707, Training Accuracy: 69.952\n",
            "Worker 1, [31/64]: Training Loss: 0.969359498, Training Accuracy: 70.864\n",
            "Worker 1, [32/64]: Training Loss: 0.960055350, Training Accuracy: 71.248\n",
            "Worker 1, [33/64]: Training Loss: 0.951882124, Training Accuracy: 71.456\n",
            "Worker 1, [34/64]: Training Loss: 0.933509257, Training Accuracy: 71.944\n",
            "Worker 1, [35/64]: Training Loss: 0.896335981, Training Accuracy: 73.272\n",
            "Worker 1, [36/64]: Training Loss: 0.885949778, Training Accuracy: 73.240\n",
            "Worker 1, [37/64]: Training Loss: 0.880509457, Training Accuracy: 73.304\n",
            "Worker 1, [38/64]: Training Loss: 0.871247314, Training Accuracy: 73.552\n",
            "Worker 1, [39/64]: Training Loss: 0.832154903, Training Accuracy: 75.032\n",
            "Worker 1, [40/64]: Training Loss: 0.807790694, Training Accuracy: 75.800\n",
            "Worker 1, [41/64]: Training Loss: 0.798736525, Training Accuracy: 76.024\n",
            "Worker 1, [42/64]: Training Loss: 0.768658482, Training Accuracy: 76.752\n",
            "Worker 1, [43/64]: Training Loss: 0.773300853, Training Accuracy: 76.192\n",
            "Worker 1, [44/64]: Training Loss: 0.766018836, Training Accuracy: 76.616\n",
            "Worker 1, [45/64]: Training Loss: 0.748584821, Training Accuracy: 77.104\n",
            "Worker 1, [46/64]: Training Loss: 0.730549560, Training Accuracy: 77.560\n",
            "Worker 1, [47/64]: Training Loss: 0.718116159, Training Accuracy: 78.192\n",
            "Worker 1, [48/64]: Training Loss: 0.706205008, Training Accuracy: 78.176\n",
            "Worker 1, [49/64]: Training Loss: 0.716618425, Training Accuracy: 77.840\n",
            "Worker 1, [50/64]: Training Loss: 0.680654749, Training Accuracy: 78.784\n",
            "Worker 1, [51/64]: Training Loss: 0.685562141, Training Accuracy: 79.080\n",
            "Worker 1, [52/64]: Training Loss: 0.668686606, Training Accuracy: 79.008\n",
            "Worker 1, [53/64]: Training Loss: 0.660056010, Training Accuracy: 79.496\n",
            "Worker 1, [54/64]: Training Loss: 0.651376101, Training Accuracy: 80.136\n",
            "Worker 1, [55/64]: Training Loss: 0.648402114, Training Accuracy: 80.256\n",
            "Worker 1, [56/64]: Training Loss: 0.610653701, Training Accuracy: 81.200\n",
            "Worker 1, [57/64]: Training Loss: 0.646220567, Training Accuracy: 80.184\n",
            "Worker 1, [58/64]: Training Loss: 0.647953213, Training Accuracy: 80.216\n",
            "Worker 1, [59/64]: Training Loss: 0.608180329, Training Accuracy: 81.192\n",
            "Worker 1, [60/64]: Training Loss: 0.569092117, Training Accuracy: 82.544\n",
            "Worker 1, [61/64]: Training Loss: 0.589150192, Training Accuracy: 81.976\n",
            "Worker 1, [62/64]: Training Loss: 0.606716525, Training Accuracy: 81.416\n",
            "Worker 1, [63/64]: Training Loss: 0.576855262, Training Accuracy: 82.064\n",
            "Worker 1, [64/64]: Training Loss: 0.562167591, Training Accuracy: 82.408\n",
            "Time taken for training worker 1: 0:06:29.602629\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/64]: Training Loss: 2.927137222, Training Accuracy: 27.056\n",
            "Worker 2, [02/64]: Training Loss: 2.494958831, Training Accuracy: 35.072\n",
            "Worker 2, [03/64]: Training Loss: 2.310921313, Training Accuracy: 38.848\n",
            "Worker 2, [04/64]: Training Loss: 2.173150985, Training Accuracy: 41.576\n",
            "Worker 2, [05/64]: Training Loss: 2.075688158, Training Accuracy: 43.376\n",
            "Worker 2, [06/64]: Training Loss: 1.978176951, Training Accuracy: 45.416\n",
            "Worker 2, [07/64]: Training Loss: 1.920249007, Training Accuracy: 46.696\n",
            "Worker 2, [08/64]: Training Loss: 1.855764005, Training Accuracy: 48.904\n",
            "Worker 2, [09/64]: Training Loss: 1.774486999, Training Accuracy: 50.992\n",
            "Worker 2, [10/64]: Training Loss: 1.718216246, Training Accuracy: 51.608\n",
            "Worker 2, [11/64]: Training Loss: 1.660835619, Training Accuracy: 52.680\n",
            "Worker 2, [12/64]: Training Loss: 1.610330187, Training Accuracy: 54.408\n",
            "Worker 2, [13/64]: Training Loss: 1.559265303, Training Accuracy: 55.896\n",
            "Worker 2, [14/64]: Training Loss: 1.502892496, Training Accuracy: 57.248\n",
            "Worker 2, [15/64]: Training Loss: 1.465979503, Training Accuracy: 57.296\n",
            "Worker 2, [16/64]: Training Loss: 1.417480776, Training Accuracy: 58.888\n",
            "Worker 2, [17/64]: Training Loss: 1.398100305, Training Accuracy: 59.528\n",
            "Worker 2, [18/64]: Training Loss: 1.332733720, Training Accuracy: 61.640\n",
            "Worker 2, [19/64]: Training Loss: 1.328706993, Training Accuracy: 60.976\n",
            "Worker 2, [20/64]: Training Loss: 1.274312043, Training Accuracy: 63.120\n",
            "Worker 2, [21/64]: Training Loss: 1.222441674, Training Accuracy: 64.096\n",
            "Worker 2, [22/64]: Training Loss: 1.197817168, Training Accuracy: 64.416\n",
            "Worker 2, [23/64]: Training Loss: 1.188695252, Training Accuracy: 64.792\n",
            "Worker 2, [24/64]: Training Loss: 1.142964797, Training Accuracy: 65.976\n",
            "Worker 2, [25/64]: Training Loss: 1.102204331, Training Accuracy: 67.336\n",
            "Worker 2, [26/64]: Training Loss: 1.080988064, Training Accuracy: 67.920\n",
            "Worker 2, [27/64]: Training Loss: 1.068445981, Training Accuracy: 68.064\n",
            "Worker 2, [28/64]: Training Loss: 1.049417646, Training Accuracy: 68.816\n",
            "Worker 2, [29/64]: Training Loss: 1.023603909, Training Accuracy: 69.632\n",
            "Worker 2, [30/64]: Training Loss: 0.975319737, Training Accuracy: 70.712\n",
            "Worker 2, [31/64]: Training Loss: 0.966685224, Training Accuracy: 70.576\n",
            "Worker 2, [32/64]: Training Loss: 0.967409575, Training Accuracy: 71.000\n",
            "Worker 2, [33/64]: Training Loss: 0.946630691, Training Accuracy: 71.360\n",
            "Worker 2, [34/64]: Training Loss: 0.923225068, Training Accuracy: 71.816\n",
            "Worker 2, [35/64]: Training Loss: 0.893872504, Training Accuracy: 73.000\n",
            "Worker 2, [36/64]: Training Loss: 0.878121494, Training Accuracy: 73.112\n",
            "Worker 2, [37/64]: Training Loss: 0.857681740, Training Accuracy: 73.816\n",
            "Worker 2, [38/64]: Training Loss: 0.872334000, Training Accuracy: 73.912\n",
            "Worker 2, [39/64]: Training Loss: 0.836707088, Training Accuracy: 74.768\n",
            "Worker 2, [40/64]: Training Loss: 0.801211800, Training Accuracy: 75.472\n",
            "Worker 2, [41/64]: Training Loss: 0.807598101, Training Accuracy: 75.088\n",
            "Worker 2, [42/64]: Training Loss: 0.795101566, Training Accuracy: 75.800\n",
            "Worker 2, [43/64]: Training Loss: 0.793373273, Training Accuracy: 75.888\n",
            "Worker 2, [44/64]: Training Loss: 0.760496558, Training Accuracy: 76.480\n",
            "Worker 2, [45/64]: Training Loss: 0.734796311, Training Accuracy: 77.416\n",
            "Worker 2, [46/64]: Training Loss: 0.733286977, Training Accuracy: 77.016\n",
            "Worker 2, [47/64]: Training Loss: 0.712057346, Training Accuracy: 77.992\n",
            "Worker 2, [48/64]: Training Loss: 0.715245786, Training Accuracy: 77.848\n",
            "Worker 2, [49/64]: Training Loss: 0.721548076, Training Accuracy: 78.176\n",
            "Worker 2, [50/64]: Training Loss: 0.684645460, Training Accuracy: 78.552\n",
            "Worker 2, [51/64]: Training Loss: 0.722224532, Training Accuracy: 77.640\n",
            "Worker 2, [52/64]: Training Loss: 0.678204138, Training Accuracy: 78.952\n",
            "Worker 2, [53/64]: Training Loss: 0.665890839, Training Accuracy: 79.576\n",
            "Worker 2, [54/64]: Training Loss: 0.634881224, Training Accuracy: 80.328\n",
            "Worker 2, [55/64]: Training Loss: 0.617320709, Training Accuracy: 80.712\n",
            "Worker 2, [56/64]: Training Loss: 0.655185455, Training Accuracy: 80.168\n",
            "Worker 2, [57/64]: Training Loss: 0.622488877, Training Accuracy: 80.480\n",
            "Worker 2, [58/64]: Training Loss: 0.623186963, Training Accuracy: 80.192\n",
            "Worker 2, [59/64]: Training Loss: 0.609042169, Training Accuracy: 81.032\n",
            "Worker 2, [60/64]: Training Loss: 0.607840538, Training Accuracy: 81.536\n",
            "Worker 2, [61/64]: Training Loss: 0.594101980, Training Accuracy: 81.408\n",
            "Worker 2, [62/64]: Training Loss: 0.619785814, Training Accuracy: 81.128\n",
            "Worker 2, [63/64]: Training Loss: 0.576276556, Training Accuracy: 82.728\n",
            "Worker 2, [64/64]: Training Loss: 0.585493702, Training Accuracy: 81.952\n",
            "Time taken for training worker 2: 0:06:28.331287\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/64]: Training Loss: 2.927276952, Training Accuracy: 27.272\n",
            "Worker 3, [02/64]: Training Loss: 2.495124620, Training Accuracy: 35.136\n",
            "Worker 3, [03/64]: Training Loss: 2.337056673, Training Accuracy: 37.992\n",
            "Worker 3, [04/64]: Training Loss: 2.193589622, Training Accuracy: 41.008\n",
            "Worker 3, [05/64]: Training Loss: 2.100747022, Training Accuracy: 43.104\n",
            "Worker 3, [06/64]: Training Loss: 2.019440111, Training Accuracy: 44.952\n",
            "Worker 3, [07/64]: Training Loss: 1.933592705, Training Accuracy: 46.752\n",
            "Worker 3, [08/64]: Training Loss: 1.872326898, Training Accuracy: 48.376\n",
            "Worker 3, [09/64]: Training Loss: 1.796797069, Training Accuracy: 50.032\n",
            "Worker 3, [10/64]: Training Loss: 1.735378748, Training Accuracy: 51.344\n",
            "Worker 3, [11/64]: Training Loss: 1.691403092, Training Accuracy: 52.368\n",
            "Worker 3, [12/64]: Training Loss: 1.639050623, Training Accuracy: 54.368\n",
            "Worker 3, [13/64]: Training Loss: 1.562535727, Training Accuracy: 55.392\n",
            "Worker 3, [14/64]: Training Loss: 1.531881760, Training Accuracy: 56.888\n",
            "Worker 3, [15/64]: Training Loss: 1.507428107, Training Accuracy: 57.064\n",
            "Worker 3, [16/64]: Training Loss: 1.450808410, Training Accuracy: 58.192\n",
            "Worker 3, [17/64]: Training Loss: 1.403068205, Training Accuracy: 59.576\n",
            "Worker 3, [18/64]: Training Loss: 1.391271209, Training Accuracy: 59.760\n",
            "Worker 3, [19/64]: Training Loss: 1.344360776, Training Accuracy: 61.128\n",
            "Worker 3, [20/64]: Training Loss: 1.298105735, Training Accuracy: 62.072\n",
            "Worker 3, [21/64]: Training Loss: 1.264289764, Training Accuracy: 62.768\n",
            "Worker 3, [22/64]: Training Loss: 1.238631168, Training Accuracy: 63.760\n",
            "Worker 3, [23/64]: Training Loss: 1.200684905, Training Accuracy: 65.136\n",
            "Worker 3, [24/64]: Training Loss: 1.165304747, Training Accuracy: 65.664\n",
            "Worker 3, [25/64]: Training Loss: 1.165483097, Training Accuracy: 65.568\n",
            "Worker 3, [26/64]: Training Loss: 1.094448312, Training Accuracy: 67.568\n",
            "Worker 3, [27/64]: Training Loss: 1.097726011, Training Accuracy: 67.176\n",
            "Worker 3, [28/64]: Training Loss: 1.056040033, Training Accuracy: 68.720\n",
            "Worker 3, [29/64]: Training Loss: 1.056364841, Training Accuracy: 68.800\n",
            "Worker 3, [30/64]: Training Loss: 1.000963616, Training Accuracy: 69.784\n",
            "Worker 3, [31/64]: Training Loss: 1.006263067, Training Accuracy: 69.896\n",
            "Worker 3, [32/64]: Training Loss: 0.979181866, Training Accuracy: 70.552\n",
            "Worker 3, [33/64]: Training Loss: 0.931651232, Training Accuracy: 71.904\n",
            "Worker 3, [34/64]: Training Loss: 0.943497461, Training Accuracy: 72.008\n",
            "Worker 3, [35/64]: Training Loss: 0.893946105, Training Accuracy: 73.112\n",
            "Worker 3, [36/64]: Training Loss: 0.902494480, Training Accuracy: 73.088\n",
            "Worker 3, [37/64]: Training Loss: 0.887393458, Training Accuracy: 73.160\n",
            "Worker 3, [38/64]: Training Loss: 0.834747566, Training Accuracy: 74.472\n",
            "Worker 3, [39/64]: Training Loss: 0.835534013, Training Accuracy: 75.264\n",
            "Worker 3, [40/64]: Training Loss: 0.850228360, Training Accuracy: 74.352\n",
            "Worker 3, [41/64]: Training Loss: 0.807991885, Training Accuracy: 75.272\n",
            "Worker 3, [42/64]: Training Loss: 0.800575212, Training Accuracy: 75.648\n",
            "Worker 3, [43/64]: Training Loss: 0.784218965, Training Accuracy: 76.088\n",
            "Worker 3, [44/64]: Training Loss: 0.780836234, Training Accuracy: 76.344\n",
            "Worker 3, [45/64]: Training Loss: 0.768135115, Training Accuracy: 76.560\n",
            "Worker 3, [46/64]: Training Loss: 0.737139206, Training Accuracy: 77.504\n",
            "Worker 3, [47/64]: Training Loss: 0.725194461, Training Accuracy: 77.728\n",
            "Worker 3, [48/64]: Training Loss: 0.699620861, Training Accuracy: 78.432\n",
            "Worker 3, [49/64]: Training Loss: 0.720446981, Training Accuracy: 77.968\n",
            "Worker 3, [50/64]: Training Loss: 0.686629902, Training Accuracy: 79.048\n",
            "Worker 3, [51/64]: Training Loss: 0.721187119, Training Accuracy: 78.032\n",
            "Worker 3, [52/64]: Training Loss: 0.675578748, Training Accuracy: 79.320\n",
            "Worker 3, [53/64]: Training Loss: 0.664610672, Training Accuracy: 79.632\n",
            "Worker 3, [54/64]: Training Loss: 0.679916766, Training Accuracy: 79.296\n",
            "Worker 3, [55/64]: Training Loss: 0.655638289, Training Accuracy: 79.760\n",
            "Worker 3, [56/64]: Training Loss: 0.648185294, Training Accuracy: 79.784\n",
            "Worker 3, [57/64]: Training Loss: 0.636114122, Training Accuracy: 80.568\n",
            "Worker 3, [58/64]: Training Loss: 0.609681802, Training Accuracy: 81.336\n",
            "Worker 3, [59/64]: Training Loss: 0.625173970, Training Accuracy: 80.944\n",
            "Worker 3, [60/64]: Training Loss: 0.620602906, Training Accuracy: 80.968\n",
            "Worker 3, [61/64]: Training Loss: 0.610542275, Training Accuracy: 81.368\n",
            "Worker 3, [62/64]: Training Loss: 0.582502268, Training Accuracy: 82.144\n",
            "Worker 3, [63/64]: Training Loss: 0.594378181, Training Accuracy: 81.848\n",
            "Worker 3, [64/64]: Training Loss: 0.577161588, Training Accuracy: 82.352\n",
            "Time taken for training worker 3: 0:06:28.397399\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/64]: Training Loss: 2.939229070, Training Accuracy: 27.296\n",
            "Worker 4, [02/64]: Training Loss: 2.510196994, Training Accuracy: 34.904\n",
            "Worker 4, [03/64]: Training Loss: 2.337180740, Training Accuracy: 38.160\n",
            "Worker 4, [04/64]: Training Loss: 2.221968300, Training Accuracy: 41.312\n",
            "Worker 4, [05/64]: Training Loss: 2.102866227, Training Accuracy: 43.064\n",
            "Worker 4, [06/64]: Training Loss: 2.024408246, Training Accuracy: 44.808\n",
            "Worker 4, [07/64]: Training Loss: 1.941937008, Training Accuracy: 46.744\n",
            "Worker 4, [08/64]: Training Loss: 1.879509062, Training Accuracy: 48.288\n",
            "Worker 4, [09/64]: Training Loss: 1.817607272, Training Accuracy: 49.856\n",
            "Worker 4, [10/64]: Training Loss: 1.753275301, Training Accuracy: 50.944\n",
            "Worker 4, [11/64]: Training Loss: 1.688723345, Training Accuracy: 52.800\n",
            "Worker 4, [12/64]: Training Loss: 1.648751585, Training Accuracy: 53.536\n",
            "Worker 4, [13/64]: Training Loss: 1.607940173, Training Accuracy: 54.384\n",
            "Worker 4, [14/64]: Training Loss: 1.560727508, Training Accuracy: 55.496\n",
            "Worker 4, [15/64]: Training Loss: 1.495937190, Training Accuracy: 57.688\n",
            "Worker 4, [16/64]: Training Loss: 1.449845259, Training Accuracy: 58.520\n",
            "Worker 4, [17/64]: Training Loss: 1.420400024, Training Accuracy: 59.280\n",
            "Worker 4, [18/64]: Training Loss: 1.372420229, Training Accuracy: 60.280\n",
            "Worker 4, [19/64]: Training Loss: 1.374968577, Training Accuracy: 60.416\n",
            "Worker 4, [20/64]: Training Loss: 1.302777254, Training Accuracy: 62.040\n",
            "Worker 4, [21/64]: Training Loss: 1.284548521, Training Accuracy: 62.592\n",
            "Worker 4, [22/64]: Training Loss: 1.252903054, Training Accuracy: 62.888\n",
            "Worker 4, [23/64]: Training Loss: 1.200646310, Training Accuracy: 65.064\n",
            "Worker 4, [24/64]: Training Loss: 1.183791115, Training Accuracy: 65.400\n",
            "Worker 4, [25/64]: Training Loss: 1.178234796, Training Accuracy: 65.752\n",
            "Worker 4, [26/64]: Training Loss: 1.121497519, Training Accuracy: 66.968\n",
            "Worker 4, [27/64]: Training Loss: 1.105532201, Training Accuracy: 67.120\n",
            "Worker 4, [28/64]: Training Loss: 1.075740925, Training Accuracy: 68.544\n",
            "Worker 4, [29/64]: Training Loss: 1.053136175, Training Accuracy: 68.592\n",
            "Worker 4, [30/64]: Training Loss: 1.034674412, Training Accuracy: 69.152\n",
            "Worker 4, [31/64]: Training Loss: 1.009730969, Training Accuracy: 69.968\n",
            "Worker 4, [32/64]: Training Loss: 0.976841014, Training Accuracy: 70.944\n",
            "Worker 4, [33/64]: Training Loss: 0.974018588, Training Accuracy: 71.080\n",
            "Worker 4, [34/64]: Training Loss: 0.931954268, Training Accuracy: 72.048\n",
            "Worker 4, [35/64]: Training Loss: 0.922665163, Training Accuracy: 72.480\n",
            "Worker 4, [36/64]: Training Loss: 0.917083220, Training Accuracy: 72.368\n",
            "Worker 4, [37/64]: Training Loss: 0.882875373, Training Accuracy: 72.952\n",
            "Worker 4, [38/64]: Training Loss: 0.874212733, Training Accuracy: 73.704\n",
            "Worker 4, [39/64]: Training Loss: 0.863283240, Training Accuracy: 73.808\n",
            "Worker 4, [40/64]: Training Loss: 0.843635425, Training Accuracy: 74.728\n",
            "Worker 4, [41/64]: Training Loss: 0.828113910, Training Accuracy: 74.936\n",
            "Worker 4, [42/64]: Training Loss: 0.813914519, Training Accuracy: 75.376\n",
            "Worker 4, [43/64]: Training Loss: 0.788668431, Training Accuracy: 76.120\n",
            "Worker 4, [44/64]: Training Loss: 0.784016114, Training Accuracy: 76.208\n",
            "Worker 4, [45/64]: Training Loss: 0.758613796, Training Accuracy: 77.120\n",
            "Worker 4, [46/64]: Training Loss: 0.750736134, Training Accuracy: 77.328\n",
            "Worker 4, [47/64]: Training Loss: 0.749148233, Training Accuracy: 77.000\n",
            "Worker 4, [48/64]: Training Loss: 0.737472794, Training Accuracy: 77.520\n",
            "Worker 4, [49/64]: Training Loss: 0.729426681, Training Accuracy: 77.576\n",
            "Worker 4, [50/64]: Training Loss: 0.691779189, Training Accuracy: 78.968\n",
            "Worker 4, [51/64]: Training Loss: 0.711012725, Training Accuracy: 78.248\n",
            "Worker 4, [52/64]: Training Loss: 0.707350735, Training Accuracy: 78.464\n",
            "Worker 4, [53/64]: Training Loss: 0.690033250, Training Accuracy: 78.848\n",
            "Worker 4, [54/64]: Training Loss: 0.664084348, Training Accuracy: 80.064\n",
            "Worker 4, [55/64]: Training Loss: 0.640796936, Training Accuracy: 80.600\n",
            "Worker 4, [56/64]: Training Loss: 0.633862567, Training Accuracy: 80.720\n",
            "Worker 4, [57/64]: Training Loss: 0.630470609, Training Accuracy: 80.952\n",
            "Worker 4, [58/64]: Training Loss: 0.655997661, Training Accuracy: 80.152\n",
            "Worker 4, [59/64]: Training Loss: 0.630902995, Training Accuracy: 80.912\n",
            "Worker 4, [60/64]: Training Loss: 0.605637155, Training Accuracy: 81.488\n",
            "Worker 4, [61/64]: Training Loss: 0.617758290, Training Accuracy: 81.072\n",
            "Worker 4, [62/64]: Training Loss: 0.607627305, Training Accuracy: 81.304\n",
            "Worker 4, [63/64]: Training Loss: 0.586920833, Training Accuracy: 82.072\n",
            "Worker 4, [64/64]: Training Loss: 0.571787636, Training Accuracy: 82.424\n",
            "Time taken for training worker 4: 0:06:30.460731\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002842\n",
            "Global Update 02: Test Loss: 4.001948534, Test Accuracy: 40.360\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:51:55.854187\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:4\n",
            "==================================================\n",
            "Worker 1, [01/04]: Training Loss: 4.597491255, Training Accuracy: 1.472\n",
            "Worker 1, [02/04]: Training Loss: 4.406681226, Training Accuracy: 3.648\n",
            "Worker 1, [03/04]: Training Loss: 4.156238189, Training Accuracy: 5.680\n",
            "Worker 1, [04/04]: Training Loss: 4.027430011, Training Accuracy: 6.592\n",
            "Time taken for training worker 1: 0:00:11.813346\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.595790513, Training Accuracy: 2.160\n",
            "Worker 2, [02/04]: Training Loss: 4.412743948, Training Accuracy: 3.664\n",
            "Worker 2, [03/04]: Training Loss: 4.187023822, Training Accuracy: 5.776\n",
            "Worker 2, [04/04]: Training Loss: 4.036725679, Training Accuracy: 7.808\n",
            "Time taken for training worker 2: 0:00:12.685753\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 4.596884976, Training Accuracy: 1.568\n",
            "Worker 3, [02/04]: Training Loss: 4.414741944, Training Accuracy: 3.296\n",
            "Worker 3, [03/04]: Training Loss: 4.180357376, Training Accuracy: 5.456\n",
            "Worker 3, [04/04]: Training Loss: 4.035548614, Training Accuracy: 7.264\n",
            "Time taken for training worker 3: 0:00:12.452516\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 4.596086541, Training Accuracy: 1.744\n",
            "Worker 4, [02/04]: Training Loss: 4.393046058, Training Accuracy: 3.664\n",
            "Worker 4, [03/04]: Training Loss: 4.154025523, Training Accuracy: 5.744\n",
            "Worker 4, [04/04]: Training Loss: 4.027087336, Training Accuracy: 7.392\n",
            "Time taken for training worker 4: 0:00:12.599910\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 4.596034142, Training Accuracy: 1.952\n",
            "Worker 5, [02/04]: Training Loss: 4.405978855, Training Accuracy: 3.584\n",
            "Worker 5, [03/04]: Training Loss: 4.192252057, Training Accuracy: 5.184\n",
            "Worker 5, [04/04]: Training Loss: 4.070563460, Training Accuracy: 6.496\n",
            "Time taken for training worker 5: 0:00:11.813959\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 4.595150120, Training Accuracy: 2.016\n",
            "Worker 6, [02/04]: Training Loss: 4.417665117, Training Accuracy: 3.456\n",
            "Worker 6, [03/04]: Training Loss: 4.184357483, Training Accuracy: 5.824\n",
            "Worker 6, [04/04]: Training Loss: 4.049710429, Training Accuracy: 7.088\n",
            "Time taken for training worker 6: 0:00:11.915769\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 4.596559072, Training Accuracy: 1.504\n",
            "Worker 7, [02/04]: Training Loss: 4.424840893, Training Accuracy: 3.520\n",
            "Worker 7, [03/04]: Training Loss: 4.184350897, Training Accuracy: 6.048\n",
            "Worker 7, [04/04]: Training Loss: 4.065354036, Training Accuracy: 7.120\n",
            "Time taken for training worker 7: 0:00:11.950399\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 4.594889865, Training Accuracy: 2.000\n",
            "Worker 8, [02/04]: Training Loss: 4.393690202, Training Accuracy: 3.632\n",
            "Worker 8, [03/04]: Training Loss: 4.171449491, Training Accuracy: 5.792\n",
            "Worker 8, [04/04]: Training Loss: 4.008577232, Training Accuracy: 7.520\n",
            "Time taken for training worker 8: 0:00:13.047856\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004909\n",
            "Global Update 01: Test Loss: 3.983706705, Test Accuracy: 9.990\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.972193701, Training Accuracy: 7.936\n",
            "Worker 1, [02/04]: Training Loss: 3.851404085, Training Accuracy: 9.712\n",
            "Worker 1, [03/04]: Training Loss: 3.764129721, Training Accuracy: 10.304\n",
            "Worker 1, [04/04]: Training Loss: 3.669558751, Training Accuracy: 12.016\n",
            "Time taken for training worker 1: 0:00:11.830087\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.993840371, Training Accuracy: 8.304\n",
            "Worker 2, [02/04]: Training Loss: 3.877831627, Training Accuracy: 10.096\n",
            "Worker 2, [03/04]: Training Loss: 3.776872306, Training Accuracy: 11.920\n",
            "Worker 2, [04/04]: Training Loss: 3.688672443, Training Accuracy: 12.832\n",
            "Time taken for training worker 2: 0:00:12.044436\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.975251361, Training Accuracy: 8.816\n",
            "Worker 3, [02/04]: Training Loss: 3.868488115, Training Accuracy: 10.208\n",
            "Worker 3, [03/04]: Training Loss: 3.777483091, Training Accuracy: 11.264\n",
            "Worker 3, [04/04]: Training Loss: 3.678397047, Training Accuracy: 12.560\n",
            "Time taken for training worker 3: 0:00:12.025564\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.970166620, Training Accuracy: 8.528\n",
            "Worker 4, [02/04]: Training Loss: 3.878092673, Training Accuracy: 9.280\n",
            "Worker 4, [03/04]: Training Loss: 3.761901512, Training Accuracy: 10.800\n",
            "Worker 4, [04/04]: Training Loss: 3.663087003, Training Accuracy: 12.816\n",
            "Time taken for training worker 4: 0:00:12.448442\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 3.992898048, Training Accuracy: 8.016\n",
            "Worker 5, [02/04]: Training Loss: 3.891409003, Training Accuracy: 9.200\n",
            "Worker 5, [03/04]: Training Loss: 3.798221269, Training Accuracy: 10.880\n",
            "Worker 5, [04/04]: Training Loss: 3.694524441, Training Accuracy: 12.960\n",
            "Time taken for training worker 5: 0:00:12.456951\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 4.002074169, Training Accuracy: 7.760\n",
            "Worker 6, [02/04]: Training Loss: 3.880942106, Training Accuracy: 9.600\n",
            "Worker 6, [03/04]: Training Loss: 3.797490293, Training Accuracy: 10.848\n",
            "Worker 6, [04/04]: Training Loss: 3.688194010, Training Accuracy: 12.384\n",
            "Time taken for training worker 6: 0:00:12.060642\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 4.003129205, Training Accuracy: 8.576\n",
            "Worker 7, [02/04]: Training Loss: 3.879685696, Training Accuracy: 10.304\n",
            "Worker 7, [03/04]: Training Loss: 3.782099884, Training Accuracy: 11.136\n",
            "Worker 7, [04/04]: Training Loss: 3.713591004, Training Accuracy: 12.464\n",
            "Time taken for training worker 7: 0:00:11.651251\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 3.967803541, Training Accuracy: 8.416\n",
            "Worker 8, [02/04]: Training Loss: 3.846394388, Training Accuracy: 10.320\n",
            "Worker 8, [03/04]: Training Loss: 3.774543976, Training Accuracy: 10.832\n",
            "Worker 8, [04/04]: Training Loss: 3.695810211, Training Accuracy: 12.592\n",
            "Time taken for training worker 8: 0:00:11.582140\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003897\n",
            "Global Update 02: Test Loss: 4.144032105, Test Accuracy: 14.410\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.697663288, Training Accuracy: 12.704\n",
            "Worker 1, [02/04]: Training Loss: 3.567761217, Training Accuracy: 13.456\n",
            "Worker 1, [03/04]: Training Loss: 3.466902784, Training Accuracy: 16.032\n",
            "Worker 1, [04/04]: Training Loss: 3.386761909, Training Accuracy: 17.552\n",
            "Time taken for training worker 1: 0:00:12.979462\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.703686111, Training Accuracy: 12.912\n",
            "Worker 2, [02/04]: Training Loss: 3.570746587, Training Accuracy: 14.688\n",
            "Worker 2, [03/04]: Training Loss: 3.475074593, Training Accuracy: 16.672\n",
            "Worker 2, [04/04]: Training Loss: 3.391043201, Training Accuracy: 17.696\n",
            "Time taken for training worker 2: 0:00:12.664814\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.689237145, Training Accuracy: 13.312\n",
            "Worker 3, [02/04]: Training Loss: 3.576521311, Training Accuracy: 14.528\n",
            "Worker 3, [03/04]: Training Loss: 3.486989795, Training Accuracy: 16.096\n",
            "Worker 3, [04/04]: Training Loss: 3.424444882, Training Accuracy: 16.816\n",
            "Time taken for training worker 3: 0:00:11.776952\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.701068720, Training Accuracy: 12.832\n",
            "Worker 4, [02/04]: Training Loss: 3.556237145, Training Accuracy: 14.128\n",
            "Worker 4, [03/04]: Training Loss: 3.462421899, Training Accuracy: 16.544\n",
            "Worker 4, [04/04]: Training Loss: 3.379008242, Training Accuracy: 17.824\n",
            "Time taken for training worker 4: 0:00:12.164719\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 3.730187861, Training Accuracy: 12.784\n",
            "Worker 5, [02/04]: Training Loss: 3.579277861, Training Accuracy: 15.104\n",
            "Worker 5, [03/04]: Training Loss: 3.503571902, Training Accuracy: 16.128\n",
            "Worker 5, [04/04]: Training Loss: 3.403959204, Training Accuracy: 17.744\n",
            "Time taken for training worker 5: 0:00:11.872558\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 3.737305593, Training Accuracy: 11.968\n",
            "Worker 6, [02/04]: Training Loss: 3.578600730, Training Accuracy: 14.976\n",
            "Worker 6, [03/04]: Training Loss: 3.492295791, Training Accuracy: 15.360\n",
            "Worker 6, [04/04]: Training Loss: 3.416493309, Training Accuracy: 16.752\n",
            "Time taken for training worker 6: 0:00:12.298110\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 3.736408080, Training Accuracy: 12.144\n",
            "Worker 7, [02/04]: Training Loss: 3.595798132, Training Accuracy: 14.960\n",
            "Worker 7, [03/04]: Training Loss: 3.490404723, Training Accuracy: 15.616\n",
            "Worker 7, [04/04]: Training Loss: 3.411744543, Training Accuracy: 16.992\n",
            "Time taken for training worker 7: 0:00:12.652712\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 3.717742205, Training Accuracy: 12.416\n",
            "Worker 8, [02/04]: Training Loss: 3.567355791, Training Accuracy: 13.760\n",
            "Worker 8, [03/04]: Training Loss: 3.502186824, Training Accuracy: 15.616\n",
            "Worker 8, [04/04]: Training Loss: 3.420175479, Training Accuracy: 16.352\n",
            "Time taken for training worker 8: 0:00:12.342846\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004282\n",
            "Global Update 03: Test Loss: 3.536247589, Test Accuracy: 20.200\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.449322387, Training Accuracy: 16.560\n",
            "Worker 1, [02/04]: Training Loss: 3.298672737, Training Accuracy: 18.928\n",
            "Worker 1, [03/04]: Training Loss: 3.242316407, Training Accuracy: 20.112\n",
            "Worker 1, [04/04]: Training Loss: 3.146029613, Training Accuracy: 21.808\n",
            "Time taken for training worker 1: 0:00:11.978409\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.425882790, Training Accuracy: 17.568\n",
            "Worker 2, [02/04]: Training Loss: 3.282238732, Training Accuracy: 19.680\n",
            "Worker 2, [03/04]: Training Loss: 3.208372712, Training Accuracy: 20.368\n",
            "Worker 2, [04/04]: Training Loss: 3.142974394, Training Accuracy: 21.968\n",
            "Time taken for training worker 2: 0:00:12.131160\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.434997040, Training Accuracy: 17.200\n",
            "Worker 3, [02/04]: Training Loss: 3.341031568, Training Accuracy: 18.400\n",
            "Worker 3, [03/04]: Training Loss: 3.235204548, Training Accuracy: 20.336\n",
            "Worker 3, [04/04]: Training Loss: 3.149493057, Training Accuracy: 21.840\n",
            "Time taken for training worker 3: 0:00:11.747972\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.433599606, Training Accuracy: 17.024\n",
            "Worker 4, [02/04]: Training Loss: 3.303310903, Training Accuracy: 18.880\n",
            "Worker 4, [03/04]: Training Loss: 3.219682377, Training Accuracy: 20.416\n",
            "Worker 4, [04/04]: Training Loss: 3.150650774, Training Accuracy: 21.728\n",
            "Time taken for training worker 4: 0:00:11.942213\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 3.456157066, Training Accuracy: 16.880\n",
            "Worker 5, [02/04]: Training Loss: 3.324774445, Training Accuracy: 19.696\n",
            "Worker 5, [03/04]: Training Loss: 3.243498082, Training Accuracy: 20.272\n",
            "Worker 5, [04/04]: Training Loss: 3.170326238, Training Accuracy: 21.440\n",
            "Time taken for training worker 5: 0:00:11.998285\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 3.450715046, Training Accuracy: 17.648\n",
            "Worker 6, [02/04]: Training Loss: 3.346424446, Training Accuracy: 18.352\n",
            "Worker 6, [03/04]: Training Loss: 3.231461484, Training Accuracy: 20.704\n",
            "Worker 6, [04/04]: Training Loss: 3.160945822, Training Accuracy: 22.320\n",
            "Time taken for training worker 6: 0:00:12.213174\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 3.471792425, Training Accuracy: 17.184\n",
            "Worker 7, [02/04]: Training Loss: 3.331376477, Training Accuracy: 18.848\n",
            "Worker 7, [03/04]: Training Loss: 3.264791265, Training Accuracy: 20.192\n",
            "Worker 7, [04/04]: Training Loss: 3.180538907, Training Accuracy: 21.408\n",
            "Time taken for training worker 7: 0:00:12.137192\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 3.440390803, Training Accuracy: 16.400\n",
            "Worker 8, [02/04]: Training Loss: 3.288457282, Training Accuracy: 19.152\n",
            "Worker 8, [03/04]: Training Loss: 3.213733449, Training Accuracy: 19.728\n",
            "Worker 8, [04/04]: Training Loss: 3.147813138, Training Accuracy: 21.824\n",
            "Time taken for training worker 8: 0:00:12.446932\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003976\n",
            "Global Update 04: Test Loss: 3.092118849, Test Accuracy: 24.240\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.180990338, Training Accuracy: 21.136\n",
            "Worker 1, [02/04]: Training Loss: 3.085294429, Training Accuracy: 23.024\n",
            "Worker 1, [03/04]: Training Loss: 3.013774298, Training Accuracy: 23.360\n",
            "Worker 1, [04/04]: Training Loss: 2.917616555, Training Accuracy: 25.424\n",
            "Time taken for training worker 1: 0:00:13.027128\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.181493003, Training Accuracy: 22.768\n",
            "Worker 2, [02/04]: Training Loss: 3.074079606, Training Accuracy: 23.984\n",
            "Worker 2, [03/04]: Training Loss: 3.004045002, Training Accuracy: 24.528\n",
            "Worker 2, [04/04]: Training Loss: 2.937888484, Training Accuracy: 25.360\n",
            "Time taken for training worker 2: 0:00:12.636606\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.224642328, Training Accuracy: 20.896\n",
            "Worker 3, [02/04]: Training Loss: 3.105645897, Training Accuracy: 22.192\n",
            "Worker 3, [03/04]: Training Loss: 3.016422649, Training Accuracy: 24.112\n",
            "Worker 3, [04/04]: Training Loss: 2.958098976, Training Accuracy: 24.976\n",
            "Time taken for training worker 3: 0:00:12.634846\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.200146103, Training Accuracy: 22.208\n",
            "Worker 4, [02/04]: Training Loss: 3.085796590, Training Accuracy: 22.976\n",
            "Worker 4, [03/04]: Training Loss: 3.015933287, Training Accuracy: 24.848\n",
            "Worker 4, [04/04]: Training Loss: 2.941853095, Training Accuracy: 25.344\n",
            "Time taken for training worker 4: 0:00:12.449198\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 3.217361861, Training Accuracy: 21.824\n",
            "Worker 5, [02/04]: Training Loss: 3.114925166, Training Accuracy: 22.624\n",
            "Worker 5, [03/04]: Training Loss: 3.027791128, Training Accuracy: 24.672\n",
            "Worker 5, [04/04]: Training Loss: 2.989001534, Training Accuracy: 25.072\n",
            "Time taken for training worker 5: 0:00:12.647202\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 3.251610746, Training Accuracy: 20.960\n",
            "Worker 6, [02/04]: Training Loss: 3.128196108, Training Accuracy: 22.080\n",
            "Worker 6, [03/04]: Training Loss: 3.033477606, Training Accuracy: 24.064\n",
            "Worker 6, [04/04]: Training Loss: 2.951618212, Training Accuracy: 25.792\n",
            "Time taken for training worker 6: 0:00:12.269327\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 3.245681741, Training Accuracy: 20.704\n",
            "Worker 7, [02/04]: Training Loss: 3.118576077, Training Accuracy: 22.464\n",
            "Worker 7, [03/04]: Training Loss: 3.024780200, Training Accuracy: 23.456\n",
            "Worker 7, [04/04]: Training Loss: 2.970572501, Training Accuracy: 25.328\n",
            "Time taken for training worker 7: 0:00:12.100170\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 3.207592149, Training Accuracy: 21.040\n",
            "Worker 8, [02/04]: Training Loss: 3.100873881, Training Accuracy: 22.448\n",
            "Worker 8, [03/04]: Training Loss: 3.024291345, Training Accuracy: 23.936\n",
            "Worker 8, [04/04]: Training Loss: 2.948083209, Training Accuracy: 24.880\n",
            "Time taken for training worker 8: 0:00:11.916112\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003893\n",
            "Global Update 05: Test Loss: 2.887031015, Test Accuracy: 28.060\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.017545350, Training Accuracy: 24.784\n",
            "Worker 1, [02/04]: Training Loss: 2.924100946, Training Accuracy: 26.192\n",
            "Worker 1, [03/04]: Training Loss: 2.823162653, Training Accuracy: 27.568\n",
            "Worker 1, [04/04]: Training Loss: 2.755340124, Training Accuracy: 28.544\n",
            "Time taken for training worker 1: 0:00:11.989490\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.010223445, Training Accuracy: 25.200\n",
            "Worker 2, [02/04]: Training Loss: 2.919895369, Training Accuracy: 26.624\n",
            "Worker 2, [03/04]: Training Loss: 2.825061891, Training Accuracy: 28.016\n",
            "Worker 2, [04/04]: Training Loss: 2.764562210, Training Accuracy: 28.832\n",
            "Time taken for training worker 2: 0:00:11.980329\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.039484834, Training Accuracy: 24.896\n",
            "Worker 3, [02/04]: Training Loss: 2.960615173, Training Accuracy: 25.232\n",
            "Worker 3, [03/04]: Training Loss: 2.857953033, Training Accuracy: 27.216\n",
            "Worker 3, [04/04]: Training Loss: 2.761162033, Training Accuracy: 29.648\n",
            "Time taken for training worker 3: 0:00:11.696392\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.006431954, Training Accuracy: 24.816\n",
            "Worker 4, [02/04]: Training Loss: 2.917391441, Training Accuracy: 26.432\n",
            "Worker 4, [03/04]: Training Loss: 2.828091446, Training Accuracy: 28.016\n",
            "Worker 4, [04/04]: Training Loss: 2.780804104, Training Accuracy: 29.024\n",
            "Time taken for training worker 4: 0:00:12.897288\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 3.076754857, Training Accuracy: 24.032\n",
            "Worker 5, [02/04]: Training Loss: 2.952105697, Training Accuracy: 25.424\n",
            "Worker 5, [03/04]: Training Loss: 2.836294323, Training Accuracy: 28.352\n",
            "Worker 5, [04/04]: Training Loss: 2.752137734, Training Accuracy: 29.408\n",
            "Time taken for training worker 5: 0:00:12.850590\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 3.050967774, Training Accuracy: 24.688\n",
            "Worker 6, [02/04]: Training Loss: 2.959701307, Training Accuracy: 25.408\n",
            "Worker 6, [03/04]: Training Loss: 2.861978587, Training Accuracy: 28.032\n",
            "Worker 6, [04/04]: Training Loss: 2.809481995, Training Accuracy: 28.160\n",
            "Time taken for training worker 6: 0:00:12.051001\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 3.059766728, Training Accuracy: 23.296\n",
            "Worker 7, [02/04]: Training Loss: 2.917993816, Training Accuracy: 26.208\n",
            "Worker 7, [03/04]: Training Loss: 2.846461087, Training Accuracy: 27.008\n",
            "Worker 7, [04/04]: Training Loss: 2.782750458, Training Accuracy: 29.200\n",
            "Time taken for training worker 7: 0:00:12.401151\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 3.058813280, Training Accuracy: 24.064\n",
            "Worker 8, [02/04]: Training Loss: 2.908372465, Training Accuracy: 26.240\n",
            "Worker 8, [03/04]: Training Loss: 2.855908399, Training Accuracy: 26.368\n",
            "Worker 8, [04/04]: Training Loss: 2.769914705, Training Accuracy: 28.208\n",
            "Time taken for training worker 8: 0:00:12.610482\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004556\n",
            "Global Update 06: Test Loss: 2.701198062, Test Accuracy: 31.670\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.844882753, Training Accuracy: 28.560\n",
            "Worker 1, [02/04]: Training Loss: 2.723520539, Training Accuracy: 30.400\n",
            "Worker 1, [03/04]: Training Loss: 2.642967151, Training Accuracy: 31.744\n",
            "Worker 1, [04/04]: Training Loss: 2.604274638, Training Accuracy: 32.656\n",
            "Time taken for training worker 1: 0:00:12.513980\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.862552241, Training Accuracy: 28.080\n",
            "Worker 2, [02/04]: Training Loss: 2.747541875, Training Accuracy: 30.080\n",
            "Worker 2, [03/04]: Training Loss: 2.668756937, Training Accuracy: 31.760\n",
            "Worker 2, [04/04]: Training Loss: 2.621783310, Training Accuracy: 31.168\n",
            "Time taken for training worker 2: 0:00:13.124812\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.886530224, Training Accuracy: 27.248\n",
            "Worker 3, [02/04]: Training Loss: 2.763005561, Training Accuracy: 29.152\n",
            "Worker 3, [03/04]: Training Loss: 2.681776966, Training Accuracy: 30.880\n",
            "Worker 3, [04/04]: Training Loss: 2.622202601, Training Accuracy: 31.728\n",
            "Time taken for training worker 3: 0:00:12.272989\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.860529926, Training Accuracy: 27.632\n",
            "Worker 4, [02/04]: Training Loss: 2.780903646, Training Accuracy: 28.272\n",
            "Worker 4, [03/04]: Training Loss: 2.648472757, Training Accuracy: 32.080\n",
            "Worker 4, [04/04]: Training Loss: 2.600623919, Training Accuracy: 32.400\n",
            "Time taken for training worker 4: 0:00:11.949139\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.909230300, Training Accuracy: 26.864\n",
            "Worker 5, [02/04]: Training Loss: 2.756416017, Training Accuracy: 29.216\n",
            "Worker 5, [03/04]: Training Loss: 2.673873580, Training Accuracy: 30.656\n",
            "Worker 5, [04/04]: Training Loss: 2.595712779, Training Accuracy: 32.752\n",
            "Time taken for training worker 5: 0:00:12.621716\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.902121629, Training Accuracy: 27.968\n",
            "Worker 6, [02/04]: Training Loss: 2.789041162, Training Accuracy: 28.912\n",
            "Worker 6, [03/04]: Training Loss: 2.668658945, Training Accuracy: 31.488\n",
            "Worker 6, [04/04]: Training Loss: 2.624682624, Training Accuracy: 32.288\n",
            "Time taken for training worker 6: 0:00:12.075942\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.893686299, Training Accuracy: 27.408\n",
            "Worker 7, [02/04]: Training Loss: 2.755851624, Training Accuracy: 29.696\n",
            "Worker 7, [03/04]: Training Loss: 2.686205229, Training Accuracy: 30.560\n",
            "Worker 7, [04/04]: Training Loss: 2.611959759, Training Accuracy: 31.440\n",
            "Time taken for training worker 7: 0:00:11.931511\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.892448708, Training Accuracy: 26.912\n",
            "Worker 8, [02/04]: Training Loss: 2.769564480, Training Accuracy: 28.848\n",
            "Worker 8, [03/04]: Training Loss: 2.706892634, Training Accuracy: 30.416\n",
            "Worker 8, [04/04]: Training Loss: 2.618515370, Training Accuracy: 31.952\n",
            "Time taken for training worker 8: 0:00:12.928330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004066\n",
            "Global Update 07: Test Loss: 2.539940729, Test Accuracy: 35.100\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.702114383, Training Accuracy: 30.688\n",
            "Worker 1, [02/04]: Training Loss: 2.595974640, Training Accuracy: 32.272\n",
            "Worker 1, [03/04]: Training Loss: 2.521606149, Training Accuracy: 34.224\n",
            "Worker 1, [04/04]: Training Loss: 2.412055526, Training Accuracy: 35.744\n",
            "Time taken for training worker 1: 0:00:11.917726\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.722583664, Training Accuracy: 30.640\n",
            "Worker 2, [02/04]: Training Loss: 2.605236640, Training Accuracy: 32.512\n",
            "Worker 2, [03/04]: Training Loss: 2.520807626, Training Accuracy: 34.352\n",
            "Worker 2, [04/04]: Training Loss: 2.416267849, Training Accuracy: 36.816\n",
            "Time taken for training worker 2: 0:00:12.715060\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.767242164, Training Accuracy: 29.904\n",
            "Worker 3, [02/04]: Training Loss: 2.611829334, Training Accuracy: 32.480\n",
            "Worker 3, [03/04]: Training Loss: 2.542569766, Training Accuracy: 33.456\n",
            "Worker 3, [04/04]: Training Loss: 2.472287081, Training Accuracy: 35.312\n",
            "Time taken for training worker 3: 0:00:12.425098\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.738524904, Training Accuracy: 30.304\n",
            "Worker 4, [02/04]: Training Loss: 2.607857743, Training Accuracy: 32.752\n",
            "Worker 4, [03/04]: Training Loss: 2.508363725, Training Accuracy: 33.824\n",
            "Worker 4, [04/04]: Training Loss: 2.462575427, Training Accuracy: 34.672\n",
            "Time taken for training worker 4: 0:00:11.878663\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.766105503, Training Accuracy: 29.744\n",
            "Worker 5, [02/04]: Training Loss: 2.647579327, Training Accuracy: 32.576\n",
            "Worker 5, [03/04]: Training Loss: 2.540689074, Training Accuracy: 33.456\n",
            "Worker 5, [04/04]: Training Loss: 2.450923685, Training Accuracy: 35.536\n",
            "Time taken for training worker 5: 0:00:12.153242\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.746979151, Training Accuracy: 30.560\n",
            "Worker 6, [02/04]: Training Loss: 2.610779621, Training Accuracy: 32.240\n",
            "Worker 6, [03/04]: Training Loss: 2.534368715, Training Accuracy: 34.208\n",
            "Worker 6, [04/04]: Training Loss: 2.467817670, Training Accuracy: 35.520\n",
            "Time taken for training worker 6: 0:00:12.640533\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.763923484, Training Accuracy: 29.664\n",
            "Worker 7, [02/04]: Training Loss: 2.645297880, Training Accuracy: 31.504\n",
            "Worker 7, [03/04]: Training Loss: 2.524515330, Training Accuracy: 34.032\n",
            "Worker 7, [04/04]: Training Loss: 2.452056503, Training Accuracy: 36.208\n",
            "Time taken for training worker 7: 0:00:12.112992\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.740952548, Training Accuracy: 30.064\n",
            "Worker 8, [02/04]: Training Loss: 2.612585771, Training Accuracy: 32.256\n",
            "Worker 8, [03/04]: Training Loss: 2.529597970, Training Accuracy: 34.320\n",
            "Worker 8, [04/04]: Training Loss: 2.482834682, Training Accuracy: 35.360\n",
            "Time taken for training worker 8: 0:00:12.029528\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003947\n",
            "Global Update 08: Test Loss: 2.436392372, Test Accuracy: 37.550\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.597672696, Training Accuracy: 32.528\n",
            "Worker 1, [02/04]: Training Loss: 2.484401194, Training Accuracy: 34.992\n",
            "Worker 1, [03/04]: Training Loss: 2.359393316, Training Accuracy: 37.888\n",
            "Worker 1, [04/04]: Training Loss: 2.315075632, Training Accuracy: 37.600\n",
            "Time taken for training worker 1: 0:00:12.312093\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.611238421, Training Accuracy: 32.960\n",
            "Worker 2, [02/04]: Training Loss: 2.459538709, Training Accuracy: 35.696\n",
            "Worker 2, [03/04]: Training Loss: 2.381740857, Training Accuracy: 37.264\n",
            "Worker 2, [04/04]: Training Loss: 2.299676552, Training Accuracy: 38.960\n",
            "Time taken for training worker 2: 0:00:12.361492\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.581841733, Training Accuracy: 33.504\n",
            "Worker 3, [02/04]: Training Loss: 2.488575471, Training Accuracy: 34.592\n",
            "Worker 3, [03/04]: Training Loss: 2.393575695, Training Accuracy: 36.560\n",
            "Worker 3, [04/04]: Training Loss: 2.300101871, Training Accuracy: 38.576\n",
            "Time taken for training worker 3: 0:00:12.225962\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.607694974, Training Accuracy: 33.184\n",
            "Worker 4, [02/04]: Training Loss: 2.511890767, Training Accuracy: 34.304\n",
            "Worker 4, [03/04]: Training Loss: 2.412493034, Training Accuracy: 36.192\n",
            "Worker 4, [04/04]: Training Loss: 2.310706490, Training Accuracy: 38.176\n",
            "Time taken for training worker 4: 0:00:12.447519\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.601617689, Training Accuracy: 32.032\n",
            "Worker 5, [02/04]: Training Loss: 2.496785562, Training Accuracy: 34.656\n",
            "Worker 5, [03/04]: Training Loss: 2.401704946, Training Accuracy: 37.040\n",
            "Worker 5, [04/04]: Training Loss: 2.307995928, Training Accuracy: 38.432\n",
            "Time taken for training worker 5: 0:00:11.668596\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.633203871, Training Accuracy: 32.704\n",
            "Worker 6, [02/04]: Training Loss: 2.502915550, Training Accuracy: 34.832\n",
            "Worker 6, [03/04]: Training Loss: 2.408169295, Training Accuracy: 36.064\n",
            "Worker 6, [04/04]: Training Loss: 2.284306458, Training Accuracy: 38.496\n",
            "Time taken for training worker 6: 0:00:11.767664\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.618400238, Training Accuracy: 32.720\n",
            "Worker 7, [02/04]: Training Loss: 2.526860899, Training Accuracy: 34.016\n",
            "Worker 7, [03/04]: Training Loss: 2.422826229, Training Accuracy: 36.112\n",
            "Worker 7, [04/04]: Training Loss: 2.349511682, Training Accuracy: 37.552\n",
            "Time taken for training worker 7: 0:00:11.955194\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.629802529, Training Accuracy: 32.720\n",
            "Worker 8, [02/04]: Training Loss: 2.502068715, Training Accuracy: 34.688\n",
            "Worker 8, [03/04]: Training Loss: 2.400836796, Training Accuracy: 37.104\n",
            "Worker 8, [04/04]: Training Loss: 2.327846386, Training Accuracy: 37.984\n",
            "Time taken for training worker 8: 0:00:12.547858\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004179\n",
            "Global Update 09: Test Loss: 2.339866449, Test Accuracy: 39.330\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.466901273, Training Accuracy: 35.968\n",
            "Worker 1, [02/04]: Training Loss: 2.343691389, Training Accuracy: 38.240\n",
            "Worker 1, [03/04]: Training Loss: 2.272014305, Training Accuracy: 39.232\n",
            "Worker 1, [04/04]: Training Loss: 2.181967103, Training Accuracy: 41.360\n",
            "Time taken for training worker 1: 0:00:11.859422\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.491136524, Training Accuracy: 35.296\n",
            "Worker 2, [02/04]: Training Loss: 2.369500661, Training Accuracy: 37.408\n",
            "Worker 2, [03/04]: Training Loss: 2.259387454, Training Accuracy: 39.904\n",
            "Worker 2, [04/04]: Training Loss: 2.157148403, Training Accuracy: 42.240\n",
            "Time taken for training worker 2: 0:00:11.684522\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.485202629, Training Accuracy: 35.088\n",
            "Worker 3, [02/04]: Training Loss: 2.352616995, Training Accuracy: 38.336\n",
            "Worker 3, [03/04]: Training Loss: 2.273827738, Training Accuracy: 39.472\n",
            "Worker 3, [04/04]: Training Loss: 2.191612849, Training Accuracy: 41.328\n",
            "Time taken for training worker 3: 0:00:12.376638\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.497787592, Training Accuracy: 34.960\n",
            "Worker 4, [02/04]: Training Loss: 2.362399304, Training Accuracy: 37.536\n",
            "Worker 4, [03/04]: Training Loss: 2.277312484, Training Accuracy: 39.072\n",
            "Worker 4, [04/04]: Training Loss: 2.172909216, Training Accuracy: 41.488\n",
            "Time taken for training worker 4: 0:00:12.078605\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.518642488, Training Accuracy: 34.624\n",
            "Worker 5, [02/04]: Training Loss: 2.357877318, Training Accuracy: 37.552\n",
            "Worker 5, [03/04]: Training Loss: 2.245719051, Training Accuracy: 40.000\n",
            "Worker 5, [04/04]: Training Loss: 2.178521507, Training Accuracy: 40.576\n",
            "Time taken for training worker 5: 0:00:12.523113\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.495587274, Training Accuracy: 34.720\n",
            "Worker 6, [02/04]: Training Loss: 2.368643020, Training Accuracy: 37.984\n",
            "Worker 6, [03/04]: Training Loss: 2.251477452, Training Accuracy: 39.808\n",
            "Worker 6, [04/04]: Training Loss: 2.163484517, Training Accuracy: 42.016\n",
            "Time taken for training worker 6: 0:00:12.975973\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.517832569, Training Accuracy: 34.256\n",
            "Worker 7, [02/04]: Training Loss: 2.378627283, Training Accuracy: 37.648\n",
            "Worker 7, [03/04]: Training Loss: 2.301933456, Training Accuracy: 38.832\n",
            "Worker 7, [04/04]: Training Loss: 2.224258197, Training Accuracy: 39.664\n",
            "Time taken for training worker 7: 0:00:12.337295\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.531709554, Training Accuracy: 34.576\n",
            "Worker 8, [02/04]: Training Loss: 2.371692452, Training Accuracy: 37.616\n",
            "Worker 8, [03/04]: Training Loss: 2.306883159, Training Accuracy: 38.608\n",
            "Worker 8, [04/04]: Training Loss: 2.195002932, Training Accuracy: 40.064\n",
            "Time taken for training worker 8: 0:00:12.322838\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004018\n",
            "Global Update 10: Test Loss: 2.264031003, Test Accuracy: 40.620\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.387785679, Training Accuracy: 37.824\n",
            "Worker 1, [02/04]: Training Loss: 2.243888263, Training Accuracy: 40.688\n",
            "Worker 1, [03/04]: Training Loss: 2.130961450, Training Accuracy: 42.176\n",
            "Worker 1, [04/04]: Training Loss: 2.061776431, Training Accuracy: 44.288\n",
            "Time taken for training worker 1: 0:00:11.993848\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.367671440, Training Accuracy: 38.176\n",
            "Worker 2, [02/04]: Training Loss: 2.264029236, Training Accuracy: 39.056\n",
            "Worker 2, [03/04]: Training Loss: 2.163859515, Training Accuracy: 41.936\n",
            "Worker 2, [04/04]: Training Loss: 2.075405618, Training Accuracy: 43.472\n",
            "Time taken for training worker 2: 0:00:12.837468\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.395179869, Training Accuracy: 37.664\n",
            "Worker 3, [02/04]: Training Loss: 2.255835180, Training Accuracy: 40.192\n",
            "Worker 3, [03/04]: Training Loss: 2.151794120, Training Accuracy: 41.632\n",
            "Worker 3, [04/04]: Training Loss: 2.083873761, Training Accuracy: 42.928\n",
            "Time taken for training worker 3: 0:00:12.154807\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.420583754, Training Accuracy: 36.400\n",
            "Worker 4, [02/04]: Training Loss: 2.266229931, Training Accuracy: 39.664\n",
            "Worker 4, [03/04]: Training Loss: 2.151381844, Training Accuracy: 42.320\n",
            "Worker 4, [04/04]: Training Loss: 2.076467295, Training Accuracy: 43.568\n",
            "Time taken for training worker 4: 0:00:11.732726\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.410595607, Training Accuracy: 37.312\n",
            "Worker 5, [02/04]: Training Loss: 2.250408104, Training Accuracy: 40.032\n",
            "Worker 5, [03/04]: Training Loss: 2.157410283, Training Accuracy: 42.144\n",
            "Worker 5, [04/04]: Training Loss: 2.087550127, Training Accuracy: 42.864\n",
            "Time taken for training worker 5: 0:00:12.263638\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.414258667, Training Accuracy: 36.368\n",
            "Worker 6, [02/04]: Training Loss: 2.279355365, Training Accuracy: 39.776\n",
            "Worker 6, [03/04]: Training Loss: 2.164547172, Training Accuracy: 41.696\n",
            "Worker 6, [04/04]: Training Loss: 2.073383388, Training Accuracy: 44.080\n",
            "Time taken for training worker 6: 0:00:12.518536\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.413291897, Training Accuracy: 36.416\n",
            "Worker 7, [02/04]: Training Loss: 2.274053125, Training Accuracy: 39.616\n",
            "Worker 7, [03/04]: Training Loss: 2.181104519, Training Accuracy: 41.888\n",
            "Worker 7, [04/04]: Training Loss: 2.069685395, Training Accuracy: 43.120\n",
            "Time taken for training worker 7: 0:00:12.796098\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.457197007, Training Accuracy: 36.368\n",
            "Worker 8, [02/04]: Training Loss: 2.287897103, Training Accuracy: 39.472\n",
            "Worker 8, [03/04]: Training Loss: 2.199565890, Training Accuracy: 41.136\n",
            "Worker 8, [04/04]: Training Loss: 2.120712870, Training Accuracy: 42.912\n",
            "Time taken for training worker 8: 0:00:12.771720\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003930\n",
            "Global Update 11: Test Loss: 2.213618519, Test Accuracy: 42.300\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.272616998, Training Accuracy: 40.080\n",
            "Worker 1, [02/04]: Training Loss: 2.138924285, Training Accuracy: 42.992\n",
            "Worker 1, [03/04]: Training Loss: 2.032875746, Training Accuracy: 44.496\n",
            "Worker 1, [04/04]: Training Loss: 1.966556213, Training Accuracy: 45.936\n",
            "Time taken for training worker 1: 0:00:12.142459\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.296414288, Training Accuracy: 39.648\n",
            "Worker 2, [02/04]: Training Loss: 2.144746068, Training Accuracy: 42.640\n",
            "Worker 2, [03/04]: Training Loss: 2.060408971, Training Accuracy: 44.304\n",
            "Worker 2, [04/04]: Training Loss: 1.992588334, Training Accuracy: 45.104\n",
            "Time taken for training worker 2: 0:00:12.356300\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.291788304, Training Accuracy: 39.664\n",
            "Worker 3, [02/04]: Training Loss: 2.159200412, Training Accuracy: 41.872\n",
            "Worker 3, [03/04]: Training Loss: 2.052676420, Training Accuracy: 44.048\n",
            "Worker 3, [04/04]: Training Loss: 1.964902317, Training Accuracy: 46.736\n",
            "Time taken for training worker 3: 0:00:12.281621\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.328514711, Training Accuracy: 38.160\n",
            "Worker 4, [02/04]: Training Loss: 2.177455257, Training Accuracy: 41.904\n",
            "Worker 4, [03/04]: Training Loss: 2.047469285, Training Accuracy: 43.936\n",
            "Worker 4, [04/04]: Training Loss: 1.962689947, Training Accuracy: 45.824\n",
            "Time taken for training worker 4: 0:00:11.761203\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.297877075, Training Accuracy: 39.264\n",
            "Worker 5, [02/04]: Training Loss: 2.160119491, Training Accuracy: 41.248\n",
            "Worker 5, [03/04]: Training Loss: 2.062307441, Training Accuracy: 43.856\n",
            "Worker 5, [04/04]: Training Loss: 1.993058544, Training Accuracy: 45.520\n",
            "Time taken for training worker 5: 0:00:12.131377\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.328112247, Training Accuracy: 38.720\n",
            "Worker 6, [02/04]: Training Loss: 2.199185336, Training Accuracy: 41.360\n",
            "Worker 6, [03/04]: Training Loss: 2.080340224, Training Accuracy: 43.264\n",
            "Worker 6, [04/04]: Training Loss: 1.981941870, Training Accuracy: 46.288\n",
            "Time taken for training worker 6: 0:00:11.947797\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.327039843, Training Accuracy: 37.920\n",
            "Worker 7, [02/04]: Training Loss: 2.197254779, Training Accuracy: 40.976\n",
            "Worker 7, [03/04]: Training Loss: 2.069492711, Training Accuracy: 44.080\n",
            "Worker 7, [04/04]: Training Loss: 2.002497173, Training Accuracy: 44.944\n",
            "Time taken for training worker 7: 0:00:12.714927\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.350984106, Training Accuracy: 38.992\n",
            "Worker 8, [02/04]: Training Loss: 2.171776196, Training Accuracy: 41.792\n",
            "Worker 8, [03/04]: Training Loss: 2.059762941, Training Accuracy: 44.128\n",
            "Worker 8, [04/04]: Training Loss: 1.998452637, Training Accuracy: 45.344\n",
            "Time taken for training worker 8: 0:00:12.132416\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003987\n",
            "Global Update 12: Test Loss: 2.178575394, Test Accuracy: 43.130\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.206266159, Training Accuracy: 41.152\n",
            "Worker 1, [02/04]: Training Loss: 2.080642912, Training Accuracy: 43.392\n",
            "Worker 1, [03/04]: Training Loss: 1.967609033, Training Accuracy: 46.288\n",
            "Worker 1, [04/04]: Training Loss: 1.870773401, Training Accuracy: 48.400\n",
            "Time taken for training worker 1: 0:00:12.293263\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.228919685, Training Accuracy: 40.976\n",
            "Worker 2, [02/04]: Training Loss: 2.105373816, Training Accuracy: 42.896\n",
            "Worker 2, [03/04]: Training Loss: 1.954117968, Training Accuracy: 46.112\n",
            "Worker 2, [04/04]: Training Loss: 1.855949272, Training Accuracy: 48.592\n",
            "Time taken for training worker 2: 0:00:12.443831\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.219899398, Training Accuracy: 41.056\n",
            "Worker 3, [02/04]: Training Loss: 2.050841907, Training Accuracy: 44.544\n",
            "Worker 3, [03/04]: Training Loss: 1.966993100, Training Accuracy: 45.712\n",
            "Worker 3, [04/04]: Training Loss: 1.887069399, Training Accuracy: 47.872\n",
            "Time taken for training worker 3: 0:00:13.008340\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.232490564, Training Accuracy: 40.848\n",
            "Worker 4, [02/04]: Training Loss: 2.058827866, Training Accuracy: 44.464\n",
            "Worker 4, [03/04]: Training Loss: 1.950930210, Training Accuracy: 47.056\n",
            "Worker 4, [04/04]: Training Loss: 1.881672803, Training Accuracy: 47.280\n",
            "Time taken for training worker 4: 0:00:12.682558\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.233492197, Training Accuracy: 40.256\n",
            "Worker 5, [02/04]: Training Loss: 2.090322763, Training Accuracy: 43.488\n",
            "Worker 5, [03/04]: Training Loss: 1.951620221, Training Accuracy: 47.008\n",
            "Worker 5, [04/04]: Training Loss: 1.859448496, Training Accuracy: 48.576\n",
            "Time taken for training worker 5: 0:00:12.272810\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.267444435, Training Accuracy: 40.016\n",
            "Worker 6, [02/04]: Training Loss: 2.089091021, Training Accuracy: 44.352\n",
            "Worker 6, [03/04]: Training Loss: 1.980854891, Training Accuracy: 45.920\n",
            "Worker 6, [04/04]: Training Loss: 1.880731556, Training Accuracy: 48.512\n",
            "Time taken for training worker 6: 0:00:11.904673\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.228505830, Training Accuracy: 40.288\n",
            "Worker 7, [02/04]: Training Loss: 2.057975740, Training Accuracy: 44.336\n",
            "Worker 7, [03/04]: Training Loss: 1.994526552, Training Accuracy: 45.456\n",
            "Worker 7, [04/04]: Training Loss: 1.901976840, Training Accuracy: 47.984\n",
            "Time taken for training worker 7: 0:00:12.196330\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.253741626, Training Accuracy: 40.816\n",
            "Worker 8, [02/04]: Training Loss: 2.091917692, Training Accuracy: 43.952\n",
            "Worker 8, [03/04]: Training Loss: 1.975914393, Training Accuracy: 46.080\n",
            "Worker 8, [04/04]: Training Loss: 1.897301339, Training Accuracy: 47.296\n",
            "Time taken for training worker 8: 0:00:11.788098\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004074\n",
            "Global Update 13: Test Loss: 2.138924711, Test Accuracy: 44.520\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.134485397, Training Accuracy: 42.896\n",
            "Worker 1, [02/04]: Training Loss: 1.987070694, Training Accuracy: 46.016\n",
            "Worker 1, [03/04]: Training Loss: 1.855792687, Training Accuracy: 48.656\n",
            "Worker 1, [04/04]: Training Loss: 1.776909154, Training Accuracy: 50.048\n",
            "Time taken for training worker 1: 0:00:12.908858\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.157202955, Training Accuracy: 42.432\n",
            "Worker 2, [02/04]: Training Loss: 1.985417162, Training Accuracy: 45.520\n",
            "Worker 2, [03/04]: Training Loss: 1.849957560, Training Accuracy: 48.576\n",
            "Worker 2, [04/04]: Training Loss: 1.802270411, Training Accuracy: 49.808\n",
            "Time taken for training worker 2: 0:00:12.070688\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.156703201, Training Accuracy: 42.512\n",
            "Worker 3, [02/04]: Training Loss: 1.998401305, Training Accuracy: 45.216\n",
            "Worker 3, [03/04]: Training Loss: 1.912809839, Training Accuracy: 48.240\n",
            "Worker 3, [04/04]: Training Loss: 1.775418637, Training Accuracy: 50.128\n",
            "Time taken for training worker 3: 0:00:11.659149\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.181661283, Training Accuracy: 42.304\n",
            "Worker 4, [02/04]: Training Loss: 1.984934098, Training Accuracy: 45.792\n",
            "Worker 4, [03/04]: Training Loss: 1.873425784, Training Accuracy: 48.528\n",
            "Worker 4, [04/04]: Training Loss: 1.778296332, Training Accuracy: 50.000\n",
            "Time taken for training worker 4: 0:00:12.831702\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.160245033, Training Accuracy: 42.240\n",
            "Worker 5, [02/04]: Training Loss: 1.978412004, Training Accuracy: 45.616\n",
            "Worker 5, [03/04]: Training Loss: 1.885915032, Training Accuracy: 48.176\n",
            "Worker 5, [04/04]: Training Loss: 1.782181407, Training Accuracy: 50.224\n",
            "Time taken for training worker 5: 0:00:12.422809\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.176832536, Training Accuracy: 42.304\n",
            "Worker 6, [02/04]: Training Loss: 1.994424176, Training Accuracy: 45.680\n",
            "Worker 6, [03/04]: Training Loss: 1.903843252, Training Accuracy: 48.048\n",
            "Worker 6, [04/04]: Training Loss: 1.854472570, Training Accuracy: 48.768\n",
            "Time taken for training worker 6: 0:00:12.245766\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.168204629, Training Accuracy: 42.000\n",
            "Worker 7, [02/04]: Training Loss: 2.009297224, Training Accuracy: 45.600\n",
            "Worker 7, [03/04]: Training Loss: 1.878911608, Training Accuracy: 47.536\n",
            "Worker 7, [04/04]: Training Loss: 1.807465885, Training Accuracy: 49.296\n",
            "Time taken for training worker 7: 0:00:12.088203\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.152593417, Training Accuracy: 43.040\n",
            "Worker 8, [02/04]: Training Loss: 2.005928333, Training Accuracy: 45.888\n",
            "Worker 8, [03/04]: Training Loss: 1.893922744, Training Accuracy: 48.016\n",
            "Worker 8, [04/04]: Training Loss: 1.823423979, Training Accuracy: 49.920\n",
            "Time taken for training worker 8: 0:00:12.019192\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003930\n",
            "Global Update 14: Test Loss: 2.098497319, Test Accuracy: 45.370\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.064945986, Training Accuracy: 44.016\n",
            "Worker 1, [02/04]: Training Loss: 1.916872493, Training Accuracy: 47.488\n",
            "Worker 1, [03/04]: Training Loss: 1.774598416, Training Accuracy: 50.352\n",
            "Worker 1, [04/04]: Training Loss: 1.701153658, Training Accuracy: 51.968\n",
            "Time taken for training worker 1: 0:00:11.932793\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.111203557, Training Accuracy: 43.760\n",
            "Worker 2, [02/04]: Training Loss: 1.902614192, Training Accuracy: 47.696\n",
            "Worker 2, [03/04]: Training Loss: 1.786091067, Training Accuracy: 49.520\n",
            "Worker 2, [04/04]: Training Loss: 1.710500717, Training Accuracy: 52.160\n",
            "Time taken for training worker 2: 0:00:12.098902\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.076402995, Training Accuracy: 44.288\n",
            "Worker 3, [02/04]: Training Loss: 1.906805277, Training Accuracy: 47.904\n",
            "Worker 3, [03/04]: Training Loss: 1.792809203, Training Accuracy: 50.448\n",
            "Worker 3, [04/04]: Training Loss: 1.681771095, Training Accuracy: 54.144\n",
            "Time taken for training worker 3: 0:00:11.595841\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.081395760, Training Accuracy: 44.208\n",
            "Worker 4, [02/04]: Training Loss: 1.903138905, Training Accuracy: 48.336\n",
            "Worker 4, [03/04]: Training Loss: 1.783733913, Training Accuracy: 50.720\n",
            "Worker 4, [04/04]: Training Loss: 1.702304461, Training Accuracy: 52.016\n",
            "Time taken for training worker 4: 0:00:12.454580\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.091514791, Training Accuracy: 44.336\n",
            "Worker 5, [02/04]: Training Loss: 1.917315972, Training Accuracy: 47.600\n",
            "Worker 5, [03/04]: Training Loss: 1.816026688, Training Accuracy: 49.632\n",
            "Worker 5, [04/04]: Training Loss: 1.707173687, Training Accuracy: 51.920\n",
            "Time taken for training worker 5: 0:00:12.325212\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.102137395, Training Accuracy: 43.456\n",
            "Worker 6, [02/04]: Training Loss: 1.924796454, Training Accuracy: 47.808\n",
            "Worker 6, [03/04]: Training Loss: 1.801170866, Training Accuracy: 50.160\n",
            "Worker 6, [04/04]: Training Loss: 1.729924826, Training Accuracy: 52.064\n",
            "Time taken for training worker 6: 0:00:12.348972\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.100046282, Training Accuracy: 43.904\n",
            "Worker 7, [02/04]: Training Loss: 1.948976840, Training Accuracy: 46.592\n",
            "Worker 7, [03/04]: Training Loss: 1.818927096, Training Accuracy: 49.568\n",
            "Worker 7, [04/04]: Training Loss: 1.745875160, Training Accuracy: 51.728\n",
            "Time taken for training worker 7: 0:00:11.790475\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.104687449, Training Accuracy: 44.112\n",
            "Worker 8, [02/04]: Training Loss: 1.924177205, Training Accuracy: 47.248\n",
            "Worker 8, [03/04]: Training Loss: 1.819587603, Training Accuracy: 49.344\n",
            "Worker 8, [04/04]: Training Loss: 1.714152101, Training Accuracy: 52.288\n",
            "Time taken for training worker 8: 0:00:12.688822\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004357\n",
            "Global Update 15: Test Loss: 2.072835115, Test Accuracy: 45.970\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.987012970, Training Accuracy: 45.584\n",
            "Worker 1, [02/04]: Training Loss: 1.795439758, Training Accuracy: 50.464\n",
            "Worker 1, [03/04]: Training Loss: 1.714145168, Training Accuracy: 52.976\n",
            "Worker 1, [04/04]: Training Loss: 1.638421490, Training Accuracy: 54.032\n",
            "Time taken for training worker 1: 0:00:12.276432\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.004726103, Training Accuracy: 45.616\n",
            "Worker 2, [02/04]: Training Loss: 1.843952612, Training Accuracy: 49.424\n",
            "Worker 2, [03/04]: Training Loss: 1.727010890, Training Accuracy: 51.600\n",
            "Worker 2, [04/04]: Training Loss: 1.629356465, Training Accuracy: 54.000\n",
            "Time taken for training worker 2: 0:00:11.814066\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.007695805, Training Accuracy: 46.128\n",
            "Worker 3, [02/04]: Training Loss: 1.832728197, Training Accuracy: 49.744\n",
            "Worker 3, [03/04]: Training Loss: 1.710517032, Training Accuracy: 52.576\n",
            "Worker 3, [04/04]: Training Loss: 1.593319799, Training Accuracy: 54.656\n",
            "Time taken for training worker 3: 0:00:12.940688\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.012810210, Training Accuracy: 44.880\n",
            "Worker 4, [02/04]: Training Loss: 1.848733023, Training Accuracy: 49.008\n",
            "Worker 4, [03/04]: Training Loss: 1.711758800, Training Accuracy: 51.344\n",
            "Worker 4, [04/04]: Training Loss: 1.634701585, Training Accuracy: 54.784\n",
            "Time taken for training worker 4: 0:00:12.489527\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.037224249, Training Accuracy: 45.408\n",
            "Worker 5, [02/04]: Training Loss: 1.856336946, Training Accuracy: 49.552\n",
            "Worker 5, [03/04]: Training Loss: 1.699618335, Training Accuracy: 51.296\n",
            "Worker 5, [04/04]: Training Loss: 1.640315623, Training Accuracy: 52.752\n",
            "Time taken for training worker 5: 0:00:12.370090\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.035676249, Training Accuracy: 45.056\n",
            "Worker 6, [02/04]: Training Loss: 1.872034309, Training Accuracy: 49.232\n",
            "Worker 6, [03/04]: Training Loss: 1.718427081, Training Accuracy: 52.112\n",
            "Worker 6, [04/04]: Training Loss: 1.635108414, Training Accuracy: 54.128\n",
            "Time taken for training worker 6: 0:00:12.231544\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.019055610, Training Accuracy: 45.440\n",
            "Worker 7, [02/04]: Training Loss: 1.823121158, Training Accuracy: 49.312\n",
            "Worker 7, [03/04]: Training Loss: 1.754841021, Training Accuracy: 50.896\n",
            "Worker 7, [04/04]: Training Loss: 1.647279073, Training Accuracy: 53.632\n",
            "Time taken for training worker 7: 0:00:11.830862\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.044875022, Training Accuracy: 45.296\n",
            "Worker 8, [02/04]: Training Loss: 1.873245290, Training Accuracy: 48.768\n",
            "Worker 8, [03/04]: Training Loss: 1.734541392, Training Accuracy: 52.032\n",
            "Worker 8, [04/04]: Training Loss: 1.654040186, Training Accuracy: 53.440\n",
            "Time taken for training worker 8: 0:00:12.358308\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004383\n",
            "Global Update 16: Test Loss: 2.061199629, Test Accuracy: 46.970\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.929804331, Training Accuracy: 46.320\n",
            "Worker 1, [02/04]: Training Loss: 1.743666340, Training Accuracy: 51.952\n",
            "Worker 1, [03/04]: Training Loss: 1.617721920, Training Accuracy: 54.688\n",
            "Worker 1, [04/04]: Training Loss: 1.542141899, Training Accuracy: 56.272\n",
            "Time taken for training worker 1: 0:00:12.383554\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.966906231, Training Accuracy: 46.496\n",
            "Worker 2, [02/04]: Training Loss: 1.769081369, Training Accuracy: 51.488\n",
            "Worker 2, [03/04]: Training Loss: 1.679108845, Training Accuracy: 52.256\n",
            "Worker 2, [04/04]: Training Loss: 1.572173789, Training Accuracy: 55.680\n",
            "Time taken for training worker 2: 0:00:12.219522\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.981281897, Training Accuracy: 46.528\n",
            "Worker 3, [02/04]: Training Loss: 1.764857715, Training Accuracy: 51.264\n",
            "Worker 3, [03/04]: Training Loss: 1.650921429, Training Accuracy: 53.872\n",
            "Worker 3, [04/04]: Training Loss: 1.509010322, Training Accuracy: 57.472\n",
            "Time taken for training worker 3: 0:00:12.215037\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.954588116, Training Accuracy: 46.800\n",
            "Worker 4, [02/04]: Training Loss: 1.783139721, Training Accuracy: 50.576\n",
            "Worker 4, [03/04]: Training Loss: 1.648118585, Training Accuracy: 54.592\n",
            "Worker 4, [04/04]: Training Loss: 1.542081197, Training Accuracy: 56.208\n",
            "Time taken for training worker 4: 0:00:12.783920\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.970525293, Training Accuracy: 46.000\n",
            "Worker 5, [02/04]: Training Loss: 1.773075914, Training Accuracy: 51.072\n",
            "Worker 5, [03/04]: Training Loss: 1.651082185, Training Accuracy: 53.488\n",
            "Worker 5, [04/04]: Training Loss: 1.544828505, Training Accuracy: 56.256\n",
            "Time taken for training worker 5: 0:00:12.328599\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.987980328, Training Accuracy: 45.920\n",
            "Worker 6, [02/04]: Training Loss: 1.810572499, Training Accuracy: 49.968\n",
            "Worker 6, [03/04]: Training Loss: 1.660514216, Training Accuracy: 53.968\n",
            "Worker 6, [04/04]: Training Loss: 1.588279345, Training Accuracy: 54.832\n",
            "Time taken for training worker 6: 0:00:12.026206\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.968900836, Training Accuracy: 45.984\n",
            "Worker 7, [02/04]: Training Loss: 1.795071918, Training Accuracy: 50.000\n",
            "Worker 7, [03/04]: Training Loss: 1.658658875, Training Accuracy: 52.608\n",
            "Worker 7, [04/04]: Training Loss: 1.575842711, Training Accuracy: 55.632\n",
            "Time taken for training worker 7: 0:00:11.713497\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.973184971, Training Accuracy: 46.976\n",
            "Worker 8, [02/04]: Training Loss: 1.794908644, Training Accuracy: 49.792\n",
            "Worker 8, [03/04]: Training Loss: 1.664152683, Training Accuracy: 53.680\n",
            "Worker 8, [04/04]: Training Loss: 1.552713825, Training Accuracy: 56.304\n",
            "Time taken for training worker 8: 0:00:12.340544\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.005874\n",
            "Global Update 17: Test Loss: 2.048843695, Test Accuracy: 47.130\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.852827368, Training Accuracy: 49.552\n",
            "Worker 1, [02/04]: Training Loss: 1.696670217, Training Accuracy: 51.824\n",
            "Worker 1, [03/04]: Training Loss: 1.550192385, Training Accuracy: 56.384\n",
            "Worker 1, [04/04]: Training Loss: 1.470139786, Training Accuracy: 57.920\n",
            "Time taken for training worker 1: 0:00:12.579845\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.877014746, Training Accuracy: 48.672\n",
            "Worker 2, [02/04]: Training Loss: 1.725180339, Training Accuracy: 52.336\n",
            "Worker 2, [03/04]: Training Loss: 1.579514949, Training Accuracy: 55.232\n",
            "Worker 2, [04/04]: Training Loss: 1.469714961, Training Accuracy: 58.688\n",
            "Time taken for training worker 2: 0:00:12.677649\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.869393520, Training Accuracy: 49.856\n",
            "Worker 3, [02/04]: Training Loss: 1.667565250, Training Accuracy: 53.872\n",
            "Worker 3, [03/04]: Training Loss: 1.587078884, Training Accuracy: 56.496\n",
            "Worker 3, [04/04]: Training Loss: 1.458763064, Training Accuracy: 57.920\n",
            "Time taken for training worker 3: 0:00:12.487964\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.894151533, Training Accuracy: 48.128\n",
            "Worker 4, [02/04]: Training Loss: 1.705697440, Training Accuracy: 52.544\n",
            "Worker 4, [03/04]: Training Loss: 1.549704130, Training Accuracy: 56.768\n",
            "Worker 4, [04/04]: Training Loss: 1.504042157, Training Accuracy: 56.736\n",
            "Time taken for training worker 4: 0:00:12.223344\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.907066273, Training Accuracy: 47.792\n",
            "Worker 5, [02/04]: Training Loss: 1.712229369, Training Accuracy: 52.496\n",
            "Worker 5, [03/04]: Training Loss: 1.578933732, Training Accuracy: 55.424\n",
            "Worker 5, [04/04]: Training Loss: 1.484866448, Training Accuracy: 56.976\n",
            "Time taken for training worker 5: 0:00:12.131666\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.913221148, Training Accuracy: 47.456\n",
            "Worker 6, [02/04]: Training Loss: 1.742570641, Training Accuracy: 51.024\n",
            "Worker 6, [03/04]: Training Loss: 1.577551592, Training Accuracy: 56.320\n",
            "Worker 6, [04/04]: Training Loss: 1.505646776, Training Accuracy: 57.280\n",
            "Time taken for training worker 6: 0:00:12.630500\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.906233129, Training Accuracy: 48.128\n",
            "Worker 7, [02/04]: Training Loss: 1.722090980, Training Accuracy: 51.616\n",
            "Worker 7, [03/04]: Training Loss: 1.605749343, Training Accuracy: 54.368\n",
            "Worker 7, [04/04]: Training Loss: 1.501777537, Training Accuracy: 57.632\n",
            "Time taken for training worker 7: 0:00:13.077855\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.897910439, Training Accuracy: 48.352\n",
            "Worker 8, [02/04]: Training Loss: 1.736868802, Training Accuracy: 51.296\n",
            "Worker 8, [03/04]: Training Loss: 1.593075136, Training Accuracy: 55.344\n",
            "Worker 8, [04/04]: Training Loss: 1.493552578, Training Accuracy: 57.488\n",
            "Time taken for training worker 8: 0:00:12.417396\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004373\n",
            "Global Update 18: Test Loss: 2.031107899, Test Accuracy: 47.580\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.816014918, Training Accuracy: 49.872\n",
            "Worker 1, [02/04]: Training Loss: 1.612402820, Training Accuracy: 54.592\n",
            "Worker 1, [03/04]: Training Loss: 1.470225045, Training Accuracy: 57.968\n",
            "Worker 1, [04/04]: Training Loss: 1.397734874, Training Accuracy: 60.448\n",
            "Time taken for training worker 1: 0:00:12.421217\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.841589498, Training Accuracy: 50.160\n",
            "Worker 2, [02/04]: Training Loss: 1.654800978, Training Accuracy: 54.272\n",
            "Worker 2, [03/04]: Training Loss: 1.493994397, Training Accuracy: 57.440\n",
            "Worker 2, [04/04]: Training Loss: 1.412637492, Training Accuracy: 59.216\n",
            "Time taken for training worker 2: 0:00:12.246832\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.790811860, Training Accuracy: 50.832\n",
            "Worker 3, [02/04]: Training Loss: 1.615208608, Training Accuracy: 54.736\n",
            "Worker 3, [03/04]: Training Loss: 1.519214594, Training Accuracy: 57.328\n",
            "Worker 3, [04/04]: Training Loss: 1.395329399, Training Accuracy: 60.576\n",
            "Time taken for training worker 3: 0:00:12.244221\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.840491612, Training Accuracy: 49.840\n",
            "Worker 4, [02/04]: Training Loss: 1.657228834, Training Accuracy: 54.112\n",
            "Worker 4, [03/04]: Training Loss: 1.497080715, Training Accuracy: 57.184\n",
            "Worker 4, [04/04]: Training Loss: 1.443221201, Training Accuracy: 58.544\n",
            "Time taken for training worker 4: 0:00:12.102529\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.843324743, Training Accuracy: 48.432\n",
            "Worker 5, [02/04]: Training Loss: 1.608059979, Training Accuracy: 54.848\n",
            "Worker 5, [03/04]: Training Loss: 1.512444673, Training Accuracy: 56.704\n",
            "Worker 5, [04/04]: Training Loss: 1.429901277, Training Accuracy: 58.304\n",
            "Time taken for training worker 5: 0:00:12.193074\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.852120516, Training Accuracy: 49.472\n",
            "Worker 6, [02/04]: Training Loss: 1.663638115, Training Accuracy: 53.792\n",
            "Worker 6, [03/04]: Training Loss: 1.536955686, Training Accuracy: 56.448\n",
            "Worker 6, [04/04]: Training Loss: 1.471794599, Training Accuracy: 57.856\n",
            "Time taken for training worker 6: 0:00:13.226435\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.829151601, Training Accuracy: 49.568\n",
            "Worker 7, [02/04]: Training Loss: 1.623411298, Training Accuracy: 54.144\n",
            "Worker 7, [03/04]: Training Loss: 1.526086192, Training Accuracy: 56.768\n",
            "Worker 7, [04/04]: Training Loss: 1.439069838, Training Accuracy: 59.456\n",
            "Time taken for training worker 7: 0:00:11.862303\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.841846243, Training Accuracy: 49.392\n",
            "Worker 8, [02/04]: Training Loss: 1.662213963, Training Accuracy: 53.216\n",
            "Worker 8, [03/04]: Training Loss: 1.535614461, Training Accuracy: 57.008\n",
            "Worker 8, [04/04]: Training Loss: 1.460693881, Training Accuracy: 58.448\n",
            "Time taken for training worker 8: 0:00:12.994042\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004531\n",
            "Global Update 19: Test Loss: 2.015986039, Test Accuracy: 47.890\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.762997981, Training Accuracy: 50.560\n",
            "Worker 1, [02/04]: Training Loss: 1.568371443, Training Accuracy: 56.144\n",
            "Worker 1, [03/04]: Training Loss: 1.439271836, Training Accuracy: 58.576\n",
            "Worker 1, [04/04]: Training Loss: 1.344298736, Training Accuracy: 61.680\n",
            "Time taken for training worker 1: 0:00:12.149710\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.756334878, Training Accuracy: 51.184\n",
            "Worker 2, [02/04]: Training Loss: 1.583674275, Training Accuracy: 55.264\n",
            "Worker 2, [03/04]: Training Loss: 1.450190515, Training Accuracy: 58.304\n",
            "Worker 2, [04/04]: Training Loss: 1.367756662, Training Accuracy: 60.608\n",
            "Time taken for training worker 2: 0:00:12.205514\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.781447870, Training Accuracy: 50.576\n",
            "Worker 3, [02/04]: Training Loss: 1.552753247, Training Accuracy: 56.448\n",
            "Worker 3, [03/04]: Training Loss: 1.430396682, Training Accuracy: 59.504\n",
            "Worker 3, [04/04]: Training Loss: 1.360822174, Training Accuracy: 61.664\n",
            "Time taken for training worker 3: 0:00:12.332098\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.775644260, Training Accuracy: 51.184\n",
            "Worker 4, [02/04]: Training Loss: 1.562482283, Training Accuracy: 55.936\n",
            "Worker 4, [03/04]: Training Loss: 1.443562726, Training Accuracy: 59.008\n",
            "Worker 4, [04/04]: Training Loss: 1.356353025, Training Accuracy: 60.736\n",
            "Time taken for training worker 4: 0:00:12.449459\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.761518553, Training Accuracy: 50.784\n",
            "Worker 5, [02/04]: Training Loss: 1.585477777, Training Accuracy: 54.960\n",
            "Worker 5, [03/04]: Training Loss: 1.446875588, Training Accuracy: 58.944\n",
            "Worker 5, [04/04]: Training Loss: 1.385037050, Training Accuracy: 60.304\n",
            "Time taken for training worker 5: 0:00:12.406530\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.787782804, Training Accuracy: 50.064\n",
            "Worker 6, [02/04]: Training Loss: 1.633574576, Training Accuracy: 54.384\n",
            "Worker 6, [03/04]: Training Loss: 1.478135614, Training Accuracy: 58.720\n",
            "Worker 6, [04/04]: Training Loss: 1.367822935, Training Accuracy: 61.344\n",
            "Time taken for training worker 6: 0:00:12.160605\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.782153728, Training Accuracy: 50.384\n",
            "Worker 7, [02/04]: Training Loss: 1.587498605, Training Accuracy: 55.456\n",
            "Worker 7, [03/04]: Training Loss: 1.450934880, Training Accuracy: 58.320\n",
            "Worker 7, [04/04]: Training Loss: 1.364961135, Training Accuracy: 60.656\n",
            "Time taken for training worker 7: 0:00:12.335285\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.820147310, Training Accuracy: 49.968\n",
            "Worker 8, [02/04]: Training Loss: 1.594058155, Training Accuracy: 55.456\n",
            "Worker 8, [03/04]: Training Loss: 1.477823979, Training Accuracy: 58.400\n",
            "Worker 8, [04/04]: Training Loss: 1.357648837, Training Accuracy: 60.928\n",
            "Time taken for training worker 8: 0:00:11.864781\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004693\n",
            "Global Update 20: Test Loss: 2.010810888, Test Accuracy: 48.590\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.703034840, Training Accuracy: 52.352\n",
            "Worker 1, [02/04]: Training Loss: 1.506414926, Training Accuracy: 56.752\n",
            "Worker 1, [03/04]: Training Loss: 1.415633467, Training Accuracy: 59.824\n",
            "Worker 1, [04/04]: Training Loss: 1.292522497, Training Accuracy: 63.072\n",
            "Time taken for training worker 1: 0:00:11.992124\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.715061082, Training Accuracy: 52.128\n",
            "Worker 2, [02/04]: Training Loss: 1.517034567, Training Accuracy: 56.400\n",
            "Worker 2, [03/04]: Training Loss: 1.403892102, Training Accuracy: 59.952\n",
            "Worker 2, [04/04]: Training Loss: 1.304380523, Training Accuracy: 62.944\n",
            "Time taken for training worker 2: 0:00:12.448084\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.714694496, Training Accuracy: 52.912\n",
            "Worker 3, [02/04]: Training Loss: 1.496392327, Training Accuracy: 57.648\n",
            "Worker 3, [03/04]: Training Loss: 1.407840776, Training Accuracy: 60.192\n",
            "Worker 3, [04/04]: Training Loss: 1.296380559, Training Accuracy: 63.104\n",
            "Time taken for training worker 3: 0:00:12.520481\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.740127020, Training Accuracy: 51.120\n",
            "Worker 4, [02/04]: Training Loss: 1.527348919, Training Accuracy: 56.848\n",
            "Worker 4, [03/04]: Training Loss: 1.422124640, Training Accuracy: 59.216\n",
            "Worker 4, [04/04]: Training Loss: 1.310999466, Training Accuracy: 61.968\n",
            "Time taken for training worker 4: 0:00:12.282183\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.721780349, Training Accuracy: 52.608\n",
            "Worker 5, [02/04]: Training Loss: 1.517930615, Training Accuracy: 56.800\n",
            "Worker 5, [03/04]: Training Loss: 1.393181696, Training Accuracy: 60.336\n",
            "Worker 5, [04/04]: Training Loss: 1.321470565, Training Accuracy: 62.016\n",
            "Time taken for training worker 5: 0:00:12.397824\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.696647267, Training Accuracy: 53.056\n",
            "Worker 6, [02/04]: Training Loss: 1.548287611, Training Accuracy: 56.720\n",
            "Worker 6, [03/04]: Training Loss: 1.426766008, Training Accuracy: 59.824\n",
            "Worker 6, [04/04]: Training Loss: 1.332599985, Training Accuracy: 61.712\n",
            "Time taken for training worker 6: 0:00:11.832138\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.730087289, Training Accuracy: 51.968\n",
            "Worker 7, [02/04]: Training Loss: 1.518452242, Training Accuracy: 56.432\n",
            "Worker 7, [03/04]: Training Loss: 1.412384500, Training Accuracy: 60.112\n",
            "Worker 7, [04/04]: Training Loss: 1.309167482, Training Accuracy: 62.160\n",
            "Time taken for training worker 7: 0:00:11.918158\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.740393456, Training Accuracy: 51.680\n",
            "Worker 8, [02/04]: Training Loss: 1.525019064, Training Accuracy: 56.720\n",
            "Worker 8, [03/04]: Training Loss: 1.394584425, Training Accuracy: 60.384\n",
            "Worker 8, [04/04]: Training Loss: 1.328294823, Training Accuracy: 61.856\n",
            "Time taken for training worker 8: 0:00:12.371824\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004271\n",
            "Global Update 21: Test Loss: 2.003518179, Test Accuracy: 48.810\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.633055203, Training Accuracy: 54.192\n",
            "Worker 1, [02/04]: Training Loss: 1.442060301, Training Accuracy: 58.656\n",
            "Worker 1, [03/04]: Training Loss: 1.318080524, Training Accuracy: 62.064\n",
            "Worker 1, [04/04]: Training Loss: 1.225051036, Training Accuracy: 65.808\n",
            "Time taken for training worker 1: 0:00:11.807015\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.655251757, Training Accuracy: 53.040\n",
            "Worker 2, [02/04]: Training Loss: 1.474357702, Training Accuracy: 58.912\n",
            "Worker 2, [03/04]: Training Loss: 1.337127538, Training Accuracy: 61.776\n",
            "Worker 2, [04/04]: Training Loss: 1.229450358, Training Accuracy: 64.656\n",
            "Time taken for training worker 2: 0:00:12.401322\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.652105533, Training Accuracy: 53.984\n",
            "Worker 3, [02/04]: Training Loss: 1.432575133, Training Accuracy: 59.408\n",
            "Worker 3, [03/04]: Training Loss: 1.319494683, Training Accuracy: 62.368\n",
            "Worker 3, [04/04]: Training Loss: 1.209896356, Training Accuracy: 65.024\n",
            "Time taken for training worker 3: 0:00:12.525825\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.657869643, Training Accuracy: 53.136\n",
            "Worker 4, [02/04]: Training Loss: 1.468777181, Training Accuracy: 58.832\n",
            "Worker 4, [03/04]: Training Loss: 1.315887725, Training Accuracy: 62.224\n",
            "Worker 4, [04/04]: Training Loss: 1.245822354, Training Accuracy: 64.128\n",
            "Time taken for training worker 4: 0:00:12.330646\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.674025937, Training Accuracy: 52.752\n",
            "Worker 5, [02/04]: Training Loss: 1.449351389, Training Accuracy: 58.096\n",
            "Worker 5, [03/04]: Training Loss: 1.321768423, Training Accuracy: 61.888\n",
            "Worker 5, [04/04]: Training Loss: 1.243297151, Training Accuracy: 64.064\n",
            "Time taken for training worker 5: 0:00:12.010151\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.674139415, Training Accuracy: 53.328\n",
            "Worker 6, [02/04]: Training Loss: 1.455780927, Training Accuracy: 59.024\n",
            "Worker 6, [03/04]: Training Loss: 1.373265956, Training Accuracy: 60.752\n",
            "Worker 6, [04/04]: Training Loss: 1.261206016, Training Accuracy: 64.080\n",
            "Time taken for training worker 6: 0:00:13.154487\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.677863570, Training Accuracy: 53.408\n",
            "Worker 7, [02/04]: Training Loss: 1.473705425, Training Accuracy: 58.128\n",
            "Worker 7, [03/04]: Training Loss: 1.324409088, Training Accuracy: 61.632\n",
            "Worker 7, [04/04]: Training Loss: 1.265194550, Training Accuracy: 63.072\n",
            "Time taken for training worker 7: 0:00:12.679879\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.665348364, Training Accuracy: 53.568\n",
            "Worker 8, [02/04]: Training Loss: 1.480411880, Training Accuracy: 57.952\n",
            "Worker 8, [03/04]: Training Loss: 1.351538462, Training Accuracy: 61.264\n",
            "Worker 8, [04/04]: Training Loss: 1.253475241, Training Accuracy: 64.256\n",
            "Time taken for training worker 8: 0:00:12.383899\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004159\n",
            "Global Update 22: Test Loss: 2.009431976, Test Accuracy: 48.880\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.584848381, Training Accuracy: 55.168\n",
            "Worker 1, [02/04]: Training Loss: 1.386823714, Training Accuracy: 60.736\n",
            "Worker 1, [03/04]: Training Loss: 1.270760616, Training Accuracy: 63.280\n",
            "Worker 1, [04/04]: Training Loss: 1.174709212, Training Accuracy: 66.048\n",
            "Time taken for training worker 1: 0:00:12.209993\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.618329535, Training Accuracy: 54.576\n",
            "Worker 2, [02/04]: Training Loss: 1.397437119, Training Accuracy: 60.304\n",
            "Worker 2, [03/04]: Training Loss: 1.285668506, Training Accuracy: 63.568\n",
            "Worker 2, [04/04]: Training Loss: 1.167873032, Training Accuracy: 65.776\n",
            "Time taken for training worker 2: 0:00:12.486829\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.612016514, Training Accuracy: 55.344\n",
            "Worker 3, [02/04]: Training Loss: 1.392787705, Training Accuracy: 60.624\n",
            "Worker 3, [03/04]: Training Loss: 1.257839332, Training Accuracy: 63.984\n",
            "Worker 3, [04/04]: Training Loss: 1.161571245, Training Accuracy: 66.592\n",
            "Time taken for training worker 3: 0:00:12.584936\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.592089907, Training Accuracy: 55.680\n",
            "Worker 4, [02/04]: Training Loss: 1.376580741, Training Accuracy: 61.392\n",
            "Worker 4, [03/04]: Training Loss: 1.289982355, Training Accuracy: 62.992\n",
            "Worker 4, [04/04]: Training Loss: 1.208647771, Training Accuracy: 65.200\n",
            "Time taken for training worker 4: 0:00:11.788761\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.622719845, Training Accuracy: 54.160\n",
            "Worker 5, [02/04]: Training Loss: 1.393862691, Training Accuracy: 59.968\n",
            "Worker 5, [03/04]: Training Loss: 1.298985591, Training Accuracy: 62.112\n",
            "Worker 5, [04/04]: Training Loss: 1.181574818, Training Accuracy: 65.504\n",
            "Time taken for training worker 5: 0:00:12.042069\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.627509464, Training Accuracy: 54.272\n",
            "Worker 6, [02/04]: Training Loss: 1.416104628, Training Accuracy: 60.016\n",
            "Worker 6, [03/04]: Training Loss: 1.293045161, Training Accuracy: 63.008\n",
            "Worker 6, [04/04]: Training Loss: 1.218908043, Training Accuracy: 64.848\n",
            "Time taken for training worker 6: 0:00:11.991406\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.601279666, Training Accuracy: 55.440\n",
            "Worker 7, [02/04]: Training Loss: 1.423967611, Training Accuracy: 59.120\n",
            "Worker 7, [03/04]: Training Loss: 1.291990394, Training Accuracy: 62.944\n",
            "Worker 7, [04/04]: Training Loss: 1.209279364, Training Accuracy: 65.120\n",
            "Time taken for training worker 7: 0:00:12.074416\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.631958816, Training Accuracy: 54.832\n",
            "Worker 8, [02/04]: Training Loss: 1.419528939, Training Accuracy: 60.112\n",
            "Worker 8, [03/04]: Training Loss: 1.318756671, Training Accuracy: 61.920\n",
            "Worker 8, [04/04]: Training Loss: 1.196681732, Training Accuracy: 64.960\n",
            "Time taken for training worker 8: 0:00:12.396954\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004149\n",
            "Global Update 23: Test Loss: 2.022680763, Test Accuracy: 48.920\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.530139765, Training Accuracy: 56.432\n",
            "Worker 1, [02/04]: Training Loss: 1.347659151, Training Accuracy: 61.136\n",
            "Worker 1, [03/04]: Training Loss: 1.234855399, Training Accuracy: 63.856\n",
            "Worker 1, [04/04]: Training Loss: 1.126371484, Training Accuracy: 67.424\n",
            "Time taken for training worker 1: 0:00:12.234847\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.532003492, Training Accuracy: 57.424\n",
            "Worker 2, [02/04]: Training Loss: 1.364001287, Training Accuracy: 61.360\n",
            "Worker 2, [03/04]: Training Loss: 1.214009025, Training Accuracy: 65.376\n",
            "Worker 2, [04/04]: Training Loss: 1.141806637, Training Accuracy: 66.304\n",
            "Time taken for training worker 2: 0:00:11.778872\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.540932316, Training Accuracy: 56.448\n",
            "Worker 3, [02/04]: Training Loss: 1.352991589, Training Accuracy: 61.424\n",
            "Worker 3, [03/04]: Training Loss: 1.229319923, Training Accuracy: 64.624\n",
            "Worker 3, [04/04]: Training Loss: 1.133902407, Training Accuracy: 67.152\n",
            "Time taken for training worker 3: 0:00:12.132718\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.547640006, Training Accuracy: 55.744\n",
            "Worker 4, [02/04]: Training Loss: 1.349471399, Training Accuracy: 62.128\n",
            "Worker 4, [03/04]: Training Loss: 1.226023323, Training Accuracy: 64.320\n",
            "Worker 4, [04/04]: Training Loss: 1.156449816, Training Accuracy: 66.464\n",
            "Time taken for training worker 4: 0:00:11.692900\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.567520260, Training Accuracy: 55.472\n",
            "Worker 5, [02/04]: Training Loss: 1.349996578, Training Accuracy: 61.200\n",
            "Worker 5, [03/04]: Training Loss: 1.207432829, Training Accuracy: 65.392\n",
            "Worker 5, [04/04]: Training Loss: 1.134348307, Training Accuracy: 67.216\n",
            "Time taken for training worker 5: 0:00:11.762030\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.574040927, Training Accuracy: 55.184\n",
            "Worker 6, [02/04]: Training Loss: 1.365612584, Training Accuracy: 61.008\n",
            "Worker 6, [03/04]: Training Loss: 1.230546523, Training Accuracy: 65.104\n",
            "Worker 6, [04/04]: Training Loss: 1.129656013, Training Accuracy: 67.920\n",
            "Time taken for training worker 6: 0:00:12.138522\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.565302084, Training Accuracy: 56.624\n",
            "Worker 7, [02/04]: Training Loss: 1.364705308, Training Accuracy: 61.232\n",
            "Worker 7, [03/04]: Training Loss: 1.238301081, Training Accuracy: 64.144\n",
            "Worker 7, [04/04]: Training Loss: 1.152685044, Training Accuracy: 66.704\n",
            "Time taken for training worker 7: 0:00:11.630226\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.576339842, Training Accuracy: 56.176\n",
            "Worker 8, [02/04]: Training Loss: 1.357304410, Training Accuracy: 61.680\n",
            "Worker 8, [03/04]: Training Loss: 1.231441949, Training Accuracy: 64.704\n",
            "Worker 8, [04/04]: Training Loss: 1.124363644, Training Accuracy: 67.472\n",
            "Time taken for training worker 8: 0:00:11.811803\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004259\n",
            "Global Update 24: Test Loss: 2.023049218, Test Accuracy: 49.310\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.495392618, Training Accuracy: 57.408\n",
            "Worker 1, [02/04]: Training Loss: 1.303276539, Training Accuracy: 62.832\n",
            "Worker 1, [03/04]: Training Loss: 1.184035093, Training Accuracy: 66.048\n",
            "Worker 1, [04/04]: Training Loss: 1.097985492, Training Accuracy: 68.816\n",
            "Time taken for training worker 1: 0:00:12.211818\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.497327129, Training Accuracy: 57.216\n",
            "Worker 2, [02/04]: Training Loss: 1.307648543, Training Accuracy: 62.448\n",
            "Worker 2, [03/04]: Training Loss: 1.167964395, Training Accuracy: 65.776\n",
            "Worker 2, [04/04]: Training Loss: 1.086692941, Training Accuracy: 68.608\n",
            "Time taken for training worker 2: 0:00:12.944072\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.517438372, Training Accuracy: 57.584\n",
            "Worker 3, [02/04]: Training Loss: 1.293915764, Training Accuracy: 62.816\n",
            "Worker 3, [03/04]: Training Loss: 1.173312947, Training Accuracy: 65.120\n",
            "Worker 3, [04/04]: Training Loss: 1.089148554, Training Accuracy: 68.320\n",
            "Time taken for training worker 3: 0:00:12.890584\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.508002226, Training Accuracy: 57.104\n",
            "Worker 4, [02/04]: Training Loss: 1.313362434, Training Accuracy: 62.592\n",
            "Worker 4, [03/04]: Training Loss: 1.182132413, Training Accuracy: 66.208\n",
            "Worker 4, [04/04]: Training Loss: 1.091477788, Training Accuracy: 68.464\n",
            "Time taken for training worker 4: 0:00:13.244317\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.518410868, Training Accuracy: 56.752\n",
            "Worker 5, [02/04]: Training Loss: 1.290194725, Training Accuracy: 62.640\n",
            "Worker 5, [03/04]: Training Loss: 1.178080991, Training Accuracy: 65.808\n",
            "Worker 5, [04/04]: Training Loss: 1.103336073, Training Accuracy: 67.888\n",
            "Time taken for training worker 5: 0:00:12.879330\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.526547853, Training Accuracy: 56.720\n",
            "Worker 6, [02/04]: Training Loss: 1.328427166, Training Accuracy: 62.400\n",
            "Worker 6, [03/04]: Training Loss: 1.194499207, Training Accuracy: 65.920\n",
            "Worker 6, [04/04]: Training Loss: 1.108191829, Training Accuracy: 68.480\n",
            "Time taken for training worker 6: 0:00:12.127456\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.521722172, Training Accuracy: 57.088\n",
            "Worker 7, [02/04]: Training Loss: 1.323920305, Training Accuracy: 62.528\n",
            "Worker 7, [03/04]: Training Loss: 1.211926005, Training Accuracy: 65.744\n",
            "Worker 7, [04/04]: Training Loss: 1.120140202, Training Accuracy: 67.696\n",
            "Time taken for training worker 7: 0:00:12.238866\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.545297086, Training Accuracy: 56.192\n",
            "Worker 8, [02/04]: Training Loss: 1.345771381, Training Accuracy: 61.536\n",
            "Worker 8, [03/04]: Training Loss: 1.195598128, Training Accuracy: 65.312\n",
            "Worker 8, [04/04]: Training Loss: 1.084489659, Training Accuracy: 68.688\n",
            "Time taken for training worker 8: 0:00:12.221850\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003998\n",
            "Global Update 25: Test Loss: 2.025270197, Test Accuracy: 49.230\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.449246830, Training Accuracy: 58.576\n",
            "Worker 1, [02/04]: Training Loss: 1.256263195, Training Accuracy: 63.632\n",
            "Worker 1, [03/04]: Training Loss: 1.125717670, Training Accuracy: 67.024\n",
            "Worker 1, [04/04]: Training Loss: 1.027135564, Training Accuracy: 70.288\n",
            "Time taken for training worker 1: 0:00:12.672224\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.439700767, Training Accuracy: 59.136\n",
            "Worker 2, [02/04]: Training Loss: 1.251257061, Training Accuracy: 64.128\n",
            "Worker 2, [03/04]: Training Loss: 1.145361443, Training Accuracy: 66.560\n",
            "Worker 2, [04/04]: Training Loss: 1.043924628, Training Accuracy: 69.488\n",
            "Time taken for training worker 2: 0:00:12.503824\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.445513696, Training Accuracy: 59.504\n",
            "Worker 3, [02/04]: Training Loss: 1.234817287, Training Accuracy: 64.576\n",
            "Worker 3, [03/04]: Training Loss: 1.131820733, Training Accuracy: 67.952\n",
            "Worker 3, [04/04]: Training Loss: 1.057120133, Training Accuracy: 69.872\n",
            "Time taken for training worker 3: 0:00:12.418326\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.451076838, Training Accuracy: 58.624\n",
            "Worker 4, [02/04]: Training Loss: 1.257376704, Training Accuracy: 63.856\n",
            "Worker 4, [03/04]: Training Loss: 1.147346573, Training Accuracy: 67.120\n",
            "Worker 4, [04/04]: Training Loss: 1.074784507, Training Accuracy: 68.864\n",
            "Time taken for training worker 4: 0:00:12.209483\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.454796151, Training Accuracy: 58.352\n",
            "Worker 5, [02/04]: Training Loss: 1.274273191, Training Accuracy: 62.416\n",
            "Worker 5, [03/04]: Training Loss: 1.138172273, Training Accuracy: 66.672\n",
            "Worker 5, [04/04]: Training Loss: 1.083122325, Training Accuracy: 68.624\n",
            "Time taken for training worker 5: 0:00:12.238635\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.469448263, Training Accuracy: 58.736\n",
            "Worker 6, [02/04]: Training Loss: 1.291741926, Training Accuracy: 63.056\n",
            "Worker 6, [03/04]: Training Loss: 1.149779676, Training Accuracy: 66.512\n",
            "Worker 6, [04/04]: Training Loss: 1.071051910, Training Accuracy: 68.944\n",
            "Time taken for training worker 6: 0:00:11.788422\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.435717836, Training Accuracy: 59.488\n",
            "Worker 7, [02/04]: Training Loss: 1.270792654, Training Accuracy: 64.768\n",
            "Worker 7, [03/04]: Training Loss: 1.163047743, Training Accuracy: 66.464\n",
            "Worker 7, [04/04]: Training Loss: 1.070894306, Training Accuracy: 68.112\n",
            "Time taken for training worker 7: 0:00:12.181480\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.462177885, Training Accuracy: 58.816\n",
            "Worker 8, [02/04]: Training Loss: 1.272365912, Training Accuracy: 63.120\n",
            "Worker 8, [03/04]: Training Loss: 1.173755596, Training Accuracy: 66.304\n",
            "Worker 8, [04/04]: Training Loss: 1.074823904, Training Accuracy: 68.448\n",
            "Time taken for training worker 8: 0:00:12.579784\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004204\n",
            "Global Update 26: Test Loss: 2.037300387, Test Accuracy: 49.530\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.388623121, Training Accuracy: 59.968\n",
            "Worker 1, [02/04]: Training Loss: 1.209442061, Training Accuracy: 65.984\n",
            "Worker 1, [03/04]: Training Loss: 1.087359638, Training Accuracy: 68.816\n",
            "Worker 1, [04/04]: Training Loss: 1.032899368, Training Accuracy: 69.392\n",
            "Time taken for training worker 1: 0:00:12.889709\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.408585857, Training Accuracy: 60.336\n",
            "Worker 2, [02/04]: Training Loss: 1.222464134, Training Accuracy: 65.360\n",
            "Worker 2, [03/04]: Training Loss: 1.081381884, Training Accuracy: 68.816\n",
            "Worker 2, [04/04]: Training Loss: 0.988575058, Training Accuracy: 70.944\n",
            "Time taken for training worker 2: 0:00:12.382595\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.394328058, Training Accuracy: 60.464\n",
            "Worker 3, [02/04]: Training Loss: 1.218700274, Training Accuracy: 65.136\n",
            "Worker 3, [03/04]: Training Loss: 1.110273029, Training Accuracy: 68.480\n",
            "Worker 3, [04/04]: Training Loss: 1.010939597, Training Accuracy: 71.136\n",
            "Time taken for training worker 3: 0:00:12.323884\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.404869765, Training Accuracy: 59.792\n",
            "Worker 4, [02/04]: Training Loss: 1.240785888, Training Accuracy: 64.912\n",
            "Worker 4, [03/04]: Training Loss: 1.130553620, Training Accuracy: 67.408\n",
            "Worker 4, [04/04]: Training Loss: 1.033094918, Training Accuracy: 69.824\n",
            "Time taken for training worker 4: 0:00:11.951304\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.410090580, Training Accuracy: 59.440\n",
            "Worker 5, [02/04]: Training Loss: 1.224927914, Training Accuracy: 63.952\n",
            "Worker 5, [03/04]: Training Loss: 1.122144099, Training Accuracy: 67.776\n",
            "Worker 5, [04/04]: Training Loss: 1.016496344, Training Accuracy: 70.464\n",
            "Time taken for training worker 5: 0:00:12.091485\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.409712605, Training Accuracy: 59.536\n",
            "Worker 6, [02/04]: Training Loss: 1.214578827, Training Accuracy: 65.088\n",
            "Worker 6, [03/04]: Training Loss: 1.114025058, Training Accuracy: 68.240\n",
            "Worker 6, [04/04]: Training Loss: 1.055565947, Training Accuracy: 69.504\n",
            "Time taken for training worker 6: 0:00:12.046050\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.421563737, Training Accuracy: 58.992\n",
            "Worker 7, [02/04]: Training Loss: 1.200096685, Training Accuracy: 65.520\n",
            "Worker 7, [03/04]: Training Loss: 1.138248032, Training Accuracy: 67.264\n",
            "Worker 7, [04/04]: Training Loss: 1.028730655, Training Accuracy: 70.688\n",
            "Time taken for training worker 7: 0:00:13.038025\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.425189344, Training Accuracy: 58.992\n",
            "Worker 8, [02/04]: Training Loss: 1.229159066, Training Accuracy: 64.656\n",
            "Worker 8, [03/04]: Training Loss: 1.103059494, Training Accuracy: 68.704\n",
            "Worker 8, [04/04]: Training Loss: 1.036447550, Training Accuracy: 70.240\n",
            "Time taken for training worker 8: 0:00:12.552520\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004598\n",
            "Global Update 27: Test Loss: 2.046156627, Test Accuracy: 49.980\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.341939385, Training Accuracy: 61.120\n",
            "Worker 1, [02/04]: Training Loss: 1.169736110, Training Accuracy: 66.320\n",
            "Worker 1, [03/04]: Training Loss: 1.067615248, Training Accuracy: 68.624\n",
            "Worker 1, [04/04]: Training Loss: 0.988993447, Training Accuracy: 71.520\n",
            "Time taken for training worker 1: 0:00:12.558672\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.360726705, Training Accuracy: 60.816\n",
            "Worker 2, [02/04]: Training Loss: 1.186905193, Training Accuracy: 65.984\n",
            "Worker 2, [03/04]: Training Loss: 1.070467079, Training Accuracy: 69.232\n",
            "Worker 2, [04/04]: Training Loss: 0.993347644, Training Accuracy: 71.744\n",
            "Time taken for training worker 2: 0:00:12.073331\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.351452165, Training Accuracy: 61.952\n",
            "Worker 3, [02/04]: Training Loss: 1.168708281, Training Accuracy: 66.144\n",
            "Worker 3, [03/04]: Training Loss: 1.069036155, Training Accuracy: 69.088\n",
            "Worker 3, [04/04]: Training Loss: 0.992879940, Training Accuracy: 71.360\n",
            "Time taken for training worker 3: 0:00:12.125395\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.343430213, Training Accuracy: 61.536\n",
            "Worker 4, [02/04]: Training Loss: 1.188765130, Training Accuracy: 65.376\n",
            "Worker 4, [03/04]: Training Loss: 1.089711183, Training Accuracy: 69.200\n",
            "Worker 4, [04/04]: Training Loss: 0.995337918, Training Accuracy: 72.032\n",
            "Time taken for training worker 4: 0:00:12.432385\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.370166327, Training Accuracy: 60.512\n",
            "Worker 5, [02/04]: Training Loss: 1.165938553, Training Accuracy: 65.968\n",
            "Worker 5, [03/04]: Training Loss: 1.080573228, Training Accuracy: 68.592\n",
            "Worker 5, [04/04]: Training Loss: 0.984520777, Training Accuracy: 71.536\n",
            "Time taken for training worker 5: 0:00:11.968527\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.378492773, Training Accuracy: 60.560\n",
            "Worker 6, [02/04]: Training Loss: 1.215857604, Training Accuracy: 65.520\n",
            "Worker 6, [03/04]: Training Loss: 1.076337142, Training Accuracy: 68.976\n",
            "Worker 6, [04/04]: Training Loss: 1.037536485, Training Accuracy: 70.352\n",
            "Time taken for training worker 6: 0:00:12.355811\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.356120807, Training Accuracy: 60.496\n",
            "Worker 7, [02/04]: Training Loss: 1.194646882, Training Accuracy: 64.944\n",
            "Worker 7, [03/04]: Training Loss: 1.094363546, Training Accuracy: 68.432\n",
            "Worker 7, [04/04]: Training Loss: 1.022398860, Training Accuracy: 69.968\n",
            "Time taken for training worker 7: 0:00:12.043214\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.344691964, Training Accuracy: 61.296\n",
            "Worker 8, [02/04]: Training Loss: 1.190171179, Training Accuracy: 65.664\n",
            "Worker 8, [03/04]: Training Loss: 1.081314444, Training Accuracy: 68.752\n",
            "Worker 8, [04/04]: Training Loss: 1.003526250, Training Accuracy: 71.408\n",
            "Time taken for training worker 8: 0:00:11.843443\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003817\n",
            "Global Update 28: Test Loss: 2.051829106, Test Accuracy: 49.770\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.304148792, Training Accuracy: 62.224\n",
            "Worker 1, [02/04]: Training Loss: 1.136334163, Training Accuracy: 67.024\n",
            "Worker 1, [03/04]: Training Loss: 1.052865430, Training Accuracy: 69.664\n",
            "Worker 1, [04/04]: Training Loss: 0.967915439, Training Accuracy: 72.368\n",
            "Time taken for training worker 1: 0:00:12.779768\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.320586701, Training Accuracy: 61.184\n",
            "Worker 2, [02/04]: Training Loss: 1.158099530, Training Accuracy: 67.024\n",
            "Worker 2, [03/04]: Training Loss: 1.036744666, Training Accuracy: 69.776\n",
            "Worker 2, [04/04]: Training Loss: 0.967217899, Training Accuracy: 71.792\n",
            "Time taken for training worker 2: 0:00:12.606047\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.342390181, Training Accuracy: 61.728\n",
            "Worker 3, [02/04]: Training Loss: 1.165005985, Training Accuracy: 67.024\n",
            "Worker 3, [03/04]: Training Loss: 1.050350164, Training Accuracy: 69.712\n",
            "Worker 3, [04/04]: Training Loss: 0.974435025, Training Accuracy: 72.144\n",
            "Time taken for training worker 3: 0:00:12.395977\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.320518432, Training Accuracy: 62.544\n",
            "Worker 4, [02/04]: Training Loss: 1.144441527, Training Accuracy: 66.880\n",
            "Worker 4, [03/04]: Training Loss: 1.072477246, Training Accuracy: 68.528\n",
            "Worker 4, [04/04]: Training Loss: 0.984734488, Training Accuracy: 72.208\n",
            "Time taken for training worker 4: 0:00:11.826105\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.336687466, Training Accuracy: 61.744\n",
            "Worker 5, [02/04]: Training Loss: 1.151991255, Training Accuracy: 66.912\n",
            "Worker 5, [03/04]: Training Loss: 1.066770380, Training Accuracy: 69.088\n",
            "Worker 5, [04/04]: Training Loss: 0.970115573, Training Accuracy: 72.032\n",
            "Time taken for training worker 5: 0:00:12.185206\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.328535877, Training Accuracy: 61.504\n",
            "Worker 6, [02/04]: Training Loss: 1.161786683, Training Accuracy: 66.416\n",
            "Worker 6, [03/04]: Training Loss: 1.076325781, Training Accuracy: 69.152\n",
            "Worker 6, [04/04]: Training Loss: 0.988253289, Training Accuracy: 71.504\n",
            "Time taken for training worker 6: 0:00:12.881424\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.305692094, Training Accuracy: 62.608\n",
            "Worker 7, [02/04]: Training Loss: 1.163666582, Training Accuracy: 66.512\n",
            "Worker 7, [03/04]: Training Loss: 1.067953480, Training Accuracy: 69.616\n",
            "Worker 7, [04/04]: Training Loss: 0.981463863, Training Accuracy: 71.648\n",
            "Time taken for training worker 7: 0:00:12.533194\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.336749808, Training Accuracy: 61.520\n",
            "Worker 8, [02/04]: Training Loss: 1.166699611, Training Accuracy: 66.928\n",
            "Worker 8, [03/04]: Training Loss: 1.040130764, Training Accuracy: 69.872\n",
            "Worker 8, [04/04]: Training Loss: 0.989198878, Training Accuracy: 71.792\n",
            "Time taken for training worker 8: 0:00:12.306677\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004085\n",
            "Global Update 29: Test Loss: 2.055013093, Test Accuracy: 50.080\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.268461679, Training Accuracy: 62.688\n",
            "Worker 1, [02/04]: Training Loss: 1.125005324, Training Accuracy: 67.776\n",
            "Worker 1, [03/04]: Training Loss: 1.022443726, Training Accuracy: 70.640\n",
            "Worker 1, [04/04]: Training Loss: 0.968282828, Training Accuracy: 72.816\n",
            "Time taken for training worker 1: 0:00:12.377664\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.261049803, Training Accuracy: 63.936\n",
            "Worker 2, [02/04]: Training Loss: 1.131620340, Training Accuracy: 67.040\n",
            "Worker 2, [03/04]: Training Loss: 1.030476505, Training Accuracy: 70.144\n",
            "Worker 2, [04/04]: Training Loss: 0.955462035, Training Accuracy: 72.080\n",
            "Time taken for training worker 2: 0:00:11.846676\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.272880009, Training Accuracy: 63.344\n",
            "Worker 3, [02/04]: Training Loss: 1.133547232, Training Accuracy: 68.032\n",
            "Worker 3, [03/04]: Training Loss: 1.033878482, Training Accuracy: 70.528\n",
            "Worker 3, [04/04]: Training Loss: 0.964340771, Training Accuracy: 72.560\n",
            "Time taken for training worker 3: 0:00:12.352770\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.292224571, Training Accuracy: 62.592\n",
            "Worker 4, [02/04]: Training Loss: 1.127878206, Training Accuracy: 67.424\n",
            "Worker 4, [03/04]: Training Loss: 1.042674557, Training Accuracy: 70.064\n",
            "Worker 4, [04/04]: Training Loss: 0.979962422, Training Accuracy: 72.448\n",
            "Time taken for training worker 4: 0:00:11.808218\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.291604872, Training Accuracy: 62.592\n",
            "Worker 5, [02/04]: Training Loss: 1.160551061, Training Accuracy: 66.736\n",
            "Worker 5, [03/04]: Training Loss: 1.049695628, Training Accuracy: 69.856\n",
            "Worker 5, [04/04]: Training Loss: 0.982979658, Training Accuracy: 71.984\n",
            "Time taken for training worker 5: 0:00:12.204169\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.279640860, Training Accuracy: 63.056\n",
            "Worker 6, [02/04]: Training Loss: 1.153488098, Training Accuracy: 66.656\n",
            "Worker 6, [03/04]: Training Loss: 1.046520149, Training Accuracy: 69.920\n",
            "Worker 6, [04/04]: Training Loss: 0.984619639, Training Accuracy: 71.840\n",
            "Time taken for training worker 6: 0:00:13.055147\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.283795722, Training Accuracy: 63.760\n",
            "Worker 7, [02/04]: Training Loss: 1.139699019, Training Accuracy: 67.696\n",
            "Worker 7, [03/04]: Training Loss: 1.034226945, Training Accuracy: 70.592\n",
            "Worker 7, [04/04]: Training Loss: 0.979445747, Training Accuracy: 72.176\n",
            "Time taken for training worker 7: 0:00:12.199882\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.305059311, Training Accuracy: 62.624\n",
            "Worker 8, [02/04]: Training Loss: 1.137935211, Training Accuracy: 67.680\n",
            "Worker 8, [03/04]: Training Loss: 1.042953621, Training Accuracy: 70.352\n",
            "Worker 8, [04/04]: Training Loss: 0.954526572, Training Accuracy: 72.624\n",
            "Time taken for training worker 8: 0:00:12.494889\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003819\n",
            "Global Update 30: Test Loss: 2.048781106, Test Accuracy: 50.250\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.237213286, Training Accuracy: 64.576\n",
            "Worker 1, [02/04]: Training Loss: 1.111276897, Training Accuracy: 67.984\n",
            "Worker 1, [03/04]: Training Loss: 1.037056335, Training Accuracy: 70.160\n",
            "Worker 1, [04/04]: Training Loss: 0.972677045, Training Accuracy: 71.216\n",
            "Time taken for training worker 1: 0:00:12.277497\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.235617590, Training Accuracy: 64.320\n",
            "Worker 2, [02/04]: Training Loss: 1.099082406, Training Accuracy: 68.336\n",
            "Worker 2, [03/04]: Training Loss: 1.027985196, Training Accuracy: 69.920\n",
            "Worker 2, [04/04]: Training Loss: 0.956194334, Training Accuracy: 72.128\n",
            "Time taken for training worker 2: 0:00:12.621752\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.246208387, Training Accuracy: 63.920\n",
            "Worker 3, [02/04]: Training Loss: 1.107328923, Training Accuracy: 68.208\n",
            "Worker 3, [03/04]: Training Loss: 1.002507719, Training Accuracy: 71.664\n",
            "Worker 3, [04/04]: Training Loss: 0.955447067, Training Accuracy: 72.672\n",
            "Time taken for training worker 3: 0:00:12.107421\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.241848735, Training Accuracy: 64.080\n",
            "Worker 4, [02/04]: Training Loss: 1.126855052, Training Accuracy: 67.920\n",
            "Worker 4, [03/04]: Training Loss: 1.048652058, Training Accuracy: 70.336\n",
            "Worker 4, [04/04]: Training Loss: 0.970297018, Training Accuracy: 71.664\n",
            "Time taken for training worker 4: 0:00:12.079586\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.260475126, Training Accuracy: 63.920\n",
            "Worker 5, [02/04]: Training Loss: 1.121284291, Training Accuracy: 67.872\n",
            "Worker 5, [03/04]: Training Loss: 1.018521293, Training Accuracy: 70.880\n",
            "Worker 5, [04/04]: Training Loss: 0.969767603, Training Accuracy: 72.592\n",
            "Time taken for training worker 5: 0:00:12.777161\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.260361752, Training Accuracy: 63.600\n",
            "Worker 6, [02/04]: Training Loss: 1.127409493, Training Accuracy: 67.152\n",
            "Worker 6, [03/04]: Training Loss: 1.043050233, Training Accuracy: 69.936\n",
            "Worker 6, [04/04]: Training Loss: 0.966192376, Training Accuracy: 72.528\n",
            "Time taken for training worker 6: 0:00:12.077240\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.265976459, Training Accuracy: 62.976\n",
            "Worker 7, [02/04]: Training Loss: 1.122581751, Training Accuracy: 67.616\n",
            "Worker 7, [03/04]: Training Loss: 1.055802235, Training Accuracy: 69.168\n",
            "Worker 7, [04/04]: Training Loss: 0.985259240, Training Accuracy: 71.600\n",
            "Time taken for training worker 7: 0:00:12.614674\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.259295120, Training Accuracy: 64.224\n",
            "Worker 8, [02/04]: Training Loss: 1.126718060, Training Accuracy: 67.328\n",
            "Worker 8, [03/04]: Training Loss: 1.057699686, Training Accuracy: 69.808\n",
            "Worker 8, [04/04]: Training Loss: 0.982223715, Training Accuracy: 71.184\n",
            "Time taken for training worker 8: 0:00:12.821641\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.005364\n",
            "Global Update 31: Test Loss: 2.046382556, Test Accuracy: 50.270\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.212648630, Training Accuracy: 64.640\n",
            "Worker 1, [02/04]: Training Loss: 1.084300135, Training Accuracy: 68.912\n",
            "Worker 1, [03/04]: Training Loss: 1.038801756, Training Accuracy: 70.144\n",
            "Worker 1, [04/04]: Training Loss: 0.942696368, Training Accuracy: 72.992\n",
            "Time taken for training worker 1: 0:00:12.090992\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.198690569, Training Accuracy: 65.632\n",
            "Worker 2, [02/04]: Training Loss: 1.094751644, Training Accuracy: 68.048\n",
            "Worker 2, [03/04]: Training Loss: 1.014062142, Training Accuracy: 70.512\n",
            "Worker 2, [04/04]: Training Loss: 0.957945080, Training Accuracy: 72.960\n",
            "Time taken for training worker 2: 0:00:11.890806\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.226229631, Training Accuracy: 64.464\n",
            "Worker 3, [02/04]: Training Loss: 1.108397725, Training Accuracy: 67.872\n",
            "Worker 3, [03/04]: Training Loss: 1.024128182, Training Accuracy: 70.688\n",
            "Worker 3, [04/04]: Training Loss: 0.983017488, Training Accuracy: 71.424\n",
            "Time taken for training worker 3: 0:00:12.619559\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.237096435, Training Accuracy: 63.808\n",
            "Worker 4, [02/04]: Training Loss: 1.110108899, Training Accuracy: 68.448\n",
            "Worker 4, [03/04]: Training Loss: 1.039564287, Training Accuracy: 70.272\n",
            "Worker 4, [04/04]: Training Loss: 0.970615965, Training Accuracy: 72.096\n",
            "Time taken for training worker 4: 0:00:11.868027\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.230428783, Training Accuracy: 64.688\n",
            "Worker 5, [02/04]: Training Loss: 1.102790005, Training Accuracy: 68.256\n",
            "Worker 5, [03/04]: Training Loss: 1.034334766, Training Accuracy: 70.064\n",
            "Worker 5, [04/04]: Training Loss: 0.982098511, Training Accuracy: 72.080\n",
            "Time taken for training worker 5: 0:00:12.255300\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.228525530, Training Accuracy: 64.816\n",
            "Worker 6, [02/04]: Training Loss: 1.116964171, Training Accuracy: 67.728\n",
            "Worker 6, [03/04]: Training Loss: 1.034158068, Training Accuracy: 69.744\n",
            "Worker 6, [04/04]: Training Loss: 0.984725504, Training Accuracy: 71.616\n",
            "Time taken for training worker 6: 0:00:12.530521\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.235955758, Training Accuracy: 64.240\n",
            "Worker 7, [02/04]: Training Loss: 1.107051528, Training Accuracy: 68.272\n",
            "Worker 7, [03/04]: Training Loss: 1.031217122, Training Accuracy: 70.528\n",
            "Worker 7, [04/04]: Training Loss: 0.985937774, Training Accuracy: 71.696\n",
            "Time taken for training worker 7: 0:00:12.171010\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.238335707, Training Accuracy: 64.352\n",
            "Worker 8, [02/04]: Training Loss: 1.113671113, Training Accuracy: 68.176\n",
            "Worker 8, [03/04]: Training Loss: 1.030358576, Training Accuracy: 70.896\n",
            "Worker 8, [04/04]: Training Loss: 0.985549482, Training Accuracy: 71.728\n",
            "Time taken for training worker 8: 0:00:13.088641\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004319\n",
            "Global Update 32: Test Loss: 2.052926011, Test Accuracy: 50.170\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.176054414, Training Accuracy: 65.968\n",
            "Worker 1, [02/04]: Training Loss: 1.097787659, Training Accuracy: 67.584\n",
            "Worker 1, [03/04]: Training Loss: 1.028756777, Training Accuracy: 70.176\n",
            "Worker 1, [04/04]: Training Loss: 0.981664640, Training Accuracy: 71.584\n",
            "Time taken for training worker 1: 0:00:11.994792\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.190914779, Training Accuracy: 65.200\n",
            "Worker 2, [02/04]: Training Loss: 1.080914890, Training Accuracy: 68.528\n",
            "Worker 2, [03/04]: Training Loss: 1.035564562, Training Accuracy: 69.760\n",
            "Worker 2, [04/04]: Training Loss: 0.969420935, Training Accuracy: 72.752\n",
            "Time taken for training worker 2: 0:00:11.996404\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.227393450, Training Accuracy: 65.200\n",
            "Worker 3, [02/04]: Training Loss: 1.091806514, Training Accuracy: 68.880\n",
            "Worker 3, [03/04]: Training Loss: 1.047441669, Training Accuracy: 70.096\n",
            "Worker 3, [04/04]: Training Loss: 0.993224472, Training Accuracy: 71.488\n",
            "Time taken for training worker 3: 0:00:12.018491\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.215443119, Training Accuracy: 65.200\n",
            "Worker 4, [02/04]: Training Loss: 1.099798760, Training Accuracy: 67.680\n",
            "Worker 4, [03/04]: Training Loss: 1.049898613, Training Accuracy: 69.536\n",
            "Worker 4, [04/04]: Training Loss: 0.995184099, Training Accuracy: 71.072\n",
            "Time taken for training worker 4: 0:00:11.693247\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.184937813, Training Accuracy: 65.424\n",
            "Worker 5, [02/04]: Training Loss: 1.104934227, Training Accuracy: 67.760\n",
            "Worker 5, [03/04]: Training Loss: 1.035054540, Training Accuracy: 70.240\n",
            "Worker 5, [04/04]: Training Loss: 0.987612514, Training Accuracy: 71.888\n",
            "Time taken for training worker 5: 0:00:12.057822\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.230073448, Training Accuracy: 64.512\n",
            "Worker 6, [02/04]: Training Loss: 1.114515196, Training Accuracy: 68.352\n",
            "Worker 6, [03/04]: Training Loss: 1.044241618, Training Accuracy: 70.320\n",
            "Worker 6, [04/04]: Training Loss: 0.993361688, Training Accuracy: 72.032\n",
            "Time taken for training worker 6: 0:00:12.848544\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.202047908, Training Accuracy: 65.104\n",
            "Worker 7, [02/04]: Training Loss: 1.111252652, Training Accuracy: 68.096\n",
            "Worker 7, [03/04]: Training Loss: 1.047783868, Training Accuracy: 69.824\n",
            "Worker 7, [04/04]: Training Loss: 1.001362257, Training Accuracy: 71.280\n",
            "Time taken for training worker 7: 0:00:12.610630\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.224182386, Training Accuracy: 64.352\n",
            "Worker 8, [02/04]: Training Loss: 1.106800751, Training Accuracy: 68.496\n",
            "Worker 8, [03/04]: Training Loss: 1.044930350, Training Accuracy: 70.032\n",
            "Worker 8, [04/04]: Training Loss: 1.014270781, Training Accuracy: 71.088\n",
            "Time taken for training worker 8: 0:00:12.661591\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.005653\n",
            "Global Update 33: Test Loss: 2.051673541, Test Accuracy: 50.300\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.182382393, Training Accuracy: 65.344\n",
            "Worker 1, [02/04]: Training Loss: 1.109558928, Training Accuracy: 67.264\n",
            "Worker 1, [03/04]: Training Loss: 1.046977189, Training Accuracy: 70.144\n",
            "Worker 1, [04/04]: Training Loss: 1.001835735, Training Accuracy: 70.976\n",
            "Time taken for training worker 1: 0:00:12.439266\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.180491201, Training Accuracy: 66.688\n",
            "Worker 2, [02/04]: Training Loss: 1.090249042, Training Accuracy: 68.576\n",
            "Worker 2, [03/04]: Training Loss: 1.053955392, Training Accuracy: 70.208\n",
            "Worker 2, [04/04]: Training Loss: 0.987429045, Training Accuracy: 71.440\n",
            "Time taken for training worker 2: 0:00:12.632177\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.195040238, Training Accuracy: 65.840\n",
            "Worker 3, [02/04]: Training Loss: 1.095302129, Training Accuracy: 68.592\n",
            "Worker 3, [03/04]: Training Loss: 1.063653211, Training Accuracy: 69.520\n",
            "Worker 3, [04/04]: Training Loss: 1.012158366, Training Accuracy: 71.184\n",
            "Time taken for training worker 3: 0:00:12.105485\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.169078553, Training Accuracy: 65.776\n",
            "Worker 4, [02/04]: Training Loss: 1.109426058, Training Accuracy: 68.416\n",
            "Worker 4, [03/04]: Training Loss: 1.063971240, Training Accuracy: 69.232\n",
            "Worker 4, [04/04]: Training Loss: 1.005932968, Training Accuracy: 70.496\n",
            "Time taken for training worker 4: 0:00:12.151007\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.198649071, Training Accuracy: 65.120\n",
            "Worker 5, [02/04]: Training Loss: 1.116760294, Training Accuracy: 68.176\n",
            "Worker 5, [03/04]: Training Loss: 1.058749074, Training Accuracy: 68.912\n",
            "Worker 5, [04/04]: Training Loss: 1.028096083, Training Accuracy: 70.336\n",
            "Time taken for training worker 5: 0:00:12.114874\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.200458626, Training Accuracy: 64.912\n",
            "Worker 6, [02/04]: Training Loss: 1.130737353, Training Accuracy: 67.680\n",
            "Worker 6, [03/04]: Training Loss: 1.081203421, Training Accuracy: 69.424\n",
            "Worker 6, [04/04]: Training Loss: 1.018649018, Training Accuracy: 70.656\n",
            "Time taken for training worker 6: 0:00:12.349154\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.179298985, Training Accuracy: 65.712\n",
            "Worker 7, [02/04]: Training Loss: 1.108555057, Training Accuracy: 68.288\n",
            "Worker 7, [03/04]: Training Loss: 1.054928707, Training Accuracy: 69.888\n",
            "Worker 7, [04/04]: Training Loss: 1.047677220, Training Accuracy: 70.288\n",
            "Time taken for training worker 7: 0:00:12.134075\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.210638557, Training Accuracy: 64.768\n",
            "Worker 8, [02/04]: Training Loss: 1.123607067, Training Accuracy: 68.400\n",
            "Worker 8, [03/04]: Training Loss: 1.061828893, Training Accuracy: 69.216\n",
            "Worker 8, [04/04]: Training Loss: 1.029667041, Training Accuracy: 69.888\n",
            "Time taken for training worker 8: 0:00:12.524865\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003950\n",
            "Global Update 34: Test Loss: 2.046533020, Test Accuracy: 50.280\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.155847118, Training Accuracy: 65.920\n",
            "Worker 1, [02/04]: Training Loss: 1.121676334, Training Accuracy: 67.344\n",
            "Worker 1, [03/04]: Training Loss: 1.069273868, Training Accuracy: 69.248\n",
            "Worker 1, [04/04]: Training Loss: 1.025660869, Training Accuracy: 69.904\n",
            "Time taken for training worker 1: 0:00:12.363707\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.138571747, Training Accuracy: 67.168\n",
            "Worker 2, [02/04]: Training Loss: 1.110645686, Training Accuracy: 67.504\n",
            "Worker 2, [03/04]: Training Loss: 1.064212286, Training Accuracy: 69.520\n",
            "Worker 2, [04/04]: Training Loss: 1.045707064, Training Accuracy: 69.552\n",
            "Time taken for training worker 2: 0:00:12.723495\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.163108089, Training Accuracy: 66.576\n",
            "Worker 3, [02/04]: Training Loss: 1.130757343, Training Accuracy: 67.072\n",
            "Worker 3, [03/04]: Training Loss: 1.085458419, Training Accuracy: 68.688\n",
            "Worker 3, [04/04]: Training Loss: 1.052073539, Training Accuracy: 69.696\n",
            "Time taken for training worker 3: 0:00:12.044139\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.187086801, Training Accuracy: 65.648\n",
            "Worker 4, [02/04]: Training Loss: 1.147845786, Training Accuracy: 66.352\n",
            "Worker 4, [03/04]: Training Loss: 1.091652527, Training Accuracy: 69.008\n",
            "Worker 4, [04/04]: Training Loss: 1.053284415, Training Accuracy: 69.360\n",
            "Time taken for training worker 4: 0:00:12.255340\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.203366091, Training Accuracy: 64.800\n",
            "Worker 5, [02/04]: Training Loss: 1.099761984, Training Accuracy: 68.208\n",
            "Worker 5, [03/04]: Training Loss: 1.084145088, Training Accuracy: 68.304\n",
            "Worker 5, [04/04]: Training Loss: 1.051998097, Training Accuracy: 69.728\n",
            "Time taken for training worker 5: 0:00:12.552851\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.194518826, Training Accuracy: 65.824\n",
            "Worker 6, [02/04]: Training Loss: 1.140195382, Training Accuracy: 67.360\n",
            "Worker 6, [03/04]: Training Loss: 1.075815694, Training Accuracy: 69.520\n",
            "Worker 6, [04/04]: Training Loss: 1.067783567, Training Accuracy: 69.360\n",
            "Time taken for training worker 6: 0:00:12.679736\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.187801502, Training Accuracy: 65.696\n",
            "Worker 7, [02/04]: Training Loss: 1.130447468, Training Accuracy: 68.016\n",
            "Worker 7, [03/04]: Training Loss: 1.087339029, Training Accuracy: 68.800\n",
            "Worker 7, [04/04]: Training Loss: 1.059881947, Training Accuracy: 69.072\n",
            "Time taken for training worker 7: 0:00:12.168975\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.168887210, Training Accuracy: 66.416\n",
            "Worker 8, [02/04]: Training Loss: 1.126940590, Training Accuracy: 67.296\n",
            "Worker 8, [03/04]: Training Loss: 1.095108923, Training Accuracy: 68.160\n",
            "Worker 8, [04/04]: Training Loss: 1.040278752, Training Accuracy: 69.968\n",
            "Time taken for training worker 8: 0:00:12.068448\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004243\n",
            "Global Update 35: Test Loss: 2.038688303, Test Accuracy: 50.380\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.170985196, Training Accuracy: 65.728\n",
            "Worker 1, [02/04]: Training Loss: 1.126721423, Training Accuracy: 67.472\n",
            "Worker 1, [03/04]: Training Loss: 1.099221928, Training Accuracy: 67.632\n",
            "Worker 1, [04/04]: Training Loss: 1.082366645, Training Accuracy: 68.608\n",
            "Time taken for training worker 1: 0:00:12.263603\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.180365585, Training Accuracy: 65.360\n",
            "Worker 2, [02/04]: Training Loss: 1.118373092, Training Accuracy: 67.584\n",
            "Worker 2, [03/04]: Training Loss: 1.095031094, Training Accuracy: 68.208\n",
            "Worker 2, [04/04]: Training Loss: 1.102329384, Training Accuracy: 68.144\n",
            "Time taken for training worker 2: 0:00:12.088607\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.166485724, Training Accuracy: 66.448\n",
            "Worker 3, [02/04]: Training Loss: 1.130699596, Training Accuracy: 67.680\n",
            "Worker 3, [03/04]: Training Loss: 1.120416212, Training Accuracy: 68.000\n",
            "Worker 3, [04/04]: Training Loss: 1.095579160, Training Accuracy: 68.224\n",
            "Time taken for training worker 3: 0:00:12.239439\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.166231512, Training Accuracy: 66.352\n",
            "Worker 4, [02/04]: Training Loss: 1.122476637, Training Accuracy: 66.688\n",
            "Worker 4, [03/04]: Training Loss: 1.112706352, Training Accuracy: 67.536\n",
            "Worker 4, [04/04]: Training Loss: 1.097201276, Training Accuracy: 67.792\n",
            "Time taken for training worker 4: 0:00:12.543064\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.192249727, Training Accuracy: 64.944\n",
            "Worker 5, [02/04]: Training Loss: 1.150788274, Training Accuracy: 66.800\n",
            "Worker 5, [03/04]: Training Loss: 1.126059900, Training Accuracy: 67.216\n",
            "Worker 5, [04/04]: Training Loss: 1.115218838, Training Accuracy: 68.016\n",
            "Time taken for training worker 5: 0:00:12.645112\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.183980861, Training Accuracy: 66.288\n",
            "Worker 6, [02/04]: Training Loss: 1.144856108, Training Accuracy: 66.976\n",
            "Worker 6, [03/04]: Training Loss: 1.115445420, Training Accuracy: 67.824\n",
            "Worker 6, [04/04]: Training Loss: 1.107536948, Training Accuracy: 68.256\n",
            "Time taken for training worker 6: 0:00:12.219452\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.163779633, Training Accuracy: 66.976\n",
            "Worker 7, [02/04]: Training Loss: 1.153898979, Training Accuracy: 66.880\n",
            "Worker 7, [03/04]: Training Loss: 1.130795484, Training Accuracy: 66.960\n",
            "Worker 7, [04/04]: Training Loss: 1.111759333, Training Accuracy: 68.432\n",
            "Time taken for training worker 7: 0:00:12.471191\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.183333505, Training Accuracy: 65.952\n",
            "Worker 8, [02/04]: Training Loss: 1.168602287, Training Accuracy: 66.480\n",
            "Worker 8, [03/04]: Training Loss: 1.109794889, Training Accuracy: 68.352\n",
            "Worker 8, [04/04]: Training Loss: 1.115254301, Training Accuracy: 68.272\n",
            "Time taken for training worker 8: 0:00:12.410043\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.005584\n",
            "Global Update 36: Test Loss: 2.032550403, Test Accuracy: 50.490\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.155674889, Training Accuracy: 66.016\n",
            "Worker 1, [02/04]: Training Loss: 1.138196484, Training Accuracy: 66.160\n",
            "Worker 1, [03/04]: Training Loss: 1.142588187, Training Accuracy: 66.272\n",
            "Worker 1, [04/04]: Training Loss: 1.124233339, Training Accuracy: 66.560\n",
            "Time taken for training worker 1: 0:00:12.182281\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.142532194, Training Accuracy: 66.896\n",
            "Worker 2, [02/04]: Training Loss: 1.149309718, Training Accuracy: 66.896\n",
            "Worker 2, [03/04]: Training Loss: 1.139831909, Training Accuracy: 67.184\n",
            "Worker 2, [04/04]: Training Loss: 1.136002096, Training Accuracy: 66.720\n",
            "Time taken for training worker 2: 0:00:12.129059\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.169890781, Training Accuracy: 66.688\n",
            "Worker 3, [02/04]: Training Loss: 1.169249287, Training Accuracy: 66.240\n",
            "Worker 3, [03/04]: Training Loss: 1.142710510, Training Accuracy: 66.832\n",
            "Worker 3, [04/04]: Training Loss: 1.154940870, Training Accuracy: 66.528\n",
            "Time taken for training worker 3: 0:00:12.713600\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.167377132, Training Accuracy: 66.256\n",
            "Worker 4, [02/04]: Training Loss: 1.149033490, Training Accuracy: 67.024\n",
            "Worker 4, [03/04]: Training Loss: 1.147385510, Training Accuracy: 66.864\n",
            "Worker 4, [04/04]: Training Loss: 1.154522212, Training Accuracy: 65.904\n",
            "Time taken for training worker 4: 0:00:11.888747\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.181573882, Training Accuracy: 66.000\n",
            "Worker 5, [02/04]: Training Loss: 1.165607582, Training Accuracy: 66.048\n",
            "Worker 5, [03/04]: Training Loss: 1.168909618, Training Accuracy: 65.552\n",
            "Worker 5, [04/04]: Training Loss: 1.149686729, Training Accuracy: 66.016\n",
            "Time taken for training worker 5: 0:00:12.215942\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.186593550, Training Accuracy: 65.776\n",
            "Worker 6, [02/04]: Training Loss: 1.167356358, Training Accuracy: 66.368\n",
            "Worker 6, [03/04]: Training Loss: 1.169926831, Training Accuracy: 66.256\n",
            "Worker 6, [04/04]: Training Loss: 1.164576541, Training Accuracy: 66.528\n",
            "Time taken for training worker 6: 0:00:12.631736\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.165041730, Training Accuracy: 66.464\n",
            "Worker 7, [02/04]: Training Loss: 1.168886362, Training Accuracy: 66.432\n",
            "Worker 7, [03/04]: Training Loss: 1.162010047, Training Accuracy: 66.832\n",
            "Worker 7, [04/04]: Training Loss: 1.138727770, Training Accuracy: 67.200\n",
            "Time taken for training worker 7: 0:00:12.257624\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.198589635, Training Accuracy: 65.296\n",
            "Worker 8, [02/04]: Training Loss: 1.183683002, Training Accuracy: 66.256\n",
            "Worker 8, [03/04]: Training Loss: 1.174799543, Training Accuracy: 66.208\n",
            "Worker 8, [04/04]: Training Loss: 1.153961242, Training Accuracy: 67.216\n",
            "Time taken for training worker 8: 0:00:12.163525\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004385\n",
            "Global Update 37: Test Loss: 2.028979112, Test Accuracy: 50.520\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 1:01:19.160412\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:8\n",
            "==================================================\n",
            "Worker 1, [01/08]: Training Loss: 4.596244676, Training Accuracy: 1.504\n",
            "Worker 1, [02/08]: Training Loss: 4.393467212, Training Accuracy: 3.568\n",
            "Worker 1, [03/08]: Training Loss: 4.139768007, Training Accuracy: 6.080\n",
            "Worker 1, [04/08]: Training Loss: 4.001885054, Training Accuracy: 7.648\n",
            "Worker 1, [05/08]: Training Loss: 3.889165579, Training Accuracy: 9.136\n",
            "Worker 1, [06/08]: Training Loss: 3.793405387, Training Accuracy: 10.480\n",
            "Worker 1, [07/08]: Training Loss: 3.702698858, Training Accuracy: 11.728\n",
            "Worker 1, [08/08]: Training Loss: 3.629385031, Training Accuracy: 13.072\n",
            "Time taken for training worker 1: 0:00:24.958846\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 4.596803388, Training Accuracy: 1.792\n",
            "Worker 2, [02/08]: Training Loss: 4.422835987, Training Accuracy: 3.936\n",
            "Worker 2, [03/08]: Training Loss: 4.178244077, Training Accuracy: 5.920\n",
            "Worker 2, [04/08]: Training Loss: 4.043824505, Training Accuracy: 7.760\n",
            "Worker 2, [05/08]: Training Loss: 3.901959103, Training Accuracy: 10.144\n",
            "Worker 2, [06/08]: Training Loss: 3.814765441, Training Accuracy: 10.784\n",
            "Worker 2, [07/08]: Training Loss: 3.713918980, Training Accuracy: 12.640\n",
            "Worker 2, [08/08]: Training Loss: 3.618578884, Training Accuracy: 14.464\n",
            "Time taken for training worker 2: 0:00:24.370703\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 4.596456450, Training Accuracy: 1.984\n",
            "Worker 3, [02/08]: Training Loss: 4.416588885, Training Accuracy: 3.552\n",
            "Worker 3, [03/08]: Training Loss: 4.181285814, Training Accuracy: 6.128\n",
            "Worker 3, [04/08]: Training Loss: 4.040505560, Training Accuracy: 7.424\n",
            "Worker 3, [05/08]: Training Loss: 3.935692196, Training Accuracy: 9.072\n",
            "Worker 3, [06/08]: Training Loss: 3.834140797, Training Accuracy: 10.160\n",
            "Worker 3, [07/08]: Training Loss: 3.738111948, Training Accuracy: 11.968\n",
            "Worker 3, [08/08]: Training Loss: 3.642475712, Training Accuracy: 13.936\n",
            "Time taken for training worker 3: 0:00:24.174714\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 4.597214329, Training Accuracy: 1.920\n",
            "Worker 4, [02/08]: Training Loss: 4.414501511, Training Accuracy: 3.552\n",
            "Worker 4, [03/08]: Training Loss: 4.170645188, Training Accuracy: 6.000\n",
            "Worker 4, [04/08]: Training Loss: 4.025181328, Training Accuracy: 7.472\n",
            "Worker 4, [05/08]: Training Loss: 3.928969648, Training Accuracy: 8.976\n",
            "Worker 4, [06/08]: Training Loss: 3.846593640, Training Accuracy: 10.080\n",
            "Worker 4, [07/08]: Training Loss: 3.735230964, Training Accuracy: 10.992\n",
            "Worker 4, [08/08]: Training Loss: 3.660213293, Training Accuracy: 13.136\n",
            "Time taken for training worker 4: 0:00:24.236321\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 4.594385040, Training Accuracy: 1.712\n",
            "Worker 5, [02/08]: Training Loss: 4.396882291, Training Accuracy: 3.696\n",
            "Worker 5, [03/08]: Training Loss: 4.180240016, Training Accuracy: 5.776\n",
            "Worker 5, [04/08]: Training Loss: 4.039002616, Training Accuracy: 6.992\n",
            "Worker 5, [05/08]: Training Loss: 3.935339385, Training Accuracy: 8.160\n",
            "Worker 5, [06/08]: Training Loss: 3.861395150, Training Accuracy: 9.920\n",
            "Worker 5, [07/08]: Training Loss: 3.773989390, Training Accuracy: 10.768\n",
            "Worker 5, [08/08]: Training Loss: 3.682024914, Training Accuracy: 13.072\n",
            "Time taken for training worker 5: 0:00:23.866997\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 4.596323159, Training Accuracy: 1.856\n",
            "Worker 6, [02/08]: Training Loss: 4.417839853, Training Accuracy: 3.456\n",
            "Worker 6, [03/08]: Training Loss: 4.189829169, Training Accuracy: 5.296\n",
            "Worker 6, [04/08]: Training Loss: 4.058499292, Training Accuracy: 7.072\n",
            "Worker 6, [05/08]: Training Loss: 3.952662896, Training Accuracy: 8.416\n",
            "Worker 6, [06/08]: Training Loss: 3.850601551, Training Accuracy: 9.696\n",
            "Worker 6, [07/08]: Training Loss: 3.774691825, Training Accuracy: 11.440\n",
            "Worker 6, [08/08]: Training Loss: 3.675986137, Training Accuracy: 12.736\n",
            "Time taken for training worker 6: 0:00:24.635367\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 4.597111459, Training Accuracy: 1.776\n",
            "Worker 7, [02/08]: Training Loss: 4.431320288, Training Accuracy: 3.088\n",
            "Worker 7, [03/08]: Training Loss: 4.191601471, Training Accuracy: 5.392\n",
            "Worker 7, [04/08]: Training Loss: 4.060434809, Training Accuracy: 7.504\n",
            "Worker 7, [05/08]: Training Loss: 3.947571173, Training Accuracy: 8.720\n",
            "Worker 7, [06/08]: Training Loss: 3.843741945, Training Accuracy: 10.128\n",
            "Worker 7, [07/08]: Training Loss: 3.750781504, Training Accuracy: 12.032\n",
            "Worker 7, [08/08]: Training Loss: 3.674174705, Training Accuracy: 13.024\n",
            "Time taken for training worker 7: 0:00:24.612061\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 4.596194569, Training Accuracy: 2.384\n",
            "Worker 8, [02/08]: Training Loss: 4.402072362, Training Accuracy: 4.128\n",
            "Worker 8, [03/08]: Training Loss: 4.166597444, Training Accuracy: 5.920\n",
            "Worker 8, [04/08]: Training Loss: 4.033607962, Training Accuracy: 6.816\n",
            "Worker 8, [05/08]: Training Loss: 3.921058492, Training Accuracy: 8.464\n",
            "Worker 8, [06/08]: Training Loss: 3.808783490, Training Accuracy: 10.416\n",
            "Worker 8, [07/08]: Training Loss: 3.741565678, Training Accuracy: 10.928\n",
            "Worker 8, [08/08]: Training Loss: 3.672399988, Training Accuracy: 12.016\n",
            "Time taken for training worker 8: 0:00:24.187382\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004976\n",
            "Global Update 01: Test Loss: 3.742546993, Test Accuracy: 15.170\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 3.704810160, Training Accuracy: 12.320\n",
            "Worker 1, [02/08]: Training Loss: 3.597267375, Training Accuracy: 13.088\n",
            "Worker 1, [03/08]: Training Loss: 3.530700287, Training Accuracy: 14.544\n",
            "Worker 1, [04/08]: Training Loss: 3.421618817, Training Accuracy: 16.496\n",
            "Worker 1, [05/08]: Training Loss: 3.320863768, Training Accuracy: 18.080\n",
            "Worker 1, [06/08]: Training Loss: 3.241689288, Training Accuracy: 19.136\n",
            "Worker 1, [07/08]: Training Loss: 3.191915213, Training Accuracy: 20.928\n",
            "Worker 1, [08/08]: Training Loss: 3.087926947, Training Accuracy: 22.384\n",
            "Time taken for training worker 1: 0:00:25.053875\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 3.710592589, Training Accuracy: 12.992\n",
            "Worker 2, [02/08]: Training Loss: 3.608339852, Training Accuracy: 13.968\n",
            "Worker 2, [03/08]: Training Loss: 3.501582618, Training Accuracy: 15.712\n",
            "Worker 2, [04/08]: Training Loss: 3.405631832, Training Accuracy: 17.664\n",
            "Worker 2, [05/08]: Training Loss: 3.326116927, Training Accuracy: 18.640\n",
            "Worker 2, [06/08]: Training Loss: 3.252776966, Training Accuracy: 20.176\n",
            "Worker 2, [07/08]: Training Loss: 3.193396826, Training Accuracy: 21.504\n",
            "Worker 2, [08/08]: Training Loss: 3.095234465, Training Accuracy: 22.832\n",
            "Time taken for training worker 2: 0:00:24.485754\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 3.716658220, Training Accuracy: 12.496\n",
            "Worker 3, [02/08]: Training Loss: 3.615131354, Training Accuracy: 14.064\n",
            "Worker 3, [03/08]: Training Loss: 3.518146914, Training Accuracy: 15.376\n",
            "Worker 3, [04/08]: Training Loss: 3.426354737, Training Accuracy: 17.040\n",
            "Worker 3, [05/08]: Training Loss: 3.360237321, Training Accuracy: 17.824\n",
            "Worker 3, [06/08]: Training Loss: 3.286186019, Training Accuracy: 19.056\n",
            "Worker 3, [07/08]: Training Loss: 3.228428488, Training Accuracy: 20.320\n",
            "Worker 3, [08/08]: Training Loss: 3.120998470, Training Accuracy: 22.320\n",
            "Time taken for training worker 3: 0:00:24.346304\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 3.707863387, Training Accuracy: 12.288\n",
            "Worker 4, [02/08]: Training Loss: 3.599852956, Training Accuracy: 13.440\n",
            "Worker 4, [03/08]: Training Loss: 3.509049627, Training Accuracy: 15.120\n",
            "Worker 4, [04/08]: Training Loss: 3.455486553, Training Accuracy: 16.960\n",
            "Worker 4, [05/08]: Training Loss: 3.337393826, Training Accuracy: 18.336\n",
            "Worker 4, [06/08]: Training Loss: 3.254088772, Training Accuracy: 20.224\n",
            "Worker 4, [07/08]: Training Loss: 3.191991176, Training Accuracy: 20.464\n",
            "Worker 4, [08/08]: Training Loss: 3.095488531, Training Accuracy: 22.864\n",
            "Time taken for training worker 4: 0:00:25.235600\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 3.759879735, Training Accuracy: 11.616\n",
            "Worker 5, [02/08]: Training Loss: 3.648829635, Training Accuracy: 13.456\n",
            "Worker 5, [03/08]: Training Loss: 3.529219009, Training Accuracy: 15.136\n",
            "Worker 5, [04/08]: Training Loss: 3.453640454, Training Accuracy: 15.952\n",
            "Worker 5, [05/08]: Training Loss: 3.398514468, Training Accuracy: 17.536\n",
            "Worker 5, [06/08]: Training Loss: 3.284458691, Training Accuracy: 19.328\n",
            "Worker 5, [07/08]: Training Loss: 3.201949815, Training Accuracy: 20.928\n",
            "Worker 5, [08/08]: Training Loss: 3.147346309, Training Accuracy: 21.920\n",
            "Time taken for training worker 5: 0:00:25.285591\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 3.744978087, Training Accuracy: 12.464\n",
            "Worker 6, [02/08]: Training Loss: 3.632396533, Training Accuracy: 13.712\n",
            "Worker 6, [03/08]: Training Loss: 3.529241022, Training Accuracy: 15.056\n",
            "Worker 6, [04/08]: Training Loss: 3.444793171, Training Accuracy: 15.744\n",
            "Worker 6, [05/08]: Training Loss: 3.364717158, Training Accuracy: 17.664\n",
            "Worker 6, [06/08]: Training Loss: 3.266645183, Training Accuracy: 19.232\n",
            "Worker 6, [07/08]: Training Loss: 3.196187915, Training Accuracy: 21.168\n",
            "Worker 6, [08/08]: Training Loss: 3.119236153, Training Accuracy: 21.888\n",
            "Time taken for training worker 6: 0:00:24.645354\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 3.726456742, Training Accuracy: 12.400\n",
            "Worker 7, [02/08]: Training Loss: 3.618082550, Training Accuracy: 13.712\n",
            "Worker 7, [03/08]: Training Loss: 3.518200346, Training Accuracy: 15.408\n",
            "Worker 7, [04/08]: Training Loss: 3.419346902, Training Accuracy: 17.184\n",
            "Worker 7, [05/08]: Training Loss: 3.364710097, Training Accuracy: 17.632\n",
            "Worker 7, [06/08]: Training Loss: 3.277500021, Training Accuracy: 18.784\n",
            "Worker 7, [07/08]: Training Loss: 3.210906649, Training Accuracy: 20.496\n",
            "Worker 7, [08/08]: Training Loss: 3.155283721, Training Accuracy: 21.296\n",
            "Time taken for training worker 7: 0:00:25.489159\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 3.706376647, Training Accuracy: 12.144\n",
            "Worker 8, [02/08]: Training Loss: 3.615162998, Training Accuracy: 13.264\n",
            "Worker 8, [03/08]: Training Loss: 3.518255769, Training Accuracy: 15.136\n",
            "Worker 8, [04/08]: Training Loss: 3.440334116, Training Accuracy: 16.768\n",
            "Worker 8, [05/08]: Training Loss: 3.377211554, Training Accuracy: 16.992\n",
            "Worker 8, [06/08]: Training Loss: 3.271776229, Training Accuracy: 19.008\n",
            "Worker 8, [07/08]: Training Loss: 3.211072652, Training Accuracy: 20.720\n",
            "Worker 8, [08/08]: Training Loss: 3.132649777, Training Accuracy: 21.664\n",
            "Time taken for training worker 8: 0:00:25.404223\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004520\n",
            "Global Update 02: Test Loss: 3.682606579, Test Accuracy: 22.290\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 3.309561912, Training Accuracy: 19.200\n",
            "Worker 1, [02/08]: Training Loss: 3.148440848, Training Accuracy: 22.256\n",
            "Worker 1, [03/08]: Training Loss: 3.058292384, Training Accuracy: 23.472\n",
            "Worker 1, [04/08]: Training Loss: 3.001580674, Training Accuracy: 24.224\n",
            "Worker 1, [05/08]: Training Loss: 2.895227693, Training Accuracy: 26.064\n",
            "Worker 1, [06/08]: Training Loss: 2.826917240, Training Accuracy: 26.992\n",
            "Worker 1, [07/08]: Training Loss: 2.786112780, Training Accuracy: 27.440\n",
            "Worker 1, [08/08]: Training Loss: 2.730062161, Training Accuracy: 28.800\n",
            "Time taken for training worker 1: 0:00:25.918313\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 3.321897431, Training Accuracy: 19.280\n",
            "Worker 2, [02/08]: Training Loss: 3.128495311, Training Accuracy: 22.640\n",
            "Worker 2, [03/08]: Training Loss: 3.052138489, Training Accuracy: 23.152\n",
            "Worker 2, [04/08]: Training Loss: 2.988751005, Training Accuracy: 24.336\n",
            "Worker 2, [05/08]: Training Loss: 2.907719965, Training Accuracy: 26.960\n",
            "Worker 2, [06/08]: Training Loss: 2.856922099, Training Accuracy: 27.088\n",
            "Worker 2, [07/08]: Training Loss: 2.807419166, Training Accuracy: 27.936\n",
            "Worker 2, [08/08]: Training Loss: 2.728383660, Training Accuracy: 29.168\n",
            "Time taken for training worker 2: 0:00:24.860329\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 3.336955494, Training Accuracy: 19.744\n",
            "Worker 3, [02/08]: Training Loss: 3.177636548, Training Accuracy: 22.032\n",
            "Worker 3, [03/08]: Training Loss: 3.099646320, Training Accuracy: 22.240\n",
            "Worker 3, [04/08]: Training Loss: 2.999437013, Training Accuracy: 25.248\n",
            "Worker 3, [05/08]: Training Loss: 2.926789525, Training Accuracy: 25.888\n",
            "Worker 3, [06/08]: Training Loss: 2.871289124, Training Accuracy: 26.000\n",
            "Worker 3, [07/08]: Training Loss: 2.799332169, Training Accuracy: 28.464\n",
            "Worker 3, [08/08]: Training Loss: 2.729731599, Training Accuracy: 29.776\n",
            "Time taken for training worker 3: 0:00:24.951843\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 3.294046237, Training Accuracy: 19.952\n",
            "Worker 4, [02/08]: Training Loss: 3.140733858, Training Accuracy: 21.920\n",
            "Worker 4, [03/08]: Training Loss: 3.039567860, Training Accuracy: 24.144\n",
            "Worker 4, [04/08]: Training Loss: 2.973677642, Training Accuracy: 25.216\n",
            "Worker 4, [05/08]: Training Loss: 2.900370963, Training Accuracy: 26.544\n",
            "Worker 4, [06/08]: Training Loss: 2.831234428, Training Accuracy: 27.168\n",
            "Worker 4, [07/08]: Training Loss: 2.788885372, Training Accuracy: 28.896\n",
            "Worker 4, [08/08]: Training Loss: 2.718373058, Training Accuracy: 29.296\n",
            "Time taken for training worker 4: 0:00:24.799207\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 3.334810471, Training Accuracy: 19.776\n",
            "Worker 5, [02/08]: Training Loss: 3.170178798, Training Accuracy: 21.696\n",
            "Worker 5, [03/08]: Training Loss: 3.064770370, Training Accuracy: 23.760\n",
            "Worker 5, [04/08]: Training Loss: 3.006890477, Training Accuracy: 24.816\n",
            "Worker 5, [05/08]: Training Loss: 2.939314723, Training Accuracy: 25.696\n",
            "Worker 5, [06/08]: Training Loss: 2.866345797, Training Accuracy: 26.640\n",
            "Worker 5, [07/08]: Training Loss: 2.791778124, Training Accuracy: 27.728\n",
            "Worker 5, [08/08]: Training Loss: 2.729866770, Training Accuracy: 29.056\n",
            "Time taken for training worker 5: 0:00:25.300059\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 3.312423764, Training Accuracy: 19.424\n",
            "Worker 6, [02/08]: Training Loss: 3.166069384, Training Accuracy: 21.568\n",
            "Worker 6, [03/08]: Training Loss: 3.071667121, Training Accuracy: 23.168\n",
            "Worker 6, [04/08]: Training Loss: 2.988430062, Training Accuracy: 24.640\n",
            "Worker 6, [05/08]: Training Loss: 2.906069018, Training Accuracy: 26.544\n",
            "Worker 6, [06/08]: Training Loss: 2.864189287, Training Accuracy: 26.608\n",
            "Worker 6, [07/08]: Training Loss: 2.788844089, Training Accuracy: 28.416\n",
            "Worker 6, [08/08]: Training Loss: 2.726507411, Training Accuracy: 28.944\n",
            "Time taken for training worker 6: 0:00:24.756629\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 3.320573758, Training Accuracy: 18.880\n",
            "Worker 7, [02/08]: Training Loss: 3.160733221, Training Accuracy: 21.760\n",
            "Worker 7, [03/08]: Training Loss: 3.086016436, Training Accuracy: 22.848\n",
            "Worker 7, [04/08]: Training Loss: 3.012728723, Training Accuracy: 23.792\n",
            "Worker 7, [05/08]: Training Loss: 2.934342725, Training Accuracy: 25.600\n",
            "Worker 7, [06/08]: Training Loss: 2.893982123, Training Accuracy: 25.856\n",
            "Worker 7, [07/08]: Training Loss: 2.799380298, Training Accuracy: 27.584\n",
            "Worker 7, [08/08]: Training Loss: 2.744439860, Training Accuracy: 29.216\n",
            "Time taken for training worker 7: 0:00:24.146995\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 3.344370443, Training Accuracy: 18.896\n",
            "Worker 8, [02/08]: Training Loss: 3.158067450, Training Accuracy: 21.360\n",
            "Worker 8, [03/08]: Training Loss: 3.062213543, Training Accuracy: 23.312\n",
            "Worker 8, [04/08]: Training Loss: 2.976546832, Training Accuracy: 24.496\n",
            "Worker 8, [05/08]: Training Loss: 2.930741439, Training Accuracy: 25.328\n",
            "Worker 8, [06/08]: Training Loss: 2.874540365, Training Accuracy: 26.192\n",
            "Worker 8, [07/08]: Training Loss: 2.798671827, Training Accuracy: 27.904\n",
            "Worker 8, [08/08]: Training Loss: 2.720953491, Training Accuracy: 29.280\n",
            "Time taken for training worker 8: 0:00:24.525514\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004129\n",
            "Global Update 03: Test Loss: 3.346024342, Test Accuracy: 29.020\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 3.024497655, Training Accuracy: 25.168\n",
            "Worker 1, [02/08]: Training Loss: 2.810541890, Training Accuracy: 28.432\n",
            "Worker 1, [03/08]: Training Loss: 2.734204302, Training Accuracy: 30.160\n",
            "Worker 1, [04/08]: Training Loss: 2.637404420, Training Accuracy: 31.824\n",
            "Worker 1, [05/08]: Training Loss: 2.544234760, Training Accuracy: 33.328\n",
            "Worker 1, [06/08]: Training Loss: 2.511856663, Training Accuracy: 33.808\n",
            "Worker 1, [07/08]: Training Loss: 2.446680080, Training Accuracy: 34.784\n",
            "Worker 1, [08/08]: Training Loss: 2.394895719, Training Accuracy: 35.200\n",
            "Time taken for training worker 1: 0:00:25.373923\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 3.026890847, Training Accuracy: 25.472\n",
            "Worker 2, [02/08]: Training Loss: 2.829435049, Training Accuracy: 28.720\n",
            "Worker 2, [03/08]: Training Loss: 2.714318263, Training Accuracy: 29.584\n",
            "Worker 2, [04/08]: Training Loss: 2.623800465, Training Accuracy: 31.920\n",
            "Worker 2, [05/08]: Training Loss: 2.566205083, Training Accuracy: 32.816\n",
            "Worker 2, [06/08]: Training Loss: 2.474307267, Training Accuracy: 33.984\n",
            "Worker 2, [07/08]: Training Loss: 2.444436315, Training Accuracy: 35.568\n",
            "Worker 2, [08/08]: Training Loss: 2.361157060, Training Accuracy: 37.008\n",
            "Time taken for training worker 2: 0:00:24.042668\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 3.073687067, Training Accuracy: 24.176\n",
            "Worker 3, [02/08]: Training Loss: 2.823760446, Training Accuracy: 28.448\n",
            "Worker 3, [03/08]: Training Loss: 2.735270104, Training Accuracy: 30.160\n",
            "Worker 3, [04/08]: Training Loss: 2.668226563, Training Accuracy: 30.960\n",
            "Worker 3, [05/08]: Training Loss: 2.580183990, Training Accuracy: 32.176\n",
            "Worker 3, [06/08]: Training Loss: 2.527090615, Training Accuracy: 33.264\n",
            "Worker 3, [07/08]: Training Loss: 2.452477051, Training Accuracy: 35.072\n",
            "Worker 3, [08/08]: Training Loss: 2.372232159, Training Accuracy: 36.096\n",
            "Time taken for training worker 3: 0:00:24.057568\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 3.019727989, Training Accuracy: 25.072\n",
            "Worker 4, [02/08]: Training Loss: 2.823405893, Training Accuracy: 27.968\n",
            "Worker 4, [03/08]: Training Loss: 2.746762514, Training Accuracy: 30.064\n",
            "Worker 4, [04/08]: Training Loss: 2.638703261, Training Accuracy: 31.792\n",
            "Worker 4, [05/08]: Training Loss: 2.562793312, Training Accuracy: 32.880\n",
            "Worker 4, [06/08]: Training Loss: 2.525917384, Training Accuracy: 33.776\n",
            "Worker 4, [07/08]: Training Loss: 2.426416301, Training Accuracy: 35.264\n",
            "Worker 4, [08/08]: Training Loss: 2.358312135, Training Accuracy: 37.520\n",
            "Time taken for training worker 4: 0:00:24.114542\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 3.050086778, Training Accuracy: 25.072\n",
            "Worker 5, [02/08]: Training Loss: 2.832875497, Training Accuracy: 28.512\n",
            "Worker 5, [03/08]: Training Loss: 2.745170056, Training Accuracy: 30.048\n",
            "Worker 5, [04/08]: Training Loss: 2.648972214, Training Accuracy: 31.792\n",
            "Worker 5, [05/08]: Training Loss: 2.612703440, Training Accuracy: 31.872\n",
            "Worker 5, [06/08]: Training Loss: 2.536973243, Training Accuracy: 32.880\n",
            "Worker 5, [07/08]: Training Loss: 2.444457226, Training Accuracy: 34.640\n",
            "Worker 5, [08/08]: Training Loss: 2.368207777, Training Accuracy: 36.864\n",
            "Time taken for training worker 5: 0:00:24.399700\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 3.040768636, Training Accuracy: 25.504\n",
            "Worker 6, [02/08]: Training Loss: 2.843364441, Training Accuracy: 27.968\n",
            "Worker 6, [03/08]: Training Loss: 2.731908762, Training Accuracy: 29.520\n",
            "Worker 6, [04/08]: Training Loss: 2.657857180, Training Accuracy: 30.768\n",
            "Worker 6, [05/08]: Training Loss: 2.632895951, Training Accuracy: 31.696\n",
            "Worker 6, [06/08]: Training Loss: 2.545194998, Training Accuracy: 33.424\n",
            "Worker 6, [07/08]: Training Loss: 2.436148141, Training Accuracy: 35.792\n",
            "Worker 6, [08/08]: Training Loss: 2.382379521, Training Accuracy: 36.128\n",
            "Time taken for training worker 6: 0:00:24.719659\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 3.058558644, Training Accuracy: 24.128\n",
            "Worker 7, [02/08]: Training Loss: 2.854457704, Training Accuracy: 27.632\n",
            "Worker 7, [03/08]: Training Loss: 2.761579100, Training Accuracy: 29.696\n",
            "Worker 7, [04/08]: Training Loss: 2.672097870, Training Accuracy: 30.416\n",
            "Worker 7, [05/08]: Training Loss: 2.578626358, Training Accuracy: 32.416\n",
            "Worker 7, [06/08]: Training Loss: 2.532550306, Training Accuracy: 33.344\n",
            "Worker 7, [07/08]: Training Loss: 2.470670019, Training Accuracy: 34.176\n",
            "Worker 7, [08/08]: Training Loss: 2.428100112, Training Accuracy: 35.056\n",
            "Time taken for training worker 7: 0:00:24.376277\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 3.038821026, Training Accuracy: 24.624\n",
            "Worker 8, [02/08]: Training Loss: 2.836085363, Training Accuracy: 28.240\n",
            "Worker 8, [03/08]: Training Loss: 2.746409175, Training Accuracy: 29.088\n",
            "Worker 8, [04/08]: Training Loss: 2.669185809, Training Accuracy: 31.152\n",
            "Worker 8, [05/08]: Training Loss: 2.599687253, Training Accuracy: 31.856\n",
            "Worker 8, [06/08]: Training Loss: 2.552134353, Training Accuracy: 32.656\n",
            "Worker 8, [07/08]: Training Loss: 2.502526601, Training Accuracy: 33.488\n",
            "Worker 8, [08/08]: Training Loss: 2.405119597, Training Accuracy: 35.552\n",
            "Time taken for training worker 8: 0:00:24.937689\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004457\n",
            "Global Update 04: Test Loss: 2.709143277, Test Accuracy: 33.760\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.733907449, Training Accuracy: 31.344\n",
            "Worker 1, [02/08]: Training Loss: 2.584133876, Training Accuracy: 33.440\n",
            "Worker 1, [03/08]: Training Loss: 2.450442338, Training Accuracy: 35.184\n",
            "Worker 1, [04/08]: Training Loss: 2.344298462, Training Accuracy: 38.272\n",
            "Worker 1, [05/08]: Training Loss: 2.279934676, Training Accuracy: 39.264\n",
            "Worker 1, [06/08]: Training Loss: 2.244564421, Training Accuracy: 39.232\n",
            "Worker 1, [07/08]: Training Loss: 2.125821088, Training Accuracy: 41.712\n",
            "Worker 1, [08/08]: Training Loss: 2.089687523, Training Accuracy: 42.432\n",
            "Time taken for training worker 1: 0:00:23.743575\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.741183709, Training Accuracy: 31.296\n",
            "Worker 2, [02/08]: Training Loss: 2.556441198, Training Accuracy: 33.808\n",
            "Worker 2, [03/08]: Training Loss: 2.429193086, Training Accuracy: 36.208\n",
            "Worker 2, [04/08]: Training Loss: 2.369526671, Training Accuracy: 37.152\n",
            "Worker 2, [05/08]: Training Loss: 2.295926465, Training Accuracy: 38.544\n",
            "Worker 2, [06/08]: Training Loss: 2.202056722, Training Accuracy: 40.304\n",
            "Worker 2, [07/08]: Training Loss: 2.147479006, Training Accuracy: 41.568\n",
            "Worker 2, [08/08]: Training Loss: 2.096108134, Training Accuracy: 41.744\n",
            "Time taken for training worker 2: 0:00:24.517885\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.742536820, Training Accuracy: 30.752\n",
            "Worker 3, [02/08]: Training Loss: 2.581352300, Training Accuracy: 33.248\n",
            "Worker 3, [03/08]: Training Loss: 2.474742909, Training Accuracy: 34.816\n",
            "Worker 3, [04/08]: Training Loss: 2.367851464, Training Accuracy: 36.976\n",
            "Worker 3, [05/08]: Training Loss: 2.285360622, Training Accuracy: 38.688\n",
            "Worker 3, [06/08]: Training Loss: 2.233729437, Training Accuracy: 39.712\n",
            "Worker 3, [07/08]: Training Loss: 2.133835127, Training Accuracy: 42.048\n",
            "Worker 3, [08/08]: Training Loss: 2.091617228, Training Accuracy: 42.240\n",
            "Time taken for training worker 3: 0:00:24.896251\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.749497326, Training Accuracy: 30.144\n",
            "Worker 4, [02/08]: Training Loss: 2.563143112, Training Accuracy: 33.088\n",
            "Worker 4, [03/08]: Training Loss: 2.454728185, Training Accuracy: 35.456\n",
            "Worker 4, [04/08]: Training Loss: 2.368742276, Training Accuracy: 37.504\n",
            "Worker 4, [05/08]: Training Loss: 2.256658171, Training Accuracy: 38.896\n",
            "Worker 4, [06/08]: Training Loss: 2.199782936, Training Accuracy: 40.928\n",
            "Worker 4, [07/08]: Training Loss: 2.149854886, Training Accuracy: 41.328\n",
            "Worker 4, [08/08]: Training Loss: 2.065441171, Training Accuracy: 43.248\n",
            "Time taken for training worker 4: 0:00:24.537049\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 2.764161086, Training Accuracy: 29.888\n",
            "Worker 5, [02/08]: Training Loss: 2.577089453, Training Accuracy: 32.592\n",
            "Worker 5, [03/08]: Training Loss: 2.471116256, Training Accuracy: 34.928\n",
            "Worker 5, [04/08]: Training Loss: 2.383103296, Training Accuracy: 37.360\n",
            "Worker 5, [05/08]: Training Loss: 2.298447221, Training Accuracy: 38.816\n",
            "Worker 5, [06/08]: Training Loss: 2.185809995, Training Accuracy: 41.200\n",
            "Worker 5, [07/08]: Training Loss: 2.127798853, Training Accuracy: 42.368\n",
            "Worker 5, [08/08]: Training Loss: 2.053942898, Training Accuracy: 43.792\n",
            "Time taken for training worker 5: 0:00:25.092861\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 2.757133484, Training Accuracy: 30.224\n",
            "Worker 6, [02/08]: Training Loss: 2.582021073, Training Accuracy: 33.008\n",
            "Worker 6, [03/08]: Training Loss: 2.460141867, Training Accuracy: 35.312\n",
            "Worker 6, [04/08]: Training Loss: 2.375656231, Training Accuracy: 37.088\n",
            "Worker 6, [05/08]: Training Loss: 2.293658446, Training Accuracy: 38.192\n",
            "Worker 6, [06/08]: Training Loss: 2.224777227, Training Accuracy: 39.264\n",
            "Worker 6, [07/08]: Training Loss: 2.155973503, Training Accuracy: 41.184\n",
            "Worker 6, [08/08]: Training Loss: 2.086575102, Training Accuracy: 42.480\n",
            "Time taken for training worker 6: 0:00:25.910529\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 2.762584966, Training Accuracy: 30.416\n",
            "Worker 7, [02/08]: Training Loss: 2.602181880, Training Accuracy: 32.528\n",
            "Worker 7, [03/08]: Training Loss: 2.471968893, Training Accuracy: 35.280\n",
            "Worker 7, [04/08]: Training Loss: 2.391387720, Training Accuracy: 35.968\n",
            "Worker 7, [05/08]: Training Loss: 2.314484863, Training Accuracy: 37.712\n",
            "Worker 7, [06/08]: Training Loss: 2.234587939, Training Accuracy: 39.440\n",
            "Worker 7, [07/08]: Training Loss: 2.159415709, Training Accuracy: 41.456\n",
            "Worker 7, [08/08]: Training Loss: 2.117391625, Training Accuracy: 41.968\n",
            "Time taken for training worker 7: 0:00:25.723427\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.737881785, Training Accuracy: 30.560\n",
            "Worker 8, [02/08]: Training Loss: 2.579044404, Training Accuracy: 33.152\n",
            "Worker 8, [03/08]: Training Loss: 2.480917471, Training Accuracy: 35.312\n",
            "Worker 8, [04/08]: Training Loss: 2.366216452, Training Accuracy: 36.912\n",
            "Worker 8, [05/08]: Training Loss: 2.320720510, Training Accuracy: 37.680\n",
            "Worker 8, [06/08]: Training Loss: 2.238591594, Training Accuracy: 39.440\n",
            "Worker 8, [07/08]: Training Loss: 2.169592797, Training Accuracy: 40.960\n",
            "Worker 8, [08/08]: Training Loss: 2.079197777, Training Accuracy: 42.736\n",
            "Time taken for training worker 8: 0:00:23.948925\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004094\n",
            "Global Update 05: Test Loss: 2.449650498, Test Accuracy: 38.100\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.532447027, Training Accuracy: 34.752\n",
            "Worker 1, [02/08]: Training Loss: 2.319925747, Training Accuracy: 38.656\n",
            "Worker 1, [03/08]: Training Loss: 2.221301418, Training Accuracy: 40.272\n",
            "Worker 1, [04/08]: Training Loss: 2.123119399, Training Accuracy: 42.240\n",
            "Worker 1, [05/08]: Training Loss: 2.018383153, Training Accuracy: 44.560\n",
            "Worker 1, [06/08]: Training Loss: 1.951831781, Training Accuracy: 46.416\n",
            "Worker 1, [07/08]: Training Loss: 1.855405700, Training Accuracy: 48.016\n",
            "Worker 1, [08/08]: Training Loss: 1.855945913, Training Accuracy: 48.384\n",
            "Time taken for training worker 1: 0:00:24.923565\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.534934900, Training Accuracy: 34.832\n",
            "Worker 2, [02/08]: Training Loss: 2.330709879, Training Accuracy: 39.056\n",
            "Worker 2, [03/08]: Training Loss: 2.234012122, Training Accuracy: 40.128\n",
            "Worker 2, [04/08]: Training Loss: 2.119117371, Training Accuracy: 42.928\n",
            "Worker 2, [05/08]: Training Loss: 2.046565765, Training Accuracy: 44.160\n",
            "Worker 2, [06/08]: Training Loss: 1.977448260, Training Accuracy: 45.936\n",
            "Worker 2, [07/08]: Training Loss: 1.921113571, Training Accuracy: 46.272\n",
            "Worker 2, [08/08]: Training Loss: 1.837644496, Training Accuracy: 48.240\n",
            "Time taken for training worker 2: 0:00:24.006283\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.530276607, Training Accuracy: 34.288\n",
            "Worker 3, [02/08]: Training Loss: 2.350222339, Training Accuracy: 38.192\n",
            "Worker 3, [03/08]: Training Loss: 2.221825343, Training Accuracy: 40.592\n",
            "Worker 3, [04/08]: Training Loss: 2.144649040, Training Accuracy: 42.320\n",
            "Worker 3, [05/08]: Training Loss: 2.055911354, Training Accuracy: 44.944\n",
            "Worker 3, [06/08]: Training Loss: 1.966459516, Training Accuracy: 45.168\n",
            "Worker 3, [07/08]: Training Loss: 1.922728120, Training Accuracy: 47.040\n",
            "Worker 3, [08/08]: Training Loss: 1.822721976, Training Accuracy: 49.104\n",
            "Time taken for training worker 3: 0:00:24.603354\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.535363231, Training Accuracy: 34.352\n",
            "Worker 4, [02/08]: Training Loss: 2.346732069, Training Accuracy: 37.488\n",
            "Worker 4, [03/08]: Training Loss: 2.220472916, Training Accuracy: 40.656\n",
            "Worker 4, [04/08]: Training Loss: 2.122981383, Training Accuracy: 41.680\n",
            "Worker 4, [05/08]: Training Loss: 2.052000215, Training Accuracy: 43.568\n",
            "Worker 4, [06/08]: Training Loss: 1.972637031, Training Accuracy: 45.568\n",
            "Worker 4, [07/08]: Training Loss: 1.903248853, Training Accuracy: 46.912\n",
            "Worker 4, [08/08]: Training Loss: 1.818960391, Training Accuracy: 49.504\n",
            "Time taken for training worker 4: 0:00:24.911071\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 2.528438833, Training Accuracy: 35.552\n",
            "Worker 5, [02/08]: Training Loss: 2.369125540, Training Accuracy: 37.728\n",
            "Worker 5, [03/08]: Training Loss: 2.216479416, Training Accuracy: 40.448\n",
            "Worker 5, [04/08]: Training Loss: 2.136435022, Training Accuracy: 42.528\n",
            "Worker 5, [05/08]: Training Loss: 2.025374162, Training Accuracy: 44.336\n",
            "Worker 5, [06/08]: Training Loss: 1.983559909, Training Accuracy: 45.376\n",
            "Worker 5, [07/08]: Training Loss: 1.919098607, Training Accuracy: 46.592\n",
            "Worker 5, [08/08]: Training Loss: 1.842659218, Training Accuracy: 47.824\n",
            "Time taken for training worker 5: 0:00:25.334080\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 2.539283638, Training Accuracy: 35.008\n",
            "Worker 6, [02/08]: Training Loss: 2.358251834, Training Accuracy: 38.080\n",
            "Worker 6, [03/08]: Training Loss: 2.249701406, Training Accuracy: 40.144\n",
            "Worker 6, [04/08]: Training Loss: 2.150184276, Training Accuracy: 41.616\n",
            "Worker 6, [05/08]: Training Loss: 2.057396372, Training Accuracy: 43.984\n",
            "Worker 6, [06/08]: Training Loss: 1.988490808, Training Accuracy: 45.296\n",
            "Worker 6, [07/08]: Training Loss: 1.907170509, Training Accuracy: 47.280\n",
            "Worker 6, [08/08]: Training Loss: 1.840796884, Training Accuracy: 48.464\n",
            "Time taken for training worker 6: 0:00:25.207764\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 2.558771666, Training Accuracy: 34.384\n",
            "Worker 7, [02/08]: Training Loss: 2.350246589, Training Accuracy: 37.168\n",
            "Worker 7, [03/08]: Training Loss: 2.256494886, Training Accuracy: 39.440\n",
            "Worker 7, [04/08]: Training Loss: 2.147647147, Training Accuracy: 41.456\n",
            "Worker 7, [05/08]: Training Loss: 2.050585753, Training Accuracy: 43.472\n",
            "Worker 7, [06/08]: Training Loss: 1.974336292, Training Accuracy: 45.440\n",
            "Worker 7, [07/08]: Training Loss: 1.953891650, Training Accuracy: 45.440\n",
            "Worker 7, [08/08]: Training Loss: 1.835011998, Training Accuracy: 48.640\n",
            "Time taken for training worker 7: 0:00:24.351168\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.546333989, Training Accuracy: 34.544\n",
            "Worker 8, [02/08]: Training Loss: 2.346393544, Training Accuracy: 38.176\n",
            "Worker 8, [03/08]: Training Loss: 2.208536071, Training Accuracy: 39.856\n",
            "Worker 8, [04/08]: Training Loss: 2.151793336, Training Accuracy: 42.528\n",
            "Worker 8, [05/08]: Training Loss: 2.075856537, Training Accuracy: 43.184\n",
            "Worker 8, [06/08]: Training Loss: 1.982381456, Training Accuracy: 45.776\n",
            "Worker 8, [07/08]: Training Loss: 1.901721412, Training Accuracy: 47.632\n",
            "Worker 8, [08/08]: Training Loss: 1.865644676, Training Accuracy: 47.920\n",
            "Time taken for training worker 8: 0:00:24.594561\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004032\n",
            "Global Update 06: Test Loss: 2.329339544, Test Accuracy: 40.910\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.345447683, Training Accuracy: 38.320\n",
            "Worker 1, [02/08]: Training Loss: 2.136403385, Training Accuracy: 42.384\n",
            "Worker 1, [03/08]: Training Loss: 2.011195475, Training Accuracy: 44.624\n",
            "Worker 1, [04/08]: Training Loss: 1.903755952, Training Accuracy: 47.344\n",
            "Worker 1, [05/08]: Training Loss: 1.849497810, Training Accuracy: 48.816\n",
            "Worker 1, [06/08]: Training Loss: 1.723626597, Training Accuracy: 50.768\n",
            "Worker 1, [07/08]: Training Loss: 1.681725945, Training Accuracy: 52.112\n",
            "Worker 1, [08/08]: Training Loss: 1.616991455, Training Accuracy: 54.000\n",
            "Time taken for training worker 1: 0:00:25.404755\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.361405376, Training Accuracy: 38.160\n",
            "Worker 2, [02/08]: Training Loss: 2.154665199, Training Accuracy: 42.432\n",
            "Worker 2, [03/08]: Training Loss: 2.017727162, Training Accuracy: 44.976\n",
            "Worker 2, [04/08]: Training Loss: 1.926951728, Training Accuracy: 46.608\n",
            "Worker 2, [05/08]: Training Loss: 1.844088529, Training Accuracy: 48.800\n",
            "Worker 2, [06/08]: Training Loss: 1.751129992, Training Accuracy: 51.168\n",
            "Worker 2, [07/08]: Training Loss: 1.683527505, Training Accuracy: 52.048\n",
            "Worker 2, [08/08]: Training Loss: 1.652549499, Training Accuracy: 53.296\n",
            "Time taken for training worker 2: 0:00:25.525128\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.350072268, Training Accuracy: 38.224\n",
            "Worker 3, [02/08]: Training Loss: 2.157436005, Training Accuracy: 42.112\n",
            "Worker 3, [03/08]: Training Loss: 2.018476417, Training Accuracy: 45.152\n",
            "Worker 3, [04/08]: Training Loss: 1.913571690, Training Accuracy: 47.456\n",
            "Worker 3, [05/08]: Training Loss: 1.855885084, Training Accuracy: 48.192\n",
            "Worker 3, [06/08]: Training Loss: 1.770144845, Training Accuracy: 50.656\n",
            "Worker 3, [07/08]: Training Loss: 1.708245922, Training Accuracy: 51.840\n",
            "Worker 3, [08/08]: Training Loss: 1.635091957, Training Accuracy: 52.912\n",
            "Time taken for training worker 3: 0:00:24.020818\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.366098783, Training Accuracy: 38.672\n",
            "Worker 4, [02/08]: Training Loss: 2.148974413, Training Accuracy: 42.704\n",
            "Worker 4, [03/08]: Training Loss: 2.029911187, Training Accuracy: 44.544\n",
            "Worker 4, [04/08]: Training Loss: 1.918464303, Training Accuracy: 47.088\n",
            "Worker 4, [05/08]: Training Loss: 1.824875750, Training Accuracy: 49.296\n",
            "Worker 4, [06/08]: Training Loss: 1.742911456, Training Accuracy: 50.832\n",
            "Worker 4, [07/08]: Training Loss: 1.700385019, Training Accuracy: 51.712\n",
            "Worker 4, [08/08]: Training Loss: 1.620066878, Training Accuracy: 53.760\n",
            "Time taken for training worker 4: 0:00:24.335774\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 2.354732485, Training Accuracy: 38.128\n",
            "Worker 5, [02/08]: Training Loss: 2.135433249, Training Accuracy: 42.256\n",
            "Worker 5, [03/08]: Training Loss: 2.001888010, Training Accuracy: 45.136\n",
            "Worker 5, [04/08]: Training Loss: 1.882144398, Training Accuracy: 47.024\n",
            "Worker 5, [05/08]: Training Loss: 1.847435204, Training Accuracy: 48.112\n",
            "Worker 5, [06/08]: Training Loss: 1.763322025, Training Accuracy: 50.928\n",
            "Worker 5, [07/08]: Training Loss: 1.656346187, Training Accuracy: 52.768\n",
            "Worker 5, [08/08]: Training Loss: 1.580680365, Training Accuracy: 55.024\n",
            "Time taken for training worker 5: 0:00:25.538043\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 2.379966808, Training Accuracy: 37.664\n",
            "Worker 6, [02/08]: Training Loss: 2.184934705, Training Accuracy: 41.136\n",
            "Worker 6, [03/08]: Training Loss: 2.056927675, Training Accuracy: 44.304\n",
            "Worker 6, [04/08]: Training Loss: 1.962887268, Training Accuracy: 46.512\n",
            "Worker 6, [05/08]: Training Loss: 1.837657179, Training Accuracy: 48.880\n",
            "Worker 6, [06/08]: Training Loss: 1.810379461, Training Accuracy: 49.280\n",
            "Worker 6, [07/08]: Training Loss: 1.684504140, Training Accuracy: 52.480\n",
            "Worker 6, [08/08]: Training Loss: 1.640818359, Training Accuracy: 53.856\n",
            "Time taken for training worker 6: 0:00:24.979347\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 2.382912547, Training Accuracy: 37.536\n",
            "Worker 7, [02/08]: Training Loss: 2.185932561, Training Accuracy: 41.904\n",
            "Worker 7, [03/08]: Training Loss: 2.058773426, Training Accuracy: 43.824\n",
            "Worker 7, [04/08]: Training Loss: 1.948445715, Training Accuracy: 46.832\n",
            "Worker 7, [05/08]: Training Loss: 1.842230271, Training Accuracy: 48.592\n",
            "Worker 7, [06/08]: Training Loss: 1.810429966, Training Accuracy: 49.648\n",
            "Worker 7, [07/08]: Training Loss: 1.723932472, Training Accuracy: 52.000\n",
            "Worker 7, [08/08]: Training Loss: 1.650180985, Training Accuracy: 53.264\n",
            "Time taken for training worker 7: 0:00:24.081357\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.378254852, Training Accuracy: 37.904\n",
            "Worker 8, [02/08]: Training Loss: 2.170186661, Training Accuracy: 41.472\n",
            "Worker 8, [03/08]: Training Loss: 2.056237487, Training Accuracy: 44.112\n",
            "Worker 8, [04/08]: Training Loss: 1.930559618, Training Accuracy: 46.544\n",
            "Worker 8, [05/08]: Training Loss: 1.848227281, Training Accuracy: 48.224\n",
            "Worker 8, [06/08]: Training Loss: 1.802626649, Training Accuracy: 50.112\n",
            "Worker 8, [07/08]: Training Loss: 1.725059256, Training Accuracy: 51.504\n",
            "Worker 8, [08/08]: Training Loss: 1.627311542, Training Accuracy: 54.032\n",
            "Time taken for training worker 8: 0:00:23.927137\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004346\n",
            "Global Update 07: Test Loss: 2.287915893, Test Accuracy: 42.310\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.201071579, Training Accuracy: 41.168\n",
            "Worker 1, [02/08]: Training Loss: 1.996427507, Training Accuracy: 44.800\n",
            "Worker 1, [03/08]: Training Loss: 1.831274489, Training Accuracy: 49.088\n",
            "Worker 1, [04/08]: Training Loss: 1.704398823, Training Accuracy: 51.920\n",
            "Worker 1, [05/08]: Training Loss: 1.643607393, Training Accuracy: 52.832\n",
            "Worker 1, [06/08]: Training Loss: 1.530177205, Training Accuracy: 56.032\n",
            "Worker 1, [07/08]: Training Loss: 1.477319365, Training Accuracy: 57.776\n",
            "Worker 1, [08/08]: Training Loss: 1.419736918, Training Accuracy: 58.656\n",
            "Time taken for training worker 1: 0:00:24.828297\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.221161882, Training Accuracy: 40.784\n",
            "Worker 2, [02/08]: Training Loss: 1.981771836, Training Accuracy: 46.256\n",
            "Worker 2, [03/08]: Training Loss: 1.837250819, Training Accuracy: 48.560\n",
            "Worker 2, [04/08]: Training Loss: 1.758368838, Training Accuracy: 51.024\n",
            "Worker 2, [05/08]: Training Loss: 1.663003806, Training Accuracy: 53.808\n",
            "Worker 2, [06/08]: Training Loss: 1.575550606, Training Accuracy: 54.992\n",
            "Worker 2, [07/08]: Training Loss: 1.509889649, Training Accuracy: 56.496\n",
            "Worker 2, [08/08]: Training Loss: 1.413143547, Training Accuracy: 59.456\n",
            "Time taken for training worker 2: 0:00:24.918025\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.229772210, Training Accuracy: 41.152\n",
            "Worker 3, [02/08]: Training Loss: 1.988774929, Training Accuracy: 45.792\n",
            "Worker 3, [03/08]: Training Loss: 1.825750330, Training Accuracy: 49.840\n",
            "Worker 3, [04/08]: Training Loss: 1.727039251, Training Accuracy: 51.280\n",
            "Worker 3, [05/08]: Training Loss: 1.657608896, Training Accuracy: 53.024\n",
            "Worker 3, [06/08]: Training Loss: 1.571285468, Training Accuracy: 56.288\n",
            "Worker 3, [07/08]: Training Loss: 1.505670196, Training Accuracy: 57.232\n",
            "Worker 3, [08/08]: Training Loss: 1.421320036, Training Accuracy: 58.976\n",
            "Time taken for training worker 3: 0:00:24.483856\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.206301186, Training Accuracy: 41.840\n",
            "Worker 4, [02/08]: Training Loss: 1.970080179, Training Accuracy: 46.000\n",
            "Worker 4, [03/08]: Training Loss: 1.851696618, Training Accuracy: 48.720\n",
            "Worker 4, [04/08]: Training Loss: 1.746205904, Training Accuracy: 50.896\n",
            "Worker 4, [05/08]: Training Loss: 1.643271567, Training Accuracy: 53.712\n",
            "Worker 4, [06/08]: Training Loss: 1.559362312, Training Accuracy: 55.536\n",
            "Worker 4, [07/08]: Training Loss: 1.489949908, Training Accuracy: 56.480\n",
            "Worker 4, [08/08]: Training Loss: 1.450715624, Training Accuracy: 58.480\n",
            "Time taken for training worker 4: 0:00:24.377611\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 2.213685852, Training Accuracy: 41.440\n",
            "Worker 5, [02/08]: Training Loss: 1.973531430, Training Accuracy: 46.160\n",
            "Worker 5, [03/08]: Training Loss: 1.845393199, Training Accuracy: 49.232\n",
            "Worker 5, [04/08]: Training Loss: 1.724796829, Training Accuracy: 51.904\n",
            "Worker 5, [05/08]: Training Loss: 1.639042611, Training Accuracy: 53.248\n",
            "Worker 5, [06/08]: Training Loss: 1.561716176, Training Accuracy: 55.120\n",
            "Worker 5, [07/08]: Training Loss: 1.497567166, Training Accuracy: 56.896\n",
            "Worker 5, [08/08]: Training Loss: 1.415776661, Training Accuracy: 58.640\n",
            "Time taken for training worker 5: 0:00:25.272039\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 2.242452971, Training Accuracy: 40.448\n",
            "Worker 6, [02/08]: Training Loss: 2.014825960, Training Accuracy: 45.280\n",
            "Worker 6, [03/08]: Training Loss: 1.884267372, Training Accuracy: 47.840\n",
            "Worker 6, [04/08]: Training Loss: 1.763532241, Training Accuracy: 51.120\n",
            "Worker 6, [05/08]: Training Loss: 1.670346681, Training Accuracy: 52.960\n",
            "Worker 6, [06/08]: Training Loss: 1.561388648, Training Accuracy: 55.936\n",
            "Worker 6, [07/08]: Training Loss: 1.503361443, Training Accuracy: 56.896\n",
            "Worker 6, [08/08]: Training Loss: 1.462616286, Training Accuracy: 58.080\n",
            "Time taken for training worker 6: 0:00:24.579497\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 2.238071162, Training Accuracy: 41.360\n",
            "Worker 7, [02/08]: Training Loss: 2.002620246, Training Accuracy: 45.760\n",
            "Worker 7, [03/08]: Training Loss: 1.884809280, Training Accuracy: 47.616\n",
            "Worker 7, [04/08]: Training Loss: 1.755577619, Training Accuracy: 50.704\n",
            "Worker 7, [05/08]: Training Loss: 1.659233063, Training Accuracy: 52.576\n",
            "Worker 7, [06/08]: Training Loss: 1.566088832, Training Accuracy: 55.712\n",
            "Worker 7, [07/08]: Training Loss: 1.482065891, Training Accuracy: 57.856\n",
            "Worker 7, [08/08]: Training Loss: 1.463189204, Training Accuracy: 57.872\n",
            "Time taken for training worker 7: 0:00:24.118383\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.231789125, Training Accuracy: 40.592\n",
            "Worker 8, [02/08]: Training Loss: 2.039223190, Training Accuracy: 44.736\n",
            "Worker 8, [03/08]: Training Loss: 1.854315321, Training Accuracy: 49.152\n",
            "Worker 8, [04/08]: Training Loss: 1.775071987, Training Accuracy: 50.304\n",
            "Worker 8, [05/08]: Training Loss: 1.674391865, Training Accuracy: 52.880\n",
            "Worker 8, [06/08]: Training Loss: 1.581059542, Training Accuracy: 55.504\n",
            "Worker 8, [07/08]: Training Loss: 1.500129394, Training Accuracy: 56.912\n",
            "Worker 8, [08/08]: Training Loss: 1.465224865, Training Accuracy: 58.128\n",
            "Time taken for training worker 8: 0:00:25.686965\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003814\n",
            "Global Update 08: Test Loss: 2.269216580, Test Accuracy: 44.020\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.072499008, Training Accuracy: 44.128\n",
            "Worker 1, [02/08]: Training Loss: 1.820108558, Training Accuracy: 49.040\n",
            "Worker 1, [03/08]: Training Loss: 1.659026416, Training Accuracy: 53.280\n",
            "Worker 1, [04/08]: Training Loss: 1.546548809, Training Accuracy: 56.176\n",
            "Worker 1, [05/08]: Training Loss: 1.454152372, Training Accuracy: 58.656\n",
            "Worker 1, [06/08]: Training Loss: 1.393784774, Training Accuracy: 60.176\n",
            "Worker 1, [07/08]: Training Loss: 1.323580841, Training Accuracy: 61.168\n",
            "Worker 1, [08/08]: Training Loss: 1.262378302, Training Accuracy: 63.024\n",
            "Time taken for training worker 1: 0:00:24.527837\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.069431065, Training Accuracy: 45.024\n",
            "Worker 2, [02/08]: Training Loss: 1.857028601, Training Accuracy: 49.360\n",
            "Worker 2, [03/08]: Training Loss: 1.709808184, Training Accuracy: 52.352\n",
            "Worker 2, [04/08]: Training Loss: 1.560586927, Training Accuracy: 55.312\n",
            "Worker 2, [05/08]: Training Loss: 1.489924309, Training Accuracy: 58.304\n",
            "Worker 2, [06/08]: Training Loss: 1.405127018, Training Accuracy: 59.536\n",
            "Worker 2, [07/08]: Training Loss: 1.335448482, Training Accuracy: 60.912\n",
            "Worker 2, [08/08]: Training Loss: 1.267002290, Training Accuracy: 62.880\n",
            "Time taken for training worker 2: 0:00:24.363482\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.046704422, Training Accuracy: 44.544\n",
            "Worker 3, [02/08]: Training Loss: 1.837098418, Training Accuracy: 48.544\n",
            "Worker 3, [03/08]: Training Loss: 1.682738450, Training Accuracy: 53.168\n",
            "Worker 3, [04/08]: Training Loss: 1.596904519, Training Accuracy: 54.656\n",
            "Worker 3, [05/08]: Training Loss: 1.484651557, Training Accuracy: 57.600\n",
            "Worker 3, [06/08]: Training Loss: 1.400429721, Training Accuracy: 60.304\n",
            "Worker 3, [07/08]: Training Loss: 1.371297049, Training Accuracy: 60.064\n",
            "Worker 3, [08/08]: Training Loss: 1.278657956, Training Accuracy: 62.464\n",
            "Time taken for training worker 3: 0:00:24.690194\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.090055061, Training Accuracy: 44.576\n",
            "Worker 4, [02/08]: Training Loss: 1.815598955, Training Accuracy: 49.824\n",
            "Worker 4, [03/08]: Training Loss: 1.681783446, Training Accuracy: 52.128\n",
            "Worker 4, [04/08]: Training Loss: 1.561700606, Training Accuracy: 56.000\n",
            "Worker 4, [05/08]: Training Loss: 1.497653849, Training Accuracy: 57.456\n",
            "Worker 4, [06/08]: Training Loss: 1.391911148, Training Accuracy: 60.464\n",
            "Worker 4, [07/08]: Training Loss: 1.331921972, Training Accuracy: 61.072\n",
            "Worker 4, [08/08]: Training Loss: 1.264362193, Training Accuracy: 63.104\n",
            "Time taken for training worker 4: 0:00:24.046279\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 2.071744932, Training Accuracy: 43.680\n",
            "Worker 5, [02/08]: Training Loss: 1.821533497, Training Accuracy: 49.888\n",
            "Worker 5, [03/08]: Training Loss: 1.693512436, Training Accuracy: 53.056\n",
            "Worker 5, [04/08]: Training Loss: 1.589079511, Training Accuracy: 55.040\n",
            "Worker 5, [05/08]: Training Loss: 1.463997068, Training Accuracy: 57.680\n",
            "Worker 5, [06/08]: Training Loss: 1.414022942, Training Accuracy: 59.344\n",
            "Worker 5, [07/08]: Training Loss: 1.315983437, Training Accuracy: 62.464\n",
            "Worker 5, [08/08]: Training Loss: 1.256248201, Training Accuracy: 63.136\n",
            "Time taken for training worker 5: 0:00:24.837448\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 2.107048515, Training Accuracy: 43.280\n",
            "Worker 6, [02/08]: Training Loss: 1.841039305, Training Accuracy: 48.768\n",
            "Worker 6, [03/08]: Training Loss: 1.709583334, Training Accuracy: 52.784\n",
            "Worker 6, [04/08]: Training Loss: 1.589372585, Training Accuracy: 55.264\n",
            "Worker 6, [05/08]: Training Loss: 1.490838190, Training Accuracy: 57.648\n",
            "Worker 6, [06/08]: Training Loss: 1.396444753, Training Accuracy: 60.480\n",
            "Worker 6, [07/08]: Training Loss: 1.370996099, Training Accuracy: 60.336\n",
            "Worker 6, [08/08]: Training Loss: 1.285199888, Training Accuracy: 62.304\n",
            "Time taken for training worker 6: 0:00:25.176472\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 2.077130960, Training Accuracy: 43.648\n",
            "Worker 7, [02/08]: Training Loss: 1.873860690, Training Accuracy: 48.352\n",
            "Worker 7, [03/08]: Training Loss: 1.691132897, Training Accuracy: 52.832\n",
            "Worker 7, [04/08]: Training Loss: 1.593812640, Training Accuracy: 54.256\n",
            "Worker 7, [05/08]: Training Loss: 1.490797085, Training Accuracy: 56.448\n",
            "Worker 7, [06/08]: Training Loss: 1.443970765, Training Accuracy: 58.016\n",
            "Worker 7, [07/08]: Training Loss: 1.339047122, Training Accuracy: 61.536\n",
            "Worker 7, [08/08]: Training Loss: 1.264642509, Training Accuracy: 62.816\n",
            "Time taken for training worker 7: 0:00:25.158903\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.115432231, Training Accuracy: 43.360\n",
            "Worker 8, [02/08]: Training Loss: 1.857616751, Training Accuracy: 49.024\n",
            "Worker 8, [03/08]: Training Loss: 1.691312004, Training Accuracy: 52.064\n",
            "Worker 8, [04/08]: Training Loss: 1.583227022, Training Accuracy: 55.104\n",
            "Worker 8, [05/08]: Training Loss: 1.533506251, Training Accuracy: 56.048\n",
            "Worker 8, [06/08]: Training Loss: 1.420766872, Training Accuracy: 59.664\n",
            "Worker 8, [07/08]: Training Loss: 1.337629429, Training Accuracy: 60.992\n",
            "Worker 8, [08/08]: Training Loss: 1.296487423, Training Accuracy: 63.024\n",
            "Time taken for training worker 8: 0:00:24.577003\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003803\n",
            "Global Update 09: Test Loss: 2.251158945, Test Accuracy: 44.920\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.937027892, Training Accuracy: 46.992\n",
            "Worker 1, [02/08]: Training Loss: 1.690759700, Training Accuracy: 52.528\n",
            "Worker 1, [03/08]: Training Loss: 1.543181112, Training Accuracy: 56.512\n",
            "Worker 1, [04/08]: Training Loss: 1.397128275, Training Accuracy: 60.368\n",
            "Worker 1, [05/08]: Training Loss: 1.283325670, Training Accuracy: 62.624\n",
            "Worker 1, [06/08]: Training Loss: 1.217413858, Training Accuracy: 64.288\n",
            "Worker 1, [07/08]: Training Loss: 1.186471457, Training Accuracy: 65.664\n",
            "Worker 1, [08/08]: Training Loss: 1.133385752, Training Accuracy: 65.984\n",
            "Time taken for training worker 1: 0:00:24.196077\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.953094974, Training Accuracy: 47.072\n",
            "Worker 2, [02/08]: Training Loss: 1.702782765, Training Accuracy: 52.624\n",
            "Worker 2, [03/08]: Training Loss: 1.542433003, Training Accuracy: 56.592\n",
            "Worker 2, [04/08]: Training Loss: 1.440341060, Training Accuracy: 58.800\n",
            "Worker 2, [05/08]: Training Loss: 1.344421806, Training Accuracy: 61.264\n",
            "Worker 2, [06/08]: Training Loss: 1.239085316, Training Accuracy: 64.208\n",
            "Worker 2, [07/08]: Training Loss: 1.170084239, Training Accuracy: 65.536\n",
            "Worker 2, [08/08]: Training Loss: 1.142068961, Training Accuracy: 65.616\n",
            "Time taken for training worker 2: 0:00:25.265764\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.947565586, Training Accuracy: 47.360\n",
            "Worker 3, [02/08]: Training Loss: 1.687489382, Training Accuracy: 53.024\n",
            "Worker 3, [03/08]: Training Loss: 1.569237568, Training Accuracy: 55.616\n",
            "Worker 3, [04/08]: Training Loss: 1.430636681, Training Accuracy: 59.296\n",
            "Worker 3, [05/08]: Training Loss: 1.339811436, Training Accuracy: 61.184\n",
            "Worker 3, [06/08]: Training Loss: 1.267675789, Training Accuracy: 63.200\n",
            "Worker 3, [07/08]: Training Loss: 1.193754463, Training Accuracy: 64.736\n",
            "Worker 3, [08/08]: Training Loss: 1.143481047, Training Accuracy: 66.528\n",
            "Time taken for training worker 3: 0:00:24.677542\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.949714979, Training Accuracy: 46.688\n",
            "Worker 4, [02/08]: Training Loss: 1.719513291, Training Accuracy: 52.128\n",
            "Worker 4, [03/08]: Training Loss: 1.539286863, Training Accuracy: 55.744\n",
            "Worker 4, [04/08]: Training Loss: 1.415268168, Training Accuracy: 59.504\n",
            "Worker 4, [05/08]: Training Loss: 1.355768228, Training Accuracy: 60.800\n",
            "Worker 4, [06/08]: Training Loss: 1.262965104, Training Accuracy: 63.392\n",
            "Worker 4, [07/08]: Training Loss: 1.163853327, Training Accuracy: 66.176\n",
            "Worker 4, [08/08]: Training Loss: 1.146843410, Training Accuracy: 66.304\n",
            "Time taken for training worker 4: 0:00:24.939537\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.963021960, Training Accuracy: 47.088\n",
            "Worker 5, [02/08]: Training Loss: 1.707548402, Training Accuracy: 52.496\n",
            "Worker 5, [03/08]: Training Loss: 1.542692302, Training Accuracy: 56.928\n",
            "Worker 5, [04/08]: Training Loss: 1.419504361, Training Accuracy: 58.816\n",
            "Worker 5, [05/08]: Training Loss: 1.332817031, Training Accuracy: 61.504\n",
            "Worker 5, [06/08]: Training Loss: 1.256806014, Training Accuracy: 63.024\n",
            "Worker 5, [07/08]: Training Loss: 1.164017061, Training Accuracy: 65.456\n",
            "Worker 5, [08/08]: Training Loss: 1.089841487, Training Accuracy: 68.336\n",
            "Time taken for training worker 5: 0:00:24.774350\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.975936410, Training Accuracy: 46.416\n",
            "Worker 6, [02/08]: Training Loss: 1.726380702, Training Accuracy: 52.000\n",
            "Worker 6, [03/08]: Training Loss: 1.548685310, Training Accuracy: 56.176\n",
            "Worker 6, [04/08]: Training Loss: 1.429859890, Training Accuracy: 59.232\n",
            "Worker 6, [05/08]: Training Loss: 1.326810831, Training Accuracy: 61.520\n",
            "Worker 6, [06/08]: Training Loss: 1.270754227, Training Accuracy: 63.344\n",
            "Worker 6, [07/08]: Training Loss: 1.225663798, Training Accuracy: 64.064\n",
            "Worker 6, [08/08]: Training Loss: 1.101725376, Training Accuracy: 67.584\n",
            "Time taken for training worker 6: 0:00:25.035301\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.962520530, Training Accuracy: 47.152\n",
            "Worker 7, [02/08]: Training Loss: 1.705619437, Training Accuracy: 52.432\n",
            "Worker 7, [03/08]: Training Loss: 1.529877683, Training Accuracy: 56.384\n",
            "Worker 7, [04/08]: Training Loss: 1.433759341, Training Accuracy: 59.056\n",
            "Worker 7, [05/08]: Training Loss: 1.332578993, Training Accuracy: 61.392\n",
            "Worker 7, [06/08]: Training Loss: 1.259970396, Training Accuracy: 63.296\n",
            "Worker 7, [07/08]: Training Loss: 1.201669868, Training Accuracy: 64.544\n",
            "Worker 7, [08/08]: Training Loss: 1.135788054, Training Accuracy: 66.480\n",
            "Time taken for training worker 7: 0:00:24.923328\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.005803613, Training Accuracy: 45.712\n",
            "Worker 8, [02/08]: Training Loss: 1.716437049, Training Accuracy: 52.512\n",
            "Worker 8, [03/08]: Training Loss: 1.549619380, Training Accuracy: 56.912\n",
            "Worker 8, [04/08]: Training Loss: 1.462947116, Training Accuracy: 58.736\n",
            "Worker 8, [05/08]: Training Loss: 1.350995208, Training Accuracy: 60.944\n",
            "Worker 8, [06/08]: Training Loss: 1.265255018, Training Accuracy: 62.448\n",
            "Worker 8, [07/08]: Training Loss: 1.210560644, Training Accuracy: 65.136\n",
            "Worker 8, [08/08]: Training Loss: 1.184470545, Training Accuracy: 65.488\n",
            "Time taken for training worker 8: 0:00:24.179154\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003919\n",
            "Global Update 10: Test Loss: 2.252932394, Test Accuracy: 45.690\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.806971076, Training Accuracy: 50.784\n",
            "Worker 1, [02/08]: Training Loss: 1.554185083, Training Accuracy: 56.336\n",
            "Worker 1, [03/08]: Training Loss: 1.395585419, Training Accuracy: 59.696\n",
            "Worker 1, [04/08]: Training Loss: 1.276250513, Training Accuracy: 62.736\n",
            "Worker 1, [05/08]: Training Loss: 1.168258099, Training Accuracy: 65.648\n",
            "Worker 1, [06/08]: Training Loss: 1.110465253, Training Accuracy: 67.184\n",
            "Worker 1, [07/08]: Training Loss: 1.025291197, Training Accuracy: 68.864\n",
            "Worker 1, [08/08]: Training Loss: 1.000573763, Training Accuracy: 69.824\n",
            "Time taken for training worker 1: 0:00:24.338053\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.830881738, Training Accuracy: 50.400\n",
            "Worker 2, [02/08]: Training Loss: 1.577799185, Training Accuracy: 55.408\n",
            "Worker 2, [03/08]: Training Loss: 1.450913693, Training Accuracy: 58.512\n",
            "Worker 2, [04/08]: Training Loss: 1.302324033, Training Accuracy: 62.816\n",
            "Worker 2, [05/08]: Training Loss: 1.194644278, Training Accuracy: 65.008\n",
            "Worker 2, [06/08]: Training Loss: 1.105072421, Training Accuracy: 67.456\n",
            "Worker 2, [07/08]: Training Loss: 1.080187369, Training Accuracy: 68.624\n",
            "Worker 2, [08/08]: Training Loss: 1.000375740, Training Accuracy: 70.768\n",
            "Time taken for training worker 2: 0:00:24.656970\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.855799434, Training Accuracy: 49.472\n",
            "Worker 3, [02/08]: Training Loss: 1.584610797, Training Accuracy: 55.392\n",
            "Worker 3, [03/08]: Training Loss: 1.405813265, Training Accuracy: 60.400\n",
            "Worker 3, [04/08]: Training Loss: 1.309406958, Training Accuracy: 62.688\n",
            "Worker 3, [05/08]: Training Loss: 1.197959776, Training Accuracy: 64.848\n",
            "Worker 3, [06/08]: Training Loss: 1.169259177, Training Accuracy: 65.888\n",
            "Worker 3, [07/08]: Training Loss: 1.055230506, Training Accuracy: 69.488\n",
            "Worker 3, [08/08]: Training Loss: 0.965867222, Training Accuracy: 71.488\n",
            "Time taken for training worker 3: 0:00:24.659989\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.853373605, Training Accuracy: 49.008\n",
            "Worker 4, [02/08]: Training Loss: 1.570143911, Training Accuracy: 56.480\n",
            "Worker 4, [03/08]: Training Loss: 1.412722828, Training Accuracy: 60.128\n",
            "Worker 4, [04/08]: Training Loss: 1.294138191, Training Accuracy: 62.160\n",
            "Worker 4, [05/08]: Training Loss: 1.216462116, Training Accuracy: 64.272\n",
            "Worker 4, [06/08]: Training Loss: 1.145305870, Training Accuracy: 66.800\n",
            "Worker 4, [07/08]: Training Loss: 1.065902440, Training Accuracy: 68.672\n",
            "Worker 4, [08/08]: Training Loss: 1.006802400, Training Accuracy: 70.752\n",
            "Time taken for training worker 4: 0:00:25.021944\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.839027484, Training Accuracy: 49.792\n",
            "Worker 5, [02/08]: Training Loss: 1.587184563, Training Accuracy: 55.840\n",
            "Worker 5, [03/08]: Training Loss: 1.423867150, Training Accuracy: 59.856\n",
            "Worker 5, [04/08]: Training Loss: 1.281664942, Training Accuracy: 63.200\n",
            "Worker 5, [05/08]: Training Loss: 1.181696492, Training Accuracy: 64.992\n",
            "Worker 5, [06/08]: Training Loss: 1.109282068, Training Accuracy: 67.424\n",
            "Worker 5, [07/08]: Training Loss: 1.049084167, Training Accuracy: 69.328\n",
            "Worker 5, [08/08]: Training Loss: 1.017660569, Training Accuracy: 69.408\n",
            "Time taken for training worker 5: 0:00:25.407799\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.858327586, Training Accuracy: 49.040\n",
            "Worker 6, [02/08]: Training Loss: 1.599386626, Training Accuracy: 55.376\n",
            "Worker 6, [03/08]: Training Loss: 1.435027681, Training Accuracy: 59.440\n",
            "Worker 6, [04/08]: Training Loss: 1.318924884, Training Accuracy: 61.904\n",
            "Worker 6, [05/08]: Training Loss: 1.231385134, Training Accuracy: 64.464\n",
            "Worker 6, [06/08]: Training Loss: 1.123096802, Training Accuracy: 67.392\n",
            "Worker 6, [07/08]: Training Loss: 1.085573583, Training Accuracy: 68.096\n",
            "Worker 6, [08/08]: Training Loss: 0.994271056, Training Accuracy: 70.992\n",
            "Time taken for training worker 6: 0:00:23.840284\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.833878146, Training Accuracy: 49.376\n",
            "Worker 7, [02/08]: Training Loss: 1.585594570, Training Accuracy: 55.696\n",
            "Worker 7, [03/08]: Training Loss: 1.444806214, Training Accuracy: 58.224\n",
            "Worker 7, [04/08]: Training Loss: 1.323223662, Training Accuracy: 62.048\n",
            "Worker 7, [05/08]: Training Loss: 1.232582437, Training Accuracy: 64.064\n",
            "Worker 7, [06/08]: Training Loss: 1.121780479, Training Accuracy: 67.264\n",
            "Worker 7, [07/08]: Training Loss: 1.067127916, Training Accuracy: 69.088\n",
            "Worker 7, [08/08]: Training Loss: 0.990417210, Training Accuracy: 70.480\n",
            "Time taken for training worker 7: 0:00:24.749500\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.875322662, Training Accuracy: 49.072\n",
            "Worker 8, [02/08]: Training Loss: 1.589428787, Training Accuracy: 54.816\n",
            "Worker 8, [03/08]: Training Loss: 1.456443840, Training Accuracy: 59.136\n",
            "Worker 8, [04/08]: Training Loss: 1.295031402, Training Accuracy: 63.008\n",
            "Worker 8, [05/08]: Training Loss: 1.223571811, Training Accuracy: 64.928\n",
            "Worker 8, [06/08]: Training Loss: 1.139135704, Training Accuracy: 67.056\n",
            "Worker 8, [07/08]: Training Loss: 1.101850071, Training Accuracy: 67.584\n",
            "Worker 8, [08/08]: Training Loss: 1.003711888, Training Accuracy: 70.208\n",
            "Time taken for training worker 8: 0:00:24.012236\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004342\n",
            "Global Update 11: Test Loss: 2.273260618, Test Accuracy: 46.230\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.691530250, Training Accuracy: 52.592\n",
            "Worker 1, [02/08]: Training Loss: 1.440348069, Training Accuracy: 59.104\n",
            "Worker 1, [03/08]: Training Loss: 1.302346418, Training Accuracy: 62.224\n",
            "Worker 1, [04/08]: Training Loss: 1.159228672, Training Accuracy: 66.480\n",
            "Worker 1, [05/08]: Training Loss: 1.065519233, Training Accuracy: 68.960\n",
            "Worker 1, [06/08]: Training Loss: 0.983405234, Training Accuracy: 71.312\n",
            "Worker 1, [07/08]: Training Loss: 0.970515641, Training Accuracy: 71.120\n",
            "Worker 1, [08/08]: Training Loss: 0.864194594, Training Accuracy: 75.136\n",
            "Time taken for training worker 1: 0:00:23.990287\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.745571833, Training Accuracy: 51.184\n",
            "Worker 2, [02/08]: Training Loss: 1.458350596, Training Accuracy: 58.784\n",
            "Worker 2, [03/08]: Training Loss: 1.305368149, Training Accuracy: 62.272\n",
            "Worker 2, [04/08]: Training Loss: 1.188177753, Training Accuracy: 65.840\n",
            "Worker 2, [05/08]: Training Loss: 1.085415860, Training Accuracy: 68.192\n",
            "Worker 2, [06/08]: Training Loss: 1.016817298, Training Accuracy: 70.656\n",
            "Worker 2, [07/08]: Training Loss: 0.941945112, Training Accuracy: 72.464\n",
            "Worker 2, [08/08]: Training Loss: 0.887193900, Training Accuracy: 73.824\n",
            "Time taken for training worker 2: 0:00:24.288125\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.744014661, Training Accuracy: 51.648\n",
            "Worker 3, [02/08]: Training Loss: 1.456199544, Training Accuracy: 58.368\n",
            "Worker 3, [03/08]: Training Loss: 1.315731220, Training Accuracy: 62.464\n",
            "Worker 3, [04/08]: Training Loss: 1.186336675, Training Accuracy: 66.064\n",
            "Worker 3, [05/08]: Training Loss: 1.093685399, Training Accuracy: 68.096\n",
            "Worker 3, [06/08]: Training Loss: 1.023435839, Training Accuracy: 70.128\n",
            "Worker 3, [07/08]: Training Loss: 0.960726428, Training Accuracy: 70.992\n",
            "Worker 3, [08/08]: Training Loss: 0.901388230, Training Accuracy: 73.504\n",
            "Time taken for training worker 3: 0:00:24.255146\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.733121621, Training Accuracy: 51.888\n",
            "Worker 4, [02/08]: Training Loss: 1.464764947, Training Accuracy: 57.952\n",
            "Worker 4, [03/08]: Training Loss: 1.305895564, Training Accuracy: 62.832\n",
            "Worker 4, [04/08]: Training Loss: 1.212910052, Training Accuracy: 64.864\n",
            "Worker 4, [05/08]: Training Loss: 1.096078039, Training Accuracy: 67.232\n",
            "Worker 4, [06/08]: Training Loss: 1.024713024, Training Accuracy: 69.904\n",
            "Worker 4, [07/08]: Training Loss: 0.974765139, Training Accuracy: 71.824\n",
            "Worker 4, [08/08]: Training Loss: 0.883646605, Training Accuracy: 73.808\n",
            "Time taken for training worker 4: 0:00:25.124765\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.723072279, Training Accuracy: 51.728\n",
            "Worker 5, [02/08]: Training Loss: 1.447344656, Training Accuracy: 58.528\n",
            "Worker 5, [03/08]: Training Loss: 1.295661798, Training Accuracy: 62.896\n",
            "Worker 5, [04/08]: Training Loss: 1.172077861, Training Accuracy: 65.712\n",
            "Worker 5, [05/08]: Training Loss: 1.063071531, Training Accuracy: 69.296\n",
            "Worker 5, [06/08]: Training Loss: 1.003792749, Training Accuracy: 70.640\n",
            "Worker 5, [07/08]: Training Loss: 0.950616251, Training Accuracy: 72.704\n",
            "Worker 5, [08/08]: Training Loss: 0.873987686, Training Accuracy: 73.952\n",
            "Time taken for training worker 5: 0:00:24.796694\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.744800666, Training Accuracy: 51.152\n",
            "Worker 6, [02/08]: Training Loss: 1.481862672, Training Accuracy: 57.984\n",
            "Worker 6, [03/08]: Training Loss: 1.334425111, Training Accuracy: 62.112\n",
            "Worker 6, [04/08]: Training Loss: 1.202245799, Training Accuracy: 65.104\n",
            "Worker 6, [05/08]: Training Loss: 1.115753017, Training Accuracy: 67.584\n",
            "Worker 6, [06/08]: Training Loss: 1.037757456, Training Accuracy: 70.624\n",
            "Worker 6, [07/08]: Training Loss: 0.957187410, Training Accuracy: 72.448\n",
            "Worker 6, [08/08]: Training Loss: 0.904250105, Training Accuracy: 73.600\n",
            "Time taken for training worker 6: 0:00:24.528351\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.746191133, Training Accuracy: 51.760\n",
            "Worker 7, [02/08]: Training Loss: 1.456054363, Training Accuracy: 58.624\n",
            "Worker 7, [03/08]: Training Loss: 1.294379181, Training Accuracy: 62.448\n",
            "Worker 7, [04/08]: Training Loss: 1.187086925, Training Accuracy: 65.280\n",
            "Worker 7, [05/08]: Training Loss: 1.089312871, Training Accuracy: 68.576\n",
            "Worker 7, [06/08]: Training Loss: 1.022313386, Training Accuracy: 70.880\n",
            "Worker 7, [07/08]: Training Loss: 0.951873686, Training Accuracy: 72.528\n",
            "Worker 7, [08/08]: Training Loss: 0.930347590, Training Accuracy: 72.832\n",
            "Time taken for training worker 7: 0:00:24.844656\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.751243779, Training Accuracy: 51.504\n",
            "Worker 8, [02/08]: Training Loss: 1.464801577, Training Accuracy: 58.384\n",
            "Worker 8, [03/08]: Training Loss: 1.327028781, Training Accuracy: 62.128\n",
            "Worker 8, [04/08]: Training Loss: 1.208930068, Training Accuracy: 65.104\n",
            "Worker 8, [05/08]: Training Loss: 1.116671791, Training Accuracy: 67.936\n",
            "Worker 8, [06/08]: Training Loss: 1.052339785, Training Accuracy: 69.744\n",
            "Worker 8, [07/08]: Training Loss: 0.985739820, Training Accuracy: 71.072\n",
            "Worker 8, [08/08]: Training Loss: 0.916852353, Training Accuracy: 72.960\n",
            "Time taken for training worker 8: 0:00:24.596670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.005761\n",
            "Global Update 12: Test Loss: 2.307105711, Test Accuracy: 47.060\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.591351111, Training Accuracy: 54.448\n",
            "Worker 1, [02/08]: Training Loss: 1.348770864, Training Accuracy: 60.944\n",
            "Worker 1, [03/08]: Training Loss: 1.162327906, Training Accuracy: 65.744\n",
            "Worker 1, [04/08]: Training Loss: 1.078956897, Training Accuracy: 68.288\n",
            "Worker 1, [05/08]: Training Loss: 0.996877218, Training Accuracy: 71.520\n",
            "Worker 1, [06/08]: Training Loss: 0.915503267, Training Accuracy: 73.632\n",
            "Worker 1, [07/08]: Training Loss: 0.831291776, Training Accuracy: 74.976\n",
            "Worker 1, [08/08]: Training Loss: 0.788420657, Training Accuracy: 77.008\n",
            "Time taken for training worker 1: 0:00:25.933053\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.622646942, Training Accuracy: 54.736\n",
            "Worker 2, [02/08]: Training Loss: 1.346214429, Training Accuracy: 61.232\n",
            "Worker 2, [03/08]: Training Loss: 1.193573282, Training Accuracy: 65.856\n",
            "Worker 2, [04/08]: Training Loss: 1.098140765, Training Accuracy: 67.888\n",
            "Worker 2, [05/08]: Training Loss: 0.982267293, Training Accuracy: 71.856\n",
            "Worker 2, [06/08]: Training Loss: 0.926329150, Training Accuracy: 73.184\n",
            "Worker 2, [07/08]: Training Loss: 0.857394946, Training Accuracy: 75.216\n",
            "Worker 2, [08/08]: Training Loss: 0.799304654, Training Accuracy: 75.984\n",
            "Time taken for training worker 2: 0:00:24.513595\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.669771027, Training Accuracy: 54.016\n",
            "Worker 3, [02/08]: Training Loss: 1.371382349, Training Accuracy: 60.432\n",
            "Worker 3, [03/08]: Training Loss: 1.237670684, Training Accuracy: 64.448\n",
            "Worker 3, [04/08]: Training Loss: 1.107652049, Training Accuracy: 67.776\n",
            "Worker 3, [05/08]: Training Loss: 1.052243267, Training Accuracy: 69.104\n",
            "Worker 3, [06/08]: Training Loss: 0.926411673, Training Accuracy: 73.168\n",
            "Worker 3, [07/08]: Training Loss: 0.865359803, Training Accuracy: 74.864\n",
            "Worker 3, [08/08]: Training Loss: 0.811623245, Training Accuracy: 76.112\n",
            "Time taken for training worker 3: 0:00:24.403104\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.633827154, Training Accuracy: 54.816\n",
            "Worker 4, [02/08]: Training Loss: 1.364212407, Training Accuracy: 60.720\n",
            "Worker 4, [03/08]: Training Loss: 1.236397991, Training Accuracy: 64.320\n",
            "Worker 4, [04/08]: Training Loss: 1.085723535, Training Accuracy: 68.928\n",
            "Worker 4, [05/08]: Training Loss: 0.997107370, Training Accuracy: 71.152\n",
            "Worker 4, [06/08]: Training Loss: 0.935528121, Training Accuracy: 72.304\n",
            "Worker 4, [07/08]: Training Loss: 0.902743272, Training Accuracy: 73.488\n",
            "Worker 4, [08/08]: Training Loss: 0.812552067, Training Accuracy: 75.888\n",
            "Time taken for training worker 4: 0:00:25.782241\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.636167879, Training Accuracy: 53.856\n",
            "Worker 5, [02/08]: Training Loss: 1.349917062, Training Accuracy: 60.752\n",
            "Worker 5, [03/08]: Training Loss: 1.186884358, Training Accuracy: 65.888\n",
            "Worker 5, [04/08]: Training Loss: 1.093314170, Training Accuracy: 68.240\n",
            "Worker 5, [05/08]: Training Loss: 0.990049622, Training Accuracy: 70.720\n",
            "Worker 5, [06/08]: Training Loss: 0.940363149, Training Accuracy: 72.448\n",
            "Worker 5, [07/08]: Training Loss: 0.879201934, Training Accuracy: 73.680\n",
            "Worker 5, [08/08]: Training Loss: 0.810809295, Training Accuracy: 76.096\n",
            "Time taken for training worker 5: 0:00:24.458496\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.649719479, Training Accuracy: 53.424\n",
            "Worker 6, [02/08]: Training Loss: 1.342458381, Training Accuracy: 61.632\n",
            "Worker 6, [03/08]: Training Loss: 1.217444767, Training Accuracy: 64.560\n",
            "Worker 6, [04/08]: Training Loss: 1.101402915, Training Accuracy: 67.568\n",
            "Worker 6, [05/08]: Training Loss: 1.022989441, Training Accuracy: 70.384\n",
            "Worker 6, [06/08]: Training Loss: 0.952261222, Training Accuracy: 72.064\n",
            "Worker 6, [07/08]: Training Loss: 0.873250989, Training Accuracy: 74.688\n",
            "Worker 6, [08/08]: Training Loss: 0.808544905, Training Accuracy: 76.384\n",
            "Time taken for training worker 6: 0:00:24.120306\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.639572152, Training Accuracy: 54.272\n",
            "Worker 7, [02/08]: Training Loss: 1.365989717, Training Accuracy: 60.656\n",
            "Worker 7, [03/08]: Training Loss: 1.200830017, Training Accuracy: 65.392\n",
            "Worker 7, [04/08]: Training Loss: 1.104445094, Training Accuracy: 67.648\n",
            "Worker 7, [05/08]: Training Loss: 1.026974751, Training Accuracy: 69.744\n",
            "Worker 7, [06/08]: Training Loss: 0.953083609, Training Accuracy: 72.544\n",
            "Worker 7, [07/08]: Training Loss: 0.857449772, Training Accuracy: 74.768\n",
            "Worker 7, [08/08]: Training Loss: 0.815714676, Training Accuracy: 75.616\n",
            "Time taken for training worker 7: 0:00:24.349938\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.685654787, Training Accuracy: 53.504\n",
            "Worker 8, [02/08]: Training Loss: 1.388220082, Training Accuracy: 60.176\n",
            "Worker 8, [03/08]: Training Loss: 1.217099730, Training Accuracy: 65.424\n",
            "Worker 8, [04/08]: Training Loss: 1.127855104, Training Accuracy: 67.392\n",
            "Worker 8, [05/08]: Training Loss: 1.028297826, Training Accuracy: 70.064\n",
            "Worker 8, [06/08]: Training Loss: 0.965186917, Training Accuracy: 71.840\n",
            "Worker 8, [07/08]: Training Loss: 0.878998270, Training Accuracy: 74.176\n",
            "Worker 8, [08/08]: Training Loss: 0.828084029, Training Accuracy: 75.248\n",
            "Time taken for training worker 8: 0:00:24.494155\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004018\n",
            "Global Update 13: Test Loss: 2.325689441, Test Accuracy: 47.240\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.509117713, Training Accuracy: 57.232\n",
            "Worker 1, [02/08]: Training Loss: 1.260735640, Training Accuracy: 62.576\n",
            "Worker 1, [03/08]: Training Loss: 1.099968338, Training Accuracy: 67.232\n",
            "Worker 1, [04/08]: Training Loss: 0.987451263, Training Accuracy: 71.584\n",
            "Worker 1, [05/08]: Training Loss: 0.938794692, Training Accuracy: 72.752\n",
            "Worker 1, [06/08]: Training Loss: 0.867562549, Training Accuracy: 74.672\n",
            "Worker 1, [07/08]: Training Loss: 0.808689224, Training Accuracy: 76.080\n",
            "Worker 1, [08/08]: Training Loss: 0.751491551, Training Accuracy: 78.144\n",
            "Time taken for training worker 1: 0:00:23.645092\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.538142055, Training Accuracy: 57.552\n",
            "Worker 2, [02/08]: Training Loss: 1.258946104, Training Accuracy: 64.016\n",
            "Worker 2, [03/08]: Training Loss: 1.125574514, Training Accuracy: 67.632\n",
            "Worker 2, [04/08]: Training Loss: 1.040761942, Training Accuracy: 69.696\n",
            "Worker 2, [05/08]: Training Loss: 0.932060633, Training Accuracy: 72.800\n",
            "Worker 2, [06/08]: Training Loss: 0.871804333, Training Accuracy: 74.864\n",
            "Worker 2, [07/08]: Training Loss: 0.799228050, Training Accuracy: 76.672\n",
            "Worker 2, [08/08]: Training Loss: 0.776872711, Training Accuracy: 78.128\n",
            "Time taken for training worker 2: 0:00:23.547755\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.554572903, Training Accuracy: 56.752\n",
            "Worker 3, [02/08]: Training Loss: 1.271557518, Training Accuracy: 63.600\n",
            "Worker 3, [03/08]: Training Loss: 1.129708814, Training Accuracy: 67.648\n",
            "Worker 3, [04/08]: Training Loss: 1.056081796, Training Accuracy: 70.064\n",
            "Worker 3, [05/08]: Training Loss: 0.955167719, Training Accuracy: 71.872\n",
            "Worker 3, [06/08]: Training Loss: 0.901746417, Training Accuracy: 73.872\n",
            "Worker 3, [07/08]: Training Loss: 0.818533424, Training Accuracy: 76.288\n",
            "Worker 3, [08/08]: Training Loss: 0.773835049, Training Accuracy: 77.424\n",
            "Time taken for training worker 3: 0:00:24.640833\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.526937450, Training Accuracy: 56.768\n",
            "Worker 4, [02/08]: Training Loss: 1.301314002, Training Accuracy: 62.720\n",
            "Worker 4, [03/08]: Training Loss: 1.131701084, Training Accuracy: 66.176\n",
            "Worker 4, [04/08]: Training Loss: 1.042346695, Training Accuracy: 70.032\n",
            "Worker 4, [05/08]: Training Loss: 0.974373049, Training Accuracy: 71.936\n",
            "Worker 4, [06/08]: Training Loss: 0.882181354, Training Accuracy: 74.448\n",
            "Worker 4, [07/08]: Training Loss: 0.832702286, Training Accuracy: 75.488\n",
            "Worker 4, [08/08]: Training Loss: 0.791749166, Training Accuracy: 76.864\n",
            "Time taken for training worker 4: 0:00:24.684393\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.532918565, Training Accuracy: 57.104\n",
            "Worker 5, [02/08]: Training Loss: 1.292175407, Training Accuracy: 62.800\n",
            "Worker 5, [03/08]: Training Loss: 1.123253142, Training Accuracy: 67.600\n",
            "Worker 5, [04/08]: Training Loss: 1.036331268, Training Accuracy: 70.000\n",
            "Worker 5, [05/08]: Training Loss: 0.936106214, Training Accuracy: 72.240\n",
            "Worker 5, [06/08]: Training Loss: 0.863521775, Training Accuracy: 74.640\n",
            "Worker 5, [07/08]: Training Loss: 0.807674060, Training Accuracy: 76.880\n",
            "Worker 5, [08/08]: Training Loss: 0.768184741, Training Accuracy: 77.520\n",
            "Time taken for training worker 5: 0:00:24.732176\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.556129475, Training Accuracy: 55.344\n",
            "Worker 6, [02/08]: Training Loss: 1.301473280, Training Accuracy: 62.112\n",
            "Worker 6, [03/08]: Training Loss: 1.139031312, Training Accuracy: 66.320\n",
            "Worker 6, [04/08]: Training Loss: 1.028626875, Training Accuracy: 69.568\n",
            "Worker 6, [05/08]: Training Loss: 0.965033518, Training Accuracy: 72.000\n",
            "Worker 6, [06/08]: Training Loss: 0.884068588, Training Accuracy: 74.192\n",
            "Worker 6, [07/08]: Training Loss: 0.821401301, Training Accuracy: 75.616\n",
            "Worker 6, [08/08]: Training Loss: 0.770345562, Training Accuracy: 77.856\n",
            "Time taken for training worker 6: 0:00:24.727869\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.545515508, Training Accuracy: 56.560\n",
            "Worker 7, [02/08]: Training Loss: 1.283034263, Training Accuracy: 63.024\n",
            "Worker 7, [03/08]: Training Loss: 1.110324223, Training Accuracy: 68.384\n",
            "Worker 7, [04/08]: Training Loss: 1.005905426, Training Accuracy: 70.352\n",
            "Worker 7, [05/08]: Training Loss: 0.952786681, Training Accuracy: 72.512\n",
            "Worker 7, [06/08]: Training Loss: 0.893508868, Training Accuracy: 74.000\n",
            "Worker 7, [07/08]: Training Loss: 0.815437675, Training Accuracy: 75.856\n",
            "Worker 7, [08/08]: Training Loss: 0.784806357, Training Accuracy: 77.168\n",
            "Time taken for training worker 7: 0:00:25.383776\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.542493493, Training Accuracy: 55.760\n",
            "Worker 8, [02/08]: Training Loss: 1.274117001, Training Accuracy: 63.824\n",
            "Worker 8, [03/08]: Training Loss: 1.158432129, Training Accuracy: 66.624\n",
            "Worker 8, [04/08]: Training Loss: 1.049611401, Training Accuracy: 70.032\n",
            "Worker 8, [05/08]: Training Loss: 0.970930512, Training Accuracy: 72.064\n",
            "Worker 8, [06/08]: Training Loss: 0.882364555, Training Accuracy: 74.224\n",
            "Worker 8, [07/08]: Training Loss: 0.830030286, Training Accuracy: 75.680\n",
            "Worker 8, [08/08]: Training Loss: 0.786414176, Training Accuracy: 77.248\n",
            "Time taken for training worker 8: 0:00:24.284964\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003912\n",
            "Global Update 14: Test Loss: 2.321516029, Test Accuracy: 47.460\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.425334212, Training Accuracy: 58.896\n",
            "Worker 1, [02/08]: Training Loss: 1.198126441, Training Accuracy: 64.576\n",
            "Worker 1, [03/08]: Training Loss: 1.090334300, Training Accuracy: 67.456\n",
            "Worker 1, [04/08]: Training Loss: 0.972246949, Training Accuracy: 71.168\n",
            "Worker 1, [05/08]: Training Loss: 0.897372896, Training Accuracy: 74.080\n",
            "Worker 1, [06/08]: Training Loss: 0.849221051, Training Accuracy: 75.296\n",
            "Worker 1, [07/08]: Training Loss: 0.797562143, Training Accuracy: 76.576\n",
            "Worker 1, [08/08]: Training Loss: 0.742194751, Training Accuracy: 78.928\n",
            "Time taken for training worker 1: 0:00:24.770208\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.433037799, Training Accuracy: 58.992\n",
            "Worker 2, [02/08]: Training Loss: 1.198608826, Training Accuracy: 65.280\n",
            "Worker 2, [03/08]: Training Loss: 1.080750903, Training Accuracy: 68.528\n",
            "Worker 2, [04/08]: Training Loss: 1.005881346, Training Accuracy: 70.960\n",
            "Worker 2, [05/08]: Training Loss: 0.912360318, Training Accuracy: 73.824\n",
            "Worker 2, [06/08]: Training Loss: 0.852977958, Training Accuracy: 75.216\n",
            "Worker 2, [07/08]: Training Loss: 0.804652898, Training Accuracy: 76.512\n",
            "Worker 2, [08/08]: Training Loss: 0.767300452, Training Accuracy: 77.648\n",
            "Time taken for training worker 2: 0:00:24.351840\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.452085720, Training Accuracy: 58.976\n",
            "Worker 3, [02/08]: Training Loss: 1.229571909, Training Accuracy: 64.016\n",
            "Worker 3, [03/08]: Training Loss: 1.091439569, Training Accuracy: 68.528\n",
            "Worker 3, [04/08]: Training Loss: 1.017044378, Training Accuracy: 70.384\n",
            "Worker 3, [05/08]: Training Loss: 0.923982903, Training Accuracy: 73.168\n",
            "Worker 3, [06/08]: Training Loss: 0.877963543, Training Accuracy: 75.088\n",
            "Worker 3, [07/08]: Training Loss: 0.820923850, Training Accuracy: 76.960\n",
            "Worker 3, [08/08]: Training Loss: 0.772518773, Training Accuracy: 77.680\n",
            "Time taken for training worker 3: 0:00:24.652694\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.471145630, Training Accuracy: 58.576\n",
            "Worker 4, [02/08]: Training Loss: 1.219995690, Training Accuracy: 64.480\n",
            "Worker 4, [03/08]: Training Loss: 1.102235048, Training Accuracy: 67.984\n",
            "Worker 4, [04/08]: Training Loss: 1.010087739, Training Accuracy: 69.856\n",
            "Worker 4, [05/08]: Training Loss: 0.917487380, Training Accuracy: 73.248\n",
            "Worker 4, [06/08]: Training Loss: 0.852498280, Training Accuracy: 75.392\n",
            "Worker 4, [07/08]: Training Loss: 0.808880064, Training Accuracy: 76.992\n",
            "Worker 4, [08/08]: Training Loss: 0.780317574, Training Accuracy: 77.328\n",
            "Time taken for training worker 4: 0:00:24.751505\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.482881267, Training Accuracy: 57.760\n",
            "Worker 5, [02/08]: Training Loss: 1.201961908, Training Accuracy: 65.040\n",
            "Worker 5, [03/08]: Training Loss: 1.097821154, Training Accuracy: 68.048\n",
            "Worker 5, [04/08]: Training Loss: 0.993845668, Training Accuracy: 70.768\n",
            "Worker 5, [05/08]: Training Loss: 0.912714101, Training Accuracy: 73.488\n",
            "Worker 5, [06/08]: Training Loss: 0.856598319, Training Accuracy: 75.264\n",
            "Worker 5, [07/08]: Training Loss: 0.822563840, Training Accuracy: 75.648\n",
            "Worker 5, [08/08]: Training Loss: 0.753158192, Training Accuracy: 78.960\n",
            "Time taken for training worker 5: 0:00:24.184900\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.453009806, Training Accuracy: 58.128\n",
            "Worker 6, [02/08]: Training Loss: 1.222276078, Training Accuracy: 64.304\n",
            "Worker 6, [03/08]: Training Loss: 1.104770950, Training Accuracy: 67.696\n",
            "Worker 6, [04/08]: Training Loss: 1.041705250, Training Accuracy: 70.064\n",
            "Worker 6, [05/08]: Training Loss: 0.934464711, Training Accuracy: 73.328\n",
            "Worker 6, [06/08]: Training Loss: 0.859977057, Training Accuracy: 74.672\n",
            "Worker 6, [07/08]: Training Loss: 0.798691178, Training Accuracy: 76.336\n",
            "Worker 6, [08/08]: Training Loss: 0.740353741, Training Accuracy: 78.672\n",
            "Time taken for training worker 6: 0:00:23.816115\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.447799161, Training Accuracy: 59.264\n",
            "Worker 7, [02/08]: Training Loss: 1.216692726, Training Accuracy: 64.736\n",
            "Worker 7, [03/08]: Training Loss: 1.109307003, Training Accuracy: 68.272\n",
            "Worker 7, [04/08]: Training Loss: 1.009632491, Training Accuracy: 71.248\n",
            "Worker 7, [05/08]: Training Loss: 0.948118414, Training Accuracy: 72.368\n",
            "Worker 7, [06/08]: Training Loss: 0.871366674, Training Accuracy: 74.544\n",
            "Worker 7, [07/08]: Training Loss: 0.824728343, Training Accuracy: 75.952\n",
            "Worker 7, [08/08]: Training Loss: 0.774272495, Training Accuracy: 77.760\n",
            "Time taken for training worker 7: 0:00:24.838461\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.458218885, Training Accuracy: 58.192\n",
            "Worker 8, [02/08]: Training Loss: 1.235663305, Training Accuracy: 63.808\n",
            "Worker 8, [03/08]: Training Loss: 1.114187941, Training Accuracy: 68.224\n",
            "Worker 8, [04/08]: Training Loss: 1.008464494, Training Accuracy: 70.688\n",
            "Worker 8, [05/08]: Training Loss: 0.952063811, Training Accuracy: 72.160\n",
            "Worker 8, [06/08]: Training Loss: 0.889983921, Training Accuracy: 74.080\n",
            "Worker 8, [07/08]: Training Loss: 0.851573996, Training Accuracy: 75.280\n",
            "Worker 8, [08/08]: Training Loss: 0.795339040, Training Accuracy: 77.536\n",
            "Time taken for training worker 8: 0:00:25.520757\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003973\n",
            "Global Update 15: Test Loss: 2.331534281, Test Accuracy: 47.840\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.349141421, Training Accuracy: 60.960\n",
            "Worker 1, [02/08]: Training Loss: 1.184791295, Training Accuracy: 65.344\n",
            "Worker 1, [03/08]: Training Loss: 1.064120726, Training Accuracy: 68.672\n",
            "Worker 1, [04/08]: Training Loss: 1.010229296, Training Accuracy: 70.656\n",
            "Worker 1, [05/08]: Training Loss: 0.932339005, Training Accuracy: 72.752\n",
            "Worker 1, [06/08]: Training Loss: 0.891568492, Training Accuracy: 74.016\n",
            "Worker 1, [07/08]: Training Loss: 0.837332752, Training Accuracy: 76.064\n",
            "Worker 1, [08/08]: Training Loss: 0.774902857, Training Accuracy: 77.664\n",
            "Time taken for training worker 1: 0:00:24.262043\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.376736910, Training Accuracy: 60.208\n",
            "Worker 2, [02/08]: Training Loss: 1.186145213, Training Accuracy: 65.808\n",
            "Worker 2, [03/08]: Training Loss: 1.082374164, Training Accuracy: 68.608\n",
            "Worker 2, [04/08]: Training Loss: 1.019021872, Training Accuracy: 70.304\n",
            "Worker 2, [05/08]: Training Loss: 0.930682968, Training Accuracy: 73.568\n",
            "Worker 2, [06/08]: Training Loss: 0.892039594, Training Accuracy: 74.592\n",
            "Worker 2, [07/08]: Training Loss: 0.845716769, Training Accuracy: 76.192\n",
            "Worker 2, [08/08]: Training Loss: 0.817654303, Training Accuracy: 76.240\n",
            "Time taken for training worker 2: 0:00:24.214112\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.363947714, Training Accuracy: 60.768\n",
            "Worker 3, [02/08]: Training Loss: 1.216082996, Training Accuracy: 64.848\n",
            "Worker 3, [03/08]: Training Loss: 1.114221384, Training Accuracy: 67.632\n",
            "Worker 3, [04/08]: Training Loss: 1.031894715, Training Accuracy: 70.032\n",
            "Worker 3, [05/08]: Training Loss: 0.941074905, Training Accuracy: 73.120\n",
            "Worker 3, [06/08]: Training Loss: 0.899610727, Training Accuracy: 74.464\n",
            "Worker 3, [07/08]: Training Loss: 0.845983778, Training Accuracy: 75.152\n",
            "Worker 3, [08/08]: Training Loss: 0.806674156, Training Accuracy: 77.440\n",
            "Time taken for training worker 3: 0:00:25.432177\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.375017219, Training Accuracy: 60.064\n",
            "Worker 4, [02/08]: Training Loss: 1.203052567, Training Accuracy: 65.120\n",
            "Worker 4, [03/08]: Training Loss: 1.087328847, Training Accuracy: 68.576\n",
            "Worker 4, [04/08]: Training Loss: 1.009457914, Training Accuracy: 70.720\n",
            "Worker 4, [05/08]: Training Loss: 0.956794393, Training Accuracy: 72.448\n",
            "Worker 4, [06/08]: Training Loss: 0.898755379, Training Accuracy: 73.968\n",
            "Worker 4, [07/08]: Training Loss: 0.836293940, Training Accuracy: 75.648\n",
            "Worker 4, [08/08]: Training Loss: 0.829604146, Training Accuracy: 75.872\n",
            "Time taken for training worker 4: 0:00:24.461699\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.385089983, Training Accuracy: 59.632\n",
            "Worker 5, [02/08]: Training Loss: 1.195246869, Training Accuracy: 64.704\n",
            "Worker 5, [03/08]: Training Loss: 1.104761612, Training Accuracy: 68.128\n",
            "Worker 5, [04/08]: Training Loss: 1.004366338, Training Accuracy: 71.456\n",
            "Worker 5, [05/08]: Training Loss: 0.938695809, Training Accuracy: 72.848\n",
            "Worker 5, [06/08]: Training Loss: 0.911333661, Training Accuracy: 73.760\n",
            "Worker 5, [07/08]: Training Loss: 0.846646120, Training Accuracy: 75.792\n",
            "Worker 5, [08/08]: Training Loss: 0.823152976, Training Accuracy: 75.824\n",
            "Time taken for training worker 5: 0:00:24.343194\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.402346092, Training Accuracy: 59.856\n",
            "Worker 6, [02/08]: Training Loss: 1.191803131, Training Accuracy: 64.672\n",
            "Worker 6, [03/08]: Training Loss: 1.087331178, Training Accuracy: 68.656\n",
            "Worker 6, [04/08]: Training Loss: 1.039766606, Training Accuracy: 70.160\n",
            "Worker 6, [05/08]: Training Loss: 0.945285629, Training Accuracy: 72.480\n",
            "Worker 6, [06/08]: Training Loss: 0.881794464, Training Accuracy: 74.720\n",
            "Worker 6, [07/08]: Training Loss: 0.851975552, Training Accuracy: 75.664\n",
            "Worker 6, [08/08]: Training Loss: 0.813771940, Training Accuracy: 76.336\n",
            "Time taken for training worker 6: 0:00:24.538710\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.393702807, Training Accuracy: 59.296\n",
            "Worker 7, [02/08]: Training Loss: 1.203633098, Training Accuracy: 66.032\n",
            "Worker 7, [03/08]: Training Loss: 1.096277767, Training Accuracy: 68.352\n",
            "Worker 7, [04/08]: Training Loss: 1.024980563, Training Accuracy: 70.080\n",
            "Worker 7, [05/08]: Training Loss: 0.977611937, Training Accuracy: 72.016\n",
            "Worker 7, [06/08]: Training Loss: 0.897146646, Training Accuracy: 74.096\n",
            "Worker 7, [07/08]: Training Loss: 0.858374032, Training Accuracy: 74.800\n",
            "Worker 7, [08/08]: Training Loss: 0.817084933, Training Accuracy: 76.656\n",
            "Time taken for training worker 7: 0:00:25.061433\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.363268535, Training Accuracy: 60.816\n",
            "Worker 8, [02/08]: Training Loss: 1.240044331, Training Accuracy: 63.952\n",
            "Worker 8, [03/08]: Training Loss: 1.100182441, Training Accuracy: 67.808\n",
            "Worker 8, [04/08]: Training Loss: 1.046186771, Training Accuracy: 69.488\n",
            "Worker 8, [05/08]: Training Loss: 0.985411630, Training Accuracy: 71.648\n",
            "Worker 8, [06/08]: Training Loss: 0.918401314, Training Accuracy: 73.456\n",
            "Worker 8, [07/08]: Training Loss: 0.868614240, Training Accuracy: 75.792\n",
            "Worker 8, [08/08]: Training Loss: 0.831491138, Training Accuracy: 76.576\n",
            "Time taken for training worker 8: 0:00:24.464685\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004089\n",
            "Global Update 16: Test Loss: 2.313580196, Test Accuracy: 47.890\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.292983565, Training Accuracy: 62.400\n",
            "Worker 1, [02/08]: Training Loss: 1.168599470, Training Accuracy: 65.648\n",
            "Worker 1, [03/08]: Training Loss: 1.095987448, Training Accuracy: 68.352\n",
            "Worker 1, [04/08]: Training Loss: 1.046404539, Training Accuracy: 70.064\n",
            "Worker 1, [05/08]: Training Loss: 1.013443682, Training Accuracy: 70.400\n",
            "Worker 1, [06/08]: Training Loss: 0.964721289, Training Accuracy: 71.936\n",
            "Worker 1, [07/08]: Training Loss: 0.926618950, Training Accuracy: 72.592\n",
            "Worker 1, [08/08]: Training Loss: 0.889966386, Training Accuracy: 74.640\n",
            "Time taken for training worker 1: 0:00:25.328460\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.328498939, Training Accuracy: 61.392\n",
            "Worker 2, [02/08]: Training Loss: 1.200018604, Training Accuracy: 65.008\n",
            "Worker 2, [03/08]: Training Loss: 1.131254996, Training Accuracy: 67.328\n",
            "Worker 2, [04/08]: Training Loss: 1.060425774, Training Accuracy: 70.224\n",
            "Worker 2, [05/08]: Training Loss: 1.020992617, Training Accuracy: 70.624\n",
            "Worker 2, [06/08]: Training Loss: 0.991439016, Training Accuracy: 71.728\n",
            "Worker 2, [07/08]: Training Loss: 0.952006622, Training Accuracy: 72.208\n",
            "Worker 2, [08/08]: Training Loss: 0.921985064, Training Accuracy: 73.376\n",
            "Time taken for training worker 2: 0:00:23.769199\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.314592762, Training Accuracy: 62.160\n",
            "Worker 3, [02/08]: Training Loss: 1.209181242, Training Accuracy: 65.088\n",
            "Worker 3, [03/08]: Training Loss: 1.140241451, Training Accuracy: 66.736\n",
            "Worker 3, [04/08]: Training Loss: 1.081107564, Training Accuracy: 68.480\n",
            "Worker 3, [05/08]: Training Loss: 1.040630859, Training Accuracy: 70.160\n",
            "Worker 3, [06/08]: Training Loss: 1.000839281, Training Accuracy: 70.912\n",
            "Worker 3, [07/08]: Training Loss: 0.950889602, Training Accuracy: 72.640\n",
            "Worker 3, [08/08]: Training Loss: 0.909465732, Training Accuracy: 74.400\n",
            "Time taken for training worker 3: 0:00:24.720949\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.328434791, Training Accuracy: 61.664\n",
            "Worker 4, [02/08]: Training Loss: 1.216940977, Training Accuracy: 64.880\n",
            "Worker 4, [03/08]: Training Loss: 1.150046594, Training Accuracy: 66.528\n",
            "Worker 4, [04/08]: Training Loss: 1.085520517, Training Accuracy: 68.144\n",
            "Worker 4, [05/08]: Training Loss: 1.019359948, Training Accuracy: 70.832\n",
            "Worker 4, [06/08]: Training Loss: 0.996985736, Training Accuracy: 71.136\n",
            "Worker 4, [07/08]: Training Loss: 0.972656183, Training Accuracy: 72.656\n",
            "Worker 4, [08/08]: Training Loss: 0.931750823, Training Accuracy: 73.520\n",
            "Time taken for training worker 4: 0:00:23.710065\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.323611402, Training Accuracy: 61.888\n",
            "Worker 5, [02/08]: Training Loss: 1.202751100, Training Accuracy: 64.784\n",
            "Worker 5, [03/08]: Training Loss: 1.144930725, Training Accuracy: 66.960\n",
            "Worker 5, [04/08]: Training Loss: 1.060223266, Training Accuracy: 68.864\n",
            "Worker 5, [05/08]: Training Loss: 1.041456977, Training Accuracy: 69.600\n",
            "Worker 5, [06/08]: Training Loss: 0.997101306, Training Accuracy: 70.496\n",
            "Worker 5, [07/08]: Training Loss: 0.942264717, Training Accuracy: 72.480\n",
            "Worker 5, [08/08]: Training Loss: 0.920155275, Training Accuracy: 72.704\n",
            "Time taken for training worker 5: 0:00:24.029682\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.333657021, Training Accuracy: 61.440\n",
            "Worker 6, [02/08]: Training Loss: 1.220118530, Training Accuracy: 64.048\n",
            "Worker 6, [03/08]: Training Loss: 1.149002759, Training Accuracy: 66.752\n",
            "Worker 6, [04/08]: Training Loss: 1.067373222, Training Accuracy: 68.416\n",
            "Worker 6, [05/08]: Training Loss: 1.036889768, Training Accuracy: 69.504\n",
            "Worker 6, [06/08]: Training Loss: 1.013384037, Training Accuracy: 70.960\n",
            "Worker 6, [07/08]: Training Loss: 0.956496399, Training Accuracy: 72.608\n",
            "Worker 6, [08/08]: Training Loss: 0.921789630, Training Accuracy: 73.024\n",
            "Time taken for training worker 6: 0:00:24.626676\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.330540358, Training Accuracy: 60.816\n",
            "Worker 7, [02/08]: Training Loss: 1.206335715, Training Accuracy: 65.248\n",
            "Worker 7, [03/08]: Training Loss: 1.132616727, Training Accuracy: 66.944\n",
            "Worker 7, [04/08]: Training Loss: 1.092263009, Training Accuracy: 68.416\n",
            "Worker 7, [05/08]: Training Loss: 1.038302364, Training Accuracy: 69.728\n",
            "Worker 7, [06/08]: Training Loss: 0.999370257, Training Accuracy: 71.376\n",
            "Worker 7, [07/08]: Training Loss: 0.974785788, Training Accuracy: 72.352\n",
            "Worker 7, [08/08]: Training Loss: 0.911559486, Training Accuracy: 73.856\n",
            "Time taken for training worker 7: 0:00:24.440452\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.364638249, Training Accuracy: 61.248\n",
            "Worker 8, [02/08]: Training Loss: 1.226274536, Training Accuracy: 64.096\n",
            "Worker 8, [03/08]: Training Loss: 1.156865253, Training Accuracy: 66.128\n",
            "Worker 8, [04/08]: Training Loss: 1.069481968, Training Accuracy: 68.528\n",
            "Worker 8, [05/08]: Training Loss: 1.056874298, Training Accuracy: 69.472\n",
            "Worker 8, [06/08]: Training Loss: 1.010454547, Training Accuracy: 70.208\n",
            "Worker 8, [07/08]: Training Loss: 0.953259001, Training Accuracy: 72.128\n",
            "Worker 8, [08/08]: Training Loss: 0.934388396, Training Accuracy: 72.560\n",
            "Time taken for training worker 8: 0:00:24.564386\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004164\n",
            "Global Update 17: Test Loss: 2.279698403, Test Accuracy: 47.880\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.249823008, Training Accuracy: 63.344\n",
            "Worker 1, [02/08]: Training Loss: 1.234139911, Training Accuracy: 64.256\n",
            "Worker 1, [03/08]: Training Loss: 1.181354742, Training Accuracy: 64.976\n",
            "Worker 1, [04/08]: Training Loss: 1.171357612, Training Accuracy: 66.016\n",
            "Worker 1, [05/08]: Training Loss: 1.127690564, Training Accuracy: 67.056\n",
            "Worker 1, [06/08]: Training Loss: 1.129322591, Training Accuracy: 66.544\n",
            "Worker 1, [07/08]: Training Loss: 1.087895007, Training Accuracy: 68.272\n",
            "Worker 1, [08/08]: Training Loss: 1.070656156, Training Accuracy: 68.912\n",
            "Time taken for training worker 1: 0:00:24.622823\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.292578022, Training Accuracy: 62.144\n",
            "Worker 2, [02/08]: Training Loss: 1.253307573, Training Accuracy: 64.176\n",
            "Worker 2, [03/08]: Training Loss: 1.231214625, Training Accuracy: 64.800\n",
            "Worker 2, [04/08]: Training Loss: 1.199988986, Training Accuracy: 65.328\n",
            "Worker 2, [05/08]: Training Loss: 1.177132529, Training Accuracy: 65.760\n",
            "Worker 2, [06/08]: Training Loss: 1.140526576, Training Accuracy: 66.432\n",
            "Worker 2, [07/08]: Training Loss: 1.114288784, Training Accuracy: 67.296\n",
            "Worker 2, [08/08]: Training Loss: 1.103660257, Training Accuracy: 68.560\n",
            "Time taken for training worker 2: 0:00:24.290278\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.300241385, Training Accuracy: 62.160\n",
            "Worker 3, [02/08]: Training Loss: 1.249750538, Training Accuracy: 64.112\n",
            "Worker 3, [03/08]: Training Loss: 1.195331942, Training Accuracy: 65.648\n",
            "Worker 3, [04/08]: Training Loss: 1.171340726, Training Accuracy: 65.536\n",
            "Worker 3, [05/08]: Training Loss: 1.168483593, Training Accuracy: 66.784\n",
            "Worker 3, [06/08]: Training Loss: 1.141743242, Training Accuracy: 66.976\n",
            "Worker 3, [07/08]: Training Loss: 1.118416846, Training Accuracy: 67.296\n",
            "Worker 3, [08/08]: Training Loss: 1.109555783, Training Accuracy: 67.824\n",
            "Time taken for training worker 3: 0:00:23.881155\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.296774288, Training Accuracy: 62.176\n",
            "Worker 4, [02/08]: Training Loss: 1.247194069, Training Accuracy: 64.192\n",
            "Worker 4, [03/08]: Training Loss: 1.222384414, Training Accuracy: 64.272\n",
            "Worker 4, [04/08]: Training Loss: 1.192613822, Training Accuracy: 65.744\n",
            "Worker 4, [05/08]: Training Loss: 1.176636272, Training Accuracy: 65.840\n",
            "Worker 4, [06/08]: Training Loss: 1.134265761, Training Accuracy: 66.928\n",
            "Worker 4, [07/08]: Training Loss: 1.122420155, Training Accuracy: 67.200\n",
            "Worker 4, [08/08]: Training Loss: 1.106139735, Training Accuracy: 68.048\n",
            "Time taken for training worker 4: 0:00:24.273120\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.289832704, Training Accuracy: 62.112\n",
            "Worker 5, [02/08]: Training Loss: 1.250624866, Training Accuracy: 63.504\n",
            "Worker 5, [03/08]: Training Loss: 1.203285085, Training Accuracy: 64.624\n",
            "Worker 5, [04/08]: Training Loss: 1.186692302, Training Accuracy: 65.616\n",
            "Worker 5, [05/08]: Training Loss: 1.159415716, Training Accuracy: 66.496\n",
            "Worker 5, [06/08]: Training Loss: 1.131955421, Training Accuracy: 67.648\n",
            "Worker 5, [07/08]: Training Loss: 1.110538329, Training Accuracy: 67.616\n",
            "Worker 5, [08/08]: Training Loss: 1.099915752, Training Accuracy: 67.904\n",
            "Time taken for training worker 5: 0:00:24.291572\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.298671411, Training Accuracy: 62.368\n",
            "Worker 6, [02/08]: Training Loss: 1.278322835, Training Accuracy: 63.024\n",
            "Worker 6, [03/08]: Training Loss: 1.219331543, Training Accuracy: 64.208\n",
            "Worker 6, [04/08]: Training Loss: 1.188821258, Training Accuracy: 65.216\n",
            "Worker 6, [05/08]: Training Loss: 1.178968811, Training Accuracy: 65.872\n",
            "Worker 6, [06/08]: Training Loss: 1.135862129, Training Accuracy: 66.656\n",
            "Worker 6, [07/08]: Training Loss: 1.130368831, Training Accuracy: 66.736\n",
            "Worker 6, [08/08]: Training Loss: 1.103384505, Training Accuracy: 67.856\n",
            "Time taken for training worker 6: 0:00:25.727970\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.291928477, Training Accuracy: 62.576\n",
            "Worker 7, [02/08]: Training Loss: 1.242404502, Training Accuracy: 63.920\n",
            "Worker 7, [03/08]: Training Loss: 1.201468744, Training Accuracy: 64.144\n",
            "Worker 7, [04/08]: Training Loss: 1.183233348, Training Accuracy: 65.600\n",
            "Worker 7, [05/08]: Training Loss: 1.150090359, Training Accuracy: 66.272\n",
            "Worker 7, [06/08]: Training Loss: 1.145723548, Training Accuracy: 67.008\n",
            "Worker 7, [07/08]: Training Loss: 1.141190626, Training Accuracy: 67.520\n",
            "Worker 7, [08/08]: Training Loss: 1.104989320, Training Accuracy: 67.472\n",
            "Time taken for training worker 7: 0:00:25.289261\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.302368050, Training Accuracy: 61.936\n",
            "Worker 8, [02/08]: Training Loss: 1.284171609, Training Accuracy: 62.384\n",
            "Worker 8, [03/08]: Training Loss: 1.226418220, Training Accuracy: 64.288\n",
            "Worker 8, [04/08]: Training Loss: 1.197967353, Training Accuracy: 64.864\n",
            "Worker 8, [05/08]: Training Loss: 1.192489779, Training Accuracy: 65.344\n",
            "Worker 8, [06/08]: Training Loss: 1.148100292, Training Accuracy: 66.928\n",
            "Worker 8, [07/08]: Training Loss: 1.156913427, Training Accuracy: 66.096\n",
            "Worker 8, [08/08]: Training Loss: 1.112916917, Training Accuracy: 67.584\n",
            "Time taken for training worker 8: 0:00:25.245535\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004418\n",
            "Global Update 18: Test Loss: 2.245819785, Test Accuracy: 47.810\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:59:31.020101\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:16\n",
            "==================================================\n",
            "Worker 1, [01/16]: Training Loss: 4.596959844, Training Accuracy: 1.392\n",
            "Worker 1, [02/16]: Training Loss: 4.401985631, Training Accuracy: 3.472\n",
            "Worker 1, [03/16]: Training Loss: 4.148002191, Training Accuracy: 5.696\n",
            "Worker 1, [04/16]: Training Loss: 4.012693865, Training Accuracy: 7.120\n",
            "Worker 1, [05/16]: Training Loss: 3.892691301, Training Accuracy: 8.992\n",
            "Worker 1, [06/16]: Training Loss: 3.797984722, Training Accuracy: 10.304\n",
            "Worker 1, [07/16]: Training Loss: 3.710957184, Training Accuracy: 12.128\n",
            "Worker 1, [08/16]: Training Loss: 3.645823265, Training Accuracy: 12.928\n",
            "Worker 1, [09/16]: Training Loss: 3.570685211, Training Accuracy: 13.792\n",
            "Worker 1, [10/16]: Training Loss: 3.488552626, Training Accuracy: 15.968\n",
            "Worker 1, [11/16]: Training Loss: 3.411120911, Training Accuracy: 16.672\n",
            "Worker 1, [12/16]: Training Loss: 3.338209539, Training Accuracy: 17.392\n",
            "Worker 1, [13/16]: Training Loss: 3.284087821, Training Accuracy: 19.184\n",
            "Worker 1, [14/16]: Training Loss: 3.199930356, Training Accuracy: 20.304\n",
            "Worker 1, [15/16]: Training Loss: 3.144935664, Training Accuracy: 20.880\n",
            "Worker 1, [16/16]: Training Loss: 3.070137438, Training Accuracy: 22.688\n",
            "Time taken for training worker 1: 0:00:48.438010\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 4.597241372, Training Accuracy: 1.712\n",
            "Worker 2, [02/16]: Training Loss: 4.432430044, Training Accuracy: 3.408\n",
            "Worker 2, [03/16]: Training Loss: 4.207832395, Training Accuracy: 5.536\n",
            "Worker 2, [04/16]: Training Loss: 4.038445285, Training Accuracy: 7.792\n",
            "Worker 2, [05/16]: Training Loss: 3.913189577, Training Accuracy: 9.920\n",
            "Worker 2, [06/16]: Training Loss: 3.798207003, Training Accuracy: 10.832\n",
            "Worker 2, [07/16]: Training Loss: 3.751792594, Training Accuracy: 12.048\n",
            "Worker 2, [08/16]: Training Loss: 3.645216674, Training Accuracy: 13.664\n",
            "Worker 2, [09/16]: Training Loss: 3.543110105, Training Accuracy: 14.944\n",
            "Worker 2, [10/16]: Training Loss: 3.486019755, Training Accuracy: 16.320\n",
            "Worker 2, [11/16]: Training Loss: 3.398276198, Training Accuracy: 17.504\n",
            "Worker 2, [12/16]: Training Loss: 3.312543351, Training Accuracy: 19.104\n",
            "Worker 2, [13/16]: Training Loss: 3.270195392, Training Accuracy: 19.408\n",
            "Worker 2, [14/16]: Training Loss: 3.182263425, Training Accuracy: 20.608\n",
            "Worker 2, [15/16]: Training Loss: 3.109702285, Training Accuracy: 22.320\n",
            "Worker 2, [16/16]: Training Loss: 3.052775707, Training Accuracy: 23.104\n",
            "Time taken for training worker 2: 0:00:51.216261\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 4.595883666, Training Accuracy: 1.568\n",
            "Worker 3, [02/16]: Training Loss: 4.414752513, Training Accuracy: 3.328\n",
            "Worker 3, [03/16]: Training Loss: 4.174665916, Training Accuracy: 5.648\n",
            "Worker 3, [04/16]: Training Loss: 4.049706167, Training Accuracy: 7.552\n",
            "Worker 3, [05/16]: Training Loss: 3.935697419, Training Accuracy: 9.040\n",
            "Worker 3, [06/16]: Training Loss: 3.832767331, Training Accuracy: 10.320\n",
            "Worker 3, [07/16]: Training Loss: 3.744249317, Training Accuracy: 11.760\n",
            "Worker 3, [08/16]: Training Loss: 3.661843519, Training Accuracy: 12.736\n",
            "Worker 3, [09/16]: Training Loss: 3.572930003, Training Accuracy: 14.336\n",
            "Worker 3, [10/16]: Training Loss: 3.498919901, Training Accuracy: 15.712\n",
            "Worker 3, [11/16]: Training Loss: 3.418821014, Training Accuracy: 16.736\n",
            "Worker 3, [12/16]: Training Loss: 3.361105123, Training Accuracy: 18.544\n",
            "Worker 3, [13/16]: Training Loss: 3.285670307, Training Accuracy: 18.688\n",
            "Worker 3, [14/16]: Training Loss: 3.215635815, Training Accuracy: 20.768\n",
            "Worker 3, [15/16]: Training Loss: 3.159194066, Training Accuracy: 21.184\n",
            "Worker 3, [16/16]: Training Loss: 3.098427945, Training Accuracy: 22.576\n",
            "Time taken for training worker 3: 0:00:51.238943\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 4.595527688, Training Accuracy: 1.328\n",
            "Worker 4, [02/16]: Training Loss: 4.399898709, Training Accuracy: 3.360\n",
            "Worker 4, [03/16]: Training Loss: 4.154640731, Training Accuracy: 5.872\n",
            "Worker 4, [04/16]: Training Loss: 4.028153446, Training Accuracy: 7.456\n",
            "Worker 4, [05/16]: Training Loss: 3.936955053, Training Accuracy: 8.416\n",
            "Worker 4, [06/16]: Training Loss: 3.822875578, Training Accuracy: 10.080\n",
            "Worker 4, [07/16]: Training Loss: 3.727906293, Training Accuracy: 11.792\n",
            "Worker 4, [08/16]: Training Loss: 3.638452070, Training Accuracy: 12.512\n",
            "Worker 4, [09/16]: Training Loss: 3.579932337, Training Accuracy: 13.936\n",
            "Worker 4, [10/16]: Training Loss: 3.488601699, Training Accuracy: 15.552\n",
            "Worker 4, [11/16]: Training Loss: 3.434416596, Training Accuracy: 16.816\n",
            "Worker 4, [12/16]: Training Loss: 3.340619245, Training Accuracy: 18.032\n",
            "Worker 4, [13/16]: Training Loss: 3.261247849, Training Accuracy: 19.744\n",
            "Worker 4, [14/16]: Training Loss: 3.185292945, Training Accuracy: 20.992\n",
            "Worker 4, [15/16]: Training Loss: 3.117469965, Training Accuracy: 21.568\n",
            "Worker 4, [16/16]: Training Loss: 3.046703271, Training Accuracy: 23.104\n",
            "Time taken for training worker 4: 0:00:48.980325\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 4.592999215, Training Accuracy: 1.840\n",
            "Worker 5, [02/16]: Training Loss: 4.394938941, Training Accuracy: 3.856\n",
            "Worker 5, [03/16]: Training Loss: 4.190009356, Training Accuracy: 5.072\n",
            "Worker 5, [04/16]: Training Loss: 4.053767786, Training Accuracy: 6.768\n",
            "Worker 5, [05/16]: Training Loss: 3.951874295, Training Accuracy: 8.608\n",
            "Worker 5, [06/16]: Training Loss: 3.868868412, Training Accuracy: 9.584\n",
            "Worker 5, [07/16]: Training Loss: 3.780989377, Training Accuracy: 11.696\n",
            "Worker 5, [08/16]: Training Loss: 3.687064979, Training Accuracy: 12.576\n",
            "Worker 5, [09/16]: Training Loss: 3.628788885, Training Accuracy: 13.072\n",
            "Worker 5, [10/16]: Training Loss: 3.535851238, Training Accuracy: 14.432\n",
            "Worker 5, [11/16]: Training Loss: 3.451939201, Training Accuracy: 16.720\n",
            "Worker 5, [12/16]: Training Loss: 3.408108826, Training Accuracy: 17.264\n",
            "Worker 5, [13/16]: Training Loss: 3.337886226, Training Accuracy: 17.808\n",
            "Worker 5, [14/16]: Training Loss: 3.224464421, Training Accuracy: 20.896\n",
            "Worker 5, [15/16]: Training Loss: 3.173807397, Training Accuracy: 20.944\n",
            "Worker 5, [16/16]: Training Loss: 3.099170894, Training Accuracy: 21.840\n",
            "Time taken for training worker 5: 0:00:49.404992\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 4.595370911, Training Accuracy: 2.048\n",
            "Worker 6, [02/16]: Training Loss: 4.409724236, Training Accuracy: 3.568\n",
            "Worker 6, [03/16]: Training Loss: 4.184485774, Training Accuracy: 5.936\n",
            "Worker 6, [04/16]: Training Loss: 4.050515279, Training Accuracy: 6.912\n",
            "Worker 6, [05/16]: Training Loss: 3.944684574, Training Accuracy: 8.800\n",
            "Worker 6, [06/16]: Training Loss: 3.830631835, Training Accuracy: 10.384\n",
            "Worker 6, [07/16]: Training Loss: 3.744006293, Training Accuracy: 11.648\n",
            "Worker 6, [08/16]: Training Loss: 3.668668346, Training Accuracy: 12.608\n",
            "Worker 6, [09/16]: Training Loss: 3.578261480, Training Accuracy: 14.064\n",
            "Worker 6, [10/16]: Training Loss: 3.504453533, Training Accuracy: 15.488\n",
            "Worker 6, [11/16]: Training Loss: 3.424040084, Training Accuracy: 16.992\n",
            "Worker 6, [12/16]: Training Loss: 3.351948398, Training Accuracy: 17.888\n",
            "Worker 6, [13/16]: Training Loss: 3.287923280, Training Accuracy: 19.376\n",
            "Worker 6, [14/16]: Training Loss: 3.206219435, Training Accuracy: 19.984\n",
            "Worker 6, [15/16]: Training Loss: 3.161081317, Training Accuracy: 21.296\n",
            "Worker 6, [16/16]: Training Loss: 3.102188359, Training Accuracy: 22.064\n",
            "Time taken for training worker 6: 0:00:48.431156\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 4.597055343, Training Accuracy: 1.792\n",
            "Worker 7, [02/16]: Training Loss: 4.429426801, Training Accuracy: 3.280\n",
            "Worker 7, [03/16]: Training Loss: 4.198162952, Training Accuracy: 5.648\n",
            "Worker 7, [04/16]: Training Loss: 4.056349465, Training Accuracy: 7.296\n",
            "Worker 7, [05/16]: Training Loss: 3.952072148, Training Accuracy: 8.864\n",
            "Worker 7, [06/16]: Training Loss: 3.847347342, Training Accuracy: 9.968\n",
            "Worker 7, [07/16]: Training Loss: 3.756950213, Training Accuracy: 11.760\n",
            "Worker 7, [08/16]: Training Loss: 3.679521738, Training Accuracy: 12.704\n",
            "Worker 7, [09/16]: Training Loss: 3.593245509, Training Accuracy: 13.920\n",
            "Worker 7, [10/16]: Training Loss: 3.517279177, Training Accuracy: 15.312\n",
            "Worker 7, [11/16]: Training Loss: 3.456487573, Training Accuracy: 15.968\n",
            "Worker 7, [12/16]: Training Loss: 3.373189795, Training Accuracy: 17.968\n",
            "Worker 7, [13/16]: Training Loss: 3.313792105, Training Accuracy: 18.736\n",
            "Worker 7, [14/16]: Training Loss: 3.245563675, Training Accuracy: 19.744\n",
            "Worker 7, [15/16]: Training Loss: 3.152870723, Training Accuracy: 21.232\n",
            "Worker 7, [16/16]: Training Loss: 3.126516620, Training Accuracy: 21.872\n",
            "Time taken for training worker 7: 0:00:50.260266\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 4.595519616, Training Accuracy: 1.968\n",
            "Worker 8, [02/16]: Training Loss: 4.405646743, Training Accuracy: 3.456\n",
            "Worker 8, [03/16]: Training Loss: 4.172808652, Training Accuracy: 5.600\n",
            "Worker 8, [04/16]: Training Loss: 4.035676715, Training Accuracy: 7.232\n",
            "Worker 8, [05/16]: Training Loss: 3.917043910, Training Accuracy: 8.848\n",
            "Worker 8, [06/16]: Training Loss: 3.806158835, Training Accuracy: 10.560\n",
            "Worker 8, [07/16]: Training Loss: 3.722768886, Training Accuracy: 11.920\n",
            "Worker 8, [08/16]: Training Loss: 3.678366921, Training Accuracy: 12.208\n",
            "Worker 8, [09/16]: Training Loss: 3.587469962, Training Accuracy: 13.456\n",
            "Worker 8, [10/16]: Training Loss: 3.517841118, Training Accuracy: 14.816\n",
            "Worker 8, [11/16]: Training Loss: 3.438315340, Training Accuracy: 15.376\n",
            "Worker 8, [12/16]: Training Loss: 3.372947776, Training Accuracy: 17.536\n",
            "Worker 8, [13/16]: Training Loss: 3.302505902, Training Accuracy: 18.432\n",
            "Worker 8, [14/16]: Training Loss: 3.253322913, Training Accuracy: 18.832\n",
            "Worker 8, [15/16]: Training Loss: 3.173516390, Training Accuracy: 20.736\n",
            "Worker 8, [16/16]: Training Loss: 3.098442469, Training Accuracy: 21.808\n",
            "Time taken for training worker 8: 0:00:47.988743\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003938\n",
            "Global Update 01: Test Loss: 3.545991041, Test Accuracy: 20.960\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 3.386643388, Training Accuracy: 17.968\n",
            "Worker 1, [02/16]: Training Loss: 3.237471325, Training Accuracy: 19.936\n",
            "Worker 1, [03/16]: Training Loss: 3.147937787, Training Accuracy: 21.504\n",
            "Worker 1, [04/16]: Training Loss: 3.057889812, Training Accuracy: 22.992\n",
            "Worker 1, [05/16]: Training Loss: 3.016403855, Training Accuracy: 23.904\n",
            "Worker 1, [06/16]: Training Loss: 2.888589660, Training Accuracy: 26.016\n",
            "Worker 1, [07/16]: Training Loss: 2.845543219, Training Accuracy: 26.672\n",
            "Worker 1, [08/16]: Training Loss: 2.775930757, Training Accuracy: 27.936\n",
            "Worker 1, [09/16]: Training Loss: 2.701591677, Training Accuracy: 29.680\n",
            "Worker 1, [10/16]: Training Loss: 2.657022019, Training Accuracy: 30.560\n",
            "Worker 1, [11/16]: Training Loss: 2.580173925, Training Accuracy: 32.112\n",
            "Worker 1, [12/16]: Training Loss: 2.532251542, Training Accuracy: 32.976\n",
            "Worker 1, [13/16]: Training Loss: 2.470726608, Training Accuracy: 33.792\n",
            "Worker 1, [14/16]: Training Loss: 2.431645021, Training Accuracy: 34.544\n",
            "Worker 1, [15/16]: Training Loss: 2.346197814, Training Accuracy: 37.520\n",
            "Worker 1, [16/16]: Training Loss: 2.311623094, Training Accuracy: 37.712\n",
            "Time taken for training worker 1: 0:00:49.434886\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 3.400541274, Training Accuracy: 18.064\n",
            "Worker 2, [02/16]: Training Loss: 3.245607933, Training Accuracy: 20.544\n",
            "Worker 2, [03/16]: Training Loss: 3.137731367, Training Accuracy: 22.832\n",
            "Worker 2, [04/16]: Training Loss: 3.079420649, Training Accuracy: 23.104\n",
            "Worker 2, [05/16]: Training Loss: 2.997803053, Training Accuracy: 24.272\n",
            "Worker 2, [06/16]: Training Loss: 2.941631222, Training Accuracy: 25.664\n",
            "Worker 2, [07/16]: Training Loss: 2.848483436, Training Accuracy: 27.664\n",
            "Worker 2, [08/16]: Training Loss: 2.762446579, Training Accuracy: 28.688\n",
            "Worker 2, [09/16]: Training Loss: 2.727798440, Training Accuracy: 28.928\n",
            "Worker 2, [10/16]: Training Loss: 2.680255131, Training Accuracy: 30.560\n",
            "Worker 2, [11/16]: Training Loss: 2.593416236, Training Accuracy: 31.840\n",
            "Worker 2, [12/16]: Training Loss: 2.526810155, Training Accuracy: 33.232\n",
            "Worker 2, [13/16]: Training Loss: 2.461002984, Training Accuracy: 34.032\n",
            "Worker 2, [14/16]: Training Loss: 2.403894646, Training Accuracy: 35.584\n",
            "Worker 2, [15/16]: Training Loss: 2.385747591, Training Accuracy: 36.368\n",
            "Worker 2, [16/16]: Training Loss: 2.304593766, Training Accuracy: 38.608\n",
            "Time taken for training worker 2: 0:00:49.110225\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 3.403253005, Training Accuracy: 18.768\n",
            "Worker 3, [02/16]: Training Loss: 3.270836932, Training Accuracy: 20.240\n",
            "Worker 3, [03/16]: Training Loss: 3.186956457, Training Accuracy: 21.072\n",
            "Worker 3, [04/16]: Training Loss: 3.065345351, Training Accuracy: 23.536\n",
            "Worker 3, [05/16]: Training Loss: 3.018591903, Training Accuracy: 24.816\n",
            "Worker 3, [06/16]: Training Loss: 2.909081931, Training Accuracy: 25.888\n",
            "Worker 3, [07/16]: Training Loss: 2.860881136, Training Accuracy: 26.864\n",
            "Worker 3, [08/16]: Training Loss: 2.797051836, Training Accuracy: 28.832\n",
            "Worker 3, [09/16]: Training Loss: 2.747080825, Training Accuracy: 29.328\n",
            "Worker 3, [10/16]: Training Loss: 2.672584220, Training Accuracy: 30.032\n",
            "Worker 3, [11/16]: Training Loss: 2.629326056, Training Accuracy: 30.672\n",
            "Worker 3, [12/16]: Training Loss: 2.561256788, Training Accuracy: 32.352\n",
            "Worker 3, [13/16]: Training Loss: 2.476903259, Training Accuracy: 34.016\n",
            "Worker 3, [14/16]: Training Loss: 2.407680022, Training Accuracy: 36.096\n",
            "Worker 3, [15/16]: Training Loss: 2.382862534, Training Accuracy: 36.192\n",
            "Worker 3, [16/16]: Training Loss: 2.320314525, Training Accuracy: 37.856\n",
            "Time taken for training worker 3: 0:00:49.819423\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 3.397527490, Training Accuracy: 18.000\n",
            "Worker 4, [02/16]: Training Loss: 3.254565061, Training Accuracy: 19.952\n",
            "Worker 4, [03/16]: Training Loss: 3.166362106, Training Accuracy: 21.376\n",
            "Worker 4, [04/16]: Training Loss: 3.074060187, Training Accuracy: 23.488\n",
            "Worker 4, [05/16]: Training Loss: 3.023276331, Training Accuracy: 24.752\n",
            "Worker 4, [06/16]: Training Loss: 2.914355288, Training Accuracy: 26.016\n",
            "Worker 4, [07/16]: Training Loss: 2.866749326, Training Accuracy: 26.784\n",
            "Worker 4, [08/16]: Training Loss: 2.804845387, Training Accuracy: 27.232\n",
            "Worker 4, [09/16]: Training Loss: 2.736630277, Training Accuracy: 29.376\n",
            "Worker 4, [10/16]: Training Loss: 2.676102108, Training Accuracy: 31.120\n",
            "Worker 4, [11/16]: Training Loss: 2.593122855, Training Accuracy: 31.968\n",
            "Worker 4, [12/16]: Training Loss: 2.540248516, Training Accuracy: 33.248\n",
            "Worker 4, [13/16]: Training Loss: 2.468552061, Training Accuracy: 34.720\n",
            "Worker 4, [14/16]: Training Loss: 2.418496260, Training Accuracy: 35.488\n",
            "Worker 4, [15/16]: Training Loss: 2.372322471, Training Accuracy: 36.896\n",
            "Worker 4, [16/16]: Training Loss: 2.312696214, Training Accuracy: 37.648\n",
            "Time taken for training worker 4: 0:00:50.459178\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 3.414674083, Training Accuracy: 17.680\n",
            "Worker 5, [02/16]: Training Loss: 3.269681249, Training Accuracy: 20.080\n",
            "Worker 5, [03/16]: Training Loss: 3.185764089, Training Accuracy: 20.880\n",
            "Worker 5, [04/16]: Training Loss: 3.083189115, Training Accuracy: 23.376\n",
            "Worker 5, [05/16]: Training Loss: 3.024370665, Training Accuracy: 24.336\n",
            "Worker 5, [06/16]: Training Loss: 2.957633160, Training Accuracy: 25.744\n",
            "Worker 5, [07/16]: Training Loss: 2.883382678, Training Accuracy: 26.688\n",
            "Worker 5, [08/16]: Training Loss: 2.814461711, Training Accuracy: 27.808\n",
            "Worker 5, [09/16]: Training Loss: 2.741681620, Training Accuracy: 28.992\n",
            "Worker 5, [10/16]: Training Loss: 2.653051031, Training Accuracy: 31.312\n",
            "Worker 5, [11/16]: Training Loss: 2.619497353, Training Accuracy: 32.032\n",
            "Worker 5, [12/16]: Training Loss: 2.542489942, Training Accuracy: 33.344\n",
            "Worker 5, [13/16]: Training Loss: 2.508376927, Training Accuracy: 34.064\n",
            "Worker 5, [14/16]: Training Loss: 2.436220344, Training Accuracy: 34.752\n",
            "Worker 5, [15/16]: Training Loss: 2.383428761, Training Accuracy: 36.560\n",
            "Worker 5, [16/16]: Training Loss: 2.293066358, Training Accuracy: 38.528\n",
            "Time taken for training worker 5: 0:00:48.711965\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 3.410828783, Training Accuracy: 17.472\n",
            "Worker 6, [02/16]: Training Loss: 3.268562952, Training Accuracy: 19.840\n",
            "Worker 6, [03/16]: Training Loss: 3.200493163, Training Accuracy: 20.992\n",
            "Worker 6, [04/16]: Training Loss: 3.123940193, Training Accuracy: 22.336\n",
            "Worker 6, [05/16]: Training Loss: 3.039757541, Training Accuracy: 23.712\n",
            "Worker 6, [06/16]: Training Loss: 2.943358660, Training Accuracy: 25.280\n",
            "Worker 6, [07/16]: Training Loss: 2.883649906, Training Accuracy: 26.752\n",
            "Worker 6, [08/16]: Training Loss: 2.825587073, Training Accuracy: 26.848\n",
            "Worker 6, [09/16]: Training Loss: 2.755747343, Training Accuracy: 28.320\n",
            "Worker 6, [10/16]: Training Loss: 2.672163958, Training Accuracy: 30.496\n",
            "Worker 6, [11/16]: Training Loss: 2.639117560, Training Accuracy: 30.272\n",
            "Worker 6, [12/16]: Training Loss: 2.541794745, Training Accuracy: 33.456\n",
            "Worker 6, [13/16]: Training Loss: 2.487025535, Training Accuracy: 33.152\n",
            "Worker 6, [14/16]: Training Loss: 2.417832798, Training Accuracy: 35.520\n",
            "Worker 6, [15/16]: Training Loss: 2.372043998, Training Accuracy: 37.248\n",
            "Worker 6, [16/16]: Training Loss: 2.323736058, Training Accuracy: 37.440\n",
            "Time taken for training worker 6: 0:00:48.921118\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 3.419510771, Training Accuracy: 17.584\n",
            "Worker 7, [02/16]: Training Loss: 3.281459529, Training Accuracy: 19.712\n",
            "Worker 7, [03/16]: Training Loss: 3.211922064, Training Accuracy: 20.944\n",
            "Worker 7, [04/16]: Training Loss: 3.096107870, Training Accuracy: 22.768\n",
            "Worker 7, [05/16]: Training Loss: 3.023732103, Training Accuracy: 23.904\n",
            "Worker 7, [06/16]: Training Loss: 2.940830423, Training Accuracy: 25.360\n",
            "Worker 7, [07/16]: Training Loss: 2.851803721, Training Accuracy: 26.928\n",
            "Worker 7, [08/16]: Training Loss: 2.826700060, Training Accuracy: 27.776\n",
            "Worker 7, [09/16]: Training Loss: 2.739070092, Training Accuracy: 28.432\n",
            "Worker 7, [10/16]: Training Loss: 2.688680218, Training Accuracy: 29.952\n",
            "Worker 7, [11/16]: Training Loss: 2.652023345, Training Accuracy: 30.832\n",
            "Worker 7, [12/16]: Training Loss: 2.542347918, Training Accuracy: 32.704\n",
            "Worker 7, [13/16]: Training Loss: 2.515170161, Training Accuracy: 33.424\n",
            "Worker 7, [14/16]: Training Loss: 2.418572588, Training Accuracy: 35.712\n",
            "Worker 7, [15/16]: Training Loss: 2.363000702, Training Accuracy: 36.416\n",
            "Worker 7, [16/16]: Training Loss: 2.329076024, Training Accuracy: 37.024\n",
            "Time taken for training worker 7: 0:00:49.009373\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 3.424755415, Training Accuracy: 17.536\n",
            "Worker 8, [02/16]: Training Loss: 3.277592956, Training Accuracy: 19.696\n",
            "Worker 8, [03/16]: Training Loss: 3.180381291, Training Accuracy: 20.672\n",
            "Worker 8, [04/16]: Training Loss: 3.096330171, Training Accuracy: 22.208\n",
            "Worker 8, [05/16]: Training Loss: 3.019095467, Training Accuracy: 23.696\n",
            "Worker 8, [06/16]: Training Loss: 2.954619305, Training Accuracy: 24.976\n",
            "Worker 8, [07/16]: Training Loss: 2.871764419, Training Accuracy: 25.520\n",
            "Worker 8, [08/16]: Training Loss: 2.822592314, Training Accuracy: 27.760\n",
            "Worker 8, [09/16]: Training Loss: 2.732370406, Training Accuracy: 29.536\n",
            "Worker 8, [10/16]: Training Loss: 2.703819752, Training Accuracy: 29.376\n",
            "Worker 8, [11/16]: Training Loss: 2.658033738, Training Accuracy: 30.352\n",
            "Worker 8, [12/16]: Training Loss: 2.573495568, Training Accuracy: 32.304\n",
            "Worker 8, [13/16]: Training Loss: 2.505525025, Training Accuracy: 33.200\n",
            "Worker 8, [14/16]: Training Loss: 2.422981167, Training Accuracy: 34.576\n",
            "Worker 8, [15/16]: Training Loss: 2.396080421, Training Accuracy: 35.920\n",
            "Worker 8, [16/16]: Training Loss: 2.324208223, Training Accuracy: 36.928\n",
            "Time taken for training worker 8: 0:00:49.187693\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004661\n",
            "Global Update 02: Test Loss: 3.375913969, Test Accuracy: 30.390\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.912248928, Training Accuracy: 27.280\n",
            "Worker 1, [02/16]: Training Loss: 2.664154710, Training Accuracy: 31.344\n",
            "Worker 1, [03/16]: Training Loss: 2.579727946, Training Accuracy: 32.672\n",
            "Worker 1, [04/16]: Training Loss: 2.455884199, Training Accuracy: 35.312\n",
            "Worker 1, [05/16]: Training Loss: 2.362576735, Training Accuracy: 37.168\n",
            "Worker 1, [06/16]: Training Loss: 2.325850155, Training Accuracy: 37.888\n",
            "Worker 1, [07/16]: Training Loss: 2.226969619, Training Accuracy: 39.376\n",
            "Worker 1, [08/16]: Training Loss: 2.157605940, Training Accuracy: 41.328\n",
            "Worker 1, [09/16]: Training Loss: 2.095106105, Training Accuracy: 42.320\n",
            "Worker 1, [10/16]: Training Loss: 2.049342036, Training Accuracy: 43.728\n",
            "Worker 1, [11/16]: Training Loss: 1.960057357, Training Accuracy: 45.776\n",
            "Worker 1, [12/16]: Training Loss: 1.935448692, Training Accuracy: 45.760\n",
            "Worker 1, [13/16]: Training Loss: 1.835262254, Training Accuracy: 47.600\n",
            "Worker 1, [14/16]: Training Loss: 1.810988480, Training Accuracy: 48.880\n",
            "Worker 1, [15/16]: Training Loss: 1.797424178, Training Accuracy: 49.232\n",
            "Worker 1, [16/16]: Training Loss: 1.673533037, Training Accuracy: 51.488\n",
            "Time taken for training worker 1: 0:00:47.701176\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.953487348, Training Accuracy: 27.600\n",
            "Worker 2, [02/16]: Training Loss: 2.700033801, Training Accuracy: 30.720\n",
            "Worker 2, [03/16]: Training Loss: 2.568169927, Training Accuracy: 32.992\n",
            "Worker 2, [04/16]: Training Loss: 2.473647509, Training Accuracy: 34.672\n",
            "Worker 2, [05/16]: Training Loss: 2.376656755, Training Accuracy: 37.792\n",
            "Worker 2, [06/16]: Training Loss: 2.308279609, Training Accuracy: 38.096\n",
            "Worker 2, [07/16]: Training Loss: 2.222669678, Training Accuracy: 39.216\n",
            "Worker 2, [08/16]: Training Loss: 2.151089865, Training Accuracy: 41.408\n",
            "Worker 2, [09/16]: Training Loss: 2.090783595, Training Accuracy: 41.920\n",
            "Worker 2, [10/16]: Training Loss: 2.058442769, Training Accuracy: 43.232\n",
            "Worker 2, [11/16]: Training Loss: 1.957411417, Training Accuracy: 46.048\n",
            "Worker 2, [12/16]: Training Loss: 1.906085680, Training Accuracy: 46.864\n",
            "Worker 2, [13/16]: Training Loss: 1.856301355, Training Accuracy: 47.392\n",
            "Worker 2, [14/16]: Training Loss: 1.818451883, Training Accuracy: 48.752\n",
            "Worker 2, [15/16]: Training Loss: 1.748507113, Training Accuracy: 50.208\n",
            "Worker 2, [16/16]: Training Loss: 1.693612964, Training Accuracy: 51.936\n",
            "Time taken for training worker 2: 0:00:49.404046\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.927118973, Training Accuracy: 26.992\n",
            "Worker 3, [02/16]: Training Loss: 2.706156721, Training Accuracy: 30.320\n",
            "Worker 3, [03/16]: Training Loss: 2.573150443, Training Accuracy: 32.944\n",
            "Worker 3, [04/16]: Training Loss: 2.479902371, Training Accuracy: 34.208\n",
            "Worker 3, [05/16]: Training Loss: 2.374255980, Training Accuracy: 36.784\n",
            "Worker 3, [06/16]: Training Loss: 2.310502680, Training Accuracy: 38.128\n",
            "Worker 3, [07/16]: Training Loss: 2.246513413, Training Accuracy: 39.152\n",
            "Worker 3, [08/16]: Training Loss: 2.165675714, Training Accuracy: 41.152\n",
            "Worker 3, [09/16]: Training Loss: 2.105904074, Training Accuracy: 42.464\n",
            "Worker 3, [10/16]: Training Loss: 2.058901494, Training Accuracy: 43.328\n",
            "Worker 3, [11/16]: Training Loss: 1.988807250, Training Accuracy: 45.376\n",
            "Worker 3, [12/16]: Training Loss: 1.901676409, Training Accuracy: 46.592\n",
            "Worker 3, [13/16]: Training Loss: 1.882102199, Training Accuracy: 48.016\n",
            "Worker 3, [14/16]: Training Loss: 1.823053387, Training Accuracy: 48.816\n",
            "Worker 3, [15/16]: Training Loss: 1.742361151, Training Accuracy: 51.728\n",
            "Worker 3, [16/16]: Training Loss: 1.709360510, Training Accuracy: 51.296\n",
            "Time taken for training worker 3: 0:00:48.727820\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.947540466, Training Accuracy: 26.016\n",
            "Worker 4, [02/16]: Training Loss: 2.713693380, Training Accuracy: 30.608\n",
            "Worker 4, [03/16]: Training Loss: 2.579314227, Training Accuracy: 32.672\n",
            "Worker 4, [04/16]: Training Loss: 2.474643851, Training Accuracy: 34.480\n",
            "Worker 4, [05/16]: Training Loss: 2.401366849, Training Accuracy: 36.384\n",
            "Worker 4, [06/16]: Training Loss: 2.287777559, Training Accuracy: 38.592\n",
            "Worker 4, [07/16]: Training Loss: 2.228480858, Training Accuracy: 39.744\n",
            "Worker 4, [08/16]: Training Loss: 2.171143981, Training Accuracy: 40.480\n",
            "Worker 4, [09/16]: Training Loss: 2.153377638, Training Accuracy: 41.360\n",
            "Worker 4, [10/16]: Training Loss: 2.046729359, Training Accuracy: 43.056\n",
            "Worker 4, [11/16]: Training Loss: 2.029533456, Training Accuracy: 43.952\n",
            "Worker 4, [12/16]: Training Loss: 1.923960104, Training Accuracy: 45.792\n",
            "Worker 4, [13/16]: Training Loss: 1.839424038, Training Accuracy: 48.544\n",
            "Worker 4, [14/16]: Training Loss: 1.816001740, Training Accuracy: 48.928\n",
            "Worker 4, [15/16]: Training Loss: 1.739722563, Training Accuracy: 50.048\n",
            "Worker 4, [16/16]: Training Loss: 1.700003746, Training Accuracy: 52.032\n",
            "Time taken for training worker 4: 0:00:49.911284\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 2.947500852, Training Accuracy: 27.184\n",
            "Worker 5, [02/16]: Training Loss: 2.701541993, Training Accuracy: 30.640\n",
            "Worker 5, [03/16]: Training Loss: 2.585121505, Training Accuracy: 32.560\n",
            "Worker 5, [04/16]: Training Loss: 2.478098372, Training Accuracy: 35.248\n",
            "Worker 5, [05/16]: Training Loss: 2.366500578, Training Accuracy: 36.816\n",
            "Worker 5, [06/16]: Training Loss: 2.307601684, Training Accuracy: 38.352\n",
            "Worker 5, [07/16]: Training Loss: 2.233132735, Training Accuracy: 40.256\n",
            "Worker 5, [08/16]: Training Loss: 2.156586812, Training Accuracy: 40.896\n",
            "Worker 5, [09/16]: Training Loss: 2.090807808, Training Accuracy: 42.256\n",
            "Worker 5, [10/16]: Training Loss: 2.034432718, Training Accuracy: 43.872\n",
            "Worker 5, [11/16]: Training Loss: 1.961674971, Training Accuracy: 45.648\n",
            "Worker 5, [12/16]: Training Loss: 1.926039442, Training Accuracy: 46.192\n",
            "Worker 5, [13/16]: Training Loss: 1.861901590, Training Accuracy: 48.496\n",
            "Worker 5, [14/16]: Training Loss: 1.792690282, Training Accuracy: 48.496\n",
            "Worker 5, [15/16]: Training Loss: 1.800068504, Training Accuracy: 48.976\n",
            "Worker 5, [16/16]: Training Loss: 1.699220196, Training Accuracy: 51.760\n",
            "Time taken for training worker 5: 0:00:49.997442\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 2.942399441, Training Accuracy: 27.040\n",
            "Worker 6, [02/16]: Training Loss: 2.708719543, Training Accuracy: 30.448\n",
            "Worker 6, [03/16]: Training Loss: 2.585185241, Training Accuracy: 32.928\n",
            "Worker 6, [04/16]: Training Loss: 2.486298522, Training Accuracy: 34.352\n",
            "Worker 6, [05/16]: Training Loss: 2.407827988, Training Accuracy: 35.952\n",
            "Worker 6, [06/16]: Training Loss: 2.333417200, Training Accuracy: 37.664\n",
            "Worker 6, [07/16]: Training Loss: 2.255519452, Training Accuracy: 39.904\n",
            "Worker 6, [08/16]: Training Loss: 2.193818881, Training Accuracy: 40.400\n",
            "Worker 6, [09/16]: Training Loss: 2.114415714, Training Accuracy: 41.632\n",
            "Worker 6, [10/16]: Training Loss: 2.060290730, Training Accuracy: 43.456\n",
            "Worker 6, [11/16]: Training Loss: 2.010957933, Training Accuracy: 44.656\n",
            "Worker 6, [12/16]: Training Loss: 1.965744019, Training Accuracy: 45.664\n",
            "Worker 6, [13/16]: Training Loss: 1.868146035, Training Accuracy: 47.584\n",
            "Worker 6, [14/16]: Training Loss: 1.824132384, Training Accuracy: 48.464\n",
            "Worker 6, [15/16]: Training Loss: 1.799204296, Training Accuracy: 48.960\n",
            "Worker 6, [16/16]: Training Loss: 1.717987164, Training Accuracy: 50.992\n",
            "Time taken for training worker 6: 0:00:48.218863\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 2.944120456, Training Accuracy: 26.768\n",
            "Worker 7, [02/16]: Training Loss: 2.698875965, Training Accuracy: 31.312\n",
            "Worker 7, [03/16]: Training Loss: 2.610505503, Training Accuracy: 32.272\n",
            "Worker 7, [04/16]: Training Loss: 2.500841705, Training Accuracy: 34.048\n",
            "Worker 7, [05/16]: Training Loss: 2.405871098, Training Accuracy: 36.464\n",
            "Worker 7, [06/16]: Training Loss: 2.321693931, Training Accuracy: 37.808\n",
            "Worker 7, [07/16]: Training Loss: 2.266185072, Training Accuracy: 38.352\n",
            "Worker 7, [08/16]: Training Loss: 2.191056028, Training Accuracy: 41.040\n",
            "Worker 7, [09/16]: Training Loss: 2.105834170, Training Accuracy: 41.728\n",
            "Worker 7, [10/16]: Training Loss: 2.057851670, Training Accuracy: 43.136\n",
            "Worker 7, [11/16]: Training Loss: 1.978441123, Training Accuracy: 45.568\n",
            "Worker 7, [12/16]: Training Loss: 1.969116295, Training Accuracy: 45.872\n",
            "Worker 7, [13/16]: Training Loss: 1.894750444, Training Accuracy: 46.832\n",
            "Worker 7, [14/16]: Training Loss: 1.820203647, Training Accuracy: 48.544\n",
            "Worker 7, [15/16]: Training Loss: 1.782166002, Training Accuracy: 50.208\n",
            "Worker 7, [16/16]: Training Loss: 1.721187163, Training Accuracy: 51.184\n",
            "Time taken for training worker 7: 0:00:49.250376\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 2.940044690, Training Accuracy: 27.168\n",
            "Worker 8, [02/16]: Training Loss: 2.716276711, Training Accuracy: 29.648\n",
            "Worker 8, [03/16]: Training Loss: 2.598484115, Training Accuracy: 32.368\n",
            "Worker 8, [04/16]: Training Loss: 2.494438968, Training Accuracy: 34.752\n",
            "Worker 8, [05/16]: Training Loss: 2.401067112, Training Accuracy: 36.240\n",
            "Worker 8, [06/16]: Training Loss: 2.329116495, Training Accuracy: 37.216\n",
            "Worker 8, [07/16]: Training Loss: 2.233733923, Training Accuracy: 39.760\n",
            "Worker 8, [08/16]: Training Loss: 2.182252509, Training Accuracy: 39.984\n",
            "Worker 8, [09/16]: Training Loss: 2.119195962, Training Accuracy: 41.600\n",
            "Worker 8, [10/16]: Training Loss: 2.059696938, Training Accuracy: 42.800\n",
            "Worker 8, [11/16]: Training Loss: 1.966380663, Training Accuracy: 45.840\n",
            "Worker 8, [12/16]: Training Loss: 1.881477009, Training Accuracy: 47.168\n",
            "Worker 8, [13/16]: Training Loss: 1.886632672, Training Accuracy: 46.864\n",
            "Worker 8, [14/16]: Training Loss: 1.819012719, Training Accuracy: 48.576\n",
            "Worker 8, [15/16]: Training Loss: 1.797464935, Training Accuracy: 48.960\n",
            "Worker 8, [16/16]: Training Loss: 1.726693510, Training Accuracy: 51.296\n",
            "Time taken for training worker 8: 0:00:48.532804\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004027\n",
            "Global Update 03: Test Loss: 3.447818083, Test Accuracy: 36.870\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.677847733, Training Accuracy: 33.520\n",
            "Worker 1, [02/16]: Training Loss: 2.320115780, Training Accuracy: 38.368\n",
            "Worker 1, [03/16]: Training Loss: 2.154684890, Training Accuracy: 42.464\n",
            "Worker 1, [04/16]: Training Loss: 2.045923533, Training Accuracy: 44.096\n",
            "Worker 1, [05/16]: Training Loss: 1.930910440, Training Accuracy: 46.544\n",
            "Worker 1, [06/16]: Training Loss: 1.854618833, Training Accuracy: 48.352\n",
            "Worker 1, [07/16]: Training Loss: 1.757130224, Training Accuracy: 50.240\n",
            "Worker 1, [08/16]: Training Loss: 1.723518851, Training Accuracy: 51.952\n",
            "Worker 1, [09/16]: Training Loss: 1.614088813, Training Accuracy: 54.096\n",
            "Worker 1, [10/16]: Training Loss: 1.565049586, Training Accuracy: 55.200\n",
            "Worker 1, [11/16]: Training Loss: 1.504469143, Training Accuracy: 56.944\n",
            "Worker 1, [12/16]: Training Loss: 1.462661257, Training Accuracy: 58.112\n",
            "Worker 1, [13/16]: Training Loss: 1.402545668, Training Accuracy: 59.680\n",
            "Worker 1, [14/16]: Training Loss: 1.345642974, Training Accuracy: 61.024\n",
            "Worker 1, [15/16]: Training Loss: 1.299537679, Training Accuracy: 62.912\n",
            "Worker 1, [16/16]: Training Loss: 1.259149045, Training Accuracy: 62.928\n",
            "Time taken for training worker 1: 0:00:49.759614\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.694062019, Training Accuracy: 33.632\n",
            "Worker 2, [02/16]: Training Loss: 2.329326451, Training Accuracy: 39.216\n",
            "Worker 2, [03/16]: Training Loss: 2.177835727, Training Accuracy: 41.632\n",
            "Worker 2, [04/16]: Training Loss: 2.030331891, Training Accuracy: 44.304\n",
            "Worker 2, [05/16]: Training Loss: 1.949457209, Training Accuracy: 46.304\n",
            "Worker 2, [06/16]: Training Loss: 1.845813263, Training Accuracy: 49.264\n",
            "Worker 2, [07/16]: Training Loss: 1.788987168, Training Accuracy: 50.144\n",
            "Worker 2, [08/16]: Training Loss: 1.687111705, Training Accuracy: 52.096\n",
            "Worker 2, [09/16]: Training Loss: 1.603639434, Training Accuracy: 53.840\n",
            "Worker 2, [10/16]: Training Loss: 1.589395432, Training Accuracy: 54.336\n",
            "Worker 2, [11/16]: Training Loss: 1.538715240, Training Accuracy: 55.104\n",
            "Worker 2, [12/16]: Training Loss: 1.415289695, Training Accuracy: 59.360\n",
            "Worker 2, [13/16]: Training Loss: 1.406113350, Training Accuracy: 59.136\n",
            "Worker 2, [14/16]: Training Loss: 1.368912131, Training Accuracy: 60.720\n",
            "Worker 2, [15/16]: Training Loss: 1.337621746, Training Accuracy: 60.480\n",
            "Worker 2, [16/16]: Training Loss: 1.293604463, Training Accuracy: 61.856\n",
            "Time taken for training worker 2: 0:00:50.014131\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.674216464, Training Accuracy: 32.640\n",
            "Worker 3, [02/16]: Training Loss: 2.340517752, Training Accuracy: 38.736\n",
            "Worker 3, [03/16]: Training Loss: 2.179458808, Training Accuracy: 41.888\n",
            "Worker 3, [04/16]: Training Loss: 2.019069189, Training Accuracy: 44.896\n",
            "Worker 3, [05/16]: Training Loss: 1.946826878, Training Accuracy: 46.768\n",
            "Worker 3, [06/16]: Training Loss: 1.839139987, Training Accuracy: 49.088\n",
            "Worker 3, [07/16]: Training Loss: 1.771379620, Training Accuracy: 49.680\n",
            "Worker 3, [08/16]: Training Loss: 1.679909559, Training Accuracy: 52.672\n",
            "Worker 3, [09/16]: Training Loss: 1.610435595, Training Accuracy: 54.704\n",
            "Worker 3, [10/16]: Training Loss: 1.562470760, Training Accuracy: 55.296\n",
            "Worker 3, [11/16]: Training Loss: 1.515124308, Training Accuracy: 56.512\n",
            "Worker 3, [12/16]: Training Loss: 1.432861389, Training Accuracy: 58.224\n",
            "Worker 3, [13/16]: Training Loss: 1.396778472, Training Accuracy: 59.792\n",
            "Worker 3, [14/16]: Training Loss: 1.299192858, Training Accuracy: 62.480\n",
            "Worker 3, [15/16]: Training Loss: 1.318412235, Training Accuracy: 62.192\n",
            "Worker 3, [16/16]: Training Loss: 1.234137350, Training Accuracy: 63.888\n",
            "Time taken for training worker 3: 0:00:49.279336\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.690088618, Training Accuracy: 33.280\n",
            "Worker 4, [02/16]: Training Loss: 2.329210469, Training Accuracy: 38.608\n",
            "Worker 4, [03/16]: Training Loss: 2.179254632, Training Accuracy: 41.120\n",
            "Worker 4, [04/16]: Training Loss: 2.064467697, Training Accuracy: 44.016\n",
            "Worker 4, [05/16]: Training Loss: 1.946764231, Training Accuracy: 46.896\n",
            "Worker 4, [06/16]: Training Loss: 1.848873668, Training Accuracy: 48.688\n",
            "Worker 4, [07/16]: Training Loss: 1.794343098, Training Accuracy: 49.584\n",
            "Worker 4, [08/16]: Training Loss: 1.710127894, Training Accuracy: 51.536\n",
            "Worker 4, [09/16]: Training Loss: 1.599918476, Training Accuracy: 54.912\n",
            "Worker 4, [10/16]: Training Loss: 1.561671127, Training Accuracy: 55.664\n",
            "Worker 4, [11/16]: Training Loss: 1.492174751, Training Accuracy: 57.056\n",
            "Worker 4, [12/16]: Training Loss: 1.439300486, Training Accuracy: 58.320\n",
            "Worker 4, [13/16]: Training Loss: 1.409220953, Training Accuracy: 58.784\n",
            "Worker 4, [14/16]: Training Loss: 1.339883687, Training Accuracy: 60.944\n",
            "Worker 4, [15/16]: Training Loss: 1.320316794, Training Accuracy: 61.600\n",
            "Worker 4, [16/16]: Training Loss: 1.262974624, Training Accuracy: 63.328\n",
            "Time taken for training worker 4: 0:00:48.561634\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 2.723329512, Training Accuracy: 31.856\n",
            "Worker 5, [02/16]: Training Loss: 2.335740536, Training Accuracy: 38.384\n",
            "Worker 5, [03/16]: Training Loss: 2.165678334, Training Accuracy: 41.648\n",
            "Worker 5, [04/16]: Training Loss: 2.038849457, Training Accuracy: 44.096\n",
            "Worker 5, [05/16]: Training Loss: 1.926636180, Training Accuracy: 45.824\n",
            "Worker 5, [06/16]: Training Loss: 1.857030904, Training Accuracy: 48.736\n",
            "Worker 5, [07/16]: Training Loss: 1.753984757, Training Accuracy: 50.800\n",
            "Worker 5, [08/16]: Training Loss: 1.711220578, Training Accuracy: 51.712\n",
            "Worker 5, [09/16]: Training Loss: 1.637044967, Training Accuracy: 53.760\n",
            "Worker 5, [10/16]: Training Loss: 1.581661650, Training Accuracy: 54.864\n",
            "Worker 5, [11/16]: Training Loss: 1.499572129, Training Accuracy: 56.544\n",
            "Worker 5, [12/16]: Training Loss: 1.441722935, Training Accuracy: 57.968\n",
            "Worker 5, [13/16]: Training Loss: 1.414076912, Training Accuracy: 58.672\n",
            "Worker 5, [14/16]: Training Loss: 1.325161820, Training Accuracy: 61.776\n",
            "Worker 5, [15/16]: Training Loss: 1.303654142, Training Accuracy: 61.824\n",
            "Worker 5, [16/16]: Training Loss: 1.265307375, Training Accuracy: 63.072\n",
            "Time taken for training worker 5: 0:00:48.592818\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 2.704713556, Training Accuracy: 33.424\n",
            "Worker 6, [02/16]: Training Loss: 2.340484090, Training Accuracy: 37.648\n",
            "Worker 6, [03/16]: Training Loss: 2.164466239, Training Accuracy: 42.128\n",
            "Worker 6, [04/16]: Training Loss: 2.052411045, Training Accuracy: 44.640\n",
            "Worker 6, [05/16]: Training Loss: 1.962448770, Training Accuracy: 46.224\n",
            "Worker 6, [06/16]: Training Loss: 1.853934821, Training Accuracy: 47.984\n",
            "Worker 6, [07/16]: Training Loss: 1.802243851, Training Accuracy: 49.424\n",
            "Worker 6, [08/16]: Training Loss: 1.713998310, Training Accuracy: 51.184\n",
            "Worker 6, [09/16]: Training Loss: 1.674269466, Training Accuracy: 52.640\n",
            "Worker 6, [10/16]: Training Loss: 1.580307846, Training Accuracy: 53.904\n",
            "Worker 6, [11/16]: Training Loss: 1.520319086, Training Accuracy: 56.624\n",
            "Worker 6, [12/16]: Training Loss: 1.462007336, Training Accuracy: 57.680\n",
            "Worker 6, [13/16]: Training Loss: 1.410780402, Training Accuracy: 58.528\n",
            "Worker 6, [14/16]: Training Loss: 1.360778185, Training Accuracy: 60.480\n",
            "Worker 6, [15/16]: Training Loss: 1.337431103, Training Accuracy: 60.832\n",
            "Worker 6, [16/16]: Training Loss: 1.310186589, Training Accuracy: 62.160\n",
            "Time taken for training worker 6: 0:00:49.028468\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 2.680853943, Training Accuracy: 32.480\n",
            "Worker 7, [02/16]: Training Loss: 2.374900436, Training Accuracy: 38.128\n",
            "Worker 7, [03/16]: Training Loss: 2.196630975, Training Accuracy: 41.520\n",
            "Worker 7, [04/16]: Training Loss: 2.068067754, Training Accuracy: 44.080\n",
            "Worker 7, [05/16]: Training Loss: 1.969298225, Training Accuracy: 45.920\n",
            "Worker 7, [06/16]: Training Loss: 1.871036949, Training Accuracy: 47.424\n",
            "Worker 7, [07/16]: Training Loss: 1.818593365, Training Accuracy: 48.800\n",
            "Worker 7, [08/16]: Training Loss: 1.687988492, Training Accuracy: 52.112\n",
            "Worker 7, [09/16]: Training Loss: 1.625974796, Training Accuracy: 54.000\n",
            "Worker 7, [10/16]: Training Loss: 1.561363686, Training Accuracy: 54.656\n",
            "Worker 7, [11/16]: Training Loss: 1.509763816, Training Accuracy: 56.336\n",
            "Worker 7, [12/16]: Training Loss: 1.454278243, Training Accuracy: 58.384\n",
            "Worker 7, [13/16]: Training Loss: 1.379871173, Training Accuracy: 59.344\n",
            "Worker 7, [14/16]: Training Loss: 1.379681068, Training Accuracy: 59.936\n",
            "Worker 7, [15/16]: Training Loss: 1.295667111, Training Accuracy: 62.368\n",
            "Worker 7, [16/16]: Training Loss: 1.288532651, Training Accuracy: 62.576\n",
            "Time taken for training worker 7: 0:00:49.765583\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 2.699671963, Training Accuracy: 32.576\n",
            "Worker 8, [02/16]: Training Loss: 2.350545589, Training Accuracy: 37.744\n",
            "Worker 8, [03/16]: Training Loss: 2.178811167, Training Accuracy: 41.456\n",
            "Worker 8, [04/16]: Training Loss: 2.081167387, Training Accuracy: 43.056\n",
            "Worker 8, [05/16]: Training Loss: 1.954280095, Training Accuracy: 46.016\n",
            "Worker 8, [06/16]: Training Loss: 1.872561635, Training Accuracy: 48.592\n",
            "Worker 8, [07/16]: Training Loss: 1.794854293, Training Accuracy: 50.112\n",
            "Worker 8, [08/16]: Training Loss: 1.699930826, Training Accuracy: 51.728\n",
            "Worker 8, [09/16]: Training Loss: 1.635606928, Training Accuracy: 53.584\n",
            "Worker 8, [10/16]: Training Loss: 1.583312535, Training Accuracy: 55.072\n",
            "Worker 8, [11/16]: Training Loss: 1.540288144, Training Accuracy: 55.776\n",
            "Worker 8, [12/16]: Training Loss: 1.480746379, Training Accuracy: 57.296\n",
            "Worker 8, [13/16]: Training Loss: 1.469774890, Training Accuracy: 58.080\n",
            "Worker 8, [14/16]: Training Loss: 1.372821410, Training Accuracy: 60.496\n",
            "Worker 8, [15/16]: Training Loss: 1.284160350, Training Accuracy: 62.336\n",
            "Worker 8, [16/16]: Training Loss: 1.306497571, Training Accuracy: 62.288\n",
            "Time taken for training worker 8: 0:00:50.811104\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004576\n",
            "Global Update 04: Test Loss: 2.774487628, Test Accuracy: 39.680\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.361204201, Training Accuracy: 39.408\n",
            "Worker 1, [02/16]: Training Loss: 1.995025620, Training Accuracy: 46.624\n",
            "Worker 1, [03/16]: Training Loss: 1.825223858, Training Accuracy: 49.040\n",
            "Worker 1, [04/16]: Training Loss: 1.697387163, Training Accuracy: 52.608\n",
            "Worker 1, [05/16]: Training Loss: 1.546390155, Training Accuracy: 56.416\n",
            "Worker 1, [06/16]: Training Loss: 1.413388547, Training Accuracy: 59.520\n",
            "Worker 1, [07/16]: Training Loss: 1.391241307, Training Accuracy: 60.672\n",
            "Worker 1, [08/16]: Training Loss: 1.316219292, Training Accuracy: 62.160\n",
            "Worker 1, [09/16]: Training Loss: 1.248301114, Training Accuracy: 63.680\n",
            "Worker 1, [10/16]: Training Loss: 1.177982426, Training Accuracy: 64.752\n",
            "Worker 1, [11/16]: Training Loss: 1.134022122, Training Accuracy: 66.192\n",
            "Worker 1, [12/16]: Training Loss: 1.067066332, Training Accuracy: 67.776\n",
            "Worker 1, [13/16]: Training Loss: 1.027827861, Training Accuracy: 68.960\n",
            "Worker 1, [14/16]: Training Loss: 0.972462540, Training Accuracy: 70.288\n",
            "Worker 1, [15/16]: Training Loss: 0.942779114, Training Accuracy: 71.456\n",
            "Worker 1, [16/16]: Training Loss: 0.939069269, Training Accuracy: 72.144\n",
            "Time taken for training worker 1: 0:00:55.370735\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.370578836, Training Accuracy: 39.744\n",
            "Worker 2, [02/16]: Training Loss: 2.008150367, Training Accuracy: 45.488\n",
            "Worker 2, [03/16]: Training Loss: 1.842930819, Training Accuracy: 48.480\n",
            "Worker 2, [04/16]: Training Loss: 1.685829812, Training Accuracy: 53.104\n",
            "Worker 2, [05/16]: Training Loss: 1.564902699, Training Accuracy: 55.648\n",
            "Worker 2, [06/16]: Training Loss: 1.471091180, Training Accuracy: 58.400\n",
            "Worker 2, [07/16]: Training Loss: 1.393336590, Training Accuracy: 59.392\n",
            "Worker 2, [08/16]: Training Loss: 1.321459941, Training Accuracy: 61.856\n",
            "Worker 2, [09/16]: Training Loss: 1.228048408, Training Accuracy: 63.760\n",
            "Worker 2, [10/16]: Training Loss: 1.181472583, Training Accuracy: 65.248\n",
            "Worker 2, [11/16]: Training Loss: 1.133518147, Training Accuracy: 66.448\n",
            "Worker 2, [12/16]: Training Loss: 1.045181404, Training Accuracy: 69.120\n",
            "Worker 2, [13/16]: Training Loss: 1.024604965, Training Accuracy: 69.200\n",
            "Worker 2, [14/16]: Training Loss: 0.988220550, Training Accuracy: 70.992\n",
            "Worker 2, [15/16]: Training Loss: 0.985607302, Training Accuracy: 71.104\n",
            "Worker 2, [16/16]: Training Loss: 0.901013394, Training Accuracy: 73.312\n",
            "Time taken for training worker 2: 0:00:53.446195\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.364446391, Training Accuracy: 38.368\n",
            "Worker 3, [02/16]: Training Loss: 2.030609032, Training Accuracy: 44.688\n",
            "Worker 3, [03/16]: Training Loss: 1.831465759, Training Accuracy: 49.360\n",
            "Worker 3, [04/16]: Training Loss: 1.725668227, Training Accuracy: 51.696\n",
            "Worker 3, [05/16]: Training Loss: 1.573411174, Training Accuracy: 55.520\n",
            "Worker 3, [06/16]: Training Loss: 1.486036158, Training Accuracy: 58.112\n",
            "Worker 3, [07/16]: Training Loss: 1.404055230, Training Accuracy: 59.888\n",
            "Worker 3, [08/16]: Training Loss: 1.298799555, Training Accuracy: 62.752\n",
            "Worker 3, [09/16]: Training Loss: 1.265448699, Training Accuracy: 62.768\n",
            "Worker 3, [10/16]: Training Loss: 1.197303453, Training Accuracy: 64.320\n",
            "Worker 3, [11/16]: Training Loss: 1.137724448, Training Accuracy: 66.016\n",
            "Worker 3, [12/16]: Training Loss: 1.099621505, Training Accuracy: 67.808\n",
            "Worker 3, [13/16]: Training Loss: 1.050399668, Training Accuracy: 69.200\n",
            "Worker 3, [14/16]: Training Loss: 0.984215795, Training Accuracy: 70.080\n",
            "Worker 3, [15/16]: Training Loss: 0.922896299, Training Accuracy: 72.640\n",
            "Worker 3, [16/16]: Training Loss: 0.905264630, Training Accuracy: 73.088\n",
            "Time taken for training worker 3: 0:00:51.219002\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.397586864, Training Accuracy: 38.368\n",
            "Worker 4, [02/16]: Training Loss: 2.039405361, Training Accuracy: 45.504\n",
            "Worker 4, [03/16]: Training Loss: 1.855336350, Training Accuracy: 48.816\n",
            "Worker 4, [04/16]: Training Loss: 1.710386912, Training Accuracy: 52.272\n",
            "Worker 4, [05/16]: Training Loss: 1.599711326, Training Accuracy: 54.576\n",
            "Worker 4, [06/16]: Training Loss: 1.479061427, Training Accuracy: 57.408\n",
            "Worker 4, [07/16]: Training Loss: 1.430786876, Training Accuracy: 58.528\n",
            "Worker 4, [08/16]: Training Loss: 1.303827019, Training Accuracy: 62.192\n",
            "Worker 4, [09/16]: Training Loss: 1.254085917, Training Accuracy: 63.984\n",
            "Worker 4, [10/16]: Training Loss: 1.213238994, Training Accuracy: 64.896\n",
            "Worker 4, [11/16]: Training Loss: 1.118119289, Training Accuracy: 67.424\n",
            "Worker 4, [12/16]: Training Loss: 1.093412917, Training Accuracy: 67.856\n",
            "Worker 4, [13/16]: Training Loss: 1.015250418, Training Accuracy: 69.440\n",
            "Worker 4, [14/16]: Training Loss: 1.027066532, Training Accuracy: 69.904\n",
            "Worker 4, [15/16]: Training Loss: 0.991866192, Training Accuracy: 70.320\n",
            "Worker 4, [16/16]: Training Loss: 0.962705225, Training Accuracy: 71.200\n",
            "Time taken for training worker 4: 0:00:47.849411\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 2.377128363, Training Accuracy: 38.704\n",
            "Worker 5, [02/16]: Training Loss: 2.035419142, Training Accuracy: 44.672\n",
            "Worker 5, [03/16]: Training Loss: 1.851958275, Training Accuracy: 49.328\n",
            "Worker 5, [04/16]: Training Loss: 1.662419907, Training Accuracy: 52.624\n",
            "Worker 5, [05/16]: Training Loss: 1.581080263, Training Accuracy: 55.024\n",
            "Worker 5, [06/16]: Training Loss: 1.470498710, Training Accuracy: 58.256\n",
            "Worker 5, [07/16]: Training Loss: 1.393051544, Training Accuracy: 59.936\n",
            "Worker 5, [08/16]: Training Loss: 1.303652834, Training Accuracy: 61.968\n",
            "Worker 5, [09/16]: Training Loss: 1.251097460, Training Accuracy: 63.536\n",
            "Worker 5, [10/16]: Training Loss: 1.185683213, Training Accuracy: 65.792\n",
            "Worker 5, [11/16]: Training Loss: 1.111872755, Training Accuracy: 67.216\n",
            "Worker 5, [12/16]: Training Loss: 1.082693969, Training Accuracy: 67.936\n",
            "Worker 5, [13/16]: Training Loss: 1.020040162, Training Accuracy: 69.552\n",
            "Worker 5, [14/16]: Training Loss: 1.005221961, Training Accuracy: 70.624\n",
            "Worker 5, [15/16]: Training Loss: 0.945236643, Training Accuracy: 72.080\n",
            "Worker 5, [16/16]: Training Loss: 0.915677457, Training Accuracy: 72.416\n",
            "Time taken for training worker 5: 0:00:50.126336\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 2.426357671, Training Accuracy: 37.632\n",
            "Worker 6, [02/16]: Training Loss: 2.038478539, Training Accuracy: 44.912\n",
            "Worker 6, [03/16]: Training Loss: 1.843411250, Training Accuracy: 49.040\n",
            "Worker 6, [04/16]: Training Loss: 1.738030510, Training Accuracy: 52.288\n",
            "Worker 6, [05/16]: Training Loss: 1.612841407, Training Accuracy: 54.720\n",
            "Worker 6, [06/16]: Training Loss: 1.499172756, Training Accuracy: 57.792\n",
            "Worker 6, [07/16]: Training Loss: 1.433083261, Training Accuracy: 57.984\n",
            "Worker 6, [08/16]: Training Loss: 1.322724145, Training Accuracy: 61.552\n",
            "Worker 6, [09/16]: Training Loss: 1.288747158, Training Accuracy: 63.280\n",
            "Worker 6, [10/16]: Training Loss: 1.232613057, Training Accuracy: 64.080\n",
            "Worker 6, [11/16]: Training Loss: 1.157044197, Training Accuracy: 65.840\n",
            "Worker 6, [12/16]: Training Loss: 1.109978670, Training Accuracy: 67.408\n",
            "Worker 6, [13/16]: Training Loss: 1.055342787, Training Accuracy: 68.480\n",
            "Worker 6, [14/16]: Training Loss: 1.011365464, Training Accuracy: 70.240\n",
            "Worker 6, [15/16]: Training Loss: 0.957006556, Training Accuracy: 71.136\n",
            "Worker 6, [16/16]: Training Loss: 0.908787918, Training Accuracy: 72.304\n",
            "Time taken for training worker 6: 0:00:50.417934\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 2.364416142, Training Accuracy: 39.344\n",
            "Worker 7, [02/16]: Training Loss: 2.046365383, Training Accuracy: 44.736\n",
            "Worker 7, [03/16]: Training Loss: 1.848978707, Training Accuracy: 49.040\n",
            "Worker 7, [04/16]: Training Loss: 1.713847551, Training Accuracy: 52.608\n",
            "Worker 7, [05/16]: Training Loss: 1.575220074, Training Accuracy: 55.456\n",
            "Worker 7, [06/16]: Training Loss: 1.499727384, Training Accuracy: 57.440\n",
            "Worker 7, [07/16]: Training Loss: 1.399345017, Training Accuracy: 59.472\n",
            "Worker 7, [08/16]: Training Loss: 1.321723849, Training Accuracy: 61.760\n",
            "Worker 7, [09/16]: Training Loss: 1.263256222, Training Accuracy: 63.440\n",
            "Worker 7, [10/16]: Training Loss: 1.194763667, Training Accuracy: 64.656\n",
            "Worker 7, [11/16]: Training Loss: 1.139618364, Training Accuracy: 66.320\n",
            "Worker 7, [12/16]: Training Loss: 1.094666274, Training Accuracy: 68.240\n",
            "Worker 7, [13/16]: Training Loss: 1.022163194, Training Accuracy: 69.536\n",
            "Worker 7, [14/16]: Training Loss: 1.001589908, Training Accuracy: 70.064\n",
            "Worker 7, [15/16]: Training Loss: 1.031112907, Training Accuracy: 69.616\n",
            "Worker 7, [16/16]: Training Loss: 0.953785760, Training Accuracy: 71.504\n",
            "Time taken for training worker 7: 0:00:48.683584\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 2.423174251, Training Accuracy: 38.016\n",
            "Worker 8, [02/16]: Training Loss: 2.069875389, Training Accuracy: 44.816\n",
            "Worker 8, [03/16]: Training Loss: 1.862484642, Training Accuracy: 48.960\n",
            "Worker 8, [04/16]: Training Loss: 1.732378000, Training Accuracy: 50.960\n",
            "Worker 8, [05/16]: Training Loss: 1.609206080, Training Accuracy: 54.816\n",
            "Worker 8, [06/16]: Training Loss: 1.539911307, Training Accuracy: 55.840\n",
            "Worker 8, [07/16]: Training Loss: 1.438786289, Training Accuracy: 58.544\n",
            "Worker 8, [08/16]: Training Loss: 1.338604537, Training Accuracy: 61.312\n",
            "Worker 8, [09/16]: Training Loss: 1.244786652, Training Accuracy: 63.616\n",
            "Worker 8, [10/16]: Training Loss: 1.213959407, Training Accuracy: 64.944\n",
            "Worker 8, [11/16]: Training Loss: 1.125292290, Training Accuracy: 66.752\n",
            "Worker 8, [12/16]: Training Loss: 1.095296216, Training Accuracy: 67.904\n",
            "Worker 8, [13/16]: Training Loss: 1.057956072, Training Accuracy: 68.576\n",
            "Worker 8, [14/16]: Training Loss: 1.028046658, Training Accuracy: 69.312\n",
            "Worker 8, [15/16]: Training Loss: 0.969743885, Training Accuracy: 71.168\n",
            "Worker 8, [16/16]: Training Loss: 0.936100541, Training Accuracy: 71.808\n",
            "Time taken for training worker 8: 0:00:49.401574\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004002\n",
            "Global Update 05: Test Loss: 2.539195074, Test Accuracy: 40.980\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.086359383, Training Accuracy: 44.624\n",
            "Worker 1, [02/16]: Training Loss: 1.718665780, Training Accuracy: 51.312\n",
            "Worker 1, [03/16]: Training Loss: 1.551391177, Training Accuracy: 56.320\n",
            "Worker 1, [04/16]: Training Loss: 1.389365967, Training Accuracy: 60.336\n",
            "Worker 1, [05/16]: Training Loss: 1.294341521, Training Accuracy: 63.136\n",
            "Worker 1, [06/16]: Training Loss: 1.156609290, Training Accuracy: 66.144\n",
            "Worker 1, [07/16]: Training Loss: 1.066017133, Training Accuracy: 68.720\n",
            "Worker 1, [08/16]: Training Loss: 0.993932913, Training Accuracy: 70.416\n",
            "Worker 1, [09/16]: Training Loss: 0.937851222, Training Accuracy: 72.928\n",
            "Worker 1, [10/16]: Training Loss: 0.912562190, Training Accuracy: 72.608\n",
            "Worker 1, [11/16]: Training Loss: 0.847573966, Training Accuracy: 74.208\n",
            "Worker 1, [12/16]: Training Loss: 0.781095526, Training Accuracy: 76.336\n",
            "Worker 1, [13/16]: Training Loss: 0.726204894, Training Accuracy: 78.688\n",
            "Worker 1, [14/16]: Training Loss: 0.711192584, Training Accuracy: 78.816\n",
            "Worker 1, [15/16]: Training Loss: 0.709305659, Training Accuracy: 78.336\n",
            "Worker 1, [16/16]: Training Loss: 0.676361701, Training Accuracy: 80.032\n",
            "Time taken for training worker 1: 0:00:49.245565\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.096818255, Training Accuracy: 43.984\n",
            "Worker 2, [02/16]: Training Loss: 1.755011285, Training Accuracy: 51.392\n",
            "Worker 2, [03/16]: Training Loss: 1.612071658, Training Accuracy: 55.872\n",
            "Worker 2, [04/16]: Training Loss: 1.410562546, Training Accuracy: 60.256\n",
            "Worker 2, [05/16]: Training Loss: 1.275830672, Training Accuracy: 63.984\n",
            "Worker 2, [06/16]: Training Loss: 1.165089893, Training Accuracy: 65.776\n",
            "Worker 2, [07/16]: Training Loss: 1.101917587, Training Accuracy: 67.472\n",
            "Worker 2, [08/16]: Training Loss: 1.005022284, Training Accuracy: 70.144\n",
            "Worker 2, [09/16]: Training Loss: 0.967800990, Training Accuracy: 70.640\n",
            "Worker 2, [10/16]: Training Loss: 0.899756478, Training Accuracy: 73.712\n",
            "Worker 2, [11/16]: Training Loss: 0.843489924, Training Accuracy: 74.768\n",
            "Worker 2, [12/16]: Training Loss: 0.831617594, Training Accuracy: 75.280\n",
            "Worker 2, [13/16]: Training Loss: 0.773765982, Training Accuracy: 76.720\n",
            "Worker 2, [14/16]: Training Loss: 0.736262169, Training Accuracy: 78.608\n",
            "Worker 2, [15/16]: Training Loss: 0.704587318, Training Accuracy: 78.576\n",
            "Worker 2, [16/16]: Training Loss: 0.633998566, Training Accuracy: 80.960\n",
            "Time taken for training worker 2: 0:00:49.163680\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.123859418, Training Accuracy: 44.000\n",
            "Worker 3, [02/16]: Training Loss: 1.750967871, Training Accuracy: 51.872\n",
            "Worker 3, [03/16]: Training Loss: 1.552154961, Training Accuracy: 56.160\n",
            "Worker 3, [04/16]: Training Loss: 1.412937161, Training Accuracy: 59.808\n",
            "Worker 3, [05/16]: Training Loss: 1.267801082, Training Accuracy: 63.520\n",
            "Worker 3, [06/16]: Training Loss: 1.210497970, Training Accuracy: 64.720\n",
            "Worker 3, [07/16]: Training Loss: 1.128302830, Training Accuracy: 67.408\n",
            "Worker 3, [08/16]: Training Loss: 0.999791786, Training Accuracy: 70.320\n",
            "Worker 3, [09/16]: Training Loss: 0.970353955, Training Accuracy: 71.712\n",
            "Worker 3, [10/16]: Training Loss: 0.910591136, Training Accuracy: 73.648\n",
            "Worker 3, [11/16]: Training Loss: 0.870469002, Training Accuracy: 74.000\n",
            "Worker 3, [12/16]: Training Loss: 0.818415091, Training Accuracy: 75.904\n",
            "Worker 3, [13/16]: Training Loss: 0.766353762, Training Accuracy: 76.896\n",
            "Worker 3, [14/16]: Training Loss: 0.747741540, Training Accuracy: 77.632\n",
            "Worker 3, [15/16]: Training Loss: 0.675278592, Training Accuracy: 79.744\n",
            "Worker 3, [16/16]: Training Loss: 0.663301923, Training Accuracy: 80.272\n",
            "Time taken for training worker 3: 0:00:50.644985\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.125874545, Training Accuracy: 44.400\n",
            "Worker 4, [02/16]: Training Loss: 1.787618946, Training Accuracy: 50.160\n",
            "Worker 4, [03/16]: Training Loss: 1.559855159, Training Accuracy: 55.632\n",
            "Worker 4, [04/16]: Training Loss: 1.394429319, Training Accuracy: 59.920\n",
            "Worker 4, [05/16]: Training Loss: 1.294978214, Training Accuracy: 62.304\n",
            "Worker 4, [06/16]: Training Loss: 1.206165791, Training Accuracy: 64.608\n",
            "Worker 4, [07/16]: Training Loss: 1.111024630, Training Accuracy: 67.408\n",
            "Worker 4, [08/16]: Training Loss: 1.020207238, Training Accuracy: 69.488\n",
            "Worker 4, [09/16]: Training Loss: 0.946076369, Training Accuracy: 72.240\n",
            "Worker 4, [10/16]: Training Loss: 0.889743813, Training Accuracy: 73.712\n",
            "Worker 4, [11/16]: Training Loss: 0.861911768, Training Accuracy: 73.616\n",
            "Worker 4, [12/16]: Training Loss: 0.809209036, Training Accuracy: 75.840\n",
            "Worker 4, [13/16]: Training Loss: 0.754631947, Training Accuracy: 77.216\n",
            "Worker 4, [14/16]: Training Loss: 0.729205670, Training Accuracy: 78.496\n",
            "Worker 4, [15/16]: Training Loss: 0.688034330, Training Accuracy: 79.632\n",
            "Worker 4, [16/16]: Training Loss: 0.652432139, Training Accuracy: 80.864\n",
            "Time taken for training worker 4: 0:00:48.439658\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 2.117935111, Training Accuracy: 44.416\n",
            "Worker 5, [02/16]: Training Loss: 1.757536344, Training Accuracy: 51.360\n",
            "Worker 5, [03/16]: Training Loss: 1.570111075, Training Accuracy: 56.144\n",
            "Worker 5, [04/16]: Training Loss: 1.424333949, Training Accuracy: 59.104\n",
            "Worker 5, [05/16]: Training Loss: 1.282483179, Training Accuracy: 62.976\n",
            "Worker 5, [06/16]: Training Loss: 1.180834108, Training Accuracy: 65.408\n",
            "Worker 5, [07/16]: Training Loss: 1.100603797, Training Accuracy: 67.232\n",
            "Worker 5, [08/16]: Training Loss: 1.024089199, Training Accuracy: 69.664\n",
            "Worker 5, [09/16]: Training Loss: 0.940459402, Training Accuracy: 72.192\n",
            "Worker 5, [10/16]: Training Loss: 0.912228530, Training Accuracy: 72.640\n",
            "Worker 5, [11/16]: Training Loss: 0.866806099, Training Accuracy: 74.464\n",
            "Worker 5, [12/16]: Training Loss: 0.792342659, Training Accuracy: 75.472\n",
            "Worker 5, [13/16]: Training Loss: 0.755651425, Training Accuracy: 77.760\n",
            "Worker 5, [14/16]: Training Loss: 0.708075576, Training Accuracy: 78.672\n",
            "Worker 5, [15/16]: Training Loss: 0.691413715, Training Accuracy: 79.184\n",
            "Worker 5, [16/16]: Training Loss: 0.663664673, Training Accuracy: 80.272\n",
            "Time taken for training worker 5: 0:00:49.207701\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 2.132397081, Training Accuracy: 42.896\n",
            "Worker 6, [02/16]: Training Loss: 1.794218725, Training Accuracy: 50.832\n",
            "Worker 6, [03/16]: Training Loss: 1.607881022, Training Accuracy: 55.136\n",
            "Worker 6, [04/16]: Training Loss: 1.431759923, Training Accuracy: 59.136\n",
            "Worker 6, [05/16]: Training Loss: 1.302735643, Training Accuracy: 62.080\n",
            "Worker 6, [06/16]: Training Loss: 1.242929485, Training Accuracy: 64.272\n",
            "Worker 6, [07/16]: Training Loss: 1.147686531, Training Accuracy: 67.472\n",
            "Worker 6, [08/16]: Training Loss: 1.066146199, Training Accuracy: 69.040\n",
            "Worker 6, [09/16]: Training Loss: 0.995036326, Training Accuracy: 70.752\n",
            "Worker 6, [10/16]: Training Loss: 0.940725955, Training Accuracy: 71.984\n",
            "Worker 6, [11/16]: Training Loss: 0.895869697, Training Accuracy: 72.800\n",
            "Worker 6, [12/16]: Training Loss: 0.842333655, Training Accuracy: 74.656\n",
            "Worker 6, [13/16]: Training Loss: 0.803795608, Training Accuracy: 76.000\n",
            "Worker 6, [14/16]: Training Loss: 0.734366858, Training Accuracy: 78.240\n",
            "Worker 6, [15/16]: Training Loss: 0.729082720, Training Accuracy: 78.176\n",
            "Worker 6, [16/16]: Training Loss: 0.709787814, Training Accuracy: 78.640\n",
            "Time taken for training worker 6: 0:00:49.447371\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 2.139872549, Training Accuracy: 43.408\n",
            "Worker 7, [02/16]: Training Loss: 1.779339754, Training Accuracy: 51.008\n",
            "Worker 7, [03/16]: Training Loss: 1.563818660, Training Accuracy: 55.808\n",
            "Worker 7, [04/16]: Training Loss: 1.443734679, Training Accuracy: 58.784\n",
            "Worker 7, [05/16]: Training Loss: 1.303215720, Training Accuracy: 62.384\n",
            "Worker 7, [06/16]: Training Loss: 1.198350815, Training Accuracy: 65.360\n",
            "Worker 7, [07/16]: Training Loss: 1.115562467, Training Accuracy: 67.840\n",
            "Worker 7, [08/16]: Training Loss: 1.051286281, Training Accuracy: 68.816\n",
            "Worker 7, [09/16]: Training Loss: 0.978517193, Training Accuracy: 70.752\n",
            "Worker 7, [10/16]: Training Loss: 0.925369670, Training Accuracy: 72.624\n",
            "Worker 7, [11/16]: Training Loss: 0.856772441, Training Accuracy: 74.048\n",
            "Worker 7, [12/16]: Training Loss: 0.818848179, Training Accuracy: 75.552\n",
            "Worker 7, [13/16]: Training Loss: 0.802377156, Training Accuracy: 76.288\n",
            "Worker 7, [14/16]: Training Loss: 0.787642934, Training Accuracy: 76.288\n",
            "Worker 7, [15/16]: Training Loss: 0.689280311, Training Accuracy: 78.960\n",
            "Worker 7, [16/16]: Training Loss: 0.662290678, Training Accuracy: 80.208\n",
            "Time taken for training worker 7: 0:00:48.916664\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 2.115964055, Training Accuracy: 44.208\n",
            "Worker 8, [02/16]: Training Loss: 1.773223343, Training Accuracy: 51.040\n",
            "Worker 8, [03/16]: Training Loss: 1.586365177, Training Accuracy: 55.616\n",
            "Worker 8, [04/16]: Training Loss: 1.425775080, Training Accuracy: 59.728\n",
            "Worker 8, [05/16]: Training Loss: 1.290426776, Training Accuracy: 63.184\n",
            "Worker 8, [06/16]: Training Loss: 1.204001722, Training Accuracy: 65.312\n",
            "Worker 8, [07/16]: Training Loss: 1.118903891, Training Accuracy: 67.888\n",
            "Worker 8, [08/16]: Training Loss: 1.049131282, Training Accuracy: 69.216\n",
            "Worker 8, [09/16]: Training Loss: 0.990677212, Training Accuracy: 71.040\n",
            "Worker 8, [10/16]: Training Loss: 0.929433647, Training Accuracy: 72.608\n",
            "Worker 8, [11/16]: Training Loss: 0.867563649, Training Accuracy: 74.320\n",
            "Worker 8, [12/16]: Training Loss: 0.812920101, Training Accuracy: 76.112\n",
            "Worker 8, [13/16]: Training Loss: 0.799228398, Training Accuracy: 76.192\n",
            "Worker 8, [14/16]: Training Loss: 0.728774472, Training Accuracy: 78.176\n",
            "Worker 8, [15/16]: Training Loss: 0.726878213, Training Accuracy: 78.272\n",
            "Worker 8, [16/16]: Training Loss: 0.668682357, Training Accuracy: 80.304\n",
            "Time taken for training worker 8: 0:00:48.888806\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.007159\n",
            "Global Update 06: Test Loss: 2.670225540, Test Accuracy: 42.470\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.878451684, Training Accuracy: 48.848\n",
            "Worker 1, [02/16]: Training Loss: 1.547009513, Training Accuracy: 55.456\n",
            "Worker 1, [03/16]: Training Loss: 1.360709314, Training Accuracy: 60.672\n",
            "Worker 1, [04/16]: Training Loss: 1.196303251, Training Accuracy: 65.056\n",
            "Worker 1, [05/16]: Training Loss: 1.076642997, Training Accuracy: 68.800\n",
            "Worker 1, [06/16]: Training Loss: 1.015873134, Training Accuracy: 70.336\n",
            "Worker 1, [07/16]: Training Loss: 0.928728912, Training Accuracy: 72.960\n",
            "Worker 1, [08/16]: Training Loss: 0.832784233, Training Accuracy: 75.568\n",
            "Worker 1, [09/16]: Training Loss: 0.767905695, Training Accuracy: 77.632\n",
            "Worker 1, [10/16]: Training Loss: 0.719860186, Training Accuracy: 78.448\n",
            "Worker 1, [11/16]: Training Loss: 0.672872198, Training Accuracy: 79.488\n",
            "Worker 1, [12/16]: Training Loss: 0.655209320, Training Accuracy: 80.400\n",
            "Worker 1, [13/16]: Training Loss: 0.610264307, Training Accuracy: 81.792\n",
            "Worker 1, [14/16]: Training Loss: 0.563549258, Training Accuracy: 83.728\n",
            "Worker 1, [15/16]: Training Loss: 0.540040876, Training Accuracy: 83.792\n",
            "Worker 1, [16/16]: Training Loss: 0.524388043, Training Accuracy: 84.192\n",
            "Time taken for training worker 1: 0:00:50.136513\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.889408980, Training Accuracy: 49.136\n",
            "Worker 2, [02/16]: Training Loss: 1.557313993, Training Accuracy: 55.440\n",
            "Worker 2, [03/16]: Training Loss: 1.356331962, Training Accuracy: 60.752\n",
            "Worker 2, [04/16]: Training Loss: 1.214591054, Training Accuracy: 64.176\n",
            "Worker 2, [05/16]: Training Loss: 1.095389528, Training Accuracy: 68.032\n",
            "Worker 2, [06/16]: Training Loss: 1.003825674, Training Accuracy: 70.480\n",
            "Worker 2, [07/16]: Training Loss: 0.900788443, Training Accuracy: 74.272\n",
            "Worker 2, [08/16]: Training Loss: 0.844611212, Training Accuracy: 75.744\n",
            "Worker 2, [09/16]: Training Loss: 0.794110608, Training Accuracy: 76.784\n",
            "Worker 2, [10/16]: Training Loss: 0.711875333, Training Accuracy: 78.752\n",
            "Worker 2, [11/16]: Training Loss: 0.658415532, Training Accuracy: 80.592\n",
            "Worker 2, [12/16]: Training Loss: 0.642024072, Training Accuracy: 81.632\n",
            "Worker 2, [13/16]: Training Loss: 0.608343734, Training Accuracy: 81.472\n",
            "Worker 2, [14/16]: Training Loss: 0.588740449, Training Accuracy: 82.576\n",
            "Worker 2, [15/16]: Training Loss: 0.542394031, Training Accuracy: 84.448\n",
            "Worker 2, [16/16]: Training Loss: 0.512735494, Training Accuracy: 85.312\n",
            "Time taken for training worker 2: 0:00:48.065836\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.901758139, Training Accuracy: 47.904\n",
            "Worker 3, [02/16]: Training Loss: 1.526825109, Training Accuracy: 56.624\n",
            "Worker 3, [03/16]: Training Loss: 1.390679448, Training Accuracy: 59.584\n",
            "Worker 3, [04/16]: Training Loss: 1.190921271, Training Accuracy: 65.616\n",
            "Worker 3, [05/16]: Training Loss: 1.118843019, Training Accuracy: 67.440\n",
            "Worker 3, [06/16]: Training Loss: 0.983573555, Training Accuracy: 71.120\n",
            "Worker 3, [07/16]: Training Loss: 0.935688216, Training Accuracy: 72.768\n",
            "Worker 3, [08/16]: Training Loss: 0.852747829, Training Accuracy: 75.136\n",
            "Worker 3, [09/16]: Training Loss: 0.774013737, Training Accuracy: 77.248\n",
            "Worker 3, [10/16]: Training Loss: 0.723839968, Training Accuracy: 78.496\n",
            "Worker 3, [11/16]: Training Loss: 0.675659150, Training Accuracy: 79.632\n",
            "Worker 3, [12/16]: Training Loss: 0.626613920, Training Accuracy: 81.136\n",
            "Worker 3, [13/16]: Training Loss: 0.618361416, Training Accuracy: 82.112\n",
            "Worker 3, [14/16]: Training Loss: 0.577686338, Training Accuracy: 83.344\n",
            "Worker 3, [15/16]: Training Loss: 0.529755522, Training Accuracy: 84.128\n",
            "Worker 3, [16/16]: Training Loss: 0.536029028, Training Accuracy: 83.536\n",
            "Time taken for training worker 3: 0:00:48.050711\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.898571117, Training Accuracy: 48.496\n",
            "Worker 4, [02/16]: Training Loss: 1.549884941, Training Accuracy: 55.696\n",
            "Worker 4, [03/16]: Training Loss: 1.369595375, Training Accuracy: 60.432\n",
            "Worker 4, [04/16]: Training Loss: 1.211998561, Training Accuracy: 64.480\n",
            "Worker 4, [05/16]: Training Loss: 1.116824554, Training Accuracy: 68.032\n",
            "Worker 4, [06/16]: Training Loss: 1.002542884, Training Accuracy: 70.656\n",
            "Worker 4, [07/16]: Training Loss: 0.923451882, Training Accuracy: 72.752\n",
            "Worker 4, [08/16]: Training Loss: 0.857425158, Training Accuracy: 74.864\n",
            "Worker 4, [09/16]: Training Loss: 0.794107329, Training Accuracy: 76.752\n",
            "Worker 4, [10/16]: Training Loss: 0.727539865, Training Accuracy: 78.288\n",
            "Worker 4, [11/16]: Training Loss: 0.701079399, Training Accuracy: 79.664\n",
            "Worker 4, [12/16]: Training Loss: 0.648970035, Training Accuracy: 80.720\n",
            "Worker 4, [13/16]: Training Loss: 0.612013653, Training Accuracy: 81.856\n",
            "Worker 4, [14/16]: Training Loss: 0.578997516, Training Accuracy: 82.848\n",
            "Worker 4, [15/16]: Training Loss: 0.514533062, Training Accuracy: 84.928\n",
            "Worker 4, [16/16]: Training Loss: 0.524522578, Training Accuracy: 84.064\n",
            "Time taken for training worker 4: 0:00:48.649219\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 1.897592815, Training Accuracy: 48.080\n",
            "Worker 5, [02/16]: Training Loss: 1.546684646, Training Accuracy: 56.704\n",
            "Worker 5, [03/16]: Training Loss: 1.361918908, Training Accuracy: 61.248\n",
            "Worker 5, [04/16]: Training Loss: 1.234670016, Training Accuracy: 64.544\n",
            "Worker 5, [05/16]: Training Loss: 1.100856366, Training Accuracy: 67.776\n",
            "Worker 5, [06/16]: Training Loss: 0.976586709, Training Accuracy: 71.024\n",
            "Worker 5, [07/16]: Training Loss: 0.897153530, Training Accuracy: 73.424\n",
            "Worker 5, [08/16]: Training Loss: 0.868456263, Training Accuracy: 74.160\n",
            "Worker 5, [09/16]: Training Loss: 0.794448602, Training Accuracy: 76.624\n",
            "Worker 5, [10/16]: Training Loss: 0.752110342, Training Accuracy: 77.776\n",
            "Worker 5, [11/16]: Training Loss: 0.680442686, Training Accuracy: 80.368\n",
            "Worker 5, [12/16]: Training Loss: 0.638561222, Training Accuracy: 80.896\n",
            "Worker 5, [13/16]: Training Loss: 0.613520473, Training Accuracy: 82.000\n",
            "Worker 5, [14/16]: Training Loss: 0.547469548, Training Accuracy: 83.776\n",
            "Worker 5, [15/16]: Training Loss: 0.560723759, Training Accuracy: 83.744\n",
            "Worker 5, [16/16]: Training Loss: 0.516921650, Training Accuracy: 84.624\n",
            "Time taken for training worker 5: 0:00:49.988150\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 1.942207727, Training Accuracy: 47.760\n",
            "Worker 6, [02/16]: Training Loss: 1.606168652, Training Accuracy: 55.504\n",
            "Worker 6, [03/16]: Training Loss: 1.385575211, Training Accuracy: 60.192\n",
            "Worker 6, [04/16]: Training Loss: 1.247293879, Training Accuracy: 63.744\n",
            "Worker 6, [05/16]: Training Loss: 1.134482307, Training Accuracy: 66.736\n",
            "Worker 6, [06/16]: Training Loss: 1.045379966, Training Accuracy: 69.328\n",
            "Worker 6, [07/16]: Training Loss: 0.925610340, Training Accuracy: 72.464\n",
            "Worker 6, [08/16]: Training Loss: 0.862456727, Training Accuracy: 74.848\n",
            "Worker 6, [09/16]: Training Loss: 0.847567605, Training Accuracy: 74.640\n",
            "Worker 6, [10/16]: Training Loss: 0.753914808, Training Accuracy: 77.232\n",
            "Worker 6, [11/16]: Training Loss: 0.720188181, Training Accuracy: 78.624\n",
            "Worker 6, [12/16]: Training Loss: 0.667141528, Training Accuracy: 80.448\n",
            "Worker 6, [13/16]: Training Loss: 0.630686070, Training Accuracy: 81.776\n",
            "Worker 6, [14/16]: Training Loss: 0.569588853, Training Accuracy: 83.136\n",
            "Worker 6, [15/16]: Training Loss: 0.539193231, Training Accuracy: 84.272\n",
            "Worker 6, [16/16]: Training Loss: 0.509237415, Training Accuracy: 84.144\n",
            "Time taken for training worker 6: 0:00:49.379512\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 1.926692236, Training Accuracy: 47.728\n",
            "Worker 7, [02/16]: Training Loss: 1.580538086, Training Accuracy: 55.136\n",
            "Worker 7, [03/16]: Training Loss: 1.384302769, Training Accuracy: 59.968\n",
            "Worker 7, [04/16]: Training Loss: 1.212309251, Training Accuracy: 65.328\n",
            "Worker 7, [05/16]: Training Loss: 1.139575441, Training Accuracy: 67.184\n",
            "Worker 7, [06/16]: Training Loss: 1.028356142, Training Accuracy: 70.032\n",
            "Worker 7, [07/16]: Training Loss: 0.937128436, Training Accuracy: 73.312\n",
            "Worker 7, [08/16]: Training Loss: 0.874537393, Training Accuracy: 74.592\n",
            "Worker 7, [09/16]: Training Loss: 0.829380235, Training Accuracy: 75.504\n",
            "Worker 7, [10/16]: Training Loss: 0.753973677, Training Accuracy: 78.096\n",
            "Worker 7, [11/16]: Training Loss: 0.706630703, Training Accuracy: 79.248\n",
            "Worker 7, [12/16]: Training Loss: 0.655276731, Training Accuracy: 80.400\n",
            "Worker 7, [13/16]: Training Loss: 0.630863996, Training Accuracy: 81.376\n",
            "Worker 7, [14/16]: Training Loss: 0.598307545, Training Accuracy: 82.080\n",
            "Worker 7, [15/16]: Training Loss: 0.575024346, Training Accuracy: 82.640\n",
            "Worker 7, [16/16]: Training Loss: 0.540207953, Training Accuracy: 84.208\n",
            "Time taken for training worker 7: 0:00:49.695218\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 1.935449527, Training Accuracy: 48.144\n",
            "Worker 8, [02/16]: Training Loss: 1.584736891, Training Accuracy: 55.664\n",
            "Worker 8, [03/16]: Training Loss: 1.394411878, Training Accuracy: 60.512\n",
            "Worker 8, [04/16]: Training Loss: 1.219664703, Training Accuracy: 64.464\n",
            "Worker 8, [05/16]: Training Loss: 1.096791797, Training Accuracy: 68.512\n",
            "Worker 8, [06/16]: Training Loss: 1.002415867, Training Accuracy: 70.816\n",
            "Worker 8, [07/16]: Training Loss: 0.899683508, Training Accuracy: 73.792\n",
            "Worker 8, [08/16]: Training Loss: 0.854443613, Training Accuracy: 74.896\n",
            "Worker 8, [09/16]: Training Loss: 0.800481360, Training Accuracy: 76.896\n",
            "Worker 8, [10/16]: Training Loss: 0.766480311, Training Accuracy: 77.216\n",
            "Worker 8, [11/16]: Training Loss: 0.696369885, Training Accuracy: 80.128\n",
            "Worker 8, [12/16]: Training Loss: 0.661804446, Training Accuracy: 81.200\n",
            "Worker 8, [13/16]: Training Loss: 0.608613305, Training Accuracy: 82.496\n",
            "Worker 8, [14/16]: Training Loss: 0.572741647, Training Accuracy: 82.816\n",
            "Worker 8, [15/16]: Training Loss: 0.571135349, Training Accuracy: 82.976\n",
            "Worker 8, [16/16]: Training Loss: 0.516628103, Training Accuracy: 84.592\n",
            "Time taken for training worker 8: 0:00:49.108046\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.011127\n",
            "Global Update 07: Test Loss: 2.908752156, Test Accuracy: 43.330\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.735084996, Training Accuracy: 52.240\n",
            "Worker 1, [02/16]: Training Loss: 1.432304044, Training Accuracy: 58.768\n",
            "Worker 1, [03/16]: Training Loss: 1.238927999, Training Accuracy: 63.856\n",
            "Worker 1, [04/16]: Training Loss: 1.133179475, Training Accuracy: 67.136\n",
            "Worker 1, [05/16]: Training Loss: 1.012834531, Training Accuracy: 70.416\n",
            "Worker 1, [06/16]: Training Loss: 0.927948662, Training Accuracy: 72.832\n",
            "Worker 1, [07/16]: Training Loss: 0.862290455, Training Accuracy: 75.088\n",
            "Worker 1, [08/16]: Training Loss: 0.808791715, Training Accuracy: 76.336\n",
            "Worker 1, [09/16]: Training Loss: 0.772143911, Training Accuracy: 77.376\n",
            "Worker 1, [10/16]: Training Loss: 0.710495406, Training Accuracy: 79.344\n",
            "Worker 1, [11/16]: Training Loss: 0.652081572, Training Accuracy: 80.512\n",
            "Worker 1, [12/16]: Training Loss: 0.619790331, Training Accuracy: 81.792\n",
            "Worker 1, [13/16]: Training Loss: 0.596679860, Training Accuracy: 83.072\n",
            "Worker 1, [14/16]: Training Loss: 0.547161730, Training Accuracy: 84.192\n",
            "Worker 1, [15/16]: Training Loss: 0.525651160, Training Accuracy: 84.928\n",
            "Worker 1, [16/16]: Training Loss: 0.525957865, Training Accuracy: 84.864\n",
            "Time taken for training worker 1: 0:00:50.199124\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.779350052, Training Accuracy: 52.336\n",
            "Worker 2, [02/16]: Training Loss: 1.431102353, Training Accuracy: 59.440\n",
            "Worker 2, [03/16]: Training Loss: 1.260735770, Training Accuracy: 63.152\n",
            "Worker 2, [04/16]: Training Loss: 1.136374095, Training Accuracy: 67.040\n",
            "Worker 2, [05/16]: Training Loss: 1.050871793, Training Accuracy: 69.424\n",
            "Worker 2, [06/16]: Training Loss: 0.947304081, Training Accuracy: 72.816\n",
            "Worker 2, [07/16]: Training Loss: 0.893558803, Training Accuracy: 73.424\n",
            "Worker 2, [08/16]: Training Loss: 0.832885265, Training Accuracy: 75.840\n",
            "Worker 2, [09/16]: Training Loss: 0.739902341, Training Accuracy: 78.448\n",
            "Worker 2, [10/16]: Training Loss: 0.701911390, Training Accuracy: 79.232\n",
            "Worker 2, [11/16]: Training Loss: 0.687586090, Training Accuracy: 79.920\n",
            "Worker 2, [12/16]: Training Loss: 0.660436950, Training Accuracy: 81.072\n",
            "Worker 2, [13/16]: Training Loss: 0.582552241, Training Accuracy: 83.776\n",
            "Worker 2, [14/16]: Training Loss: 0.560635174, Training Accuracy: 84.224\n",
            "Worker 2, [15/16]: Training Loss: 0.546497852, Training Accuracy: 84.544\n",
            "Worker 2, [16/16]: Training Loss: 0.516601602, Training Accuracy: 85.616\n",
            "Time taken for training worker 2: 0:00:49.055811\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.762812576, Training Accuracy: 52.496\n",
            "Worker 3, [02/16]: Training Loss: 1.429693101, Training Accuracy: 59.536\n",
            "Worker 3, [03/16]: Training Loss: 1.245957550, Training Accuracy: 64.368\n",
            "Worker 3, [04/16]: Training Loss: 1.150480509, Training Accuracy: 66.288\n",
            "Worker 3, [05/16]: Training Loss: 1.053461446, Training Accuracy: 69.456\n",
            "Worker 3, [06/16]: Training Loss: 0.951058127, Training Accuracy: 71.984\n",
            "Worker 3, [07/16]: Training Loss: 0.876103936, Training Accuracy: 74.304\n",
            "Worker 3, [08/16]: Training Loss: 0.797915826, Training Accuracy: 76.352\n",
            "Worker 3, [09/16]: Training Loss: 0.770541173, Training Accuracy: 77.968\n",
            "Worker 3, [10/16]: Training Loss: 0.721484731, Training Accuracy: 79.344\n",
            "Worker 3, [11/16]: Training Loss: 0.689899773, Training Accuracy: 79.552\n",
            "Worker 3, [12/16]: Training Loss: 0.630179765, Training Accuracy: 81.744\n",
            "Worker 3, [13/16]: Training Loss: 0.602813867, Training Accuracy: 82.528\n",
            "Worker 3, [14/16]: Training Loss: 0.570664219, Training Accuracy: 84.080\n",
            "Worker 3, [15/16]: Training Loss: 0.548884764, Training Accuracy: 84.352\n",
            "Worker 3, [16/16]: Training Loss: 0.526216000, Training Accuracy: 85.264\n",
            "Time taken for training worker 3: 0:00:50.081656\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.795639719, Training Accuracy: 51.344\n",
            "Worker 4, [02/16]: Training Loss: 1.421377384, Training Accuracy: 58.944\n",
            "Worker 4, [03/16]: Training Loss: 1.258550212, Training Accuracy: 63.280\n",
            "Worker 4, [04/16]: Training Loss: 1.164088462, Training Accuracy: 65.280\n",
            "Worker 4, [05/16]: Training Loss: 1.041509304, Training Accuracy: 69.952\n",
            "Worker 4, [06/16]: Training Loss: 0.958347287, Training Accuracy: 72.112\n",
            "Worker 4, [07/16]: Training Loss: 0.903941539, Training Accuracy: 73.440\n",
            "Worker 4, [08/16]: Training Loss: 0.840092835, Training Accuracy: 75.088\n",
            "Worker 4, [09/16]: Training Loss: 0.793545537, Training Accuracy: 76.704\n",
            "Worker 4, [10/16]: Training Loss: 0.717856675, Training Accuracy: 79.072\n",
            "Worker 4, [11/16]: Training Loss: 0.707733349, Training Accuracy: 79.392\n",
            "Worker 4, [12/16]: Training Loss: 0.658479794, Training Accuracy: 80.624\n",
            "Worker 4, [13/16]: Training Loss: 0.613285609, Training Accuracy: 82.560\n",
            "Worker 4, [14/16]: Training Loss: 0.571915426, Training Accuracy: 83.664\n",
            "Worker 4, [15/16]: Training Loss: 0.564204610, Training Accuracy: 83.632\n",
            "Worker 4, [16/16]: Training Loss: 0.520149871, Training Accuracy: 84.864\n",
            "Time taken for training worker 4: 0:00:49.523489\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 1.770134383, Training Accuracy: 52.512\n",
            "Worker 5, [02/16]: Training Loss: 1.435714488, Training Accuracy: 58.768\n",
            "Worker 5, [03/16]: Training Loss: 1.259454781, Training Accuracy: 64.400\n",
            "Worker 5, [04/16]: Training Loss: 1.129167326, Training Accuracy: 67.040\n",
            "Worker 5, [05/16]: Training Loss: 1.055856101, Training Accuracy: 69.136\n",
            "Worker 5, [06/16]: Training Loss: 0.938230847, Training Accuracy: 72.496\n",
            "Worker 5, [07/16]: Training Loss: 0.866586173, Training Accuracy: 75.200\n",
            "Worker 5, [08/16]: Training Loss: 0.836039826, Training Accuracy: 75.328\n",
            "Worker 5, [09/16]: Training Loss: 0.764616409, Training Accuracy: 78.192\n",
            "Worker 5, [10/16]: Training Loss: 0.720622715, Training Accuracy: 79.184\n",
            "Worker 5, [11/16]: Training Loss: 0.692765478, Training Accuracy: 79.600\n",
            "Worker 5, [12/16]: Training Loss: 0.623445964, Training Accuracy: 81.872\n",
            "Worker 5, [13/16]: Training Loss: 0.593683552, Training Accuracy: 82.304\n",
            "Worker 5, [14/16]: Training Loss: 0.560516482, Training Accuracy: 83.968\n",
            "Worker 5, [15/16]: Training Loss: 0.534523468, Training Accuracy: 84.368\n",
            "Worker 5, [16/16]: Training Loss: 0.510837858, Training Accuracy: 85.200\n",
            "Time taken for training worker 5: 0:00:47.332365\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 1.814289519, Training Accuracy: 51.168\n",
            "Worker 6, [02/16]: Training Loss: 1.465265304, Training Accuracy: 58.048\n",
            "Worker 6, [03/16]: Training Loss: 1.289729754, Training Accuracy: 62.576\n",
            "Worker 6, [04/16]: Training Loss: 1.135523845, Training Accuracy: 66.704\n",
            "Worker 6, [05/16]: Training Loss: 1.052239591, Training Accuracy: 69.568\n",
            "Worker 6, [06/16]: Training Loss: 0.972770269, Training Accuracy: 71.456\n",
            "Worker 6, [07/16]: Training Loss: 0.910199283, Training Accuracy: 73.040\n",
            "Worker 6, [08/16]: Training Loss: 0.841456514, Training Accuracy: 76.016\n",
            "Worker 6, [09/16]: Training Loss: 0.792202361, Training Accuracy: 76.128\n",
            "Worker 6, [10/16]: Training Loss: 0.749311236, Training Accuracy: 78.064\n",
            "Worker 6, [11/16]: Training Loss: 0.726650952, Training Accuracy: 79.264\n",
            "Worker 6, [12/16]: Training Loss: 0.657610847, Training Accuracy: 81.152\n",
            "Worker 6, [13/16]: Training Loss: 0.643955358, Training Accuracy: 81.264\n",
            "Worker 6, [14/16]: Training Loss: 0.598415794, Training Accuracy: 82.864\n",
            "Worker 6, [15/16]: Training Loss: 0.570316688, Training Accuracy: 83.600\n",
            "Worker 6, [16/16]: Training Loss: 0.515152304, Training Accuracy: 85.312\n",
            "Time taken for training worker 6: 0:00:50.087885\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 1.790312068, Training Accuracy: 52.320\n",
            "Worker 7, [02/16]: Training Loss: 1.456404996, Training Accuracy: 58.752\n",
            "Worker 7, [03/16]: Training Loss: 1.294531957, Training Accuracy: 61.696\n",
            "Worker 7, [04/16]: Training Loss: 1.169112915, Training Accuracy: 65.840\n",
            "Worker 7, [05/16]: Training Loss: 1.077026978, Training Accuracy: 68.688\n",
            "Worker 7, [06/16]: Training Loss: 0.991882169, Training Accuracy: 71.808\n",
            "Worker 7, [07/16]: Training Loss: 0.922819844, Training Accuracy: 73.088\n",
            "Worker 7, [08/16]: Training Loss: 0.851587872, Training Accuracy: 75.456\n",
            "Worker 7, [09/16]: Training Loss: 0.795533189, Training Accuracy: 76.736\n",
            "Worker 7, [10/16]: Training Loss: 0.737477682, Training Accuracy: 78.512\n",
            "Worker 7, [11/16]: Training Loss: 0.695139735, Training Accuracy: 79.792\n",
            "Worker 7, [12/16]: Training Loss: 0.663415945, Training Accuracy: 80.752\n",
            "Worker 7, [13/16]: Training Loss: 0.619908957, Training Accuracy: 82.064\n",
            "Worker 7, [14/16]: Training Loss: 0.582802545, Training Accuracy: 83.312\n",
            "Worker 7, [15/16]: Training Loss: 0.560960537, Training Accuracy: 84.448\n",
            "Worker 7, [16/16]: Training Loss: 0.526502994, Training Accuracy: 84.640\n",
            "Time taken for training worker 7: 0:00:48.910384\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 1.791418995, Training Accuracy: 51.952\n",
            "Worker 8, [02/16]: Training Loss: 1.431468340, Training Accuracy: 59.536\n",
            "Worker 8, [03/16]: Training Loss: 1.290567908, Training Accuracy: 62.512\n",
            "Worker 8, [04/16]: Training Loss: 1.170987061, Training Accuracy: 66.704\n",
            "Worker 8, [05/16]: Training Loss: 1.054122143, Training Accuracy: 69.376\n",
            "Worker 8, [06/16]: Training Loss: 0.969613851, Training Accuracy: 71.376\n",
            "Worker 8, [07/16]: Training Loss: 0.905289134, Training Accuracy: 73.712\n",
            "Worker 8, [08/16]: Training Loss: 0.828899702, Training Accuracy: 76.160\n",
            "Worker 8, [09/16]: Training Loss: 0.787695140, Training Accuracy: 77.200\n",
            "Worker 8, [10/16]: Training Loss: 0.742552538, Training Accuracy: 78.816\n",
            "Worker 8, [11/16]: Training Loss: 0.688884482, Training Accuracy: 80.480\n",
            "Worker 8, [12/16]: Training Loss: 0.644387173, Training Accuracy: 81.216\n",
            "Worker 8, [13/16]: Training Loss: 0.623733299, Training Accuracy: 82.352\n",
            "Worker 8, [14/16]: Training Loss: 0.588325749, Training Accuracy: 83.328\n",
            "Worker 8, [15/16]: Training Loss: 0.557050419, Training Accuracy: 84.320\n",
            "Worker 8, [16/16]: Training Loss: 0.536718008, Training Accuracy: 84.944\n",
            "Time taken for training worker 8: 0:00:49.993446\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004232\n",
            "Global Update 08: Test Loss: 2.912777594, Test Accuracy: 43.540\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.637471158, Training Accuracy: 56.016\n",
            "Worker 1, [02/16]: Training Loss: 1.452318014, Training Accuracy: 58.544\n",
            "Worker 1, [03/16]: Training Loss: 1.318676851, Training Accuracy: 62.112\n",
            "Worker 1, [04/16]: Training Loss: 1.227666344, Training Accuracy: 64.576\n",
            "Worker 1, [05/16]: Training Loss: 1.160996346, Training Accuracy: 66.704\n",
            "Worker 1, [06/16]: Training Loss: 1.136350342, Training Accuracy: 66.304\n",
            "Worker 1, [07/16]: Training Loss: 1.085136727, Training Accuracy: 67.936\n",
            "Worker 1, [08/16]: Training Loss: 1.019513147, Training Accuracy: 70.624\n",
            "Worker 1, [09/16]: Training Loss: 0.974446318, Training Accuracy: 71.264\n",
            "Worker 1, [10/16]: Training Loss: 0.969259073, Training Accuracy: 71.440\n",
            "Worker 1, [11/16]: Training Loss: 0.912873191, Training Accuracy: 73.024\n",
            "Worker 1, [12/16]: Training Loss: 0.874571516, Training Accuracy: 74.096\n",
            "Worker 1, [13/16]: Training Loss: 0.846909702, Training Accuracy: 74.800\n",
            "Worker 1, [14/16]: Training Loss: 0.829627593, Training Accuracy: 75.376\n",
            "Worker 1, [15/16]: Training Loss: 0.804394870, Training Accuracy: 76.704\n",
            "Worker 1, [16/16]: Training Loss: 0.784542261, Training Accuracy: 77.888\n",
            "Time taken for training worker 1: 0:00:48.467836\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.653108733, Training Accuracy: 56.240\n",
            "Worker 2, [02/16]: Training Loss: 1.453838014, Training Accuracy: 58.688\n",
            "Worker 2, [03/16]: Training Loss: 1.357613930, Training Accuracy: 61.408\n",
            "Worker 2, [04/16]: Training Loss: 1.250957277, Training Accuracy: 64.384\n",
            "Worker 2, [05/16]: Training Loss: 1.168654425, Training Accuracy: 65.648\n",
            "Worker 2, [06/16]: Training Loss: 1.141398927, Training Accuracy: 66.576\n",
            "Worker 2, [07/16]: Training Loss: 1.098423867, Training Accuracy: 68.288\n",
            "Worker 2, [08/16]: Training Loss: 1.045564172, Training Accuracy: 69.664\n",
            "Worker 2, [09/16]: Training Loss: 1.014367756, Training Accuracy: 70.016\n",
            "Worker 2, [10/16]: Training Loss: 0.970928485, Training Accuracy: 71.696\n",
            "Worker 2, [11/16]: Training Loss: 0.926413959, Training Accuracy: 73.328\n",
            "Worker 2, [12/16]: Training Loss: 0.903918594, Training Accuracy: 72.832\n",
            "Worker 2, [13/16]: Training Loss: 0.867348304, Training Accuracy: 75.408\n",
            "Worker 2, [14/16]: Training Loss: 0.846631340, Training Accuracy: 75.424\n",
            "Worker 2, [15/16]: Training Loss: 0.819128262, Training Accuracy: 76.496\n",
            "Worker 2, [16/16]: Training Loss: 0.781434050, Training Accuracy: 77.632\n",
            "Time taken for training worker 2: 0:00:49.361712\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.658882159, Training Accuracy: 55.792\n",
            "Worker 3, [02/16]: Training Loss: 1.457867236, Training Accuracy: 58.080\n",
            "Worker 3, [03/16]: Training Loss: 1.346847818, Training Accuracy: 60.864\n",
            "Worker 3, [04/16]: Training Loss: 1.250910592, Training Accuracy: 63.600\n",
            "Worker 3, [05/16]: Training Loss: 1.187639158, Training Accuracy: 64.592\n",
            "Worker 3, [06/16]: Training Loss: 1.141263924, Training Accuracy: 66.576\n",
            "Worker 3, [07/16]: Training Loss: 1.099514936, Training Accuracy: 67.760\n",
            "Worker 3, [08/16]: Training Loss: 1.032688388, Training Accuracy: 69.792\n",
            "Worker 3, [09/16]: Training Loss: 0.978444443, Training Accuracy: 70.880\n",
            "Worker 3, [10/16]: Training Loss: 0.957441038, Training Accuracy: 71.504\n",
            "Worker 3, [11/16]: Training Loss: 0.943636228, Training Accuracy: 72.560\n",
            "Worker 3, [12/16]: Training Loss: 0.887959913, Training Accuracy: 74.064\n",
            "Worker 3, [13/16]: Training Loss: 0.874042762, Training Accuracy: 74.928\n",
            "Worker 3, [14/16]: Training Loss: 0.832432850, Training Accuracy: 75.520\n",
            "Worker 3, [15/16]: Training Loss: 0.823834578, Training Accuracy: 76.176\n",
            "Worker 3, [16/16]: Training Loss: 0.765470695, Training Accuracy: 77.808\n",
            "Time taken for training worker 3: 0:00:48.743459\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.705774443, Training Accuracy: 55.504\n",
            "Worker 4, [02/16]: Training Loss: 1.475358826, Training Accuracy: 58.288\n",
            "Worker 4, [03/16]: Training Loss: 1.363705790, Training Accuracy: 60.432\n",
            "Worker 4, [04/16]: Training Loss: 1.272996758, Training Accuracy: 63.088\n",
            "Worker 4, [05/16]: Training Loss: 1.224131304, Training Accuracy: 64.016\n",
            "Worker 4, [06/16]: Training Loss: 1.148174421, Training Accuracy: 65.680\n",
            "Worker 4, [07/16]: Training Loss: 1.115812528, Training Accuracy: 66.880\n",
            "Worker 4, [08/16]: Training Loss: 1.040909083, Training Accuracy: 69.152\n",
            "Worker 4, [09/16]: Training Loss: 1.016574970, Training Accuracy: 69.872\n",
            "Worker 4, [10/16]: Training Loss: 0.967315997, Training Accuracy: 71.568\n",
            "Worker 4, [11/16]: Training Loss: 0.930381551, Training Accuracy: 71.952\n",
            "Worker 4, [12/16]: Training Loss: 0.909928120, Training Accuracy: 72.960\n",
            "Worker 4, [13/16]: Training Loss: 0.878581154, Training Accuracy: 73.792\n",
            "Worker 4, [14/16]: Training Loss: 0.840743390, Training Accuracy: 75.136\n",
            "Worker 4, [15/16]: Training Loss: 0.815474892, Training Accuracy: 76.400\n",
            "Worker 4, [16/16]: Training Loss: 0.798399833, Training Accuracy: 76.960\n",
            "Time taken for training worker 4: 0:00:48.293249\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 1.676955042, Training Accuracy: 55.696\n",
            "Worker 5, [02/16]: Training Loss: 1.467888602, Training Accuracy: 58.720\n",
            "Worker 5, [03/16]: Training Loss: 1.346310578, Training Accuracy: 60.608\n",
            "Worker 5, [04/16]: Training Loss: 1.244547970, Training Accuracy: 64.064\n",
            "Worker 5, [05/16]: Training Loss: 1.204994006, Training Accuracy: 65.536\n",
            "Worker 5, [06/16]: Training Loss: 1.131313064, Training Accuracy: 66.928\n",
            "Worker 5, [07/16]: Training Loss: 1.066167168, Training Accuracy: 68.544\n",
            "Worker 5, [08/16]: Training Loss: 1.038051648, Training Accuracy: 69.424\n",
            "Worker 5, [09/16]: Training Loss: 0.987237330, Training Accuracy: 71.440\n",
            "Worker 5, [10/16]: Training Loss: 0.964078904, Training Accuracy: 71.328\n",
            "Worker 5, [11/16]: Training Loss: 0.933129100, Training Accuracy: 72.560\n",
            "Worker 5, [12/16]: Training Loss: 0.885815035, Training Accuracy: 73.856\n",
            "Worker 5, [13/16]: Training Loss: 0.844508291, Training Accuracy: 75.856\n",
            "Worker 5, [14/16]: Training Loss: 0.849196939, Training Accuracy: 75.392\n",
            "Worker 5, [15/16]: Training Loss: 0.824666983, Training Accuracy: 76.400\n",
            "Worker 5, [16/16]: Training Loss: 0.778153556, Training Accuracy: 77.504\n",
            "Time taken for training worker 5: 0:00:50.342745\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 1.695205140, Training Accuracy: 55.024\n",
            "Worker 6, [02/16]: Training Loss: 1.501234919, Training Accuracy: 57.744\n",
            "Worker 6, [03/16]: Training Loss: 1.357449907, Training Accuracy: 60.272\n",
            "Worker 6, [04/16]: Training Loss: 1.274467681, Training Accuracy: 62.320\n",
            "Worker 6, [05/16]: Training Loss: 1.215169520, Training Accuracy: 64.240\n",
            "Worker 6, [06/16]: Training Loss: 1.165934870, Training Accuracy: 65.488\n",
            "Worker 6, [07/16]: Training Loss: 1.080285255, Training Accuracy: 68.224\n",
            "Worker 6, [08/16]: Training Loss: 1.056910667, Training Accuracy: 68.992\n",
            "Worker 6, [09/16]: Training Loss: 1.015378065, Training Accuracy: 69.872\n",
            "Worker 6, [10/16]: Training Loss: 0.986606186, Training Accuracy: 70.272\n",
            "Worker 6, [11/16]: Training Loss: 0.958027663, Training Accuracy: 71.344\n",
            "Worker 6, [12/16]: Training Loss: 0.917284621, Training Accuracy: 73.152\n",
            "Worker 6, [13/16]: Training Loss: 0.888199688, Training Accuracy: 73.856\n",
            "Worker 6, [14/16]: Training Loss: 0.864654109, Training Accuracy: 74.496\n",
            "Worker 6, [15/16]: Training Loss: 0.841200513, Training Accuracy: 75.184\n",
            "Worker 6, [16/16]: Training Loss: 0.826293151, Training Accuracy: 75.968\n",
            "Time taken for training worker 6: 0:00:49.100007\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 1.689464781, Training Accuracy: 55.360\n",
            "Worker 7, [02/16]: Training Loss: 1.481873387, Training Accuracy: 58.544\n",
            "Worker 7, [03/16]: Training Loss: 1.359931066, Training Accuracy: 60.464\n",
            "Worker 7, [04/16]: Training Loss: 1.274056582, Training Accuracy: 63.360\n",
            "Worker 7, [05/16]: Training Loss: 1.214037372, Training Accuracy: 64.448\n",
            "Worker 7, [06/16]: Training Loss: 1.137924631, Training Accuracy: 67.040\n",
            "Worker 7, [07/16]: Training Loss: 1.107684739, Training Accuracy: 67.568\n",
            "Worker 7, [08/16]: Training Loss: 1.065290002, Training Accuracy: 69.408\n",
            "Worker 7, [09/16]: Training Loss: 1.034385523, Training Accuracy: 69.504\n",
            "Worker 7, [10/16]: Training Loss: 0.995235544, Training Accuracy: 70.704\n",
            "Worker 7, [11/16]: Training Loss: 0.962399167, Training Accuracy: 71.376\n",
            "Worker 7, [12/16]: Training Loss: 0.922999583, Training Accuracy: 73.104\n",
            "Worker 7, [13/16]: Training Loss: 0.892360705, Training Accuracy: 74.112\n",
            "Worker 7, [14/16]: Training Loss: 0.870749462, Training Accuracy: 74.128\n",
            "Worker 7, [15/16]: Training Loss: 0.817561157, Training Accuracy: 76.032\n",
            "Worker 7, [16/16]: Training Loss: 0.800368998, Training Accuracy: 76.784\n",
            "Time taken for training worker 7: 0:00:47.971686\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 1.718483169, Training Accuracy: 54.784\n",
            "Worker 8, [02/16]: Training Loss: 1.473908837, Training Accuracy: 58.736\n",
            "Worker 8, [03/16]: Training Loss: 1.351202226, Training Accuracy: 61.424\n",
            "Worker 8, [04/16]: Training Loss: 1.263130563, Training Accuracy: 63.344\n",
            "Worker 8, [05/16]: Training Loss: 1.213529650, Training Accuracy: 64.192\n",
            "Worker 8, [06/16]: Training Loss: 1.156865443, Training Accuracy: 67.200\n",
            "Worker 8, [07/16]: Training Loss: 1.101864120, Training Accuracy: 67.792\n",
            "Worker 8, [08/16]: Training Loss: 1.071277729, Training Accuracy: 68.624\n",
            "Worker 8, [09/16]: Training Loss: 1.024763183, Training Accuracy: 69.936\n",
            "Worker 8, [10/16]: Training Loss: 0.973642164, Training Accuracy: 71.232\n",
            "Worker 8, [11/16]: Training Loss: 0.934331506, Training Accuracy: 72.720\n",
            "Worker 8, [12/16]: Training Loss: 0.908423295, Training Accuracy: 73.376\n",
            "Worker 8, [13/16]: Training Loss: 0.853731716, Training Accuracy: 75.280\n",
            "Worker 8, [14/16]: Training Loss: 0.851044567, Training Accuracy: 75.104\n",
            "Worker 8, [15/16]: Training Loss: 0.821254339, Training Accuracy: 76.192\n",
            "Worker 8, [16/16]: Training Loss: 0.793935668, Training Accuracy: 77.104\n",
            "Time taken for training worker 8: 0:00:49.392852\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004453\n",
            "Global Update 09: Test Loss: 2.663717090, Test Accuracy: 43.500\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:59:28.733204\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:32\n",
            "==================================================\n",
            "Worker 1, [01/32]: Training Loss: 4.596800439, Training Accuracy: 1.584\n",
            "Worker 1, [02/32]: Training Loss: 4.408742691, Training Accuracy: 3.536\n",
            "Worker 1, [03/32]: Training Loss: 4.159892875, Training Accuracy: 5.872\n",
            "Worker 1, [04/32]: Training Loss: 4.014487510, Training Accuracy: 7.152\n",
            "Worker 1, [05/32]: Training Loss: 3.900653126, Training Accuracy: 8.672\n",
            "Worker 1, [06/32]: Training Loss: 3.807511950, Training Accuracy: 10.272\n",
            "Worker 1, [07/32]: Training Loss: 3.735249624, Training Accuracy: 11.728\n",
            "Worker 1, [08/32]: Training Loss: 3.647406527, Training Accuracy: 12.960\n",
            "Worker 1, [09/32]: Training Loss: 3.575619863, Training Accuracy: 13.968\n",
            "Worker 1, [10/32]: Training Loss: 3.500001027, Training Accuracy: 14.816\n",
            "Worker 1, [11/32]: Training Loss: 3.429272328, Training Accuracy: 16.384\n",
            "Worker 1, [12/32]: Training Loss: 3.373330082, Training Accuracy: 16.960\n",
            "Worker 1, [13/32]: Training Loss: 3.307308591, Training Accuracy: 17.952\n",
            "Worker 1, [14/32]: Training Loss: 3.227643290, Training Accuracy: 19.312\n",
            "Worker 1, [15/32]: Training Loss: 3.177792357, Training Accuracy: 20.960\n",
            "Worker 1, [16/32]: Training Loss: 3.107692665, Training Accuracy: 21.648\n",
            "Worker 1, [17/32]: Training Loss: 3.048954171, Training Accuracy: 22.592\n",
            "Worker 1, [18/32]: Training Loss: 2.972789134, Training Accuracy: 25.088\n",
            "Worker 1, [19/32]: Training Loss: 2.904759006, Training Accuracy: 25.184\n",
            "Worker 1, [20/32]: Training Loss: 2.884741190, Training Accuracy: 26.704\n",
            "Worker 1, [21/32]: Training Loss: 2.777073846, Training Accuracy: 27.776\n",
            "Worker 1, [22/32]: Training Loss: 2.745531503, Training Accuracy: 28.400\n",
            "Worker 1, [23/32]: Training Loss: 2.709272759, Training Accuracy: 29.568\n",
            "Worker 1, [24/32]: Training Loss: 2.650773949, Training Accuracy: 30.544\n",
            "Worker 1, [25/32]: Training Loss: 2.593948257, Training Accuracy: 31.760\n",
            "Worker 1, [26/32]: Training Loss: 2.541227792, Training Accuracy: 33.200\n",
            "Worker 1, [27/32]: Training Loss: 2.477126544, Training Accuracy: 33.840\n",
            "Worker 1, [28/32]: Training Loss: 2.394402047, Training Accuracy: 36.096\n",
            "Worker 1, [29/32]: Training Loss: 2.351697367, Training Accuracy: 36.624\n",
            "Worker 1, [30/32]: Training Loss: 2.310430752, Training Accuracy: 38.464\n",
            "Worker 1, [31/32]: Training Loss: 2.222039208, Training Accuracy: 39.760\n",
            "Worker 1, [32/32]: Training Loss: 2.207099173, Training Accuracy: 40.160\n",
            "Time taken for training worker 1: 0:01:39.164217\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 4.596425893, Training Accuracy: 1.792\n",
            "Worker 2, [02/32]: Training Loss: 4.421379318, Training Accuracy: 3.520\n",
            "Worker 2, [03/32]: Training Loss: 4.191213345, Training Accuracy: 5.440\n",
            "Worker 2, [04/32]: Training Loss: 4.044691266, Training Accuracy: 7.568\n",
            "Worker 2, [05/32]: Training Loss: 3.924039739, Training Accuracy: 9.552\n",
            "Worker 2, [06/32]: Training Loss: 3.826431793, Training Accuracy: 10.944\n",
            "Worker 2, [07/32]: Training Loss: 3.718923379, Training Accuracy: 12.832\n",
            "Worker 2, [08/32]: Training Loss: 3.618266424, Training Accuracy: 14.272\n",
            "Worker 2, [09/32]: Training Loss: 3.554760342, Training Accuracy: 14.352\n",
            "Worker 2, [10/32]: Training Loss: 3.458140502, Training Accuracy: 16.464\n",
            "Worker 2, [11/32]: Training Loss: 3.387679032, Training Accuracy: 17.600\n",
            "Worker 2, [12/32]: Training Loss: 3.307143367, Training Accuracy: 19.040\n",
            "Worker 2, [13/32]: Training Loss: 3.254904793, Training Accuracy: 20.304\n",
            "Worker 2, [14/32]: Training Loss: 3.157130910, Training Accuracy: 21.920\n",
            "Worker 2, [15/32]: Training Loss: 3.108054044, Training Accuracy: 22.336\n",
            "Worker 2, [16/32]: Training Loss: 3.066993020, Training Accuracy: 23.360\n",
            "Worker 2, [17/32]: Training Loss: 3.000554104, Training Accuracy: 24.400\n",
            "Worker 2, [18/32]: Training Loss: 2.936255983, Training Accuracy: 25.712\n",
            "Worker 2, [19/32]: Training Loss: 2.874267060, Training Accuracy: 26.560\n",
            "Worker 2, [20/32]: Training Loss: 2.815571240, Training Accuracy: 27.296\n",
            "Worker 2, [21/32]: Training Loss: 2.757585757, Training Accuracy: 28.368\n",
            "Worker 2, [22/32]: Training Loss: 2.709366796, Training Accuracy: 29.840\n",
            "Worker 2, [23/32]: Training Loss: 2.638848402, Training Accuracy: 31.024\n",
            "Worker 2, [24/32]: Training Loss: 2.592189740, Training Accuracy: 31.824\n",
            "Worker 2, [25/32]: Training Loss: 2.555971705, Training Accuracy: 31.840\n",
            "Worker 2, [26/32]: Training Loss: 2.491325667, Training Accuracy: 33.936\n",
            "Worker 2, [27/32]: Training Loss: 2.432662645, Training Accuracy: 34.272\n",
            "Worker 2, [28/32]: Training Loss: 2.383115957, Training Accuracy: 35.968\n",
            "Worker 2, [29/32]: Training Loss: 2.333551297, Training Accuracy: 36.752\n",
            "Worker 2, [30/32]: Training Loss: 2.271905339, Training Accuracy: 38.368\n",
            "Worker 2, [31/32]: Training Loss: 2.229822962, Training Accuracy: 39.392\n",
            "Worker 2, [32/32]: Training Loss: 2.153021873, Training Accuracy: 40.080\n",
            "Time taken for training worker 2: 0:01:59.976323\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 4.596470191, Training Accuracy: 1.312\n",
            "Worker 3, [02/32]: Training Loss: 4.412368001, Training Accuracy: 3.312\n",
            "Worker 3, [03/32]: Training Loss: 4.176153925, Training Accuracy: 5.840\n",
            "Worker 3, [04/32]: Training Loss: 4.034227705, Training Accuracy: 7.888\n",
            "Worker 3, [05/32]: Training Loss: 3.935742381, Training Accuracy: 8.880\n",
            "Worker 3, [06/32]: Training Loss: 3.832569533, Training Accuracy: 10.688\n",
            "Worker 3, [07/32]: Training Loss: 3.735745883, Training Accuracy: 11.872\n",
            "Worker 3, [08/32]: Training Loss: 3.654735694, Training Accuracy: 13.280\n",
            "Worker 3, [09/32]: Training Loss: 3.563068821, Training Accuracy: 14.320\n",
            "Worker 3, [10/32]: Training Loss: 3.498552865, Training Accuracy: 15.440\n",
            "Worker 3, [11/32]: Training Loss: 3.422681942, Training Accuracy: 17.504\n",
            "Worker 3, [12/32]: Training Loss: 3.354292500, Training Accuracy: 18.064\n",
            "Worker 3, [13/32]: Training Loss: 3.254240622, Training Accuracy: 19.744\n",
            "Worker 3, [14/32]: Training Loss: 3.213937592, Training Accuracy: 20.896\n",
            "Worker 3, [15/32]: Training Loss: 3.136826359, Training Accuracy: 22.016\n",
            "Worker 3, [16/32]: Training Loss: 3.067958022, Training Accuracy: 22.496\n",
            "Worker 3, [17/32]: Training Loss: 3.034436143, Training Accuracy: 23.616\n",
            "Worker 3, [18/32]: Training Loss: 2.954162213, Training Accuracy: 25.104\n",
            "Worker 3, [19/32]: Training Loss: 2.863773857, Training Accuracy: 26.336\n",
            "Worker 3, [20/32]: Training Loss: 2.844093070, Training Accuracy: 27.120\n",
            "Worker 3, [21/32]: Training Loss: 2.790228228, Training Accuracy: 28.000\n",
            "Worker 3, [22/32]: Training Loss: 2.703874649, Training Accuracy: 29.344\n",
            "Worker 3, [23/32]: Training Loss: 2.625173218, Training Accuracy: 30.848\n",
            "Worker 3, [24/32]: Training Loss: 2.610450613, Training Accuracy: 30.928\n",
            "Worker 3, [25/32]: Training Loss: 2.555036017, Training Accuracy: 32.688\n",
            "Worker 3, [26/32]: Training Loss: 2.503254401, Training Accuracy: 33.776\n",
            "Worker 3, [27/32]: Training Loss: 2.416816607, Training Accuracy: 35.648\n",
            "Worker 3, [28/32]: Training Loss: 2.362526506, Training Accuracy: 36.528\n",
            "Worker 3, [29/32]: Training Loss: 2.309923998, Training Accuracy: 37.376\n",
            "Worker 3, [30/32]: Training Loss: 2.258172422, Training Accuracy: 39.136\n",
            "Worker 3, [31/32]: Training Loss: 2.192182052, Training Accuracy: 40.256\n",
            "Worker 3, [32/32]: Training Loss: 2.139349923, Training Accuracy: 41.744\n",
            "Time taken for training worker 3: 0:02:54.603544\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 4.596836932, Training Accuracy: 1.552\n",
            "Worker 4, [02/32]: Training Loss: 4.402813751, Training Accuracy: 4.192\n",
            "Worker 4, [03/32]: Training Loss: 4.167041776, Training Accuracy: 5.568\n",
            "Worker 4, [04/32]: Training Loss: 4.026253048, Training Accuracy: 7.392\n",
            "Worker 4, [05/32]: Training Loss: 3.914980896, Training Accuracy: 8.912\n",
            "Worker 4, [06/32]: Training Loss: 3.824307751, Training Accuracy: 10.608\n",
            "Worker 4, [07/32]: Training Loss: 3.749355567, Training Accuracy: 11.408\n",
            "Worker 4, [08/32]: Training Loss: 3.655195185, Training Accuracy: 12.832\n",
            "Worker 4, [09/32]: Training Loss: 3.571741763, Training Accuracy: 14.288\n",
            "Worker 4, [10/32]: Training Loss: 3.507013769, Training Accuracy: 15.392\n",
            "Worker 4, [11/32]: Training Loss: 3.416657810, Training Accuracy: 17.104\n",
            "Worker 4, [12/32]: Training Loss: 3.330896930, Training Accuracy: 18.544\n",
            "Worker 4, [13/32]: Training Loss: 3.303425813, Training Accuracy: 18.704\n",
            "Worker 4, [14/32]: Training Loss: 3.227140964, Training Accuracy: 20.064\n",
            "Worker 4, [15/32]: Training Loss: 3.134439232, Training Accuracy: 21.472\n",
            "Worker 4, [16/32]: Training Loss: 3.092078277, Training Accuracy: 22.640\n",
            "Worker 4, [17/32]: Training Loss: 3.011040936, Training Accuracy: 23.424\n",
            "Worker 4, [18/32]: Training Loss: 2.951837676, Training Accuracy: 25.072\n",
            "Worker 4, [19/32]: Training Loss: 2.902190926, Training Accuracy: 26.304\n",
            "Worker 4, [20/32]: Training Loss: 2.836618732, Training Accuracy: 27.328\n",
            "Worker 4, [21/32]: Training Loss: 2.790178654, Training Accuracy: 27.312\n",
            "Worker 4, [22/32]: Training Loss: 2.716781001, Training Accuracy: 30.032\n",
            "Worker 4, [23/32]: Training Loss: 2.674675046, Training Accuracy: 29.920\n",
            "Worker 4, [24/32]: Training Loss: 2.626400605, Training Accuracy: 31.200\n",
            "Worker 4, [25/32]: Training Loss: 2.560091584, Training Accuracy: 32.112\n",
            "Worker 4, [26/32]: Training Loss: 2.502527689, Training Accuracy: 34.352\n",
            "Worker 4, [27/32]: Training Loss: 2.465048741, Training Accuracy: 33.728\n",
            "Worker 4, [28/32]: Training Loss: 2.406893351, Training Accuracy: 36.272\n",
            "Worker 4, [29/32]: Training Loss: 2.352662229, Training Accuracy: 36.144\n",
            "Worker 4, [30/32]: Training Loss: 2.352789454, Training Accuracy: 36.752\n",
            "Worker 4, [31/32]: Training Loss: 2.258323854, Training Accuracy: 38.720\n",
            "Worker 4, [32/32]: Training Loss: 2.199218513, Training Accuracy: 39.680\n",
            "Time taken for training worker 4: 0:03:13.883251\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/32]: Training Loss: 4.595564589, Training Accuracy: 1.408\n",
            "Worker 5, [02/32]: Training Loss: 4.404999397, Training Accuracy: 3.632\n",
            "Worker 5, [03/32]: Training Loss: 4.182789897, Training Accuracy: 5.232\n",
            "Worker 5, [04/32]: Training Loss: 4.046844178, Training Accuracy: 7.072\n",
            "Worker 5, [05/32]: Training Loss: 3.945782795, Training Accuracy: 9.008\n",
            "Worker 5, [06/32]: Training Loss: 3.858585370, Training Accuracy: 9.760\n",
            "Worker 5, [07/32]: Training Loss: 3.764321449, Training Accuracy: 11.424\n",
            "Worker 5, [08/32]: Training Loss: 3.676259542, Training Accuracy: 12.560\n",
            "Worker 5, [09/32]: Training Loss: 3.605459758, Training Accuracy: 13.536\n",
            "Worker 5, [10/32]: Training Loss: 3.533703539, Training Accuracy: 15.008\n",
            "Worker 5, [11/32]: Training Loss: 3.468979186, Training Accuracy: 16.048\n",
            "Worker 5, [12/32]: Training Loss: 3.374756212, Training Accuracy: 17.984\n",
            "Worker 5, [13/32]: Training Loss: 3.272500588, Training Accuracy: 18.640\n",
            "Worker 5, [14/32]: Training Loss: 3.244349472, Training Accuracy: 19.728\n",
            "Worker 5, [15/32]: Training Loss: 3.197111225, Training Accuracy: 20.560\n",
            "Worker 5, [16/32]: Training Loss: 3.117137461, Training Accuracy: 22.304\n",
            "Worker 5, [17/32]: Training Loss: 3.029142703, Training Accuracy: 23.568\n",
            "Worker 5, [18/32]: Training Loss: 2.946014222, Training Accuracy: 25.040\n",
            "Worker 5, [19/32]: Training Loss: 2.927044401, Training Accuracy: 25.680\n",
            "Worker 5, [20/32]: Training Loss: 2.866508523, Training Accuracy: 26.464\n",
            "Worker 5, [21/32]: Training Loss: 2.795634858, Training Accuracy: 27.792\n",
            "Worker 5, [22/32]: Training Loss: 2.720857971, Training Accuracy: 28.368\n",
            "Worker 5, [23/32]: Training Loss: 2.700322424, Training Accuracy: 30.080\n",
            "Worker 5, [24/32]: Training Loss: 2.669642804, Training Accuracy: 29.888\n",
            "Worker 5, [25/32]: Training Loss: 2.539397785, Training Accuracy: 32.592\n",
            "Worker 5, [26/32]: Training Loss: 2.542401589, Training Accuracy: 32.720\n",
            "Worker 5, [27/32]: Training Loss: 2.482823695, Training Accuracy: 33.568\n",
            "Worker 5, [28/32]: Training Loss: 2.409584118, Training Accuracy: 35.408\n",
            "Worker 5, [29/32]: Training Loss: 2.350461292, Training Accuracy: 37.184\n",
            "Worker 5, [30/32]: Training Loss: 2.319394676, Training Accuracy: 38.128\n",
            "Worker 5, [31/32]: Training Loss: 2.263994123, Training Accuracy: 38.848\n",
            "Worker 5, [32/32]: Training Loss: 2.191009574, Training Accuracy: 40.848\n",
            "Time taken for training worker 5: 0:03:20.675936\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/32]: Training Loss: 4.594551641, Training Accuracy: 1.952\n",
            "Worker 6, [02/32]: Training Loss: 4.412678251, Training Accuracy: 3.376\n",
            "Worker 6, [03/32]: Training Loss: 4.175364813, Training Accuracy: 5.472\n",
            "Worker 6, [04/32]: Training Loss: 4.065792322, Training Accuracy: 6.880\n",
            "Worker 6, [05/32]: Training Loss: 3.941016416, Training Accuracy: 8.640\n",
            "Worker 6, [06/32]: Training Loss: 3.846435128, Training Accuracy: 9.984\n",
            "Worker 6, [07/32]: Training Loss: 3.759983513, Training Accuracy: 11.264\n",
            "Worker 6, [08/32]: Training Loss: 3.675407106, Training Accuracy: 12.032\n",
            "Worker 6, [09/32]: Training Loss: 3.600993821, Training Accuracy: 14.096\n",
            "Worker 6, [10/32]: Training Loss: 3.524015777, Training Accuracy: 15.440\n",
            "Worker 6, [11/32]: Training Loss: 3.433307691, Training Accuracy: 16.656\n",
            "Worker 6, [12/32]: Training Loss: 3.364120262, Training Accuracy: 17.936\n",
            "Worker 6, [13/32]: Training Loss: 3.266731720, Training Accuracy: 19.968\n",
            "Worker 6, [14/32]: Training Loss: 3.217337088, Training Accuracy: 19.632\n",
            "Worker 6, [15/32]: Training Loss: 3.154066297, Training Accuracy: 21.392\n",
            "Worker 6, [16/32]: Training Loss: 3.113842088, Training Accuracy: 22.064\n",
            "Worker 6, [17/32]: Training Loss: 2.997075889, Training Accuracy: 23.664\n",
            "Worker 6, [18/32]: Training Loss: 2.961481260, Training Accuracy: 24.864\n",
            "Worker 6, [19/32]: Training Loss: 2.875761090, Training Accuracy: 26.128\n",
            "Worker 6, [20/32]: Training Loss: 2.826772583, Training Accuracy: 27.008\n",
            "Worker 6, [21/32]: Training Loss: 2.774780826, Training Accuracy: 28.080\n",
            "Worker 6, [22/32]: Training Loss: 2.740467237, Training Accuracy: 29.120\n",
            "Worker 6, [23/32]: Training Loss: 2.645755357, Training Accuracy: 30.640\n",
            "Worker 6, [24/32]: Training Loss: 2.602690040, Training Accuracy: 31.280\n",
            "Worker 6, [25/32]: Training Loss: 2.538931704, Training Accuracy: 33.184\n",
            "Worker 6, [26/32]: Training Loss: 2.458096546, Training Accuracy: 34.784\n",
            "Worker 6, [27/32]: Training Loss: 2.421571095, Training Accuracy: 35.584\n",
            "Worker 6, [28/32]: Training Loss: 2.411219520, Training Accuracy: 35.296\n",
            "Worker 6, [29/32]: Training Loss: 2.348684172, Training Accuracy: 36.672\n",
            "Worker 6, [30/32]: Training Loss: 2.284751840, Training Accuracy: 37.520\n",
            "Worker 6, [31/32]: Training Loss: 2.214543121, Training Accuracy: 40.256\n",
            "Worker 6, [32/32]: Training Loss: 2.165040748, Training Accuracy: 41.248\n",
            "Time taken for training worker 6: 0:03:20.895086\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/32]: Training Loss: 4.597431776, Training Accuracy: 1.712\n",
            "Worker 7, [02/32]: Training Loss: 4.433376950, Training Accuracy: 3.088\n",
            "Worker 7, [03/32]: Training Loss: 4.219680523, Training Accuracy: 5.344\n",
            "Worker 7, [04/32]: Training Loss: 4.068530375, Training Accuracy: 7.264\n",
            "Worker 7, [05/32]: Training Loss: 3.965532755, Training Accuracy: 8.672\n",
            "Worker 7, [06/32]: Training Loss: 3.862947423, Training Accuracy: 10.384\n",
            "Worker 7, [07/32]: Training Loss: 3.769372137, Training Accuracy: 11.456\n",
            "Worker 7, [08/32]: Training Loss: 3.677997210, Training Accuracy: 12.864\n",
            "Worker 7, [09/32]: Training Loss: 3.598508840, Training Accuracy: 13.344\n",
            "Worker 7, [10/32]: Training Loss: 3.540729574, Training Accuracy: 15.120\n",
            "Worker 7, [11/32]: Training Loss: 3.433943581, Training Accuracy: 16.496\n",
            "Worker 7, [12/32]: Training Loss: 3.395213151, Training Accuracy: 17.280\n",
            "Worker 7, [13/32]: Training Loss: 3.324412448, Training Accuracy: 18.496\n",
            "Worker 7, [14/32]: Training Loss: 3.220142094, Training Accuracy: 20.304\n",
            "Worker 7, [15/32]: Training Loss: 3.178205191, Training Accuracy: 21.328\n",
            "Worker 7, [16/32]: Training Loss: 3.119862666, Training Accuracy: 22.448\n",
            "Worker 7, [17/32]: Training Loss: 3.083586921, Training Accuracy: 22.256\n",
            "Worker 7, [18/32]: Training Loss: 3.001637610, Training Accuracy: 24.272\n",
            "Worker 7, [19/32]: Training Loss: 2.940227521, Training Accuracy: 24.816\n",
            "Worker 7, [20/32]: Training Loss: 2.888882571, Training Accuracy: 25.952\n",
            "Worker 7, [21/32]: Training Loss: 2.829171035, Training Accuracy: 26.768\n",
            "Worker 7, [22/32]: Training Loss: 2.785581854, Training Accuracy: 28.528\n",
            "Worker 7, [23/32]: Training Loss: 2.737083029, Training Accuracy: 27.888\n",
            "Worker 7, [24/32]: Training Loss: 2.688629262, Training Accuracy: 29.952\n",
            "Worker 7, [25/32]: Training Loss: 2.621197759, Training Accuracy: 30.800\n",
            "Worker 7, [26/32]: Training Loss: 2.548443595, Training Accuracy: 32.128\n",
            "Worker 7, [27/32]: Training Loss: 2.516712690, Training Accuracy: 33.184\n",
            "Worker 7, [28/32]: Training Loss: 2.458240626, Training Accuracy: 34.368\n",
            "Worker 7, [29/32]: Training Loss: 2.420894294, Training Accuracy: 35.504\n",
            "Worker 7, [30/32]: Training Loss: 2.394464169, Training Accuracy: 36.384\n",
            "Worker 7, [31/32]: Training Loss: 2.334797294, Training Accuracy: 36.880\n",
            "Worker 7, [32/32]: Training Loss: 2.265065758, Training Accuracy: 38.976\n",
            "Time taken for training worker 7: 0:03:22.921023\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/32]: Training Loss: 4.594345623, Training Accuracy: 2.432\n",
            "Worker 8, [02/32]: Training Loss: 4.389464446, Training Accuracy: 3.904\n",
            "Worker 8, [03/32]: Training Loss: 4.163569810, Training Accuracy: 6.304\n",
            "Worker 8, [04/32]: Training Loss: 4.028060799, Training Accuracy: 7.024\n",
            "Worker 8, [05/32]: Training Loss: 3.895320634, Training Accuracy: 9.168\n",
            "Worker 8, [06/32]: Training Loss: 3.808566205, Training Accuracy: 10.672\n",
            "Worker 8, [07/32]: Training Loss: 3.712910662, Training Accuracy: 12.160\n",
            "Worker 8, [08/32]: Training Loss: 3.641444342, Training Accuracy: 12.400\n",
            "Worker 8, [09/32]: Training Loss: 3.595566338, Training Accuracy: 13.504\n",
            "Worker 8, [10/32]: Training Loss: 3.504282684, Training Accuracy: 14.800\n",
            "Worker 8, [11/32]: Training Loss: 3.406894231, Training Accuracy: 16.240\n",
            "Worker 8, [12/32]: Training Loss: 3.358296219, Training Accuracy: 17.440\n",
            "Worker 8, [13/32]: Training Loss: 3.287185362, Training Accuracy: 18.416\n",
            "Worker 8, [14/32]: Training Loss: 3.222888241, Training Accuracy: 20.448\n",
            "Worker 8, [15/32]: Training Loss: 3.154624701, Training Accuracy: 20.704\n",
            "Worker 8, [16/32]: Training Loss: 3.078715191, Training Accuracy: 22.592\n",
            "Worker 8, [17/32]: Training Loss: 3.047857946, Training Accuracy: 23.568\n",
            "Worker 8, [18/32]: Training Loss: 2.975531140, Training Accuracy: 23.792\n",
            "Worker 8, [19/32]: Training Loss: 2.898562317, Training Accuracy: 26.128\n",
            "Worker 8, [20/32]: Training Loss: 2.871817961, Training Accuracy: 25.168\n",
            "Worker 8, [21/32]: Training Loss: 2.812499903, Training Accuracy: 27.760\n",
            "Worker 8, [22/32]: Training Loss: 2.751468707, Training Accuracy: 28.672\n",
            "Worker 8, [23/32]: Training Loss: 2.705643758, Training Accuracy: 29.280\n",
            "Worker 8, [24/32]: Training Loss: 2.625047436, Training Accuracy: 31.392\n",
            "Worker 8, [25/32]: Training Loss: 2.586712793, Training Accuracy: 31.552\n",
            "Worker 8, [26/32]: Training Loss: 2.543970758, Training Accuracy: 32.960\n",
            "Worker 8, [27/32]: Training Loss: 2.433800952, Training Accuracy: 34.448\n",
            "Worker 8, [28/32]: Training Loss: 2.406063038, Training Accuracy: 35.008\n",
            "Worker 8, [29/32]: Training Loss: 2.359758838, Training Accuracy: 36.176\n",
            "Worker 8, [30/32]: Training Loss: 2.330547308, Training Accuracy: 37.872\n",
            "Worker 8, [31/32]: Training Loss: 2.249295781, Training Accuracy: 38.160\n",
            "Worker 8, [32/32]: Training Loss: 2.199101811, Training Accuracy: 40.256\n",
            "Time taken for training worker 8: 0:03:18.549906\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.020694\n",
            "Global Update 01: Test Loss: 3.648740914, Test Accuracy: 21.740\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 3.224947854, Training Accuracy: 21.872\n",
            "Worker 1, [02/32]: Training Loss: 2.940202200, Training Accuracy: 25.632\n",
            "Worker 1, [03/32]: Training Loss: 2.860902416, Training Accuracy: 27.184\n",
            "Worker 1, [04/32]: Training Loss: 2.718924561, Training Accuracy: 30.176\n",
            "Worker 1, [05/32]: Training Loss: 2.660600593, Training Accuracy: 30.464\n",
            "Worker 1, [06/32]: Training Loss: 2.532209236, Training Accuracy: 33.152\n",
            "Worker 1, [07/32]: Training Loss: 2.472918243, Training Accuracy: 34.352\n",
            "Worker 1, [08/32]: Training Loss: 2.375807687, Training Accuracy: 36.624\n",
            "Worker 1, [09/32]: Training Loss: 2.314798884, Training Accuracy: 37.776\n",
            "Worker 1, [10/32]: Training Loss: 2.282654480, Training Accuracy: 38.512\n",
            "Worker 1, [11/32]: Training Loss: 2.172998265, Training Accuracy: 40.848\n",
            "Worker 1, [12/32]: Training Loss: 2.117135256, Training Accuracy: 42.240\n",
            "Worker 1, [13/32]: Training Loss: 2.085291825, Training Accuracy: 42.512\n",
            "Worker 1, [14/32]: Training Loss: 2.006932374, Training Accuracy: 44.400\n",
            "Worker 1, [15/32]: Training Loss: 1.970786979, Training Accuracy: 45.536\n",
            "Worker 1, [16/32]: Training Loss: 1.889028608, Training Accuracy: 47.040\n",
            "Worker 1, [17/32]: Training Loss: 1.859753702, Training Accuracy: 47.808\n",
            "Worker 1, [18/32]: Training Loss: 1.766978899, Training Accuracy: 50.176\n",
            "Worker 1, [19/32]: Training Loss: 1.756633496, Training Accuracy: 49.872\n",
            "Worker 1, [20/32]: Training Loss: 1.693403250, Training Accuracy: 51.920\n",
            "Worker 1, [21/32]: Training Loss: 1.638847666, Training Accuracy: 53.008\n",
            "Worker 1, [22/32]: Training Loss: 1.598636383, Training Accuracy: 54.224\n",
            "Worker 1, [23/32]: Training Loss: 1.525231625, Training Accuracy: 56.080\n",
            "Worker 1, [24/32]: Training Loss: 1.512932365, Training Accuracy: 56.944\n",
            "Worker 1, [25/32]: Training Loss: 1.494237549, Training Accuracy: 56.464\n",
            "Worker 1, [26/32]: Training Loss: 1.417949214, Training Accuracy: 58.768\n",
            "Worker 1, [27/32]: Training Loss: 1.374375877, Training Accuracy: 59.664\n",
            "Worker 1, [28/32]: Training Loss: 1.344966711, Training Accuracy: 60.528\n",
            "Worker 1, [29/32]: Training Loss: 1.263870130, Training Accuracy: 62.976\n",
            "Worker 1, [30/32]: Training Loss: 1.288760599, Training Accuracy: 61.632\n",
            "Worker 1, [31/32]: Training Loss: 1.244215140, Training Accuracy: 62.928\n",
            "Worker 1, [32/32]: Training Loss: 1.206965040, Training Accuracy: 64.336\n",
            "Time taken for training worker 1: 0:03:18.124004\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 3.205577009, Training Accuracy: 21.632\n",
            "Worker 2, [02/32]: Training Loss: 2.961473733, Training Accuracy: 25.680\n",
            "Worker 2, [03/32]: Training Loss: 2.821753772, Training Accuracy: 28.080\n",
            "Worker 2, [04/32]: Training Loss: 2.752061970, Training Accuracy: 29.488\n",
            "Worker 2, [05/32]: Training Loss: 2.645417381, Training Accuracy: 31.712\n",
            "Worker 2, [06/32]: Training Loss: 2.567838717, Training Accuracy: 32.480\n",
            "Worker 2, [07/32]: Training Loss: 2.515032343, Training Accuracy: 33.360\n",
            "Worker 2, [08/32]: Training Loss: 2.410582601, Training Accuracy: 34.912\n",
            "Worker 2, [09/32]: Training Loss: 2.339121434, Training Accuracy: 36.736\n",
            "Worker 2, [10/32]: Training Loss: 2.259134817, Training Accuracy: 38.560\n",
            "Worker 2, [11/32]: Training Loss: 2.207154050, Training Accuracy: 40.144\n",
            "Worker 2, [12/32]: Training Loss: 2.145536924, Training Accuracy: 41.472\n",
            "Worker 2, [13/32]: Training Loss: 2.072462180, Training Accuracy: 43.760\n",
            "Worker 2, [14/32]: Training Loss: 2.028308326, Training Accuracy: 43.536\n",
            "Worker 2, [15/32]: Training Loss: 1.964100756, Training Accuracy: 45.472\n",
            "Worker 2, [16/32]: Training Loss: 1.918404161, Training Accuracy: 46.368\n",
            "Worker 2, [17/32]: Training Loss: 1.835832620, Training Accuracy: 47.808\n",
            "Worker 2, [18/32]: Training Loss: 1.801742262, Training Accuracy: 48.864\n",
            "Worker 2, [19/32]: Training Loss: 1.726223148, Training Accuracy: 51.232\n",
            "Worker 2, [20/32]: Training Loss: 1.698490453, Training Accuracy: 51.760\n",
            "Worker 2, [21/32]: Training Loss: 1.666536216, Training Accuracy: 52.640\n",
            "Worker 2, [22/32]: Training Loss: 1.562314940, Training Accuracy: 55.472\n",
            "Worker 2, [23/32]: Training Loss: 1.565291625, Training Accuracy: 54.960\n",
            "Worker 2, [24/32]: Training Loss: 1.537380234, Training Accuracy: 55.040\n",
            "Worker 2, [25/32]: Training Loss: 1.487208992, Training Accuracy: 57.152\n",
            "Worker 2, [26/32]: Training Loss: 1.433717237, Training Accuracy: 58.080\n",
            "Worker 2, [27/32]: Training Loss: 1.382426999, Training Accuracy: 59.008\n",
            "Worker 2, [28/32]: Training Loss: 1.366709527, Training Accuracy: 59.952\n",
            "Worker 2, [29/32]: Training Loss: 1.320405857, Training Accuracy: 61.168\n",
            "Worker 2, [30/32]: Training Loss: 1.267643905, Training Accuracy: 62.000\n",
            "Worker 2, [31/32]: Training Loss: 1.214415271, Training Accuracy: 63.568\n",
            "Worker 2, [32/32]: Training Loss: 1.230365591, Training Accuracy: 63.392\n",
            "Time taken for training worker 2: 0:03:20.823867\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 3.238593929, Training Accuracy: 21.008\n",
            "Worker 3, [02/32]: Training Loss: 2.981338895, Training Accuracy: 26.080\n",
            "Worker 3, [03/32]: Training Loss: 2.841604622, Training Accuracy: 27.024\n",
            "Worker 3, [04/32]: Training Loss: 2.759109091, Training Accuracy: 28.400\n",
            "Worker 3, [05/32]: Training Loss: 2.628700782, Training Accuracy: 31.664\n",
            "Worker 3, [06/32]: Training Loss: 2.569262755, Training Accuracy: 32.368\n",
            "Worker 3, [07/32]: Training Loss: 2.500821931, Training Accuracy: 34.592\n",
            "Worker 3, [08/32]: Training Loss: 2.410035796, Training Accuracy: 36.048\n",
            "Worker 3, [09/32]: Training Loss: 2.349824425, Training Accuracy: 36.640\n",
            "Worker 3, [10/32]: Training Loss: 2.285732045, Training Accuracy: 38.288\n",
            "Worker 3, [11/32]: Training Loss: 2.205396657, Training Accuracy: 39.376\n",
            "Worker 3, [12/32]: Training Loss: 2.121987690, Training Accuracy: 41.856\n",
            "Worker 3, [13/32]: Training Loss: 2.068768183, Training Accuracy: 43.248\n",
            "Worker 3, [14/32]: Training Loss: 2.010398089, Training Accuracy: 43.296\n",
            "Worker 3, [15/32]: Training Loss: 1.961092136, Training Accuracy: 46.816\n",
            "Worker 3, [16/32]: Training Loss: 1.940761093, Training Accuracy: 46.048\n",
            "Worker 3, [17/32]: Training Loss: 1.823097453, Training Accuracy: 48.784\n",
            "Worker 3, [18/32]: Training Loss: 1.814763175, Training Accuracy: 48.656\n",
            "Worker 3, [19/32]: Training Loss: 1.779609895, Training Accuracy: 49.536\n",
            "Worker 3, [20/32]: Training Loss: 1.669155420, Training Accuracy: 51.600\n",
            "Worker 3, [21/32]: Training Loss: 1.658152135, Training Accuracy: 52.976\n",
            "Worker 3, [22/32]: Training Loss: 1.607589101, Training Accuracy: 54.224\n",
            "Worker 3, [23/32]: Training Loss: 1.568815545, Training Accuracy: 54.976\n",
            "Worker 3, [24/32]: Training Loss: 1.499799338, Training Accuracy: 57.504\n",
            "Worker 3, [25/32]: Training Loss: 1.464333296, Training Accuracy: 58.096\n",
            "Worker 3, [26/32]: Training Loss: 1.441499260, Training Accuracy: 57.856\n",
            "Worker 3, [27/32]: Training Loss: 1.378957160, Training Accuracy: 59.856\n",
            "Worker 3, [28/32]: Training Loss: 1.334276133, Training Accuracy: 60.816\n",
            "Worker 3, [29/32]: Training Loss: 1.382932645, Training Accuracy: 60.304\n",
            "Worker 3, [30/32]: Training Loss: 1.302874318, Training Accuracy: 62.048\n",
            "Worker 3, [31/32]: Training Loss: 1.273155278, Training Accuracy: 62.704\n",
            "Worker 3, [32/32]: Training Loss: 1.213134744, Training Accuracy: 63.872\n",
            "Time taken for training worker 3: 0:03:20.451169\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 3.225453301, Training Accuracy: 21.680\n",
            "Worker 4, [02/32]: Training Loss: 2.976821230, Training Accuracy: 24.768\n",
            "Worker 4, [03/32]: Training Loss: 2.839452678, Training Accuracy: 27.648\n",
            "Worker 4, [04/32]: Training Loss: 2.748332637, Training Accuracy: 28.800\n",
            "Worker 4, [05/32]: Training Loss: 2.630351895, Training Accuracy: 31.472\n",
            "Worker 4, [06/32]: Training Loss: 2.566934537, Training Accuracy: 32.688\n",
            "Worker 4, [07/32]: Training Loss: 2.493682638, Training Accuracy: 33.936\n",
            "Worker 4, [08/32]: Training Loss: 2.408159915, Training Accuracy: 36.192\n",
            "Worker 4, [09/32]: Training Loss: 2.344783751, Training Accuracy: 38.064\n",
            "Worker 4, [10/32]: Training Loss: 2.261927961, Training Accuracy: 38.512\n",
            "Worker 4, [11/32]: Training Loss: 2.203287129, Training Accuracy: 40.032\n",
            "Worker 4, [12/32]: Training Loss: 2.137901672, Training Accuracy: 40.976\n",
            "Worker 4, [13/32]: Training Loss: 2.093557198, Training Accuracy: 42.768\n",
            "Worker 4, [14/32]: Training Loss: 2.005600628, Training Accuracy: 44.080\n",
            "Worker 4, [15/32]: Training Loss: 2.003362973, Training Accuracy: 44.080\n",
            "Worker 4, [16/32]: Training Loss: 1.916646613, Training Accuracy: 45.968\n",
            "Worker 4, [17/32]: Training Loss: 1.879120214, Training Accuracy: 46.944\n",
            "Worker 4, [18/32]: Training Loss: 1.783652084, Training Accuracy: 50.144\n",
            "Worker 4, [19/32]: Training Loss: 1.766221900, Training Accuracy: 49.376\n",
            "Worker 4, [20/32]: Training Loss: 1.751718434, Training Accuracy: 50.928\n",
            "Worker 4, [21/32]: Training Loss: 1.658783202, Training Accuracy: 52.768\n",
            "Worker 4, [22/32]: Training Loss: 1.637163662, Training Accuracy: 53.248\n",
            "Worker 4, [23/32]: Training Loss: 1.591604789, Training Accuracy: 55.056\n",
            "Worker 4, [24/32]: Training Loss: 1.549081569, Training Accuracy: 55.232\n",
            "Worker 4, [25/32]: Training Loss: 1.521915727, Training Accuracy: 56.144\n",
            "Worker 4, [26/32]: Training Loss: 1.440847228, Training Accuracy: 58.512\n",
            "Worker 4, [27/32]: Training Loss: 1.415655830, Training Accuracy: 59.280\n",
            "Worker 4, [28/32]: Training Loss: 1.377701037, Training Accuracy: 59.984\n",
            "Worker 4, [29/32]: Training Loss: 1.353339469, Training Accuracy: 60.624\n",
            "Worker 4, [30/32]: Training Loss: 1.281757241, Training Accuracy: 62.112\n",
            "Worker 4, [31/32]: Training Loss: 1.268451820, Training Accuracy: 62.880\n",
            "Worker 4, [32/32]: Training Loss: 1.245384092, Training Accuracy: 63.216\n",
            "Time taken for training worker 4: 0:03:19.331891\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/32]: Training Loss: 3.256560652, Training Accuracy: 20.800\n",
            "Worker 5, [02/32]: Training Loss: 2.991141862, Training Accuracy: 24.992\n",
            "Worker 5, [03/32]: Training Loss: 2.848717580, Training Accuracy: 27.760\n",
            "Worker 5, [04/32]: Training Loss: 2.754724979, Training Accuracy: 29.040\n",
            "Worker 5, [05/32]: Training Loss: 2.650730836, Training Accuracy: 30.016\n",
            "Worker 5, [06/32]: Training Loss: 2.590738958, Training Accuracy: 32.080\n",
            "Worker 5, [07/32]: Training Loss: 2.489163443, Training Accuracy: 34.304\n",
            "Worker 5, [08/32]: Training Loss: 2.427445264, Training Accuracy: 35.456\n",
            "Worker 5, [09/32]: Training Loss: 2.318524848, Training Accuracy: 37.776\n",
            "Worker 5, [10/32]: Training Loss: 2.271663858, Training Accuracy: 38.336\n",
            "Worker 5, [11/32]: Training Loss: 2.240429877, Training Accuracy: 39.424\n",
            "Worker 5, [12/32]: Training Loss: 2.118001787, Training Accuracy: 42.256\n",
            "Worker 5, [13/32]: Training Loss: 2.089684304, Training Accuracy: 42.704\n",
            "Worker 5, [14/32]: Training Loss: 1.987208422, Training Accuracy: 45.520\n",
            "Worker 5, [15/32]: Training Loss: 1.936806229, Training Accuracy: 46.144\n",
            "Worker 5, [16/32]: Training Loss: 1.893751053, Training Accuracy: 47.232\n",
            "Worker 5, [17/32]: Training Loss: 1.845773983, Training Accuracy: 48.112\n",
            "Worker 5, [18/32]: Training Loss: 1.785694367, Training Accuracy: 49.024\n",
            "Worker 5, [19/32]: Training Loss: 1.718288170, Training Accuracy: 51.952\n",
            "Worker 5, [20/32]: Training Loss: 1.693560415, Training Accuracy: 51.776\n",
            "Worker 5, [21/32]: Training Loss: 1.647987759, Training Accuracy: 53.408\n",
            "Worker 5, [22/32]: Training Loss: 1.622481582, Training Accuracy: 53.376\n",
            "Worker 5, [23/32]: Training Loss: 1.556624375, Training Accuracy: 55.632\n",
            "Worker 5, [24/32]: Training Loss: 1.503926617, Training Accuracy: 55.808\n",
            "Worker 5, [25/32]: Training Loss: 1.457512055, Training Accuracy: 58.480\n",
            "Worker 5, [26/32]: Training Loss: 1.447739739, Training Accuracy: 59.024\n",
            "Worker 5, [27/32]: Training Loss: 1.418601389, Training Accuracy: 59.152\n",
            "Worker 5, [28/32]: Training Loss: 1.342378844, Training Accuracy: 60.336\n",
            "Worker 5, [29/32]: Training Loss: 1.314832267, Training Accuracy: 60.896\n",
            "Worker 5, [30/32]: Training Loss: 1.284345077, Training Accuracy: 62.512\n",
            "Worker 5, [31/32]: Training Loss: 1.235477215, Training Accuracy: 63.936\n",
            "Worker 5, [32/32]: Training Loss: 1.219994673, Training Accuracy: 64.160\n",
            "Time taken for training worker 5: 0:03:18.313110\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/32]: Training Loss: 3.249481294, Training Accuracy: 20.768\n",
            "Worker 6, [02/32]: Training Loss: 2.977648292, Training Accuracy: 24.448\n",
            "Worker 6, [03/32]: Training Loss: 2.849453508, Training Accuracy: 28.064\n",
            "Worker 6, [04/32]: Training Loss: 2.780348605, Training Accuracy: 28.528\n",
            "Worker 6, [05/32]: Training Loss: 2.668559729, Training Accuracy: 30.640\n",
            "Worker 6, [06/32]: Training Loss: 2.581394365, Training Accuracy: 32.768\n",
            "Worker 6, [07/32]: Training Loss: 2.494654028, Training Accuracy: 34.400\n",
            "Worker 6, [08/32]: Training Loss: 2.415941888, Training Accuracy: 35.552\n",
            "Worker 6, [09/32]: Training Loss: 2.348601088, Training Accuracy: 37.552\n",
            "Worker 6, [10/32]: Training Loss: 2.278340014, Training Accuracy: 38.544\n",
            "Worker 6, [11/32]: Training Loss: 2.217589068, Training Accuracy: 39.728\n",
            "Worker 6, [12/32]: Training Loss: 2.144276930, Training Accuracy: 41.152\n",
            "Worker 6, [13/32]: Training Loss: 2.089723206, Training Accuracy: 42.672\n",
            "Worker 6, [14/32]: Training Loss: 2.010276911, Training Accuracy: 44.480\n",
            "Worker 6, [15/32]: Training Loss: 2.008994461, Training Accuracy: 45.360\n",
            "Worker 6, [16/32]: Training Loss: 1.908650976, Training Accuracy: 46.432\n",
            "Worker 6, [17/32]: Training Loss: 1.854868260, Training Accuracy: 47.456\n",
            "Worker 6, [18/32]: Training Loss: 1.785443387, Training Accuracy: 49.568\n",
            "Worker 6, [19/32]: Training Loss: 1.733264548, Training Accuracy: 50.464\n",
            "Worker 6, [20/32]: Training Loss: 1.735779390, Training Accuracy: 50.784\n",
            "Worker 6, [21/32]: Training Loss: 1.641222432, Training Accuracy: 53.440\n",
            "Worker 6, [22/32]: Training Loss: 1.610468654, Training Accuracy: 54.096\n",
            "Worker 6, [23/32]: Training Loss: 1.575478633, Training Accuracy: 54.496\n",
            "Worker 6, [24/32]: Training Loss: 1.552485808, Training Accuracy: 55.216\n",
            "Worker 6, [25/32]: Training Loss: 1.483311201, Training Accuracy: 56.640\n",
            "Worker 6, [26/32]: Training Loss: 1.406186463, Training Accuracy: 58.992\n",
            "Worker 6, [27/32]: Training Loss: 1.411781793, Training Accuracy: 59.520\n",
            "Worker 6, [28/32]: Training Loss: 1.349672216, Training Accuracy: 59.712\n",
            "Worker 6, [29/32]: Training Loss: 1.300279564, Training Accuracy: 62.160\n",
            "Worker 6, [30/32]: Training Loss: 1.256794717, Training Accuracy: 62.672\n",
            "Worker 6, [31/32]: Training Loss: 1.277055279, Training Accuracy: 62.368\n",
            "Worker 6, [32/32]: Training Loss: 1.201911583, Training Accuracy: 64.656\n",
            "Time taken for training worker 6: 0:03:20.980046\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/32]: Training Loss: 3.247676122, Training Accuracy: 20.880\n",
            "Worker 7, [02/32]: Training Loss: 2.999603454, Training Accuracy: 24.736\n",
            "Worker 7, [03/32]: Training Loss: 2.848496169, Training Accuracy: 27.152\n",
            "Worker 7, [04/32]: Training Loss: 2.732845187, Training Accuracy: 29.232\n",
            "Worker 7, [05/32]: Training Loss: 2.653935121, Training Accuracy: 31.408\n",
            "Worker 7, [06/32]: Training Loss: 2.588757104, Training Accuracy: 32.208\n",
            "Worker 7, [07/32]: Training Loss: 2.483057601, Training Accuracy: 34.032\n",
            "Worker 7, [08/32]: Training Loss: 2.427185859, Training Accuracy: 34.912\n",
            "Worker 7, [09/32]: Training Loss: 2.340393894, Training Accuracy: 37.056\n",
            "Worker 7, [10/32]: Training Loss: 2.314278127, Training Accuracy: 37.136\n",
            "Worker 7, [11/32]: Training Loss: 2.190789427, Training Accuracy: 40.080\n",
            "Worker 7, [12/32]: Training Loss: 2.168111294, Training Accuracy: 41.168\n",
            "Worker 7, [13/32]: Training Loss: 2.087892088, Training Accuracy: 42.688\n",
            "Worker 7, [14/32]: Training Loss: 2.019144607, Training Accuracy: 43.424\n",
            "Worker 7, [15/32]: Training Loss: 1.948152721, Training Accuracy: 46.256\n",
            "Worker 7, [16/32]: Training Loss: 1.909468108, Training Accuracy: 46.752\n",
            "Worker 7, [17/32]: Training Loss: 1.842106989, Training Accuracy: 47.872\n",
            "Worker 7, [18/32]: Training Loss: 1.781883703, Training Accuracy: 49.072\n",
            "Worker 7, [19/32]: Training Loss: 1.754482261, Training Accuracy: 49.696\n",
            "Worker 7, [20/32]: Training Loss: 1.726356727, Training Accuracy: 50.976\n",
            "Worker 7, [21/32]: Training Loss: 1.688404774, Training Accuracy: 51.872\n",
            "Worker 7, [22/32]: Training Loss: 1.613242404, Training Accuracy: 53.312\n",
            "Worker 7, [23/32]: Training Loss: 1.547222917, Training Accuracy: 54.832\n",
            "Worker 7, [24/32]: Training Loss: 1.528590249, Training Accuracy: 55.808\n",
            "Worker 7, [25/32]: Training Loss: 1.423848018, Training Accuracy: 58.288\n",
            "Worker 7, [26/32]: Training Loss: 1.448278960, Training Accuracy: 57.536\n",
            "Worker 7, [27/32]: Training Loss: 1.385408594, Training Accuracy: 59.792\n",
            "Worker 7, [28/32]: Training Loss: 1.340153832, Training Accuracy: 60.560\n",
            "Worker 7, [29/32]: Training Loss: 1.335199843, Training Accuracy: 60.848\n",
            "Worker 7, [30/32]: Training Loss: 1.244217862, Training Accuracy: 63.120\n",
            "Worker 7, [31/32]: Training Loss: 1.271924508, Training Accuracy: 62.640\n",
            "Worker 7, [32/32]: Training Loss: 1.236341883, Training Accuracy: 63.344\n",
            "Time taken for training worker 7: 0:03:11.500178\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/32]: Training Loss: 3.241454947, Training Accuracy: 20.896\n",
            "Worker 8, [02/32]: Training Loss: 2.965648043, Training Accuracy: 24.960\n",
            "Worker 8, [03/32]: Training Loss: 2.857545206, Training Accuracy: 26.448\n",
            "Worker 8, [04/32]: Training Loss: 2.741635344, Training Accuracy: 28.784\n",
            "Worker 8, [05/32]: Training Loss: 2.657613606, Training Accuracy: 31.024\n",
            "Worker 8, [06/32]: Training Loss: 2.596284796, Training Accuracy: 31.840\n",
            "Worker 8, [07/32]: Training Loss: 2.519994811, Training Accuracy: 33.376\n",
            "Worker 8, [08/32]: Training Loss: 2.431102942, Training Accuracy: 35.072\n",
            "Worker 8, [09/32]: Training Loss: 2.360801769, Training Accuracy: 36.304\n",
            "Worker 8, [10/32]: Training Loss: 2.329842584, Training Accuracy: 37.648\n",
            "Worker 8, [11/32]: Training Loss: 2.219694810, Training Accuracy: 39.616\n",
            "Worker 8, [12/32]: Training Loss: 2.190139646, Training Accuracy: 40.064\n",
            "Worker 8, [13/32]: Training Loss: 2.105965960, Training Accuracy: 42.400\n",
            "Worker 8, [14/32]: Training Loss: 2.017360517, Training Accuracy: 44.688\n",
            "Worker 8, [15/32]: Training Loss: 1.972424783, Training Accuracy: 44.208\n",
            "Worker 8, [16/32]: Training Loss: 1.904518087, Training Accuracy: 46.400\n",
            "Worker 8, [17/32]: Training Loss: 1.840705370, Training Accuracy: 48.368\n",
            "Worker 8, [18/32]: Training Loss: 1.791621318, Training Accuracy: 50.048\n",
            "Worker 8, [19/32]: Training Loss: 1.752948789, Training Accuracy: 50.336\n",
            "Worker 8, [20/32]: Training Loss: 1.743808896, Training Accuracy: 50.432\n",
            "Worker 8, [21/32]: Training Loss: 1.650707194, Training Accuracy: 52.656\n",
            "Worker 8, [22/32]: Training Loss: 1.616984480, Training Accuracy: 53.824\n",
            "Worker 8, [23/32]: Training Loss: 1.571287468, Training Accuracy: 55.648\n",
            "Worker 8, [24/32]: Training Loss: 1.568567297, Training Accuracy: 55.312\n",
            "Worker 8, [25/32]: Training Loss: 1.479934044, Training Accuracy: 57.760\n",
            "Worker 8, [26/32]: Training Loss: 1.403684483, Training Accuracy: 59.392\n",
            "Worker 8, [27/32]: Training Loss: 1.389752086, Training Accuracy: 59.392\n",
            "Worker 8, [28/32]: Training Loss: 1.377803521, Training Accuracy: 59.488\n",
            "Worker 8, [29/32]: Training Loss: 1.357254727, Training Accuracy: 60.576\n",
            "Worker 8, [30/32]: Training Loss: 1.284319196, Training Accuracy: 63.072\n",
            "Worker 8, [31/32]: Training Loss: 1.241328093, Training Accuracy: 63.296\n",
            "Worker 8, [32/32]: Training Loss: 1.165560322, Training Accuracy: 65.472\n",
            "Time taken for training worker 8: 0:03:15.121840\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.015525\n",
            "Global Update 02: Test Loss: 3.555943331, Test Accuracy: 34.600\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.671288150, Training Accuracy: 34.144\n",
            "Worker 1, [02/32]: Training Loss: 2.288463537, Training Accuracy: 40.224\n",
            "Worker 1, [03/32]: Training Loss: 2.096477176, Training Accuracy: 43.600\n",
            "Worker 1, [04/32]: Training Loss: 1.972330085, Training Accuracy: 46.848\n",
            "Worker 1, [05/32]: Training Loss: 1.830462039, Training Accuracy: 49.280\n",
            "Worker 1, [06/32]: Training Loss: 1.714638580, Training Accuracy: 52.288\n",
            "Worker 1, [07/32]: Training Loss: 1.643631500, Training Accuracy: 53.808\n",
            "Worker 1, [08/32]: Training Loss: 1.531918073, Training Accuracy: 56.880\n",
            "Worker 1, [09/32]: Training Loss: 1.451644131, Training Accuracy: 58.144\n",
            "Worker 1, [10/32]: Training Loss: 1.395405468, Training Accuracy: 59.984\n",
            "Worker 1, [11/32]: Training Loss: 1.310136782, Training Accuracy: 61.792\n",
            "Worker 1, [12/32]: Training Loss: 1.236569341, Training Accuracy: 64.400\n",
            "Worker 1, [13/32]: Training Loss: 1.189400897, Training Accuracy: 65.072\n",
            "Worker 1, [14/32]: Training Loss: 1.133811439, Training Accuracy: 66.656\n",
            "Worker 1, [15/32]: Training Loss: 1.097070485, Training Accuracy: 67.840\n",
            "Worker 1, [16/32]: Training Loss: 1.015964999, Training Accuracy: 70.288\n",
            "Worker 1, [17/32]: Training Loss: 0.974765003, Training Accuracy: 70.944\n",
            "Worker 1, [18/32]: Training Loss: 0.978894313, Training Accuracy: 70.720\n",
            "Worker 1, [19/32]: Training Loss: 0.885948310, Training Accuracy: 73.728\n",
            "Worker 1, [20/32]: Training Loss: 0.885229887, Training Accuracy: 73.616\n",
            "Worker 1, [21/32]: Training Loss: 0.816032329, Training Accuracy: 75.168\n",
            "Worker 1, [22/32]: Training Loss: 0.757663748, Training Accuracy: 77.616\n",
            "Worker 1, [23/32]: Training Loss: 0.783974033, Training Accuracy: 76.224\n",
            "Worker 1, [24/32]: Training Loss: 0.780688508, Training Accuracy: 76.592\n",
            "Worker 1, [25/32]: Training Loss: 0.703312484, Training Accuracy: 78.448\n",
            "Worker 1, [26/32]: Training Loss: 0.728507098, Training Accuracy: 77.184\n",
            "Worker 1, [27/32]: Training Loss: 0.673011226, Training Accuracy: 79.120\n",
            "Worker 1, [28/32]: Training Loss: 0.641350859, Training Accuracy: 80.720\n",
            "Worker 1, [29/32]: Training Loss: 0.647781374, Training Accuracy: 79.936\n",
            "Worker 1, [30/32]: Training Loss: 0.602429628, Training Accuracy: 81.456\n",
            "Worker 1, [31/32]: Training Loss: 0.598037317, Training Accuracy: 81.648\n",
            "Worker 1, [32/32]: Training Loss: 0.578143658, Training Accuracy: 82.640\n",
            "Time taken for training worker 1: 0:03:11.247966\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.675833719, Training Accuracy: 33.264\n",
            "Worker 2, [02/32]: Training Loss: 2.293014760, Training Accuracy: 40.016\n",
            "Worker 2, [03/32]: Training Loss: 2.101206674, Training Accuracy: 43.792\n",
            "Worker 2, [04/32]: Training Loss: 1.974924741, Training Accuracy: 46.224\n",
            "Worker 2, [05/32]: Training Loss: 1.826424724, Training Accuracy: 49.616\n",
            "Worker 2, [06/32]: Training Loss: 1.726830062, Training Accuracy: 51.616\n",
            "Worker 2, [07/32]: Training Loss: 1.633083449, Training Accuracy: 54.160\n",
            "Worker 2, [08/32]: Training Loss: 1.536893795, Training Accuracy: 56.208\n",
            "Worker 2, [09/32]: Training Loss: 1.463767928, Training Accuracy: 58.224\n",
            "Worker 2, [10/32]: Training Loss: 1.394973311, Training Accuracy: 59.712\n",
            "Worker 2, [11/32]: Training Loss: 1.292401190, Training Accuracy: 62.704\n",
            "Worker 2, [12/32]: Training Loss: 1.254551688, Training Accuracy: 63.136\n",
            "Worker 2, [13/32]: Training Loss: 1.197236344, Training Accuracy: 64.672\n",
            "Worker 2, [14/32]: Training Loss: 1.130420094, Training Accuracy: 66.336\n",
            "Worker 2, [15/32]: Training Loss: 1.076132724, Training Accuracy: 69.008\n",
            "Worker 2, [16/32]: Training Loss: 1.055413728, Training Accuracy: 68.960\n",
            "Worker 2, [17/32]: Training Loss: 0.992189524, Training Accuracy: 70.096\n",
            "Worker 2, [18/32]: Training Loss: 0.931973433, Training Accuracy: 71.664\n",
            "Worker 2, [19/32]: Training Loss: 0.948373341, Training Accuracy: 71.504\n",
            "Worker 2, [20/32]: Training Loss: 0.874003717, Training Accuracy: 73.840\n",
            "Worker 2, [21/32]: Training Loss: 0.829344853, Training Accuracy: 75.344\n",
            "Worker 2, [22/32]: Training Loss: 0.826924717, Training Accuracy: 75.616\n",
            "Worker 2, [23/32]: Training Loss: 0.814333272, Training Accuracy: 75.168\n",
            "Worker 2, [24/32]: Training Loss: 0.731503680, Training Accuracy: 78.016\n",
            "Worker 2, [25/32]: Training Loss: 0.731544172, Training Accuracy: 77.904\n",
            "Worker 2, [26/32]: Training Loss: 0.682395332, Training Accuracy: 79.280\n",
            "Worker 2, [27/32]: Training Loss: 0.660495646, Training Accuracy: 79.728\n",
            "Worker 2, [28/32]: Training Loss: 0.659435979, Training Accuracy: 80.464\n",
            "Worker 2, [29/32]: Training Loss: 0.620451041, Training Accuracy: 81.184\n",
            "Worker 2, [30/32]: Training Loss: 0.629864598, Training Accuracy: 80.736\n",
            "Worker 2, [31/32]: Training Loss: 0.611090681, Training Accuracy: 81.840\n",
            "Worker 2, [32/32]: Training Loss: 0.599592049, Training Accuracy: 81.056\n",
            "Time taken for training worker 2: 0:03:11.854031\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 2.696956941, Training Accuracy: 33.104\n",
            "Worker 3, [02/32]: Training Loss: 2.284308543, Training Accuracy: 40.432\n",
            "Worker 3, [03/32]: Training Loss: 2.101427488, Training Accuracy: 43.536\n",
            "Worker 3, [04/32]: Training Loss: 1.981734484, Training Accuracy: 46.336\n",
            "Worker 3, [05/32]: Training Loss: 1.832928419, Training Accuracy: 48.592\n",
            "Worker 3, [06/32]: Training Loss: 1.738350588, Training Accuracy: 51.840\n",
            "Worker 3, [07/32]: Training Loss: 1.634755472, Training Accuracy: 54.000\n",
            "Worker 3, [08/32]: Training Loss: 1.540948715, Training Accuracy: 55.984\n",
            "Worker 3, [09/32]: Training Loss: 1.464226359, Training Accuracy: 57.840\n",
            "Worker 3, [10/32]: Training Loss: 1.419539840, Training Accuracy: 58.576\n",
            "Worker 3, [11/32]: Training Loss: 1.296798867, Training Accuracy: 62.096\n",
            "Worker 3, [12/32]: Training Loss: 1.275087495, Training Accuracy: 63.344\n",
            "Worker 3, [13/32]: Training Loss: 1.191965483, Training Accuracy: 65.520\n",
            "Worker 3, [14/32]: Training Loss: 1.164049434, Training Accuracy: 65.728\n",
            "Worker 3, [15/32]: Training Loss: 1.059206367, Training Accuracy: 68.992\n",
            "Worker 3, [16/32]: Training Loss: 1.037800659, Training Accuracy: 69.296\n",
            "Worker 3, [17/32]: Training Loss: 1.010614972, Training Accuracy: 70.272\n",
            "Worker 3, [18/32]: Training Loss: 0.957651046, Training Accuracy: 72.208\n",
            "Worker 3, [19/32]: Training Loss: 0.924706539, Training Accuracy: 72.624\n",
            "Worker 3, [20/32]: Training Loss: 0.888858484, Training Accuracy: 73.616\n",
            "Worker 3, [21/32]: Training Loss: 0.826400669, Training Accuracy: 75.232\n",
            "Worker 3, [22/32]: Training Loss: 0.818867991, Training Accuracy: 75.216\n",
            "Worker 3, [23/32]: Training Loss: 0.771985470, Training Accuracy: 76.560\n",
            "Worker 3, [24/32]: Training Loss: 0.777911696, Training Accuracy: 76.800\n",
            "Worker 3, [25/32]: Training Loss: 0.774827790, Training Accuracy: 76.848\n",
            "Worker 3, [26/32]: Training Loss: 0.729114574, Training Accuracy: 78.528\n",
            "Worker 3, [27/32]: Training Loss: 0.699171518, Training Accuracy: 79.184\n",
            "Worker 3, [28/32]: Training Loss: 0.678478951, Training Accuracy: 79.856\n",
            "Worker 3, [29/32]: Training Loss: 0.679321329, Training Accuracy: 79.264\n",
            "Worker 3, [30/32]: Training Loss: 0.654682096, Training Accuracy: 80.192\n",
            "Worker 3, [31/32]: Training Loss: 0.578671144, Training Accuracy: 82.704\n",
            "Worker 3, [32/32]: Training Loss: 0.598301299, Training Accuracy: 82.128\n",
            "Time taken for training worker 3: 0:03:14.972819\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 2.708511963, Training Accuracy: 32.608\n",
            "Worker 4, [02/32]: Training Loss: 2.304947610, Training Accuracy: 39.568\n",
            "Worker 4, [03/32]: Training Loss: 2.126280418, Training Accuracy: 42.208\n",
            "Worker 4, [04/32]: Training Loss: 1.970829865, Training Accuracy: 45.776\n",
            "Worker 4, [05/32]: Training Loss: 1.853550411, Training Accuracy: 48.832\n",
            "Worker 4, [06/32]: Training Loss: 1.745246034, Training Accuracy: 51.328\n",
            "Worker 4, [07/32]: Training Loss: 1.637343538, Training Accuracy: 53.728\n",
            "Worker 4, [08/32]: Training Loss: 1.545861456, Training Accuracy: 56.064\n",
            "Worker 4, [09/32]: Training Loss: 1.511055564, Training Accuracy: 56.960\n",
            "Worker 4, [10/32]: Training Loss: 1.395280039, Training Accuracy: 59.792\n",
            "Worker 4, [11/32]: Training Loss: 1.346036685, Training Accuracy: 60.832\n",
            "Worker 4, [12/32]: Training Loss: 1.263661132, Training Accuracy: 63.120\n",
            "Worker 4, [13/32]: Training Loss: 1.269227490, Training Accuracy: 63.184\n",
            "Worker 4, [14/32]: Training Loss: 1.135228387, Training Accuracy: 66.928\n",
            "Worker 4, [15/32]: Training Loss: 1.087055830, Training Accuracy: 67.680\n",
            "Worker 4, [16/32]: Training Loss: 1.061367400, Training Accuracy: 68.064\n",
            "Worker 4, [17/32]: Training Loss: 1.032372408, Training Accuracy: 68.784\n",
            "Worker 4, [18/32]: Training Loss: 0.991396611, Training Accuracy: 71.040\n",
            "Worker 4, [19/32]: Training Loss: 0.953496517, Training Accuracy: 71.408\n",
            "Worker 4, [20/32]: Training Loss: 0.915427858, Training Accuracy: 72.016\n",
            "Worker 4, [21/32]: Training Loss: 0.857286130, Training Accuracy: 74.304\n",
            "Worker 4, [22/32]: Training Loss: 0.829566237, Training Accuracy: 74.848\n",
            "Worker 4, [23/32]: Training Loss: 0.845404307, Training Accuracy: 73.968\n",
            "Worker 4, [24/32]: Training Loss: 0.737897919, Training Accuracy: 76.976\n",
            "Worker 4, [25/32]: Training Loss: 0.751515065, Training Accuracy: 77.168\n",
            "Worker 4, [26/32]: Training Loss: 0.723666202, Training Accuracy: 77.664\n",
            "Worker 4, [27/32]: Training Loss: 0.671092922, Training Accuracy: 79.424\n",
            "Worker 4, [28/32]: Training Loss: 0.667456559, Training Accuracy: 80.144\n",
            "Worker 4, [29/32]: Training Loss: 0.677309722, Training Accuracy: 80.032\n",
            "Worker 4, [30/32]: Training Loss: 0.599086896, Training Accuracy: 81.600\n",
            "Worker 4, [31/32]: Training Loss: 0.650361064, Training Accuracy: 81.168\n",
            "Worker 4, [32/32]: Training Loss: 0.652733183, Training Accuracy: 80.384\n",
            "Time taken for training worker 4: 0:03:13.363209\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/32]: Training Loss: 2.725801857, Training Accuracy: 33.568\n",
            "Worker 5, [02/32]: Training Loss: 2.295666545, Training Accuracy: 39.840\n",
            "Worker 5, [03/32]: Training Loss: 2.094161271, Training Accuracy: 43.536\n",
            "Worker 5, [04/32]: Training Loss: 1.969857902, Training Accuracy: 45.872\n",
            "Worker 5, [05/32]: Training Loss: 1.851810661, Training Accuracy: 48.928\n",
            "Worker 5, [06/32]: Training Loss: 1.705061355, Training Accuracy: 52.720\n",
            "Worker 5, [07/32]: Training Loss: 1.636535826, Training Accuracy: 54.304\n",
            "Worker 5, [08/32]: Training Loss: 1.557783117, Training Accuracy: 55.728\n",
            "Worker 5, [09/32]: Training Loss: 1.447753785, Training Accuracy: 58.768\n",
            "Worker 5, [10/32]: Training Loss: 1.393974280, Training Accuracy: 59.712\n",
            "Worker 5, [11/32]: Training Loss: 1.336052024, Training Accuracy: 61.120\n",
            "Worker 5, [12/32]: Training Loss: 1.245996548, Training Accuracy: 63.936\n",
            "Worker 5, [13/32]: Training Loss: 1.196433385, Training Accuracy: 65.504\n",
            "Worker 5, [14/32]: Training Loss: 1.113151301, Training Accuracy: 67.440\n",
            "Worker 5, [15/32]: Training Loss: 1.076022346, Training Accuracy: 68.144\n",
            "Worker 5, [16/32]: Training Loss: 1.050463009, Training Accuracy: 68.992\n",
            "Worker 5, [17/32]: Training Loss: 1.016361247, Training Accuracy: 70.336\n",
            "Worker 5, [18/32]: Training Loss: 0.994232404, Training Accuracy: 70.336\n",
            "Worker 5, [19/32]: Training Loss: 0.918973476, Training Accuracy: 72.592\n",
            "Worker 5, [20/32]: Training Loss: 0.923721474, Training Accuracy: 72.480\n",
            "Worker 5, [21/32]: Training Loss: 0.851969885, Training Accuracy: 74.688\n",
            "Worker 5, [22/32]: Training Loss: 0.864566925, Training Accuracy: 74.032\n",
            "Worker 5, [23/32]: Training Loss: 0.782141177, Training Accuracy: 77.072\n",
            "Worker 5, [24/32]: Training Loss: 0.757053345, Training Accuracy: 77.200\n",
            "Worker 5, [25/32]: Training Loss: 0.723974605, Training Accuracy: 78.016\n",
            "Worker 5, [26/32]: Training Loss: 0.697576795, Training Accuracy: 78.592\n",
            "Worker 5, [27/32]: Training Loss: 0.673415258, Training Accuracy: 79.968\n",
            "Worker 5, [28/32]: Training Loss: 0.636292716, Training Accuracy: 80.896\n",
            "Worker 5, [29/32]: Training Loss: 0.640010659, Training Accuracy: 81.136\n",
            "Worker 5, [30/32]: Training Loss: 0.591166533, Training Accuracy: 82.272\n",
            "Worker 5, [31/32]: Training Loss: 0.623395388, Training Accuracy: 82.000\n",
            "Worker 5, [32/32]: Training Loss: 0.597670109, Training Accuracy: 81.600\n",
            "Time taken for training worker 5: 0:03:11.552377\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/32]: Training Loss: 2.711756548, Training Accuracy: 32.992\n",
            "Worker 6, [02/32]: Training Loss: 2.299513449, Training Accuracy: 40.368\n",
            "Worker 6, [03/32]: Training Loss: 2.097581828, Training Accuracy: 43.632\n",
            "Worker 6, [04/32]: Training Loss: 1.988524725, Training Accuracy: 46.144\n",
            "Worker 6, [05/32]: Training Loss: 1.870197733, Training Accuracy: 48.800\n",
            "Worker 6, [06/32]: Training Loss: 1.734775411, Training Accuracy: 51.360\n",
            "Worker 6, [07/32]: Training Loss: 1.636923122, Training Accuracy: 53.744\n",
            "Worker 6, [08/32]: Training Loss: 1.555544724, Training Accuracy: 55.856\n",
            "Worker 6, [09/32]: Training Loss: 1.497471002, Training Accuracy: 57.344\n",
            "Worker 6, [10/32]: Training Loss: 1.409628317, Training Accuracy: 59.696\n",
            "Worker 6, [11/32]: Training Loss: 1.341187643, Training Accuracy: 60.592\n",
            "Worker 6, [12/32]: Training Loss: 1.270770967, Training Accuracy: 63.168\n",
            "Worker 6, [13/32]: Training Loss: 1.209245643, Training Accuracy: 64.640\n",
            "Worker 6, [14/32]: Training Loss: 1.144334019, Training Accuracy: 66.480\n",
            "Worker 6, [15/32]: Training Loss: 1.130137421, Training Accuracy: 66.864\n",
            "Worker 6, [16/32]: Training Loss: 1.055387054, Training Accuracy: 68.544\n",
            "Worker 6, [17/32]: Training Loss: 1.018433683, Training Accuracy: 69.712\n",
            "Worker 6, [18/32]: Training Loss: 0.961944113, Training Accuracy: 71.520\n",
            "Worker 6, [19/32]: Training Loss: 0.932455024, Training Accuracy: 72.016\n",
            "Worker 6, [20/32]: Training Loss: 0.898635784, Training Accuracy: 72.960\n",
            "Worker 6, [21/32]: Training Loss: 0.843252759, Training Accuracy: 74.400\n",
            "Worker 6, [22/32]: Training Loss: 0.795320878, Training Accuracy: 75.552\n",
            "Worker 6, [23/32]: Training Loss: 0.802207606, Training Accuracy: 75.568\n",
            "Worker 6, [24/32]: Training Loss: 0.774550159, Training Accuracy: 76.368\n",
            "Worker 6, [25/32]: Training Loss: 0.714801015, Training Accuracy: 78.592\n",
            "Worker 6, [26/32]: Training Loss: 0.747639095, Training Accuracy: 77.248\n",
            "Worker 6, [27/32]: Training Loss: 0.668963286, Training Accuracy: 79.600\n",
            "Worker 6, [28/32]: Training Loss: 0.662620718, Training Accuracy: 79.824\n",
            "Worker 6, [29/32]: Training Loss: 0.658489320, Training Accuracy: 80.224\n",
            "Worker 6, [30/32]: Training Loss: 0.613890710, Training Accuracy: 81.056\n",
            "Worker 6, [31/32]: Training Loss: 0.609763586, Training Accuracy: 81.424\n",
            "Worker 6, [32/32]: Training Loss: 0.599816052, Training Accuracy: 82.208\n",
            "Time taken for training worker 6: 0:02:22.381667\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/32]: Training Loss: 2.698236246, Training Accuracy: 33.504\n",
            "Worker 7, [02/32]: Training Loss: 2.298603370, Training Accuracy: 39.520\n",
            "Worker 7, [03/32]: Training Loss: 2.109992446, Training Accuracy: 42.944\n",
            "Worker 7, [04/32]: Training Loss: 1.973357811, Training Accuracy: 45.568\n",
            "Worker 7, [05/32]: Training Loss: 1.864158145, Training Accuracy: 48.112\n",
            "Worker 7, [06/32]: Training Loss: 1.746980707, Training Accuracy: 50.704\n",
            "Worker 7, [07/32]: Training Loss: 1.640516329, Training Accuracy: 53.648\n",
            "Worker 7, [08/32]: Training Loss: 1.564675706, Training Accuracy: 55.920\n",
            "Worker 7, [09/32]: Training Loss: 1.472407568, Training Accuracy: 57.632\n",
            "Worker 7, [10/32]: Training Loss: 1.421246739, Training Accuracy: 59.456\n",
            "Worker 7, [11/32]: Training Loss: 1.344069066, Training Accuracy: 61.856\n",
            "Worker 7, [12/32]: Training Loss: 1.282871477, Training Accuracy: 62.960\n",
            "Worker 7, [13/32]: Training Loss: 1.202843780, Training Accuracy: 64.608\n",
            "Worker 7, [14/32]: Training Loss: 1.159614782, Training Accuracy: 65.520\n",
            "Worker 7, [15/32]: Training Loss: 1.100423579, Training Accuracy: 67.008\n",
            "Worker 7, [16/32]: Training Loss: 1.102114111, Training Accuracy: 67.248\n",
            "Worker 7, [17/32]: Training Loss: 0.999696398, Training Accuracy: 70.656\n",
            "Worker 7, [18/32]: Training Loss: 0.939312419, Training Accuracy: 72.496\n",
            "Worker 7, [19/32]: Training Loss: 0.905815555, Training Accuracy: 73.248\n",
            "Worker 7, [20/32]: Training Loss: 0.911566959, Training Accuracy: 72.816\n",
            "Worker 7, [21/32]: Training Loss: 0.890196442, Training Accuracy: 72.880\n",
            "Worker 7, [22/32]: Training Loss: 0.814296697, Training Accuracy: 75.504\n",
            "Worker 7, [23/32]: Training Loss: 0.837425874, Training Accuracy: 74.912\n",
            "Worker 7, [24/32]: Training Loss: 0.757978667, Training Accuracy: 77.696\n",
            "Worker 7, [25/32]: Training Loss: 0.748270554, Training Accuracy: 77.472\n",
            "Worker 7, [26/32]: Training Loss: 0.716499500, Training Accuracy: 78.320\n",
            "Worker 7, [27/32]: Training Loss: 0.707466552, Training Accuracy: 79.264\n",
            "Worker 7, [28/32]: Training Loss: 0.644553003, Training Accuracy: 80.928\n",
            "Worker 7, [29/32]: Training Loss: 0.652687301, Training Accuracy: 80.144\n",
            "Worker 7, [30/32]: Training Loss: 0.623303589, Training Accuracy: 80.976\n",
            "Worker 7, [31/32]: Training Loss: 0.633211961, Training Accuracy: 80.112\n",
            "Worker 7, [32/32]: Training Loss: 0.603044045, Training Accuracy: 81.488\n",
            "Time taken for training worker 7: 0:01:31.136472\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/32]: Training Loss: 2.695772481, Training Accuracy: 33.280\n",
            "Worker 8, [02/32]: Training Loss: 2.305374352, Training Accuracy: 39.984\n",
            "Worker 8, [03/32]: Training Loss: 2.113754658, Training Accuracy: 43.296\n",
            "Worker 8, [04/32]: Training Loss: 1.995796437, Training Accuracy: 46.208\n",
            "Worker 8, [05/32]: Training Loss: 1.849165863, Training Accuracy: 48.640\n",
            "Worker 8, [06/32]: Training Loss: 1.767699913, Training Accuracy: 50.960\n",
            "Worker 8, [07/32]: Training Loss: 1.670362662, Training Accuracy: 53.296\n",
            "Worker 8, [08/32]: Training Loss: 1.566188829, Training Accuracy: 56.032\n",
            "Worker 8, [09/32]: Training Loss: 1.494777495, Training Accuracy: 57.456\n",
            "Worker 8, [10/32]: Training Loss: 1.442033433, Training Accuracy: 58.240\n",
            "Worker 8, [11/32]: Training Loss: 1.335751023, Training Accuracy: 61.568\n",
            "Worker 8, [12/32]: Training Loss: 1.293095468, Training Accuracy: 62.464\n",
            "Worker 8, [13/32]: Training Loss: 1.236987129, Training Accuracy: 63.936\n",
            "Worker 8, [14/32]: Training Loss: 1.142059911, Training Accuracy: 65.696\n",
            "Worker 8, [15/32]: Training Loss: 1.148240623, Training Accuracy: 66.144\n",
            "Worker 8, [16/32]: Training Loss: 1.084913107, Training Accuracy: 68.688\n",
            "Worker 8, [17/32]: Training Loss: 1.044553187, Training Accuracy: 68.624\n",
            "Worker 8, [18/32]: Training Loss: 0.985022356, Training Accuracy: 70.544\n",
            "Worker 8, [19/32]: Training Loss: 0.949690091, Training Accuracy: 71.552\n",
            "Worker 8, [20/32]: Training Loss: 0.908552501, Training Accuracy: 72.624\n",
            "Worker 8, [21/32]: Training Loss: 0.861223142, Training Accuracy: 74.144\n",
            "Worker 8, [22/32]: Training Loss: 0.844332479, Training Accuracy: 74.960\n",
            "Worker 8, [23/32]: Training Loss: 0.818315223, Training Accuracy: 75.344\n",
            "Worker 8, [24/32]: Training Loss: 0.760668679, Training Accuracy: 76.864\n",
            "Worker 8, [25/32]: Training Loss: 0.734640185, Training Accuracy: 77.488\n",
            "Worker 8, [26/32]: Training Loss: 0.739908573, Training Accuracy: 77.952\n",
            "Worker 8, [27/32]: Training Loss: 0.709809707, Training Accuracy: 78.352\n",
            "Worker 8, [28/32]: Training Loss: 0.676572430, Training Accuracy: 79.472\n",
            "Worker 8, [29/32]: Training Loss: 0.606996673, Training Accuracy: 81.888\n",
            "Worker 8, [30/32]: Training Loss: 0.644108684, Training Accuracy: 80.000\n",
            "Worker 8, [31/32]: Training Loss: 0.610182041, Training Accuracy: 81.840\n",
            "Worker 8, [32/32]: Training Loss: 0.555955343, Training Accuracy: 82.880\n",
            "Time taken for training worker 8: 0:01:32.021444\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003798\n",
            "Global Update 03: Test Loss: 5.185563929, Test Accuracy: 40.010\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.874494976, Training Accuracy: 40.128\n",
            "Worker 1, [02/32]: Training Loss: 2.071351610, Training Accuracy: 45.904\n",
            "Worker 1, [03/32]: Training Loss: 1.879568431, Training Accuracy: 49.888\n",
            "Worker 1, [04/32]: Training Loss: 1.742111549, Training Accuracy: 52.064\n",
            "Worker 1, [05/32]: Training Loss: 1.605135955, Training Accuracy: 55.968\n",
            "Worker 1, [06/32]: Training Loss: 1.478742980, Training Accuracy: 58.592\n",
            "Worker 1, [07/32]: Training Loss: 1.418024439, Training Accuracy: 59.728\n",
            "Worker 1, [08/32]: Training Loss: 1.334144753, Training Accuracy: 62.032\n",
            "Worker 1, [09/32]: Training Loss: 1.245096326, Training Accuracy: 63.888\n",
            "Worker 1, [10/32]: Training Loss: 1.171828009, Training Accuracy: 66.832\n",
            "Worker 1, [11/32]: Training Loss: 1.113847178, Training Accuracy: 68.224\n",
            "Worker 1, [12/32]: Training Loss: 1.045366031, Training Accuracy: 69.824\n",
            "Worker 1, [13/32]: Training Loss: 1.007647853, Training Accuracy: 71.616\n",
            "Worker 1, [14/32]: Training Loss: 0.932269121, Training Accuracy: 73.104\n",
            "Worker 1, [15/32]: Training Loss: 0.899136798, Training Accuracy: 73.904\n",
            "Worker 1, [16/32]: Training Loss: 0.840617370, Training Accuracy: 75.728\n",
            "Worker 1, [17/32]: Training Loss: 0.798493886, Training Accuracy: 77.392\n",
            "Worker 1, [18/32]: Training Loss: 0.753628616, Training Accuracy: 78.320\n",
            "Worker 1, [19/32]: Training Loss: 0.741380636, Training Accuracy: 78.080\n",
            "Worker 1, [20/32]: Training Loss: 0.688775448, Training Accuracy: 80.496\n",
            "Worker 1, [21/32]: Training Loss: 0.646191153, Training Accuracy: 81.504\n",
            "Worker 1, [22/32]: Training Loss: 0.606891376, Training Accuracy: 82.400\n",
            "Worker 1, [23/32]: Training Loss: 0.588216550, Training Accuracy: 82.880\n",
            "Worker 1, [24/32]: Training Loss: 0.553471066, Training Accuracy: 84.448\n",
            "Worker 1, [25/32]: Training Loss: 0.545443976, Training Accuracy: 83.712\n",
            "Worker 1, [26/32]: Training Loss: 0.517439972, Training Accuracy: 84.816\n",
            "Worker 1, [27/32]: Training Loss: 0.487600634, Training Accuracy: 86.400\n",
            "Worker 1, [28/32]: Training Loss: 0.476056140, Training Accuracy: 86.400\n",
            "Worker 1, [29/32]: Training Loss: 0.459897269, Training Accuracy: 86.736\n",
            "Worker 1, [30/32]: Training Loss: 0.436660703, Training Accuracy: 87.600\n",
            "Worker 1, [31/32]: Training Loss: 0.429204324, Training Accuracy: 87.680\n",
            "Worker 1, [32/32]: Training Loss: 0.399181090, Training Accuracy: 88.864\n",
            "Time taken for training worker 1: 0:01:27.460844\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.874788631, Training Accuracy: 39.920\n",
            "Worker 2, [02/32]: Training Loss: 2.095470157, Training Accuracy: 45.440\n",
            "Worker 2, [03/32]: Training Loss: 1.905332489, Training Accuracy: 48.848\n",
            "Worker 2, [04/32]: Training Loss: 1.745363694, Training Accuracy: 52.240\n",
            "Worker 2, [05/32]: Training Loss: 1.622503970, Training Accuracy: 55.232\n",
            "Worker 2, [06/32]: Training Loss: 1.518382212, Training Accuracy: 57.120\n",
            "Worker 2, [07/32]: Training Loss: 1.407532241, Training Accuracy: 61.040\n",
            "Worker 2, [08/32]: Training Loss: 1.344319238, Training Accuracy: 61.920\n",
            "Worker 2, [09/32]: Training Loss: 1.261297085, Training Accuracy: 64.432\n",
            "Worker 2, [10/32]: Training Loss: 1.187886398, Training Accuracy: 66.672\n",
            "Worker 2, [11/32]: Training Loss: 1.136707558, Training Accuracy: 67.392\n",
            "Worker 2, [12/32]: Training Loss: 1.056303699, Training Accuracy: 69.728\n",
            "Worker 2, [13/32]: Training Loss: 1.002498882, Training Accuracy: 70.592\n",
            "Worker 2, [14/32]: Training Loss: 0.957314778, Training Accuracy: 72.352\n",
            "Worker 2, [15/32]: Training Loss: 0.892055112, Training Accuracy: 74.640\n",
            "Worker 2, [16/32]: Training Loss: 0.858028131, Training Accuracy: 74.688\n",
            "Worker 2, [17/32]: Training Loss: 0.796993487, Training Accuracy: 77.360\n",
            "Worker 2, [18/32]: Training Loss: 0.774404068, Training Accuracy: 77.456\n",
            "Worker 2, [19/32]: Training Loss: 0.744680431, Training Accuracy: 78.480\n",
            "Worker 2, [20/32]: Training Loss: 0.690105547, Training Accuracy: 79.776\n",
            "Worker 2, [21/32]: Training Loss: 0.650270620, Training Accuracy: 80.928\n",
            "Worker 2, [22/32]: Training Loss: 0.625914583, Training Accuracy: 81.520\n",
            "Worker 2, [23/32]: Training Loss: 0.605811270, Training Accuracy: 82.224\n",
            "Worker 2, [24/32]: Training Loss: 0.577677312, Training Accuracy: 82.560\n",
            "Worker 2, [25/32]: Training Loss: 0.554190483, Training Accuracy: 84.144\n",
            "Worker 2, [26/32]: Training Loss: 0.532535282, Training Accuracy: 84.304\n",
            "Worker 2, [27/32]: Training Loss: 0.509462581, Training Accuracy: 85.472\n",
            "Worker 2, [28/32]: Training Loss: 0.477442872, Training Accuracy: 86.480\n",
            "Worker 2, [29/32]: Training Loss: 0.464630270, Training Accuracy: 86.288\n",
            "Worker 2, [30/32]: Training Loss: 0.453344765, Training Accuracy: 87.248\n",
            "Worker 2, [31/32]: Training Loss: 0.426768486, Training Accuracy: 88.416\n",
            "Worker 2, [32/32]: Training Loss: 0.412478737, Training Accuracy: 88.160\n",
            "Time taken for training worker 2: 0:01:26.571378\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 2.903760630, Training Accuracy: 39.648\n",
            "Worker 3, [02/32]: Training Loss: 2.076477995, Training Accuracy: 45.024\n",
            "Worker 3, [03/32]: Training Loss: 1.873301397, Training Accuracy: 49.216\n",
            "Worker 3, [04/32]: Training Loss: 1.721126567, Training Accuracy: 52.880\n",
            "Worker 3, [05/32]: Training Loss: 1.615888377, Training Accuracy: 54.960\n",
            "Worker 3, [06/32]: Training Loss: 1.530827154, Training Accuracy: 56.960\n",
            "Worker 3, [07/32]: Training Loss: 1.438938789, Training Accuracy: 59.552\n",
            "Worker 3, [08/32]: Training Loss: 1.353396005, Training Accuracy: 61.744\n",
            "Worker 3, [09/32]: Training Loss: 1.253803605, Training Accuracy: 64.096\n",
            "Worker 3, [10/32]: Training Loss: 1.189356524, Training Accuracy: 65.648\n",
            "Worker 3, [11/32]: Training Loss: 1.123368069, Training Accuracy: 67.984\n",
            "Worker 3, [12/32]: Training Loss: 1.084421994, Training Accuracy: 68.624\n",
            "Worker 3, [13/32]: Training Loss: 1.034222663, Training Accuracy: 69.936\n",
            "Worker 3, [14/32]: Training Loss: 0.952514637, Training Accuracy: 72.384\n",
            "Worker 3, [15/32]: Training Loss: 0.897605929, Training Accuracy: 74.272\n",
            "Worker 3, [16/32]: Training Loss: 0.881884103, Training Accuracy: 74.512\n",
            "Worker 3, [17/32]: Training Loss: 0.820943774, Training Accuracy: 76.240\n",
            "Worker 3, [18/32]: Training Loss: 0.792563864, Training Accuracy: 77.280\n",
            "Worker 3, [19/32]: Training Loss: 0.746163312, Training Accuracy: 78.320\n",
            "Worker 3, [20/32]: Training Loss: 0.720896322, Training Accuracy: 79.632\n",
            "Worker 3, [21/32]: Training Loss: 0.679502478, Training Accuracy: 80.624\n",
            "Worker 3, [22/32]: Training Loss: 0.628992893, Training Accuracy: 81.696\n",
            "Worker 3, [23/32]: Training Loss: 0.621061435, Training Accuracy: 81.936\n",
            "Worker 3, [24/32]: Training Loss: 0.590537974, Training Accuracy: 82.320\n",
            "Worker 3, [25/32]: Training Loss: 0.539544542, Training Accuracy: 84.736\n",
            "Worker 3, [26/32]: Training Loss: 0.550146209, Training Accuracy: 84.080\n",
            "Worker 3, [27/32]: Training Loss: 0.511849192, Training Accuracy: 85.600\n",
            "Worker 3, [28/32]: Training Loss: 0.492261988, Training Accuracy: 85.680\n",
            "Worker 3, [29/32]: Training Loss: 0.472092830, Training Accuracy: 86.912\n",
            "Worker 3, [30/32]: Training Loss: 0.463298575, Training Accuracy: 87.088\n",
            "Worker 3, [31/32]: Training Loss: 0.444301816, Training Accuracy: 87.408\n",
            "Worker 3, [32/32]: Training Loss: 0.410008922, Training Accuracy: 88.400\n",
            "Time taken for training worker 3: 0:01:25.440772\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 2.934837077, Training Accuracy: 38.992\n",
            "Worker 4, [02/32]: Training Loss: 2.074259580, Training Accuracy: 44.992\n",
            "Worker 4, [03/32]: Training Loss: 1.888545011, Training Accuracy: 48.976\n",
            "Worker 4, [04/32]: Training Loss: 1.765037140, Training Accuracy: 51.408\n",
            "Worker 4, [05/32]: Training Loss: 1.617418615, Training Accuracy: 55.104\n",
            "Worker 4, [06/32]: Training Loss: 1.526217646, Training Accuracy: 57.104\n",
            "Worker 4, [07/32]: Training Loss: 1.441325286, Training Accuracy: 58.752\n",
            "Worker 4, [08/32]: Training Loss: 1.352521492, Training Accuracy: 61.408\n",
            "Worker 4, [09/32]: Training Loss: 1.287543870, Training Accuracy: 62.688\n",
            "Worker 4, [10/32]: Training Loss: 1.188188024, Training Accuracy: 66.176\n",
            "Worker 4, [11/32]: Training Loss: 1.133925070, Training Accuracy: 67.280\n",
            "Worker 4, [12/32]: Training Loss: 1.077442227, Training Accuracy: 69.264\n",
            "Worker 4, [13/32]: Training Loss: 1.020410026, Training Accuracy: 70.704\n",
            "Worker 4, [14/32]: Training Loss: 0.972038202, Training Accuracy: 71.904\n",
            "Worker 4, [15/32]: Training Loss: 0.930993502, Training Accuracy: 73.360\n",
            "Worker 4, [16/32]: Training Loss: 0.851428524, Training Accuracy: 75.280\n",
            "Worker 4, [17/32]: Training Loss: 0.823294177, Training Accuracy: 76.256\n",
            "Worker 4, [18/32]: Training Loss: 0.789547426, Training Accuracy: 76.688\n",
            "Worker 4, [19/32]: Training Loss: 0.729022568, Training Accuracy: 79.264\n",
            "Worker 4, [20/32]: Training Loss: 0.718470799, Training Accuracy: 79.408\n",
            "Worker 4, [21/32]: Training Loss: 0.702538372, Training Accuracy: 79.200\n",
            "Worker 4, [22/32]: Training Loss: 0.633415999, Training Accuracy: 81.856\n",
            "Worker 4, [23/32]: Training Loss: 0.623407542, Training Accuracy: 81.808\n",
            "Worker 4, [24/32]: Training Loss: 0.591867234, Training Accuracy: 83.264\n",
            "Worker 4, [25/32]: Training Loss: 0.569441153, Training Accuracy: 83.376\n",
            "Worker 4, [26/32]: Training Loss: 0.547451128, Training Accuracy: 84.032\n",
            "Worker 4, [27/32]: Training Loss: 0.536471293, Training Accuracy: 84.080\n",
            "Worker 4, [28/32]: Training Loss: 0.511670636, Training Accuracy: 84.528\n",
            "Worker 4, [29/32]: Training Loss: 0.497266599, Training Accuracy: 85.648\n",
            "Worker 4, [30/32]: Training Loss: 0.445692883, Training Accuracy: 87.168\n",
            "Worker 4, [31/32]: Training Loss: 0.436680738, Training Accuracy: 87.328\n",
            "Worker 4, [32/32]: Training Loss: 0.428121481, Training Accuracy: 87.632\n",
            "Time taken for training worker 4: 0:01:24.348114\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/32]: Training Loss: 2.926921959, Training Accuracy: 39.808\n",
            "Worker 5, [02/32]: Training Loss: 2.112599440, Training Accuracy: 45.120\n",
            "Worker 5, [03/32]: Training Loss: 1.872956714, Training Accuracy: 49.152\n",
            "Worker 5, [04/32]: Training Loss: 1.752728218, Training Accuracy: 51.712\n",
            "Worker 5, [05/32]: Training Loss: 1.654298439, Training Accuracy: 54.656\n",
            "Worker 5, [06/32]: Training Loss: 1.531794070, Training Accuracy: 57.184\n",
            "Worker 5, [07/32]: Training Loss: 1.422239537, Training Accuracy: 59.168\n",
            "Worker 5, [08/32]: Training Loss: 1.348158517, Training Accuracy: 60.752\n",
            "Worker 5, [09/32]: Training Loss: 1.279711383, Training Accuracy: 63.440\n",
            "Worker 5, [10/32]: Training Loss: 1.171577650, Training Accuracy: 66.608\n",
            "Worker 5, [11/32]: Training Loss: 1.107512561, Training Accuracy: 67.504\n",
            "Worker 5, [12/32]: Training Loss: 1.073525193, Training Accuracy: 68.800\n",
            "Worker 5, [13/32]: Training Loss: 1.013762530, Training Accuracy: 70.784\n",
            "Worker 5, [14/32]: Training Loss: 0.935963044, Training Accuracy: 73.568\n",
            "Worker 5, [15/32]: Training Loss: 0.897882463, Training Accuracy: 73.792\n",
            "Worker 5, [16/32]: Training Loss: 0.862477619, Training Accuracy: 74.912\n",
            "Worker 5, [17/32]: Training Loss: 0.816601732, Training Accuracy: 76.768\n",
            "Worker 5, [18/32]: Training Loss: 0.777388154, Training Accuracy: 78.208\n",
            "Worker 5, [19/32]: Training Loss: 0.749570301, Training Accuracy: 79.008\n",
            "Worker 5, [20/32]: Training Loss: 0.685167176, Training Accuracy: 80.464\n",
            "Worker 5, [21/32]: Training Loss: 0.674322919, Training Accuracy: 80.624\n",
            "Worker 5, [22/32]: Training Loss: 0.631680480, Training Accuracy: 81.552\n",
            "Worker 5, [23/32]: Training Loss: 0.608476773, Training Accuracy: 82.688\n",
            "Worker 5, [24/32]: Training Loss: 0.587680144, Training Accuracy: 82.528\n",
            "Worker 5, [25/32]: Training Loss: 0.551122491, Training Accuracy: 84.464\n",
            "Worker 5, [26/32]: Training Loss: 0.530211898, Training Accuracy: 84.912\n",
            "Worker 5, [27/32]: Training Loss: 0.519554326, Training Accuracy: 84.640\n",
            "Worker 5, [28/32]: Training Loss: 0.493861539, Training Accuracy: 85.696\n",
            "Worker 5, [29/32]: Training Loss: 0.451078731, Training Accuracy: 87.344\n",
            "Worker 5, [30/32]: Training Loss: 0.449326252, Training Accuracy: 86.944\n",
            "Worker 5, [31/32]: Training Loss: 0.424389881, Training Accuracy: 87.776\n",
            "Worker 5, [32/32]: Training Loss: 0.407513818, Training Accuracy: 88.816\n",
            "Time taken for training worker 5: 0:01:31.398629\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/32]: Training Loss: 2.930950475, Training Accuracy: 39.648\n",
            "Worker 6, [02/32]: Training Loss: 2.122482690, Training Accuracy: 45.616\n",
            "Worker 6, [03/32]: Training Loss: 1.905489015, Training Accuracy: 48.592\n",
            "Worker 6, [04/32]: Training Loss: 1.764097851, Training Accuracy: 52.384\n",
            "Worker 6, [05/32]: Training Loss: 1.624545377, Training Accuracy: 54.256\n",
            "Worker 6, [06/32]: Training Loss: 1.551165649, Training Accuracy: 56.416\n",
            "Worker 6, [07/32]: Training Loss: 1.449242072, Training Accuracy: 59.584\n",
            "Worker 6, [08/32]: Training Loss: 1.348114128, Training Accuracy: 62.224\n",
            "Worker 6, [09/32]: Training Loss: 1.275425763, Training Accuracy: 64.048\n",
            "Worker 6, [10/32]: Training Loss: 1.197575946, Training Accuracy: 65.136\n",
            "Worker 6, [11/32]: Training Loss: 1.137178617, Training Accuracy: 67.552\n",
            "Worker 6, [12/32]: Training Loss: 1.090377119, Training Accuracy: 69.136\n",
            "Worker 6, [13/32]: Training Loss: 1.025246572, Training Accuracy: 69.856\n",
            "Worker 6, [14/32]: Training Loss: 0.970407994, Training Accuracy: 71.952\n",
            "Worker 6, [15/32]: Training Loss: 0.914594007, Training Accuracy: 73.856\n",
            "Worker 6, [16/32]: Training Loss: 0.857677679, Training Accuracy: 76.288\n",
            "Worker 6, [17/32]: Training Loss: 0.829442141, Training Accuracy: 75.616\n",
            "Worker 6, [18/32]: Training Loss: 0.775621920, Training Accuracy: 77.472\n",
            "Worker 6, [19/32]: Training Loss: 0.724558996, Training Accuracy: 79.136\n",
            "Worker 6, [20/32]: Training Loss: 0.680483685, Training Accuracy: 80.256\n",
            "Worker 6, [21/32]: Training Loss: 0.659820240, Training Accuracy: 80.416\n",
            "Worker 6, [22/32]: Training Loss: 0.654503107, Training Accuracy: 80.528\n",
            "Worker 6, [23/32]: Training Loss: 0.605471369, Training Accuracy: 82.576\n",
            "Worker 6, [24/32]: Training Loss: 0.579995499, Training Accuracy: 83.440\n",
            "Worker 6, [25/32]: Training Loss: 0.545344590, Training Accuracy: 84.592\n",
            "Worker 6, [26/32]: Training Loss: 0.522085167, Training Accuracy: 84.960\n",
            "Worker 6, [27/32]: Training Loss: 0.524871205, Training Accuracy: 84.448\n",
            "Worker 6, [28/32]: Training Loss: 0.478166317, Training Accuracy: 85.872\n",
            "Worker 6, [29/32]: Training Loss: 0.451736741, Training Accuracy: 86.576\n",
            "Worker 6, [30/32]: Training Loss: 0.441273314, Training Accuracy: 87.152\n",
            "Worker 6, [31/32]: Training Loss: 0.428066857, Training Accuracy: 87.808\n",
            "Worker 6, [32/32]: Training Loss: 0.416981447, Training Accuracy: 88.160\n",
            "Time taken for training worker 6: 0:01:26.541901\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/32]: Training Loss: 2.900629698, Training Accuracy: 39.376\n",
            "Worker 7, [02/32]: Training Loss: 2.102369930, Training Accuracy: 44.320\n",
            "Worker 7, [03/32]: Training Loss: 1.906320160, Training Accuracy: 49.184\n",
            "Worker 7, [04/32]: Training Loss: 1.718279030, Training Accuracy: 52.240\n",
            "Worker 7, [05/32]: Training Loss: 1.617043046, Training Accuracy: 54.960\n",
            "Worker 7, [06/32]: Training Loss: 1.533169262, Training Accuracy: 56.976\n",
            "Worker 7, [07/32]: Training Loss: 1.416643259, Training Accuracy: 60.752\n",
            "Worker 7, [08/32]: Training Loss: 1.359015519, Training Accuracy: 61.632\n",
            "Worker 7, [09/32]: Training Loss: 1.285146918, Training Accuracy: 63.888\n",
            "Worker 7, [10/32]: Training Loss: 1.177770453, Training Accuracy: 66.624\n",
            "Worker 7, [11/32]: Training Loss: 1.122214686, Training Accuracy: 67.840\n",
            "Worker 7, [12/32]: Training Loss: 1.063884739, Training Accuracy: 69.120\n",
            "Worker 7, [13/32]: Training Loss: 1.013766464, Training Accuracy: 70.704\n",
            "Worker 7, [14/32]: Training Loss: 1.009800815, Training Accuracy: 70.896\n",
            "Worker 7, [15/32]: Training Loss: 0.895582609, Training Accuracy: 73.840\n",
            "Worker 7, [16/32]: Training Loss: 0.836413127, Training Accuracy: 75.616\n",
            "Worker 7, [17/32]: Training Loss: 0.815173775, Training Accuracy: 76.336\n",
            "Worker 7, [18/32]: Training Loss: 0.764972334, Training Accuracy: 78.192\n",
            "Worker 7, [19/32]: Training Loss: 0.735731653, Training Accuracy: 78.848\n",
            "Worker 7, [20/32]: Training Loss: 0.708394750, Training Accuracy: 79.456\n",
            "Worker 7, [21/32]: Training Loss: 0.681526881, Training Accuracy: 80.752\n",
            "Worker 7, [22/32]: Training Loss: 0.642326538, Training Accuracy: 82.112\n",
            "Worker 7, [23/32]: Training Loss: 0.598294121, Training Accuracy: 82.656\n",
            "Worker 7, [24/32]: Training Loss: 0.580531721, Training Accuracy: 83.408\n",
            "Worker 7, [25/32]: Training Loss: 0.566755312, Training Accuracy: 83.840\n",
            "Worker 7, [26/32]: Training Loss: 0.518833858, Training Accuracy: 84.656\n",
            "Worker 7, [27/32]: Training Loss: 0.504077268, Training Accuracy: 85.152\n",
            "Worker 7, [28/32]: Training Loss: 0.478106168, Training Accuracy: 86.400\n",
            "Worker 7, [29/32]: Training Loss: 0.475373184, Training Accuracy: 86.064\n",
            "Worker 7, [30/32]: Training Loss: 0.470129232, Training Accuracy: 86.976\n",
            "Worker 7, [31/32]: Training Loss: 0.442408766, Training Accuracy: 87.296\n",
            "Worker 7, [32/32]: Training Loss: 0.422972754, Training Accuracy: 88.112\n",
            "Time taken for training worker 7: 0:01:24.524916\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/32]: Training Loss: 2.878493006, Training Accuracy: 40.016\n",
            "Worker 8, [02/32]: Training Loss: 2.130121853, Training Accuracy: 43.648\n",
            "Worker 8, [03/32]: Training Loss: 1.898641490, Training Accuracy: 48.560\n",
            "Worker 8, [04/32]: Training Loss: 1.748562590, Training Accuracy: 52.256\n",
            "Worker 8, [05/32]: Training Loss: 1.651705591, Training Accuracy: 54.256\n",
            "Worker 8, [06/32]: Training Loss: 1.546403192, Training Accuracy: 57.232\n",
            "Worker 8, [07/32]: Training Loss: 1.465791628, Training Accuracy: 59.136\n",
            "Worker 8, [08/32]: Training Loss: 1.371322737, Training Accuracy: 61.760\n",
            "Worker 8, [09/32]: Training Loss: 1.288835158, Training Accuracy: 63.728\n",
            "Worker 8, [10/32]: Training Loss: 1.189730908, Training Accuracy: 66.352\n",
            "Worker 8, [11/32]: Training Loss: 1.162281117, Training Accuracy: 66.928\n",
            "Worker 8, [12/32]: Training Loss: 1.087224440, Training Accuracy: 68.256\n",
            "Worker 8, [13/32]: Training Loss: 1.026989024, Training Accuracy: 70.624\n",
            "Worker 8, [14/32]: Training Loss: 0.955109118, Training Accuracy: 72.368\n",
            "Worker 8, [15/32]: Training Loss: 0.905771107, Training Accuracy: 74.224\n",
            "Worker 8, [16/32]: Training Loss: 0.887831750, Training Accuracy: 75.024\n",
            "Worker 8, [17/32]: Training Loss: 0.831114731, Training Accuracy: 76.464\n",
            "Worker 8, [18/32]: Training Loss: 0.783230668, Training Accuracy: 77.264\n",
            "Worker 8, [19/32]: Training Loss: 0.741276637, Training Accuracy: 78.528\n",
            "Worker 8, [20/32]: Training Loss: 0.713637180, Training Accuracy: 79.024\n",
            "Worker 8, [21/32]: Training Loss: 0.687105963, Training Accuracy: 80.352\n",
            "Worker 8, [22/32]: Training Loss: 0.650373154, Training Accuracy: 80.944\n",
            "Worker 8, [23/32]: Training Loss: 0.605034053, Training Accuracy: 82.992\n",
            "Worker 8, [24/32]: Training Loss: 0.580537427, Training Accuracy: 83.120\n",
            "Worker 8, [25/32]: Training Loss: 0.559373787, Training Accuracy: 84.240\n",
            "Worker 8, [26/32]: Training Loss: 0.533057547, Training Accuracy: 84.512\n",
            "Worker 8, [27/32]: Training Loss: 0.529675449, Training Accuracy: 84.832\n",
            "Worker 8, [28/32]: Training Loss: 0.476484127, Training Accuracy: 86.416\n",
            "Worker 8, [29/32]: Training Loss: 0.456127966, Training Accuracy: 86.944\n",
            "Worker 8, [30/32]: Training Loss: 0.440018023, Training Accuracy: 87.536\n",
            "Worker 8, [31/32]: Training Loss: 0.443608245, Training Accuracy: 87.488\n",
            "Worker 8, [32/32]: Training Loss: 0.428561415, Training Accuracy: 88.176\n",
            "Time taken for training worker 8: 0:01:25.083532\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003948\n",
            "Global Update 04: Test Loss: 4.326718157, Test Accuracy: 40.040\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 1:22:41.561671\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:64\n",
            "==================================================\n",
            "Worker 1, [01/64]: Training Loss: 4.596535955, Training Accuracy: 1.296\n",
            "Worker 1, [02/64]: Training Loss: 4.403325222, Training Accuracy: 3.600\n",
            "Worker 1, [03/64]: Training Loss: 4.131942710, Training Accuracy: 5.808\n",
            "Worker 1, [04/64]: Training Loss: 4.004316962, Training Accuracy: 7.232\n",
            "Worker 1, [05/64]: Training Loss: 3.887697419, Training Accuracy: 9.152\n",
            "Worker 1, [06/64]: Training Loss: 3.802270894, Training Accuracy: 9.936\n",
            "Worker 1, [07/64]: Training Loss: 3.715356491, Training Accuracy: 11.472\n",
            "Worker 1, [08/64]: Training Loss: 3.647779421, Training Accuracy: 12.800\n",
            "Worker 1, [09/64]: Training Loss: 3.554371294, Training Accuracy: 13.776\n",
            "Worker 1, [10/64]: Training Loss: 3.498907627, Training Accuracy: 14.720\n",
            "Worker 1, [11/64]: Training Loss: 3.404503759, Training Accuracy: 16.688\n",
            "Worker 1, [12/64]: Training Loss: 3.342209711, Training Accuracy: 17.584\n",
            "Worker 1, [13/64]: Training Loss: 3.259259946, Training Accuracy: 19.376\n",
            "Worker 1, [14/64]: Training Loss: 3.199244429, Training Accuracy: 20.272\n",
            "Worker 1, [15/64]: Training Loss: 3.138203487, Training Accuracy: 21.808\n",
            "Worker 1, [16/64]: Training Loss: 3.064472065, Training Accuracy: 22.672\n",
            "Worker 1, [17/64]: Training Loss: 3.015157381, Training Accuracy: 23.584\n",
            "Worker 1, [18/64]: Training Loss: 2.948796374, Training Accuracy: 25.072\n",
            "Worker 1, [19/64]: Training Loss: 2.881372576, Training Accuracy: 25.680\n",
            "Worker 1, [20/64]: Training Loss: 2.818491564, Training Accuracy: 27.088\n",
            "Worker 1, [21/64]: Training Loss: 2.774716715, Training Accuracy: 28.256\n",
            "Worker 1, [22/64]: Training Loss: 2.701421942, Training Accuracy: 29.216\n",
            "Worker 1, [23/64]: Training Loss: 2.645414678, Training Accuracy: 30.400\n",
            "Worker 1, [24/64]: Training Loss: 2.595682760, Training Accuracy: 32.432\n",
            "Worker 1, [25/64]: Training Loss: 2.541526239, Training Accuracy: 32.688\n",
            "Worker 1, [26/64]: Training Loss: 2.482489937, Training Accuracy: 33.168\n",
            "Worker 1, [27/64]: Training Loss: 2.391449673, Training Accuracy: 35.376\n",
            "Worker 1, [28/64]: Training Loss: 2.356170861, Training Accuracy: 35.952\n",
            "Worker 1, [29/64]: Training Loss: 2.307901126, Training Accuracy: 37.536\n",
            "Worker 1, [30/64]: Training Loss: 2.240142564, Training Accuracy: 38.624\n",
            "Worker 1, [31/64]: Training Loss: 2.208527722, Training Accuracy: 39.920\n",
            "Worker 1, [32/64]: Training Loss: 2.152185569, Training Accuracy: 41.488\n",
            "Worker 1, [33/64]: Training Loss: 2.132634236, Training Accuracy: 41.760\n",
            "Worker 1, [34/64]: Training Loss: 2.055140319, Training Accuracy: 42.960\n",
            "Worker 1, [35/64]: Training Loss: 2.001542703, Training Accuracy: 43.744\n",
            "Worker 1, [36/64]: Training Loss: 1.937634317, Training Accuracy: 45.824\n",
            "Worker 1, [37/64]: Training Loss: 1.921674255, Training Accuracy: 45.872\n",
            "Worker 1, [38/64]: Training Loss: 1.836711453, Training Accuracy: 47.536\n",
            "Worker 1, [39/64]: Training Loss: 1.820045603, Training Accuracy: 49.056\n",
            "Worker 1, [40/64]: Training Loss: 1.754432074, Training Accuracy: 50.768\n",
            "Worker 1, [41/64]: Training Loss: 1.726036180, Training Accuracy: 50.448\n",
            "Worker 1, [42/64]: Training Loss: 1.688540938, Training Accuracy: 51.360\n",
            "Worker 1, [43/64]: Training Loss: 1.642126620, Training Accuracy: 53.024\n",
            "Worker 1, [44/64]: Training Loss: 1.642453067, Training Accuracy: 52.352\n",
            "Worker 1, [45/64]: Training Loss: 1.591026410, Training Accuracy: 54.288\n",
            "Worker 1, [46/64]: Training Loss: 1.511758321, Training Accuracy: 56.928\n",
            "Worker 1, [47/64]: Training Loss: 1.526615991, Training Accuracy: 55.408\n",
            "Worker 1, [48/64]: Training Loss: 1.504259505, Training Accuracy: 56.544\n",
            "Worker 1, [49/64]: Training Loss: 1.441565252, Training Accuracy: 58.240\n",
            "Worker 1, [50/64]: Training Loss: 1.408600002, Training Accuracy: 57.984\n",
            "Worker 1, [51/64]: Training Loss: 1.401796023, Training Accuracy: 59.088\n",
            "Worker 1, [52/64]: Training Loss: 1.333721795, Training Accuracy: 61.296\n",
            "Worker 1, [53/64]: Training Loss: 1.287742063, Training Accuracy: 61.936\n",
            "Worker 1, [54/64]: Training Loss: 1.281697361, Training Accuracy: 62.832\n",
            "Worker 1, [55/64]: Training Loss: 1.223666879, Training Accuracy: 63.808\n",
            "Worker 1, [56/64]: Training Loss: 1.212523760, Training Accuracy: 63.552\n",
            "Worker 1, [57/64]: Training Loss: 1.181638444, Training Accuracy: 64.944\n",
            "Worker 1, [58/64]: Training Loss: 1.199815548, Training Accuracy: 64.624\n",
            "Worker 1, [59/64]: Training Loss: 1.205925837, Training Accuracy: 64.160\n",
            "Worker 1, [60/64]: Training Loss: 1.188593866, Training Accuracy: 64.672\n",
            "Worker 1, [61/64]: Training Loss: 1.138521420, Training Accuracy: 66.144\n",
            "Worker 1, [62/64]: Training Loss: 1.075713690, Training Accuracy: 68.080\n",
            "Worker 1, [63/64]: Training Loss: 1.092368331, Training Accuracy: 67.152\n",
            "Worker 1, [64/64]: Training Loss: 1.125842743, Training Accuracy: 66.736\n",
            "Time taken for training worker 1: 0:02:49.925545\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/64]: Training Loss: 4.595425007, Training Accuracy: 1.792\n",
            "Worker 2, [02/64]: Training Loss: 4.412765440, Training Accuracy: 3.568\n",
            "Worker 2, [03/64]: Training Loss: 4.198381203, Training Accuracy: 5.552\n",
            "Worker 2, [04/64]: Training Loss: 4.037620272, Training Accuracy: 7.840\n",
            "Worker 2, [05/64]: Training Loss: 3.911164695, Training Accuracy: 10.160\n",
            "Worker 2, [06/64]: Training Loss: 3.832814764, Training Accuracy: 10.992\n",
            "Worker 2, [07/64]: Training Loss: 3.713684386, Training Accuracy: 12.512\n",
            "Worker 2, [08/64]: Training Loss: 3.642728635, Training Accuracy: 13.504\n",
            "Worker 2, [09/64]: Training Loss: 3.542639815, Training Accuracy: 15.520\n",
            "Worker 2, [10/64]: Training Loss: 3.470784056, Training Accuracy: 16.304\n",
            "Worker 2, [11/64]: Training Loss: 3.379568562, Training Accuracy: 17.040\n",
            "Worker 2, [12/64]: Training Loss: 3.306071629, Training Accuracy: 18.592\n",
            "Worker 2, [13/64]: Training Loss: 3.232428410, Training Accuracy: 20.672\n",
            "Worker 2, [14/64]: Training Loss: 3.208185950, Training Accuracy: 20.016\n",
            "Worker 2, [15/64]: Training Loss: 3.135958122, Training Accuracy: 21.536\n",
            "Worker 2, [16/64]: Training Loss: 3.047161504, Training Accuracy: 23.120\n",
            "Worker 2, [17/64]: Training Loss: 2.991817985, Training Accuracy: 25.040\n",
            "Worker 2, [18/64]: Training Loss: 2.933051114, Training Accuracy: 25.264\n",
            "Worker 2, [19/64]: Training Loss: 2.873277671, Training Accuracy: 25.824\n",
            "Worker 2, [20/64]: Training Loss: 2.836137078, Training Accuracy: 27.152\n",
            "Worker 2, [21/64]: Training Loss: 2.779934555, Training Accuracy: 28.000\n",
            "Worker 2, [22/64]: Training Loss: 2.717765835, Training Accuracy: 29.808\n",
            "Worker 2, [23/64]: Training Loss: 2.644013759, Training Accuracy: 30.432\n",
            "Worker 2, [24/64]: Training Loss: 2.593023607, Training Accuracy: 32.416\n",
            "Worker 2, [25/64]: Training Loss: 2.528320950, Training Accuracy: 33.184\n",
            "Worker 2, [26/64]: Training Loss: 2.486052660, Training Accuracy: 34.816\n",
            "Worker 2, [27/64]: Training Loss: 2.432234426, Training Accuracy: 35.696\n",
            "Worker 2, [28/64]: Training Loss: 2.391534477, Training Accuracy: 35.552\n",
            "Worker 2, [29/64]: Training Loss: 2.357365915, Training Accuracy: 36.592\n",
            "Worker 2, [30/64]: Training Loss: 2.315449402, Training Accuracy: 37.648\n",
            "Worker 2, [31/64]: Training Loss: 2.263865701, Training Accuracy: 38.304\n",
            "Worker 2, [32/64]: Training Loss: 2.163479159, Training Accuracy: 40.992\n",
            "Worker 2, [33/64]: Training Loss: 2.127947770, Training Accuracy: 41.280\n",
            "Worker 2, [34/64]: Training Loss: 2.072339288, Training Accuracy: 42.928\n",
            "Worker 2, [35/64]: Training Loss: 2.037995979, Training Accuracy: 43.152\n",
            "Worker 2, [36/64]: Training Loss: 2.012018269, Training Accuracy: 44.752\n",
            "Worker 2, [37/64]: Training Loss: 1.904806969, Training Accuracy: 46.736\n",
            "Worker 2, [38/64]: Training Loss: 1.890149949, Training Accuracy: 47.664\n",
            "Worker 2, [39/64]: Training Loss: 1.854283654, Training Accuracy: 47.536\n",
            "Worker 2, [40/64]: Training Loss: 1.819183797, Training Accuracy: 48.848\n",
            "Worker 2, [41/64]: Training Loss: 1.760028374, Training Accuracy: 50.080\n",
            "Worker 2, [42/64]: Training Loss: 1.674891028, Training Accuracy: 52.448\n",
            "Worker 2, [43/64]: Training Loss: 1.683419855, Training Accuracy: 52.480\n",
            "Worker 2, [44/64]: Training Loss: 1.675190019, Training Accuracy: 52.480\n",
            "Worker 2, [45/64]: Training Loss: 1.614563341, Training Accuracy: 54.064\n",
            "Worker 2, [46/64]: Training Loss: 1.520289928, Training Accuracy: 55.408\n",
            "Worker 2, [47/64]: Training Loss: 1.576079401, Training Accuracy: 54.496\n",
            "Worker 2, [48/64]: Training Loss: 1.468729350, Training Accuracy: 56.960\n",
            "Worker 2, [49/64]: Training Loss: 1.500868373, Training Accuracy: 56.624\n",
            "Worker 2, [50/64]: Training Loss: 1.447858703, Training Accuracy: 58.272\n",
            "Worker 2, [51/64]: Training Loss: 1.385683805, Training Accuracy: 59.200\n",
            "Worker 2, [52/64]: Training Loss: 1.341479217, Training Accuracy: 60.944\n",
            "Worker 2, [53/64]: Training Loss: 1.361693311, Training Accuracy: 61.008\n",
            "Worker 2, [54/64]: Training Loss: 1.273279005, Training Accuracy: 62.528\n",
            "Worker 2, [55/64]: Training Loss: 1.292629314, Training Accuracy: 61.968\n",
            "Worker 2, [56/64]: Training Loss: 1.258305758, Training Accuracy: 63.280\n",
            "Worker 2, [57/64]: Training Loss: 1.284541288, Training Accuracy: 62.480\n",
            "Worker 2, [58/64]: Training Loss: 1.174542517, Training Accuracy: 64.912\n",
            "Worker 2, [59/64]: Training Loss: 1.172449869, Training Accuracy: 65.856\n",
            "Worker 2, [60/64]: Training Loss: 1.164588026, Training Accuracy: 65.344\n",
            "Worker 2, [61/64]: Training Loss: 1.091481951, Training Accuracy: 67.952\n",
            "Worker 2, [62/64]: Training Loss: 1.136655006, Training Accuracy: 65.872\n",
            "Worker 2, [63/64]: Training Loss: 1.125247022, Training Accuracy: 67.072\n",
            "Worker 2, [64/64]: Training Loss: 1.039931988, Training Accuracy: 69.168\n",
            "Time taken for training worker 2: 0:02:49.771811\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/64]: Training Loss: 4.596782081, Training Accuracy: 1.696\n",
            "Worker 3, [02/64]: Training Loss: 4.418128816, Training Accuracy: 3.360\n",
            "Worker 3, [03/64]: Training Loss: 4.168228154, Training Accuracy: 5.296\n",
            "Worker 3, [04/64]: Training Loss: 4.042968750, Training Accuracy: 7.328\n",
            "Worker 3, [05/64]: Training Loss: 3.924729072, Training Accuracy: 8.912\n",
            "Worker 3, [06/64]: Training Loss: 3.829687929, Training Accuracy: 10.416\n",
            "Worker 3, [07/64]: Training Loss: 3.741428438, Training Accuracy: 11.504\n",
            "Worker 3, [08/64]: Training Loss: 3.654154649, Training Accuracy: 13.200\n",
            "Worker 3, [09/64]: Training Loss: 3.589245414, Training Accuracy: 14.512\n",
            "Worker 3, [10/64]: Training Loss: 3.528053955, Training Accuracy: 15.392\n",
            "Worker 3, [11/64]: Training Loss: 3.414054554, Training Accuracy: 16.560\n",
            "Worker 3, [12/64]: Training Loss: 3.350459391, Training Accuracy: 18.496\n",
            "Worker 3, [13/64]: Training Loss: 3.290746176, Training Accuracy: 19.024\n",
            "Worker 3, [14/64]: Training Loss: 3.228857688, Training Accuracy: 20.224\n",
            "Worker 3, [15/64]: Training Loss: 3.171757258, Training Accuracy: 21.104\n",
            "Worker 3, [16/64]: Training Loss: 3.081522946, Training Accuracy: 22.640\n",
            "Worker 3, [17/64]: Training Loss: 3.038227731, Training Accuracy: 23.232\n",
            "Worker 3, [18/64]: Training Loss: 2.986725031, Training Accuracy: 24.112\n",
            "Worker 3, [19/64]: Training Loss: 2.941053383, Training Accuracy: 25.376\n",
            "Worker 3, [20/64]: Training Loss: 2.832393955, Training Accuracy: 27.600\n",
            "Worker 3, [21/64]: Training Loss: 2.809193531, Training Accuracy: 28.416\n",
            "Worker 3, [22/64]: Training Loss: 2.721726899, Training Accuracy: 28.624\n",
            "Worker 3, [23/64]: Training Loss: 2.692301512, Training Accuracy: 29.968\n",
            "Worker 3, [24/64]: Training Loss: 2.615151087, Training Accuracy: 31.360\n",
            "Worker 3, [25/64]: Training Loss: 2.583323316, Training Accuracy: 32.304\n",
            "Worker 3, [26/64]: Training Loss: 2.499538293, Training Accuracy: 33.824\n",
            "Worker 3, [27/64]: Training Loss: 2.457930454, Training Accuracy: 35.200\n",
            "Worker 3, [28/64]: Training Loss: 2.421988420, Training Accuracy: 35.616\n",
            "Worker 3, [29/64]: Training Loss: 2.363592840, Training Accuracy: 36.160\n",
            "Worker 3, [30/64]: Training Loss: 2.286565923, Training Accuracy: 37.936\n",
            "Worker 3, [31/64]: Training Loss: 2.204341148, Training Accuracy: 40.224\n",
            "Worker 3, [32/64]: Training Loss: 2.200597273, Training Accuracy: 40.240\n",
            "Worker 3, [33/64]: Training Loss: 2.151679472, Training Accuracy: 41.408\n",
            "Worker 3, [34/64]: Training Loss: 2.082729548, Training Accuracy: 42.864\n",
            "Worker 3, [35/64]: Training Loss: 2.010571016, Training Accuracy: 44.688\n",
            "Worker 3, [36/64]: Training Loss: 2.002738235, Training Accuracy: 44.480\n",
            "Worker 3, [37/64]: Training Loss: 1.948760940, Training Accuracy: 45.328\n",
            "Worker 3, [38/64]: Training Loss: 1.900581654, Training Accuracy: 46.464\n",
            "Worker 3, [39/64]: Training Loss: 1.855808646, Training Accuracy: 47.840\n",
            "Worker 3, [40/64]: Training Loss: 1.859676600, Training Accuracy: 48.608\n",
            "Worker 3, [41/64]: Training Loss: 1.741167946, Training Accuracy: 50.592\n",
            "Worker 3, [42/64]: Training Loss: 1.752376880, Training Accuracy: 50.400\n",
            "Worker 3, [43/64]: Training Loss: 1.677890582, Training Accuracy: 53.008\n",
            "Worker 3, [44/64]: Training Loss: 1.619018720, Training Accuracy: 53.952\n",
            "Worker 3, [45/64]: Training Loss: 1.612358656, Training Accuracy: 53.616\n",
            "Worker 3, [46/64]: Training Loss: 1.623643758, Training Accuracy: 54.128\n",
            "Worker 3, [47/64]: Training Loss: 1.537290183, Training Accuracy: 55.248\n",
            "Worker 3, [48/64]: Training Loss: 1.525900788, Training Accuracy: 55.920\n",
            "Worker 3, [49/64]: Training Loss: 1.472264121, Training Accuracy: 57.712\n",
            "Worker 3, [50/64]: Training Loss: 1.418736301, Training Accuracy: 58.000\n",
            "Worker 3, [51/64]: Training Loss: 1.401812651, Training Accuracy: 60.192\n",
            "Worker 3, [52/64]: Training Loss: 1.395575605, Training Accuracy: 60.112\n",
            "Worker 3, [53/64]: Training Loss: 1.343785644, Training Accuracy: 60.464\n",
            "Worker 3, [54/64]: Training Loss: 1.337191562, Training Accuracy: 60.736\n",
            "Worker 3, [55/64]: Training Loss: 1.345578633, Training Accuracy: 61.120\n",
            "Worker 3, [56/64]: Training Loss: 1.267045626, Training Accuracy: 63.024\n",
            "Worker 3, [57/64]: Training Loss: 1.267957302, Training Accuracy: 63.104\n",
            "Worker 3, [58/64]: Training Loss: 1.242322256, Training Accuracy: 63.296\n",
            "Worker 3, [59/64]: Training Loss: 1.236088894, Training Accuracy: 63.616\n",
            "Worker 3, [60/64]: Training Loss: 1.176296184, Training Accuracy: 65.120\n",
            "Worker 3, [61/64]: Training Loss: 1.167045517, Training Accuracy: 65.888\n",
            "Worker 3, [62/64]: Training Loss: 1.091885137, Training Accuracy: 67.712\n",
            "Worker 3, [63/64]: Training Loss: 1.109686804, Training Accuracy: 67.856\n",
            "Worker 3, [64/64]: Training Loss: 1.052746637, Training Accuracy: 68.704\n",
            "Time taken for training worker 3: 0:02:53.296418\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/64]: Training Loss: 4.596573752, Training Accuracy: 1.696\n",
            "Worker 4, [02/64]: Training Loss: 4.421231489, Training Accuracy: 3.584\n",
            "Worker 4, [03/64]: Training Loss: 4.170528699, Training Accuracy: 5.200\n",
            "Worker 4, [04/64]: Training Loss: 4.028740632, Training Accuracy: 7.184\n",
            "Worker 4, [05/64]: Training Loss: 3.916959203, Training Accuracy: 8.464\n",
            "Worker 4, [06/64]: Training Loss: 3.820895042, Training Accuracy: 10.512\n",
            "Worker 4, [07/64]: Training Loss: 3.731089217, Training Accuracy: 11.872\n",
            "Worker 4, [08/64]: Training Loss: 3.637749081, Training Accuracy: 13.424\n",
            "Worker 4, [09/64]: Training Loss: 3.580074332, Training Accuracy: 14.592\n",
            "Worker 4, [10/64]: Training Loss: 3.506097937, Training Accuracy: 15.376\n",
            "Worker 4, [11/64]: Training Loss: 3.426259484, Training Accuracy: 16.672\n",
            "Worker 4, [12/64]: Training Loss: 3.335505714, Training Accuracy: 19.056\n",
            "Worker 4, [13/64]: Training Loss: 3.295993598, Training Accuracy: 19.136\n",
            "Worker 4, [14/64]: Training Loss: 3.217307249, Training Accuracy: 20.256\n",
            "Worker 4, [15/64]: Training Loss: 3.162139345, Training Accuracy: 21.056\n",
            "Worker 4, [16/64]: Training Loss: 3.111571876, Training Accuracy: 21.824\n",
            "Worker 4, [17/64]: Training Loss: 3.007477974, Training Accuracy: 24.000\n",
            "Worker 4, [18/64]: Training Loss: 2.930385132, Training Accuracy: 25.392\n",
            "Worker 4, [19/64]: Training Loss: 2.911173811, Training Accuracy: 26.048\n",
            "Worker 4, [20/64]: Training Loss: 2.838558951, Training Accuracy: 27.056\n",
            "Worker 4, [21/64]: Training Loss: 2.792670289, Training Accuracy: 28.080\n",
            "Worker 4, [22/64]: Training Loss: 2.726052396, Training Accuracy: 29.216\n",
            "Worker 4, [23/64]: Training Loss: 2.682563801, Training Accuracy: 30.080\n",
            "Worker 4, [24/64]: Training Loss: 2.617461669, Training Accuracy: 31.664\n",
            "Worker 4, [25/64]: Training Loss: 2.552092300, Training Accuracy: 32.672\n",
            "Worker 4, [26/64]: Training Loss: 2.520029304, Training Accuracy: 33.248\n",
            "Worker 4, [27/64]: Training Loss: 2.456630153, Training Accuracy: 34.240\n",
            "Worker 4, [28/64]: Training Loss: 2.397253674, Training Accuracy: 35.920\n",
            "Worker 4, [29/64]: Training Loss: 2.303095967, Training Accuracy: 38.512\n",
            "Worker 4, [30/64]: Training Loss: 2.271607186, Training Accuracy: 37.888\n",
            "Worker 4, [31/64]: Training Loss: 2.239721388, Training Accuracy: 39.312\n",
            "Worker 4, [32/64]: Training Loss: 2.221721260, Training Accuracy: 39.376\n",
            "Worker 4, [33/64]: Training Loss: 2.108115059, Training Accuracy: 42.128\n",
            "Worker 4, [34/64]: Training Loss: 2.092866799, Training Accuracy: 42.384\n",
            "Worker 4, [35/64]: Training Loss: 2.013445234, Training Accuracy: 44.768\n",
            "Worker 4, [36/64]: Training Loss: 2.009148177, Training Accuracy: 44.800\n",
            "Worker 4, [37/64]: Training Loss: 1.930770970, Training Accuracy: 45.792\n",
            "Worker 4, [38/64]: Training Loss: 1.920847710, Training Accuracy: 46.576\n",
            "Worker 4, [39/64]: Training Loss: 1.829432646, Training Accuracy: 48.096\n",
            "Worker 4, [40/64]: Training Loss: 1.813029502, Training Accuracy: 48.464\n",
            "Worker 4, [41/64]: Training Loss: 1.781553155, Training Accuracy: 49.408\n",
            "Worker 4, [42/64]: Training Loss: 1.699333198, Training Accuracy: 51.392\n",
            "Worker 4, [43/64]: Training Loss: 1.696053395, Training Accuracy: 51.568\n",
            "Worker 4, [44/64]: Training Loss: 1.676076540, Training Accuracy: 52.624\n",
            "Worker 4, [45/64]: Training Loss: 1.603688825, Training Accuracy: 54.272\n",
            "Worker 4, [46/64]: Training Loss: 1.599120590, Training Accuracy: 53.936\n",
            "Worker 4, [47/64]: Training Loss: 1.542513394, Training Accuracy: 56.112\n",
            "Worker 4, [48/64]: Training Loss: 1.491537173, Training Accuracy: 56.864\n",
            "Worker 4, [49/64]: Training Loss: 1.515167002, Training Accuracy: 56.960\n",
            "Worker 4, [50/64]: Training Loss: 1.488930146, Training Accuracy: 57.120\n",
            "Worker 4, [51/64]: Training Loss: 1.444157413, Training Accuracy: 58.032\n",
            "Worker 4, [52/64]: Training Loss: 1.392300225, Training Accuracy: 60.048\n",
            "Worker 4, [53/64]: Training Loss: 1.390068647, Training Accuracy: 59.968\n",
            "Worker 4, [54/64]: Training Loss: 1.330783812, Training Accuracy: 61.568\n",
            "Worker 4, [55/64]: Training Loss: 1.307795265, Training Accuracy: 62.576\n",
            "Worker 4, [56/64]: Training Loss: 1.303444432, Training Accuracy: 62.208\n",
            "Worker 4, [57/64]: Training Loss: 1.202957192, Training Accuracy: 64.928\n",
            "Worker 4, [58/64]: Training Loss: 1.217157581, Training Accuracy: 64.272\n",
            "Worker 4, [59/64]: Training Loss: 1.225166192, Training Accuracy: 64.704\n",
            "Worker 4, [60/64]: Training Loss: 1.208062181, Training Accuracy: 65.008\n",
            "Worker 4, [61/64]: Training Loss: 1.121320981, Training Accuracy: 66.848\n",
            "Worker 4, [62/64]: Training Loss: 1.132541175, Training Accuracy: 66.336\n",
            "Worker 4, [63/64]: Training Loss: 1.074746609, Training Accuracy: 68.400\n",
            "Worker 4, [64/64]: Training Loss: 1.144596690, Training Accuracy: 66.592\n",
            "Time taken for training worker 4: 0:02:52.167138\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/64]: Training Loss: 4.596176722, Training Accuracy: 1.840\n",
            "Worker 5, [02/64]: Training Loss: 4.406198725, Training Accuracy: 3.728\n",
            "Worker 5, [03/64]: Training Loss: 4.174382285, Training Accuracy: 5.104\n",
            "Worker 5, [04/64]: Training Loss: 4.040519877, Training Accuracy: 7.056\n",
            "Worker 5, [05/64]: Training Loss: 3.947234944, Training Accuracy: 8.688\n",
            "Worker 5, [06/64]: Training Loss: 3.849468277, Training Accuracy: 10.112\n",
            "Worker 5, [07/64]: Training Loss: 3.756212857, Training Accuracy: 11.760\n",
            "Worker 5, [08/64]: Training Loss: 3.685736773, Training Accuracy: 12.272\n",
            "Worker 5, [09/64]: Training Loss: 3.600709959, Training Accuracy: 14.256\n",
            "Worker 5, [10/64]: Training Loss: 3.533993826, Training Accuracy: 15.152\n",
            "Worker 5, [11/64]: Training Loss: 3.448233398, Training Accuracy: 16.688\n",
            "Worker 5, [12/64]: Training Loss: 3.360003469, Training Accuracy: 18.192\n",
            "Worker 5, [13/64]: Training Loss: 3.309756208, Training Accuracy: 18.864\n",
            "Worker 5, [14/64]: Training Loss: 3.225891284, Training Accuracy: 19.552\n",
            "Worker 5, [15/64]: Training Loss: 3.194890718, Training Accuracy: 20.544\n",
            "Worker 5, [16/64]: Training Loss: 3.107093755, Training Accuracy: 22.336\n",
            "Worker 5, [17/64]: Training Loss: 3.029233636, Training Accuracy: 23.776\n",
            "Worker 5, [18/64]: Training Loss: 2.958081445, Training Accuracy: 25.024\n",
            "Worker 5, [19/64]: Training Loss: 2.903289542, Training Accuracy: 25.584\n",
            "Worker 5, [20/64]: Training Loss: 2.851907913, Training Accuracy: 26.320\n",
            "Worker 5, [21/64]: Training Loss: 2.775994897, Training Accuracy: 28.512\n",
            "Worker 5, [22/64]: Training Loss: 2.700692719, Training Accuracy: 29.680\n",
            "Worker 5, [23/64]: Training Loss: 2.667856859, Training Accuracy: 30.544\n",
            "Worker 5, [24/64]: Training Loss: 2.602170321, Training Accuracy: 31.136\n",
            "Worker 5, [25/64]: Training Loss: 2.517740011, Training Accuracy: 33.424\n",
            "Worker 5, [26/64]: Training Loss: 2.470400080, Training Accuracy: 34.704\n",
            "Worker 5, [27/64]: Training Loss: 2.413251994, Training Accuracy: 36.464\n",
            "Worker 5, [28/64]: Training Loss: 2.379054014, Training Accuracy: 36.128\n",
            "Worker 5, [29/64]: Training Loss: 2.282991744, Training Accuracy: 38.032\n",
            "Worker 5, [30/64]: Training Loss: 2.257060636, Training Accuracy: 38.656\n",
            "Worker 5, [31/64]: Training Loss: 2.182666155, Training Accuracy: 40.912\n",
            "Worker 5, [32/64]: Training Loss: 2.130134370, Training Accuracy: 42.160\n",
            "Worker 5, [33/64]: Training Loss: 2.103172777, Training Accuracy: 42.288\n",
            "Worker 5, [34/64]: Training Loss: 2.036112456, Training Accuracy: 43.280\n",
            "Worker 5, [35/64]: Training Loss: 1.963166800, Training Accuracy: 45.280\n",
            "Worker 5, [36/64]: Training Loss: 1.950296597, Training Accuracy: 45.744\n",
            "Worker 5, [37/64]: Training Loss: 1.897026736, Training Accuracy: 46.896\n",
            "Worker 5, [38/64]: Training Loss: 1.858253213, Training Accuracy: 47.920\n",
            "Worker 5, [39/64]: Training Loss: 1.826916901, Training Accuracy: 47.552\n",
            "Worker 5, [40/64]: Training Loss: 1.760358909, Training Accuracy: 49.728\n",
            "Worker 5, [41/64]: Training Loss: 1.747039526, Training Accuracy: 50.304\n",
            "Worker 5, [42/64]: Training Loss: 1.665484376, Training Accuracy: 52.528\n",
            "Worker 5, [43/64]: Training Loss: 1.657371017, Training Accuracy: 52.928\n",
            "Worker 5, [44/64]: Training Loss: 1.624731307, Training Accuracy: 54.688\n",
            "Worker 5, [45/64]: Training Loss: 1.559554768, Training Accuracy: 55.456\n",
            "Worker 5, [46/64]: Training Loss: 1.498887713, Training Accuracy: 56.432\n",
            "Worker 5, [47/64]: Training Loss: 1.460615929, Training Accuracy: 57.328\n",
            "Worker 5, [48/64]: Training Loss: 1.437730582, Training Accuracy: 58.688\n",
            "Worker 5, [49/64]: Training Loss: 1.452614987, Training Accuracy: 57.952\n",
            "Worker 5, [50/64]: Training Loss: 1.428469848, Training Accuracy: 58.480\n",
            "Worker 5, [51/64]: Training Loss: 1.389580544, Training Accuracy: 59.344\n",
            "Worker 5, [52/64]: Training Loss: 1.341668385, Training Accuracy: 60.880\n",
            "Worker 5, [53/64]: Training Loss: 1.305237545, Training Accuracy: 61.776\n",
            "Worker 5, [54/64]: Training Loss: 1.337433803, Training Accuracy: 60.512\n",
            "Worker 5, [55/64]: Training Loss: 1.270821469, Training Accuracy: 62.864\n",
            "Worker 5, [56/64]: Training Loss: 1.207088457, Training Accuracy: 64.352\n",
            "Worker 5, [57/64]: Training Loss: 1.205867275, Training Accuracy: 64.960\n",
            "Worker 5, [58/64]: Training Loss: 1.230681790, Training Accuracy: 64.608\n",
            "Worker 5, [59/64]: Training Loss: 1.187088554, Training Accuracy: 65.024\n",
            "Worker 5, [60/64]: Training Loss: 1.163775891, Training Accuracy: 65.248\n",
            "Worker 5, [61/64]: Training Loss: 1.120846542, Training Accuracy: 66.544\n",
            "Worker 5, [62/64]: Training Loss: 1.125810368, Training Accuracy: 66.480\n",
            "Worker 5, [63/64]: Training Loss: 1.064451252, Training Accuracy: 68.224\n",
            "Worker 5, [64/64]: Training Loss: 1.052142011, Training Accuracy: 68.800\n",
            "Time taken for training worker 5: 0:02:54.100580\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/64]: Training Loss: 4.594929768, Training Accuracy: 1.872\n",
            "Worker 6, [02/64]: Training Loss: 4.408096756, Training Accuracy: 3.680\n",
            "Worker 6, [03/64]: Training Loss: 4.173220250, Training Accuracy: 5.616\n",
            "Worker 6, [04/64]: Training Loss: 4.059399831, Training Accuracy: 6.896\n",
            "Worker 6, [05/64]: Training Loss: 3.943172073, Training Accuracy: 8.528\n",
            "Worker 6, [06/64]: Training Loss: 3.846141107, Training Accuracy: 10.464\n",
            "Worker 6, [07/64]: Training Loss: 3.749217423, Training Accuracy: 11.760\n",
            "Worker 6, [08/64]: Training Loss: 3.646185011, Training Accuracy: 13.200\n",
            "Worker 6, [09/64]: Training Loss: 3.585550724, Training Accuracy: 14.512\n",
            "Worker 6, [10/64]: Training Loss: 3.487606574, Training Accuracy: 15.664\n",
            "Worker 6, [11/64]: Training Loss: 3.425273482, Training Accuracy: 16.608\n",
            "Worker 6, [12/64]: Training Loss: 3.348156730, Training Accuracy: 18.416\n",
            "Worker 6, [13/64]: Training Loss: 3.256646266, Training Accuracy: 20.048\n",
            "Worker 6, [14/64]: Training Loss: 3.191590514, Training Accuracy: 21.488\n",
            "Worker 6, [15/64]: Training Loss: 3.110443334, Training Accuracy: 22.384\n",
            "Worker 6, [16/64]: Training Loss: 3.055418803, Training Accuracy: 23.072\n",
            "Worker 6, [17/64]: Training Loss: 2.996447522, Training Accuracy: 23.968\n",
            "Worker 6, [18/64]: Training Loss: 2.967838810, Training Accuracy: 24.912\n",
            "Worker 6, [19/64]: Training Loss: 2.862995384, Training Accuracy: 27.168\n",
            "Worker 6, [20/64]: Training Loss: 2.825355503, Training Accuracy: 27.680\n",
            "Worker 6, [21/64]: Training Loss: 2.743504682, Training Accuracy: 28.848\n",
            "Worker 6, [22/64]: Training Loss: 2.687574815, Training Accuracy: 30.240\n",
            "Worker 6, [23/64]: Training Loss: 2.616618390, Training Accuracy: 31.216\n",
            "Worker 6, [24/64]: Training Loss: 2.577119353, Training Accuracy: 32.496\n",
            "Worker 6, [25/64]: Training Loss: 2.539705707, Training Accuracy: 32.848\n",
            "Worker 6, [26/64]: Training Loss: 2.465663808, Training Accuracy: 34.576\n",
            "Worker 6, [27/64]: Training Loss: 2.386297416, Training Accuracy: 35.856\n",
            "Worker 6, [28/64]: Training Loss: 2.363159457, Training Accuracy: 36.912\n",
            "Worker 6, [29/64]: Training Loss: 2.298301998, Training Accuracy: 38.096\n",
            "Worker 6, [30/64]: Training Loss: 2.238076903, Training Accuracy: 38.976\n",
            "Worker 6, [31/64]: Training Loss: 2.220806481, Training Accuracy: 39.520\n",
            "Worker 6, [32/64]: Training Loss: 2.181204459, Training Accuracy: 40.256\n",
            "Worker 6, [33/64]: Training Loss: 2.105223118, Training Accuracy: 42.096\n",
            "Worker 6, [34/64]: Training Loss: 2.061866321, Training Accuracy: 43.072\n",
            "Worker 6, [35/64]: Training Loss: 2.008243493, Training Accuracy: 43.984\n",
            "Worker 6, [36/64]: Training Loss: 1.961095904, Training Accuracy: 45.200\n",
            "Worker 6, [37/64]: Training Loss: 1.914685546, Training Accuracy: 47.312\n",
            "Worker 6, [38/64]: Training Loss: 1.877651550, Training Accuracy: 47.136\n",
            "Worker 6, [39/64]: Training Loss: 1.813990756, Training Accuracy: 49.104\n",
            "Worker 6, [40/64]: Training Loss: 1.802817631, Training Accuracy: 48.912\n",
            "Worker 6, [41/64]: Training Loss: 1.707943970, Training Accuracy: 51.520\n",
            "Worker 6, [42/64]: Training Loss: 1.725941657, Training Accuracy: 51.520\n",
            "Worker 6, [43/64]: Training Loss: 1.686903842, Training Accuracy: 51.872\n",
            "Worker 6, [44/64]: Training Loss: 1.600307349, Training Accuracy: 54.080\n",
            "Worker 6, [45/64]: Training Loss: 1.626014658, Training Accuracy: 53.456\n",
            "Worker 6, [46/64]: Training Loss: 1.494077160, Training Accuracy: 56.416\n",
            "Worker 6, [47/64]: Training Loss: 1.499278335, Training Accuracy: 56.432\n",
            "Worker 6, [48/64]: Training Loss: 1.480902461, Training Accuracy: 56.848\n",
            "Worker 6, [49/64]: Training Loss: 1.492028618, Training Accuracy: 57.472\n",
            "Worker 6, [50/64]: Training Loss: 1.442227753, Training Accuracy: 58.688\n",
            "Worker 6, [51/64]: Training Loss: 1.386392608, Training Accuracy: 60.128\n",
            "Worker 6, [52/64]: Training Loss: 1.370094564, Training Accuracy: 60.288\n",
            "Worker 6, [53/64]: Training Loss: 1.318983309, Training Accuracy: 61.184\n",
            "Worker 6, [54/64]: Training Loss: 1.337591351, Training Accuracy: 60.912\n",
            "Worker 6, [55/64]: Training Loss: 1.320358587, Training Accuracy: 61.792\n",
            "Worker 6, [56/64]: Training Loss: 1.281954444, Training Accuracy: 63.072\n",
            "Worker 6, [57/64]: Training Loss: 1.235769422, Training Accuracy: 63.936\n",
            "Worker 6, [58/64]: Training Loss: 1.159299596, Training Accuracy: 66.208\n",
            "Worker 6, [59/64]: Training Loss: 1.242895310, Training Accuracy: 63.936\n",
            "Worker 6, [60/64]: Training Loss: 1.173113726, Training Accuracy: 65.632\n",
            "Worker 6, [61/64]: Training Loss: 1.185444979, Training Accuracy: 64.512\n",
            "Worker 6, [62/64]: Training Loss: 1.163426826, Training Accuracy: 65.712\n",
            "Worker 6, [63/64]: Training Loss: 1.154885712, Training Accuracy: 65.984\n",
            "Worker 6, [64/64]: Training Loss: 1.056969502, Training Accuracy: 69.600\n",
            "Time taken for training worker 6: 0:02:45.371916\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/64]: Training Loss: 4.597732077, Training Accuracy: 1.536\n",
            "Worker 7, [02/64]: Training Loss: 4.434155124, Training Accuracy: 3.056\n",
            "Worker 7, [03/64]: Training Loss: 4.207737565, Training Accuracy: 5.680\n",
            "Worker 7, [04/64]: Training Loss: 4.058682359, Training Accuracy: 7.152\n",
            "Worker 7, [05/64]: Training Loss: 3.942294048, Training Accuracy: 8.944\n",
            "Worker 7, [06/64]: Training Loss: 3.855634991, Training Accuracy: 10.000\n",
            "Worker 7, [07/64]: Training Loss: 3.749654784, Training Accuracy: 11.008\n",
            "Worker 7, [08/64]: Training Loss: 3.685536750, Training Accuracy: 12.464\n",
            "Worker 7, [09/64]: Training Loss: 3.583313947, Training Accuracy: 14.592\n",
            "Worker 7, [10/64]: Training Loss: 3.512900649, Training Accuracy: 15.824\n",
            "Worker 7, [11/64]: Training Loss: 3.428013388, Training Accuracy: 17.344\n",
            "Worker 7, [12/64]: Training Loss: 3.369112730, Training Accuracy: 18.144\n",
            "Worker 7, [13/64]: Training Loss: 3.304133177, Training Accuracy: 19.408\n",
            "Worker 7, [14/64]: Training Loss: 3.243634985, Training Accuracy: 20.272\n",
            "Worker 7, [15/64]: Training Loss: 3.156346258, Training Accuracy: 21.376\n",
            "Worker 7, [16/64]: Training Loss: 3.100165031, Training Accuracy: 22.256\n",
            "Worker 7, [17/64]: Training Loss: 3.044174316, Training Accuracy: 23.264\n",
            "Worker 7, [18/64]: Training Loss: 2.970329477, Training Accuracy: 24.688\n",
            "Worker 7, [19/64]: Training Loss: 2.932960223, Training Accuracy: 26.112\n",
            "Worker 7, [20/64]: Training Loss: 2.863767461, Training Accuracy: 26.688\n",
            "Worker 7, [21/64]: Training Loss: 2.792625739, Training Accuracy: 28.320\n",
            "Worker 7, [22/64]: Training Loss: 2.755959598, Training Accuracy: 28.192\n",
            "Worker 7, [23/64]: Training Loss: 2.721153747, Training Accuracy: 29.504\n",
            "Worker 7, [24/64]: Training Loss: 2.604001225, Training Accuracy: 31.248\n",
            "Worker 7, [25/64]: Training Loss: 2.590307596, Training Accuracy: 31.664\n",
            "Worker 7, [26/64]: Training Loss: 2.528688572, Training Accuracy: 33.248\n",
            "Worker 7, [27/64]: Training Loss: 2.441912279, Training Accuracy: 34.528\n",
            "Worker 7, [28/64]: Training Loss: 2.394696430, Training Accuracy: 35.552\n",
            "Worker 7, [29/64]: Training Loss: 2.419327123, Training Accuracy: 35.328\n",
            "Worker 7, [30/64]: Training Loss: 2.316462480, Training Accuracy: 37.472\n",
            "Worker 7, [31/64]: Training Loss: 2.244599353, Training Accuracy: 38.304\n",
            "Worker 7, [32/64]: Training Loss: 2.204181208, Training Accuracy: 39.040\n",
            "Worker 7, [33/64]: Training Loss: 2.142566388, Training Accuracy: 41.264\n",
            "Worker 7, [34/64]: Training Loss: 2.151728365, Training Accuracy: 41.104\n",
            "Worker 7, [35/64]: Training Loss: 2.050735767, Training Accuracy: 42.704\n",
            "Worker 7, [36/64]: Training Loss: 2.015995344, Training Accuracy: 43.488\n",
            "Worker 7, [37/64]: Training Loss: 2.004685761, Training Accuracy: 44.448\n",
            "Worker 7, [38/64]: Training Loss: 1.911616240, Training Accuracy: 46.496\n",
            "Worker 7, [39/64]: Training Loss: 1.917954417, Training Accuracy: 46.128\n",
            "Worker 7, [40/64]: Training Loss: 1.885946201, Training Accuracy: 47.280\n",
            "Worker 7, [41/64]: Training Loss: 1.769925645, Training Accuracy: 49.360\n",
            "Worker 7, [42/64]: Training Loss: 1.776624726, Training Accuracy: 49.440\n",
            "Worker 7, [43/64]: Training Loss: 1.788861837, Training Accuracy: 49.392\n",
            "Worker 7, [44/64]: Training Loss: 1.691746863, Training Accuracy: 51.760\n",
            "Worker 7, [45/64]: Training Loss: 1.619832167, Training Accuracy: 53.168\n",
            "Worker 7, [46/64]: Training Loss: 1.576543061, Training Accuracy: 54.288\n",
            "Worker 7, [47/64]: Training Loss: 1.561226520, Training Accuracy: 55.104\n",
            "Worker 7, [48/64]: Training Loss: 1.569845510, Training Accuracy: 55.072\n",
            "Worker 7, [49/64]: Training Loss: 1.528424279, Training Accuracy: 55.584\n",
            "Worker 7, [50/64]: Training Loss: 1.510647161, Training Accuracy: 56.848\n",
            "Worker 7, [51/64]: Training Loss: 1.448628260, Training Accuracy: 57.952\n",
            "Worker 7, [52/64]: Training Loss: 1.452259683, Training Accuracy: 58.288\n",
            "Worker 7, [53/64]: Training Loss: 1.361230889, Training Accuracy: 60.720\n",
            "Worker 7, [54/64]: Training Loss: 1.307275678, Training Accuracy: 61.616\n",
            "Worker 7, [55/64]: Training Loss: 1.340802458, Training Accuracy: 60.288\n",
            "Worker 7, [56/64]: Training Loss: 1.310738190, Training Accuracy: 62.400\n",
            "Worker 7, [57/64]: Training Loss: 1.256582644, Training Accuracy: 63.088\n",
            "Worker 7, [58/64]: Training Loss: 1.218315759, Training Accuracy: 64.272\n",
            "Worker 7, [59/64]: Training Loss: 1.265790827, Training Accuracy: 63.360\n",
            "Worker 7, [60/64]: Training Loss: 1.263050552, Training Accuracy: 63.088\n",
            "Worker 7, [61/64]: Training Loss: 1.204138164, Training Accuracy: 65.184\n",
            "Worker 7, [62/64]: Training Loss: 1.163845342, Training Accuracy: 65.952\n",
            "Worker 7, [63/64]: Training Loss: 1.171864477, Training Accuracy: 65.536\n",
            "Worker 7, [64/64]: Training Loss: 1.103236428, Training Accuracy: 67.472\n",
            "Time taken for training worker 7: 0:02:54.324248\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/64]: Training Loss: 4.596768715, Training Accuracy: 1.840\n",
            "Worker 8, [02/64]: Training Loss: 4.416414694, Training Accuracy: 3.632\n",
            "Worker 8, [03/64]: Training Loss: 4.177934700, Training Accuracy: 5.504\n",
            "Worker 8, [04/64]: Training Loss: 4.027350686, Training Accuracy: 7.104\n",
            "Worker 8, [05/64]: Training Loss: 3.897524724, Training Accuracy: 8.768\n",
            "Worker 8, [06/64]: Training Loss: 3.817241258, Training Accuracy: 9.792\n",
            "Worker 8, [07/64]: Training Loss: 3.737013515, Training Accuracy: 11.120\n",
            "Worker 8, [08/64]: Training Loss: 3.635449103, Training Accuracy: 13.248\n",
            "Worker 8, [09/64]: Training Loss: 3.580627565, Training Accuracy: 14.512\n",
            "Worker 8, [10/64]: Training Loss: 3.519917729, Training Accuracy: 14.960\n",
            "Worker 8, [11/64]: Training Loss: 3.438483075, Training Accuracy: 15.872\n",
            "Worker 8, [12/64]: Training Loss: 3.349687421, Training Accuracy: 17.520\n",
            "Worker 8, [13/64]: Training Loss: 3.285340548, Training Accuracy: 18.224\n",
            "Worker 8, [14/64]: Training Loss: 3.216022324, Training Accuracy: 19.840\n",
            "Worker 8, [15/64]: Training Loss: 3.138777837, Training Accuracy: 20.640\n",
            "Worker 8, [16/64]: Training Loss: 3.114125887, Training Accuracy: 22.384\n",
            "Worker 8, [17/64]: Training Loss: 3.018982046, Training Accuracy: 22.672\n",
            "Worker 8, [18/64]: Training Loss: 2.975227220, Training Accuracy: 23.952\n",
            "Worker 8, [19/64]: Training Loss: 2.902225219, Training Accuracy: 24.832\n",
            "Worker 8, [20/64]: Training Loss: 2.860193953, Training Accuracy: 25.952\n",
            "Worker 8, [21/64]: Training Loss: 2.815874511, Training Accuracy: 27.040\n",
            "Worker 8, [22/64]: Training Loss: 2.742849503, Training Accuracy: 28.736\n",
            "Worker 8, [23/64]: Training Loss: 2.703397534, Training Accuracy: 28.800\n",
            "Worker 8, [24/64]: Training Loss: 2.633949330, Training Accuracy: 29.936\n",
            "Worker 8, [25/64]: Training Loss: 2.606031817, Training Accuracy: 31.360\n",
            "Worker 8, [26/64]: Training Loss: 2.499567849, Training Accuracy: 33.632\n",
            "Worker 8, [27/64]: Training Loss: 2.506778323, Training Accuracy: 33.072\n",
            "Worker 8, [28/64]: Training Loss: 2.433369838, Training Accuracy: 34.592\n",
            "Worker 8, [29/64]: Training Loss: 2.371125180, Training Accuracy: 35.856\n",
            "Worker 8, [30/64]: Training Loss: 2.329428183, Training Accuracy: 36.192\n",
            "Worker 8, [31/64]: Training Loss: 2.268276328, Training Accuracy: 38.992\n",
            "Worker 8, [32/64]: Training Loss: 2.201745691, Training Accuracy: 40.368\n",
            "Worker 8, [33/64]: Training Loss: 2.157585527, Training Accuracy: 40.336\n",
            "Worker 8, [34/64]: Training Loss: 2.118922549, Training Accuracy: 41.888\n",
            "Worker 8, [35/64]: Training Loss: 2.044266906, Training Accuracy: 42.864\n",
            "Worker 8, [36/64]: Training Loss: 2.044933250, Training Accuracy: 43.472\n",
            "Worker 8, [37/64]: Training Loss: 1.963630646, Training Accuracy: 44.720\n",
            "Worker 8, [38/64]: Training Loss: 1.902240012, Training Accuracy: 47.312\n",
            "Worker 8, [39/64]: Training Loss: 1.900622157, Training Accuracy: 46.768\n",
            "Worker 8, [40/64]: Training Loss: 1.844345688, Training Accuracy: 48.784\n",
            "Worker 8, [41/64]: Training Loss: 1.796979319, Training Accuracy: 49.312\n",
            "Worker 8, [42/64]: Training Loss: 1.737666405, Training Accuracy: 50.560\n",
            "Worker 8, [43/64]: Training Loss: 1.703280544, Training Accuracy: 51.552\n",
            "Worker 8, [44/64]: Training Loss: 1.671711874, Training Accuracy: 52.048\n",
            "Worker 8, [45/64]: Training Loss: 1.613953370, Training Accuracy: 53.808\n",
            "Worker 8, [46/64]: Training Loss: 1.570041150, Training Accuracy: 54.032\n",
            "Worker 8, [47/64]: Training Loss: 1.583492084, Training Accuracy: 54.720\n",
            "Worker 8, [48/64]: Training Loss: 1.510534184, Training Accuracy: 55.456\n",
            "Worker 8, [49/64]: Training Loss: 1.470111051, Training Accuracy: 57.200\n",
            "Worker 8, [50/64]: Training Loss: 1.452581264, Training Accuracy: 57.776\n",
            "Worker 8, [51/64]: Training Loss: 1.407779543, Training Accuracy: 58.880\n",
            "Worker 8, [52/64]: Training Loss: 1.422705132, Training Accuracy: 58.496\n",
            "Worker 8, [53/64]: Training Loss: 1.374397780, Training Accuracy: 59.808\n",
            "Worker 8, [54/64]: Training Loss: 1.377951957, Training Accuracy: 59.200\n",
            "Worker 8, [55/64]: Training Loss: 1.335560380, Training Accuracy: 61.296\n",
            "Worker 8, [56/64]: Training Loss: 1.290594766, Training Accuracy: 61.296\n",
            "Worker 8, [57/64]: Training Loss: 1.258718806, Training Accuracy: 62.880\n",
            "Worker 8, [58/64]: Training Loss: 1.241513629, Training Accuracy: 63.536\n",
            "Worker 8, [59/64]: Training Loss: 1.251393770, Training Accuracy: 63.760\n",
            "Worker 8, [60/64]: Training Loss: 1.220689488, Training Accuracy: 64.096\n",
            "Worker 8, [61/64]: Training Loss: 1.155539342, Training Accuracy: 66.128\n",
            "Worker 8, [62/64]: Training Loss: 1.215698285, Training Accuracy: 63.904\n",
            "Worker 8, [63/64]: Training Loss: 1.141000770, Training Accuracy: 66.192\n",
            "Worker 8, [64/64]: Training Loss: 1.125548410, Training Accuracy: 66.608\n",
            "Time taken for training worker 8: 0:02:50.834456\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003587\n",
            "Global Update 01: Test Loss: 3.843529688, Test Accuracy: 20.260\n",
            "**************************************************\n",
            "Worker 1, [01/64]: Training Loss: 3.291472559, Training Accuracy: 20.432\n",
            "Worker 1, [02/64]: Training Loss: 2.862188157, Training Accuracy: 27.024\n",
            "Worker 1, [03/64]: Training Loss: 2.679319401, Training Accuracy: 30.448\n",
            "Worker 1, [04/64]: Training Loss: 2.526964723, Training Accuracy: 34.304\n",
            "Worker 1, [05/64]: Training Loss: 2.449583000, Training Accuracy: 35.568\n",
            "Worker 1, [06/64]: Training Loss: 2.312183188, Training Accuracy: 37.920\n",
            "Worker 1, [07/64]: Training Loss: 2.209260880, Training Accuracy: 39.904\n",
            "Worker 1, [08/64]: Training Loss: 2.117911732, Training Accuracy: 42.608\n",
            "Worker 1, [09/64]: Training Loss: 2.044282621, Training Accuracy: 43.904\n",
            "Worker 1, [10/64]: Training Loss: 1.985048860, Training Accuracy: 45.696\n",
            "Worker 1, [11/64]: Training Loss: 1.908641532, Training Accuracy: 46.464\n",
            "Worker 1, [12/64]: Training Loss: 1.827685848, Training Accuracy: 48.432\n",
            "Worker 1, [13/64]: Training Loss: 1.776978931, Training Accuracy: 50.048\n",
            "Worker 1, [14/64]: Training Loss: 1.687268718, Training Accuracy: 51.872\n",
            "Worker 1, [15/64]: Training Loss: 1.628343969, Training Accuracy: 53.920\n",
            "Worker 1, [16/64]: Training Loss: 1.573765358, Training Accuracy: 54.688\n",
            "Worker 1, [17/64]: Training Loss: 1.488936280, Training Accuracy: 57.248\n",
            "Worker 1, [18/64]: Training Loss: 1.443953217, Training Accuracy: 58.496\n",
            "Worker 1, [19/64]: Training Loss: 1.399119349, Training Accuracy: 59.488\n",
            "Worker 1, [20/64]: Training Loss: 1.349980189, Training Accuracy: 60.944\n",
            "Worker 1, [21/64]: Training Loss: 1.308097113, Training Accuracy: 61.440\n",
            "Worker 1, [22/64]: Training Loss: 1.259010793, Training Accuracy: 62.720\n",
            "Worker 1, [23/64]: Training Loss: 1.226818708, Training Accuracy: 64.192\n",
            "Worker 1, [24/64]: Training Loss: 1.161947724, Training Accuracy: 65.760\n",
            "Worker 1, [25/64]: Training Loss: 1.126118319, Training Accuracy: 66.272\n",
            "Worker 1, [26/64]: Training Loss: 1.103664975, Training Accuracy: 67.328\n",
            "Worker 1, [27/64]: Training Loss: 1.058557933, Training Accuracy: 68.240\n",
            "Worker 1, [28/64]: Training Loss: 1.004107596, Training Accuracy: 70.240\n",
            "Worker 1, [29/64]: Training Loss: 0.970321285, Training Accuracy: 70.608\n",
            "Worker 1, [30/64]: Training Loss: 0.934798947, Training Accuracy: 72.352\n",
            "Worker 1, [31/64]: Training Loss: 0.923862078, Training Accuracy: 72.528\n",
            "Worker 1, [32/64]: Training Loss: 0.891629590, Training Accuracy: 73.056\n",
            "Worker 1, [33/64]: Training Loss: 0.829982186, Training Accuracy: 75.008\n",
            "Worker 1, [34/64]: Training Loss: 0.837638101, Training Accuracy: 74.688\n",
            "Worker 1, [35/64]: Training Loss: 0.765854213, Training Accuracy: 77.008\n",
            "Worker 1, [36/64]: Training Loss: 0.793463255, Training Accuracy: 75.376\n",
            "Worker 1, [37/64]: Training Loss: 0.750088335, Training Accuracy: 77.488\n",
            "Worker 1, [38/64]: Training Loss: 0.690632134, Training Accuracy: 78.592\n",
            "Worker 1, [39/64]: Training Loss: 0.705634572, Training Accuracy: 78.256\n",
            "Worker 1, [40/64]: Training Loss: 0.697063024, Training Accuracy: 78.880\n",
            "Worker 1, [41/64]: Training Loss: 0.682478779, Training Accuracy: 78.640\n",
            "Worker 1, [42/64]: Training Loss: 0.637176881, Training Accuracy: 80.240\n",
            "Worker 1, [43/64]: Training Loss: 0.625902712, Training Accuracy: 81.088\n",
            "Worker 1, [44/64]: Training Loss: 0.620404193, Training Accuracy: 81.248\n",
            "Worker 1, [45/64]: Training Loss: 0.571764969, Training Accuracy: 83.408\n",
            "Worker 1, [46/64]: Training Loss: 0.570438161, Training Accuracy: 83.264\n",
            "Worker 1, [47/64]: Training Loss: 0.545296272, Training Accuracy: 83.008\n",
            "Worker 1, [48/64]: Training Loss: 0.565036796, Training Accuracy: 82.752\n",
            "Worker 1, [49/64]: Training Loss: 0.527909027, Training Accuracy: 83.712\n",
            "Worker 1, [50/64]: Training Loss: 0.535287034, Training Accuracy: 83.552\n",
            "Worker 1, [51/64]: Training Loss: 0.481737308, Training Accuracy: 85.392\n",
            "Worker 1, [52/64]: Training Loss: 0.491160477, Training Accuracy: 84.304\n",
            "Worker 1, [53/64]: Training Loss: 0.476483837, Training Accuracy: 84.960\n",
            "Worker 1, [54/64]: Training Loss: 0.476331780, Training Accuracy: 85.888\n",
            "Worker 1, [55/64]: Training Loss: 0.451374302, Training Accuracy: 86.416\n",
            "Worker 1, [56/64]: Training Loss: 0.474145611, Training Accuracy: 85.024\n",
            "Worker 1, [57/64]: Training Loss: 0.467868415, Training Accuracy: 85.856\n",
            "Worker 1, [58/64]: Training Loss: 0.419245367, Training Accuracy: 87.472\n",
            "Worker 1, [59/64]: Training Loss: 0.430496469, Training Accuracy: 86.816\n",
            "Worker 1, [60/64]: Training Loss: 0.405181778, Training Accuracy: 87.504\n",
            "Worker 1, [61/64]: Training Loss: 0.434919219, Training Accuracy: 86.400\n",
            "Worker 1, [62/64]: Training Loss: 0.422878831, Training Accuracy: 86.560\n",
            "Worker 1, [63/64]: Training Loss: 0.377812603, Training Accuracy: 88.176\n",
            "Worker 1, [64/64]: Training Loss: 0.375115134, Training Accuracy: 88.208\n",
            "Time taken for training worker 1: 0:02:49.833753\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/64]: Training Loss: 3.287639041, Training Accuracy: 21.568\n",
            "Worker 2, [02/64]: Training Loss: 2.869566219, Training Accuracy: 27.232\n",
            "Worker 2, [03/64]: Training Loss: 2.695122634, Training Accuracy: 31.056\n",
            "Worker 2, [04/64]: Training Loss: 2.541963346, Training Accuracy: 33.920\n",
            "Worker 2, [05/64]: Training Loss: 2.441697492, Training Accuracy: 35.328\n",
            "Worker 2, [06/64]: Training Loss: 2.323385638, Training Accuracy: 39.088\n",
            "Worker 2, [07/64]: Training Loss: 2.229841365, Training Accuracy: 39.984\n",
            "Worker 2, [08/64]: Training Loss: 2.158884770, Training Accuracy: 41.856\n",
            "Worker 2, [09/64]: Training Loss: 2.047581658, Training Accuracy: 44.576\n",
            "Worker 2, [10/64]: Training Loss: 1.992210561, Training Accuracy: 45.312\n",
            "Worker 2, [11/64]: Training Loss: 1.918165594, Training Accuracy: 46.944\n",
            "Worker 2, [12/64]: Training Loss: 1.854908579, Training Accuracy: 47.952\n",
            "Worker 2, [13/64]: Training Loss: 1.787825038, Training Accuracy: 49.776\n",
            "Worker 2, [14/64]: Training Loss: 1.728382244, Training Accuracy: 51.040\n",
            "Worker 2, [15/64]: Training Loss: 1.629398206, Training Accuracy: 53.456\n",
            "Worker 2, [16/64]: Training Loss: 1.554732696, Training Accuracy: 55.552\n",
            "Worker 2, [17/64]: Training Loss: 1.511926782, Training Accuracy: 56.048\n",
            "Worker 2, [18/64]: Training Loss: 1.462558519, Training Accuracy: 57.440\n",
            "Worker 2, [19/64]: Training Loss: 1.403168965, Training Accuracy: 59.280\n",
            "Worker 2, [20/64]: Training Loss: 1.364630708, Training Accuracy: 60.112\n",
            "Worker 2, [21/64]: Training Loss: 1.304675491, Training Accuracy: 61.696\n",
            "Worker 2, [22/64]: Training Loss: 1.234678687, Training Accuracy: 63.248\n",
            "Worker 2, [23/64]: Training Loss: 1.227717659, Training Accuracy: 63.872\n",
            "Worker 2, [24/64]: Training Loss: 1.186449849, Training Accuracy: 64.784\n",
            "Worker 2, [25/64]: Training Loss: 1.108827337, Training Accuracy: 67.280\n",
            "Worker 2, [26/64]: Training Loss: 1.054253788, Training Accuracy: 68.416\n",
            "Worker 2, [27/64]: Training Loss: 1.050353762, Training Accuracy: 67.728\n",
            "Worker 2, [28/64]: Training Loss: 1.017867122, Training Accuracy: 69.632\n",
            "Worker 2, [29/64]: Training Loss: 0.970123716, Training Accuracy: 70.528\n",
            "Worker 2, [30/64]: Training Loss: 0.936942852, Training Accuracy: 71.312\n",
            "Worker 2, [31/64]: Training Loss: 0.909602390, Training Accuracy: 71.968\n",
            "Worker 2, [32/64]: Training Loss: 0.855614950, Training Accuracy: 74.656\n",
            "Worker 2, [33/64]: Training Loss: 0.866150754, Training Accuracy: 74.048\n",
            "Worker 2, [34/64]: Training Loss: 0.842336622, Training Accuracy: 74.304\n",
            "Worker 2, [35/64]: Training Loss: 0.795225863, Training Accuracy: 75.792\n",
            "Worker 2, [36/64]: Training Loss: 0.787991943, Training Accuracy: 76.032\n",
            "Worker 2, [37/64]: Training Loss: 0.748271691, Training Accuracy: 77.296\n",
            "Worker 2, [38/64]: Training Loss: 0.713869939, Training Accuracy: 78.000\n",
            "Worker 2, [39/64]: Training Loss: 0.720230596, Training Accuracy: 78.112\n",
            "Worker 2, [40/64]: Training Loss: 0.676311111, Training Accuracy: 79.168\n",
            "Worker 2, [41/64]: Training Loss: 0.675853941, Training Accuracy: 79.472\n",
            "Worker 2, [42/64]: Training Loss: 0.638042704, Training Accuracy: 79.952\n",
            "Worker 2, [43/64]: Training Loss: 0.614191181, Training Accuracy: 81.104\n",
            "Worker 2, [44/64]: Training Loss: 0.598722306, Training Accuracy: 81.472\n",
            "Worker 2, [45/64]: Training Loss: 0.578116835, Training Accuracy: 82.640\n",
            "Worker 2, [46/64]: Training Loss: 0.589885631, Training Accuracy: 81.776\n",
            "Worker 2, [47/64]: Training Loss: 0.571112170, Training Accuracy: 82.816\n",
            "Worker 2, [48/64]: Training Loss: 0.522068754, Training Accuracy: 83.968\n",
            "Worker 2, [49/64]: Training Loss: 0.505453908, Training Accuracy: 84.720\n",
            "Worker 2, [50/64]: Training Loss: 0.516051775, Training Accuracy: 84.464\n",
            "Worker 2, [51/64]: Training Loss: 0.517824102, Training Accuracy: 84.480\n",
            "Worker 2, [52/64]: Training Loss: 0.485688354, Training Accuracy: 85.248\n",
            "Worker 2, [53/64]: Training Loss: 0.516417537, Training Accuracy: 84.544\n",
            "Worker 2, [54/64]: Training Loss: 0.472287090, Training Accuracy: 85.360\n",
            "Worker 2, [55/64]: Training Loss: 0.450714481, Training Accuracy: 86.288\n",
            "Worker 2, [56/64]: Training Loss: 0.455373137, Training Accuracy: 85.808\n",
            "Worker 2, [57/64]: Training Loss: 0.415824771, Training Accuracy: 87.248\n",
            "Worker 2, [58/64]: Training Loss: 0.430589182, Training Accuracy: 86.928\n",
            "Worker 2, [59/64]: Training Loss: 0.440937620, Training Accuracy: 86.000\n",
            "Worker 2, [60/64]: Training Loss: 0.450268490, Training Accuracy: 86.208\n",
            "Worker 2, [61/64]: Training Loss: 0.421435633, Training Accuracy: 87.520\n",
            "Worker 2, [62/64]: Training Loss: 0.396628400, Training Accuracy: 88.048\n",
            "Worker 2, [63/64]: Training Loss: 0.387111466, Training Accuracy: 88.192\n",
            "Worker 2, [64/64]: Training Loss: 0.361360394, Training Accuracy: 89.024\n",
            "Time taken for training worker 2: 0:02:48.465203\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/64]: Training Loss: 3.305331914, Training Accuracy: 21.008\n",
            "Worker 3, [02/64]: Training Loss: 2.904913097, Training Accuracy: 26.864\n",
            "Worker 3, [03/64]: Training Loss: 2.698574981, Training Accuracy: 30.976\n",
            "Worker 3, [04/64]: Training Loss: 2.556495749, Training Accuracy: 33.760\n",
            "Worker 3, [05/64]: Training Loss: 2.438558656, Training Accuracy: 35.680\n",
            "Worker 3, [06/64]: Training Loss: 2.343551890, Training Accuracy: 36.976\n",
            "Worker 3, [07/64]: Training Loss: 2.254719672, Training Accuracy: 39.040\n",
            "Worker 3, [08/64]: Training Loss: 2.152333621, Training Accuracy: 41.920\n",
            "Worker 3, [09/64]: Training Loss: 2.099702886, Training Accuracy: 42.720\n",
            "Worker 3, [10/64]: Training Loss: 1.998452893, Training Accuracy: 45.136\n",
            "Worker 3, [11/64]: Training Loss: 1.934785300, Training Accuracy: 45.984\n",
            "Worker 3, [12/64]: Training Loss: 1.861228504, Training Accuracy: 47.856\n",
            "Worker 3, [13/64]: Training Loss: 1.784292168, Training Accuracy: 50.080\n",
            "Worker 3, [14/64]: Training Loss: 1.740244752, Training Accuracy: 50.848\n",
            "Worker 3, [15/64]: Training Loss: 1.684760405, Training Accuracy: 52.416\n",
            "Worker 3, [16/64]: Training Loss: 1.589408400, Training Accuracy: 54.416\n",
            "Worker 3, [17/64]: Training Loss: 1.511027717, Training Accuracy: 57.184\n",
            "Worker 3, [18/64]: Training Loss: 1.488229839, Training Accuracy: 56.928\n",
            "Worker 3, [19/64]: Training Loss: 1.437773318, Training Accuracy: 58.176\n",
            "Worker 3, [20/64]: Training Loss: 1.363823003, Training Accuracy: 59.872\n",
            "Worker 3, [21/64]: Training Loss: 1.337236982, Training Accuracy: 61.184\n",
            "Worker 3, [22/64]: Training Loss: 1.290546256, Training Accuracy: 61.968\n",
            "Worker 3, [23/64]: Training Loss: 1.251621299, Training Accuracy: 63.088\n",
            "Worker 3, [24/64]: Training Loss: 1.195510011, Training Accuracy: 65.280\n",
            "Worker 3, [25/64]: Training Loss: 1.108680231, Training Accuracy: 66.960\n",
            "Worker 3, [26/64]: Training Loss: 1.100609093, Training Accuracy: 67.888\n",
            "Worker 3, [27/64]: Training Loss: 1.107161769, Training Accuracy: 67.024\n",
            "Worker 3, [28/64]: Training Loss: 0.991917136, Training Accuracy: 70.688\n",
            "Worker 3, [29/64]: Training Loss: 0.993510279, Training Accuracy: 70.000\n",
            "Worker 3, [30/64]: Training Loss: 0.945120935, Training Accuracy: 71.840\n",
            "Worker 3, [31/64]: Training Loss: 0.912120433, Training Accuracy: 73.136\n",
            "Worker 3, [32/64]: Training Loss: 0.899106311, Training Accuracy: 73.040\n",
            "Worker 3, [33/64]: Training Loss: 0.839702167, Training Accuracy: 74.640\n",
            "Worker 3, [34/64]: Training Loss: 0.823395352, Training Accuracy: 76.048\n",
            "Worker 3, [35/64]: Training Loss: 0.796951448, Training Accuracy: 75.936\n",
            "Worker 3, [36/64]: Training Loss: 0.820326805, Training Accuracy: 75.184\n",
            "Worker 3, [37/64]: Training Loss: 0.766356353, Training Accuracy: 76.768\n",
            "Worker 3, [38/64]: Training Loss: 0.723147513, Training Accuracy: 77.968\n",
            "Worker 3, [39/64]: Training Loss: 0.713334992, Training Accuracy: 78.768\n",
            "Worker 3, [40/64]: Training Loss: 0.700720438, Training Accuracy: 78.560\n",
            "Worker 3, [41/64]: Training Loss: 0.674109888, Training Accuracy: 79.744\n",
            "Worker 3, [42/64]: Training Loss: 0.661835291, Training Accuracy: 79.936\n",
            "Worker 3, [43/64]: Training Loss: 0.661409490, Training Accuracy: 80.224\n",
            "Worker 3, [44/64]: Training Loss: 0.611428978, Training Accuracy: 81.184\n",
            "Worker 3, [45/64]: Training Loss: 0.600995319, Training Accuracy: 81.584\n",
            "Worker 3, [46/64]: Training Loss: 0.594855039, Training Accuracy: 81.712\n",
            "Worker 3, [47/64]: Training Loss: 0.559153371, Training Accuracy: 82.640\n",
            "Worker 3, [48/64]: Training Loss: 0.562846470, Training Accuracy: 82.864\n",
            "Worker 3, [49/64]: Training Loss: 0.547932898, Training Accuracy: 82.768\n",
            "Worker 3, [50/64]: Training Loss: 0.551167768, Training Accuracy: 83.200\n",
            "Worker 3, [51/64]: Training Loss: 0.532947987, Training Accuracy: 83.568\n",
            "Worker 3, [52/64]: Training Loss: 0.519280008, Training Accuracy: 84.176\n",
            "Worker 3, [53/64]: Training Loss: 0.545041501, Training Accuracy: 83.168\n",
            "Worker 3, [54/64]: Training Loss: 0.518412440, Training Accuracy: 84.224\n",
            "Worker 3, [55/64]: Training Loss: 0.462681758, Training Accuracy: 86.304\n",
            "Worker 3, [56/64]: Training Loss: 0.454918305, Training Accuracy: 86.016\n",
            "Worker 3, [57/64]: Training Loss: 0.456145356, Training Accuracy: 85.856\n",
            "Worker 3, [58/64]: Training Loss: 0.406542585, Training Accuracy: 87.184\n",
            "Worker 3, [59/64]: Training Loss: 0.435234779, Training Accuracy: 86.896\n",
            "Worker 3, [60/64]: Training Loss: 0.427533809, Training Accuracy: 87.168\n",
            "Worker 3, [61/64]: Training Loss: 0.426448314, Training Accuracy: 86.928\n",
            "Worker 3, [62/64]: Training Loss: 0.409402453, Training Accuracy: 87.792\n",
            "Worker 3, [63/64]: Training Loss: 0.399047096, Training Accuracy: 87.664\n",
            "Worker 3, [64/64]: Training Loss: 0.428443978, Training Accuracy: 86.880\n",
            "Time taken for training worker 3: 0:02:47.794441\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/64]: Training Loss: 3.322563016, Training Accuracy: 20.816\n",
            "Worker 4, [02/64]: Training Loss: 2.876712259, Training Accuracy: 27.504\n",
            "Worker 4, [03/64]: Training Loss: 2.687665409, Training Accuracy: 30.800\n",
            "Worker 4, [04/64]: Training Loss: 2.566561497, Training Accuracy: 33.248\n",
            "Worker 4, [05/64]: Training Loss: 2.418439726, Training Accuracy: 35.840\n",
            "Worker 4, [06/64]: Training Loss: 2.330671281, Training Accuracy: 37.120\n",
            "Worker 4, [07/64]: Training Loss: 2.250694420, Training Accuracy: 39.552\n",
            "Worker 4, [08/64]: Training Loss: 2.128137001, Training Accuracy: 42.544\n",
            "Worker 4, [09/64]: Training Loss: 2.046317714, Training Accuracy: 44.320\n",
            "Worker 4, [10/64]: Training Loss: 1.995775153, Training Accuracy: 44.464\n",
            "Worker 4, [11/64]: Training Loss: 1.906989565, Training Accuracy: 47.056\n",
            "Worker 4, [12/64]: Training Loss: 1.852488159, Training Accuracy: 48.704\n",
            "Worker 4, [13/64]: Training Loss: 1.789996581, Training Accuracy: 49.792\n",
            "Worker 4, [14/64]: Training Loss: 1.709126289, Training Accuracy: 51.216\n",
            "Worker 4, [15/64]: Training Loss: 1.639154148, Training Accuracy: 54.064\n",
            "Worker 4, [16/64]: Training Loss: 1.580645824, Training Accuracy: 55.056\n",
            "Worker 4, [17/64]: Training Loss: 1.515244446, Training Accuracy: 56.336\n",
            "Worker 4, [18/64]: Training Loss: 1.495312930, Training Accuracy: 56.752\n",
            "Worker 4, [19/64]: Training Loss: 1.404425416, Training Accuracy: 59.632\n",
            "Worker 4, [20/64]: Training Loss: 1.345537312, Training Accuracy: 61.680\n",
            "Worker 4, [21/64]: Training Loss: 1.332460079, Training Accuracy: 60.688\n",
            "Worker 4, [22/64]: Training Loss: 1.284237525, Training Accuracy: 62.944\n",
            "Worker 4, [23/64]: Training Loss: 1.200787892, Training Accuracy: 64.720\n",
            "Worker 4, [24/64]: Training Loss: 1.216571386, Training Accuracy: 64.000\n",
            "Worker 4, [25/64]: Training Loss: 1.120969797, Training Accuracy: 66.416\n",
            "Worker 4, [26/64]: Training Loss: 1.105508210, Training Accuracy: 67.312\n",
            "Worker 4, [27/64]: Training Loss: 1.037910432, Training Accuracy: 69.616\n",
            "Worker 4, [28/64]: Training Loss: 1.009847308, Training Accuracy: 69.888\n",
            "Worker 4, [29/64]: Training Loss: 1.013944429, Training Accuracy: 70.016\n",
            "Worker 4, [30/64]: Training Loss: 0.956565036, Training Accuracy: 71.392\n",
            "Worker 4, [31/64]: Training Loss: 0.938520366, Training Accuracy: 72.496\n",
            "Worker 4, [32/64]: Training Loss: 0.864287241, Training Accuracy: 73.712\n",
            "Worker 4, [33/64]: Training Loss: 0.847941897, Training Accuracy: 74.288\n",
            "Worker 4, [34/64]: Training Loss: 0.872618929, Training Accuracy: 73.680\n",
            "Worker 4, [35/64]: Training Loss: 0.815620466, Training Accuracy: 75.728\n",
            "Worker 4, [36/64]: Training Loss: 0.755629753, Training Accuracy: 77.104\n",
            "Worker 4, [37/64]: Training Loss: 0.764124248, Training Accuracy: 77.152\n",
            "Worker 4, [38/64]: Training Loss: 0.724755682, Training Accuracy: 77.936\n",
            "Worker 4, [39/64]: Training Loss: 0.732049772, Training Accuracy: 77.376\n",
            "Worker 4, [40/64]: Training Loss: 0.746376379, Training Accuracy: 77.408\n",
            "Worker 4, [41/64]: Training Loss: 0.670966291, Training Accuracy: 79.712\n",
            "Worker 4, [42/64]: Training Loss: 0.648454915, Training Accuracy: 79.744\n",
            "Worker 4, [43/64]: Training Loss: 0.647075695, Training Accuracy: 80.336\n",
            "Worker 4, [44/64]: Training Loss: 0.629490487, Training Accuracy: 81.520\n",
            "Worker 4, [45/64]: Training Loss: 0.598799257, Training Accuracy: 81.712\n",
            "Worker 4, [46/64]: Training Loss: 0.554121020, Training Accuracy: 83.168\n",
            "Worker 4, [47/64]: Training Loss: 0.550299900, Training Accuracy: 83.152\n",
            "Worker 4, [48/64]: Training Loss: 0.542027678, Training Accuracy: 83.472\n",
            "Worker 4, [49/64]: Training Loss: 0.561775773, Training Accuracy: 82.784\n",
            "Worker 4, [50/64]: Training Loss: 0.518989915, Training Accuracy: 84.384\n",
            "Worker 4, [51/64]: Training Loss: 0.548101742, Training Accuracy: 83.344\n",
            "Worker 4, [52/64]: Training Loss: 0.510073754, Training Accuracy: 84.416\n",
            "Worker 4, [53/64]: Training Loss: 0.504305493, Training Accuracy: 84.352\n",
            "Worker 4, [54/64]: Training Loss: 0.481658438, Training Accuracy: 85.088\n",
            "Worker 4, [55/64]: Training Loss: 0.457333417, Training Accuracy: 85.680\n",
            "Worker 4, [56/64]: Training Loss: 0.485739632, Training Accuracy: 84.976\n",
            "Worker 4, [57/64]: Training Loss: 0.450905908, Training Accuracy: 86.144\n",
            "Worker 4, [58/64]: Training Loss: 0.492033755, Training Accuracy: 85.600\n",
            "Worker 4, [59/64]: Training Loss: 0.421878055, Training Accuracy: 87.520\n",
            "Worker 4, [60/64]: Training Loss: 0.423471842, Training Accuracy: 87.264\n",
            "Worker 4, [61/64]: Training Loss: 0.399901954, Training Accuracy: 87.776\n",
            "Worker 4, [62/64]: Training Loss: 0.437854660, Training Accuracy: 86.672\n",
            "Worker 4, [63/64]: Training Loss: 0.399034354, Training Accuracy: 87.712\n",
            "Worker 4, [64/64]: Training Loss: 0.403535779, Training Accuracy: 87.712\n",
            "Time taken for training worker 4: 0:02:52.869157\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/64]: Training Loss: 3.301066540, Training Accuracy: 21.168\n",
            "Worker 5, [02/64]: Training Loss: 2.884801502, Training Accuracy: 26.512\n",
            "Worker 5, [03/64]: Training Loss: 2.682845877, Training Accuracy: 31.296\n",
            "Worker 5, [04/64]: Training Loss: 2.549169246, Training Accuracy: 33.440\n",
            "Worker 5, [05/64]: Training Loss: 2.423410384, Training Accuracy: 36.480\n",
            "Worker 5, [06/64]: Training Loss: 2.334915403, Training Accuracy: 37.920\n",
            "Worker 5, [07/64]: Training Loss: 2.206648440, Training Accuracy: 40.144\n",
            "Worker 5, [08/64]: Training Loss: 2.133531736, Training Accuracy: 42.352\n",
            "Worker 5, [09/64]: Training Loss: 2.065194698, Training Accuracy: 44.496\n",
            "Worker 5, [10/64]: Training Loss: 1.958042901, Training Accuracy: 45.728\n",
            "Worker 5, [11/64]: Training Loss: 1.897398263, Training Accuracy: 47.520\n",
            "Worker 5, [12/64]: Training Loss: 1.810780976, Training Accuracy: 49.936\n",
            "Worker 5, [13/64]: Training Loss: 1.771209527, Training Accuracy: 49.440\n",
            "Worker 5, [14/64]: Training Loss: 1.662853831, Training Accuracy: 52.592\n",
            "Worker 5, [15/64]: Training Loss: 1.592578118, Training Accuracy: 54.272\n",
            "Worker 5, [16/64]: Training Loss: 1.564001434, Training Accuracy: 55.456\n",
            "Worker 5, [17/64]: Training Loss: 1.498377531, Training Accuracy: 57.312\n",
            "Worker 5, [18/64]: Training Loss: 1.428464922, Training Accuracy: 59.056\n",
            "Worker 5, [19/64]: Training Loss: 1.414971109, Training Accuracy: 58.800\n",
            "Worker 5, [20/64]: Training Loss: 1.309529706, Training Accuracy: 61.872\n",
            "Worker 5, [21/64]: Training Loss: 1.279296793, Training Accuracy: 62.464\n",
            "Worker 5, [22/64]: Training Loss: 1.248988313, Training Accuracy: 62.624\n",
            "Worker 5, [23/64]: Training Loss: 1.208136183, Training Accuracy: 65.168\n",
            "Worker 5, [24/64]: Training Loss: 1.171292501, Training Accuracy: 65.872\n",
            "Worker 5, [25/64]: Training Loss: 1.112813338, Training Accuracy: 66.704\n",
            "Worker 5, [26/64]: Training Loss: 1.046157242, Training Accuracy: 68.528\n",
            "Worker 5, [27/64]: Training Loss: 1.039176855, Training Accuracy: 68.848\n",
            "Worker 5, [28/64]: Training Loss: 1.014734040, Training Accuracy: 69.632\n",
            "Worker 5, [29/64]: Training Loss: 0.961274032, Training Accuracy: 71.680\n",
            "Worker 5, [30/64]: Training Loss: 0.912167731, Training Accuracy: 72.496\n",
            "Worker 5, [31/64]: Training Loss: 0.892289525, Training Accuracy: 72.928\n",
            "Worker 5, [32/64]: Training Loss: 0.879505128, Training Accuracy: 73.680\n",
            "Worker 5, [33/64]: Training Loss: 0.830867266, Training Accuracy: 74.752\n",
            "Worker 5, [34/64]: Training Loss: 0.775928803, Training Accuracy: 76.496\n",
            "Worker 5, [35/64]: Training Loss: 0.836921257, Training Accuracy: 74.320\n",
            "Worker 5, [36/64]: Training Loss: 0.767794441, Training Accuracy: 76.864\n",
            "Worker 5, [37/64]: Training Loss: 0.736710739, Training Accuracy: 77.680\n",
            "Worker 5, [38/64]: Training Loss: 0.743017030, Training Accuracy: 76.944\n",
            "Worker 5, [39/64]: Training Loss: 0.712190691, Training Accuracy: 78.240\n",
            "Worker 5, [40/64]: Training Loss: 0.699884119, Training Accuracy: 78.912\n",
            "Worker 5, [41/64]: Training Loss: 0.660018837, Training Accuracy: 79.616\n",
            "Worker 5, [42/64]: Training Loss: 0.653106310, Training Accuracy: 80.224\n",
            "Worker 5, [43/64]: Training Loss: 0.630418773, Training Accuracy: 80.224\n",
            "Worker 5, [44/64]: Training Loss: 0.578167884, Training Accuracy: 82.800\n",
            "Worker 5, [45/64]: Training Loss: 0.578235408, Training Accuracy: 82.528\n",
            "Worker 5, [46/64]: Training Loss: 0.557692001, Training Accuracy: 83.104\n",
            "Worker 5, [47/64]: Training Loss: 0.573127078, Training Accuracy: 82.464\n",
            "Worker 5, [48/64]: Training Loss: 0.551181160, Training Accuracy: 83.344\n",
            "Worker 5, [49/64]: Training Loss: 0.518128120, Training Accuracy: 84.336\n",
            "Worker 5, [50/64]: Training Loss: 0.548013793, Training Accuracy: 83.200\n",
            "Worker 5, [51/64]: Training Loss: 0.490971303, Training Accuracy: 85.584\n",
            "Worker 5, [52/64]: Training Loss: 0.472338196, Training Accuracy: 85.472\n",
            "Worker 5, [53/64]: Training Loss: 0.488359654, Training Accuracy: 85.056\n",
            "Worker 5, [54/64]: Training Loss: 0.495543912, Training Accuracy: 84.752\n",
            "Worker 5, [55/64]: Training Loss: 0.460523194, Training Accuracy: 85.632\n",
            "Worker 5, [56/64]: Training Loss: 0.422790540, Training Accuracy: 87.104\n",
            "Worker 5, [57/64]: Training Loss: 0.469072194, Training Accuracy: 85.264\n",
            "Worker 5, [58/64]: Training Loss: 0.452078969, Training Accuracy: 86.336\n",
            "Worker 5, [59/64]: Training Loss: 0.454126416, Training Accuracy: 85.728\n",
            "Worker 5, [60/64]: Training Loss: 0.397818645, Training Accuracy: 87.680\n",
            "Worker 5, [61/64]: Training Loss: 0.422849412, Training Accuracy: 87.248\n",
            "Worker 5, [62/64]: Training Loss: 0.395611776, Training Accuracy: 87.840\n",
            "Worker 5, [63/64]: Training Loss: 0.415253789, Training Accuracy: 87.168\n",
            "Worker 5, [64/64]: Training Loss: 0.379438934, Training Accuracy: 88.336\n",
            "Time taken for training worker 5: 0:02:49.162532\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/64]: Training Loss: 3.309538895, Training Accuracy: 21.312\n",
            "Worker 6, [02/64]: Training Loss: 2.889206042, Training Accuracy: 27.776\n",
            "Worker 6, [03/64]: Training Loss: 2.681354265, Training Accuracy: 31.264\n",
            "Worker 6, [04/64]: Training Loss: 2.531926879, Training Accuracy: 33.984\n",
            "Worker 6, [05/64]: Training Loss: 2.413758263, Training Accuracy: 36.208\n",
            "Worker 6, [06/64]: Training Loss: 2.316631916, Training Accuracy: 38.256\n",
            "Worker 6, [07/64]: Training Loss: 2.228215051, Training Accuracy: 40.416\n",
            "Worker 6, [08/64]: Training Loss: 2.132223686, Training Accuracy: 42.432\n",
            "Worker 6, [09/64]: Training Loss: 2.072298146, Training Accuracy: 44.048\n",
            "Worker 6, [10/64]: Training Loss: 1.980660619, Training Accuracy: 46.016\n",
            "Worker 6, [11/64]: Training Loss: 1.895499959, Training Accuracy: 47.824\n",
            "Worker 6, [12/64]: Training Loss: 1.843298932, Training Accuracy: 49.008\n",
            "Worker 6, [13/64]: Training Loss: 1.780016101, Training Accuracy: 50.320\n",
            "Worker 6, [14/64]: Training Loss: 1.698631159, Training Accuracy: 51.680\n",
            "Worker 6, [15/64]: Training Loss: 1.669000860, Training Accuracy: 51.664\n",
            "Worker 6, [16/64]: Training Loss: 1.582441959, Training Accuracy: 54.992\n",
            "Worker 6, [17/64]: Training Loss: 1.511154705, Training Accuracy: 56.400\n",
            "Worker 6, [18/64]: Training Loss: 1.461612493, Training Accuracy: 58.048\n",
            "Worker 6, [19/64]: Training Loss: 1.403980244, Training Accuracy: 59.600\n",
            "Worker 6, [20/64]: Training Loss: 1.382388614, Training Accuracy: 59.488\n",
            "Worker 6, [21/64]: Training Loss: 1.318889426, Training Accuracy: 61.536\n",
            "Worker 6, [22/64]: Training Loss: 1.233258997, Training Accuracy: 64.256\n",
            "Worker 6, [23/64]: Training Loss: 1.237822003, Training Accuracy: 63.888\n",
            "Worker 6, [24/64]: Training Loss: 1.162012727, Training Accuracy: 65.824\n",
            "Worker 6, [25/64]: Training Loss: 1.145192496, Training Accuracy: 65.520\n",
            "Worker 6, [26/64]: Training Loss: 1.100996034, Training Accuracy: 67.680\n",
            "Worker 6, [27/64]: Training Loss: 1.047107376, Training Accuracy: 68.896\n",
            "Worker 6, [28/64]: Training Loss: 1.013077982, Training Accuracy: 69.424\n",
            "Worker 6, [29/64]: Training Loss: 0.995865202, Training Accuracy: 70.432\n",
            "Worker 6, [30/64]: Training Loss: 0.944272737, Training Accuracy: 71.872\n",
            "Worker 6, [31/64]: Training Loss: 0.922900986, Training Accuracy: 71.968\n",
            "Worker 6, [32/64]: Training Loss: 0.872109888, Training Accuracy: 73.680\n",
            "Worker 6, [33/64]: Training Loss: 0.856704130, Training Accuracy: 74.464\n",
            "Worker 6, [34/64]: Training Loss: 0.848185321, Training Accuracy: 74.640\n",
            "Worker 6, [35/64]: Training Loss: 0.799852116, Training Accuracy: 76.016\n",
            "Worker 6, [36/64]: Training Loss: 0.799745272, Training Accuracy: 75.584\n",
            "Worker 6, [37/64]: Training Loss: 0.746948579, Training Accuracy: 77.120\n",
            "Worker 6, [38/64]: Training Loss: 0.714335738, Training Accuracy: 77.776\n",
            "Worker 6, [39/64]: Training Loss: 0.692629299, Training Accuracy: 78.528\n",
            "Worker 6, [40/64]: Training Loss: 0.669499645, Training Accuracy: 79.904\n",
            "Worker 6, [41/64]: Training Loss: 0.674415888, Training Accuracy: 79.408\n",
            "Worker 6, [42/64]: Training Loss: 0.634108184, Training Accuracy: 80.736\n",
            "Worker 6, [43/64]: Training Loss: 0.646697013, Training Accuracy: 80.016\n",
            "Worker 6, [44/64]: Training Loss: 0.649841094, Training Accuracy: 80.272\n",
            "Worker 6, [45/64]: Training Loss: 0.615044121, Training Accuracy: 80.320\n",
            "Worker 6, [46/64]: Training Loss: 0.608976667, Training Accuracy: 81.696\n",
            "Worker 6, [47/64]: Training Loss: 0.597450820, Training Accuracy: 81.632\n",
            "Worker 6, [48/64]: Training Loss: 0.550620149, Training Accuracy: 83.104\n",
            "Worker 6, [49/64]: Training Loss: 0.537162860, Training Accuracy: 83.312\n",
            "Worker 6, [50/64]: Training Loss: 0.518548572, Training Accuracy: 83.600\n",
            "Worker 6, [51/64]: Training Loss: 0.481830568, Training Accuracy: 85.168\n",
            "Worker 6, [52/64]: Training Loss: 0.470036411, Training Accuracy: 85.392\n",
            "Worker 6, [53/64]: Training Loss: 0.496612153, Training Accuracy: 84.912\n",
            "Worker 6, [54/64]: Training Loss: 0.471319650, Training Accuracy: 85.792\n",
            "Worker 6, [55/64]: Training Loss: 0.466563419, Training Accuracy: 85.936\n",
            "Worker 6, [56/64]: Training Loss: 0.465359084, Training Accuracy: 85.552\n",
            "Worker 6, [57/64]: Training Loss: 0.421898851, Training Accuracy: 87.344\n",
            "Worker 6, [58/64]: Training Loss: 0.438708477, Training Accuracy: 86.384\n",
            "Worker 6, [59/64]: Training Loss: 0.436139718, Training Accuracy: 86.192\n",
            "Worker 6, [60/64]: Training Loss: 0.387826273, Training Accuracy: 87.824\n",
            "Worker 6, [61/64]: Training Loss: 0.386457234, Training Accuracy: 87.888\n",
            "Worker 6, [62/64]: Training Loss: 0.385182418, Training Accuracy: 87.856\n",
            "Worker 6, [63/64]: Training Loss: 0.374098385, Training Accuracy: 88.448\n",
            "Worker 6, [64/64]: Training Loss: 0.390992463, Training Accuracy: 88.528\n",
            "Time taken for training worker 6: 0:02:50.841259\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/64]: Training Loss: 3.326540606, Training Accuracy: 19.952\n",
            "Worker 7, [02/64]: Training Loss: 2.889370624, Training Accuracy: 26.944\n",
            "Worker 7, [03/64]: Training Loss: 2.693816336, Training Accuracy: 30.848\n",
            "Worker 7, [04/64]: Training Loss: 2.553651725, Training Accuracy: 33.632\n",
            "Worker 7, [05/64]: Training Loss: 2.443654194, Training Accuracy: 35.408\n",
            "Worker 7, [06/64]: Training Loss: 2.330765236, Training Accuracy: 37.376\n",
            "Worker 7, [07/64]: Training Loss: 2.245171656, Training Accuracy: 39.072\n",
            "Worker 7, [08/64]: Training Loss: 2.144883620, Training Accuracy: 41.296\n",
            "Worker 7, [09/64]: Training Loss: 2.071585823, Training Accuracy: 43.376\n",
            "Worker 7, [10/64]: Training Loss: 1.995265781, Training Accuracy: 45.072\n",
            "Worker 7, [11/64]: Training Loss: 1.929293125, Training Accuracy: 45.664\n",
            "Worker 7, [12/64]: Training Loss: 1.830371213, Training Accuracy: 48.432\n",
            "Worker 7, [13/64]: Training Loss: 1.789230698, Training Accuracy: 50.192\n",
            "Worker 7, [14/64]: Training Loss: 1.720427573, Training Accuracy: 51.472\n",
            "Worker 7, [15/64]: Training Loss: 1.664728173, Training Accuracy: 53.072\n",
            "Worker 7, [16/64]: Training Loss: 1.607017530, Training Accuracy: 54.144\n",
            "Worker 7, [17/64]: Training Loss: 1.537930116, Training Accuracy: 55.136\n",
            "Worker 7, [18/64]: Training Loss: 1.499771146, Training Accuracy: 56.992\n",
            "Worker 7, [19/64]: Training Loss: 1.418169054, Training Accuracy: 58.720\n",
            "Worker 7, [20/64]: Training Loss: 1.402258191, Training Accuracy: 58.608\n",
            "Worker 7, [21/64]: Training Loss: 1.312855513, Training Accuracy: 61.856\n",
            "Worker 7, [22/64]: Training Loss: 1.270590698, Training Accuracy: 62.784\n",
            "Worker 7, [23/64]: Training Loss: 1.219803922, Training Accuracy: 63.120\n",
            "Worker 7, [24/64]: Training Loss: 1.197280692, Training Accuracy: 64.768\n",
            "Worker 7, [25/64]: Training Loss: 1.155723477, Training Accuracy: 66.240\n",
            "Worker 7, [26/64]: Training Loss: 1.111022387, Training Accuracy: 66.960\n",
            "Worker 7, [27/64]: Training Loss: 1.056885845, Training Accuracy: 68.464\n",
            "Worker 7, [28/64]: Training Loss: 1.038311069, Training Accuracy: 68.480\n",
            "Worker 7, [29/64]: Training Loss: 1.007480492, Training Accuracy: 70.272\n",
            "Worker 7, [30/64]: Training Loss: 0.953931493, Training Accuracy: 71.728\n",
            "Worker 7, [31/64]: Training Loss: 0.946125748, Training Accuracy: 72.224\n",
            "Worker 7, [32/64]: Training Loss: 0.908942521, Training Accuracy: 72.704\n",
            "Worker 7, [33/64]: Training Loss: 0.879419879, Training Accuracy: 73.264\n",
            "Worker 7, [34/64]: Training Loss: 0.851948498, Training Accuracy: 74.464\n",
            "Worker 7, [35/64]: Training Loss: 0.789197658, Training Accuracy: 76.288\n",
            "Worker 7, [36/64]: Training Loss: 0.801735963, Training Accuracy: 76.080\n",
            "Worker 7, [37/64]: Training Loss: 0.794063357, Training Accuracy: 75.680\n",
            "Worker 7, [38/64]: Training Loss: 0.719451731, Training Accuracy: 78.512\n",
            "Worker 7, [39/64]: Training Loss: 0.704151679, Training Accuracy: 78.704\n",
            "Worker 7, [40/64]: Training Loss: 0.694768619, Training Accuracy: 78.864\n",
            "Worker 7, [41/64]: Training Loss: 0.660857195, Training Accuracy: 80.032\n",
            "Worker 7, [42/64]: Training Loss: 0.653799716, Training Accuracy: 79.888\n",
            "Worker 7, [43/64]: Training Loss: 0.667122745, Training Accuracy: 80.064\n",
            "Worker 7, [44/64]: Training Loss: 0.619031351, Training Accuracy: 81.408\n",
            "Worker 7, [45/64]: Training Loss: 0.595465389, Training Accuracy: 81.760\n",
            "Worker 7, [46/64]: Training Loss: 0.591007898, Training Accuracy: 81.936\n",
            "Worker 7, [47/64]: Training Loss: 0.573535421, Training Accuracy: 81.984\n",
            "Worker 7, [48/64]: Training Loss: 0.552891746, Training Accuracy: 82.816\n",
            "Worker 7, [49/64]: Training Loss: 0.563740008, Training Accuracy: 82.704\n",
            "Worker 7, [50/64]: Training Loss: 0.541473908, Training Accuracy: 83.792\n",
            "Worker 7, [51/64]: Training Loss: 0.578896542, Training Accuracy: 82.352\n",
            "Worker 7, [52/64]: Training Loss: 0.536507529, Training Accuracy: 83.440\n",
            "Worker 7, [53/64]: Training Loss: 0.506699866, Training Accuracy: 84.544\n",
            "Worker 7, [54/64]: Training Loss: 0.459624256, Training Accuracy: 86.016\n",
            "Worker 7, [55/64]: Training Loss: 0.468595743, Training Accuracy: 85.856\n",
            "Worker 7, [56/64]: Training Loss: 0.476905775, Training Accuracy: 85.328\n",
            "Worker 7, [57/64]: Training Loss: 0.455183243, Training Accuracy: 86.112\n",
            "Worker 7, [58/64]: Training Loss: 0.479201001, Training Accuracy: 85.472\n",
            "Worker 7, [59/64]: Training Loss: 0.410962867, Training Accuracy: 86.976\n",
            "Worker 7, [60/64]: Training Loss: 0.409264364, Training Accuracy: 87.568\n",
            "Worker 7, [61/64]: Training Loss: 0.410939920, Training Accuracy: 87.216\n",
            "Worker 7, [62/64]: Training Loss: 0.471213346, Training Accuracy: 85.792\n",
            "Worker 7, [63/64]: Training Loss: 0.387506180, Training Accuracy: 87.584\n",
            "Worker 7, [64/64]: Training Loss: 0.413037990, Training Accuracy: 88.032\n",
            "Time taken for training worker 7: 0:02:49.770302\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/64]: Training Loss: 3.315958991, Training Accuracy: 21.232\n",
            "Worker 8, [02/64]: Training Loss: 2.890946191, Training Accuracy: 27.312\n",
            "Worker 8, [03/64]: Training Loss: 2.701764934, Training Accuracy: 30.064\n",
            "Worker 8, [04/64]: Training Loss: 2.577213618, Training Accuracy: 32.704\n",
            "Worker 8, [05/64]: Training Loss: 2.448674726, Training Accuracy: 35.200\n",
            "Worker 8, [06/64]: Training Loss: 2.349282081, Training Accuracy: 37.584\n",
            "Worker 8, [07/64]: Training Loss: 2.247491463, Training Accuracy: 38.784\n",
            "Worker 8, [08/64]: Training Loss: 2.176251868, Training Accuracy: 41.424\n",
            "Worker 8, [09/64]: Training Loss: 2.108570819, Training Accuracy: 42.464\n",
            "Worker 8, [10/64]: Training Loss: 1.985715024, Training Accuracy: 45.168\n",
            "Worker 8, [11/64]: Training Loss: 1.933365458, Training Accuracy: 46.448\n",
            "Worker 8, [12/64]: Training Loss: 1.880745670, Training Accuracy: 47.664\n",
            "Worker 8, [13/64]: Training Loss: 1.791252532, Training Accuracy: 49.776\n",
            "Worker 8, [14/64]: Training Loss: 1.728723896, Training Accuracy: 51.008\n",
            "Worker 8, [15/64]: Training Loss: 1.675369465, Training Accuracy: 52.960\n",
            "Worker 8, [16/64]: Training Loss: 1.619805975, Training Accuracy: 53.776\n",
            "Worker 8, [17/64]: Training Loss: 1.562695837, Training Accuracy: 55.008\n",
            "Worker 8, [18/64]: Training Loss: 1.484870236, Training Accuracy: 57.152\n",
            "Worker 8, [19/64]: Training Loss: 1.413725885, Training Accuracy: 58.000\n",
            "Worker 8, [20/64]: Training Loss: 1.380755001, Training Accuracy: 59.376\n",
            "Worker 8, [21/64]: Training Loss: 1.341843628, Training Accuracy: 60.400\n",
            "Worker 8, [22/64]: Training Loss: 1.266277761, Training Accuracy: 62.576\n",
            "Worker 8, [23/64]: Training Loss: 1.228601263, Training Accuracy: 63.968\n",
            "Worker 8, [24/64]: Training Loss: 1.190334540, Training Accuracy: 64.720\n",
            "Worker 8, [25/64]: Training Loss: 1.127380712, Training Accuracy: 66.464\n",
            "Worker 8, [26/64]: Training Loss: 1.122724719, Training Accuracy: 66.336\n",
            "Worker 8, [27/64]: Training Loss: 1.074407005, Training Accuracy: 67.888\n",
            "Worker 8, [28/64]: Training Loss: 1.042926454, Training Accuracy: 69.328\n",
            "Worker 8, [29/64]: Training Loss: 0.991138137, Training Accuracy: 70.064\n",
            "Worker 8, [30/64]: Training Loss: 0.981151162, Training Accuracy: 70.928\n",
            "Worker 8, [31/64]: Training Loss: 0.963444640, Training Accuracy: 71.104\n",
            "Worker 8, [32/64]: Training Loss: 0.891670476, Training Accuracy: 73.280\n",
            "Worker 8, [33/64]: Training Loss: 0.862152694, Training Accuracy: 73.760\n",
            "Worker 8, [34/64]: Training Loss: 0.849357010, Training Accuracy: 74.000\n",
            "Worker 8, [35/64]: Training Loss: 0.793244858, Training Accuracy: 75.680\n",
            "Worker 8, [36/64]: Training Loss: 0.812204191, Training Accuracy: 75.056\n",
            "Worker 8, [37/64]: Training Loss: 0.748568346, Training Accuracy: 77.296\n",
            "Worker 8, [38/64]: Training Loss: 0.734195412, Training Accuracy: 77.696\n",
            "Worker 8, [39/64]: Training Loss: 0.724235661, Training Accuracy: 78.224\n",
            "Worker 8, [40/64]: Training Loss: 0.716726685, Training Accuracy: 77.728\n",
            "Worker 8, [41/64]: Training Loss: 0.700962202, Training Accuracy: 78.272\n",
            "Worker 8, [42/64]: Training Loss: 0.647850055, Training Accuracy: 80.784\n",
            "Worker 8, [43/64]: Training Loss: 0.659011874, Training Accuracy: 79.968\n",
            "Worker 8, [44/64]: Training Loss: 0.627566444, Training Accuracy: 80.672\n",
            "Worker 8, [45/64]: Training Loss: 0.593901717, Training Accuracy: 81.760\n",
            "Worker 8, [46/64]: Training Loss: 0.590577744, Training Accuracy: 81.776\n",
            "Worker 8, [47/64]: Training Loss: 0.580601794, Training Accuracy: 82.496\n",
            "Worker 8, [48/64]: Training Loss: 0.569223797, Training Accuracy: 82.496\n",
            "Worker 8, [49/64]: Training Loss: 0.526606101, Training Accuracy: 83.584\n",
            "Worker 8, [50/64]: Training Loss: 0.550658671, Training Accuracy: 83.200\n",
            "Worker 8, [51/64]: Training Loss: 0.517996679, Training Accuracy: 84.000\n",
            "Worker 8, [52/64]: Training Loss: 0.501988211, Training Accuracy: 84.400\n",
            "Worker 8, [53/64]: Training Loss: 0.485465560, Training Accuracy: 84.624\n",
            "Worker 8, [54/64]: Training Loss: 0.514590183, Training Accuracy: 84.144\n",
            "Worker 8, [55/64]: Training Loss: 0.501964243, Training Accuracy: 84.400\n",
            "Worker 8, [56/64]: Training Loss: 0.464591316, Training Accuracy: 85.984\n",
            "Worker 8, [57/64]: Training Loss: 0.442636524, Training Accuracy: 86.304\n",
            "Worker 8, [58/64]: Training Loss: 0.413971221, Training Accuracy: 86.960\n",
            "Worker 8, [59/64]: Training Loss: 0.423311829, Training Accuracy: 86.784\n",
            "Worker 8, [60/64]: Training Loss: 0.452382651, Training Accuracy: 86.624\n",
            "Worker 8, [61/64]: Training Loss: 0.427427677, Training Accuracy: 86.784\n",
            "Worker 8, [62/64]: Training Loss: 0.403469805, Training Accuracy: 87.728\n",
            "Worker 8, [63/64]: Training Loss: 0.437852474, Training Accuracy: 86.912\n",
            "Worker 8, [64/64]: Training Loss: 0.403524904, Training Accuracy: 87.664\n",
            "Time taken for training worker 8: 0:02:52.388711\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003815\n",
            "Global Update 02: Test Loss: 4.915232519, Test Accuracy: 33.980\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:45:32.926374\n",
            "//////////////////////////////////////////////////\n"
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "beta = 0.4\n",
        "alpha = 1\n",
        "parameters = {'lr': lr, 'wd': wd, 'beta': beta}\n",
        "K = [2, 4, 8]\n",
        "J = [4, 8, 16, 32 , 64]\n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize slow model\n",
        "slow_model = LeNet5()\n",
        "slow_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "for k in K:\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    local_SGD_SlowMo(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, beta, alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Personal Contribution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SHAT (Asyncronous Approach)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class SHAT_PS_Optimizer(Optimizer):\n",
        "    def __init__(self, global_model, lr=0.01):\n",
        "        self.global_model = global_model\n",
        "        self.lr = lr\n",
        "        params = list(global_model.parameters())\n",
        "        super(SHAT_PS_Optimizer, self).__init__(params, {'lr': lr})\n",
        "        \n",
        "    def step(self, gradients_model):\n",
        "        # Update global model parameters with gradients \n",
        "        with torch.no_grad():\n",
        "            for key, param in self.global_model.state_dict().items():\n",
        "                param.copy_(param -  self.lr * gradients_model.state_dict()[key])\n",
        "\n",
        "        return None\n",
        "\n",
        "# For example total iterations 150*8 = 1200 to hacve the same number of iterations as local SGD in other words 150 true epochs\n",
        "# For example: Worker 1 has 15 iterations, Worker 2 has 15 iterations, Worker 3 has 15 iterations, Worker 4 has 15 iterations\n",
        "# All have the same number of iterations\n",
        "\n",
        "def generate_computation_latency_sequence(K, each_worker_iteration):\n",
        "    \"\"\"\n",
        "    Simulates a sequence of operations for K workers with random computation latencies.\n",
        "\n",
        "    The function generates a list of operations, each performed by a worker, sorted by\n",
        "    their latencies. Each worker performs exactly t operations, and the latencies are\n",
        "    scaled so that the fastest worker's latency is normalized to 1.\n",
        "\n",
        "    Parameters:\n",
        "    K (int): The number of workers in the simulation.\n",
        "    t (int): The number of operations each computer will perform.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - result (list of dicts): A list of dictionaries where each dictionary represents\n",
        "          an operation performed by a computer. The keys in the dictionary are:\n",
        "              - \"total_iterations\" (int): The total number of iterations performed.\n",
        "              - \"computer\" (int): The ID of the computer performing the operation.\n",
        "              - \"value\" (int): The latency value associated with that operation.\n",
        "        - computation_powers (list of ints): The original random computation latencies for each computer.\n",
        "        - scaled_powers (list of floats): The computation latencies scaled such that the fastest\n",
        "          worker's latency is 1.\n",
        "    \"\"\"\n",
        "    # Generate random computation powers\n",
        "    computation_powers = [random.randint(5000, 10000) for _ in range(K)]\n",
        "\n",
        "    # Find the minimum computation power\n",
        "    min_power = min(computation_powers)\n",
        "\n",
        "    # Scale the computation powers so that the fastest (smallest value) is equal to 1\n",
        "    scaled_powers = [power / min_power for power in computation_powers]\n",
        "\n",
        "    # Initialize a list to store the dictionaries\n",
        "    result = []\n",
        "\n",
        "    # Store the number of turns taken by each worker\n",
        "    turns_taken = [0] * K\n",
        "    total_number_of_iterations = 0\n",
        "    # Generate the dictionaries until all computers have 15 turns\n",
        "    while any(turn < each_worker_iteration for turn in turns_taken):\n",
        "        total_number_of_iterations += 1\n",
        "        # Generate the next possible dictionaries for each worker\n",
        "        possible_entries = []\n",
        "        for index in range(K):\n",
        "            if turns_taken[index] < each_worker_iteration:  # Only consider computers with less than 15 turns\n",
        "                operation_value = computation_powers[index] * (turns_taken[index] + 1)\n",
        "                possible_entries.append({\"total_iterations\": total_number_of_iterations,\"worker\": index, \"value\": operation_value})\n",
        "\n",
        "        # Sort the possible dictionaries by their operation value\n",
        "        possible_entries.sort(key=lambda x: x[\"value\"])\n",
        "\n",
        "        # Add the dictionary with the smallest value to the result\n",
        "        chosen_entry = possible_entries[0]\n",
        "        result.append(chosen_entry)\n",
        "\n",
        "        # Update the turn count for the chosen worker\n",
        "        chosen_index = chosen_entry[\"worker\"] \n",
        "        turns_taken[chosen_index] += 1\n",
        "\n",
        "    return result, computation_powers, scaled_powers\n",
        "\n",
        "\n",
        "# For example total iterations 150*8 = 1200 to have the same number of iterations as local SGD in other words 150 true epochs\n",
        "# For example: Worker 1 has 15 iterations, Worker 2 has 30 iterations, Worker 3 has 45 iterations, Worker 4 has 60 iterations\n",
        "'''def generate_computation_latency_sequence(K, total_iterations):\n",
        "    \"\"\"\n",
        "    Simulates a sequence of operations for K workers with random computation latencies.\n",
        "\n",
        "    The function generates a list of operations, each performed by a worker, sorted by\n",
        "    their latencies. The total number of iterations is limited by `total_iterations`,\n",
        "    and the latencies are scaled so that the fastest worker's latency is normalized to 1.\n",
        "\n",
        "    Parameters:\n",
        "    K (int): The number of workers in the simulation.\n",
        "    total_iterations (int): The total number of iterations across all workers.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - result (list of dicts): A list of dictionaries where each dictionary represents\n",
        "          an operation performed by a worker. The keys in the dictionary are:\n",
        "              - \"total_iterations\" (int): The total number of iterations performed.\n",
        "              - \"worker\" (int): The ID of the worker performing the operation.\n",
        "              - \"value\" (int): The latency value associated with that operation.\n",
        "        - computation_powers (list of ints): The original random computation latencies for each worker.\n",
        "        - scaled_powers (list of floats): The computation latencies scaled such that the fastest\n",
        "          worker's latency is 1.\n",
        "    \"\"\"\n",
        "    # Generate random computation powers\n",
        "    computation_powers = [random.randint(5000, 10000) for _ in range(K)]\n",
        "\n",
        "    # Find the minimum computation power\n",
        "    min_power = min(computation_powers)\n",
        "\n",
        "    # Scale the computation powers so that the fastest (smallest value) is equal to 1\n",
        "    scaled_powers = [power / min_power for power in computation_powers]\n",
        "\n",
        "    # Initialize a list to store the dictionaries\n",
        "    result = []\n",
        "\n",
        "    # Store the number of turns taken by each worker\n",
        "    turns_taken = [0] * K\n",
        "    total_number_of_iterations_performed = 0\n",
        "\n",
        "    # Generate the dictionaries until the total number of iterations is reached\n",
        "    while total_number_of_iterations_performed < total_iterations:\n",
        "        total_number_of_iterations_performed += 1\n",
        "\n",
        "        # Generate the next possible dictionaries for each worker\n",
        "        possible_entries = []\n",
        "        for index in range(K):\n",
        "            # Calculate the operation value based on the current number of turns for this worker\n",
        "            operation_value = computation_powers[index] * (turns_taken[index] + 1)\n",
        "            possible_entries.append({\n",
        "                \"total_iterations\": total_number_of_iterations_performed,\n",
        "                \"worker\": index,  # Now starting index from 0\n",
        "                \"value\": operation_value\n",
        "            })\n",
        "\n",
        "        # Sort the possible dictionaries by their operation value\n",
        "        possible_entries.sort(key=lambda x: x[\"value\"])\n",
        "\n",
        "        # Add the dictionary with the smallest value to the result\n",
        "        chosen_entry = possible_entries[0]\n",
        "        result.append(chosen_entry)\n",
        "\n",
        "        # Update the turn count for the chosen worker\n",
        "        chosen_index = chosen_entry[\"worker\"]\n",
        "        turns_taken[chosen_index] += 1\n",
        "\n",
        "    return result, computation_powers, scaled_powers\n",
        "'''\n",
        "\n",
        "def calculate_gradients_model(global_model, local_model, lr):\n",
        "    gradients_dict = {key: torch.zeros_like(value) for key, value in local_model.state_dict().items()}\n",
        "    \n",
        "    for key, value in local_model.state_dict().items():\n",
        "        gradients_dict[key] += ((global_model.state_dict()[key] - value)/lr)\n",
        "    # TODO: lr ro check konim \n",
        "    \n",
        "    return gradients_dict\n",
        "\n",
        "def update_global_model(global_model, gradients_model, lr):\n",
        "    new_weights = {}\n",
        "    for key, value in global_model.state_dict().items():\n",
        "        new_weights[key] = value - lr * gradients_model.state_dict()[key]\n",
        "\n",
        "    return new_weights\n",
        "\n",
        "\n",
        "def calculate_s_i(k,ci):\n",
        "    return float(k/ci)\n",
        "\n",
        "def calculate_alpha_i(si, k):\n",
        "    alpha = 1 - (si / math.log(k))\n",
        "    return alpha\n",
        "\n",
        "def updatel_local_model(global_model, local_model, alpha_i):\n",
        "    new_weights = {}\n",
        "    for key, value in local_model.state_dict().items():\n",
        "        new_weights[key] = (1 - alpha_i) * value + alpha_i * global_model.state_dict()[key]\n",
        "    \n",
        "    return new_weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def SHAT_PS(shard_loaders, loss_fn, k, lr, wd, initial_state_dict, num_epochs):  \n",
        "    # SHAT Parameter Server to manage workers\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    # Initialize a model with same value of param for each chunk\n",
        "    local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "    for model in local_models:\n",
        "      model.load_state_dict(initial_state_dict)\n",
        "\n",
        "    # # Initialize the global model\n",
        "    global_model = LeNet5().to(device)\n",
        "    global_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "    global_optimizer = SHAT_PS_Optimizer(global_model, lr)\n",
        "  \n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=num_epochs)\n",
        "    \n",
        "    checkpoint = load_checkpoint('shat', 64, {'k': k})\n",
        "    if checkpoint is not None:\n",
        "      global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      global_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      \n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn)\n",
        "      print(f'Global Update: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      \n",
        "      return None\n",
        "    # Generate a sequence of computation latency to simulate the difference of computation latency (Lower computation Latency means higher computation power)\n",
        "    computation_latency_sequence, computation_latency, scaled_computation_latency = generate_computation_latency_sequence(k, num_epochs)\n",
        "\n",
        "    # Print the original and scaled computation latency\n",
        "    print(\"Original Computation Latency:\", computation_latency)\n",
        "    print(\"Scaled Computation Latency:\", scaled_computation_latency)\n",
        "\n",
        "    # print sequence of workers based on their computation latency\n",
        "    print(f'workers simulated orders based on computation latency:{[entry[\"worker\"]+1 for entry in computation_latency_sequence]}')\n",
        "\n",
        "    C = [1 for _ in range(k)] # Staleness counter\n",
        "    local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "    \n",
        "    #This specifies turn of the model\n",
        "    for iteration_index, worker in enumerate([entry['worker'] for entry in computation_latency_sequence]):\n",
        "      iteration_start_time = time.time()\n",
        "      print('*'*50)\n",
        "\n",
        "      train_loss, train_accuracy = train(local_models[worker], shard_loaders[worker], local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "      print(f'Worker {worker+1}, [{iteration_index+1:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      print('*'*50)\n",
        "      \n",
        "      '''PS server: receive model from the worker and calculate diff model (gradient)'''\n",
        "      gradients_model = LeNet5().to(device)\n",
        "      gradients_model.load_state_dict(calculate_gradients_model(global_model, local_models[worker], lr))\n",
        "      \n",
        "      # Computing the staleness of each worker\n",
        "      for i in C :\n",
        "        if i != worker:\n",
        "          i += 1\n",
        "          \n",
        "      '''PS Server update global model'''\n",
        "      global_optimizer.step(gradients_model)\n",
        "      \n",
        "      '''send updated model to the worker'''\n",
        "      '''calucale the staleness of the worker αi ← si − logn ,    s ← n/ci'''\n",
        "      s_i = calculate_s_i(k,C[worker])\n",
        "      alpha_i = calculate_alpha_i(s_i, k)\n",
        "      '''update worker local model ba w ← (1 − α )w + α w̃'''\n",
        "      local_models[worker].load_state_dict(updatel_local_model(global_model, local_models[worker], alpha_i))\n",
        "\n",
        "      '''continue outer loop in PS'''\n",
        "      '''ci = 0 ya 1'''\n",
        "      C[worker] = 1\n",
        "      \n",
        "      iteration_end_time = time.time()\n",
        "     \n",
        "      print(f'Time taken for worker {worker+1} : {str(timedelta(seconds=iteration_end_time - iteration_start_time))}')\n",
        "      print('-'*50)\n",
        "      print(f'Iteration: {iteration_index+1:02}/{k*num_epochs:02} - True epochs: {num_epochs}')\n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "      print(f'Global Update {iteration_index+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      print('-'*50)\n",
        "    \n",
        "    # Save checkpoint\n",
        "    save_checkpoint({\n",
        "              'epoch': num_epochs,\n",
        "              'model_state_dict': global_model.state_dict(),\n",
        "              'optimizer_state_dict': global_optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss': loss_fn\n",
        "              }, 149, 64, 'shat', {'k': k})\n",
        "    \n",
        "    total_end_time = time.time()\n",
        "    print('/'*50)\n",
        "    print(f'Total time taken for SHAT: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "    print('/'*50)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2\n",
            "==================================================\n",
            "Original Computation Latency: [8401, 7812]\n",
            "Scaled Computation Latency: [1.0753968253968254, 1.0]\n",
            "workers simulated orders based on computation latency:[2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "**************************************************\n",
            "Worker 2, [01]: Training Loss: 4.307266207, Training Accuracy: 4.612\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.785191\n",
            "--------------------------------------------------\n",
            "Iteration: 01/300 - True epochs: 150\n",
            "Local Step 01: Test Loss: 3.962921501, Test Accuracy: 8.640\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [02]: Training Loss: 4.309138212, Training Accuracy: 4.256\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.448703\n",
            "--------------------------------------------------\n",
            "Iteration: 02/300 - True epochs: 150\n",
            "Local Step 02: Test Loss: 3.969725398, Test Accuracy: 8.600\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [03]: Training Loss: 3.862986856, Training Accuracy: 10.284\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.097537\n",
            "--------------------------------------------------\n",
            "Iteration: 03/300 - True epochs: 150\n",
            "Local Step 03: Test Loss: 3.736107696, Test Accuracy: 11.650\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [04]: Training Loss: 3.842571163, Training Accuracy: 10.704\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.012339\n",
            "--------------------------------------------------\n",
            "Iteration: 04/300 - True epochs: 150\n",
            "Local Step 04: Test Loss: 3.613090187, Test Accuracy: 14.850\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [05]: Training Loss: 3.623173966, Training Accuracy: 14.076\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.533577\n",
            "--------------------------------------------------\n",
            "Iteration: 05/300 - True epochs: 150\n",
            "Local Step 05: Test Loss: 3.472416038, Test Accuracy: 17.570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [06]: Training Loss: 3.588647497, Training Accuracy: 14.488\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.259110\n",
            "--------------------------------------------------\n",
            "Iteration: 06/300 - True epochs: 150\n",
            "Local Step 06: Test Loss: 3.398047888, Test Accuracy: 17.960\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [07]: Training Loss: 3.408781441, Training Accuracy: 17.820\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:13.789727\n",
            "--------------------------------------------------\n",
            "Iteration: 07/300 - True epochs: 150\n",
            "Local Step 07: Test Loss: 3.237761611, Test Accuracy: 21.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [08]: Training Loss: 3.367197131, Training Accuracy: 18.072\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.518731\n",
            "--------------------------------------------------\n",
            "Iteration: 08/300 - True epochs: 150\n",
            "Local Step 08: Test Loss: 3.258052437, Test Accuracy: 20.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [09]: Training Loss: 3.231208661, Training Accuracy: 20.964\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.710526\n",
            "--------------------------------------------------\n",
            "Iteration: 09/300 - True epochs: 150\n",
            "Local Step 09: Test Loss: 3.049135196, Test Accuracy: 24.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [10]: Training Loss: 3.209609209, Training Accuracy: 20.788\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.542538\n",
            "--------------------------------------------------\n",
            "Iteration: 10/300 - True epochs: 150\n",
            "Local Step 10: Test Loss: 3.145104123, Test Accuracy: 23.380\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [11]: Training Loss: 3.083139157, Training Accuracy: 23.728\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.370942\n",
            "--------------------------------------------------\n",
            "Iteration: 11/300 - True epochs: 150\n",
            "Local Step 11: Test Loss: 2.911708127, Test Accuracy: 26.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [12]: Training Loss: 3.080627202, Training Accuracy: 23.376\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.249046\n",
            "--------------------------------------------------\n",
            "Iteration: 12/300 - True epochs: 150\n",
            "Local Step 12: Test Loss: 2.964871466, Test Accuracy: 25.440\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [13]: Training Loss: 2.948377138, Training Accuracy: 26.036\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.217724\n",
            "--------------------------------------------------\n",
            "Iteration: 13/300 - True epochs: 150\n",
            "Local Step 13: Test Loss: 2.823096283, Test Accuracy: 28.470\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [14]: Training Loss: 2.939524274, Training Accuracy: 25.944\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.054156\n",
            "--------------------------------------------------\n",
            "Iteration: 14/300 - True epochs: 150\n",
            "Local Step 14: Test Loss: 2.903328619, Test Accuracy: 27.350\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [15]: Training Loss: 2.841187194, Training Accuracy: 28.536\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.031217\n",
            "--------------------------------------------------\n",
            "Iteration: 15/300 - True epochs: 150\n",
            "Local Step 15: Test Loss: 2.763697150, Test Accuracy: 30.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [16]: Training Loss: 2.831092252, Training Accuracy: 28.080\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.780063\n",
            "--------------------------------------------------\n",
            "Iteration: 16/300 - True epochs: 150\n",
            "Local Step 16: Test Loss: 2.706835501, Test Accuracy: 31.060\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [17]: Training Loss: 2.755563582, Training Accuracy: 30.032\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:13.388372\n",
            "--------------------------------------------------\n",
            "Iteration: 17/300 - True epochs: 150\n",
            "Local Step 17: Test Loss: 2.665699757, Test Accuracy: 31.620\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [18]: Training Loss: 2.746146267, Training Accuracy: 29.872\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:14.373163\n",
            "--------------------------------------------------\n",
            "Iteration: 18/300 - True epochs: 150\n",
            "Local Step 18: Test Loss: 2.747074975, Test Accuracy: 31.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [19]: Training Loss: 2.674640752, Training Accuracy: 31.592\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.810237\n",
            "--------------------------------------------------\n",
            "Iteration: 19/300 - True epochs: 150\n",
            "Local Step 19: Test Loss: 2.617861140, Test Accuracy: 32.950\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [20]: Training Loss: 2.678509245, Training Accuracy: 31.192\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.549926\n",
            "--------------------------------------------------\n",
            "Iteration: 20/300 - True epochs: 150\n",
            "Local Step 20: Test Loss: 2.577193652, Test Accuracy: 33.910\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [21]: Training Loss: 2.594971994, Training Accuracy: 33.044\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.472699\n",
            "--------------------------------------------------\n",
            "Iteration: 21/300 - True epochs: 150\n",
            "Local Step 21: Test Loss: 2.581815076, Test Accuracy: 34.570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [22]: Training Loss: 2.606191811, Training Accuracy: 32.736\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.885898\n",
            "--------------------------------------------------\n",
            "Iteration: 22/300 - True epochs: 150\n",
            "Local Step 22: Test Loss: 2.598075756, Test Accuracy: 34.330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [23]: Training Loss: 2.517986441, Training Accuracy: 34.588\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.828829\n",
            "--------------------------------------------------\n",
            "Iteration: 23/300 - True epochs: 150\n",
            "Local Step 23: Test Loss: 2.579178072, Test Accuracy: 34.430\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [24]: Training Loss: 2.542183955, Training Accuracy: 33.780\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.897320\n",
            "--------------------------------------------------\n",
            "Iteration: 24/300 - True epochs: 150\n",
            "Local Step 24: Test Loss: 2.546564928, Test Accuracy: 34.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [25]: Training Loss: 2.468160079, Training Accuracy: 36.124\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:13.412148\n",
            "--------------------------------------------------\n",
            "Iteration: 25/300 - True epochs: 150\n",
            "Local Step 25: Test Loss: 2.493443537, Test Accuracy: 36.280\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [26]: Training Loss: 2.477433051, Training Accuracy: 35.460\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:14.524886\n",
            "--------------------------------------------------\n",
            "Iteration: 26/300 - True epochs: 150\n",
            "Local Step 26: Test Loss: 2.566112125, Test Accuracy: 34.500\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [27]: Training Loss: 2.417228085, Training Accuracy: 37.096\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.420542\n",
            "--------------------------------------------------\n",
            "Iteration: 27/300 - True epochs: 150\n",
            "Local Step 27: Test Loss: 2.550839131, Test Accuracy: 34.820\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [28]: Training Loss: 2.362977679, Training Accuracy: 37.924\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:13.129168\n",
            "--------------------------------------------------\n",
            "Iteration: 28/300 - True epochs: 150\n",
            "Local Step 28: Test Loss: 2.459559993, Test Accuracy: 36.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [29]: Training Loss: 2.436816190, Training Accuracy: 35.936\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.002356\n",
            "--------------------------------------------------\n",
            "Iteration: 29/300 - True epochs: 150\n",
            "Local Step 29: Test Loss: 2.572410734, Test Accuracy: 34.750\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [30]: Training Loss: 2.302783347, Training Accuracy: 39.288\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.114341\n",
            "--------------------------------------------------\n",
            "Iteration: 30/300 - True epochs: 150\n",
            "Local Step 30: Test Loss: 2.418571501, Test Accuracy: 37.960\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [31]: Training Loss: 2.379752872, Training Accuracy: 37.464\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.142432\n",
            "--------------------------------------------------\n",
            "Iteration: 31/300 - True epochs: 150\n",
            "Local Step 31: Test Loss: 2.476226665, Test Accuracy: 36.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [32]: Training Loss: 2.274897894, Training Accuracy: 39.876\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.371975\n",
            "--------------------------------------------------\n",
            "Iteration: 32/300 - True epochs: 150\n",
            "Local Step 32: Test Loss: 2.424000762, Test Accuracy: 37.670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [33]: Training Loss: 2.332900806, Training Accuracy: 38.556\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.809459\n",
            "--------------------------------------------------\n",
            "Iteration: 33/300 - True epochs: 150\n",
            "Local Step 33: Test Loss: 2.424099479, Test Accuracy: 37.790\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [34]: Training Loss: 2.236345936, Training Accuracy: 40.644\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:13.067182\n",
            "--------------------------------------------------\n",
            "Iteration: 34/300 - True epochs: 150\n",
            "Local Step 34: Test Loss: 2.470349476, Test Accuracy: 37.890\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [35]: Training Loss: 2.288456417, Training Accuracy: 39.224\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.545454\n",
            "--------------------------------------------------\n",
            "Iteration: 35/300 - True epochs: 150\n",
            "Local Step 35: Test Loss: 2.415537118, Test Accuracy: 37.570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [36]: Training Loss: 2.189590788, Training Accuracy: 41.576\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.804519\n",
            "--------------------------------------------------\n",
            "Iteration: 36/300 - True epochs: 150\n",
            "Local Step 36: Test Loss: 2.388232094, Test Accuracy: 39.620\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [37]: Training Loss: 2.241521174, Training Accuracy: 40.004\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.524845\n",
            "--------------------------------------------------\n",
            "Iteration: 37/300 - True epochs: 150\n",
            "Local Step 37: Test Loss: 2.385981744, Test Accuracy: 39.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [38]: Training Loss: 2.156198593, Training Accuracy: 42.236\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.687872\n",
            "--------------------------------------------------\n",
            "Iteration: 38/300 - True epochs: 150\n",
            "Local Step 38: Test Loss: 2.393181947, Test Accuracy: 39.680\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [39]: Training Loss: 2.216042266, Training Accuracy: 40.648\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.437738\n",
            "--------------------------------------------------\n",
            "Iteration: 39/300 - True epochs: 150\n",
            "Local Step 39: Test Loss: 2.368866381, Test Accuracy: 39.220\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [40]: Training Loss: 2.118082158, Training Accuracy: 43.364\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:13.392267\n",
            "--------------------------------------------------\n",
            "Iteration: 40/300 - True epochs: 150\n",
            "Local Step 40: Test Loss: 2.488111464, Test Accuracy: 37.670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [41]: Training Loss: 2.189567298, Training Accuracy: 41.232\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.768543\n",
            "--------------------------------------------------\n",
            "Iteration: 41/300 - True epochs: 150\n",
            "Local Step 41: Test Loss: 2.433329883, Test Accuracy: 37.900\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [42]: Training Loss: 2.100002941, Training Accuracy: 43.708\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.097884\n",
            "--------------------------------------------------\n",
            "Iteration: 42/300 - True epochs: 150\n",
            "Local Step 42: Test Loss: 2.439856771, Test Accuracy: 39.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [43]: Training Loss: 2.135329790, Training Accuracy: 42.884\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.919472\n",
            "--------------------------------------------------\n",
            "Iteration: 43/300 - True epochs: 150\n",
            "Local Step 43: Test Loss: 2.355207768, Test Accuracy: 39.370\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [44]: Training Loss: 2.061738190, Training Accuracy: 44.064\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.535483\n",
            "--------------------------------------------------\n",
            "Iteration: 44/300 - True epochs: 150\n",
            "Local Step 44: Test Loss: 2.332630416, Test Accuracy: 40.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [45]: Training Loss: 2.113084389, Training Accuracy: 43.136\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.837109\n",
            "--------------------------------------------------\n",
            "Iteration: 45/300 - True epochs: 150\n",
            "Local Step 45: Test Loss: 2.325959683, Test Accuracy: 40.170\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [46]: Training Loss: 2.033766303, Training Accuracy: 45.180\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.003263\n",
            "--------------------------------------------------\n",
            "Iteration: 46/300 - True epochs: 150\n",
            "Local Step 46: Test Loss: 2.308186854, Test Accuracy: 41.110\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [47]: Training Loss: 2.077847982, Training Accuracy: 44.032\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.289923\n",
            "--------------------------------------------------\n",
            "Iteration: 47/300 - True epochs: 150\n",
            "Local Step 47: Test Loss: 2.328176461, Test Accuracy: 40.890\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [48]: Training Loss: 2.026473212, Training Accuracy: 45.344\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.351197\n",
            "--------------------------------------------------\n",
            "Iteration: 48/300 - True epochs: 150\n",
            "Local Step 48: Test Loss: 2.348655716, Test Accuracy: 40.470\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [49]: Training Loss: 2.044595862, Training Accuracy: 45.108\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.992693\n",
            "--------------------------------------------------\n",
            "Iteration: 49/300 - True epochs: 150\n",
            "Local Step 49: Test Loss: 2.325418916, Test Accuracy: 40.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [50]: Training Loss: 1.992757248, Training Accuracy: 46.128\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.087758\n",
            "--------------------------------------------------\n",
            "Iteration: 50/300 - True epochs: 150\n",
            "Local Step 50: Test Loss: 2.413536774, Test Accuracy: 40.080\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [51]: Training Loss: 2.022879432, Training Accuracy: 44.956\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.976850\n",
            "--------------------------------------------------\n",
            "Iteration: 51/300 - True epochs: 150\n",
            "Local Step 51: Test Loss: 2.370133111, Test Accuracy: 39.760\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [52]: Training Loss: 1.979710519, Training Accuracy: 46.284\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.015400\n",
            "--------------------------------------------------\n",
            "Iteration: 52/300 - True epochs: 150\n",
            "Local Step 52: Test Loss: 2.335606791, Test Accuracy: 40.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [53]: Training Loss: 2.000820539, Training Accuracy: 45.444\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.999991\n",
            "--------------------------------------------------\n",
            "Iteration: 53/300 - True epochs: 150\n",
            "Local Step 53: Test Loss: 2.351107824, Test Accuracy: 40.890\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [54]: Training Loss: 1.927847551, Training Accuracy: 47.428\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.768801\n",
            "--------------------------------------------------\n",
            "Iteration: 54/300 - True epochs: 150\n",
            "Local Step 54: Test Loss: 2.301804041, Test Accuracy: 42.500\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [55]: Training Loss: 1.902443729, Training Accuracy: 48.140\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.965580\n",
            "--------------------------------------------------\n",
            "Iteration: 55/300 - True epochs: 150\n",
            "Local Step 55: Test Loss: 2.353393711, Test Accuracy: 41.360\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [56]: Training Loss: 1.977730435, Training Accuracy: 46.480\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.923634\n",
            "--------------------------------------------------\n",
            "Iteration: 56/300 - True epochs: 150\n",
            "Local Step 56: Test Loss: 2.324267141, Test Accuracy: 40.950\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [57]: Training Loss: 1.891372388, Training Accuracy: 48.176\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.145304\n",
            "--------------------------------------------------\n",
            "Iteration: 57/300 - True epochs: 150\n",
            "Local Step 57: Test Loss: 2.390939456, Test Accuracy: 39.800\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [58]: Training Loss: 1.946322289, Training Accuracy: 46.908\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.313208\n",
            "--------------------------------------------------\n",
            "Iteration: 58/300 - True epochs: 150\n",
            "Local Step 58: Test Loss: 2.339459396, Test Accuracy: 40.710\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [59]: Training Loss: 1.889765832, Training Accuracy: 48.324\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.311110\n",
            "--------------------------------------------------\n",
            "Iteration: 59/300 - True epochs: 150\n",
            "Local Step 59: Test Loss: 2.316431905, Test Accuracy: 42.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [60]: Training Loss: 1.924972865, Training Accuracy: 47.032\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.307629\n",
            "--------------------------------------------------\n",
            "Iteration: 60/300 - True epochs: 150\n",
            "Local Step 60: Test Loss: 2.297756147, Test Accuracy: 41.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [61]: Training Loss: 1.841617276, Training Accuracy: 49.320\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.355232\n",
            "--------------------------------------------------\n",
            "Iteration: 61/300 - True epochs: 150\n",
            "Local Step 61: Test Loss: 2.326667761, Test Accuracy: 42.130\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [62]: Training Loss: 1.885674189, Training Accuracy: 48.164\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.136796\n",
            "--------------------------------------------------\n",
            "Iteration: 62/300 - True epochs: 150\n",
            "Local Step 62: Test Loss: 2.321105350, Test Accuracy: 42.320\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [63]: Training Loss: 1.829859299, Training Accuracy: 49.704\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.672907\n",
            "--------------------------------------------------\n",
            "Iteration: 63/300 - True epochs: 150\n",
            "Local Step 63: Test Loss: 2.353853625, Test Accuracy: 41.190\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [64]: Training Loss: 1.881464803, Training Accuracy: 48.460\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.651393\n",
            "--------------------------------------------------\n",
            "Iteration: 64/300 - True epochs: 150\n",
            "Local Step 64: Test Loss: 2.374279132, Test Accuracy: 40.690\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [65]: Training Loss: 1.811782277, Training Accuracy: 50.228\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.888829\n",
            "--------------------------------------------------\n",
            "Iteration: 65/300 - True epochs: 150\n",
            "Local Step 65: Test Loss: 2.365599755, Test Accuracy: 42.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [66]: Training Loss: 1.880530995, Training Accuracy: 48.664\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.886627\n",
            "--------------------------------------------------\n",
            "Iteration: 66/300 - True epochs: 150\n",
            "Local Step 66: Test Loss: 2.365313987, Test Accuracy: 40.790\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [67]: Training Loss: 1.797497457, Training Accuracy: 50.628\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.240608\n",
            "--------------------------------------------------\n",
            "Iteration: 67/300 - True epochs: 150\n",
            "Local Step 67: Test Loss: 2.333511652, Test Accuracy: 42.430\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [68]: Training Loss: 1.851987744, Training Accuracy: 49.104\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.169176\n",
            "--------------------------------------------------\n",
            "Iteration: 68/300 - True epochs: 150\n",
            "Local Step 68: Test Loss: 2.357594805, Test Accuracy: 41.350\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [69]: Training Loss: 1.776796139, Training Accuracy: 51.032\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.632877\n",
            "--------------------------------------------------\n",
            "Iteration: 69/300 - True epochs: 150\n",
            "Local Step 69: Test Loss: 2.367034257, Test Accuracy: 41.590\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [70]: Training Loss: 1.827509424, Training Accuracy: 49.264\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.132342\n",
            "--------------------------------------------------\n",
            "Iteration: 70/300 - True epochs: 150\n",
            "Local Step 70: Test Loss: 2.310561648, Test Accuracy: 41.300\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [71]: Training Loss: 1.782733148, Training Accuracy: 50.908\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.863952\n",
            "--------------------------------------------------\n",
            "Iteration: 71/300 - True epochs: 150\n",
            "Local Step 71: Test Loss: 2.380623422, Test Accuracy: 41.230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [72]: Training Loss: 1.807890168, Training Accuracy: 50.196\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.663997\n",
            "--------------------------------------------------\n",
            "Iteration: 72/300 - True epochs: 150\n",
            "Local Step 72: Test Loss: 2.369613009, Test Accuracy: 41.390\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [73]: Training Loss: 1.764003771, Training Accuracy: 51.072\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.387758\n",
            "--------------------------------------------------\n",
            "Iteration: 73/300 - True epochs: 150\n",
            "Local Step 73: Test Loss: 2.363113251, Test Accuracy: 41.540\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [74]: Training Loss: 1.799707710, Training Accuracy: 50.444\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.048225\n",
            "--------------------------------------------------\n",
            "Iteration: 74/300 - True epochs: 150\n",
            "Local Step 74: Test Loss: 2.432257162, Test Accuracy: 40.300\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [75]: Training Loss: 1.743780617, Training Accuracy: 51.888\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.962205\n",
            "--------------------------------------------------\n",
            "Iteration: 75/300 - True epochs: 150\n",
            "Local Step 75: Test Loss: 2.321610947, Test Accuracy: 42.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [76]: Training Loss: 1.796590772, Training Accuracy: 50.256\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.895330\n",
            "--------------------------------------------------\n",
            "Iteration: 76/300 - True epochs: 150\n",
            "Local Step 76: Test Loss: 2.336995650, Test Accuracy: 41.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [77]: Training Loss: 1.716012498, Training Accuracy: 52.416\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.927721\n",
            "--------------------------------------------------\n",
            "Iteration: 77/300 - True epochs: 150\n",
            "Local Step 77: Test Loss: 2.382884844, Test Accuracy: 40.980\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [78]: Training Loss: 1.777701860, Training Accuracy: 50.748\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.929684\n",
            "--------------------------------------------------\n",
            "Iteration: 78/300 - True epochs: 150\n",
            "Local Step 78: Test Loss: 2.312583501, Test Accuracy: 42.120\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [79]: Training Loss: 1.721553503, Training Accuracy: 52.416\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.108668\n",
            "--------------------------------------------------\n",
            "Iteration: 79/300 - True epochs: 150\n",
            "Local Step 79: Test Loss: 2.400319117, Test Accuracy: 41.040\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [80]: Training Loss: 1.753945665, Training Accuracy: 51.520\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.892006\n",
            "--------------------------------------------------\n",
            "Iteration: 80/300 - True epochs: 150\n",
            "Local Step 80: Test Loss: 2.414717152, Test Accuracy: 41.430\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [81]: Training Loss: 1.706681071, Training Accuracy: 52.476\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.043745\n",
            "--------------------------------------------------\n",
            "Iteration: 81/300 - True epochs: 150\n",
            "Local Step 81: Test Loss: 2.344795159, Test Accuracy: 42.760\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [82]: Training Loss: 1.697354979, Training Accuracy: 53.096\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.882866\n",
            "--------------------------------------------------\n",
            "Iteration: 82/300 - True epochs: 150\n",
            "Local Step 82: Test Loss: 2.343668658, Test Accuracy: 42.690\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [83]: Training Loss: 1.730165892, Training Accuracy: 52.264\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.908547\n",
            "--------------------------------------------------\n",
            "Iteration: 83/300 - True epochs: 150\n",
            "Local Step 83: Test Loss: 2.371823072, Test Accuracy: 41.880\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [84]: Training Loss: 1.678862725, Training Accuracy: 52.912\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.819008\n",
            "--------------------------------------------------\n",
            "Iteration: 84/300 - True epochs: 150\n",
            "Local Step 84: Test Loss: 2.384290610, Test Accuracy: 42.840\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [85]: Training Loss: 1.723513583, Training Accuracy: 52.068\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.330446\n",
            "--------------------------------------------------\n",
            "Iteration: 85/300 - True epochs: 150\n",
            "Local Step 85: Test Loss: 2.321222465, Test Accuracy: 41.980\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [86]: Training Loss: 1.675216883, Training Accuracy: 53.228\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.451370\n",
            "--------------------------------------------------\n",
            "Iteration: 86/300 - True epochs: 150\n",
            "Local Step 86: Test Loss: 2.370028124, Test Accuracy: 42.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [87]: Training Loss: 1.715082480, Training Accuracy: 51.996\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.130963\n",
            "--------------------------------------------------\n",
            "Iteration: 87/300 - True epochs: 150\n",
            "Local Step 87: Test Loss: 2.349063323, Test Accuracy: 40.850\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [88]: Training Loss: 1.643174608, Training Accuracy: 54.528\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.242207\n",
            "--------------------------------------------------\n",
            "Iteration: 88/300 - True epochs: 150\n",
            "Local Step 88: Test Loss: 2.382697371, Test Accuracy: 41.960\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [89]: Training Loss: 1.695108158, Training Accuracy: 52.696\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.750749\n",
            "--------------------------------------------------\n",
            "Iteration: 89/300 - True epochs: 150\n",
            "Local Step 89: Test Loss: 2.327723151, Test Accuracy: 42.480\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [90]: Training Loss: 1.645224484, Training Accuracy: 54.256\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.930233\n",
            "--------------------------------------------------\n",
            "Iteration: 90/300 - True epochs: 150\n",
            "Local Step 90: Test Loss: 2.374334518, Test Accuracy: 43.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [91]: Training Loss: 1.692615024, Training Accuracy: 52.812\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.411338\n",
            "--------------------------------------------------\n",
            "Iteration: 91/300 - True epochs: 150\n",
            "Local Step 91: Test Loss: 2.311883949, Test Accuracy: 42.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [92]: Training Loss: 1.633497422, Training Accuracy: 54.732\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.231732\n",
            "--------------------------------------------------\n",
            "Iteration: 92/300 - True epochs: 150\n",
            "Local Step 92: Test Loss: 2.334643753, Test Accuracy: 42.150\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [93]: Training Loss: 1.665277212, Training Accuracy: 53.712\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.399502\n",
            "--------------------------------------------------\n",
            "Iteration: 93/300 - True epochs: 150\n",
            "Local Step 93: Test Loss: 2.376962267, Test Accuracy: 41.640\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [94]: Training Loss: 1.636014510, Training Accuracy: 54.732\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.360248\n",
            "--------------------------------------------------\n",
            "Iteration: 94/300 - True epochs: 150\n",
            "Local Step 94: Test Loss: 2.441858929, Test Accuracy: 41.460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [95]: Training Loss: 1.672359375, Training Accuracy: 53.044\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.065193\n",
            "--------------------------------------------------\n",
            "Iteration: 95/300 - True epochs: 150\n",
            "Local Step 95: Test Loss: 2.311094129, Test Accuracy: 42.370\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [96]: Training Loss: 1.611190862, Training Accuracy: 54.684\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.016852\n",
            "--------------------------------------------------\n",
            "Iteration: 96/300 - True epochs: 150\n",
            "Local Step 96: Test Loss: 2.388359976, Test Accuracy: 42.450\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [97]: Training Loss: 1.632547794, Training Accuracy: 53.892\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.182968\n",
            "--------------------------------------------------\n",
            "Iteration: 97/300 - True epochs: 150\n",
            "Local Step 97: Test Loss: 2.428525151, Test Accuracy: 41.600\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [98]: Training Loss: 1.610904879, Training Accuracy: 54.884\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.211840\n",
            "--------------------------------------------------\n",
            "Iteration: 98/300 - True epochs: 150\n",
            "Local Step 98: Test Loss: 2.366450135, Test Accuracy: 43.680\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [99]: Training Loss: 1.638761768, Training Accuracy: 54.040\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.949078\n",
            "--------------------------------------------------\n",
            "Iteration: 99/300 - True epochs: 150\n",
            "Local Step 99: Test Loss: 2.294668006, Test Accuracy: 42.740\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [100]: Training Loss: 1.592052367, Training Accuracy: 55.472\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.564964\n",
            "--------------------------------------------------\n",
            "Iteration: 100/300 - True epochs: 150\n",
            "Local Step 100: Test Loss: 2.398129557, Test Accuracy: 43.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [101]: Training Loss: 1.618452079, Training Accuracy: 54.608\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.130598\n",
            "--------------------------------------------------\n",
            "Iteration: 101/300 - True epochs: 150\n",
            "Local Step 101: Test Loss: 2.416925276, Test Accuracy: 43.270\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [102]: Training Loss: 1.578348713, Training Accuracy: 55.704\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.132454\n",
            "--------------------------------------------------\n",
            "Iteration: 102/300 - True epochs: 150\n",
            "Local Step 102: Test Loss: 2.389072483, Test Accuracy: 42.830\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [103]: Training Loss: 1.623071643, Training Accuracy: 54.572\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.879329\n",
            "--------------------------------------------------\n",
            "Iteration: 103/300 - True epochs: 150\n",
            "Local Step 103: Test Loss: 2.380595950, Test Accuracy: 42.130\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [104]: Training Loss: 1.591300063, Training Accuracy: 55.300\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.177327\n",
            "--------------------------------------------------\n",
            "Iteration: 104/300 - True epochs: 150\n",
            "Local Step 104: Test Loss: 2.367215251, Test Accuracy: 43.170\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [105]: Training Loss: 1.615804373, Training Accuracy: 54.508\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.067953\n",
            "--------------------------------------------------\n",
            "Iteration: 105/300 - True epochs: 150\n",
            "Local Step 105: Test Loss: 2.368705109, Test Accuracy: 42.250\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [106]: Training Loss: 1.570583886, Training Accuracy: 55.844\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.690439\n",
            "--------------------------------------------------\n",
            "Iteration: 106/300 - True epochs: 150\n",
            "Local Step 106: Test Loss: 2.505965827, Test Accuracy: 42.330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [107]: Training Loss: 1.613621510, Training Accuracy: 54.492\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.080331\n",
            "--------------------------------------------------\n",
            "Iteration: 107/300 - True epochs: 150\n",
            "Local Step 107: Test Loss: 2.443328911, Test Accuracy: 41.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [108]: Training Loss: 1.558591851, Training Accuracy: 56.212\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.644791\n",
            "--------------------------------------------------\n",
            "Iteration: 108/300 - True epochs: 150\n",
            "Local Step 108: Test Loss: 2.519922523, Test Accuracy: 42.450\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [109]: Training Loss: 1.605997875, Training Accuracy: 54.740\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.156201\n",
            "--------------------------------------------------\n",
            "Iteration: 109/300 - True epochs: 150\n",
            "Local Step 109: Test Loss: 2.310254543, Test Accuracy: 42.390\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [110]: Training Loss: 1.570621353, Training Accuracy: 55.948\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.836160\n",
            "--------------------------------------------------\n",
            "Iteration: 110/300 - True epochs: 150\n",
            "Local Step 110: Test Loss: 2.487884884, Test Accuracy: 41.620\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [111]: Training Loss: 1.549263808, Training Accuracy: 56.256\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.133449\n",
            "--------------------------------------------------\n",
            "Iteration: 111/300 - True epochs: 150\n",
            "Local Step 111: Test Loss: 2.418280304, Test Accuracy: 42.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [112]: Training Loss: 1.586128391, Training Accuracy: 55.424\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.889945\n",
            "--------------------------------------------------\n",
            "Iteration: 112/300 - True epochs: 150\n",
            "Local Step 112: Test Loss: 2.429578413, Test Accuracy: 41.140\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [113]: Training Loss: 1.532927686, Training Accuracy: 56.948\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.241102\n",
            "--------------------------------------------------\n",
            "Iteration: 113/300 - True epochs: 150\n",
            "Local Step 113: Test Loss: 2.453191037, Test Accuracy: 41.180\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [114]: Training Loss: 1.590853226, Training Accuracy: 55.312\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.845919\n",
            "--------------------------------------------------\n",
            "Iteration: 114/300 - True epochs: 150\n",
            "Local Step 114: Test Loss: 2.407731185, Test Accuracy: 42.440\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [115]: Training Loss: 1.529376423, Training Accuracy: 56.460\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.163716\n",
            "--------------------------------------------------\n",
            "Iteration: 115/300 - True epochs: 150\n",
            "Local Step 115: Test Loss: 2.443346850, Test Accuracy: 42.210\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [116]: Training Loss: 1.578451288, Training Accuracy: 55.392\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.144553\n",
            "--------------------------------------------------\n",
            "Iteration: 116/300 - True epochs: 150\n",
            "Local Step 116: Test Loss: 2.403876742, Test Accuracy: 42.270\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [117]: Training Loss: 1.526285717, Training Accuracy: 56.904\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.852060\n",
            "--------------------------------------------------\n",
            "Iteration: 117/300 - True epochs: 150\n",
            "Local Step 117: Test Loss: 2.449533334, Test Accuracy: 41.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [118]: Training Loss: 1.550192365, Training Accuracy: 56.556\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.107851\n",
            "--------------------------------------------------\n",
            "Iteration: 118/300 - True epochs: 150\n",
            "Local Step 118: Test Loss: 2.387538202, Test Accuracy: 42.980\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [119]: Training Loss: 1.518678546, Training Accuracy: 56.872\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.839066\n",
            "--------------------------------------------------\n",
            "Iteration: 119/300 - True epochs: 150\n",
            "Local Step 119: Test Loss: 2.415479668, Test Accuracy: 42.200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [120]: Training Loss: 1.539401928, Training Accuracy: 56.376\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.275643\n",
            "--------------------------------------------------\n",
            "Iteration: 120/300 - True epochs: 150\n",
            "Local Step 120: Test Loss: 2.405098997, Test Accuracy: 42.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [121]: Training Loss: 1.519842132, Training Accuracy: 57.316\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.794070\n",
            "--------------------------------------------------\n",
            "Iteration: 121/300 - True epochs: 150\n",
            "Local Step 121: Test Loss: 2.375881687, Test Accuracy: 43.360\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [122]: Training Loss: 1.553678690, Training Accuracy: 56.148\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.363733\n",
            "--------------------------------------------------\n",
            "Iteration: 122/300 - True epochs: 150\n",
            "Local Step 122: Test Loss: 2.444774379, Test Accuracy: 42.460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [123]: Training Loss: 1.495723012, Training Accuracy: 57.688\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.295404\n",
            "--------------------------------------------------\n",
            "Iteration: 123/300 - True epochs: 150\n",
            "Local Step 123: Test Loss: 2.439473400, Test Accuracy: 42.630\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [124]: Training Loss: 1.521082693, Training Accuracy: 57.164\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.071916\n",
            "--------------------------------------------------\n",
            "Iteration: 124/300 - True epochs: 150\n",
            "Local Step 124: Test Loss: 2.330997482, Test Accuracy: 42.200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [125]: Training Loss: 1.490161913, Training Accuracy: 58.340\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.619630\n",
            "--------------------------------------------------\n",
            "Iteration: 125/300 - True epochs: 150\n",
            "Local Step 125: Test Loss: 2.430800760, Test Accuracy: 42.900\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [126]: Training Loss: 1.529211666, Training Accuracy: 56.556\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.004337\n",
            "--------------------------------------------------\n",
            "Iteration: 126/300 - True epochs: 150\n",
            "Local Step 126: Test Loss: 2.459029628, Test Accuracy: 41.770\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [127]: Training Loss: 1.487772101, Training Accuracy: 57.980\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.075034\n",
            "--------------------------------------------------\n",
            "Iteration: 127/300 - True epochs: 150\n",
            "Local Step 127: Test Loss: 2.438335651, Test Accuracy: 42.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [128]: Training Loss: 1.520188284, Training Accuracy: 56.672\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.162441\n",
            "--------------------------------------------------\n",
            "Iteration: 128/300 - True epochs: 150\n",
            "Local Step 128: Test Loss: 2.445298354, Test Accuracy: 41.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [129]: Training Loss: 1.500040796, Training Accuracy: 57.672\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.173065\n",
            "--------------------------------------------------\n",
            "Iteration: 129/300 - True epochs: 150\n",
            "Local Step 129: Test Loss: 2.442378384, Test Accuracy: 43.240\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [130]: Training Loss: 1.521390071, Training Accuracy: 56.768\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.369153\n",
            "--------------------------------------------------\n",
            "Iteration: 130/300 - True epochs: 150\n",
            "Local Step 130: Test Loss: 2.443131197, Test Accuracy: 42.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [131]: Training Loss: 1.484811995, Training Accuracy: 57.936\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.356212\n",
            "--------------------------------------------------\n",
            "Iteration: 131/300 - True epochs: 150\n",
            "Local Step 131: Test Loss: 2.418521009, Test Accuracy: 42.260\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [132]: Training Loss: 1.509393702, Training Accuracy: 56.960\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.069793\n",
            "--------------------------------------------------\n",
            "Iteration: 132/300 - True epochs: 150\n",
            "Local Step 132: Test Loss: 2.398621136, Test Accuracy: 42.960\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [133]: Training Loss: 1.469441585, Training Accuracy: 58.000\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.246597\n",
            "--------------------------------------------------\n",
            "Iteration: 133/300 - True epochs: 150\n",
            "Local Step 133: Test Loss: 2.412243688, Test Accuracy: 43.640\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [134]: Training Loss: 1.517865845, Training Accuracy: 57.224\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.716800\n",
            "--------------------------------------------------\n",
            "Iteration: 134/300 - True epochs: 150\n",
            "Local Step 134: Test Loss: 2.466687226, Test Accuracy: 42.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [135]: Training Loss: 1.460598661, Training Accuracy: 58.464\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.174656\n",
            "--------------------------------------------------\n",
            "Iteration: 135/300 - True epochs: 150\n",
            "Local Step 135: Test Loss: 2.375568887, Test Accuracy: 43.480\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [136]: Training Loss: 1.471348511, Training Accuracy: 57.824\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.790121\n",
            "--------------------------------------------------\n",
            "Iteration: 136/300 - True epochs: 150\n",
            "Local Step 136: Test Loss: 2.515237546, Test Accuracy: 41.220\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [137]: Training Loss: 1.486528497, Training Accuracy: 57.828\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.657218\n",
            "--------------------------------------------------\n",
            "Iteration: 137/300 - True epochs: 150\n",
            "Local Step 137: Test Loss: 2.362846441, Test Accuracy: 43.910\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [138]: Training Loss: 1.448988298, Training Accuracy: 58.648\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.411874\n",
            "--------------------------------------------------\n",
            "Iteration: 138/300 - True epochs: 150\n",
            "Local Step 138: Test Loss: 2.457177028, Test Accuracy: 43.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [139]: Training Loss: 1.473434451, Training Accuracy: 57.860\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.729422\n",
            "--------------------------------------------------\n",
            "Iteration: 139/300 - True epochs: 150\n",
            "Local Step 139: Test Loss: 2.371517029, Test Accuracy: 42.480\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [140]: Training Loss: 1.468902042, Training Accuracy: 58.452\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.354306\n",
            "--------------------------------------------------\n",
            "Iteration: 140/300 - True epochs: 150\n",
            "Local Step 140: Test Loss: 2.449704791, Test Accuracy: 41.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [141]: Training Loss: 1.488478193, Training Accuracy: 57.444\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.815269\n",
            "--------------------------------------------------\n",
            "Iteration: 141/300 - True epochs: 150\n",
            "Local Step 141: Test Loss: 2.516453834, Test Accuracy: 41.830\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [142]: Training Loss: 1.444334126, Training Accuracy: 58.696\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.783422\n",
            "--------------------------------------------------\n",
            "Iteration: 142/300 - True epochs: 150\n",
            "Local Step 142: Test Loss: 2.404272256, Test Accuracy: 43.840\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [143]: Training Loss: 1.477036433, Training Accuracy: 57.892\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.687238\n",
            "--------------------------------------------------\n",
            "Iteration: 143/300 - True epochs: 150\n",
            "Local Step 143: Test Loss: 2.439977711, Test Accuracy: 42.460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [144]: Training Loss: 1.452582020, Training Accuracy: 59.080\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.767365\n",
            "--------------------------------------------------\n",
            "Iteration: 144/300 - True epochs: 150\n",
            "Local Step 144: Test Loss: 2.426954899, Test Accuracy: 43.540\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [145]: Training Loss: 1.494384428, Training Accuracy: 57.492\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.479749\n",
            "--------------------------------------------------\n",
            "Iteration: 145/300 - True epochs: 150\n",
            "Local Step 145: Test Loss: 2.495509651, Test Accuracy: 42.320\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [146]: Training Loss: 1.429858987, Training Accuracy: 58.876\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.808482\n",
            "--------------------------------------------------\n",
            "Iteration: 146/300 - True epochs: 150\n",
            "Local Step 146: Test Loss: 2.459731879, Test Accuracy: 42.730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [147]: Training Loss: 1.466008549, Training Accuracy: 58.156\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.817061\n",
            "--------------------------------------------------\n",
            "Iteration: 147/300 - True epochs: 150\n",
            "Local Step 147: Test Loss: 2.434403577, Test Accuracy: 42.180\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [148]: Training Loss: 1.436731146, Training Accuracy: 59.168\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.068363\n",
            "--------------------------------------------------\n",
            "Iteration: 148/300 - True epochs: 150\n",
            "Local Step 148: Test Loss: 2.440872923, Test Accuracy: 42.650\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [149]: Training Loss: 1.456572163, Training Accuracy: 58.292\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.942682\n",
            "--------------------------------------------------\n",
            "Iteration: 149/300 - True epochs: 150\n",
            "Local Step 149: Test Loss: 2.439800342, Test Accuracy: 42.420\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [150]: Training Loss: 1.439225933, Training Accuracy: 59.180\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.061281\n",
            "--------------------------------------------------\n",
            "Iteration: 150/300 - True epochs: 150\n",
            "Local Step 150: Test Loss: 2.480655754, Test Accuracy: 43.330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [151]: Training Loss: 1.460389635, Training Accuracy: 58.368\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.124708\n",
            "--------------------------------------------------\n",
            "Iteration: 151/300 - True epochs: 150\n",
            "Local Step 151: Test Loss: 2.473668514, Test Accuracy: 42.290\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [152]: Training Loss: 1.431217319, Training Accuracy: 59.000\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.220493\n",
            "--------------------------------------------------\n",
            "Iteration: 152/300 - True epochs: 150\n",
            "Local Step 152: Test Loss: 2.433812841, Test Accuracy: 43.120\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [153]: Training Loss: 1.461733385, Training Accuracy: 58.208\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.855685\n",
            "--------------------------------------------------\n",
            "Iteration: 153/300 - True epochs: 150\n",
            "Local Step 153: Test Loss: 2.423473006, Test Accuracy: 42.360\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [154]: Training Loss: 1.423075076, Training Accuracy: 59.484\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.445452\n",
            "--------------------------------------------------\n",
            "Iteration: 154/300 - True epochs: 150\n",
            "Local Step 154: Test Loss: 2.424990893, Test Accuracy: 43.710\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [155]: Training Loss: 1.449331375, Training Accuracy: 58.380\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.082797\n",
            "--------------------------------------------------\n",
            "Iteration: 155/300 - True epochs: 150\n",
            "Local Step 155: Test Loss: 2.549938492, Test Accuracy: 40.970\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [156]: Training Loss: 1.401414246, Training Accuracy: 60.344\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.330510\n",
            "--------------------------------------------------\n",
            "Iteration: 156/300 - True epochs: 150\n",
            "Local Step 156: Test Loss: 2.493363295, Test Accuracy: 42.050\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [157]: Training Loss: 1.453028822, Training Accuracy: 58.464\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.077999\n",
            "--------------------------------------------------\n",
            "Iteration: 157/300 - True epochs: 150\n",
            "Local Step 157: Test Loss: 2.528478941, Test Accuracy: 41.720\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [158]: Training Loss: 1.428803340, Training Accuracy: 59.488\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.602661\n",
            "--------------------------------------------------\n",
            "Iteration: 158/300 - True epochs: 150\n",
            "Local Step 158: Test Loss: 2.461252400, Test Accuracy: 43.470\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [159]: Training Loss: 1.467230911, Training Accuracy: 58.024\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.816154\n",
            "--------------------------------------------------\n",
            "Iteration: 159/300 - True epochs: 150\n",
            "Local Step 159: Test Loss: 2.419782135, Test Accuracy: 42.480\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [160]: Training Loss: 1.395513078, Training Accuracy: 60.512\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.868261\n",
            "--------------------------------------------------\n",
            "Iteration: 160/300 - True epochs: 150\n",
            "Local Step 160: Test Loss: 2.478763173, Test Accuracy: 43.590\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [161]: Training Loss: 1.442747430, Training Accuracy: 59.012\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.251265\n",
            "--------------------------------------------------\n",
            "Iteration: 161/300 - True epochs: 150\n",
            "Local Step 161: Test Loss: 2.422231793, Test Accuracy: 42.550\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [162]: Training Loss: 1.420903121, Training Accuracy: 59.608\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.082555\n",
            "--------------------------------------------------\n",
            "Iteration: 162/300 - True epochs: 150\n",
            "Local Step 162: Test Loss: 2.429615439, Test Accuracy: 43.740\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [163]: Training Loss: 1.414657167, Training Accuracy: 59.308\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.213812\n",
            "--------------------------------------------------\n",
            "Iteration: 163/300 - True epochs: 150\n",
            "Local Step 163: Test Loss: 2.453085028, Test Accuracy: 42.880\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [164]: Training Loss: 1.391042488, Training Accuracy: 60.148\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.230873\n",
            "--------------------------------------------------\n",
            "Iteration: 164/300 - True epochs: 150\n",
            "Local Step 164: Test Loss: 2.538965035, Test Accuracy: 42.200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [165]: Training Loss: 1.417851409, Training Accuracy: 59.812\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.017470\n",
            "--------------------------------------------------\n",
            "Iteration: 165/300 - True epochs: 150\n",
            "Local Step 165: Test Loss: 2.404810018, Test Accuracy: 43.920\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [166]: Training Loss: 1.436243917, Training Accuracy: 58.952\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.071271\n",
            "--------------------------------------------------\n",
            "Iteration: 166/300 - True epochs: 150\n",
            "Local Step 166: Test Loss: 2.528008622, Test Accuracy: 41.730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [167]: Training Loss: 1.391472107, Training Accuracy: 60.568\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.895445\n",
            "--------------------------------------------------\n",
            "Iteration: 167/300 - True epochs: 150\n",
            "Local Step 167: Test Loss: 2.452545762, Test Accuracy: 43.590\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [168]: Training Loss: 1.423600820, Training Accuracy: 59.192\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.853913\n",
            "--------------------------------------------------\n",
            "Iteration: 168/300 - True epochs: 150\n",
            "Local Step 168: Test Loss: 2.444590311, Test Accuracy: 42.530\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [169]: Training Loss: 1.384296401, Training Accuracy: 60.508\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.865506\n",
            "--------------------------------------------------\n",
            "Iteration: 169/300 - True epochs: 150\n",
            "Local Step 169: Test Loss: 2.520838427, Test Accuracy: 42.600\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [170]: Training Loss: 1.399404471, Training Accuracy: 59.492\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.026462\n",
            "--------------------------------------------------\n",
            "Iteration: 170/300 - True epochs: 150\n",
            "Local Step 170: Test Loss: 2.442123867, Test Accuracy: 42.910\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [171]: Training Loss: 1.401749141, Training Accuracy: 59.796\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.920823\n",
            "--------------------------------------------------\n",
            "Iteration: 171/300 - True epochs: 150\n",
            "Local Step 171: Test Loss: 2.457414204, Test Accuracy: 43.080\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [172]: Training Loss: 1.417820585, Training Accuracy: 59.516\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.948499\n",
            "--------------------------------------------------\n",
            "Iteration: 172/300 - True epochs: 150\n",
            "Local Step 172: Test Loss: 2.418578607, Test Accuracy: 43.430\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [173]: Training Loss: 1.399025792, Training Accuracy: 60.212\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.875836\n",
            "--------------------------------------------------\n",
            "Iteration: 173/300 - True epochs: 150\n",
            "Local Step 173: Test Loss: 2.457191243, Test Accuracy: 43.680\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [174]: Training Loss: 1.410434637, Training Accuracy: 59.572\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.898130\n",
            "--------------------------------------------------\n",
            "Iteration: 174/300 - True epochs: 150\n",
            "Local Step 174: Test Loss: 2.492517151, Test Accuracy: 41.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [175]: Training Loss: 1.389345649, Training Accuracy: 60.140\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.026858\n",
            "--------------------------------------------------\n",
            "Iteration: 175/300 - True epochs: 150\n",
            "Local Step 175: Test Loss: 2.431340760, Test Accuracy: 43.530\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [176]: Training Loss: 1.409632831, Training Accuracy: 59.580\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.346122\n",
            "--------------------------------------------------\n",
            "Iteration: 176/300 - True epochs: 150\n",
            "Local Step 176: Test Loss: 2.425367806, Test Accuracy: 42.420\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [177]: Training Loss: 1.364399757, Training Accuracy: 60.968\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.873055\n",
            "--------------------------------------------------\n",
            "Iteration: 177/300 - True epochs: 150\n",
            "Local Step 177: Test Loss: 2.451453297, Test Accuracy: 43.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [178]: Training Loss: 1.408525555, Training Accuracy: 59.588\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.889261\n",
            "--------------------------------------------------\n",
            "Iteration: 178/300 - True epochs: 150\n",
            "Local Step 178: Test Loss: 2.411965634, Test Accuracy: 43.280\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [179]: Training Loss: 1.373747294, Training Accuracy: 60.748\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.184320\n",
            "--------------------------------------------------\n",
            "Iteration: 179/300 - True epochs: 150\n",
            "Local Step 179: Test Loss: 2.455862989, Test Accuracy: 43.120\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [180]: Training Loss: 1.399818199, Training Accuracy: 59.808\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.126934\n",
            "--------------------------------------------------\n",
            "Iteration: 180/300 - True epochs: 150\n",
            "Local Step 180: Test Loss: 2.451768994, Test Accuracy: 43.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [181]: Training Loss: 1.366510724, Training Accuracy: 60.824\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.557115\n",
            "--------------------------------------------------\n",
            "Iteration: 181/300 - True epochs: 150\n",
            "Local Step 181: Test Loss: 2.535652265, Test Accuracy: 42.340\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [182]: Training Loss: 1.405757552, Training Accuracy: 59.760\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.919626\n",
            "--------------------------------------------------\n",
            "Iteration: 182/300 - True epochs: 150\n",
            "Local Step 182: Test Loss: 2.470821421, Test Accuracy: 42.540\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [183]: Training Loss: 1.383203516, Training Accuracy: 60.800\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.915128\n",
            "--------------------------------------------------\n",
            "Iteration: 183/300 - True epochs: 150\n",
            "Local Step 183: Test Loss: 2.566644448, Test Accuracy: 43.050\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [184]: Training Loss: 1.397274279, Training Accuracy: 59.996\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.635890\n",
            "--------------------------------------------------\n",
            "Iteration: 184/300 - True epochs: 150\n",
            "Local Step 184: Test Loss: 2.493922479, Test Accuracy: 42.790\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [185]: Training Loss: 1.368526651, Training Accuracy: 60.744\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.102839\n",
            "--------------------------------------------------\n",
            "Iteration: 185/300 - True epochs: 150\n",
            "Local Step 185: Test Loss: 2.448899177, Test Accuracy: 44.050\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [186]: Training Loss: 1.390989602, Training Accuracy: 59.852\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.054556\n",
            "--------------------------------------------------\n",
            "Iteration: 186/300 - True epochs: 150\n",
            "Local Step 186: Test Loss: 2.414883611, Test Accuracy: 43.130\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [187]: Training Loss: 1.335962327, Training Accuracy: 61.780\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.964597\n",
            "--------------------------------------------------\n",
            "Iteration: 187/300 - True epochs: 150\n",
            "Local Step 187: Test Loss: 2.446106630, Test Accuracy: 43.730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [188]: Training Loss: 1.378418940, Training Accuracy: 60.440\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.423978\n",
            "--------------------------------------------------\n",
            "Iteration: 188/300 - True epochs: 150\n",
            "Local Step 188: Test Loss: 2.441326881, Test Accuracy: 42.740\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [189]: Training Loss: 1.354733240, Training Accuracy: 61.180\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.777945\n",
            "--------------------------------------------------\n",
            "Iteration: 189/300 - True epochs: 150\n",
            "Local Step 189: Test Loss: 2.499136150, Test Accuracy: 43.790\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [190]: Training Loss: 1.394376273, Training Accuracy: 60.344\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.502228\n",
            "--------------------------------------------------\n",
            "Iteration: 190/300 - True epochs: 150\n",
            "Local Step 190: Test Loss: 2.523399930, Test Accuracy: 41.200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [191]: Training Loss: 1.375425602, Training Accuracy: 60.640\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.891622\n",
            "--------------------------------------------------\n",
            "Iteration: 191/300 - True epochs: 150\n",
            "Local Step 191: Test Loss: 2.510918061, Test Accuracy: 43.170\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [192]: Training Loss: 1.355917263, Training Accuracy: 61.308\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.052000\n",
            "--------------------------------------------------\n",
            "Iteration: 192/300 - True epochs: 150\n",
            "Local Step 192: Test Loss: 2.468377255, Test Accuracy: 42.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [193]: Training Loss: 1.401158770, Training Accuracy: 59.952\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.105756\n",
            "--------------------------------------------------\n",
            "Iteration: 193/300 - True epochs: 150\n",
            "Local Step 193: Test Loss: 2.495261280, Test Accuracy: 43.140\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [194]: Training Loss: 1.346089345, Training Accuracy: 61.504\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.810895\n",
            "--------------------------------------------------\n",
            "Iteration: 194/300 - True epochs: 150\n",
            "Local Step 194: Test Loss: 2.528426069, Test Accuracy: 43.020\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [195]: Training Loss: 1.361481685, Training Accuracy: 60.836\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.815531\n",
            "--------------------------------------------------\n",
            "Iteration: 195/300 - True epochs: 150\n",
            "Local Step 195: Test Loss: 2.445679081, Test Accuracy: 42.840\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [196]: Training Loss: 1.334475657, Training Accuracy: 61.676\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.949970\n",
            "--------------------------------------------------\n",
            "Iteration: 196/300 - True epochs: 150\n",
            "Local Step 196: Test Loss: 2.554653715, Test Accuracy: 43.180\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [197]: Training Loss: 1.374136870, Training Accuracy: 60.444\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.840410\n",
            "--------------------------------------------------\n",
            "Iteration: 197/300 - True epochs: 150\n",
            "Local Step 197: Test Loss: 2.474954485, Test Accuracy: 42.770\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [198]: Training Loss: 1.338289787, Training Accuracy: 61.292\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.231446\n",
            "--------------------------------------------------\n",
            "Iteration: 198/300 - True epochs: 150\n",
            "Local Step 198: Test Loss: 2.454755764, Test Accuracy: 43.460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [199]: Training Loss: 1.368663527, Training Accuracy: 60.424\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.437227\n",
            "--------------------------------------------------\n",
            "Iteration: 199/300 - True epochs: 150\n",
            "Local Step 199: Test Loss: 2.457136804, Test Accuracy: 43.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [200]: Training Loss: 1.360552422, Training Accuracy: 61.036\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.877260\n",
            "--------------------------------------------------\n",
            "Iteration: 200/300 - True epochs: 150\n",
            "Local Step 200: Test Loss: 2.486099043, Test Accuracy: 42.720\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [201]: Training Loss: 1.359686015, Training Accuracy: 60.936\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.825234\n",
            "--------------------------------------------------\n",
            "Iteration: 201/300 - True epochs: 150\n",
            "Local Step 201: Test Loss: 2.487225533, Test Accuracy: 42.800\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [202]: Training Loss: 1.346466447, Training Accuracy: 61.316\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.203577\n",
            "--------------------------------------------------\n",
            "Iteration: 202/300 - True epochs: 150\n",
            "Local Step 202: Test Loss: 2.533778282, Test Accuracy: 42.820\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [203]: Training Loss: 1.357826309, Training Accuracy: 60.904\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.856060\n",
            "--------------------------------------------------\n",
            "Iteration: 203/300 - True epochs: 150\n",
            "Local Step 203: Test Loss: 2.468724886, Test Accuracy: 42.250\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [204]: Training Loss: 1.331506377, Training Accuracy: 61.832\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.181202\n",
            "--------------------------------------------------\n",
            "Iteration: 204/300 - True epochs: 150\n",
            "Local Step 204: Test Loss: 2.500889073, Test Accuracy: 42.540\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [205]: Training Loss: 1.367327054, Training Accuracy: 60.924\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.155590\n",
            "--------------------------------------------------\n",
            "Iteration: 205/300 - True epochs: 150\n",
            "Local Step 205: Test Loss: 2.440065552, Test Accuracy: 43.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [206]: Training Loss: 1.336244747, Training Accuracy: 61.384\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.842299\n",
            "--------------------------------------------------\n",
            "Iteration: 206/300 - True epochs: 150\n",
            "Local Step 206: Test Loss: 2.490532799, Test Accuracy: 43.490\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [207]: Training Loss: 1.347347367, Training Accuracy: 61.116\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.909136\n",
            "--------------------------------------------------\n",
            "Iteration: 207/300 - True epochs: 150\n",
            "Local Step 207: Test Loss: 2.454958311, Test Accuracy: 42.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [208]: Training Loss: 1.319497065, Training Accuracy: 61.908\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.173018\n",
            "--------------------------------------------------\n",
            "Iteration: 208/300 - True epochs: 150\n",
            "Local Step 208: Test Loss: 2.531908706, Test Accuracy: 43.530\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [209]: Training Loss: 1.349788413, Training Accuracy: 61.260\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.880174\n",
            "--------------------------------------------------\n",
            "Iteration: 209/300 - True epochs: 150\n",
            "Local Step 209: Test Loss: 2.427398385, Test Accuracy: 42.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [210]: Training Loss: 1.323625501, Training Accuracy: 61.976\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.536861\n",
            "--------------------------------------------------\n",
            "Iteration: 210/300 - True epochs: 150\n",
            "Local Step 210: Test Loss: 2.524636273, Test Accuracy: 42.600\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [211]: Training Loss: 1.342425236, Training Accuracy: 61.284\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.487703\n",
            "--------------------------------------------------\n",
            "Iteration: 211/300 - True epochs: 150\n",
            "Local Step 211: Test Loss: 2.494500361, Test Accuracy: 41.930\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [212]: Training Loss: 1.323545344, Training Accuracy: 62.028\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.621798\n",
            "--------------------------------------------------\n",
            "Iteration: 212/300 - True epochs: 150\n",
            "Local Step 212: Test Loss: 2.508169873, Test Accuracy: 43.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [213]: Training Loss: 1.356029343, Training Accuracy: 60.740\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.893290\n",
            "--------------------------------------------------\n",
            "Iteration: 213/300 - True epochs: 150\n",
            "Local Step 213: Test Loss: 2.504486983, Test Accuracy: 41.480\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [214]: Training Loss: 1.331264502, Training Accuracy: 61.804\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.680395\n",
            "--------------------------------------------------\n",
            "Iteration: 214/300 - True epochs: 150\n",
            "Local Step 214: Test Loss: 2.466188102, Test Accuracy: 43.720\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [215]: Training Loss: 1.348344200, Training Accuracy: 61.316\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.131311\n",
            "--------------------------------------------------\n",
            "Iteration: 215/300 - True epochs: 150\n",
            "Local Step 215: Test Loss: 2.406497068, Test Accuracy: 44.080\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [216]: Training Loss: 1.329280066, Training Accuracy: 61.960\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.788816\n",
            "--------------------------------------------------\n",
            "Iteration: 216/300 - True epochs: 150\n",
            "Local Step 216: Test Loss: 2.616812824, Test Accuracy: 42.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [217]: Training Loss: 1.349444706, Training Accuracy: 61.220\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.295387\n",
            "--------------------------------------------------\n",
            "Iteration: 217/300 - True epochs: 150\n",
            "Local Step 217: Test Loss: 2.526136936, Test Accuracy: 42.120\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [218]: Training Loss: 1.323637118, Training Accuracy: 62.060\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.705279\n",
            "--------------------------------------------------\n",
            "Iteration: 218/300 - True epochs: 150\n",
            "Local Step 218: Test Loss: 2.525024801, Test Accuracy: 42.320\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [219]: Training Loss: 1.341982084, Training Accuracy: 61.368\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.751054\n",
            "--------------------------------------------------\n",
            "Iteration: 219/300 - True epochs: 150\n",
            "Local Step 219: Test Loss: 2.444938184, Test Accuracy: 42.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [220]: Training Loss: 1.322807634, Training Accuracy: 62.036\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.876811\n",
            "--------------------------------------------------\n",
            "Iteration: 220/300 - True epochs: 150\n",
            "Local Step 220: Test Loss: 2.534552482, Test Accuracy: 42.800\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [221]: Training Loss: 1.332173179, Training Accuracy: 61.932\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.821583\n",
            "--------------------------------------------------\n",
            "Iteration: 221/300 - True epochs: 150\n",
            "Local Step 221: Test Loss: 2.444205412, Test Accuracy: 43.430\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [222]: Training Loss: 1.348146597, Training Accuracy: 61.004\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.378696\n",
            "--------------------------------------------------\n",
            "Iteration: 222/300 - True epochs: 150\n",
            "Local Step 222: Test Loss: 2.485425834, Test Accuracy: 42.490\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [223]: Training Loss: 1.297462520, Training Accuracy: 62.652\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.948632\n",
            "--------------------------------------------------\n",
            "Iteration: 223/300 - True epochs: 150\n",
            "Local Step 223: Test Loss: 2.464958376, Test Accuracy: 44.380\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [224]: Training Loss: 1.337962174, Training Accuracy: 61.348\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.116378\n",
            "--------------------------------------------------\n",
            "Iteration: 224/300 - True epochs: 150\n",
            "Local Step 224: Test Loss: 2.488401014, Test Accuracy: 43.590\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [225]: Training Loss: 1.292359813, Training Accuracy: 62.744\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.001073\n",
            "--------------------------------------------------\n",
            "Iteration: 225/300 - True epochs: 150\n",
            "Local Step 225: Test Loss: 2.444082632, Test Accuracy: 43.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [226]: Training Loss: 1.356482677, Training Accuracy: 61.072\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.133312\n",
            "--------------------------------------------------\n",
            "Iteration: 226/300 - True epochs: 150\n",
            "Local Step 226: Test Loss: 2.468179284, Test Accuracy: 42.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [227]: Training Loss: 1.305938816, Training Accuracy: 62.380\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.169234\n",
            "--------------------------------------------------\n",
            "Iteration: 227/300 - True epochs: 150\n",
            "Local Step 227: Test Loss: 2.522662832, Test Accuracy: 42.180\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [228]: Training Loss: 1.329439023, Training Accuracy: 61.676\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.941451\n",
            "--------------------------------------------------\n",
            "Iteration: 228/300 - True epochs: 150\n",
            "Local Step 228: Test Loss: 2.506320455, Test Accuracy: 42.290\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [229]: Training Loss: 1.310060815, Training Accuracy: 62.056\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.761254\n",
            "--------------------------------------------------\n",
            "Iteration: 229/300 - True epochs: 150\n",
            "Local Step 229: Test Loss: 2.563845744, Test Accuracy: 42.940\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [230]: Training Loss: 1.343081663, Training Accuracy: 61.312\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.253396\n",
            "--------------------------------------------------\n",
            "Iteration: 230/300 - True epochs: 150\n",
            "Local Step 230: Test Loss: 2.484595021, Test Accuracy: 42.690\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [231]: Training Loss: 1.309472373, Training Accuracy: 62.280\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.727136\n",
            "--------------------------------------------------\n",
            "Iteration: 231/300 - True epochs: 150\n",
            "Local Step 231: Test Loss: 2.499531864, Test Accuracy: 44.130\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [232]: Training Loss: 1.339649705, Training Accuracy: 61.184\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.795218\n",
            "--------------------------------------------------\n",
            "Iteration: 232/300 - True epochs: 150\n",
            "Local Step 232: Test Loss: 2.454269322, Test Accuracy: 43.220\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [233]: Training Loss: 1.302143968, Training Accuracy: 62.772\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.821124\n",
            "--------------------------------------------------\n",
            "Iteration: 233/300 - True epochs: 150\n",
            "Local Step 233: Test Loss: 2.579667240, Test Accuracy: 42.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [234]: Training Loss: 1.325520618, Training Accuracy: 61.684\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.129205\n",
            "--------------------------------------------------\n",
            "Iteration: 234/300 - True epochs: 150\n",
            "Local Step 234: Test Loss: 2.531430805, Test Accuracy: 42.960\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [235]: Training Loss: 1.277289793, Training Accuracy: 63.272\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.506796\n",
            "--------------------------------------------------\n",
            "Iteration: 235/300 - True epochs: 150\n",
            "Local Step 235: Test Loss: 2.540192701, Test Accuracy: 42.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [236]: Training Loss: 1.331779824, Training Accuracy: 61.400\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.789410\n",
            "--------------------------------------------------\n",
            "Iteration: 236/300 - True epochs: 150\n",
            "Local Step 236: Test Loss: 2.492439775, Test Accuracy: 42.430\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [237]: Training Loss: 1.301788324, Training Accuracy: 62.620\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.846769\n",
            "--------------------------------------------------\n",
            "Iteration: 237/300 - True epochs: 150\n",
            "Local Step 237: Test Loss: 2.541898409, Test Accuracy: 42.940\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [238]: Training Loss: 1.334525427, Training Accuracy: 61.192\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.418116\n",
            "--------------------------------------------------\n",
            "Iteration: 238/300 - True epochs: 150\n",
            "Local Step 238: Test Loss: 2.582185624, Test Accuracy: 42.260\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [239]: Training Loss: 1.302646313, Training Accuracy: 62.476\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.157512\n",
            "--------------------------------------------------\n",
            "Iteration: 239/300 - True epochs: 150\n",
            "Local Step 239: Test Loss: 2.471441559, Test Accuracy: 44.270\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [240]: Training Loss: 1.325060943, Training Accuracy: 61.728\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.098804\n",
            "--------------------------------------------------\n",
            "Iteration: 240/300 - True epochs: 150\n",
            "Local Step 240: Test Loss: 2.466303781, Test Accuracy: 43.360\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [241]: Training Loss: 1.295234781, Training Accuracy: 62.872\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.517144\n",
            "--------------------------------------------------\n",
            "Iteration: 241/300 - True epochs: 150\n",
            "Local Step 241: Test Loss: 2.503237183, Test Accuracy: 43.660\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [242]: Training Loss: 1.320025309, Training Accuracy: 62.256\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.059278\n",
            "--------------------------------------------------\n",
            "Iteration: 242/300 - True epochs: 150\n",
            "Local Step 242: Test Loss: 2.544652337, Test Accuracy: 42.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [243]: Training Loss: 1.270258409, Training Accuracy: 63.368\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.811012\n",
            "--------------------------------------------------\n",
            "Iteration: 243/300 - True epochs: 150\n",
            "Local Step 243: Test Loss: 2.570203572, Test Accuracy: 43.440\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [244]: Training Loss: 1.323106421, Training Accuracy: 62.208\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.081304\n",
            "--------------------------------------------------\n",
            "Iteration: 244/300 - True epochs: 150\n",
            "Local Step 244: Test Loss: 2.531613138, Test Accuracy: 42.920\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [245]: Training Loss: 1.299498853, Training Accuracy: 62.336\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.887505\n",
            "--------------------------------------------------\n",
            "Iteration: 245/300 - True epochs: 150\n",
            "Local Step 245: Test Loss: 2.472096054, Test Accuracy: 43.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [246]: Training Loss: 1.331960482, Training Accuracy: 61.684\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.618970\n",
            "--------------------------------------------------\n",
            "Iteration: 246/300 - True epochs: 150\n",
            "Local Step 246: Test Loss: 2.500351620, Test Accuracy: 42.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [247]: Training Loss: 1.286160020, Training Accuracy: 62.928\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.788955\n",
            "--------------------------------------------------\n",
            "Iteration: 247/300 - True epochs: 150\n",
            "Local Step 247: Test Loss: 2.494907773, Test Accuracy: 44.450\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [248]: Training Loss: 1.306015763, Training Accuracy: 62.664\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.801161\n",
            "--------------------------------------------------\n",
            "Iteration: 248/300 - True epochs: 150\n",
            "Local Step 248: Test Loss: 2.484681003, Test Accuracy: 43.970\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [249]: Training Loss: 1.321997957, Training Accuracy: 61.936\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.156187\n",
            "--------------------------------------------------\n",
            "Iteration: 249/300 - True epochs: 150\n",
            "Local Step 249: Test Loss: 2.520461280, Test Accuracy: 42.850\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [250]: Training Loss: 1.265393552, Training Accuracy: 63.848\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.144943\n",
            "--------------------------------------------------\n",
            "Iteration: 250/300 - True epochs: 150\n",
            "Local Step 250: Test Loss: 2.553888908, Test Accuracy: 42.720\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [251]: Training Loss: 1.310608695, Training Accuracy: 62.284\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.107592\n",
            "--------------------------------------------------\n",
            "Iteration: 251/300 - True epochs: 150\n",
            "Local Step 251: Test Loss: 2.544657024, Test Accuracy: 42.480\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [252]: Training Loss: 1.279063743, Training Accuracy: 63.120\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.105982\n",
            "--------------------------------------------------\n",
            "Iteration: 252/300 - True epochs: 150\n",
            "Local Step 252: Test Loss: 2.491242227, Test Accuracy: 43.930\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [253]: Training Loss: 1.329276529, Training Accuracy: 61.744\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.654191\n",
            "--------------------------------------------------\n",
            "Iteration: 253/300 - True epochs: 150\n",
            "Local Step 253: Test Loss: 2.583895096, Test Accuracy: 42.840\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [254]: Training Loss: 1.269286926, Training Accuracy: 63.332\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.142964\n",
            "--------------------------------------------------\n",
            "Iteration: 254/300 - True epochs: 150\n",
            "Local Step 254: Test Loss: 2.540264467, Test Accuracy: 43.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [255]: Training Loss: 1.294207246, Training Accuracy: 62.672\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.030449\n",
            "--------------------------------------------------\n",
            "Iteration: 255/300 - True epochs: 150\n",
            "Local Step 255: Test Loss: 2.556633664, Test Accuracy: 42.600\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [256]: Training Loss: 1.295816631, Training Accuracy: 62.612\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.906284\n",
            "--------------------------------------------------\n",
            "Iteration: 256/300 - True epochs: 150\n",
            "Local Step 256: Test Loss: 2.486758638, Test Accuracy: 43.310\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [257]: Training Loss: 1.330195407, Training Accuracy: 61.936\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.079868\n",
            "--------------------------------------------------\n",
            "Iteration: 257/300 - True epochs: 150\n",
            "Local Step 257: Test Loss: 2.592393622, Test Accuracy: 42.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [258]: Training Loss: 1.313150157, Training Accuracy: 62.336\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.105191\n",
            "--------------------------------------------------\n",
            "Iteration: 258/300 - True epochs: 150\n",
            "Local Step 258: Test Loss: 2.583327526, Test Accuracy: 43.530\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [259]: Training Loss: 1.295934053, Training Accuracy: 62.188\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.803294\n",
            "--------------------------------------------------\n",
            "Iteration: 259/300 - True epochs: 150\n",
            "Local Step 259: Test Loss: 2.568792008, Test Accuracy: 42.630\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [260]: Training Loss: 1.282874034, Training Accuracy: 63.360\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.608086\n",
            "--------------------------------------------------\n",
            "Iteration: 260/300 - True epochs: 150\n",
            "Local Step 260: Test Loss: 2.549707755, Test Accuracy: 44.310\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [261]: Training Loss: 1.315814797, Training Accuracy: 61.680\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.139228\n",
            "--------------------------------------------------\n",
            "Iteration: 261/300 - True epochs: 150\n",
            "Local Step 261: Test Loss: 2.603288686, Test Accuracy: 41.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [262]: Training Loss: 1.266204648, Training Accuracy: 63.480\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.760348\n",
            "--------------------------------------------------\n",
            "Iteration: 262/300 - True epochs: 150\n",
            "Local Step 262: Test Loss: 2.490660181, Test Accuracy: 43.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [263]: Training Loss: 1.317402290, Training Accuracy: 62.224\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.848322\n",
            "--------------------------------------------------\n",
            "Iteration: 263/300 - True epochs: 150\n",
            "Local Step 263: Test Loss: 2.519534206, Test Accuracy: 42.590\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [264]: Training Loss: 1.286337801, Training Accuracy: 62.992\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.898772\n",
            "--------------------------------------------------\n",
            "Iteration: 264/300 - True epochs: 150\n",
            "Local Step 264: Test Loss: 2.591658895, Test Accuracy: 43.280\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [265]: Training Loss: 1.296919157, Training Accuracy: 62.760\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.705063\n",
            "--------------------------------------------------\n",
            "Iteration: 265/300 - True epochs: 150\n",
            "Local Step 265: Test Loss: 2.577821612, Test Accuracy: 43.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [266]: Training Loss: 1.266463935, Training Accuracy: 63.132\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.647979\n",
            "--------------------------------------------------\n",
            "Iteration: 266/300 - True epochs: 150\n",
            "Local Step 266: Test Loss: 2.573585612, Test Accuracy: 43.390\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [267]: Training Loss: 1.307879674, Training Accuracy: 62.452\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.467585\n",
            "--------------------------------------------------\n",
            "Iteration: 267/300 - True epochs: 150\n",
            "Local Step 267: Test Loss: 2.455007924, Test Accuracy: 43.640\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [268]: Training Loss: 1.296356866, Training Accuracy: 63.048\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.755882\n",
            "--------------------------------------------------\n",
            "Iteration: 268/300 - True epochs: 150\n",
            "Local Step 268: Test Loss: 2.545889902, Test Accuracy: 42.850\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [269]: Training Loss: 1.274705423, Training Accuracy: 62.896\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.805983\n",
            "--------------------------------------------------\n",
            "Iteration: 269/300 - True epochs: 150\n",
            "Local Step 269: Test Loss: 2.615847593, Test Accuracy: 41.330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [270]: Training Loss: 1.273089824, Training Accuracy: 63.168\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.871233\n",
            "--------------------------------------------------\n",
            "Iteration: 270/300 - True epochs: 150\n",
            "Local Step 270: Test Loss: 2.503303767, Test Accuracy: 44.510\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [271]: Training Loss: 1.272263099, Training Accuracy: 63.112\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.887951\n",
            "--------------------------------------------------\n",
            "Iteration: 271/300 - True epochs: 150\n",
            "Local Step 271: Test Loss: 2.499320913, Test Accuracy: 43.080\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [272]: Training Loss: 1.260470734, Training Accuracy: 63.460\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.627698\n",
            "--------------------------------------------------\n",
            "Iteration: 272/300 - True epochs: 150\n",
            "Local Step 272: Test Loss: 2.496804180, Test Accuracy: 43.570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [273]: Training Loss: 1.312331167, Training Accuracy: 62.416\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.441566\n",
            "--------------------------------------------------\n",
            "Iteration: 273/300 - True epochs: 150\n",
            "Local Step 273: Test Loss: 2.535971962, Test Accuracy: 41.800\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [274]: Training Loss: 1.289364444, Training Accuracy: 62.936\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.980298\n",
            "--------------------------------------------------\n",
            "Iteration: 274/300 - True epochs: 150\n",
            "Local Step 274: Test Loss: 2.624175687, Test Accuracy: 42.170\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [275]: Training Loss: 1.271834877, Training Accuracy: 63.148\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.411934\n",
            "--------------------------------------------------\n",
            "Iteration: 275/300 - True epochs: 150\n",
            "Local Step 275: Test Loss: 2.542459163, Test Accuracy: 42.130\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [276]: Training Loss: 1.288467795, Training Accuracy: 63.040\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.846724\n",
            "--------------------------------------------------\n",
            "Iteration: 276/300 - True epochs: 150\n",
            "Local Step 276: Test Loss: 2.551907426, Test Accuracy: 42.290\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [277]: Training Loss: 1.239508159, Training Accuracy: 64.092\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.162813\n",
            "--------------------------------------------------\n",
            "Iteration: 277/300 - True epochs: 150\n",
            "Local Step 277: Test Loss: 2.644723857, Test Accuracy: 42.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [278]: Training Loss: 1.286302955, Training Accuracy: 62.592\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.288104\n",
            "--------------------------------------------------\n",
            "Iteration: 278/300 - True epochs: 150\n",
            "Local Step 278: Test Loss: 2.527455710, Test Accuracy: 43.000\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [279]: Training Loss: 1.261089843, Training Accuracy: 63.596\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.909778\n",
            "--------------------------------------------------\n",
            "Iteration: 279/300 - True epochs: 150\n",
            "Local Step 279: Test Loss: 2.602332168, Test Accuracy: 43.930\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [280]: Training Loss: 1.284759122, Training Accuracy: 62.532\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.725141\n",
            "--------------------------------------------------\n",
            "Iteration: 280/300 - True epochs: 150\n",
            "Local Step 280: Test Loss: 2.486112659, Test Accuracy: 43.520\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [281]: Training Loss: 1.256594237, Training Accuracy: 63.540\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.401200\n",
            "--------------------------------------------------\n",
            "Iteration: 281/300 - True epochs: 150\n",
            "Local Step 281: Test Loss: 2.557174986, Test Accuracy: 42.900\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [282]: Training Loss: 1.291568637, Training Accuracy: 62.608\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.803445\n",
            "--------------------------------------------------\n",
            "Iteration: 282/300 - True epochs: 150\n",
            "Local Step 282: Test Loss: 2.546368882, Test Accuracy: 42.370\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [283]: Training Loss: 1.268869392, Training Accuracy: 63.184\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.795453\n",
            "--------------------------------------------------\n",
            "Iteration: 283/300 - True epochs: 150\n",
            "Local Step 283: Test Loss: 2.537467997, Test Accuracy: 43.750\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [284]: Training Loss: 1.307946428, Training Accuracy: 62.416\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.808751\n",
            "--------------------------------------------------\n",
            "Iteration: 284/300 - True epochs: 150\n",
            "Local Step 284: Test Loss: 2.514098472, Test Accuracy: 42.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [285]: Training Loss: 1.273674947, Training Accuracy: 63.160\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.491062\n",
            "--------------------------------------------------\n",
            "Iteration: 285/300 - True epochs: 150\n",
            "Local Step 285: Test Loss: 2.509624411, Test Accuracy: 43.720\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [286]: Training Loss: 1.278385689, Training Accuracy: 62.960\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.822074\n",
            "--------------------------------------------------\n",
            "Iteration: 286/300 - True epochs: 150\n",
            "Local Step 286: Test Loss: 2.590593661, Test Accuracy: 41.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [287]: Training Loss: 1.269909250, Training Accuracy: 63.356\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.304976\n",
            "--------------------------------------------------\n",
            "Iteration: 287/300 - True epochs: 150\n",
            "Local Step 287: Test Loss: 2.627899231, Test Accuracy: 42.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [288]: Training Loss: 1.308690826, Training Accuracy: 61.988\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.038400\n",
            "--------------------------------------------------\n",
            "Iteration: 288/300 - True epochs: 150\n",
            "Local Step 288: Test Loss: 2.509814523, Test Accuracy: 42.350\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [289]: Training Loss: 1.254153833, Training Accuracy: 63.828\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:09.979375\n",
            "--------------------------------------------------\n",
            "Iteration: 289/300 - True epochs: 150\n",
            "Local Step 289: Test Loss: 2.571063755, Test Accuracy: 43.060\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [290]: Training Loss: 1.271984878, Training Accuracy: 63.332\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.388375\n",
            "--------------------------------------------------\n",
            "Iteration: 290/300 - True epochs: 150\n",
            "Local Step 290: Test Loss: 2.524607567, Test Accuracy: 41.790\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [291]: Training Loss: 1.280101093, Training Accuracy: 63.220\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.974053\n",
            "--------------------------------------------------\n",
            "Iteration: 291/300 - True epochs: 150\n",
            "Local Step 291: Test Loss: 2.513562433, Test Accuracy: 42.910\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [292]: Training Loss: 1.283154731, Training Accuracy: 62.716\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.641508\n",
            "--------------------------------------------------\n",
            "Iteration: 292/300 - True epochs: 150\n",
            "Local Step 292: Test Loss: 2.545837413, Test Accuracy: 43.020\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [293]: Training Loss: 1.274747406, Training Accuracy: 63.136\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.748760\n",
            "--------------------------------------------------\n",
            "Iteration: 293/300 - True epochs: 150\n",
            "Local Step 293: Test Loss: 2.543494318, Test Accuracy: 42.210\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [294]: Training Loss: 1.282393339, Training Accuracy: 62.844\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.276371\n",
            "--------------------------------------------------\n",
            "Iteration: 294/300 - True epochs: 150\n",
            "Local Step 294: Test Loss: 2.507006430, Test Accuracy: 42.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [295]: Training Loss: 1.278884495, Training Accuracy: 62.884\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.080696\n",
            "--------------------------------------------------\n",
            "Iteration: 295/300 - True epochs: 150\n",
            "Local Step 295: Test Loss: 2.554977918, Test Accuracy: 43.130\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [296]: Training Loss: 1.268223498, Training Accuracy: 63.224\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.040329\n",
            "--------------------------------------------------\n",
            "Iteration: 296/300 - True epochs: 150\n",
            "Local Step 296: Test Loss: 2.605016580, Test Accuracy: 41.650\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [297]: Training Loss: 1.278753780, Training Accuracy: 62.832\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.435914\n",
            "--------------------------------------------------\n",
            "Iteration: 297/300 - True epochs: 150\n",
            "Local Step 297: Test Loss: 2.562860189, Test Accuracy: 42.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [298]: Training Loss: 1.260792581, Training Accuracy: 63.676\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:09.922354\n",
            "--------------------------------------------------\n",
            "Iteration: 298/300 - True epochs: 150\n",
            "Local Step 298: Test Loss: 2.505924258, Test Accuracy: 42.890\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [299]: Training Loss: 1.264138978, Training Accuracy: 63.372\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.723880\n",
            "--------------------------------------------------\n",
            "Iteration: 299/300 - True epochs: 150\n",
            "Local Step 299: Test Loss: 2.510334385, Test Accuracy: 43.510\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [300]: Training Loss: 1.269564679, Training Accuracy: 63.176\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.172108\n",
            "--------------------------------------------------\n",
            "Iteration: 300/300 - True epochs: 150\n",
            "Local Step 300: Test Loss: 2.518077209, Test Accuracy: 42.510\n",
            "--------------------------------------------------\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for SHAT: 1:07:44.699027\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:4\n",
            "==================================================\n",
            "Original Computation Latency: [5071, 6660, 7534, 7959]\n",
            "Scaled Computation Latency: [1.0, 1.3133504239794913, 1.4857030171563794, 1.5695129165845]\n",
            "workers simulated orders based on computation latency:[1, 2, 3, 4, 1, 2, 3, 1, 4, 2, 1, 3, 4, 1, 2, 3, 1, 4, 2, 1, 3, 4, 2, 1, 3, 1, 2, 4, 1, 3, 2, 4, 1, 2, 3, 1, 4, 1, 2, 3, 1, 4, 2, 3, 1, 4, 2, 1, 3, 1, 2, 4, 3, 1, 2, 4, 1, 3, 2, 1, 4, 3, 1, 2, 4, 1, 3, 2, 1, 4, 2, 3, 1, 2, 1, 4, 3, 1, 2, 4, 3, 1, 2, 1, 3, 4, 2, 1, 3, 4, 1, 2, 1, 3, 4, 2, 1, 3, 2, 4, 1, 1, 2, 3, 4, 1, 2, 3, 1, 4, 2, 1, 3, 4, 1, 2, 3, 1, 4, 2, 1, 3, 2, 4, 1, 3, 1, 2, 4, 1, 3, 2, 4, 1, 3, 2, 1, 4, 2, 1, 3, 1, 4, 2, 3, 1, 2, 4, 1, 3, 2, 1, 4, 3, 1, 2, 4, 3, 1, 2, 1, 4, 3, 2, 1, 4, 3, 1, 2, 1, 3, 2, 4, 1, 2, 3, 1, 4, 1, 2, 3, 4, 1, 2, 3, 1, 4, 2, 1, 3, 4, 1, 2, 3, 1, 4, 2, 1, 3, 2, 4, 1, 3, 2, 1, 4, 1, 2, 3, 1, 4, 2, 3, 1, 4, 2, 1, 3, 1, 4, 2, 3, 1, 2, 4, 1, 3, 2, 1, 4, 3, 1, 2, 4, 1, 3, 2, 1, 4, 3, 2, 1, 1, 4, 2, 3, 1, 2, 4, 3, 1, 2, 1, 4, 3, 1, 2, 3, 4, 1, 2, 1, 3, 4, 2, 1, 3, 4, 2, 1, 1, 3, 2, 4, 1, 2, 3, 1, 4, 2, 1, 3, 4, 1, 2, 3, 1, 4, 2, 1, 3, 4, 2, 1, 3, 1, 2, 4, 1, 3, 2, 4, 1, 3, 2, 1, 4, 1, 3, 2, 1, 4, 2, 3, 1, 4, 2, 3, 1, 1, 2, 4, 3, 1, 2, 4, 3, 1, 2, 1, 4, 3, 1, 2, 4, 3, 1, 2, 1, 3, 4, 2, 1, 3, 2, 1, 4, 1, 2, 3, 4, 1, 2, 3, 1, 4, 2, 1, 3, 4, 1, 2, 3, 1, 4, 2, 1, 3, 2, 4, 1, 3, 1, 2, 4, 1, 3, 2, 4, 1, 2, 3, 1, 4, 1, 2, 3, 1, 4, 2, 3, 1, 4, 2, 1, 3, 1, 2, 4, 3, 1, 2, 4, 1, 3, 2, 1, 4, 3, 2, 1, 4, 1, 3, 2, 1, 4, 2, 3, 1, 2, 1, 4, 3, 1, 2, 3, 4, 1, 2, 1, 3, 4, 2, 1, 3, 4, 1, 2, 1, 3, 4, 2, 1, 3, 2, 4, 1, 1, 3, 2, 4, 1, 2, 3, 1, 4, 2, 1, 3, 4, 2, 3, 4, 2, 3, 2, 4, 3, 2, 4, 3, 2, 4, 3, 2, 4, 2, 3, 4, 2, 3, 2, 4, 3, 2, 4, 3, 2, 4, 3, 2, 4, 3, 2, 3, 4, 2, 3, 2, 4, 2, 3, 4, 2, 3, 4, 2, 3, 4, 2, 3, 4, 2, 3, 2, 4, 3, 2, 4, 3, 2, 4, 2, 3, 4, 2, 3, 4, 2, 3, 2, 4, 3, 2, 4, 3, 2, 4, 3, 2, 4, 3, 2, 4, 3, 2, 4, 2, 3, 2, 4, 3, 2, 3, 4, 2, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
            "**************************************************\n",
            "Worker 1, [01]: Training Loss: 4.492833658, Training Accuracy: 2.936\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.063864\n",
            "--------------------------------------------------\n",
            "Iteration: 01/600 - True epochs: 150\n",
            "Local Step 01: Test Loss: 4.230510200, Test Accuracy: 5.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [02]: Training Loss: 4.486785694, Training Accuracy: 2.984\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.025993\n",
            "--------------------------------------------------\n",
            "Iteration: 02/600 - True epochs: 150\n",
            "Local Step 02: Test Loss: 4.237453556, Test Accuracy: 4.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [03]: Training Loss: 4.485228986, Training Accuracy: 2.528\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.391989\n",
            "--------------------------------------------------\n",
            "Iteration: 03/600 - True epochs: 150\n",
            "Local Step 03: Test Loss: 4.234284635, Test Accuracy: 4.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [04]: Training Loss: 4.481392134, Training Accuracy: 2.536\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:04.931850\n",
            "--------------------------------------------------\n",
            "Iteration: 04/600 - True epochs: 150\n",
            "Local Step 04: Test Loss: 4.243300528, Test Accuracy: 4.170\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [05]: Training Loss: 4.112560665, Training Accuracy: 6.384\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.684519\n",
            "--------------------------------------------------\n",
            "Iteration: 05/600 - True epochs: 150\n",
            "Local Step 05: Test Loss: 3.960473301, Test Accuracy: 8.110\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [06]: Training Loss: 4.125644962, Training Accuracy: 6.176\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:04.928622\n",
            "--------------------------------------------------\n",
            "Iteration: 06/600 - True epochs: 150\n",
            "Local Step 06: Test Loss: 3.962347857, Test Accuracy: 8.550\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [07]: Training Loss: 4.117518517, Training Accuracy: 6.152\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.392439\n",
            "--------------------------------------------------\n",
            "Iteration: 07/600 - True epochs: 150\n",
            "Local Step 07: Test Loss: 3.955421601, Test Accuracy: 10.020\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [08]: Training Loss: 3.899606371, Training Accuracy: 9.624\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.002033\n",
            "--------------------------------------------------\n",
            "Iteration: 08/600 - True epochs: 150\n",
            "Local Step 08: Test Loss: 3.802840277, Test Accuracy: 10.920\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [09]: Training Loss: 4.113779634, Training Accuracy: 6.368\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.091328\n",
            "--------------------------------------------------\n",
            "Iteration: 09/600 - True epochs: 150\n",
            "Local Step 09: Test Loss: 3.960972684, Test Accuracy: 9.250\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [10]: Training Loss: 3.917883571, Training Accuracy: 9.376\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.072809\n",
            "--------------------------------------------------\n",
            "Iteration: 10/600 - True epochs: 150\n",
            "Local Step 10: Test Loss: 3.805241519, Test Accuracy: 11.670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [11]: Training Loss: 3.753971723, Training Accuracy: 11.544\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.208047\n",
            "--------------------------------------------------\n",
            "Iteration: 11/600 - True epochs: 150\n",
            "Local Step 11: Test Loss: 3.645251207, Test Accuracy: 13.670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [12]: Training Loss: 3.908568528, Training Accuracy: 9.480\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.038627\n",
            "--------------------------------------------------\n",
            "Iteration: 12/600 - True epochs: 150\n",
            "Local Step 12: Test Loss: 3.808582283, Test Accuracy: 11.900\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [13]: Training Loss: 3.912667341, Training Accuracy: 8.976\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.117848\n",
            "--------------------------------------------------\n",
            "Iteration: 13/600 - True epochs: 150\n",
            "Local Step 13: Test Loss: 3.760782337, Test Accuracy: 12.460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [14]: Training Loss: 3.616351780, Training Accuracy: 13.888\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.319504\n",
            "--------------------------------------------------\n",
            "Iteration: 14/600 - True epochs: 150\n",
            "Local Step 14: Test Loss: 3.564763109, Test Accuracy: 15.320\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [15]: Training Loss: 3.774742956, Training Accuracy: 11.608\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.028900\n",
            "--------------------------------------------------\n",
            "Iteration: 15/600 - True epochs: 150\n",
            "Local Step 15: Test Loss: 3.632125678, Test Accuracy: 14.250\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [16]: Training Loss: 3.765504033, Training Accuracy: 11.400\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.103962\n",
            "--------------------------------------------------\n",
            "Iteration: 16/600 - True epochs: 150\n",
            "Local Step 16: Test Loss: 3.723095221, Test Accuracy: 12.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [17]: Training Loss: 3.490997261, Training Accuracy: 16.216\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.055602\n",
            "--------------------------------------------------\n",
            "Iteration: 17/600 - True epochs: 150\n",
            "Local Step 17: Test Loss: 3.431000032, Test Accuracy: 18.130\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [18]: Training Loss: 3.753892857, Training Accuracy: 11.864\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.420420\n",
            "--------------------------------------------------\n",
            "Iteration: 18/600 - True epochs: 150\n",
            "Local Step 18: Test Loss: 3.743457926, Test Accuracy: 12.680\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [19]: Training Loss: 3.645877682, Training Accuracy: 13.344\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:04.983316\n",
            "--------------------------------------------------\n",
            "Iteration: 19/600 - True epochs: 150\n",
            "Local Step 19: Test Loss: 3.546885115, Test Accuracy: 15.200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [20]: Training Loss: 3.385484080, Training Accuracy: 18.200\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.456649\n",
            "--------------------------------------------------\n",
            "Iteration: 20/600 - True epochs: 150\n",
            "Local Step 20: Test Loss: 3.388748413, Test Accuracy: 18.730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [21]: Training Loss: 3.635841123, Training Accuracy: 13.352\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.197938\n",
            "--------------------------------------------------\n",
            "Iteration: 21/600 - True epochs: 150\n",
            "Local Step 21: Test Loss: 3.531357001, Test Accuracy: 16.290\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [22]: Training Loss: 3.636654581, Training Accuracy: 13.448\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.644677\n",
            "--------------------------------------------------\n",
            "Iteration: 22/600 - True epochs: 150\n",
            "Local Step 22: Test Loss: 3.549405913, Test Accuracy: 15.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [23]: Training Loss: 3.511807350, Training Accuracy: 15.816\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.061796\n",
            "--------------------------------------------------\n",
            "Iteration: 23/600 - True epochs: 150\n",
            "Local Step 23: Test Loss: 3.413624748, Test Accuracy: 18.260\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [24]: Training Loss: 3.279469198, Training Accuracy: 19.824\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.235379\n",
            "--------------------------------------------------\n",
            "Iteration: 24/600 - True epochs: 150\n",
            "Local Step 24: Test Loss: 3.336679105, Test Accuracy: 19.280\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [25]: Training Loss: 3.505670882, Training Accuracy: 16.016\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.641044\n",
            "--------------------------------------------------\n",
            "Iteration: 25/600 - True epochs: 150\n",
            "Local Step 25: Test Loss: 3.405362398, Test Accuracy: 18.050\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [26]: Training Loss: 3.172350055, Training Accuracy: 21.968\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:04.912828\n",
            "--------------------------------------------------\n",
            "Iteration: 26/600 - True epochs: 150\n",
            "Local Step 26: Test Loss: 3.136432587, Test Accuracy: 23.130\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [27]: Training Loss: 3.401567472, Training Accuracy: 17.688\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.148515\n",
            "--------------------------------------------------\n",
            "Iteration: 27/600 - True epochs: 150\n",
            "Local Step 27: Test Loss: 3.340750145, Test Accuracy: 19.020\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [28]: Training Loss: 3.495476414, Training Accuracy: 16.048\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.709580\n",
            "--------------------------------------------------\n",
            "Iteration: 28/600 - True epochs: 150\n",
            "Local Step 28: Test Loss: 3.452984117, Test Accuracy: 16.730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [29]: Training Loss: 3.070183018, Training Accuracy: 23.392\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:04.936298\n",
            "--------------------------------------------------\n",
            "Iteration: 29/600 - True epochs: 150\n",
            "Local Step 29: Test Loss: 3.118112128, Test Accuracy: 23.870\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [30]: Training Loss: 3.389136805, Training Accuracy: 17.584\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.687562\n",
            "--------------------------------------------------\n",
            "Iteration: 30/600 - True epochs: 150\n",
            "Local Step 30: Test Loss: 3.494907695, Test Accuracy: 17.190\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [31]: Training Loss: 3.292173955, Training Accuracy: 19.616\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.052311\n",
            "--------------------------------------------------\n",
            "Iteration: 31/600 - True epochs: 150\n",
            "Local Step 31: Test Loss: 3.296006642, Test Accuracy: 19.650\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [32]: Training Loss: 3.384650308, Training Accuracy: 17.752\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.384505\n",
            "--------------------------------------------------\n",
            "Iteration: 32/600 - True epochs: 150\n",
            "Local Step 32: Test Loss: 3.323125532, Test Accuracy: 19.550\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [33]: Training Loss: 2.966958992, Training Accuracy: 24.856\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:04.941865\n",
            "--------------------------------------------------\n",
            "Iteration: 33/600 - True epochs: 150\n",
            "Local Step 33: Test Loss: 3.054024838, Test Accuracy: 24.600\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [34]: Training Loss: 3.189544595, Training Accuracy: 20.624\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.571499\n",
            "--------------------------------------------------\n",
            "Iteration: 34/600 - True epochs: 150\n",
            "Local Step 34: Test Loss: 3.211065212, Test Accuracy: 22.000\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [35]: Training Loss: 3.283310398, Training Accuracy: 19.160\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.471832\n",
            "--------------------------------------------------\n",
            "Iteration: 35/600 - True epochs: 150\n",
            "Local Step 35: Test Loss: 3.314917782, Test Accuracy: 19.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [36]: Training Loss: 2.922079911, Training Accuracy: 26.608\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:04.939085\n",
            "--------------------------------------------------\n",
            "Iteration: 36/600 - True epochs: 150\n",
            "Local Step 36: Test Loss: 2.950287576, Test Accuracy: 26.360\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [37]: Training Loss: 3.259419288, Training Accuracy: 19.800\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.614924\n",
            "--------------------------------------------------\n",
            "Iteration: 37/600 - True epochs: 150\n",
            "Local Step 37: Test Loss: 3.203724893, Test Accuracy: 22.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [38]: Training Loss: 2.833003217, Training Accuracy: 27.840\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:04.933122\n",
            "--------------------------------------------------\n",
            "Iteration: 38/600 - True epochs: 150\n",
            "Local Step 38: Test Loss: 2.907055310, Test Accuracy: 27.090\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [39]: Training Loss: 3.122218308, Training Accuracy: 22.248\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:04.939577\n",
            "--------------------------------------------------\n",
            "Iteration: 39/600 - True epochs: 150\n",
            "Local Step 39: Test Loss: 3.096246542, Test Accuracy: 24.180\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [40]: Training Loss: 3.185586039, Training Accuracy: 21.528\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.063013\n",
            "--------------------------------------------------\n",
            "Iteration: 40/600 - True epochs: 150\n",
            "Local Step 40: Test Loss: 3.216586083, Test Accuracy: 21.200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [41]: Training Loss: 2.746054551, Training Accuracy: 29.368\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:04.936517\n",
            "--------------------------------------------------\n",
            "Iteration: 41/600 - True epochs: 150\n",
            "Local Step 41: Test Loss: 2.937656371, Test Accuracy: 28.110\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [42]: Training Loss: 3.181945548, Training Accuracy: 21.048\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:04.987306\n",
            "--------------------------------------------------\n",
            "Iteration: 42/600 - True epochs: 150\n",
            "Local Step 42: Test Loss: 3.195951324, Test Accuracy: 21.880\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [43]: Training Loss: 3.035887738, Training Accuracy: 24.344\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:04.969249\n",
            "--------------------------------------------------\n",
            "Iteration: 43/600 - True epochs: 150\n",
            "Local Step 43: Test Loss: 3.052641664, Test Accuracy: 24.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [44]: Training Loss: 3.105098941, Training Accuracy: 22.552\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:04.971749\n",
            "--------------------------------------------------\n",
            "Iteration: 44/600 - True epochs: 150\n",
            "Local Step 44: Test Loss: 3.044716235, Test Accuracy: 24.960\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [45]: Training Loss: 2.697644256, Training Accuracy: 30.512\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.270983\n",
            "--------------------------------------------------\n",
            "Iteration: 45/600 - True epochs: 150\n",
            "Local Step 45: Test Loss: 2.863152714, Test Accuracy: 28.700\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [46]: Training Loss: 3.089726296, Training Accuracy: 22.720\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.036818\n",
            "--------------------------------------------------\n",
            "Iteration: 46/600 - True epochs: 150\n",
            "Local Step 46: Test Loss: 3.128460031, Test Accuracy: 23.360\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [47]: Training Loss: 2.962889014, Training Accuracy: 25.632\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:04.998162\n",
            "--------------------------------------------------\n",
            "Iteration: 47/600 - True epochs: 150\n",
            "Local Step 47: Test Loss: 3.041465285, Test Accuracy: 25.170\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [48]: Training Loss: 2.639728141, Training Accuracy: 31.072\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:04.998500\n",
            "--------------------------------------------------\n",
            "Iteration: 48/600 - True epochs: 150\n",
            "Local Step 48: Test Loss: 2.854301551, Test Accuracy: 30.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [49]: Training Loss: 3.020845607, Training Accuracy: 24.160\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:04.971894\n",
            "--------------------------------------------------\n",
            "Iteration: 49/600 - True epochs: 150\n",
            "Local Step 49: Test Loss: 3.095154352, Test Accuracy: 23.570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [50]: Training Loss: 2.572296445, Training Accuracy: 32.760\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.028448\n",
            "--------------------------------------------------\n",
            "Iteration: 50/600 - True epochs: 150\n",
            "Local Step 50: Test Loss: 2.828698512, Test Accuracy: 29.170\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [51]: Training Loss: 2.868685132, Training Accuracy: 27.472\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.270242\n",
            "--------------------------------------------------\n",
            "Iteration: 51/600 - True epochs: 150\n",
            "Local Step 51: Test Loss: 2.940596846, Test Accuracy: 27.500\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [52]: Training Loss: 2.992941353, Training Accuracy: 24.600\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.704873\n",
            "--------------------------------------------------\n",
            "Iteration: 52/600 - True epochs: 150\n",
            "Local Step 52: Test Loss: 3.047947852, Test Accuracy: 25.000\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [53]: Training Loss: 2.917973494, Training Accuracy: 26.152\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.696842\n",
            "--------------------------------------------------\n",
            "Iteration: 53/600 - True epochs: 150\n",
            "Local Step 53: Test Loss: 2.966335991, Test Accuracy: 26.370\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [54]: Training Loss: 2.502862502, Training Accuracy: 33.976\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.043479\n",
            "--------------------------------------------------\n",
            "Iteration: 54/600 - True epochs: 150\n",
            "Local Step 54: Test Loss: 2.806045350, Test Accuracy: 30.670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [55]: Training Loss: 2.804489514, Training Accuracy: 28.712\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.074042\n",
            "--------------------------------------------------\n",
            "Iteration: 55/600 - True epochs: 150\n",
            "Local Step 55: Test Loss: 2.938519753, Test Accuracy: 27.490\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [56]: Training Loss: 2.917771575, Training Accuracy: 25.816\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.087212\n",
            "--------------------------------------------------\n",
            "Iteration: 56/600 - True epochs: 150\n",
            "Local Step 56: Test Loss: 3.013871369, Test Accuracy: 25.080\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [57]: Training Loss: 2.434980734, Training Accuracy: 36.352\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.296926\n",
            "--------------------------------------------------\n",
            "Iteration: 57/600 - True epochs: 150\n",
            "Local Step 57: Test Loss: 2.705477995, Test Accuracy: 32.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [58]: Training Loss: 2.887967830, Training Accuracy: 26.536\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:04.970337\n",
            "--------------------------------------------------\n",
            "Iteration: 58/600 - True epochs: 150\n",
            "Local Step 58: Test Loss: 2.950733650, Test Accuracy: 26.550\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [59]: Training Loss: 2.751538389, Training Accuracy: 30.112\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:04.970414\n",
            "--------------------------------------------------\n",
            "Iteration: 59/600 - True epochs: 150\n",
            "Local Step 59: Test Loss: 2.864405675, Test Accuracy: 28.610\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [60]: Training Loss: 2.405243542, Training Accuracy: 36.880\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.694510\n",
            "--------------------------------------------------\n",
            "Iteration: 60/600 - True epochs: 150\n",
            "Local Step 60: Test Loss: 2.769347645, Test Accuracy: 31.520\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [61]: Training Loss: 2.852039604, Training Accuracy: 27.800\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.043860\n",
            "--------------------------------------------------\n",
            "Iteration: 61/600 - True epochs: 150\n",
            "Local Step 61: Test Loss: 2.950534177, Test Accuracy: 26.190\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [62]: Training Loss: 2.797596740, Training Accuracy: 28.680\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.326706\n",
            "--------------------------------------------------\n",
            "Iteration: 62/600 - True epochs: 150\n",
            "Local Step 62: Test Loss: 2.903725366, Test Accuracy: 27.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [63]: Training Loss: 2.364454505, Training Accuracy: 37.192\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:04.947242\n",
            "--------------------------------------------------\n",
            "Iteration: 63/600 - True epochs: 150\n",
            "Local Step 63: Test Loss: 2.765697467, Test Accuracy: 31.390\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [64]: Training Loss: 2.668647506, Training Accuracy: 31.280\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.699540\n",
            "--------------------------------------------------\n",
            "Iteration: 64/600 - True epochs: 150\n",
            "Local Step 64: Test Loss: 2.825365944, Test Accuracy: 29.690\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [65]: Training Loss: 2.770826941, Training Accuracy: 29.160\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.090117\n",
            "--------------------------------------------------\n",
            "Iteration: 65/600 - True epochs: 150\n",
            "Local Step 65: Test Loss: 2.955741588, Test Accuracy: 26.880\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [66]: Training Loss: 2.322435875, Training Accuracy: 37.736\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.069230\n",
            "--------------------------------------------------\n",
            "Iteration: 66/600 - True epochs: 150\n",
            "Local Step 66: Test Loss: 2.737006266, Test Accuracy: 32.680\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [67]: Training Loss: 2.725408273, Training Accuracy: 30.256\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:04.983041\n",
            "--------------------------------------------------\n",
            "Iteration: 67/600 - True epochs: 150\n",
            "Local Step 67: Test Loss: 2.876220828, Test Accuracy: 28.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [68]: Training Loss: 2.625159215, Training Accuracy: 31.424\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:04.931139\n",
            "--------------------------------------------------\n",
            "Iteration: 68/600 - True epochs: 150\n",
            "Local Step 68: Test Loss: 2.822492071, Test Accuracy: 29.930\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [69]: Training Loss: 2.259581857, Training Accuracy: 39.992\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.021749\n",
            "--------------------------------------------------\n",
            "Iteration: 69/600 - True epochs: 150\n",
            "Local Step 69: Test Loss: 2.757966739, Test Accuracy: 32.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [70]: Training Loss: 2.716248741, Training Accuracy: 30.040\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.158084\n",
            "--------------------------------------------------\n",
            "Iteration: 70/600 - True epochs: 150\n",
            "Local Step 70: Test Loss: 2.856125713, Test Accuracy: 28.550\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [71]: Training Loss: 2.543173388, Training Accuracy: 34.096\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.559753\n",
            "--------------------------------------------------\n",
            "Iteration: 71/600 - True epochs: 150\n",
            "Local Step 71: Test Loss: 2.869237248, Test Accuracy: 28.550\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [72]: Training Loss: 2.670775441, Training Accuracy: 31.080\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.308939\n",
            "--------------------------------------------------\n",
            "Iteration: 72/600 - True epochs: 150\n",
            "Local Step 72: Test Loss: 2.881038433, Test Accuracy: 27.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [73]: Training Loss: 2.215621062, Training Accuracy: 40.192\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.233765\n",
            "--------------------------------------------------\n",
            "Iteration: 73/600 - True epochs: 150\n",
            "Local Step 73: Test Loss: 2.771561638, Test Accuracy: 32.820\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [74]: Training Loss: 2.522357025, Training Accuracy: 34.184\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.365029\n",
            "--------------------------------------------------\n",
            "Iteration: 74/600 - True epochs: 150\n",
            "Local Step 74: Test Loss: 2.792681799, Test Accuracy: 30.440\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [75]: Training Loss: 2.196513850, Training Accuracy: 40.784\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.518294\n",
            "--------------------------------------------------\n",
            "Iteration: 75/600 - True epochs: 150\n",
            "Local Step 75: Test Loss: 2.737858437, Test Accuracy: 33.190\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [76]: Training Loss: 2.645615478, Training Accuracy: 31.592\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:04.891685\n",
            "--------------------------------------------------\n",
            "Iteration: 76/600 - True epochs: 150\n",
            "Local Step 76: Test Loss: 2.906344488, Test Accuracy: 28.470\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [77]: Training Loss: 2.599101410, Training Accuracy: 32.912\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:04.938837\n",
            "--------------------------------------------------\n",
            "Iteration: 77/600 - True epochs: 150\n",
            "Local Step 77: Test Loss: 2.881592924, Test Accuracy: 28.820\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [78]: Training Loss: 2.145922427, Training Accuracy: 42.208\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.374826\n",
            "--------------------------------------------------\n",
            "Iteration: 78/600 - True epochs: 150\n",
            "Local Step 78: Test Loss: 2.794226672, Test Accuracy: 32.360\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [79]: Training Loss: 2.464015126, Training Accuracy: 35.352\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.224608\n",
            "--------------------------------------------------\n",
            "Iteration: 79/600 - True epochs: 150\n",
            "Local Step 79: Test Loss: 2.784287032, Test Accuracy: 31.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [80]: Training Loss: 2.604698620, Training Accuracy: 32.000\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:04.965599\n",
            "--------------------------------------------------\n",
            "Iteration: 80/600 - True epochs: 150\n",
            "Local Step 80: Test Loss: 2.838442165, Test Accuracy: 29.040\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [81]: Training Loss: 2.547874401, Training Accuracy: 33.392\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:04.966749\n",
            "--------------------------------------------------\n",
            "Iteration: 81/600 - True epochs: 150\n",
            "Local Step 81: Test Loss: 2.825222067, Test Accuracy: 29.150\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [82]: Training Loss: 2.095001209, Training Accuracy: 43.008\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.355497\n",
            "--------------------------------------------------\n",
            "Iteration: 82/600 - True epochs: 150\n",
            "Local Step 82: Test Loss: 2.722061731, Test Accuracy: 33.480\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [83]: Training Loss: 2.425487405, Training Accuracy: 36.464\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.674062\n",
            "--------------------------------------------------\n",
            "Iteration: 83/600 - True epochs: 150\n",
            "Local Step 83: Test Loss: 2.771193252, Test Accuracy: 31.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [84]: Training Loss: 2.051534043, Training Accuracy: 43.896\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.277839\n",
            "--------------------------------------------------\n",
            "Iteration: 84/600 - True epochs: 150\n",
            "Local Step 84: Test Loss: 2.782738789, Test Accuracy: 32.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [85]: Training Loss: 2.488014599, Training Accuracy: 35.128\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.217710\n",
            "--------------------------------------------------\n",
            "Iteration: 85/600 - True epochs: 150\n",
            "Local Step 85: Test Loss: 2.806549586, Test Accuracy: 30.660\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [86]: Training Loss: 2.529419783, Training Accuracy: 34.016\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.101182\n",
            "--------------------------------------------------\n",
            "Iteration: 86/600 - True epochs: 150\n",
            "Local Step 86: Test Loss: 2.772160161, Test Accuracy: 30.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [87]: Training Loss: 2.367752490, Training Accuracy: 37.312\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.060048\n",
            "--------------------------------------------------\n",
            "Iteration: 87/600 - True epochs: 150\n",
            "Local Step 87: Test Loss: 2.759342340, Test Accuracy: 32.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [88]: Training Loss: 2.019758387, Training Accuracy: 44.968\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.155819\n",
            "--------------------------------------------------\n",
            "Iteration: 88/600 - True epochs: 150\n",
            "Local Step 88: Test Loss: 2.815604222, Test Accuracy: 33.060\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [89]: Training Loss: 2.431755457, Training Accuracy: 35.896\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:04.929201\n",
            "--------------------------------------------------\n",
            "Iteration: 89/600 - True epochs: 150\n",
            "Local Step 89: Test Loss: 2.834205169, Test Accuracy: 30.230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [90]: Training Loss: 2.482300035, Training Accuracy: 33.968\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:04.979788\n",
            "--------------------------------------------------\n",
            "Iteration: 90/600 - True epochs: 150\n",
            "Local Step 90: Test Loss: 2.816793954, Test Accuracy: 30.420\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [91]: Training Loss: 1.976595995, Training Accuracy: 45.824\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.654005\n",
            "--------------------------------------------------\n",
            "Iteration: 91/600 - True epochs: 150\n",
            "Local Step 91: Test Loss: 2.743714320, Test Accuracy: 33.570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [92]: Training Loss: 2.335816165, Training Accuracy: 38.248\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.531309\n",
            "--------------------------------------------------\n",
            "Iteration: 92/600 - True epochs: 150\n",
            "Local Step 92: Test Loss: 2.843427878, Test Accuracy: 30.260\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [93]: Training Loss: 1.947760460, Training Accuracy: 46.280\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.922845\n",
            "--------------------------------------------------\n",
            "Iteration: 93/600 - True epochs: 150\n",
            "Local Step 93: Test Loss: 2.900754503, Test Accuracy: 32.500\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [94]: Training Loss: 2.391690008, Training Accuracy: 36.320\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.294613\n",
            "--------------------------------------------------\n",
            "Iteration: 94/600 - True epochs: 150\n",
            "Local Step 94: Test Loss: 2.869877153, Test Accuracy: 30.000\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [95]: Training Loss: 2.431578483, Training Accuracy: 35.696\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.295117\n",
            "--------------------------------------------------\n",
            "Iteration: 95/600 - True epochs: 150\n",
            "Local Step 95: Test Loss: 2.804031090, Test Accuracy: 30.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [96]: Training Loss: 2.281593398, Training Accuracy: 38.992\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.586041\n",
            "--------------------------------------------------\n",
            "Iteration: 96/600 - True epochs: 150\n",
            "Local Step 96: Test Loss: 2.783457665, Test Accuracy: 32.800\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [97]: Training Loss: 1.922069259, Training Accuracy: 47.072\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.491333\n",
            "--------------------------------------------------\n",
            "Iteration: 97/600 - True epochs: 150\n",
            "Local Step 97: Test Loss: 2.835058686, Test Accuracy: 33.470\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [98]: Training Loss: 2.336292240, Training Accuracy: 38.048\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.419897\n",
            "--------------------------------------------------\n",
            "Iteration: 98/600 - True epochs: 150\n",
            "Local Step 98: Test Loss: 2.750000896, Test Accuracy: 31.870\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [99]: Training Loss: 2.230057159, Training Accuracy: 40.080\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.739272\n",
            "--------------------------------------------------\n",
            "Iteration: 99/600 - True epochs: 150\n",
            "Local Step 99: Test Loss: 2.714877089, Test Accuracy: 32.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [100]: Training Loss: 2.384012737, Training Accuracy: 36.680\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:06.678903\n",
            "--------------------------------------------------\n",
            "Iteration: 100/600 - True epochs: 150\n",
            "Local Step 100: Test Loss: 2.820606393, Test Accuracy: 31.230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [101]: Training Loss: 1.898932738, Training Accuracy: 47.504\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.933006\n",
            "--------------------------------------------------\n",
            "Iteration: 101/600 - True epochs: 150\n",
            "Local Step 101: Test Loss: 2.906850616, Test Accuracy: 32.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [102]: Training Loss: 1.851887805, Training Accuracy: 49.104\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.218903\n",
            "--------------------------------------------------\n",
            "Iteration: 102/600 - True epochs: 150\n",
            "Local Step 102: Test Loss: 2.935628795, Test Accuracy: 31.930\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [103]: Training Loss: 2.197928080, Training Accuracy: 40.976\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.914602\n",
            "--------------------------------------------------\n",
            "Iteration: 103/600 - True epochs: 150\n",
            "Local Step 103: Test Loss: 2.738165183, Test Accuracy: 33.190\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [104]: Training Loss: 2.265869159, Training Accuracy: 39.712\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.357984\n",
            "--------------------------------------------------\n",
            "Iteration: 104/600 - True epochs: 150\n",
            "Local Step 104: Test Loss: 2.817853448, Test Accuracy: 30.950\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [105]: Training Loss: 2.317182889, Training Accuracy: 38.232\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.849407\n",
            "--------------------------------------------------\n",
            "Iteration: 105/600 - True epochs: 150\n",
            "Local Step 105: Test Loss: 2.738340542, Test Accuracy: 31.430\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [106]: Training Loss: 1.837792097, Training Accuracy: 48.688\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.280560\n",
            "--------------------------------------------------\n",
            "Iteration: 106/600 - True epochs: 150\n",
            "Local Step 106: Test Loss: 2.864351245, Test Accuracy: 33.970\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [107]: Training Loss: 2.164853171, Training Accuracy: 42.080\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.474764\n",
            "--------------------------------------------------\n",
            "Iteration: 107/600 - True epochs: 150\n",
            "Local Step 107: Test Loss: 2.790487838, Test Accuracy: 32.900\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [108]: Training Loss: 2.254699225, Training Accuracy: 39.560\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.776037\n",
            "--------------------------------------------------\n",
            "Iteration: 108/600 - True epochs: 150\n",
            "Local Step 108: Test Loss: 2.706705610, Test Accuracy: 32.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [109]: Training Loss: 1.791512214, Training Accuracy: 50.176\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.684762\n",
            "--------------------------------------------------\n",
            "Iteration: 109/600 - True epochs: 150\n",
            "Local Step 109: Test Loss: 2.875752871, Test Accuracy: 33.920\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [110]: Training Loss: 2.292417145, Training Accuracy: 38.176\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:06.765444\n",
            "--------------------------------------------------\n",
            "Iteration: 110/600 - True epochs: 150\n",
            "Local Step 110: Test Loss: 2.737124446, Test Accuracy: 32.390\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [111]: Training Loss: 2.131127089, Training Accuracy: 42.352\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:07.022555\n",
            "--------------------------------------------------\n",
            "Iteration: 111/600 - True epochs: 150\n",
            "Local Step 111: Test Loss: 2.749416584, Test Accuracy: 32.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [112]: Training Loss: 1.799574663, Training Accuracy: 49.824\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.788554\n",
            "--------------------------------------------------\n",
            "Iteration: 112/600 - True epochs: 150\n",
            "Local Step 112: Test Loss: 2.937932517, Test Accuracy: 33.340\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [113]: Training Loss: 2.206199828, Training Accuracy: 40.928\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.705390\n",
            "--------------------------------------------------\n",
            "Iteration: 113/600 - True epochs: 150\n",
            "Local Step 113: Test Loss: 2.832684813, Test Accuracy: 31.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [114]: Training Loss: 2.232276187, Training Accuracy: 40.760\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.697602\n",
            "--------------------------------------------------\n",
            "Iteration: 114/600 - True epochs: 150\n",
            "Local Step 114: Test Loss: 2.728911278, Test Accuracy: 32.430\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [115]: Training Loss: 1.746472040, Training Accuracy: 50.864\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.341613\n",
            "--------------------------------------------------\n",
            "Iteration: 115/600 - True epochs: 150\n",
            "Local Step 115: Test Loss: 2.884472988, Test Accuracy: 33.730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [116]: Training Loss: 2.104091915, Training Accuracy: 43.056\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.949197\n",
            "--------------------------------------------------\n",
            "Iteration: 116/600 - True epochs: 150\n",
            "Local Step 116: Test Loss: 2.736030293, Test Accuracy: 33.410\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [117]: Training Loss: 2.175003908, Training Accuracy: 40.792\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.329666\n",
            "--------------------------------------------------\n",
            "Iteration: 117/600 - True epochs: 150\n",
            "Local Step 117: Test Loss: 2.824332121, Test Accuracy: 31.140\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [118]: Training Loss: 1.705000330, Training Accuracy: 52.024\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.434487\n",
            "--------------------------------------------------\n",
            "Iteration: 118/600 - True epochs: 150\n",
            "Local Step 118: Test Loss: 2.940473194, Test Accuracy: 34.570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [119]: Training Loss: 2.216566302, Training Accuracy: 40.368\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:06.026414\n",
            "--------------------------------------------------\n",
            "Iteration: 119/600 - True epochs: 150\n",
            "Local Step 119: Test Loss: 2.844752784, Test Accuracy: 31.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [120]: Training Loss: 2.050848095, Training Accuracy: 44.416\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.490783\n",
            "--------------------------------------------------\n",
            "Iteration: 120/600 - True epochs: 150\n",
            "Local Step 120: Test Loss: 2.745418620, Test Accuracy: 33.220\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [121]: Training Loss: 1.698896144, Training Accuracy: 52.296\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.744720\n",
            "--------------------------------------------------\n",
            "Iteration: 121/600 - True epochs: 150\n",
            "Local Step 121: Test Loss: 2.941980556, Test Accuracy: 34.230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [122]: Training Loss: 2.112672035, Training Accuracy: 42.296\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.361439\n",
            "--------------------------------------------------\n",
            "Iteration: 122/600 - True epochs: 150\n",
            "Local Step 122: Test Loss: 2.792369310, Test Accuracy: 33.100\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [123]: Training Loss: 1.992882623, Training Accuracy: 45.576\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.162996\n",
            "--------------------------------------------------\n",
            "Iteration: 123/600 - True epochs: 150\n",
            "Local Step 123: Test Loss: 2.756467614, Test Accuracy: 33.570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [124]: Training Loss: 2.161350615, Training Accuracy: 41.352\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:06.367421\n",
            "--------------------------------------------------\n",
            "Iteration: 124/600 - True epochs: 150\n",
            "Local Step 124: Test Loss: 2.754863214, Test Accuracy: 32.220\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [125]: Training Loss: 1.693674747, Training Accuracy: 52.960\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.744713\n",
            "--------------------------------------------------\n",
            "Iteration: 125/600 - True epochs: 150\n",
            "Local Step 125: Test Loss: 2.901540887, Test Accuracy: 34.230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [126]: Training Loss: 2.088063700, Training Accuracy: 43.304\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.105851\n",
            "--------------------------------------------------\n",
            "Iteration: 126/600 - True epochs: 150\n",
            "Local Step 126: Test Loss: 2.766402372, Test Accuracy: 33.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [127]: Training Loss: 1.636390967, Training Accuracy: 53.552\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.193682\n",
            "--------------------------------------------------\n",
            "Iteration: 127/600 - True epochs: 150\n",
            "Local Step 127: Test Loss: 2.944678547, Test Accuracy: 34.790\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [128]: Training Loss: 1.957839067, Training Accuracy: 46.472\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.963729\n",
            "--------------------------------------------------\n",
            "Iteration: 128/600 - True epochs: 150\n",
            "Local Step 128: Test Loss: 2.729520359, Test Accuracy: 33.930\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [129]: Training Loss: 2.139221422, Training Accuracy: 42.024\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.674295\n",
            "--------------------------------------------------\n",
            "Iteration: 129/600 - True epochs: 150\n",
            "Local Step 129: Test Loss: 2.821672801, Test Accuracy: 31.770\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [130]: Training Loss: 1.652929922, Training Accuracy: 53.184\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.701294\n",
            "--------------------------------------------------\n",
            "Iteration: 130/600 - True epochs: 150\n",
            "Local Step 130: Test Loss: 2.951334598, Test Accuracy: 34.260\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [131]: Training Loss: 2.062920936, Training Accuracy: 43.696\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.013661\n",
            "--------------------------------------------------\n",
            "Iteration: 131/600 - True epochs: 150\n",
            "Local Step 131: Test Loss: 2.769838535, Test Accuracy: 32.300\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [132]: Training Loss: 1.947204511, Training Accuracy: 46.496\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.424846\n",
            "--------------------------------------------------\n",
            "Iteration: 132/600 - True epochs: 150\n",
            "Local Step 132: Test Loss: 2.749952067, Test Accuracy: 34.370\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [133]: Training Loss: 2.080498291, Training Accuracy: 43.120\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.967342\n",
            "--------------------------------------------------\n",
            "Iteration: 133/600 - True epochs: 150\n",
            "Local Step 133: Test Loss: 2.747423410, Test Accuracy: 33.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [134]: Training Loss: 1.603927844, Training Accuracy: 54.888\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.462054\n",
            "--------------------------------------------------\n",
            "Iteration: 134/600 - True epochs: 150\n",
            "Local Step 134: Test Loss: 2.952293899, Test Accuracy: 34.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [135]: Training Loss: 2.015377516, Training Accuracy: 45.312\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.444631\n",
            "--------------------------------------------------\n",
            "Iteration: 135/600 - True epochs: 150\n",
            "Local Step 135: Test Loss: 2.877463739, Test Accuracy: 30.940\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [136]: Training Loss: 1.918337772, Training Accuracy: 47.728\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.769341\n",
            "--------------------------------------------------\n",
            "Iteration: 136/600 - True epochs: 150\n",
            "Local Step 136: Test Loss: 2.885723662, Test Accuracy: 32.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [137]: Training Loss: 1.584124901, Training Accuracy: 55.424\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.951107\n",
            "--------------------------------------------------\n",
            "Iteration: 137/600 - True epochs: 150\n",
            "Local Step 137: Test Loss: 2.982011408, Test Accuracy: 34.300\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [138]: Training Loss: 2.050866363, Training Accuracy: 44.312\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.389133\n",
            "--------------------------------------------------\n",
            "Iteration: 138/600 - True epochs: 150\n",
            "Local Step 138: Test Loss: 2.785563501, Test Accuracy: 33.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [139]: Training Loss: 1.877522993, Training Accuracy: 48.000\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.409220\n",
            "--------------------------------------------------\n",
            "Iteration: 139/600 - True epochs: 150\n",
            "Local Step 139: Test Loss: 2.838895763, Test Accuracy: 33.100\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [140]: Training Loss: 1.560551206, Training Accuracy: 55.168\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.389383\n",
            "--------------------------------------------------\n",
            "Iteration: 140/600 - True epochs: 150\n",
            "Local Step 140: Test Loss: 3.000528934, Test Accuracy: 34.690\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [141]: Training Loss: 1.981138828, Training Accuracy: 45.400\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.609004\n",
            "--------------------------------------------------\n",
            "Iteration: 141/600 - True epochs: 150\n",
            "Local Step 141: Test Loss: 2.901335244, Test Accuracy: 31.660\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [142]: Training Loss: 1.513965284, Training Accuracy: 56.160\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.423088\n",
            "--------------------------------------------------\n",
            "Iteration: 142/600 - True epochs: 150\n",
            "Local Step 142: Test Loss: 3.120992815, Test Accuracy: 33.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [143]: Training Loss: 2.015634179, Training Accuracy: 45.016\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.403439\n",
            "--------------------------------------------------\n",
            "Iteration: 143/600 - True epochs: 150\n",
            "Local Step 143: Test Loss: 2.822062237, Test Accuracy: 32.130\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [144]: Training Loss: 1.847880447, Training Accuracy: 48.072\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.370587\n",
            "--------------------------------------------------\n",
            "Iteration: 144/600 - True epochs: 150\n",
            "Local Step 144: Test Loss: 2.821851007, Test Accuracy: 33.370\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [145]: Training Loss: 1.954199318, Training Accuracy: 46.712\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.603636\n",
            "--------------------------------------------------\n",
            "Iteration: 145/600 - True epochs: 150\n",
            "Local Step 145: Test Loss: 2.794253322, Test Accuracy: 33.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [146]: Training Loss: 1.544854246, Training Accuracy: 56.376\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.213043\n",
            "--------------------------------------------------\n",
            "Iteration: 146/600 - True epochs: 150\n",
            "Local Step 146: Test Loss: 3.066036390, Test Accuracy: 34.790\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [147]: Training Loss: 1.806036429, Training Accuracy: 49.736\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.329171\n",
            "--------------------------------------------------\n",
            "Iteration: 147/600 - True epochs: 150\n",
            "Local Step 147: Test Loss: 2.814803099, Test Accuracy: 34.300\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [148]: Training Loss: 1.954039616, Training Accuracy: 45.464\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.753054\n",
            "--------------------------------------------------\n",
            "Iteration: 148/600 - True epochs: 150\n",
            "Local Step 148: Test Loss: 2.820874726, Test Accuracy: 33.360\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [149]: Training Loss: 1.499577448, Training Accuracy: 57.112\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.907233\n",
            "--------------------------------------------------\n",
            "Iteration: 149/600 - True epochs: 150\n",
            "Local Step 149: Test Loss: 3.081545432, Test Accuracy: 33.290\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [150]: Training Loss: 1.936642775, Training Accuracy: 46.488\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.465107\n",
            "--------------------------------------------------\n",
            "Iteration: 150/600 - True epochs: 150\n",
            "Local Step 150: Test Loss: 2.766919036, Test Accuracy: 33.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [151]: Training Loss: 1.774307395, Training Accuracy: 50.712\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.376070\n",
            "--------------------------------------------------\n",
            "Iteration: 151/600 - True epochs: 150\n",
            "Local Step 151: Test Loss: 2.867651603, Test Accuracy: 33.920\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [152]: Training Loss: 1.480986477, Training Accuracy: 57.776\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.565213\n",
            "--------------------------------------------------\n",
            "Iteration: 152/600 - True epochs: 150\n",
            "Local Step 152: Test Loss: 3.130937399, Test Accuracy: 33.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [153]: Training Loss: 1.969592019, Training Accuracy: 45.856\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.575268\n",
            "--------------------------------------------------\n",
            "Iteration: 153/600 - True epochs: 150\n",
            "Local Step 153: Test Loss: 2.758732099, Test Accuracy: 34.100\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [154]: Training Loss: 1.901595600, Training Accuracy: 47.808\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.548088\n",
            "--------------------------------------------------\n",
            "Iteration: 154/600 - True epochs: 150\n",
            "Local Step 154: Test Loss: 2.767361691, Test Accuracy: 34.820\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [155]: Training Loss: 1.444827811, Training Accuracy: 59.288\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.979965\n",
            "--------------------------------------------------\n",
            "Iteration: 155/600 - True epochs: 150\n",
            "Local Step 155: Test Loss: 3.053725695, Test Accuracy: 34.250\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [156]: Training Loss: 1.767734692, Training Accuracy: 50.464\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.354150\n",
            "--------------------------------------------------\n",
            "Iteration: 156/600 - True epochs: 150\n",
            "Local Step 156: Test Loss: 2.846875040, Test Accuracy: 33.080\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [157]: Training Loss: 1.924891507, Training Accuracy: 46.512\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.394239\n",
            "--------------------------------------------------\n",
            "Iteration: 157/600 - True epochs: 150\n",
            "Local Step 157: Test Loss: 2.778663237, Test Accuracy: 33.220\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [158]: Training Loss: 1.835259834, Training Accuracy: 49.000\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.366340\n",
            "--------------------------------------------------\n",
            "Iteration: 158/600 - True epochs: 150\n",
            "Local Step 158: Test Loss: 2.857110854, Test Accuracy: 34.180\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [159]: Training Loss: 1.427078680, Training Accuracy: 59.296\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.445765\n",
            "--------------------------------------------------\n",
            "Iteration: 159/600 - True epochs: 150\n",
            "Local Step 159: Test Loss: 3.126065026, Test Accuracy: 34.170\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [160]: Training Loss: 1.742205071, Training Accuracy: 51.152\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.941685\n",
            "--------------------------------------------------\n",
            "Iteration: 160/600 - True epochs: 150\n",
            "Local Step 160: Test Loss: 2.912524510, Test Accuracy: 32.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [161]: Training Loss: 1.458715920, Training Accuracy: 58.488\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.354654\n",
            "--------------------------------------------------\n",
            "Iteration: 161/600 - True epochs: 150\n",
            "Local Step 161: Test Loss: 3.095224068, Test Accuracy: 33.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [162]: Training Loss: 1.892924681, Training Accuracy: 47.216\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.985674\n",
            "--------------------------------------------------\n",
            "Iteration: 162/600 - True epochs: 150\n",
            "Local Step 162: Test Loss: 2.899018932, Test Accuracy: 32.570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [163]: Training Loss: 1.800391421, Training Accuracy: 49.576\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.588709\n",
            "--------------------------------------------------\n",
            "Iteration: 163/600 - True epochs: 150\n",
            "Local Step 163: Test Loss: 2.876296423, Test Accuracy: 34.040\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [164]: Training Loss: 1.722497495, Training Accuracy: 51.912\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.677541\n",
            "--------------------------------------------------\n",
            "Iteration: 164/600 - True epochs: 150\n",
            "Local Step 164: Test Loss: 2.882049564, Test Accuracy: 33.730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [165]: Training Loss: 1.404793984, Training Accuracy: 60.168\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.077719\n",
            "--------------------------------------------------\n",
            "Iteration: 165/600 - True epochs: 150\n",
            "Local Step 165: Test Loss: 3.111088754, Test Accuracy: 33.600\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [166]: Training Loss: 1.847972714, Training Accuracy: 48.600\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.879589\n",
            "--------------------------------------------------\n",
            "Iteration: 166/600 - True epochs: 150\n",
            "Local Step 166: Test Loss: 2.788856320, Test Accuracy: 34.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [167]: Training Loss: 1.779274361, Training Accuracy: 50.064\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.498956\n",
            "--------------------------------------------------\n",
            "Iteration: 167/600 - True epochs: 150\n",
            "Local Step 167: Test Loss: 2.860664699, Test Accuracy: 33.510\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [168]: Training Loss: 1.360576629, Training Accuracy: 60.464\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.732396\n",
            "--------------------------------------------------\n",
            "Iteration: 168/600 - True epochs: 150\n",
            "Local Step 168: Test Loss: 3.155200910, Test Accuracy: 34.570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [169]: Training Loss: 1.708273136, Training Accuracy: 51.952\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.971607\n",
            "--------------------------------------------------\n",
            "Iteration: 169/600 - True epochs: 150\n",
            "Local Step 169: Test Loss: 3.075358691, Test Accuracy: 34.250\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [170]: Training Loss: 1.367804446, Training Accuracy: 60.944\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.839340\n",
            "--------------------------------------------------\n",
            "Iteration: 170/600 - True epochs: 150\n",
            "Local Step 170: Test Loss: 3.266768474, Test Accuracy: 33.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [171]: Training Loss: 1.765285922, Training Accuracy: 50.760\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.679363\n",
            "--------------------------------------------------\n",
            "Iteration: 171/600 - True epochs: 150\n",
            "Local Step 171: Test Loss: 2.929593961, Test Accuracy: 34.180\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [172]: Training Loss: 1.694678230, Training Accuracy: 52.456\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.728073\n",
            "--------------------------------------------------\n",
            "Iteration: 172/600 - True epochs: 150\n",
            "Local Step 172: Test Loss: 2.937668073, Test Accuracy: 33.330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [173]: Training Loss: 1.812429559, Training Accuracy: 49.536\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.690482\n",
            "--------------------------------------------------\n",
            "Iteration: 173/600 - True epochs: 150\n",
            "Local Step 173: Test Loss: 2.844306270, Test Accuracy: 34.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [174]: Training Loss: 1.418402530, Training Accuracy: 59.528\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:07.656677\n",
            "--------------------------------------------------\n",
            "Iteration: 174/600 - True epochs: 150\n",
            "Local Step 174: Test Loss: 3.176178568, Test Accuracy: 34.610\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [175]: Training Loss: 1.665370400, Training Accuracy: 53.328\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.086829\n",
            "--------------------------------------------------\n",
            "Iteration: 175/600 - True epochs: 150\n",
            "Local Step 175: Test Loss: 2.923563991, Test Accuracy: 34.190\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [176]: Training Loss: 1.758516249, Training Accuracy: 50.992\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.306947\n",
            "--------------------------------------------------\n",
            "Iteration: 176/600 - True epochs: 150\n",
            "Local Step 176: Test Loss: 2.925930362, Test Accuracy: 34.620\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [177]: Training Loss: 1.331507092, Training Accuracy: 61.400\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.038259\n",
            "--------------------------------------------------\n",
            "Iteration: 177/600 - True epochs: 150\n",
            "Local Step 177: Test Loss: 3.199036442, Test Accuracy: 34.300\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [178]: Training Loss: 1.777263316, Training Accuracy: 50.016\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:06.090517\n",
            "--------------------------------------------------\n",
            "Iteration: 178/600 - True epochs: 150\n",
            "Local Step 178: Test Loss: 2.828695718, Test Accuracy: 33.550\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [179]: Training Loss: 1.364778962, Training Accuracy: 60.360\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.968548\n",
            "--------------------------------------------------\n",
            "Iteration: 179/600 - True epochs: 150\n",
            "Local Step 179: Test Loss: 3.245777776, Test Accuracy: 33.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [180]: Training Loss: 1.660706288, Training Accuracy: 53.288\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.444512\n",
            "--------------------------------------------------\n",
            "Iteration: 180/600 - True epochs: 150\n",
            "Local Step 180: Test Loss: 2.927055710, Test Accuracy: 34.330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [181]: Training Loss: 1.706065869, Training Accuracy: 52.312\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.742010\n",
            "--------------------------------------------------\n",
            "Iteration: 181/600 - True epochs: 150\n",
            "Local Step 181: Test Loss: 2.891085429, Test Accuracy: 33.960\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [182]: Training Loss: 1.763679953, Training Accuracy: 50.560\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:06.643288\n",
            "--------------------------------------------------\n",
            "Iteration: 182/600 - True epochs: 150\n",
            "Local Step 182: Test Loss: 2.851344092, Test Accuracy: 34.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [183]: Training Loss: 1.317787761, Training Accuracy: 61.456\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.531226\n",
            "--------------------------------------------------\n",
            "Iteration: 183/600 - True epochs: 150\n",
            "Local Step 183: Test Loss: 3.290574104, Test Accuracy: 34.520\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [184]: Training Loss: 1.614948444, Training Accuracy: 54.360\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.983992\n",
            "--------------------------------------------------\n",
            "Iteration: 184/600 - True epochs: 150\n",
            "Local Step 184: Test Loss: 3.006894666, Test Accuracy: 34.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [185]: Training Loss: 1.677110435, Training Accuracy: 52.768\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.388715\n",
            "--------------------------------------------------\n",
            "Iteration: 185/600 - True epochs: 150\n",
            "Local Step 185: Test Loss: 2.997774783, Test Accuracy: 32.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [186]: Training Loss: 1.331272694, Training Accuracy: 61.368\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.582257\n",
            "--------------------------------------------------\n",
            "Iteration: 186/600 - True epochs: 150\n",
            "Local Step 186: Test Loss: 3.243467250, Test Accuracy: 33.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [187]: Training Loss: 1.718476555, Training Accuracy: 51.544\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:06.826591\n",
            "--------------------------------------------------\n",
            "Iteration: 187/600 - True epochs: 150\n",
            "Local Step 187: Test Loss: 2.846344793, Test Accuracy: 33.850\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [188]: Training Loss: 1.596567957, Training Accuracy: 54.584\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.519413\n",
            "--------------------------------------------------\n",
            "Iteration: 188/600 - True epochs: 150\n",
            "Local Step 188: Test Loss: 2.954146820, Test Accuracy: 34.120\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [189]: Training Loss: 1.305711316, Training Accuracy: 62.640\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.296415\n",
            "--------------------------------------------------\n",
            "Iteration: 189/600 - True epochs: 150\n",
            "Local Step 189: Test Loss: 3.336342195, Test Accuracy: 34.150\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [190]: Training Loss: 1.664794238, Training Accuracy: 53.224\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:08.218885\n",
            "--------------------------------------------------\n",
            "Iteration: 190/600 - True epochs: 150\n",
            "Local Step 190: Test Loss: 2.930478351, Test Accuracy: 34.000\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [191]: Training Loss: 1.731053495, Training Accuracy: 50.680\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:07.379822\n",
            "--------------------------------------------------\n",
            "Iteration: 191/600 - True epochs: 150\n",
            "Local Step 191: Test Loss: 2.875442856, Test Accuracy: 33.510\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [192]: Training Loss: 1.273934081, Training Accuracy: 63.008\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:07.153276\n",
            "--------------------------------------------------\n",
            "Iteration: 192/600 - True epochs: 150\n",
            "Local Step 192: Test Loss: 3.336663462, Test Accuracy: 34.080\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [193]: Training Loss: 1.546350363, Training Accuracy: 55.984\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.984922\n",
            "--------------------------------------------------\n",
            "Iteration: 193/600 - True epochs: 150\n",
            "Local Step 193: Test Loss: 2.968002275, Test Accuracy: 35.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [194]: Training Loss: 1.656843887, Training Accuracy: 53.376\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.724732\n",
            "--------------------------------------------------\n",
            "Iteration: 194/600 - True epochs: 150\n",
            "Local Step 194: Test Loss: 2.957972461, Test Accuracy: 33.530\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [195]: Training Loss: 1.303816170, Training Accuracy: 62.008\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.618498\n",
            "--------------------------------------------------\n",
            "Iteration: 195/600 - True epochs: 150\n",
            "Local Step 195: Test Loss: 3.325733968, Test Accuracy: 34.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [196]: Training Loss: 1.673706556, Training Accuracy: 52.456\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.564245\n",
            "--------------------------------------------------\n",
            "Iteration: 196/600 - True epochs: 150\n",
            "Local Step 196: Test Loss: 2.852333682, Test Accuracy: 34.500\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [197]: Training Loss: 1.536328295, Training Accuracy: 56.440\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.496605\n",
            "--------------------------------------------------\n",
            "Iteration: 197/600 - True epochs: 150\n",
            "Local Step 197: Test Loss: 3.063322438, Test Accuracy: 33.820\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [198]: Training Loss: 1.252001610, Training Accuracy: 63.648\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.300859\n",
            "--------------------------------------------------\n",
            "Iteration: 198/600 - True epochs: 150\n",
            "Local Step 198: Test Loss: 3.323757793, Test Accuracy: 34.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [199]: Training Loss: 1.611375831, Training Accuracy: 54.192\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.547339\n",
            "--------------------------------------------------\n",
            "Iteration: 199/600 - True epochs: 150\n",
            "Local Step 199: Test Loss: 2.926012183, Test Accuracy: 33.880\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [200]: Training Loss: 1.533570932, Training Accuracy: 56.648\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.020124\n",
            "--------------------------------------------------\n",
            "Iteration: 200/600 - True epochs: 150\n",
            "Local Step 200: Test Loss: 3.117216548, Test Accuracy: 34.290\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [201]: Training Loss: 1.670434316, Training Accuracy: 52.688\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:06.349719\n",
            "--------------------------------------------------\n",
            "Iteration: 201/600 - True epochs: 150\n",
            "Local Step 201: Test Loss: 2.816609869, Test Accuracy: 34.460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [202]: Training Loss: 1.224656887, Training Accuracy: 64.200\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.495772\n",
            "--------------------------------------------------\n",
            "Iteration: 202/600 - True epochs: 150\n",
            "Local Step 202: Test Loss: 3.187648720, Test Accuracy: 34.850\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [203]: Training Loss: 1.578605057, Training Accuracy: 55.096\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.467179\n",
            "--------------------------------------------------\n",
            "Iteration: 203/600 - True epochs: 150\n",
            "Local Step 203: Test Loss: 3.019268455, Test Accuracy: 33.420\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [204]: Training Loss: 1.502481029, Training Accuracy: 56.664\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.332045\n",
            "--------------------------------------------------\n",
            "Iteration: 204/600 - True epochs: 150\n",
            "Local Step 204: Test Loss: 3.075180611, Test Accuracy: 33.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [205]: Training Loss: 1.244316643, Training Accuracy: 63.680\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.965040\n",
            "--------------------------------------------------\n",
            "Iteration: 205/600 - True epochs: 150\n",
            "Local Step 205: Test Loss: 3.347786578, Test Accuracy: 34.000\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [206]: Training Loss: 1.649202908, Training Accuracy: 53.816\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:06.357604\n",
            "--------------------------------------------------\n",
            "Iteration: 206/600 - True epochs: 150\n",
            "Local Step 206: Test Loss: 3.008560448, Test Accuracy: 33.550\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [207]: Training Loss: 1.218778455, Training Accuracy: 64.712\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.342161\n",
            "--------------------------------------------------\n",
            "Iteration: 207/600 - True epochs: 150\n",
            "Local Step 207: Test Loss: 3.212833790, Test Accuracy: 35.250\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [208]: Training Loss: 1.469709891, Training Accuracy: 57.856\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.448137\n",
            "--------------------------------------------------\n",
            "Iteration: 208/600 - True epochs: 150\n",
            "Local Step 208: Test Loss: 3.052414045, Test Accuracy: 34.220\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [209]: Training Loss: 1.572619552, Training Accuracy: 56.152\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.549121\n",
            "--------------------------------------------------\n",
            "Iteration: 209/600 - True epochs: 150\n",
            "Local Step 209: Test Loss: 3.048717033, Test Accuracy: 32.830\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [210]: Training Loss: 1.215060278, Training Accuracy: 64.672\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.580904\n",
            "--------------------------------------------------\n",
            "Iteration: 210/600 - True epochs: 150\n",
            "Local Step 210: Test Loss: 3.392766977, Test Accuracy: 34.370\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [211]: Training Loss: 1.658465197, Training Accuracy: 53.024\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.499310\n",
            "--------------------------------------------------\n",
            "Iteration: 211/600 - True epochs: 150\n",
            "Local Step 211: Test Loss: 2.885617879, Test Accuracy: 34.640\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [212]: Training Loss: 1.452599229, Training Accuracy: 58.248\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.925265\n",
            "--------------------------------------------------\n",
            "Iteration: 212/600 - True epochs: 150\n",
            "Local Step 212: Test Loss: 3.180984321, Test Accuracy: 32.890\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [213]: Training Loss: 1.544756545, Training Accuracy: 56.256\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.635680\n",
            "--------------------------------------------------\n",
            "Iteration: 213/600 - True epochs: 150\n",
            "Local Step 213: Test Loss: 3.041765327, Test Accuracy: 33.240\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [214]: Training Loss: 1.183475432, Training Accuracy: 65.456\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.750126\n",
            "--------------------------------------------------\n",
            "Iteration: 214/600 - True epochs: 150\n",
            "Local Step 214: Test Loss: 3.358821902, Test Accuracy: 35.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [215]: Training Loss: 1.610788889, Training Accuracy: 54.616\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.718285\n",
            "--------------------------------------------------\n",
            "Iteration: 215/600 - True epochs: 150\n",
            "Local Step 215: Test Loss: 2.969506147, Test Accuracy: 33.850\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [216]: Training Loss: 1.473671600, Training Accuracy: 58.104\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.737980\n",
            "--------------------------------------------------\n",
            "Iteration: 216/600 - True epochs: 150\n",
            "Local Step 216: Test Loss: 3.138022523, Test Accuracy: 33.060\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [217]: Training Loss: 1.194663918, Training Accuracy: 65.432\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.970264\n",
            "--------------------------------------------------\n",
            "Iteration: 217/600 - True epochs: 150\n",
            "Local Step 217: Test Loss: 3.284033917, Test Accuracy: 35.080\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [218]: Training Loss: 1.537044910, Training Accuracy: 56.304\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.378103\n",
            "--------------------------------------------------\n",
            "Iteration: 218/600 - True epochs: 150\n",
            "Local Step 218: Test Loss: 3.145928192, Test Accuracy: 33.470\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [219]: Training Loss: 1.183087890, Training Accuracy: 65.712\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.356500\n",
            "--------------------------------------------------\n",
            "Iteration: 219/600 - True epochs: 150\n",
            "Local Step 219: Test Loss: 3.350190840, Test Accuracy: 34.330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [220]: Training Loss: 1.601098140, Training Accuracy: 54.360\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.647517\n",
            "--------------------------------------------------\n",
            "Iteration: 220/600 - True epochs: 150\n",
            "Local Step 220: Test Loss: 2.972924050, Test Accuracy: 34.230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [221]: Training Loss: 1.453326388, Training Accuracy: 58.392\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.532559\n",
            "--------------------------------------------------\n",
            "Iteration: 221/600 - True epochs: 150\n",
            "Local Step 221: Test Loss: 3.106200027, Test Accuracy: 33.570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [222]: Training Loss: 1.520795759, Training Accuracy: 56.816\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.849583\n",
            "--------------------------------------------------\n",
            "Iteration: 222/600 - True epochs: 150\n",
            "Local Step 222: Test Loss: 3.092362380, Test Accuracy: 34.120\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [223]: Training Loss: 1.149700863, Training Accuracy: 66.984\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.329248\n",
            "--------------------------------------------------\n",
            "Iteration: 223/600 - True epochs: 150\n",
            "Local Step 223: Test Loss: 3.412879906, Test Accuracy: 34.670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [224]: Training Loss: 1.442011561, Training Accuracy: 58.784\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.667910\n",
            "--------------------------------------------------\n",
            "Iteration: 224/600 - True epochs: 150\n",
            "Local Step 224: Test Loss: 3.094087002, Test Accuracy: 33.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [225]: Training Loss: 1.570149094, Training Accuracy: 55.280\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.508219\n",
            "--------------------------------------------------\n",
            "Iteration: 225/600 - True epochs: 150\n",
            "Local Step 225: Test Loss: 2.977283045, Test Accuracy: 34.340\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [226]: Training Loss: 1.190616681, Training Accuracy: 65.528\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.627744\n",
            "--------------------------------------------------\n",
            "Iteration: 226/600 - True epochs: 150\n",
            "Local Step 226: Test Loss: 3.466057011, Test Accuracy: 33.940\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [227]: Training Loss: 1.489037114, Training Accuracy: 57.408\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.280710\n",
            "--------------------------------------------------\n",
            "Iteration: 227/600 - True epochs: 150\n",
            "Local Step 227: Test Loss: 3.137621151, Test Accuracy: 32.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [228]: Training Loss: 1.445198186, Training Accuracy: 58.984\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.512651\n",
            "--------------------------------------------------\n",
            "Iteration: 228/600 - True epochs: 150\n",
            "Local Step 228: Test Loss: 3.068919318, Test Accuracy: 33.710\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [229]: Training Loss: 1.216382365, Training Accuracy: 64.776\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.834015\n",
            "--------------------------------------------------\n",
            "Iteration: 229/600 - True epochs: 150\n",
            "Local Step 229: Test Loss: 3.352493134, Test Accuracy: 33.840\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [230]: Training Loss: 1.551073428, Training Accuracy: 55.384\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:06.320127\n",
            "--------------------------------------------------\n",
            "Iteration: 230/600 - True epochs: 150\n",
            "Local Step 230: Test Loss: 2.932710860, Test Accuracy: 35.120\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [231]: Training Loss: 1.492688861, Training Accuracy: 57.456\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:06.779766\n",
            "--------------------------------------------------\n",
            "Iteration: 231/600 - True epochs: 150\n",
            "Local Step 231: Test Loss: 3.107085709, Test Accuracy: 33.180\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [232]: Training Loss: 1.171705621, Training Accuracy: 66.152\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.292649\n",
            "--------------------------------------------------\n",
            "Iteration: 232/600 - True epochs: 150\n",
            "Local Step 232: Test Loss: 3.482451803, Test Accuracy: 33.760\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [233]: Training Loss: 1.387959228, Training Accuracy: 60.200\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.584275\n",
            "--------------------------------------------------\n",
            "Iteration: 233/600 - True epochs: 150\n",
            "Local Step 233: Test Loss: 3.213056198, Test Accuracy: 33.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [234]: Training Loss: 1.519587176, Training Accuracy: 56.552\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:06.024763\n",
            "--------------------------------------------------\n",
            "Iteration: 234/600 - True epochs: 150\n",
            "Local Step 234: Test Loss: 2.971087567, Test Accuracy: 34.370\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [235]: Training Loss: 1.159867739, Training Accuracy: 66.144\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.896214\n",
            "--------------------------------------------------\n",
            "Iteration: 235/600 - True epochs: 150\n",
            "Local Step 235: Test Loss: 3.414181594, Test Accuracy: 35.150\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [236]: Training Loss: 1.446611841, Training Accuracy: 58.440\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.977098\n",
            "--------------------------------------------------\n",
            "Iteration: 236/600 - True epochs: 150\n",
            "Local Step 236: Test Loss: 3.192206500, Test Accuracy: 33.640\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [237]: Training Loss: 1.385389410, Training Accuracy: 60.688\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.900432\n",
            "--------------------------------------------------\n",
            "Iteration: 237/600 - True epochs: 150\n",
            "Local Step 237: Test Loss: 3.264627077, Test Accuracy: 31.980\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [238]: Training Loss: 1.170746085, Training Accuracy: 66.056\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.159360\n",
            "--------------------------------------------------\n",
            "Iteration: 238/600 - True epochs: 150\n",
            "Local Step 238: Test Loss: 3.415806884, Test Accuracy: 34.620\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [239]: Training Loss: 1.473440081, Training Accuracy: 57.888\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.910409\n",
            "--------------------------------------------------\n",
            "Iteration: 239/600 - True epochs: 150\n",
            "Local Step 239: Test Loss: 3.094131491, Test Accuracy: 33.700\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [240]: Training Loss: 1.438177762, Training Accuracy: 59.112\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.711595\n",
            "--------------------------------------------------\n",
            "Iteration: 240/600 - True epochs: 150\n",
            "Local Step 240: Test Loss: 3.255958990, Test Accuracy: 32.500\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [241]: Training Loss: 1.349784304, Training Accuracy: 60.800\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:06.294683\n",
            "--------------------------------------------------\n",
            "Iteration: 241/600 - True epochs: 150\n",
            "Local Step 241: Test Loss: 3.118323226, Test Accuracy: 34.750\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [242]: Training Loss: 1.130545389, Training Accuracy: 67.328\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.199593\n",
            "--------------------------------------------------\n",
            "Iteration: 242/600 - True epochs: 150\n",
            "Local Step 242: Test Loss: 3.435331225, Test Accuracy: 33.530\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [243]: Training Loss: 1.118845022, Training Accuracy: 67.376\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:06.554620\n",
            "--------------------------------------------------\n",
            "Iteration: 243/600 - True epochs: 150\n",
            "Local Step 243: Test Loss: 3.492612789, Test Accuracy: 32.710\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [244]: Training Loss: 1.492633839, Training Accuracy: 57.136\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.592880\n",
            "--------------------------------------------------\n",
            "Iteration: 244/600 - True epochs: 150\n",
            "Local Step 244: Test Loss: 3.066756803, Test Accuracy: 34.320\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [245]: Training Loss: 1.336722284, Training Accuracy: 61.592\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.183594\n",
            "--------------------------------------------------\n",
            "Iteration: 245/600 - True epochs: 150\n",
            "Local Step 245: Test Loss: 3.282432020, Test Accuracy: 33.970\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [246]: Training Loss: 1.427157681, Training Accuracy: 59.000\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.189801\n",
            "--------------------------------------------------\n",
            "Iteration: 246/600 - True epochs: 150\n",
            "Local Step 246: Test Loss: 3.227478618, Test Accuracy: 32.300\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [247]: Training Loss: 1.125645609, Training Accuracy: 66.760\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.441562\n",
            "--------------------------------------------------\n",
            "Iteration: 247/600 - True epochs: 150\n",
            "Local Step 247: Test Loss: 3.583841852, Test Accuracy: 33.640\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [248]: Training Loss: 1.324212769, Training Accuracy: 61.616\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.372851\n",
            "--------------------------------------------------\n",
            "Iteration: 248/600 - True epochs: 150\n",
            "Local Step 248: Test Loss: 3.233768920, Test Accuracy: 33.000\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [249]: Training Loss: 1.480151630, Training Accuracy: 57.448\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.116117\n",
            "--------------------------------------------------\n",
            "Iteration: 249/600 - True epochs: 150\n",
            "Local Step 249: Test Loss: 3.095619038, Test Accuracy: 34.140\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [250]: Training Loss: 1.410818584, Training Accuracy: 59.248\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.231627\n",
            "--------------------------------------------------\n",
            "Iteration: 250/600 - True epochs: 150\n",
            "Local Step 250: Test Loss: 3.247766338, Test Accuracy: 33.240\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [251]: Training Loss: 1.147084807, Training Accuracy: 66.304\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.203550\n",
            "--------------------------------------------------\n",
            "Iteration: 251/600 - True epochs: 150\n",
            "Local Step 251: Test Loss: 3.494142247, Test Accuracy: 33.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [252]: Training Loss: 1.332424799, Training Accuracy: 61.728\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:05.140030\n",
            "--------------------------------------------------\n",
            "Iteration: 252/600 - True epochs: 150\n",
            "Local Step 252: Test Loss: 3.214649418, Test Accuracy: 33.350\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [253]: Training Loss: 1.100625298, Training Accuracy: 67.544\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.291741\n",
            "--------------------------------------------------\n",
            "Iteration: 253/600 - True epochs: 150\n",
            "Local Step 253: Test Loss: 3.480587370, Test Accuracy: 34.110\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 4, [254]: Training Loss: 1.436306935, Training Accuracy: 58.960\n",
            "**************************************************\n",
            "Time taken for worker 4 : 0:00:05.298223\n",
            "--------------------------------------------------\n",
            "Iteration: 254/600 - True epochs: 150\n",
            "Local Step 254: Test Loss: 3.176729529, Test Accuracy: 33.980\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 3, [255]: Training Loss: 1.387699592, Training Accuracy: 59.576\n",
            "**************************************************\n",
            "Time taken for worker 3 : 0:00:05.113218\n",
            "--------------------------------------------------\n",
            "Iteration: 255/600 - True epochs: 150\n",
            "Local Step 255: Test Loss: 3.104790763, Test Accuracy: 32.950\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [256]: Training Loss: 1.122699967, Training Accuracy: 67.080\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:05.157959\n",
            "--------------------------------------------------\n",
            "Iteration: 256/600 - True epochs: 150\n",
            "Local Step 256: Test Loss: 3.571556802, Test Accuracy: 33.270\n",
            "--------------------------------------------------\n",
            "**************************************************\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[69], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Workers:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mSHAT_PS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[68], line 38\u001b[0m, in \u001b[0;36mSHAT_PS\u001b[0;34m(shard_loaders, loss_fn, k, lr, wd, initial_state_dict, num_epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m iteration_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mis_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorker \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworker\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration_index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.9f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
            "Cell \u001b[0;32mIn[29], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn, accumulation_steps, device, is_wandb)\u001b[0m\n\u001b[1;32m      5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Initialize gradients to zero at the start of each epoch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     10\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:471\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:664\u001b[0m, in \u001b[0;36mRandomCrop.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m    img (PIL Image or Tensor): Image to be cropped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    PIL Image or Tensor: Cropped image.\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 664\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m width, height \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mget_image_size(img)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# pad the width if needed\u001b[39;00m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:485\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    483\u001b[0m     _log_api_usage_once(pad)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mpad(img, padding\u001b[38;5;241m=\u001b[39mpadding, fill\u001b[38;5;241m=\u001b[39mfill, padding_mode\u001b[38;5;241m=\u001b[39mpadding_mode)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py:201\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# RGB image\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 201\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_top\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_bottom\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_right\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Grayscale image\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/numpy/lib/arraypad.py:866\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m         roi \u001b[38;5;241m=\u001b[39m _view_roi(padded, original_area_slice, axis)\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m left_index \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m right_index \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    863\u001b[0m             \u001b[38;5;66;03m# Iteratively pad until dimension is filled with reflected\u001b[39;00m\n\u001b[1;32m    864\u001b[0m             \u001b[38;5;66;03m# values. This is necessary if the pad area is larger than\u001b[39;00m\n\u001b[1;32m    865\u001b[0m             \u001b[38;5;66;03m# the length of the original values in the current dimension.\u001b[39;00m\n\u001b[0;32m--> 866\u001b[0m             left_index, right_index \u001b[38;5;241m=\u001b[39m \u001b[43m_set_reflect_both\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[43mroi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_edge\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrap\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, (left_index, right_index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axes, pad_width):\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/numpy/lib/arraypad.py:357\u001b[0m, in \u001b[0;36m_set_reflect_both\u001b[0;34m(padded, axis, width_pair, method, include_edge)\u001b[0m\n\u001b[1;32m    352\u001b[0m     left_pad \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m chunk_length\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m right_pad \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# Pad with reflected values on right side:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# First limit chunk size which can't be larger than pad area\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     chunk_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mold_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_pad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# Slice right to left, start on or next to edge, stop relative to start\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mright_pad \u001b[38;5;241m+\u001b[39m edge_offset \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# SHAT\n",
        "lr = 1e-02\n",
        "wd = 1e-03 \n",
        "K = [4]\n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  print('='*50)\n",
        "  print(f'Number of Workers:{k}')\n",
        "  print('='*50)\n",
        "  SHAT_PS(shard_loaders, loss_fn, k, lr, wd, initial_state_dict, num_epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def LocalAdaScale(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs):\n",
        "  total_start_time = time.time()\n",
        "\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "  # global_optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "  # Initialize a scheduler for each optimizer\n",
        "  # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for loca_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    # global_model = synchronize(global_model, local_models, 1)\n",
        "    global_model = synchronize(local_models)\n",
        "\n",
        "    # for local_model in local_models:\n",
        "    #   local_model.load_state_dict(global_model.state_dict())\n",
        "    # scheduler.step()\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Local Step {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "# TODO: Schedule ro check konim \n",
        "\n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for local_SGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Our Approach 1: Layerwise Masking Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "def lock_conv_and_fc_layers(model, ratio):\n",
        "    \"\"\"\n",
        "    Locks the last `ratio` portion of the Conv2d and Linear layers in the model.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The model whose layers you want to lock.\n",
        "    - ratio (float): The ratio of Conv2d and Linear layers to lock. For example, 0.25 will lock the last 25% of such layers.\n",
        "    \"\"\"\n",
        "    # Flatten the model layers into a list (recursively)\n",
        "    layers = []\n",
        "    def get_layers(module):\n",
        "        for layer in module.children():\n",
        "            if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "                layers.append(layer)\n",
        "            if len(list(layer.children())) > 0:  # If the layer has sub-layers\n",
        "                get_layers(layer)\n",
        "    \n",
        "    get_layers(model)\n",
        "    num_layers = len(layers)\n",
        "    \n",
        "    # Calculate the number of layers to lock\n",
        "    layers_to_lock = int(ratio * num_layers)\n",
        "    if ratio * num_layers != layers_to_lock:  # If not an exact integer, round up\n",
        "        layers_to_lock += 1\n",
        "\n",
        "            \n",
        "    # Lock the last `layers_to_lock` Conv2d and Linear layers\n",
        "    count = 0\n",
        "    for layer in reversed(layers):  # Reverse the layers to lock the last ones\n",
        "        if count < layers_to_lock:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "            count += 1\n",
        "        else:\n",
        "            break\n",
        "    return layers_to_lock\n",
        "\n",
        "def synchronize(models): # average models over only unlocked parameters\n",
        "    for params in zip(*[model.parameters() for model in models]):\n",
        "        # Filter out the parameters that are locked (i.e., requires_grad is False)\n",
        "        unlocked_params = [param.data for param in params if param.requires_grad]\n",
        "        \n",
        "        # If there are any unlocked parameters, average them\n",
        "        if unlocked_params:\n",
        "            param_avg = torch.mean(torch.stack(unlocked_params), dim=0)\n",
        "            for param in params:\n",
        "                if param.requires_grad:  # Update only the unlocked parameters\n",
        "                    param.data = param_avg\n",
        "    \n",
        "    return models[0]\n",
        "\n",
        "class LMOptimizer(Optimizer):\n",
        "    def __init__(self, global_model, lr=0.01):\n",
        "        self.global_model = global_model\n",
        "        self.lr = lr\n",
        "        params = list(global_model.parameters())\n",
        "        super(LMOptimizer, self).__init__(params, {'lr': lr})\n",
        "\n",
        "    def step(self, local_models):\n",
        "        # Get global model parameters\n",
        "        global_params = list(self.global_model.parameters())\n",
        "        \n",
        "        # Initialize deltas for each parameter, same size as global parameters\n",
        "        deltas = [torch.zeros_like(param) for param in global_params]\n",
        "        \n",
        "        # Track the count of active local models for each parameter\n",
        "        active_model_counts = [0] * len(global_params)\n",
        "        \n",
        "        # Sum up differences between global model and active local models\n",
        "        for local_model in local_models:\n",
        "            local_params = list(local_model.parameters())\n",
        "            for i, param in enumerate(local_params):\n",
        "                if param.requires_grad:  # Only consider active (unlocked) parameters\n",
        "                    deltas[i] += (global_params[i] - param)\n",
        "                    active_model_counts[i] += 1  # Count this local model as active for this parameter\n",
        "        \n",
        "        # Average the delta over the number of active local models for each parameter\n",
        "        for i, delta in enumerate(deltas):\n",
        "            if active_model_counts[i] > 0:  # Only update if there are active models\n",
        "                deltas[i] /= self.lr\n",
        "                deltas[i] /= active_model_counts[i]\n",
        "        \n",
        "        # Update global model parameters\n",
        "        with torch.no_grad():\n",
        "            for i, param in enumerate(global_params):\n",
        "                if active_model_counts[i] > 0:  # Only update active parameters\n",
        "                    param.copy_(param - self.lr * deltas[i])\n",
        "        \n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def LayerwiseMaskingApproach(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, warmups):\n",
        "  total_start_time = time.time()\n",
        "  print('Start Time:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_start_time)))\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "  global_optimizer = LMOptimizer(global_model, lr=lr)\n",
        "  \n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "  \n",
        "  checkpoint = load_checkpoint('layerwise-masking', 64, {'k': k, 'j': j})\n",
        "  if checkpoint is not None:\n",
        "      global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      global_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      \n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn)\n",
        "      print(f'Global Update: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      \n",
        "      return None\n",
        "  workers_warmup_time = {}\n",
        "  print('-'*50)\n",
        "  print('Warmup rounds')\n",
        "  print('-'*50)\n",
        "\n",
        "  for warmup in range(warmups):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for local_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{local_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      time_diff = train_end_time - train_start_time\n",
        "      \n",
        "      # mock time difference for warmup\n",
        "      if worker == 1:\n",
        "        time_diff = workers_warmup_time[0] * 1.5\n",
        "      elif worker == 2:\n",
        "        time_diff = workers_warmup_time[0] * 1.75\n",
        "      elif worker == 3:\n",
        "        time_diff = workers_warmup_time[0] * 2\n",
        "        \n",
        "      workers_warmup_time[worker] = time_diff      \n",
        "      print(f'Time taken for warmup {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "\n",
        "  print(f'workers_warmup_time: {workers_warmup_time}')\n",
        "  print('Warmup finished')\n",
        "  print('-'*50)\n",
        "\n",
        "  min_time = min(workers_warmup_time.values())\n",
        "  workers_warmup_time_ratio = {worker:  (1-(min_time/time)) for worker, time in workers_warmup_time.items()}\n",
        "  print(f'scaled_workers_warmup_time: {workers_warmup_time_ratio}')\n",
        "\n",
        "  # lock models based on ratio of computation power calculated in warmup phase\n",
        "  for worker, local_model in enumerate(local_models):\n",
        "    locked_layers = lock_conv_and_fc_layers(local_model, workers_warmup_time_ratio[worker], flag = False)\n",
        "\n",
        "    print(f'Worker {worker+1} - Locked last layers : {locked_layers}') \n",
        "  \n",
        "  for iteration in range(iterations):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for local_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{local_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    global_optimizer.step(local_models)\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "    for local_model in local_models:\n",
        "       local_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Global Update {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "  \n",
        "  # Save checkpoint\n",
        "  save_checkpoint({\n",
        "              'epoch': num_epochs,\n",
        "              'model_state_dict': global_model.state_dict(),\n",
        "              'optimizer_state_dict': global_optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss': loss_fn\n",
        "              }, 149, 64, 'layerwise-masking', {'k': k, 'j': j})  \n",
        "\n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for HeteroCompSGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:2\n",
            "==================================================\n",
            "Start Time: 2024-09-06 17:58:26\n",
            "--------------------------------------------------\n",
            "Warmup rounds\n",
            "--------------------------------------------------\n",
            "Worker 1, [01/02]: Training Loss: 4.290527669, Training Accuracy: 4.696\n",
            "Worker 1, [02/02]: Training Loss: 3.832391329, Training Accuracy: 10.704\n",
            "Time taken for warmup 1: 0:00:22.373222\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 4.326378988, Training Accuracy: 4.316\n",
            "Worker 2, [02/02]: Training Loss: 3.889241827, Training Accuracy: 9.808\n",
            "Time taken for warmup 2: 0:00:22.892864\n",
            "--------------------------------------------------\n",
            "workers_warmup_time: {0: 22.37322163581848, 1: 33.55983245372772}\n",
            "Warmup finished\n",
            "--------------------------------------------------\n",
            "scaled_workers_warmup_time: {0: 0.0, 1: 0.33333333333333337}\n",
            "Worker 1 - Locked last layers : 0\n",
            "Worker 1, [01/02]: Training Loss: 3.587082469, Training Accuracy: 14.692\n",
            "Worker 1, [02/02]: Training Loss: 3.367257166, Training Accuracy: 18.496\n",
            "Time taken for training worker 1: 0:00:23.865896\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 3.790842754, Training Accuracy: 11.096\n",
            "Worker 2, [02/02]: Training Loss: 3.769265924, Training Accuracy: 11.292\n",
            "Time taken for training worker 2: 0:00:23.769126\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001615\n",
            "Local Step 01: Test Loss: 3.500100725, Test Accuracy: 17.880\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 3.202444628, Training Accuracy: 21.600\n",
            "Worker 1, [02/02]: Training Loss: 3.058261490, Training Accuracy: 24.048\n",
            "Time taken for training worker 1: 0:00:23.624545\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 3.296628659, Training Accuracy: 19.940\n",
            "Worker 2, [02/02]: Training Loss: 3.257390379, Training Accuracy: 20.708\n",
            "Time taken for training worker 2: 0:00:23.035000\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001788\n",
            "Local Step 02: Test Loss: 2.962082937, Test Accuracy: 26.150\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.950582330, Training Accuracy: 26.132\n",
            "Worker 1, [02/02]: Training Loss: 2.819966453, Training Accuracy: 28.388\n",
            "Time taken for training worker 1: 0:00:22.431792\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 3.018570879, Training Accuracy: 25.008\n",
            "Worker 2, [02/02]: Training Loss: 3.009115379, Training Accuracy: 25.392\n",
            "Time taken for training worker 2: 0:00:22.445446\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001505\n",
            "Local Step 03: Test Loss: 2.859062555, Test Accuracy: 29.860\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.740713280, Training Accuracy: 30.308\n",
            "Worker 1, [02/02]: Training Loss: 2.653039779, Training Accuracy: 31.776\n",
            "Time taken for training worker 1: 0:00:27.531230\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.866645725, Training Accuracy: 28.628\n",
            "Worker 2, [02/02]: Training Loss: 2.839655531, Training Accuracy: 29.180\n",
            "Time taken for training worker 2: 0:00:24.626469\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001657\n",
            "Local Step 04: Test Loss: 2.685148775, Test Accuracy: 32.390\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.597041404, Training Accuracy: 33.388\n",
            "Worker 1, [02/02]: Training Loss: 2.509030467, Training Accuracy: 34.488\n",
            "Time taken for training worker 1: 0:00:23.857126\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.788882461, Training Accuracy: 30.340\n",
            "Worker 2, [02/02]: Training Loss: 2.763797607, Training Accuracy: 30.572\n",
            "Time taken for training worker 2: 0:00:22.948342\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001925\n",
            "Local Step 05: Test Loss: 2.625534673, Test Accuracy: 33.960\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.466929089, Training Accuracy: 35.652\n",
            "Worker 1, [02/02]: Training Loss: 2.419764997, Training Accuracy: 36.952\n",
            "Time taken for training worker 1: 0:00:23.314776\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.693429351, Training Accuracy: 31.964\n",
            "Worker 2, [02/02]: Training Loss: 2.681338858, Training Accuracy: 32.092\n",
            "Time taken for training worker 2: 0:00:25.465023\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001460\n",
            "Local Step 06: Test Loss: 2.554248887, Test Accuracy: 36.290\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.371777893, Training Accuracy: 37.480\n",
            "Worker 1, [02/02]: Training Loss: 2.301943991, Training Accuracy: 39.100\n",
            "Time taken for training worker 1: 0:00:25.037987\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.648813900, Training Accuracy: 33.104\n",
            "Worker 2, [02/02]: Training Loss: 2.630268164, Training Accuracy: 33.420\n",
            "Time taken for training worker 2: 0:00:22.516968\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001996\n",
            "Local Step 07: Test Loss: 2.453941909, Test Accuracy: 37.690\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.275446013, Training Accuracy: 39.516\n",
            "Worker 1, [02/02]: Training Loss: 2.229106866, Training Accuracy: 40.764\n",
            "Time taken for training worker 1: 0:00:21.759237\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.590678065, Training Accuracy: 34.312\n",
            "Worker 2, [02/02]: Training Loss: 2.571466989, Training Accuracy: 35.020\n",
            "Time taken for training worker 2: 0:00:22.182629\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001627\n",
            "Local Step 08: Test Loss: 2.444015859, Test Accuracy: 39.240\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.194231431, Training Accuracy: 41.528\n",
            "Worker 1, [02/02]: Training Loss: 2.164424483, Training Accuracy: 42.152\n",
            "Time taken for training worker 1: 0:00:21.995038\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.566542088, Training Accuracy: 35.044\n",
            "Worker 2, [02/02]: Training Loss: 2.558576890, Training Accuracy: 35.220\n",
            "Time taken for training worker 2: 0:00:21.288474\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001590\n",
            "Local Step 09: Test Loss: 2.465223109, Test Accuracy: 38.860\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.138646897, Training Accuracy: 43.240\n",
            "Worker 1, [02/02]: Training Loss: 2.116147523, Training Accuracy: 43.604\n",
            "Time taken for training worker 1: 0:00:21.760995\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.558524659, Training Accuracy: 35.228\n",
            "Worker 2, [02/02]: Training Loss: 2.537161800, Training Accuracy: 35.672\n",
            "Time taken for training worker 2: 0:00:21.521258\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001490\n",
            "Local Step 10: Test Loss: 2.451317803, Test Accuracy: 39.030\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.069621216, Training Accuracy: 44.136\n",
            "Worker 1, [02/02]: Training Loss: 2.043012592, Training Accuracy: 45.156\n",
            "Time taken for training worker 1: 0:00:22.083412\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.543218975, Training Accuracy: 35.772\n",
            "Worker 2, [02/02]: Training Loss: 2.520807382, Training Accuracy: 36.020\n",
            "Time taken for training worker 2: 0:00:22.471362\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001420\n",
            "Local Step 11: Test Loss: 2.346965346, Test Accuracy: 41.290\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.014991653, Training Accuracy: 45.576\n",
            "Worker 1, [02/02]: Training Loss: 1.990724942, Training Accuracy: 46.276\n",
            "Time taken for training worker 1: 0:00:22.161007\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.509224182, Training Accuracy: 36.592\n",
            "Worker 2, [02/02]: Training Loss: 2.491149893, Training Accuracy: 36.912\n",
            "Time taken for training worker 2: 0:00:22.399176\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001503\n",
            "Local Step 12: Test Loss: 2.445360392, Test Accuracy: 38.750\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.965184027, Training Accuracy: 46.792\n",
            "Worker 1, [02/02]: Training Loss: 1.937714763, Training Accuracy: 47.244\n",
            "Time taken for training worker 1: 0:00:24.019929\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.557266332, Training Accuracy: 35.336\n",
            "Worker 2, [02/02]: Training Loss: 2.510064627, Training Accuracy: 36.044\n",
            "Time taken for training worker 2: 0:00:23.743356\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001691\n",
            "Local Step 13: Test Loss: 2.348031481, Test Accuracy: 41.380\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.919454732, Training Accuracy: 47.228\n",
            "Worker 1, [02/02]: Training Loss: 1.892705325, Training Accuracy: 48.076\n",
            "Time taken for training worker 1: 0:00:21.980752\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.513827001, Training Accuracy: 36.616\n",
            "Worker 2, [02/02]: Training Loss: 2.486923448, Training Accuracy: 36.724\n",
            "Time taken for training worker 2: 0:00:24.282125\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002105\n",
            "Local Step 14: Test Loss: 2.404784720, Test Accuracy: 41.270\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.888638853, Training Accuracy: 48.412\n",
            "Worker 1, [02/02]: Training Loss: 1.853028950, Training Accuracy: 49.092\n",
            "Time taken for training worker 1: 0:00:25.379113\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.516300385, Training Accuracy: 36.932\n",
            "Worker 2, [02/02]: Training Loss: 2.471312576, Training Accuracy: 37.308\n",
            "Time taken for training worker 2: 0:00:21.548341\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001678\n",
            "Local Step 15: Test Loss: 2.386479589, Test Accuracy: 40.590\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.854574523, Training Accuracy: 49.108\n",
            "Worker 1, [02/02]: Training Loss: 1.821305182, Training Accuracy: 49.944\n",
            "Time taken for training worker 1: 0:00:22.273587\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.487202982, Training Accuracy: 37.228\n",
            "Worker 2, [02/02]: Training Loss: 2.450304940, Training Accuracy: 37.708\n",
            "Time taken for training worker 2: 0:00:22.453641\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001688\n",
            "Local Step 16: Test Loss: 2.363624590, Test Accuracy: 42.080\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.814139439, Training Accuracy: 49.924\n",
            "Worker 1, [02/02]: Training Loss: 1.791598729, Training Accuracy: 50.344\n",
            "Time taken for training worker 1: 0:00:23.317398\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.486378494, Training Accuracy: 37.224\n",
            "Worker 2, [02/02]: Training Loss: 2.447981401, Training Accuracy: 38.292\n",
            "Time taken for training worker 2: 0:00:22.795209\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001402\n",
            "Local Step 17: Test Loss: 2.332669089, Test Accuracy: 42.040\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.766559591, Training Accuracy: 51.316\n",
            "Worker 1, [02/02]: Training Loss: 1.761351722, Training Accuracy: 51.460\n",
            "Time taken for training worker 1: 0:00:21.557531\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.482238538, Training Accuracy: 37.460\n",
            "Worker 2, [02/02]: Training Loss: 2.452090391, Training Accuracy: 38.288\n",
            "Time taken for training worker 2: 0:00:23.433939\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001963\n",
            "Local Step 18: Test Loss: 2.315288505, Test Accuracy: 42.380\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.748432952, Training Accuracy: 52.036\n",
            "Worker 1, [02/02]: Training Loss: 1.737147491, Training Accuracy: 51.588\n",
            "Time taken for training worker 1: 0:00:22.296912\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.476137705, Training Accuracy: 37.836\n",
            "Worker 2, [02/02]: Training Loss: 2.417028394, Training Accuracy: 38.688\n",
            "Time taken for training worker 2: 0:00:23.034950\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001776\n",
            "Local Step 19: Test Loss: 2.413422970, Test Accuracy: 41.460\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.726105348, Training Accuracy: 52.324\n",
            "Worker 1, [02/02]: Training Loss: 1.690001517, Training Accuracy: 52.644\n",
            "Time taken for training worker 1: 0:00:22.375039\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.486078296, Training Accuracy: 37.704\n",
            "Worker 2, [02/02]: Training Loss: 2.459722223, Training Accuracy: 37.532\n",
            "Time taken for training worker 2: 0:00:22.091006\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001631\n",
            "Local Step 20: Test Loss: 2.352641545, Test Accuracy: 42.570\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.699069939, Training Accuracy: 52.780\n",
            "Worker 1, [02/02]: Training Loss: 1.673996424, Training Accuracy: 52.976\n",
            "Time taken for training worker 1: 0:00:21.978727\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.488457639, Training Accuracy: 37.648\n",
            "Worker 2, [02/02]: Training Loss: 2.437383897, Training Accuracy: 38.212\n",
            "Time taken for training worker 2: 0:00:21.917856\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002353\n",
            "Local Step 21: Test Loss: 2.356413235, Test Accuracy: 42.080\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.675822654, Training Accuracy: 53.528\n",
            "Worker 1, [02/02]: Training Loss: 1.670517617, Training Accuracy: 53.848\n",
            "Time taken for training worker 1: 0:00:21.756851\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.499893719, Training Accuracy: 37.284\n",
            "Worker 2, [02/02]: Training Loss: 2.459228275, Training Accuracy: 38.048\n",
            "Time taken for training worker 2: 0:00:22.845090\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001815\n",
            "Local Step 22: Test Loss: 2.388679511, Test Accuracy: 42.380\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.632716136, Training Accuracy: 54.692\n",
            "Worker 1, [02/02]: Training Loss: 1.645794422, Training Accuracy: 54.140\n",
            "Time taken for training worker 1: 0:00:24.702641\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.498210115, Training Accuracy: 37.444\n",
            "Worker 2, [02/02]: Training Loss: 2.440571508, Training Accuracy: 38.204\n",
            "Time taken for training worker 2: 0:00:21.945238\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001462\n",
            "Local Step 23: Test Loss: 2.321313159, Test Accuracy: 43.200\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.619949468, Training Accuracy: 54.608\n",
            "Worker 1, [02/02]: Training Loss: 1.625097026, Training Accuracy: 54.512\n",
            "Time taken for training worker 1: 0:00:22.746008\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.483365749, Training Accuracy: 37.676\n",
            "Worker 2, [02/02]: Training Loss: 2.423757018, Training Accuracy: 38.600\n",
            "Time taken for training worker 2: 0:00:21.909421\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001509\n",
            "Local Step 24: Test Loss: 2.393166506, Test Accuracy: 42.980\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.609830752, Training Accuracy: 55.168\n",
            "Worker 1, [02/02]: Training Loss: 1.613358727, Training Accuracy: 55.040\n",
            "Time taken for training worker 1: 0:00:21.912014\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.508496139, Training Accuracy: 37.352\n",
            "Worker 2, [02/02]: Training Loss: 2.447721106, Training Accuracy: 38.684\n",
            "Time taken for training worker 2: 0:00:22.854106\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001549\n",
            "Local Step 25: Test Loss: 2.363894492, Test Accuracy: 43.070\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.579354839, Training Accuracy: 55.616\n",
            "Worker 1, [02/02]: Training Loss: 1.573867321, Training Accuracy: 55.804\n",
            "Time taken for training worker 1: 0:00:24.122938\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.509115215, Training Accuracy: 37.480\n",
            "Worker 2, [02/02]: Training Loss: 2.434356579, Training Accuracy: 38.632\n",
            "Time taken for training worker 2: 0:00:21.856620\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001559\n",
            "Local Step 26: Test Loss: 2.318720887, Test Accuracy: 43.960\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.550798269, Training Accuracy: 56.380\n",
            "Worker 1, [02/02]: Training Loss: 1.563279862, Training Accuracy: 55.912\n",
            "Time taken for training worker 1: 0:00:24.544040\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.498559826, Training Accuracy: 37.976\n",
            "Worker 2, [02/02]: Training Loss: 2.434250929, Training Accuracy: 38.784\n",
            "Time taken for training worker 2: 0:00:22.322934\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001463\n",
            "Local Step 27: Test Loss: 2.354200635, Test Accuracy: 44.460\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.554319806, Training Accuracy: 56.152\n",
            "Worker 1, [02/02]: Training Loss: 1.543350136, Training Accuracy: 56.688\n",
            "Time taken for training worker 1: 0:00:21.772406\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.485387328, Training Accuracy: 38.056\n",
            "Worker 2, [02/02]: Training Loss: 2.433014573, Training Accuracy: 38.516\n",
            "Time taken for training worker 2: 0:00:21.621946\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001676\n",
            "Local Step 28: Test Loss: 2.333210640, Test Accuracy: 43.780\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.533643692, Training Accuracy: 56.668\n",
            "Worker 1, [02/02]: Training Loss: 1.530305803, Training Accuracy: 57.148\n",
            "Time taken for training worker 1: 0:00:23.103099\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.488210514, Training Accuracy: 37.884\n",
            "Worker 2, [02/02]: Training Loss: 2.434466954, Training Accuracy: 39.092\n",
            "Time taken for training worker 2: 0:00:22.329925\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001610\n",
            "Local Step 29: Test Loss: 2.417057867, Test Accuracy: 42.380\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.524463493, Training Accuracy: 56.916\n",
            "Worker 1, [02/02]: Training Loss: 1.519832825, Training Accuracy: 57.204\n",
            "Time taken for training worker 1: 0:00:23.143772\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.504241331, Training Accuracy: 37.940\n",
            "Worker 2, [02/02]: Training Loss: 2.444262962, Training Accuracy: 38.452\n",
            "Time taken for training worker 2: 0:00:24.061145\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001534\n",
            "Local Step 30: Test Loss: 2.405174254, Test Accuracy: 42.560\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.527065466, Training Accuracy: 56.568\n",
            "Worker 1, [02/02]: Training Loss: 1.514051130, Training Accuracy: 57.188\n",
            "Time taken for training worker 1: 0:00:22.943667\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.503186067, Training Accuracy: 37.524\n",
            "Worker 2, [02/02]: Training Loss: 2.429743035, Training Accuracy: 39.268\n",
            "Time taken for training worker 2: 0:00:22.108898\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001433\n",
            "Local Step 31: Test Loss: 2.315220248, Test Accuracy: 43.660\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.492851496, Training Accuracy: 57.820\n",
            "Worker 1, [02/02]: Training Loss: 1.482861079, Training Accuracy: 58.116\n",
            "Time taken for training worker 1: 0:00:22.591836\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.487742184, Training Accuracy: 38.100\n",
            "Worker 2, [02/02]: Training Loss: 2.435027575, Training Accuracy: 38.952\n",
            "Time taken for training worker 2: 0:00:20.786774\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001504\n",
            "Local Step 32: Test Loss: 2.416404238, Test Accuracy: 42.980\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.491852126, Training Accuracy: 57.508\n",
            "Worker 1, [02/02]: Training Loss: 1.490769364, Training Accuracy: 58.048\n",
            "Time taken for training worker 1: 0:00:20.697031\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.536036813, Training Accuracy: 37.364\n",
            "Worker 2, [02/02]: Training Loss: 2.465352339, Training Accuracy: 38.260\n",
            "Time taken for training worker 2: 0:00:20.817208\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001401\n",
            "Local Step 33: Test Loss: 2.466190419, Test Accuracy: 42.850\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.483726752, Training Accuracy: 58.392\n",
            "Worker 1, [02/02]: Training Loss: 1.465978363, Training Accuracy: 58.392\n",
            "Time taken for training worker 1: 0:00:20.375763\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.506412861, Training Accuracy: 38.124\n",
            "Worker 2, [02/02]: Training Loss: 2.447334153, Training Accuracy: 38.672\n",
            "Time taken for training worker 2: 0:00:20.252556\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001566\n",
            "Local Step 34: Test Loss: 2.409797703, Test Accuracy: 42.550\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.476014637, Training Accuracy: 58.168\n",
            "Worker 1, [02/02]: Training Loss: 1.449576632, Training Accuracy: 58.968\n",
            "Time taken for training worker 1: 0:00:22.666767\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.510122571, Training Accuracy: 37.832\n",
            "Worker 2, [02/02]: Training Loss: 2.441274159, Training Accuracy: 38.856\n",
            "Time taken for training worker 2: 0:00:21.372254\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001559\n",
            "Local Step 35: Test Loss: 2.385074815, Test Accuracy: 42.870\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.444626389, Training Accuracy: 58.828\n",
            "Worker 1, [02/02]: Training Loss: 1.455641691, Training Accuracy: 58.420\n",
            "Time taken for training worker 1: 0:00:21.626518\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.492208631, Training Accuracy: 38.116\n",
            "Worker 2, [02/02]: Training Loss: 2.430567564, Training Accuracy: 38.792\n",
            "Time taken for training worker 2: 0:00:21.179286\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001456\n",
            "Local Step 36: Test Loss: 2.352884138, Test Accuracy: 43.910\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.450056595, Training Accuracy: 58.532\n",
            "Worker 1, [02/02]: Training Loss: 1.448861092, Training Accuracy: 58.976\n",
            "Time taken for training worker 1: 0:00:20.430006\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.493991286, Training Accuracy: 38.132\n",
            "Worker 2, [02/02]: Training Loss: 2.426207613, Training Accuracy: 39.196\n",
            "Time taken for training worker 2: 0:00:21.661656\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001788\n",
            "Local Step 37: Test Loss: 2.428536007, Test Accuracy: 42.750\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.434668603, Training Accuracy: 59.232\n",
            "Worker 1, [02/02]: Training Loss: 1.437076298, Training Accuracy: 59.012\n",
            "Time taken for training worker 1: 0:00:20.328344\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.514108139, Training Accuracy: 38.008\n",
            "Worker 2, [02/02]: Training Loss: 2.430462478, Training Accuracy: 39.032\n",
            "Time taken for training worker 2: 0:00:19.816525\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001345\n",
            "Local Step 38: Test Loss: 2.322845319, Test Accuracy: 44.260\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.428030535, Training Accuracy: 59.564\n",
            "Worker 1, [02/02]: Training Loss: 1.435066153, Training Accuracy: 58.728\n",
            "Time taken for training worker 1: 0:00:22.750678\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.496831923, Training Accuracy: 37.928\n",
            "Worker 2, [02/02]: Training Loss: 2.424053962, Training Accuracy: 39.184\n",
            "Time taken for training worker 2: 0:00:21.190224\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001511\n",
            "Local Step 39: Test Loss: 2.309952800, Test Accuracy: 45.060\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.413501342, Training Accuracy: 59.592\n",
            "Worker 1, [02/02]: Training Loss: 1.398115451, Training Accuracy: 59.992\n",
            "Time taken for training worker 1: 0:00:20.169426\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.498024915, Training Accuracy: 37.756\n",
            "Worker 2, [02/02]: Training Loss: 2.439266600, Training Accuracy: 38.684\n",
            "Time taken for training worker 2: 0:00:20.295372\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002276\n",
            "Local Step 40: Test Loss: 2.345957329, Test Accuracy: 44.430\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.405854874, Training Accuracy: 59.952\n",
            "Worker 1, [02/02]: Training Loss: 1.396269006, Training Accuracy: 59.968\n",
            "Time taken for training worker 1: 0:00:21.031804\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.480278407, Training Accuracy: 38.256\n",
            "Worker 2, [02/02]: Training Loss: 2.435418072, Training Accuracy: 39.336\n",
            "Time taken for training worker 2: 0:00:22.919760\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001570\n",
            "Local Step 41: Test Loss: 2.453646172, Test Accuracy: 42.440\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.386362356, Training Accuracy: 60.136\n",
            "Worker 1, [02/02]: Training Loss: 1.389138365, Training Accuracy: 60.444\n",
            "Time taken for training worker 1: 0:00:20.897578\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.523863952, Training Accuracy: 37.828\n",
            "Worker 2, [02/02]: Training Loss: 2.455175061, Training Accuracy: 38.588\n",
            "Time taken for training worker 2: 0:00:20.308581\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001456\n",
            "Local Step 42: Test Loss: 2.397267053, Test Accuracy: 43.440\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.405681047, Training Accuracy: 59.884\n",
            "Worker 1, [02/02]: Training Loss: 1.400961812, Training Accuracy: 59.880\n",
            "Time taken for training worker 1: 0:00:20.855228\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.524850691, Training Accuracy: 37.576\n",
            "Worker 2, [02/02]: Training Loss: 2.463987088, Training Accuracy: 38.312\n",
            "Time taken for training worker 2: 0:00:20.032241\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001632\n",
            "Local Step 43: Test Loss: 2.419159825, Test Accuracy: 43.590\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.370137335, Training Accuracy: 60.572\n",
            "Worker 1, [02/02]: Training Loss: 1.373788610, Training Accuracy: 60.452\n",
            "Time taken for training worker 1: 0:00:20.162529\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.490997071, Training Accuracy: 38.144\n",
            "Worker 2, [02/02]: Training Loss: 2.449329409, Training Accuracy: 39.084\n",
            "Time taken for training worker 2: 0:00:19.855196\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001503\n",
            "Local Step 44: Test Loss: 2.265148376, Test Accuracy: 44.650\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.368001031, Training Accuracy: 61.176\n",
            "Worker 1, [02/02]: Training Loss: 1.372778887, Training Accuracy: 60.640\n",
            "Time taken for training worker 1: 0:00:20.781656\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.479649678, Training Accuracy: 38.432\n",
            "Worker 2, [02/02]: Training Loss: 2.436443292, Training Accuracy: 39.116\n",
            "Time taken for training worker 2: 0:00:20.141786\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001412\n",
            "Local Step 45: Test Loss: 2.401432911, Test Accuracy: 44.640\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.373748695, Training Accuracy: 60.672\n",
            "Worker 1, [02/02]: Training Loss: 1.347050895, Training Accuracy: 61.152\n",
            "Time taken for training worker 1: 0:00:21.769069\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.512639708, Training Accuracy: 38.268\n",
            "Worker 2, [02/02]: Training Loss: 2.445832553, Training Accuracy: 38.560\n",
            "Time taken for training worker 2: 0:00:21.592344\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001507\n",
            "Local Step 46: Test Loss: 2.436157764, Test Accuracy: 43.300\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.389038344, Training Accuracy: 60.224\n",
            "Worker 1, [02/02]: Training Loss: 1.361598244, Training Accuracy: 60.784\n",
            "Time taken for training worker 1: 0:00:22.181248\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.539233453, Training Accuracy: 37.440\n",
            "Worker 2, [02/02]: Training Loss: 2.463525514, Training Accuracy: 38.056\n",
            "Time taken for training worker 2: 0:00:20.001198\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001588\n",
            "Local Step 47: Test Loss: 2.453342755, Test Accuracy: 42.920\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.378521301, Training Accuracy: 60.392\n",
            "Worker 1, [02/02]: Training Loss: 1.350813310, Training Accuracy: 61.272\n",
            "Time taken for training worker 1: 0:00:20.537071\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.517256559, Training Accuracy: 37.512\n",
            "Worker 2, [02/02]: Training Loss: 2.437788545, Training Accuracy: 39.000\n",
            "Time taken for training worker 2: 0:00:20.529463\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001333\n",
            "Local Step 48: Test Loss: 2.382312422, Test Accuracy: 43.520\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.354384845, Training Accuracy: 61.112\n",
            "Worker 1, [02/02]: Training Loss: 1.348051920, Training Accuracy: 61.452\n",
            "Time taken for training worker 1: 0:00:20.628700\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.520955131, Training Accuracy: 37.572\n",
            "Worker 2, [02/02]: Training Loss: 2.454392994, Training Accuracy: 38.180\n",
            "Time taken for training worker 2: 0:00:20.121766\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001522\n",
            "Local Step 49: Test Loss: 2.387226759, Test Accuracy: 43.860\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.347467033, Training Accuracy: 61.304\n",
            "Worker 1, [02/02]: Training Loss: 1.339839498, Training Accuracy: 61.960\n",
            "Time taken for training worker 1: 0:00:20.210274\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.513883806, Training Accuracy: 38.032\n",
            "Worker 2, [02/02]: Training Loss: 2.455950837, Training Accuracy: 38.372\n",
            "Time taken for training worker 2: 0:00:19.926613\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001478\n",
            "Local Step 50: Test Loss: 2.343832036, Test Accuracy: 43.730\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.332858799, Training Accuracy: 61.612\n",
            "Worker 1, [02/02]: Training Loss: 1.351504222, Training Accuracy: 61.220\n",
            "Time taken for training worker 1: 0:00:22.877095\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.515185998, Training Accuracy: 38.208\n",
            "Worker 2, [02/02]: Training Loss: 2.443524730, Training Accuracy: 38.772\n",
            "Time taken for training worker 2: 0:00:22.033976\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001725\n",
            "Local Step 51: Test Loss: 2.357663500, Test Accuracy: 44.070\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.353474709, Training Accuracy: 61.484\n",
            "Worker 1, [02/02]: Training Loss: 1.344833467, Training Accuracy: 61.672\n",
            "Time taken for training worker 1: 0:00:21.359202\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.508054409, Training Accuracy: 38.340\n",
            "Worker 2, [02/02]: Training Loss: 2.451701799, Training Accuracy: 39.060\n",
            "Time taken for training worker 2: 0:00:21.218272\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001444\n",
            "Local Step 52: Test Loss: 2.514947601, Test Accuracy: 42.200\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.335091231, Training Accuracy: 61.760\n",
            "Worker 1, [02/02]: Training Loss: 1.359077850, Training Accuracy: 60.976\n",
            "Time taken for training worker 1: 0:00:21.326742\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.542312615, Training Accuracy: 37.436\n",
            "Worker 2, [02/02]: Training Loss: 2.474165072, Training Accuracy: 38.352\n",
            "Time taken for training worker 2: 0:00:20.694866\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001629\n",
            "Local Step 53: Test Loss: 2.428120357, Test Accuracy: 43.140\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.335150578, Training Accuracy: 61.968\n",
            "Worker 1, [02/02]: Training Loss: 1.333923660, Training Accuracy: 61.816\n",
            "Time taken for training worker 1: 0:00:20.291440\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.518990471, Training Accuracy: 37.656\n",
            "Worker 2, [02/02]: Training Loss: 2.466158329, Training Accuracy: 38.200\n",
            "Time taken for training worker 2: 0:00:21.097869\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001692\n",
            "Local Step 54: Test Loss: 2.427106717, Test Accuracy: 44.260\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.311092789, Training Accuracy: 62.192\n",
            "Worker 1, [02/02]: Training Loss: 1.321324878, Training Accuracy: 62.192\n",
            "Time taken for training worker 1: 0:00:22.219799\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.538416418, Training Accuracy: 37.708\n",
            "Worker 2, [02/02]: Training Loss: 2.458416026, Training Accuracy: 38.800\n",
            "Time taken for training worker 2: 0:00:20.512802\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001458\n",
            "Local Step 55: Test Loss: 2.321293854, Test Accuracy: 44.590\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.327136461, Training Accuracy: 61.744\n",
            "Worker 1, [02/02]: Training Loss: 1.334663573, Training Accuracy: 61.320\n",
            "Time taken for training worker 1: 0:00:20.109670\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.479009976, Training Accuracy: 38.992\n",
            "Worker 2, [02/02]: Training Loss: 2.420788645, Training Accuracy: 39.520\n",
            "Time taken for training worker 2: 0:00:20.128552\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001477\n",
            "Local Step 56: Test Loss: 2.351827729, Test Accuracy: 44.160\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.310841275, Training Accuracy: 62.200\n",
            "Worker 1, [02/02]: Training Loss: 1.310488724, Training Accuracy: 62.148\n",
            "Time taken for training worker 1: 0:00:20.394664\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.517326926, Training Accuracy: 38.056\n",
            "Worker 2, [02/02]: Training Loss: 2.445811356, Training Accuracy: 38.604\n",
            "Time taken for training worker 2: 0:00:22.071914\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001364\n",
            "Local Step 57: Test Loss: 2.386965294, Test Accuracy: 44.840\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.287440006, Training Accuracy: 63.120\n",
            "Worker 1, [02/02]: Training Loss: 1.303652439, Training Accuracy: 62.496\n",
            "Time taken for training worker 1: 0:00:20.667332\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.520246574, Training Accuracy: 38.168\n",
            "Worker 2, [02/02]: Training Loss: 2.447258490, Training Accuracy: 38.736\n",
            "Time taken for training worker 2: 0:00:21.217369\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001500\n",
            "Local Step 58: Test Loss: 2.423226167, Test Accuracy: 44.360\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.311438237, Training Accuracy: 62.260\n",
            "Worker 1, [02/02]: Training Loss: 1.311497387, Training Accuracy: 62.012\n",
            "Time taken for training worker 1: 0:00:20.505806\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.507489740, Training Accuracy: 38.076\n",
            "Worker 2, [02/02]: Training Loss: 2.427387502, Training Accuracy: 39.312\n",
            "Time taken for training worker 2: 0:00:20.390730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001418\n",
            "Local Step 59: Test Loss: 2.390098469, Test Accuracy: 43.950\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.316310584, Training Accuracy: 62.264\n",
            "Worker 1, [02/02]: Training Loss: 1.307626642, Training Accuracy: 62.308\n",
            "Time taken for training worker 1: 0:00:21.192218\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.533121901, Training Accuracy: 37.684\n",
            "Worker 2, [02/02]: Training Loss: 2.460591430, Training Accuracy: 38.416\n",
            "Time taken for training worker 2: 0:00:20.406842\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001578\n",
            "Local Step 60: Test Loss: 2.448824236, Test Accuracy: 44.300\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.311855686, Training Accuracy: 62.148\n",
            "Worker 1, [02/02]: Training Loss: 1.302609980, Training Accuracy: 62.788\n",
            "Time taken for training worker 1: 0:00:22.972214\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.526498557, Training Accuracy: 37.720\n",
            "Worker 2, [02/02]: Training Loss: 2.470327294, Training Accuracy: 38.528\n",
            "Time taken for training worker 2: 0:00:21.854232\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001563\n",
            "Local Step 61: Test Loss: 2.379190046, Test Accuracy: 43.580\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.287811871, Training Accuracy: 63.032\n",
            "Worker 1, [02/02]: Training Loss: 1.289787180, Training Accuracy: 62.896\n",
            "Time taken for training worker 1: 0:00:21.141584\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.513688762, Training Accuracy: 37.980\n",
            "Worker 2, [02/02]: Training Loss: 2.427151925, Training Accuracy: 39.148\n",
            "Time taken for training worker 2: 0:00:21.354012\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001581\n",
            "Local Step 62: Test Loss: 2.460817671, Test Accuracy: 44.150\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.314179095, Training Accuracy: 62.096\n",
            "Worker 1, [02/02]: Training Loss: 1.289408280, Training Accuracy: 63.064\n",
            "Time taken for training worker 1: 0:00:20.247339\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.542807205, Training Accuracy: 37.532\n",
            "Worker 2, [02/02]: Training Loss: 2.451293711, Training Accuracy: 38.668\n",
            "Time taken for training worker 2: 0:00:21.448027\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001412\n",
            "Local Step 63: Test Loss: 2.343817993, Test Accuracy: 44.050\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.284051638, Training Accuracy: 62.664\n",
            "Worker 1, [02/02]: Training Loss: 1.283081003, Training Accuracy: 62.996\n",
            "Time taken for training worker 1: 0:00:20.208025\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.529923001, Training Accuracy: 37.652\n",
            "Worker 2, [02/02]: Training Loss: 2.453115209, Training Accuracy: 38.768\n",
            "Time taken for training worker 2: 0:00:20.086220\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001377\n",
            "Local Step 64: Test Loss: 2.444268779, Test Accuracy: 43.890\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.270461639, Training Accuracy: 63.096\n",
            "Worker 1, [02/02]: Training Loss: 1.321217468, Training Accuracy: 61.652\n",
            "Time taken for training worker 1: 0:00:22.246543\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.508221034, Training Accuracy: 38.208\n",
            "Worker 2, [02/02]: Training Loss: 2.443145780, Training Accuracy: 39.000\n",
            "Time taken for training worker 2: 0:00:20.453685\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001456\n",
            "Local Step 65: Test Loss: 2.372630499, Test Accuracy: 44.030\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.284277474, Training Accuracy: 63.140\n",
            "Worker 1, [02/02]: Training Loss: 1.272117463, Training Accuracy: 63.316\n",
            "Time taken for training worker 1: 0:00:21.387450\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.499991036, Training Accuracy: 38.520\n",
            "Worker 2, [02/02]: Training Loss: 2.434477590, Training Accuracy: 39.012\n",
            "Time taken for training worker 2: 0:00:20.854141\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001639\n",
            "Local Step 66: Test Loss: 2.433904817, Test Accuracy: 44.020\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.263835499, Training Accuracy: 63.424\n",
            "Worker 1, [02/02]: Training Loss: 1.282405513, Training Accuracy: 62.720\n",
            "Time taken for training worker 1: 0:00:20.540825\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.490111369, Training Accuracy: 38.436\n",
            "Worker 2, [02/02]: Training Loss: 2.443927366, Training Accuracy: 38.804\n",
            "Time taken for training worker 2: 0:00:20.377397\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001486\n",
            "Local Step 67: Test Loss: 2.424316557, Test Accuracy: 43.360\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.283823515, Training Accuracy: 62.900\n",
            "Worker 1, [02/02]: Training Loss: 1.267093021, Training Accuracy: 63.264\n",
            "Time taken for training worker 1: 0:00:20.666724\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.538514649, Training Accuracy: 37.784\n",
            "Worker 2, [02/02]: Training Loss: 2.452298414, Training Accuracy: 38.592\n",
            "Time taken for training worker 2: 0:00:21.847235\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001461\n",
            "Local Step 68: Test Loss: 2.433572232, Test Accuracy: 43.890\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.286432608, Training Accuracy: 62.620\n",
            "Worker 1, [02/02]: Training Loss: 1.284335533, Training Accuracy: 62.988\n",
            "Time taken for training worker 1: 0:00:21.213686\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.530558841, Training Accuracy: 37.764\n",
            "Worker 2, [02/02]: Training Loss: 2.446057639, Training Accuracy: 38.652\n",
            "Time taken for training worker 2: 0:00:20.527935\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001471\n",
            "Local Step 69: Test Loss: 2.450602446, Test Accuracy: 42.850\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.260904785, Training Accuracy: 63.636\n",
            "Worker 1, [02/02]: Training Loss: 1.249551596, Training Accuracy: 63.760\n",
            "Time taken for training worker 1: 0:00:21.869743\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.530087371, Training Accuracy: 38.092\n",
            "Worker 2, [02/02]: Training Loss: 2.450190552, Training Accuracy: 38.756\n",
            "Time taken for training worker 2: 0:00:22.123812\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001361\n",
            "Local Step 70: Test Loss: 2.422469437, Test Accuracy: 43.490\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.271750648, Training Accuracy: 63.252\n",
            "Worker 1, [02/02]: Training Loss: 1.256641185, Training Accuracy: 63.644\n",
            "Time taken for training worker 1: 0:00:23.073349\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.546303776, Training Accuracy: 37.624\n",
            "Worker 2, [02/02]: Training Loss: 2.467020465, Training Accuracy: 38.824\n",
            "Time taken for training worker 2: 0:00:21.199321\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001507\n",
            "Local Step 71: Test Loss: 2.441087307, Test Accuracy: 44.130\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.247592380, Training Accuracy: 64.192\n",
            "Worker 1, [02/02]: Training Loss: 1.263123983, Training Accuracy: 63.336\n",
            "Time taken for training worker 1: 0:00:22.826434\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.523656375, Training Accuracy: 37.760\n",
            "Worker 2, [02/02]: Training Loss: 2.461828363, Training Accuracy: 38.712\n",
            "Time taken for training worker 2: 0:00:20.683618\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001495\n",
            "Local Step 72: Test Loss: 2.437560891, Test Accuracy: 43.140\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.256934995, Training Accuracy: 63.616\n",
            "Worker 1, [02/02]: Training Loss: 1.263457135, Training Accuracy: 63.544\n",
            "Time taken for training worker 1: 0:00:21.941035\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.544659574, Training Accuracy: 37.820\n",
            "Worker 2, [02/02]: Training Loss: 2.464000385, Training Accuracy: 38.852\n",
            "Time taken for training worker 2: 0:00:22.578339\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001717\n",
            "Local Step 73: Test Loss: 2.464940892, Test Accuracy: 43.780\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.254166152, Training Accuracy: 63.472\n",
            "Worker 1, [02/02]: Training Loss: 1.257501884, Training Accuracy: 63.624\n",
            "Time taken for training worker 1: 0:00:22.205467\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.526579024, Training Accuracy: 38.108\n",
            "Worker 2, [02/02]: Training Loss: 2.452795927, Training Accuracy: 38.908\n",
            "Time taken for training worker 2: 0:00:21.756004\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001433\n",
            "Local Step 74: Test Loss: 2.456058425, Test Accuracy: 43.620\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.264552990, Training Accuracy: 63.108\n",
            "Worker 1, [02/02]: Training Loss: 1.254151154, Training Accuracy: 63.792\n",
            "Time taken for training worker 1: 0:00:21.088824\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.514524827, Training Accuracy: 38.336\n",
            "Worker 2, [02/02]: Training Loss: 2.444909000, Training Accuracy: 38.996\n",
            "Time taken for training worker 2: 0:00:21.927200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001581\n",
            "Local Step 75: Test Loss: 2.458402157, Test Accuracy: 43.780\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for HeteroCompSGD: 0:59:05.752554\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:4\n",
            "==================================================\n",
            "Start Time: 2024-09-06 18:57:32\n",
            "--------------------------------------------------\n",
            "Warmup rounds\n",
            "--------------------------------------------------\n",
            "Worker 1, [01/04]: Training Loss: 4.297421029, Training Accuracy: 4.672\n",
            "Worker 1, [02/04]: Training Loss: 3.838574833, Training Accuracy: 10.548\n",
            "Worker 1, [03/04]: Training Loss: 3.581170551, Training Accuracy: 14.820\n",
            "Worker 1, [04/04]: Training Loss: 3.377407028, Training Accuracy: 18.444\n",
            "Time taken for warmup 1: 0:00:41.774513\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.321355807, Training Accuracy: 4.256\n",
            "Worker 2, [02/04]: Training Loss: 3.874868575, Training Accuracy: 9.896\n",
            "Worker 2, [03/04]: Training Loss: 3.612096388, Training Accuracy: 14.176\n",
            "Worker 2, [04/04]: Training Loss: 3.394576273, Training Accuracy: 17.692\n",
            "Time taken for warmup 2: 0:00:42.443353\n",
            "--------------------------------------------------\n",
            "workers_warmup_time: {0: 41.77451252937317, 1: 62.66176879405975}\n",
            "Warmup finished\n",
            "--------------------------------------------------\n",
            "scaled_workers_warmup_time: {0: 0.0, 1: 0.33333333333333337}\n",
            "Worker 1 - Locked last layers : 0\n",
            "Worker 1, [01/04]: Training Loss: 3.191363161, Training Accuracy: 21.528\n",
            "Worker 1, [02/04]: Training Loss: 3.061011042, Training Accuracy: 24.168\n",
            "Worker 1, [03/04]: Training Loss: 2.916804775, Training Accuracy: 26.660\n",
            "Worker 1, [04/04]: Training Loss: 2.798797132, Training Accuracy: 28.456\n",
            "Time taken for training worker 1: 0:00:41.412952\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.422286856, Training Accuracy: 16.912\n",
            "Worker 2, [02/04]: Training Loss: 3.383005059, Training Accuracy: 17.704\n",
            "Worker 2, [03/04]: Training Loss: 3.370476403, Training Accuracy: 17.948\n",
            "Worker 2, [04/04]: Training Loss: 3.375107117, Training Accuracy: 17.728\n",
            "Time taken for training worker 2: 0:00:40.443833\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001495\n",
            "Local Step 01: Test Loss: 3.833197644, Test Accuracy: 19.360\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.779040657, Training Accuracy: 29.536\n",
            "Worker 1, [02/04]: Training Loss: 2.669856001, Training Accuracy: 31.440\n",
            "Worker 1, [03/04]: Training Loss: 2.584889399, Training Accuracy: 33.112\n",
            "Worker 1, [04/04]: Training Loss: 2.517522324, Training Accuracy: 34.788\n",
            "Time taken for training worker 1: 0:00:42.250731\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.927849112, Training Accuracy: 26.988\n",
            "Worker 2, [02/04]: Training Loss: 2.868576410, Training Accuracy: 27.800\n",
            "Worker 2, [03/04]: Training Loss: 2.855974165, Training Accuracy: 28.220\n",
            "Worker 2, [04/04]: Training Loss: 2.859231793, Training Accuracy: 28.004\n",
            "Time taken for training worker 2: 0:00:40.703192\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001353\n",
            "Local Step 02: Test Loss: 2.650642354, Test Accuracy: 34.860\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.490550534, Training Accuracy: 35.056\n",
            "Worker 1, [02/04]: Training Loss: 2.428124628, Training Accuracy: 36.448\n",
            "Worker 1, [03/04]: Training Loss: 2.370055320, Training Accuracy: 37.720\n",
            "Worker 1, [04/04]: Training Loss: 2.317532132, Training Accuracy: 38.800\n",
            "Time taken for training worker 1: 0:00:40.562372\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.713726899, Training Accuracy: 32.036\n",
            "Worker 2, [02/04]: Training Loss: 2.679519358, Training Accuracy: 32.764\n",
            "Worker 2, [03/04]: Training Loss: 2.666527130, Training Accuracy: 32.952\n",
            "Worker 2, [04/04]: Training Loss: 2.674861918, Training Accuracy: 32.928\n",
            "Time taken for training worker 2: 0:00:39.877221\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001447\n",
            "Local Step 03: Test Loss: 2.599769355, Test Accuracy: 36.700\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.318128896, Training Accuracy: 39.104\n",
            "Worker 1, [02/04]: Training Loss: 2.245810838, Training Accuracy: 40.520\n",
            "Worker 1, [03/04]: Training Loss: 2.206208362, Training Accuracy: 41.004\n",
            "Worker 1, [04/04]: Training Loss: 2.178605453, Training Accuracy: 41.956\n",
            "Time taken for training worker 1: 0:00:43.973932\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.608531677, Training Accuracy: 34.248\n",
            "Worker 2, [02/04]: Training Loss: 2.587799847, Training Accuracy: 34.516\n",
            "Worker 2, [03/04]: Training Loss: 2.580916107, Training Accuracy: 34.636\n",
            "Worker 2, [04/04]: Training Loss: 2.576493769, Training Accuracy: 35.220\n",
            "Time taken for training worker 2: 0:00:42.988991\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001474\n",
            "Local Step 04: Test Loss: 2.537718170, Test Accuracy: 39.080\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.145824354, Training Accuracy: 42.396\n",
            "Worker 1, [02/04]: Training Loss: 2.109530082, Training Accuracy: 43.492\n",
            "Worker 1, [03/04]: Training Loss: 2.089337457, Training Accuracy: 43.304\n",
            "Worker 1, [04/04]: Training Loss: 2.040539909, Training Accuracy: 44.740\n",
            "Time taken for training worker 1: 0:00:41.315517\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.566736102, Training Accuracy: 34.836\n",
            "Worker 2, [02/04]: Training Loss: 2.538625395, Training Accuracy: 35.244\n",
            "Worker 2, [03/04]: Training Loss: 2.524899262, Training Accuracy: 35.524\n",
            "Worker 2, [04/04]: Training Loss: 2.535835820, Training Accuracy: 35.464\n",
            "Time taken for training worker 2: 0:00:42.073962\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001391\n",
            "Local Step 05: Test Loss: 2.469508217, Test Accuracy: 40.090\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.045583835, Training Accuracy: 44.864\n",
            "Worker 1, [02/04]: Training Loss: 1.975809741, Training Accuracy: 46.632\n",
            "Worker 1, [03/04]: Training Loss: 1.966367572, Training Accuracy: 46.592\n",
            "Worker 1, [04/04]: Training Loss: 1.939244362, Training Accuracy: 47.208\n",
            "Time taken for training worker 1: 0:00:43.819180\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.517359785, Training Accuracy: 36.252\n",
            "Worker 2, [02/04]: Training Loss: 2.489195427, Training Accuracy: 36.336\n",
            "Worker 2, [03/04]: Training Loss: 2.465739061, Training Accuracy: 37.120\n",
            "Worker 2, [04/04]: Training Loss: 2.472464606, Training Accuracy: 37.236\n",
            "Time taken for training worker 2: 0:00:42.265639\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002154\n",
            "Local Step 06: Test Loss: 2.518108495, Test Accuracy: 39.650\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.948675387, Training Accuracy: 47.088\n",
            "Worker 1, [02/04]: Training Loss: 1.920606635, Training Accuracy: 47.336\n",
            "Worker 1, [03/04]: Training Loss: 1.886947369, Training Accuracy: 48.340\n",
            "Worker 1, [04/04]: Training Loss: 1.859589734, Training Accuracy: 49.340\n",
            "Time taken for training worker 1: 0:00:46.149818\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.531037838, Training Accuracy: 36.392\n",
            "Worker 2, [02/04]: Training Loss: 2.492210370, Training Accuracy: 36.668\n",
            "Worker 2, [03/04]: Training Loss: 2.471375107, Training Accuracy: 36.772\n",
            "Worker 2, [04/04]: Training Loss: 2.477733399, Training Accuracy: 37.204\n",
            "Time taken for training worker 2: 0:00:40.866867\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001492\n",
            "Local Step 07: Test Loss: 2.533659919, Test Accuracy: 41.060\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.849825867, Training Accuracy: 49.376\n",
            "Worker 1, [02/04]: Training Loss: 1.827357081, Training Accuracy: 49.516\n",
            "Worker 1, [03/04]: Training Loss: 1.807845041, Training Accuracy: 50.172\n",
            "Worker 1, [04/04]: Training Loss: 1.790988013, Training Accuracy: 50.380\n",
            "Time taken for training worker 1: 0:00:42.048660\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.502499883, Training Accuracy: 36.660\n",
            "Worker 2, [02/04]: Training Loss: 2.460234807, Training Accuracy: 37.444\n",
            "Worker 2, [03/04]: Training Loss: 2.438783579, Training Accuracy: 37.608\n",
            "Worker 2, [04/04]: Training Loss: 2.437423881, Training Accuracy: 37.740\n",
            "Time taken for training worker 2: 0:00:43.924230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001464\n",
            "Local Step 08: Test Loss: 2.441795010, Test Accuracy: 42.680\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.787907264, Training Accuracy: 50.576\n",
            "Worker 1, [02/04]: Training Loss: 1.765265903, Training Accuracy: 51.224\n",
            "Worker 1, [03/04]: Training Loss: 1.749871730, Training Accuracy: 51.416\n",
            "Worker 1, [04/04]: Training Loss: 1.746780129, Training Accuracy: 51.792\n",
            "Time taken for training worker 1: 0:00:43.004061\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.496150586, Training Accuracy: 37.496\n",
            "Worker 2, [02/04]: Training Loss: 2.430751085, Training Accuracy: 37.952\n",
            "Worker 2, [03/04]: Training Loss: 2.419027983, Training Accuracy: 38.692\n",
            "Worker 2, [04/04]: Training Loss: 2.433614293, Training Accuracy: 38.440\n",
            "Time taken for training worker 2: 0:00:43.721200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001347\n",
            "Local Step 09: Test Loss: 2.432141725, Test Accuracy: 42.350\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.730290515, Training Accuracy: 52.032\n",
            "Worker 1, [02/04]: Training Loss: 1.712262365, Training Accuracy: 52.620\n",
            "Worker 1, [03/04]: Training Loss: 1.699541697, Training Accuracy: 53.108\n",
            "Worker 1, [04/04]: Training Loss: 1.684533027, Training Accuracy: 53.356\n",
            "Time taken for training worker 1: 0:00:43.054673\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.475617205, Training Accuracy: 38.016\n",
            "Worker 2, [02/04]: Training Loss: 2.419363549, Training Accuracy: 38.388\n",
            "Worker 2, [03/04]: Training Loss: 2.411038015, Training Accuracy: 38.924\n",
            "Worker 2, [04/04]: Training Loss: 2.417369000, Training Accuracy: 38.628\n",
            "Time taken for training worker 2: 0:00:40.061749\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001421\n",
            "Local Step 10: Test Loss: 2.393404018, Test Accuracy: 43.810\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.683015046, Training Accuracy: 53.424\n",
            "Worker 1, [02/04]: Training Loss: 1.649718820, Training Accuracy: 53.784\n",
            "Worker 1, [03/04]: Training Loss: 1.645905885, Training Accuracy: 54.540\n",
            "Worker 1, [04/04]: Training Loss: 1.631794119, Training Accuracy: 54.044\n",
            "Time taken for training worker 1: 0:00:41.856943\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.462622587, Training Accuracy: 38.668\n",
            "Worker 2, [02/04]: Training Loss: 2.392499562, Training Accuracy: 39.408\n",
            "Worker 2, [03/04]: Training Loss: 2.383466603, Training Accuracy: 39.440\n",
            "Worker 2, [04/04]: Training Loss: 2.387413371, Training Accuracy: 39.452\n",
            "Time taken for training worker 2: 0:00:41.541460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001689\n",
            "Local Step 11: Test Loss: 2.458698959, Test Accuracy: 42.240\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.631615686, Training Accuracy: 54.560\n",
            "Worker 1, [02/04]: Training Loss: 1.612055438, Training Accuracy: 54.928\n",
            "Worker 1, [03/04]: Training Loss: 1.613622833, Training Accuracy: 55.140\n",
            "Worker 1, [04/04]: Training Loss: 1.583576167, Training Accuracy: 55.656\n",
            "Time taken for training worker 1: 0:00:41.482308\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.478432409, Training Accuracy: 38.048\n",
            "Worker 2, [02/04]: Training Loss: 2.419686162, Training Accuracy: 38.668\n",
            "Worker 2, [03/04]: Training Loss: 2.407548513, Training Accuracy: 38.988\n",
            "Worker 2, [04/04]: Training Loss: 2.411520507, Training Accuracy: 39.068\n",
            "Time taken for training worker 2: 0:00:41.287246\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001422\n",
            "Local Step 12: Test Loss: 2.480278472, Test Accuracy: 43.640\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.585065136, Training Accuracy: 55.704\n",
            "Worker 1, [02/04]: Training Loss: 1.583083165, Training Accuracy: 55.620\n",
            "Worker 1, [03/04]: Training Loss: 1.557441356, Training Accuracy: 56.412\n",
            "Worker 1, [04/04]: Training Loss: 1.551791331, Training Accuracy: 56.444\n",
            "Time taken for training worker 1: 0:00:42.723691\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.465901128, Training Accuracy: 38.584\n",
            "Worker 2, [02/04]: Training Loss: 2.399288840, Training Accuracy: 39.232\n",
            "Worker 2, [03/04]: Training Loss: 2.379432167, Training Accuracy: 39.816\n",
            "Worker 2, [04/04]: Training Loss: 2.381942662, Training Accuracy: 39.896\n",
            "Time taken for training worker 2: 0:00:41.961374\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001653\n",
            "Local Step 13: Test Loss: 2.367252244, Test Accuracy: 44.230\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.547425107, Training Accuracy: 56.036\n",
            "Worker 1, [02/04]: Training Loss: 1.542487559, Training Accuracy: 56.052\n",
            "Worker 1, [03/04]: Training Loss: 1.548392492, Training Accuracy: 56.376\n",
            "Worker 1, [04/04]: Training Loss: 1.538900629, Training Accuracy: 56.212\n",
            "Time taken for training worker 1: 0:00:44.132563\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.449291344, Training Accuracy: 38.664\n",
            "Worker 2, [02/04]: Training Loss: 2.382491090, Training Accuracy: 39.388\n",
            "Worker 2, [03/04]: Training Loss: 2.373103509, Training Accuracy: 39.840\n",
            "Worker 2, [04/04]: Training Loss: 2.359461273, Training Accuracy: 39.788\n",
            "Time taken for training worker 2: 0:00:40.332237\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001430\n",
            "Local Step 14: Test Loss: 2.457986597, Test Accuracy: 44.370\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.516592969, Training Accuracy: 57.632\n",
            "Worker 1, [02/04]: Training Loss: 1.514202970, Training Accuracy: 57.280\n",
            "Worker 1, [03/04]: Training Loss: 1.521916245, Training Accuracy: 57.240\n",
            "Worker 1, [04/04]: Training Loss: 1.502459879, Training Accuracy: 57.432\n",
            "Time taken for training worker 1: 0:00:41.323181\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.469096068, Training Accuracy: 38.664\n",
            "Worker 2, [02/04]: Training Loss: 2.398591677, Training Accuracy: 39.448\n",
            "Worker 2, [03/04]: Training Loss: 2.387651621, Training Accuracy: 39.936\n",
            "Worker 2, [04/04]: Training Loss: 2.384319097, Training Accuracy: 40.084\n",
            "Time taken for training worker 2: 0:00:40.779710\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001493\n",
            "Local Step 15: Test Loss: 2.404394085, Test Accuracy: 43.750\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.503192053, Training Accuracy: 57.652\n",
            "Worker 1, [02/04]: Training Loss: 1.500919170, Training Accuracy: 57.224\n",
            "Worker 1, [03/04]: Training Loss: 1.481620665, Training Accuracy: 58.132\n",
            "Worker 1, [04/04]: Training Loss: 1.473560944, Training Accuracy: 58.240\n",
            "Time taken for training worker 1: 0:00:43.780299\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.457618313, Training Accuracy: 38.952\n",
            "Worker 2, [02/04]: Training Loss: 2.410610815, Training Accuracy: 39.532\n",
            "Worker 2, [03/04]: Training Loss: 2.387659155, Training Accuracy: 39.940\n",
            "Worker 2, [04/04]: Training Loss: 2.381310760, Training Accuracy: 39.828\n",
            "Time taken for training worker 2: 0:00:45.682621\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001687\n",
            "Local Step 16: Test Loss: 2.452640379, Test Accuracy: 43.310\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.477579131, Training Accuracy: 58.080\n",
            "Worker 1, [02/04]: Training Loss: 1.460480171, Training Accuracy: 58.596\n",
            "Worker 1, [03/04]: Training Loss: 1.451083374, Training Accuracy: 58.760\n",
            "Worker 1, [04/04]: Training Loss: 1.440969548, Training Accuracy: 58.932\n",
            "Time taken for training worker 1: 0:00:46.990490\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.470762028, Training Accuracy: 38.392\n",
            "Worker 2, [02/04]: Training Loss: 2.412325451, Training Accuracy: 38.884\n",
            "Worker 2, [03/04]: Training Loss: 2.400483004, Training Accuracy: 39.260\n",
            "Worker 2, [04/04]: Training Loss: 2.400354361, Training Accuracy: 39.328\n",
            "Time taken for training worker 2: 0:00:44.057388\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001578\n",
            "Local Step 17: Test Loss: 2.505965611, Test Accuracy: 43.990\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.458760463, Training Accuracy: 58.864\n",
            "Worker 1, [02/04]: Training Loss: 1.437469555, Training Accuracy: 58.904\n",
            "Worker 1, [03/04]: Training Loss: 1.416015428, Training Accuracy: 59.664\n",
            "Worker 1, [04/04]: Training Loss: 1.418724872, Training Accuracy: 59.444\n",
            "Time taken for training worker 1: 0:00:45.129843\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.494008856, Training Accuracy: 38.260\n",
            "Worker 2, [02/04]: Training Loss: 2.420017186, Training Accuracy: 39.072\n",
            "Worker 2, [03/04]: Training Loss: 2.396735597, Training Accuracy: 39.348\n",
            "Worker 2, [04/04]: Training Loss: 2.391070813, Training Accuracy: 39.536\n",
            "Time taken for training worker 2: 0:00:45.793328\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001551\n",
            "Local Step 18: Test Loss: 2.415025492, Test Accuracy: 44.050\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.426186126, Training Accuracy: 59.224\n",
            "Worker 1, [02/04]: Training Loss: 1.416216092, Training Accuracy: 59.496\n",
            "Worker 1, [03/04]: Training Loss: 1.423098258, Training Accuracy: 59.532\n",
            "Worker 1, [04/04]: Training Loss: 1.413769209, Training Accuracy: 59.352\n",
            "Time taken for training worker 1: 0:00:46.824051\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.480609392, Training Accuracy: 38.788\n",
            "Worker 2, [02/04]: Training Loss: 2.398108751, Training Accuracy: 39.512\n",
            "Worker 2, [03/04]: Training Loss: 2.390562675, Training Accuracy: 39.700\n",
            "Worker 2, [04/04]: Training Loss: 2.387745470, Training Accuracy: 39.936\n",
            "Time taken for training worker 2: 0:00:41.950325\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001419\n",
            "Local Step 19: Test Loss: 2.523498537, Test Accuracy: 43.290\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.426690594, Training Accuracy: 59.320\n",
            "Worker 1, [02/04]: Training Loss: 1.394534967, Training Accuracy: 59.956\n",
            "Worker 1, [03/04]: Training Loss: 1.388251859, Training Accuracy: 60.592\n",
            "Worker 1, [04/04]: Training Loss: 1.383210127, Training Accuracy: 60.572\n",
            "Time taken for training worker 1: 0:00:43.701689\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.511777178, Training Accuracy: 37.864\n",
            "Worker 2, [02/04]: Training Loss: 2.435183778, Training Accuracy: 38.780\n",
            "Worker 2, [03/04]: Training Loss: 2.408844379, Training Accuracy: 39.208\n",
            "Worker 2, [04/04]: Training Loss: 2.401721576, Training Accuracy: 39.272\n",
            "Time taken for training worker 2: 0:00:41.825617\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001757\n",
            "Local Step 20: Test Loss: 2.513592623, Test Accuracy: 44.930\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.420271101, Training Accuracy: 59.668\n",
            "Worker 1, [02/04]: Training Loss: 1.409249963, Training Accuracy: 60.192\n",
            "Worker 1, [03/04]: Training Loss: 1.383024479, Training Accuracy: 60.040\n",
            "Worker 1, [04/04]: Training Loss: 1.395288258, Training Accuracy: 60.092\n",
            "Time taken for training worker 1: 0:00:45.549534\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.494867990, Training Accuracy: 38.764\n",
            "Worker 2, [02/04]: Training Loss: 2.391452674, Training Accuracy: 40.012\n",
            "Worker 2, [03/04]: Training Loss: 2.369235515, Training Accuracy: 40.144\n",
            "Worker 2, [04/04]: Training Loss: 2.356935350, Training Accuracy: 40.740\n",
            "Time taken for training worker 2: 0:00:41.559179\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001360\n",
            "Local Step 21: Test Loss: 2.492912297, Test Accuracy: 43.740\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.383485994, Training Accuracy: 60.436\n",
            "Worker 1, [02/04]: Training Loss: 1.383262897, Training Accuracy: 60.604\n",
            "Worker 1, [03/04]: Training Loss: 1.386883638, Training Accuracy: 60.396\n",
            "Worker 1, [04/04]: Training Loss: 1.357910957, Training Accuracy: 61.216\n",
            "Time taken for training worker 1: 0:00:43.414347\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.492643694, Training Accuracy: 38.504\n",
            "Worker 2, [02/04]: Training Loss: 2.420048738, Training Accuracy: 39.212\n",
            "Worker 2, [03/04]: Training Loss: 2.401414691, Training Accuracy: 39.124\n",
            "Worker 2, [04/04]: Training Loss: 2.384838336, Training Accuracy: 39.668\n",
            "Time taken for training worker 2: 0:00:43.727288\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001554\n",
            "Local Step 22: Test Loss: 2.468071824, Test Accuracy: 44.540\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.375211820, Training Accuracy: 60.884\n",
            "Worker 1, [02/04]: Training Loss: 1.357845768, Training Accuracy: 61.148\n",
            "Worker 1, [03/04]: Training Loss: 1.351868542, Training Accuracy: 60.984\n",
            "Worker 1, [04/04]: Training Loss: 1.358515685, Training Accuracy: 61.112\n",
            "Time taken for training worker 1: 0:00:44.055608\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.494011722, Training Accuracy: 38.284\n",
            "Worker 2, [02/04]: Training Loss: 2.399252106, Training Accuracy: 39.704\n",
            "Worker 2, [03/04]: Training Loss: 2.381687328, Training Accuracy: 39.884\n",
            "Worker 2, [04/04]: Training Loss: 2.372291452, Training Accuracy: 40.572\n",
            "Time taken for training worker 2: 0:00:45.722702\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001815\n",
            "Local Step 23: Test Loss: 2.420447653, Test Accuracy: 45.100\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.353252671, Training Accuracy: 60.980\n",
            "Worker 1, [02/04]: Training Loss: 1.352629408, Training Accuracy: 61.220\n",
            "Worker 1, [03/04]: Training Loss: 1.355983519, Training Accuracy: 61.504\n",
            "Worker 1, [04/04]: Training Loss: 1.329072966, Training Accuracy: 61.924\n",
            "Time taken for training worker 1: 0:00:43.452378\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.480857559, Training Accuracy: 38.644\n",
            "Worker 2, [02/04]: Training Loss: 2.414721689, Training Accuracy: 39.392\n",
            "Worker 2, [03/04]: Training Loss: 2.384255467, Training Accuracy: 39.844\n",
            "Worker 2, [04/04]: Training Loss: 2.371510895, Training Accuracy: 40.204\n",
            "Time taken for training worker 2: 0:00:41.062484\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001484\n",
            "Local Step 24: Test Loss: 2.573287205, Test Accuracy: 43.470\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.335635864, Training Accuracy: 61.700\n",
            "Worker 1, [02/04]: Training Loss: 1.361495655, Training Accuracy: 61.032\n",
            "Worker 1, [03/04]: Training Loss: 1.351134740, Training Accuracy: 61.564\n",
            "Worker 1, [04/04]: Training Loss: 1.346571370, Training Accuracy: 61.392\n",
            "Time taken for training worker 1: 0:00:41.991721\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.518492775, Training Accuracy: 38.624\n",
            "Worker 2, [02/04]: Training Loss: 2.426125558, Training Accuracy: 39.356\n",
            "Worker 2, [03/04]: Training Loss: 2.407640684, Training Accuracy: 39.564\n",
            "Worker 2, [04/04]: Training Loss: 2.402548228, Training Accuracy: 39.608\n",
            "Time taken for training worker 2: 0:00:41.539638\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001415\n",
            "Local Step 25: Test Loss: 2.473024501, Test Accuracy: 44.360\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.338895275, Training Accuracy: 61.884\n",
            "Worker 1, [02/04]: Training Loss: 1.321675823, Training Accuracy: 61.944\n",
            "Worker 1, [03/04]: Training Loss: 1.338353922, Training Accuracy: 61.364\n",
            "Worker 1, [04/04]: Training Loss: 1.322105059, Training Accuracy: 62.248\n",
            "Time taken for training worker 1: 0:00:41.871222\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.512894375, Training Accuracy: 38.584\n",
            "Worker 2, [02/04]: Training Loss: 2.411403826, Training Accuracy: 39.312\n",
            "Worker 2, [03/04]: Training Loss: 2.398465537, Training Accuracy: 39.320\n",
            "Worker 2, [04/04]: Training Loss: 2.392655448, Training Accuracy: 39.916\n",
            "Time taken for training worker 2: 0:00:40.969044\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001482\n",
            "Local Step 26: Test Loss: 2.496544380, Test Accuracy: 44.740\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.302347254, Training Accuracy: 62.168\n",
            "Worker 1, [02/04]: Training Loss: 1.324635930, Training Accuracy: 61.768\n",
            "Worker 1, [03/04]: Training Loss: 1.316866710, Training Accuracy: 61.716\n",
            "Worker 1, [04/04]: Training Loss: 1.312027937, Training Accuracy: 62.464\n",
            "Time taken for training worker 1: 0:00:41.933202\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.472016513, Training Accuracy: 39.068\n",
            "Worker 2, [02/04]: Training Loss: 2.401693055, Training Accuracy: 39.736\n",
            "Worker 2, [03/04]: Training Loss: 2.380143798, Training Accuracy: 40.276\n",
            "Worker 2, [04/04]: Training Loss: 2.367696160, Training Accuracy: 40.732\n",
            "Time taken for training worker 2: 0:00:43.308629\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001483\n",
            "Local Step 27: Test Loss: 2.583743821, Test Accuracy: 43.600\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.337195307, Training Accuracy: 61.940\n",
            "Worker 1, [02/04]: Training Loss: 1.294236936, Training Accuracy: 62.872\n",
            "Worker 1, [03/04]: Training Loss: 1.313883627, Training Accuracy: 61.912\n",
            "Worker 1, [04/04]: Training Loss: 1.314247530, Training Accuracy: 62.316\n",
            "Time taken for training worker 1: 0:00:41.663390\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.491987262, Training Accuracy: 38.916\n",
            "Worker 2, [02/04]: Training Loss: 2.410939665, Training Accuracy: 39.488\n",
            "Worker 2, [03/04]: Training Loss: 2.395924925, Training Accuracy: 39.808\n",
            "Worker 2, [04/04]: Training Loss: 2.387755858, Training Accuracy: 40.236\n",
            "Time taken for training worker 2: 0:00:42.945072\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001550\n",
            "Local Step 28: Test Loss: 2.446021864, Test Accuracy: 44.550\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.303875361, Training Accuracy: 62.516\n",
            "Worker 1, [02/04]: Training Loss: 1.281772532, Training Accuracy: 62.964\n",
            "Worker 1, [03/04]: Training Loss: 1.290768016, Training Accuracy: 62.836\n",
            "Worker 1, [04/04]: Training Loss: 1.282281913, Training Accuracy: 62.548\n",
            "Time taken for training worker 1: 0:00:42.133320\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.492867170, Training Accuracy: 38.836\n",
            "Worker 2, [02/04]: Training Loss: 2.400956436, Training Accuracy: 39.692\n",
            "Worker 2, [03/04]: Training Loss: 2.374844822, Training Accuracy: 40.128\n",
            "Worker 2, [04/04]: Training Loss: 2.368521546, Training Accuracy: 40.600\n",
            "Time taken for training worker 2: 0:00:41.945412\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001385\n",
            "Local Step 29: Test Loss: 2.573031561, Test Accuracy: 43.320\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.277186569, Training Accuracy: 63.132\n",
            "Worker 1, [02/04]: Training Loss: 1.292686181, Training Accuracy: 62.904\n",
            "Worker 1, [03/04]: Training Loss: 1.282668386, Training Accuracy: 63.184\n",
            "Worker 1, [04/04]: Training Loss: 1.265538778, Training Accuracy: 63.748\n",
            "Time taken for training worker 1: 0:00:42.886565\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.515202246, Training Accuracy: 37.976\n",
            "Worker 2, [02/04]: Training Loss: 2.416441171, Training Accuracy: 39.560\n",
            "Worker 2, [03/04]: Training Loss: 2.404397571, Training Accuracy: 40.096\n",
            "Worker 2, [04/04]: Training Loss: 2.389579570, Training Accuracy: 39.624\n",
            "Time taken for training worker 2: 0:00:41.653581\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001451\n",
            "Local Step 30: Test Loss: 2.529580888, Test Accuracy: 43.810\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.286413647, Training Accuracy: 63.116\n",
            "Worker 1, [02/04]: Training Loss: 1.295035674, Training Accuracy: 62.536\n",
            "Worker 1, [03/04]: Training Loss: 1.274499027, Training Accuracy: 63.108\n",
            "Worker 1, [04/04]: Training Loss: 1.276644412, Training Accuracy: 63.428\n",
            "Time taken for training worker 1: 0:00:42.903556\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.548086356, Training Accuracy: 37.592\n",
            "Worker 2, [02/04]: Training Loss: 2.446375416, Training Accuracy: 38.668\n",
            "Worker 2, [03/04]: Training Loss: 2.406877950, Training Accuracy: 39.196\n",
            "Worker 2, [04/04]: Training Loss: 2.405924589, Training Accuracy: 39.320\n",
            "Time taken for training worker 2: 0:00:42.271625\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002407\n",
            "Local Step 31: Test Loss: 2.564756365, Test Accuracy: 44.780\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.312669133, Training Accuracy: 61.996\n",
            "Worker 1, [02/04]: Training Loss: 1.272101187, Training Accuracy: 63.148\n",
            "Worker 1, [03/04]: Training Loss: 1.280221242, Training Accuracy: 62.804\n",
            "Worker 1, [04/04]: Training Loss: 1.266725094, Training Accuracy: 63.596\n",
            "Time taken for training worker 1: 0:00:44.247993\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.502867401, Training Accuracy: 38.208\n",
            "Worker 2, [02/04]: Training Loss: 2.403585618, Training Accuracy: 39.348\n",
            "Worker 2, [03/04]: Training Loss: 2.381782986, Training Accuracy: 40.108\n",
            "Worker 2, [04/04]: Training Loss: 2.380363169, Training Accuracy: 39.784\n",
            "Time taken for training worker 2: 0:00:41.134457\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001441\n",
            "Local Step 32: Test Loss: 2.465998937, Test Accuracy: 44.290\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.272045490, Training Accuracy: 63.292\n",
            "Worker 1, [02/04]: Training Loss: 1.292330548, Training Accuracy: 63.252\n",
            "Worker 1, [03/04]: Training Loss: 1.256483804, Training Accuracy: 63.716\n",
            "Worker 1, [04/04]: Training Loss: 1.263617505, Training Accuracy: 63.664\n",
            "Time taken for training worker 1: 0:00:41.453113\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.513865411, Training Accuracy: 38.200\n",
            "Worker 2, [02/04]: Training Loss: 2.415110576, Training Accuracy: 39.352\n",
            "Worker 2, [03/04]: Training Loss: 2.390319967, Training Accuracy: 39.764\n",
            "Worker 2, [04/04]: Training Loss: 2.391302195, Training Accuracy: 39.684\n",
            "Time taken for training worker 2: 0:00:42.480877\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001545\n",
            "Local Step 33: Test Loss: 2.601401761, Test Accuracy: 44.190\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.267479338, Training Accuracy: 63.424\n",
            "Worker 1, [02/04]: Training Loss: 1.267882600, Training Accuracy: 63.380\n",
            "Worker 1, [03/04]: Training Loss: 1.268383684, Training Accuracy: 63.456\n",
            "Worker 1, [04/04]: Training Loss: 1.278672263, Training Accuracy: 62.904\n",
            "Time taken for training worker 1: 0:00:41.856510\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.516603356, Training Accuracy: 38.636\n",
            "Worker 2, [02/04]: Training Loss: 2.429198047, Training Accuracy: 39.476\n",
            "Worker 2, [03/04]: Training Loss: 2.402276918, Training Accuracy: 39.620\n",
            "Worker 2, [04/04]: Training Loss: 2.391128526, Training Accuracy: 40.528\n",
            "Time taken for training worker 2: 0:00:40.708725\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001627\n",
            "Local Step 34: Test Loss: 2.493209701, Test Accuracy: 43.900\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.273639020, Training Accuracy: 63.196\n",
            "Worker 1, [02/04]: Training Loss: 1.263709312, Training Accuracy: 63.528\n",
            "Worker 1, [03/04]: Training Loss: 1.268791432, Training Accuracy: 63.476\n",
            "Worker 1, [04/04]: Training Loss: 1.265206761, Training Accuracy: 63.528\n",
            "Time taken for training worker 1: 0:00:43.465788\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.520877182, Training Accuracy: 38.284\n",
            "Worker 2, [02/04]: Training Loss: 2.437137298, Training Accuracy: 39.152\n",
            "Worker 2, [03/04]: Training Loss: 2.401884955, Training Accuracy: 39.664\n",
            "Worker 2, [04/04]: Training Loss: 2.394680261, Training Accuracy: 40.016\n",
            "Time taken for training worker 2: 0:00:40.916669\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001534\n",
            "Local Step 35: Test Loss: 2.576624714, Test Accuracy: 44.250\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.260545164, Training Accuracy: 63.396\n",
            "Worker 1, [02/04]: Training Loss: 1.257033598, Training Accuracy: 63.600\n",
            "Worker 1, [03/04]: Training Loss: 1.246809192, Training Accuracy: 63.992\n",
            "Worker 1, [04/04]: Training Loss: 1.253192709, Training Accuracy: 63.956\n",
            "Time taken for training worker 1: 0:00:42.425047\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.506666762, Training Accuracy: 39.004\n",
            "Worker 2, [02/04]: Training Loss: 2.430898730, Training Accuracy: 39.408\n",
            "Worker 2, [03/04]: Training Loss: 2.404319142, Training Accuracy: 39.664\n",
            "Worker 2, [04/04]: Training Loss: 2.382198079, Training Accuracy: 40.180\n",
            "Time taken for training worker 2: 0:00:43.744093\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001530\n",
            "Local Step 36: Test Loss: 2.505357505, Test Accuracy: 44.330\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.245449132, Training Accuracy: 63.732\n",
            "Worker 1, [02/04]: Training Loss: 1.252709975, Training Accuracy: 63.660\n",
            "Worker 1, [03/04]: Training Loss: 1.255087776, Training Accuracy: 63.920\n",
            "Worker 1, [04/04]: Training Loss: 1.221128296, Training Accuracy: 64.348\n",
            "Time taken for training worker 1: 0:00:42.772383\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.504126561, Training Accuracy: 39.024\n",
            "Worker 2, [02/04]: Training Loss: 2.404758013, Training Accuracy: 39.928\n",
            "Worker 2, [03/04]: Training Loss: 2.383084547, Training Accuracy: 40.208\n",
            "Worker 2, [04/04]: Training Loss: 2.369336314, Training Accuracy: 40.364\n",
            "Time taken for training worker 2: 0:00:41.101539\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001434\n",
            "Local Step 37: Test Loss: 2.557172373, Test Accuracy: 43.840\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for HeteroCompSGD: 0:55:47.016817\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:8\n",
            "==================================================\n",
            "Start Time: 2024-09-06 19:53:19\n",
            "--------------------------------------------------\n",
            "Warmup rounds\n",
            "--------------------------------------------------\n",
            "Worker 1, [01/08]: Training Loss: 4.293844397, Training Accuracy: 4.836\n",
            "Worker 1, [02/08]: Training Loss: 3.834649432, Training Accuracy: 10.440\n",
            "Worker 1, [03/08]: Training Loss: 3.582263484, Training Accuracy: 14.632\n",
            "Worker 1, [04/08]: Training Loss: 3.361770252, Training Accuracy: 18.516\n",
            "Worker 1, [05/08]: Training Loss: 3.186928381, Training Accuracy: 21.736\n",
            "Worker 1, [06/08]: Training Loss: 3.046053613, Training Accuracy: 24.148\n",
            "Worker 1, [07/08]: Training Loss: 2.901325385, Training Accuracy: 27.032\n",
            "Worker 1, [08/08]: Training Loss: 2.806754421, Training Accuracy: 28.788\n",
            "Time taken for warmup 1: 0:01:27.048649\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 4.324974733, Training Accuracy: 4.252\n",
            "Worker 2, [02/08]: Training Loss: 3.869098008, Training Accuracy: 10.448\n",
            "Worker 2, [03/08]: Training Loss: 3.631587907, Training Accuracy: 14.012\n",
            "Worker 2, [04/08]: Training Loss: 3.399749690, Training Accuracy: 17.396\n",
            "Worker 2, [05/08]: Training Loss: 3.230468359, Training Accuracy: 20.764\n",
            "Worker 2, [06/08]: Training Loss: 3.081961428, Training Accuracy: 23.340\n",
            "Worker 2, [07/08]: Training Loss: 2.957648019, Training Accuracy: 25.920\n",
            "Worker 2, [08/08]: Training Loss: 2.838866691, Training Accuracy: 28.332\n",
            "Time taken for warmup 2: 0:01:26.766457\n",
            "--------------------------------------------------\n",
            "workers_warmup_time: {0: 87.04864931106567, 1: 130.5729739665985}\n",
            "Warmup finished\n",
            "--------------------------------------------------\n",
            "scaled_workers_warmup_time: {0: 0.0, 1: 0.33333333333333337}\n",
            "Worker 1 - Locked last layers : 0\n",
            "Worker 1, [01/08]: Training Loss: 2.730588597, Training Accuracy: 30.312\n",
            "Worker 1, [02/08]: Training Loss: 2.640044204, Training Accuracy: 31.924\n",
            "Worker 1, [03/08]: Training Loss: 2.566234742, Training Accuracy: 33.400\n",
            "Worker 1, [04/08]: Training Loss: 2.492740751, Training Accuracy: 35.460\n",
            "Worker 1, [05/08]: Training Loss: 2.450361020, Training Accuracy: 36.220\n",
            "Worker 1, [06/08]: Training Loss: 2.388249812, Training Accuracy: 37.368\n",
            "Worker 1, [07/08]: Training Loss: 2.345237646, Training Accuracy: 38.328\n",
            "Worker 1, [08/08]: Training Loss: 2.309699886, Training Accuracy: 38.948\n",
            "Time taken for training worker 1: 0:01:26.150539\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.839429390, Training Accuracy: 27.912\n",
            "Worker 2, [02/08]: Training Loss: 2.807602157, Training Accuracy: 28.468\n",
            "Worker 2, [03/08]: Training Loss: 2.807537752, Training Accuracy: 28.448\n",
            "Worker 2, [04/08]: Training Loss: 2.810519938, Training Accuracy: 28.380\n",
            "Worker 2, [05/08]: Training Loss: 2.820780000, Training Accuracy: 28.468\n",
            "Worker 2, [06/08]: Training Loss: 2.830482446, Training Accuracy: 28.496\n",
            "Worker 2, [07/08]: Training Loss: 2.847609228, Training Accuracy: 28.228\n",
            "Worker 2, [08/08]: Training Loss: 2.877983660, Training Accuracy: 27.836\n",
            "Time taken for training worker 2: 0:01:29.424077\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001617\n",
            "Local Step 01: Test Loss: 3.506708496, Test Accuracy: 30.270\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.348315444, Training Accuracy: 38.156\n",
            "Worker 1, [02/08]: Training Loss: 2.261961283, Training Accuracy: 40.204\n",
            "Worker 1, [03/08]: Training Loss: 2.205970468, Training Accuracy: 40.960\n",
            "Worker 1, [04/08]: Training Loss: 2.169830482, Training Accuracy: 42.340\n",
            "Worker 1, [05/08]: Training Loss: 2.139064182, Training Accuracy: 42.704\n",
            "Worker 1, [06/08]: Training Loss: 2.098076687, Training Accuracy: 43.248\n",
            "Worker 1, [07/08]: Training Loss: 2.072873730, Training Accuracy: 44.096\n",
            "Worker 1, [08/08]: Training Loss: 2.064195546, Training Accuracy: 44.372\n",
            "Time taken for training worker 1: 0:01:29.123453\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.700586621, Training Accuracy: 32.404\n",
            "Worker 2, [02/08]: Training Loss: 2.602925932, Training Accuracy: 33.900\n",
            "Worker 2, [03/08]: Training Loss: 2.591557426, Training Accuracy: 34.284\n",
            "Worker 2, [04/08]: Training Loss: 2.597672883, Training Accuracy: 34.492\n",
            "Worker 2, [05/08]: Training Loss: 2.610575409, Training Accuracy: 33.944\n",
            "Worker 2, [06/08]: Training Loss: 2.620172261, Training Accuracy: 34.356\n",
            "Worker 2, [07/08]: Training Loss: 2.649694168, Training Accuracy: 33.568\n",
            "Worker 2, [08/08]: Training Loss: 2.683988202, Training Accuracy: 33.400\n",
            "Time taken for training worker 2: 0:01:39.525409\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001538\n",
            "Local Step 02: Test Loss: 3.070945879, Test Accuracy: 39.160\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.082553061, Training Accuracy: 43.996\n",
            "Worker 1, [02/08]: Training Loss: 2.019923242, Training Accuracy: 45.404\n",
            "Worker 1, [03/08]: Training Loss: 1.988731390, Training Accuracy: 46.168\n",
            "Worker 1, [04/08]: Training Loss: 1.972939244, Training Accuracy: 45.988\n",
            "Worker 1, [05/08]: Training Loss: 1.927669637, Training Accuracy: 47.288\n",
            "Worker 1, [06/08]: Training Loss: 1.896001562, Training Accuracy: 47.964\n",
            "Worker 1, [07/08]: Training Loss: 1.879655863, Training Accuracy: 48.624\n",
            "Worker 1, [08/08]: Training Loss: 1.868219108, Training Accuracy: 48.708\n",
            "Time taken for training worker 1: 0:01:44.928882\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.569739015, Training Accuracy: 36.080\n",
            "Worker 2, [02/08]: Training Loss: 2.468758990, Training Accuracy: 36.768\n",
            "Worker 2, [03/08]: Training Loss: 2.471994491, Training Accuracy: 36.672\n",
            "Worker 2, [04/08]: Training Loss: 2.461985959, Training Accuracy: 37.168\n",
            "Worker 2, [05/08]: Training Loss: 2.476405428, Training Accuracy: 37.028\n",
            "Worker 2, [06/08]: Training Loss: 2.502744007, Training Accuracy: 36.560\n",
            "Worker 2, [07/08]: Training Loss: 2.510431899, Training Accuracy: 36.680\n",
            "Worker 2, [08/08]: Training Loss: 2.545230769, Training Accuracy: 36.200\n",
            "Time taken for training worker 2: 0:01:45.852274\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001635\n",
            "Local Step 03: Test Loss: 2.761397706, Test Accuracy: 40.350\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.902904899, Training Accuracy: 48.444\n",
            "Worker 1, [02/08]: Training Loss: 1.869355509, Training Accuracy: 48.736\n",
            "Worker 1, [03/08]: Training Loss: 1.827372629, Training Accuracy: 49.700\n",
            "Worker 1, [04/08]: Training Loss: 1.807497724, Training Accuracy: 50.096\n",
            "Worker 1, [05/08]: Training Loss: 1.797170320, Training Accuracy: 50.532\n",
            "Worker 1, [06/08]: Training Loss: 1.759081433, Training Accuracy: 51.148\n",
            "Worker 1, [07/08]: Training Loss: 1.756803984, Training Accuracy: 51.608\n",
            "Worker 1, [08/08]: Training Loss: 1.734701905, Training Accuracy: 51.944\n",
            "Time taken for training worker 1: 0:01:51.636961\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.542917214, Training Accuracy: 36.756\n",
            "Worker 2, [02/08]: Training Loss: 2.441615017, Training Accuracy: 38.272\n",
            "Worker 2, [03/08]: Training Loss: 2.440737278, Training Accuracy: 37.940\n",
            "Worker 2, [04/08]: Training Loss: 2.440581468, Training Accuracy: 37.884\n",
            "Worker 2, [05/08]: Training Loss: 2.449052468, Training Accuracy: 38.064\n",
            "Worker 2, [06/08]: Training Loss: 2.468048758, Training Accuracy: 37.828\n",
            "Worker 2, [07/08]: Training Loss: 2.499586932, Training Accuracy: 37.572\n",
            "Worker 2, [08/08]: Training Loss: 2.526924865, Training Accuracy: 37.308\n",
            "Time taken for training worker 2: 0:01:48.782341\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001632\n",
            "Local Step 04: Test Loss: 2.865916765, Test Accuracy: 42.280\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.807283027, Training Accuracy: 50.224\n",
            "Worker 1, [02/08]: Training Loss: 1.751070963, Training Accuracy: 51.640\n",
            "Worker 1, [03/08]: Training Loss: 1.727678149, Training Accuracy: 52.380\n",
            "Worker 1, [04/08]: Training Loss: 1.701814724, Training Accuracy: 52.596\n",
            "Worker 1, [05/08]: Training Loss: 1.702099483, Training Accuracy: 52.620\n",
            "Worker 1, [06/08]: Training Loss: 1.668512374, Training Accuracy: 53.584\n",
            "Worker 1, [07/08]: Training Loss: 1.667422694, Training Accuracy: 53.512\n",
            "Worker 1, [08/08]: Training Loss: 1.643034252, Training Accuracy: 54.028\n",
            "Time taken for training worker 1: 0:01:52.086508\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.541681962, Training Accuracy: 37.480\n",
            "Worker 2, [02/08]: Training Loss: 2.433278832, Training Accuracy: 38.556\n",
            "Worker 2, [03/08]: Training Loss: 2.412473192, Training Accuracy: 38.760\n",
            "Worker 2, [04/08]: Training Loss: 2.411603714, Training Accuracy: 38.732\n",
            "Worker 2, [05/08]: Training Loss: 2.427960520, Training Accuracy: 38.440\n",
            "Worker 2, [06/08]: Training Loss: 2.435528942, Training Accuracy: 38.448\n",
            "Worker 2, [07/08]: Training Loss: 2.468260446, Training Accuracy: 38.760\n",
            "Worker 2, [08/08]: Training Loss: 2.504580643, Training Accuracy: 37.708\n",
            "Time taken for training worker 2: 0:01:42.360486\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002397\n",
            "Local Step 05: Test Loss: 3.172831444, Test Accuracy: 40.510\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.691365430, Training Accuracy: 53.116\n",
            "Worker 1, [02/08]: Training Loss: 1.656863389, Training Accuracy: 53.668\n",
            "Worker 1, [03/08]: Training Loss: 1.659426932, Training Accuracy: 53.912\n",
            "Worker 1, [04/08]: Training Loss: 1.616888507, Training Accuracy: 54.908\n",
            "Worker 1, [05/08]: Training Loss: 1.606701878, Training Accuracy: 54.752\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[87], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Workers:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Number of Local Steps:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mHeteroCompSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmups\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[86], line 67\u001b[0m, in \u001b[0;36mHeteroCompSGD\u001b[0;34m(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, warmups)\u001b[0m\n\u001b[1;32m     65\u001b[0m train_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m local_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(j):\n\u001b[0;32m---> 67\u001b[0m   train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mis_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorker \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworker\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.9f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m train_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "Cell \u001b[0;32mIn[29], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn, accumulation_steps, device, is_wandb)\u001b[0m\n\u001b[1;32m      5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Initialize gradients to zero at the start of each epoch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     10\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:471\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:270\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:355\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    352\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    354\u001b[0m dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 355\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (std \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [4]\n",
        "J = [4] # TODO: Bar asase local sgd behtarin meghdare J ro peyda konim.\n",
        "\n",
        "num_epochs = 150\n",
        "warmups = 1\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    LayerwiseMaskingApproach(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, warmups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Our Approach 2: Confidence Interval Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def update_CI_Acc(accuracies, C_level=0.8):\n",
        "    mean = np.mean(accuracies)\n",
        "    std_dev = np.std(accuracies, ddof=1)\n",
        "    t_score = stats.t.ppf(1 - (1 - C_level) / 2, len(accuracies) - 1)\n",
        "    CI = t_score * (std_dev / np.sqrt(len(accuracies)))\n",
        "    # Compute relative error\n",
        "    RE = CI / mean\n",
        "    # Compute accuracy\n",
        "    accuracy = 1 - RE\n",
        "\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def ConfidenceInetrvalApproach(shard_loaders, loss_fn, k, lr, wd, initial_state_dict, num_epochs, ci_acc_threshold):\n",
        "  total_start_time = time.time()\n",
        "  print('Start Time:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_start_time)))\n",
        "  \n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  train_accuracy_workers = {i: [] for i in range(k)}\n",
        "  steps_per_worker = {i: 0 for i in range(k)}\n",
        "  accuracy_workers = {i: 0 for i in range(k)}\n",
        "  for i, model in enumerate(local_models):\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "  \n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "  global_optimizer = LocalSGDOptimizer(global_model, lr=lr)\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=num_epochs)\n",
        "\n",
        "  checkpoint = load_checkpoint('ci-strategy_0.9', 64, {'k': k})\n",
        "  if checkpoint is not None:\n",
        "      global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      global_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      \n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn)\n",
        "      print(f'Global Update: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      \n",
        "      return None\n",
        "  \n",
        "  total_epochs = 0\n",
        "  steps_per_worker = {i: 0 for i in range(k)}\n",
        "  while not all(value > num_epochs for value in steps_per_worker.values()): \n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      if steps_per_worker[worker] > num_epochs:\n",
        "        continue\n",
        "      \n",
        "      train_start_time = time.time()\n",
        "      step_counter = 0\n",
        "      while not accuracy_workers[worker] > ci_acc_threshold:\n",
        "        \n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        train_accuracy_workers[worker].append(train_accuracy)\n",
        "\n",
        "        accuracy_workers[worker] = update_CI_Acc(train_accuracy_workers[worker])\n",
        "        step_counter += 1\n",
        "        print(accuracy_workers[worker])\n",
        "        print(f'Worker {worker+1}, [epoch: {step_counter:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "\n",
        "      train_accuracy_workers[worker] = []\n",
        "      accuracy_workers[worker] = 0\n",
        "      steps_per_worker[worker] += step_counter  \n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time() \n",
        "\n",
        "    global_optimizer.step(local_models)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    for local_model in local_models:\n",
        "       local_model.load_state_dict(global_model.state_dict())\n",
        "    \n",
        "    for local_optimizer in local_optimizers:\n",
        "        local_optimizer.param_groups[0]['lr'] = global_optimizer.param_groups[0]['lr']\n",
        "\n",
        "    total_epochs += step_counter\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Global Model: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "  \n",
        "  # Save checkpoint\n",
        "  save_checkpoint({\n",
        "              'epoch': num_epochs,\n",
        "              'model_state_dict': global_model.state_dict(),\n",
        "              'optimizer_state_dict': global_optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss': loss_fn\n",
        "              }, 149, 64, 'ci-strategy_0.9', {'k': k}) \n",
        "  \n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for Confidence Interval: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:4\n",
            "==================================================\n",
            "Start Time: 2024-09-09 22:27:17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ali/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/home/ali/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 4.488386120, Training Accuracy: 2.776\n",
            "-0.19913263586900465\n",
            "Worker 1, [epoch: 02]: Training Loss: 4.115643849, Training Accuracy: 6.320\n",
            "0.42823425367681056\n",
            "Worker 1, [epoch: 03]: Training Loss: 3.893852718, Training Accuracy: 9.152\n",
            "0.5686271463496034\n",
            "Worker 1, [epoch: 04]: Training Loss: 3.728881975, Training Accuracy: 12.160\n",
            "0.6486468895106499\n",
            "Worker 1, [epoch: 05]: Training Loss: 3.599602384, Training Accuracy: 14.296\n",
            "0.6971833289855374\n",
            "Worker 1, [epoch: 06]: Training Loss: 3.483125723, Training Accuracy: 16.480\n",
            "0.7304575968641096\n",
            "Worker 1, [epoch: 07]: Training Loss: 3.347371815, Training Accuracy: 18.536\n",
            "0.7561668551060472\n",
            "Worker 1, [epoch: 08]: Training Loss: 3.256360842, Training Accuracy: 20.256\n",
            "0.7764791851402361\n",
            "Worker 1, [epoch: 09]: Training Loss: 3.169724770, Training Accuracy: 21.880\n",
            "0.7942401547328701\n",
            "Worker 1, [epoch: 10]: Training Loss: 3.072787778, Training Accuracy: 22.952\n",
            "0.8068736813515772\n",
            "Worker 1, [epoch: 11]: Training Loss: 2.988063891, Training Accuracy: 25.072\n",
            "0.8169456872740375\n",
            "Worker 1, [epoch: 12]: Training Loss: 2.919073134, Training Accuracy: 26.888\n",
            "0.826072890144241\n",
            "Worker 1, [epoch: 13]: Training Loss: 2.828874277, Training Accuracy: 28.200\n",
            "0.8343899234572356\n",
            "Worker 1, [epoch: 14]: Training Loss: 2.755206108, Training Accuracy: 29.384\n",
            "0.8420246054585505\n",
            "Worker 1, [epoch: 15]: Training Loss: 2.690833582, Training Accuracy: 30.432\n",
            "0.8486323298106847\n",
            "Worker 1, [epoch: 16]: Training Loss: 2.630199464, Training Accuracy: 31.784\n",
            "0.854502053395822\n",
            "Worker 1, [epoch: 17]: Training Loss: 2.576473423, Training Accuracy: 33.024\n",
            "0.8596883132330051\n",
            "Worker 1, [epoch: 18]: Training Loss: 2.513687365, Training Accuracy: 34.320\n",
            "0.864717300707756\n",
            "Worker 1, [epoch: 19]: Training Loss: 2.459999365, Training Accuracy: 35.032\n",
            "0.8687215342311037\n",
            "Worker 1, [epoch: 20]: Training Loss: 2.388912621, Training Accuracy: 36.952\n",
            "0.872404790136759\n",
            "Worker 1, [epoch: 21]: Training Loss: 2.356425519, Training Accuracy: 38.136\n",
            "0.8762941113098868\n",
            "Worker 1, [epoch: 22]: Training Loss: 2.304305253, Training Accuracy: 38.376\n",
            "0.8797987887274229\n",
            "Worker 1, [epoch: 23]: Training Loss: 2.254103117, Training Accuracy: 39.528\n",
            "0.883196995552472\n",
            "Worker 1, [epoch: 24]: Training Loss: 2.224592382, Training Accuracy: 40.152\n",
            "0.8860178287550544\n",
            "Worker 1, [epoch: 25]: Training Loss: 2.167889405, Training Accuracy: 41.896\n",
            "0.8888267129133504\n",
            "Worker 1, [epoch: 26]: Training Loss: 2.135233526, Training Accuracy: 42.440\n",
            "0.8916870629614443\n",
            "Worker 1, [epoch: 27]: Training Loss: 2.101819102, Training Accuracy: 42.648\n",
            "0.8942860175708452\n",
            "Worker 1, [epoch: 28]: Training Loss: 2.051049473, Training Accuracy: 43.712\n",
            "0.8967333423930345\n",
            "Worker 1, [epoch: 29]: Training Loss: 2.021597221, Training Accuracy: 44.504\n",
            "0.8990787867465321\n",
            "Worker 1, [epoch: 30]: Training Loss: 1.972401284, Training Accuracy: 45.136\n",
            "0.9010900350635997\n",
            "Worker 1, [epoch: 31]: Training Loss: 1.938416453, Training Accuracy: 46.704\n",
            "Time taken for training worker 1: 0:02:55.701860\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 4.491545200, Training Accuracy: 2.856\n",
            "-0.23262388517894528\n",
            "Worker 2, [epoch: 02]: Training Loss: 4.092638990, Training Accuracy: 6.672\n",
            "0.3886895392544666\n",
            "Worker 2, [epoch: 03]: Training Loss: 3.872337339, Training Accuracy: 10.264\n",
            "0.5778987930310168\n",
            "Worker 2, [epoch: 04]: Training Loss: 3.733321676, Training Accuracy: 12.200\n",
            "0.6598543578311717\n",
            "Worker 2, [epoch: 05]: Training Loss: 3.573525864, Training Accuracy: 14.544\n",
            "0.7065130193643945\n",
            "Worker 2, [epoch: 06]: Training Loss: 3.455748006, Training Accuracy: 16.816\n",
            "0.7368466378209053\n",
            "Worker 2, [epoch: 07]: Training Loss: 3.327708233, Training Accuracy: 19.088\n",
            "0.7616427740567184\n",
            "Worker 2, [epoch: 08]: Training Loss: 3.216751119, Training Accuracy: 20.688\n",
            "0.7788270906637863\n",
            "Worker 2, [epoch: 09]: Training Loss: 3.126767548, Training Accuracy: 22.968\n",
            "0.794490608456115\n",
            "Worker 2, [epoch: 10]: Training Loss: 3.039931932, Training Accuracy: 24.288\n",
            "0.8086405976848594\n",
            "Worker 2, [epoch: 11]: Training Loss: 2.944455127, Training Accuracy: 25.344\n",
            "0.8211446578128829\n",
            "Worker 2, [epoch: 12]: Training Loss: 2.890767410, Training Accuracy: 26.312\n",
            "0.8303664400171709\n",
            "Worker 2, [epoch: 13]: Training Loss: 2.800994087, Training Accuracy: 28.368\n",
            "0.8376035161397019\n",
            "Worker 2, [epoch: 14]: Training Loss: 2.721174149, Training Accuracy: 30.272\n",
            "0.8451093859037333\n",
            "Worker 2, [epoch: 15]: Training Loss: 2.683283026, Training Accuracy: 30.816\n",
            "0.851261367963104\n",
            "Worker 2, [epoch: 16]: Training Loss: 2.608889017, Training Accuracy: 32.480\n",
            "0.8569147428523535\n",
            "Worker 2, [epoch: 17]: Training Loss: 2.545528763, Training Accuracy: 33.584\n",
            "0.8625402169950982\n",
            "Worker 2, [epoch: 18]: Training Loss: 2.484941838, Training Accuracy: 34.088\n",
            "0.8676221965090875\n",
            "Worker 2, [epoch: 19]: Training Loss: 2.466623479, Training Accuracy: 35.096\n",
            "0.8715578886248835\n",
            "Worker 2, [epoch: 20]: Training Loss: 2.389087760, Training Accuracy: 37.152\n",
            "0.8752844218932258\n",
            "Worker 2, [epoch: 21]: Training Loss: 2.329490898, Training Accuracy: 38.120\n",
            "0.8789485289318112\n",
            "Worker 2, [epoch: 22]: Training Loss: 2.280726919, Training Accuracy: 38.776\n",
            "0.8820347852935545\n",
            "Worker 2, [epoch: 23]: Training Loss: 2.228208768, Training Accuracy: 40.392\n",
            "0.8852532662010091\n",
            "Worker 2, [epoch: 24]: Training Loss: 2.202935510, Training Accuracy: 40.664\n",
            "0.8881848000745916\n",
            "Worker 2, [epoch: 25]: Training Loss: 2.159297823, Training Accuracy: 41.744\n",
            "0.8911245899939164\n",
            "Worker 2, [epoch: 26]: Training Loss: 2.139227532, Training Accuracy: 42.088\n",
            "0.8937693443313779\n",
            "Worker 2, [epoch: 27]: Training Loss: 2.085825971, Training Accuracy: 43.224\n",
            "0.8962355983400648\n",
            "Worker 2, [epoch: 28]: Training Loss: 2.041758542, Training Accuracy: 44.112\n",
            "0.8987161126719632\n",
            "Worker 2, [epoch: 29]: Training Loss: 2.024329901, Training Accuracy: 44.352\n",
            "0.9009565366810988\n",
            "Worker 2, [epoch: 30]: Training Loss: 1.965238192, Training Accuracy: 45.456\n",
            "Time taken for training worker 2: 0:02:56.718761\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 4.488465071, Training Accuracy: 2.368\n",
            "-0.46387604914135716\n",
            "Worker 3, [epoch: 02]: Training Loss: 4.109228386, Training Accuracy: 6.664\n",
            "0.3620500864953172\n",
            "Worker 3, [epoch: 03]: Training Loss: 3.906586594, Training Accuracy: 9.608\n",
            "0.5510029203176662\n",
            "Worker 3, [epoch: 04]: Training Loss: 3.751060792, Training Accuracy: 12.240\n",
            "0.6438773210846687\n",
            "Worker 3, [epoch: 05]: Training Loss: 3.614504304, Training Accuracy: 14.256\n",
            "0.7003557899872528\n",
            "Worker 3, [epoch: 06]: Training Loss: 3.488404724, Training Accuracy: 15.992\n",
            "0.7376683854206616\n",
            "Worker 3, [epoch: 07]: Training Loss: 3.375483402, Training Accuracy: 17.712\n",
            "0.7613929442788594\n",
            "Worker 3, [epoch: 08]: Training Loss: 3.269037845, Training Accuracy: 19.944\n",
            "0.782050973919228\n",
            "Worker 3, [epoch: 09]: Training Loss: 3.192949569, Training Accuracy: 21.112\n",
            "0.79839997727505\n",
            "Worker 3, [epoch: 10]: Training Loss: 3.098200150, Training Accuracy: 22.576\n",
            "0.8101576186501296\n",
            "Worker 3, [epoch: 11]: Training Loss: 3.005752478, Training Accuracy: 24.608\n",
            "0.8195496093463256\n",
            "Worker 3, [epoch: 12]: Training Loss: 2.902234519, Training Accuracy: 26.408\n",
            "0.8285240824925961\n",
            "Worker 3, [epoch: 13]: Training Loss: 2.830923999, Training Accuracy: 27.504\n",
            "0.8357038629809027\n",
            "Worker 3, [epoch: 14]: Training Loss: 2.752617590, Training Accuracy: 29.312\n",
            "0.8425587628970377\n",
            "Worker 3, [epoch: 15]: Training Loss: 2.693825491, Training Accuracy: 30.392\n",
            "0.8482717915216765\n",
            "Worker 3, [epoch: 16]: Training Loss: 2.623339934, Training Accuracy: 32.064\n",
            "0.8537633330774932\n",
            "Worker 3, [epoch: 17]: Training Loss: 2.573446189, Training Accuracy: 33.088\n",
            "0.8595119794388166\n",
            "Worker 3, [epoch: 18]: Training Loss: 2.528961024, Training Accuracy: 33.384\n",
            "0.8646301799316481\n",
            "Worker 3, [epoch: 19]: Training Loss: 2.464237337, Training Accuracy: 34.512\n",
            "0.8688440601188165\n",
            "Worker 3, [epoch: 20]: Training Loss: 2.397373213, Training Accuracy: 36.200\n",
            "0.8730102927034442\n",
            "Worker 3, [epoch: 21]: Training Loss: 2.372368619, Training Accuracy: 36.800\n",
            "0.8765832042388139\n",
            "Worker 3, [epoch: 22]: Training Loss: 2.304263022, Training Accuracy: 38.272\n",
            "0.8802079657232871\n",
            "Worker 3, [epoch: 23]: Training Loss: 2.281191821, Training Accuracy: 38.656\n",
            "0.8834159222216678\n",
            "Worker 3, [epoch: 24]: Training Loss: 2.222328353, Training Accuracy: 39.912\n",
            "0.886358988043651\n",
            "Worker 3, [epoch: 25]: Training Loss: 2.195228179, Training Accuracy: 40.960\n",
            "0.8892320278704334\n",
            "Worker 3, [epoch: 26]: Training Loss: 2.146944669, Training Accuracy: 41.560\n",
            "0.8918449498255059\n",
            "Worker 3, [epoch: 27]: Training Loss: 2.095342219, Training Accuracy: 42.648\n",
            "0.8942973195044648\n",
            "Worker 3, [epoch: 28]: Training Loss: 2.051627800, Training Accuracy: 43.528\n",
            "0.8966967167399439\n",
            "Worker 3, [epoch: 29]: Training Loss: 2.033250881, Training Accuracy: 44.072\n",
            "0.8990335754706983\n",
            "Worker 3, [epoch: 30]: Training Loss: 2.014595221, Training Accuracy: 44.584\n",
            "0.9012391645278442\n",
            "Worker 3, [epoch: 31]: Training Loss: 1.981386754, Training Accuracy: 45.328\n",
            "Time taken for training worker 3: 0:03:00.572583\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 4.485111908, Training Accuracy: 2.720\n",
            "-0.33075131241539535\n",
            "Worker 4, [epoch: 02]: Training Loss: 4.103516919, Training Accuracy: 6.864\n",
            "0.40335858717879236\n",
            "Worker 4, [epoch: 03]: Training Loss: 3.896121222, Training Accuracy: 9.744\n",
            "0.5835474770118095\n",
            "Worker 4, [epoch: 04]: Training Loss: 3.737339073, Training Accuracy: 11.904\n",
            "0.6636748037364577\n",
            "Worker 4, [epoch: 05]: Training Loss: 3.589167563, Training Accuracy: 14.120\n",
            "0.7071215729602559\n",
            "Worker 4, [epoch: 06]: Training Loss: 3.477094936, Training Accuracy: 16.544\n",
            "0.7382627797273102\n",
            "Worker 4, [epoch: 07]: Training Loss: 3.359781324, Training Accuracy: 18.496\n",
            "0.763083170865267\n",
            "Worker 4, [epoch: 08]: Training Loss: 3.256718183, Training Accuracy: 20.072\n",
            "0.7813456773683957\n",
            "Worker 4, [epoch: 09]: Training Loss: 3.163605583, Training Accuracy: 21.984\n",
            "0.7966241937337545\n",
            "Worker 4, [epoch: 10]: Training Loss: 3.071429906, Training Accuracy: 23.488\n",
            "0.8108781853255143\n",
            "Worker 4, [epoch: 11]: Training Loss: 3.009417067, Training Accuracy: 24.328\n",
            "0.8219274964658927\n",
            "Worker 4, [epoch: 12]: Training Loss: 2.931975565, Training Accuracy: 26.016\n",
            "0.8311887619494659\n",
            "Worker 4, [epoch: 13]: Training Loss: 2.852859507, Training Accuracy: 27.440\n",
            "0.8380605155240128\n",
            "Worker 4, [epoch: 14]: Training Loss: 2.768909948, Training Accuracy: 29.528\n",
            "0.8448876113157983\n",
            "Worker 4, [epoch: 15]: Training Loss: 2.702695191, Training Accuracy: 30.432\n",
            "0.8509745422457853\n",
            "Worker 4, [epoch: 16]: Training Loss: 2.651576470, Training Accuracy: 31.704\n",
            "0.8563239427707323\n",
            "Worker 4, [epoch: 17]: Training Loss: 2.591278951, Training Accuracy: 33.064\n",
            "0.8615885438310231\n",
            "Worker 4, [epoch: 18]: Training Loss: 2.533595223, Training Accuracy: 33.784\n",
            "0.86593337092856\n",
            "Worker 4, [epoch: 19]: Training Loss: 2.458304328, Training Accuracy: 35.456\n",
            "0.8708287804493842\n",
            "Worker 4, [epoch: 20]: Training Loss: 2.461670761, Training Accuracy: 35.192\n",
            "0.8748199488630191\n",
            "Worker 4, [epoch: 21]: Training Loss: 2.385609655, Training Accuracy: 36.920\n",
            "0.8784715282756368\n",
            "Worker 4, [epoch: 22]: Training Loss: 2.331494069, Training Accuracy: 37.960\n",
            "0.8818182153263877\n",
            "Worker 4, [epoch: 23]: Training Loss: 2.281361235, Training Accuracy: 39.000\n",
            "0.8849341191428656\n",
            "Worker 4, [epoch: 24]: Training Loss: 2.241767611, Training Accuracy: 39.944\n",
            "0.8881489966172638\n",
            "Worker 4, [epoch: 25]: Training Loss: 2.236716418, Training Accuracy: 40.080\n",
            "0.8909252546172801\n",
            "Worker 4, [epoch: 26]: Training Loss: 2.160425210, Training Accuracy: 41.488\n",
            "0.893463758380221\n",
            "Worker 4, [epoch: 27]: Training Loss: 2.126170922, Training Accuracy: 42.528\n",
            "0.8960492924087582\n",
            "Worker 4, [epoch: 28]: Training Loss: 2.105053560, Training Accuracy: 42.744\n",
            "0.8983093746720217\n",
            "Worker 4, [epoch: 29]: Training Loss: 2.049109852, Training Accuracy: 44.096\n",
            "0.9005223748367273\n",
            "Worker 4, [epoch: 30]: Training Loss: 2.015115396, Training Accuracy: 44.616\n",
            "Time taken for training worker 4: 0:02:56.309842\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002810\n",
            "Global Model: Test Loss: 3.402951994, Test Accuracy: 27.510\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.805591313, Training Accuracy: 28.856\n",
            "0.7812411392558964\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.588261546, Training Accuracy: 33.272\n",
            "0.8880043893405387\n",
            "Worker 1, [epoch: 03]: Training Loss: 2.476217591, Training Accuracy: 35.416\n",
            "0.9197098225987868\n",
            "Worker 1, [epoch: 04]: Training Loss: 2.380619955, Training Accuracy: 36.136\n",
            "Time taken for training worker 1: 0:00:23.342634\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.807224030, Training Accuracy: 28.672\n",
            "0.7477308576059174\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.557370829, Training Accuracy: 33.792\n",
            "0.8832254089829863\n",
            "Worker 2, [epoch: 03]: Training Loss: 2.475696909, Training Accuracy: 35.360\n",
            "0.9086108344722699\n",
            "Worker 2, [epoch: 04]: Training Loss: 2.372413473, Training Accuracy: 37.552\n",
            "Time taken for training worker 2: 0:00:23.626692\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 2.810861069, Training Accuracy: 28.616\n",
            "0.8106475693361587\n",
            "Worker 3, [epoch: 02]: Training Loss: 2.591074260, Training Accuracy: 32.368\n",
            "0.8821748252468758\n",
            "Worker 3, [epoch: 03]: Training Loss: 2.459842811, Training Accuracy: 35.576\n",
            "0.9131230588385002\n",
            "Worker 3, [epoch: 04]: Training Loss: 2.404675000, Training Accuracy: 36.352\n",
            "Time taken for training worker 3: 0:00:23.965541\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.814220390, Training Accuracy: 28.248\n",
            "0.7949838087823508\n",
            "Worker 4, [epoch: 02]: Training Loss: 2.617859248, Training Accuracy: 32.280\n",
            "0.8881568180726968\n",
            "Worker 4, [epoch: 03]: Training Loss: 2.501985695, Training Accuracy: 34.704\n",
            "0.9092257799710569\n",
            "Worker 4, [epoch: 04]: Training Loss: 2.413045920, Training Accuracy: 36.760\n",
            "Time taken for training worker 4: 0:00:25.027009\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002784\n",
            "Global Model: Test Loss: 2.293720386, Test Accuracy: 40.550\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.487632875, Training Accuracy: 35.240\n",
            "0.9157357565436594\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.409632973, Training Accuracy: 37.224\n",
            "Time taken for training worker 1: 0:00:11.887671\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.468804620, Training Accuracy: 35.592\n",
            "0.9303599726862029\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.383138845, Training Accuracy: 37.240\n",
            "Time taken for training worker 2: 0:00:12.776452\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 2.486369907, Training Accuracy: 34.936\n",
            "0.9040677677254141\n",
            "Worker 3, [epoch: 02]: Training Loss: 2.380717353, Training Accuracy: 37.184\n",
            "Time taken for training worker 3: 0:00:11.680332\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.498653077, Training Accuracy: 34.744\n",
            "0.9102184639307882\n",
            "Worker 4, [epoch: 02]: Training Loss: 2.412272986, Training Accuracy: 36.832\n",
            "Time taken for training worker 4: 0:00:11.616878\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003128\n",
            "Global Model: Test Loss: 2.182782347, Test Accuracy: 42.290\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.403303827, Training Accuracy: 36.984\n",
            "0.9489344148261628\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.318126079, Training Accuracy: 38.232\n",
            "Time taken for training worker 1: 0:00:11.354154\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.399055960, Training Accuracy: 37.224\n",
            "0.9543853805622385\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.315693343, Training Accuracy: 38.344\n",
            "Time taken for training worker 2: 0:00:11.584080\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 2.416680231, Training Accuracy: 36.792\n",
            "0.9473787811548141\n",
            "Worker 3, [epoch: 02]: Training Loss: 2.321164419, Training Accuracy: 38.072\n",
            "Time taken for training worker 3: 0:00:12.632452\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.421405783, Training Accuracy: 37.000\n",
            "0.9380550982977242\n",
            "Worker 4, [epoch: 02]: Training Loss: 2.360878680, Training Accuracy: 38.520\n",
            "Time taken for training worker 4: 0:00:12.557347\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002276\n",
            "Global Model: Test Loss: 2.134465028, Test Accuracy: 43.810\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.335582133, Training Accuracy: 38.496\n",
            "0.9163148820874456\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.251794460, Training Accuracy: 40.648\n",
            "Time taken for training worker 1: 0:00:11.707433\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.323416587, Training Accuracy: 38.720\n",
            "0.945654867518352\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.260284165, Training Accuracy: 40.112\n",
            "Time taken for training worker 2: 0:00:11.814707\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 2.330632357, Training Accuracy: 38.688\n",
            "0.96166250852161\n",
            "Worker 3, [epoch: 02]: Training Loss: 2.266029607, Training Accuracy: 39.664\n",
            "Time taken for training worker 3: 0:00:11.650184\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.372139172, Training Accuracy: 38.096\n",
            "0.929277686465475\n",
            "Worker 4, [epoch: 02]: Training Loss: 2.288120221, Training Accuracy: 39.888\n",
            "Time taken for training worker 4: 0:00:12.785091\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002776\n",
            "Global Model: Test Loss: 2.104915649, Test Accuracy: 44.420\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.266189925, Training Accuracy: 39.664\n",
            "0.9469259726982996\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.191633842, Training Accuracy: 41.056\n",
            "Time taken for training worker 1: 0:00:11.334432\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.277085225, Training Accuracy: 40.440\n",
            "0.9526417998754544\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.181857517, Training Accuracy: 41.704\n",
            "Time taken for training worker 2: 0:00:11.308070\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 2.276033130, Training Accuracy: 39.448\n",
            "0.9502609005359473\n",
            "Worker 3, [epoch: 02]: Training Loss: 2.214192362, Training Accuracy: 40.744\n",
            "Time taken for training worker 3: 0:00:11.316720\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.316556754, Training Accuracy: 39.192\n",
            "0.9658311274633771\n",
            "Worker 4, [epoch: 02]: Training Loss: 2.245037249, Training Accuracy: 40.072\n",
            "Time taken for training worker 4: 0:00:12.259880\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002860\n",
            "Global Model: Test Loss: 2.067607453, Test Accuracy: 45.550\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.236743950, Training Accuracy: 40.728\n",
            "0.9323033781557403\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.148461709, Training Accuracy: 42.560\n",
            "Time taken for training worker 1: 0:00:12.078821\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.227654004, Training Accuracy: 40.800\n",
            "0.9463384336543095\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.166224063, Training Accuracy: 42.248\n",
            "Time taken for training worker 2: 0:00:12.563195\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 2.260377906, Training Accuracy: 40.320\n",
            "0.934555520473006\n",
            "Worker 3, [epoch: 02]: Training Loss: 2.159124063, Training Accuracy: 42.072\n",
            "Time taken for training worker 3: 0:00:12.371705\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.252295863, Training Accuracy: 41.056\n",
            "0.9970043960120615\n",
            "Worker 4, [epoch: 02]: Training Loss: 2.204726462, Training Accuracy: 41.136\n",
            "Time taken for training worker 4: 0:00:11.798810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002877\n",
            "Global Model: Test Loss: 2.064806262, Test Accuracy: 45.800\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.187948772, Training Accuracy: 41.448\n",
            "0.9463025060832319\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.117739864, Training Accuracy: 42.920\n",
            "Time taken for training worker 1: 0:00:12.298011\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.194898110, Training Accuracy: 42.072\n",
            "0.9349805339277645\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.088498954, Training Accuracy: 43.888\n",
            "Time taken for training worker 2: 0:00:13.038438\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 2.199348676, Training Accuracy: 41.456\n",
            "0.9537850219655168\n",
            "Worker 3, [epoch: 02]: Training Loss: 2.143182373, Training Accuracy: 42.720\n",
            "Time taken for training worker 3: 0:00:11.857701\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.234175019, Training Accuracy: 40.696\n",
            "0.9212966665612025\n",
            "Worker 4, [epoch: 02]: Training Loss: 2.153536018, Training Accuracy: 42.832\n",
            "Time taken for training worker 4: 0:00:12.241362\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002364\n",
            "Global Model: Test Loss: 2.011446913, Test Accuracy: 46.720\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.156500645, Training Accuracy: 42.504\n",
            "0.9411907604355196\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.096487827, Training Accuracy: 44.160\n",
            "Time taken for training worker 1: 0:00:11.374017\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.151967465, Training Accuracy: 42.888\n",
            "0.9707158056043966\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.065872434, Training Accuracy: 43.712\n",
            "Time taken for training worker 2: 0:00:11.568401\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 2.157587415, Training Accuracy: 42.872\n",
            "0.9712685056605982\n",
            "Worker 3, [epoch: 02]: Training Loss: 2.089629342, Training Accuracy: 43.680\n",
            "Time taken for training worker 3: 0:00:11.858765\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.197767845, Training Accuracy: 41.872\n",
            "0.9411687179919048\n",
            "Worker 4, [epoch: 02]: Training Loss: 2.103698606, Training Accuracy: 43.504\n",
            "Time taken for training worker 4: 0:00:11.408951\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002748\n",
            "Global Model: Test Loss: 2.001668355, Test Accuracy: 47.470\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.092918878, Training Accuracy: 43.912\n",
            "0.9544271934273276\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.026223017, Training Accuracy: 45.232\n",
            "Time taken for training worker 1: 0:00:12.444129\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.095679619, Training Accuracy: 43.704\n",
            "0.9618871332733012\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.036787484, Training Accuracy: 44.800\n",
            "Time taken for training worker 2: 0:00:12.325248\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 2.096445775, Training Accuracy: 42.912\n",
            "0.945056459786183\n",
            "Worker 3, [epoch: 02]: Training Loss: 2.030920735, Training Accuracy: 44.472\n",
            "Time taken for training worker 3: 0:00:12.164219\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.142720266, Training Accuracy: 42.616\n",
            "0.9277830444064895\n",
            "Worker 4, [epoch: 02]: Training Loss: 2.058090122, Training Accuracy: 44.664\n",
            "Time taken for training worker 4: 0:00:11.536641\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002512\n",
            "Global Model: Test Loss: 1.960051615, Test Accuracy: 48.010\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.081856084, Training Accuracy: 44.320\n",
            "0.9334187667447318\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.989659373, Training Accuracy: 46.280\n",
            "Time taken for training worker 1: 0:00:12.001300\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.064098090, Training Accuracy: 44.336\n",
            "0.9297258189070948\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.991542736, Training Accuracy: 46.408\n",
            "Time taken for training worker 2: 0:00:13.223967\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 2.078933907, Training Accuracy: 43.832\n",
            "0.9456473637983023\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.998085957, Training Accuracy: 45.408\n",
            "Time taken for training worker 3: 0:00:11.962227\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.101239460, Training Accuracy: 44.168\n",
            "0.9770384059695957\n",
            "Worker 4, [epoch: 02]: Training Loss: 2.050441120, Training Accuracy: 44.832\n",
            "Time taken for training worker 4: 0:00:11.374023\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002546\n",
            "Global Model: Test Loss: 1.953307288, Test Accuracy: 48.260\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.016862211, Training Accuracy: 45.384\n",
            "0.9500973882374177\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.962524362, Training Accuracy: 46.880\n",
            "Time taken for training worker 1: 0:00:11.449593\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.029377873, Training Accuracy: 45.104\n",
            "0.9395273448609838\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.963169862, Training Accuracy: 46.912\n",
            "Time taken for training worker 2: 0:00:11.418933\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 2.057453477, Training Accuracy: 44.632\n",
            "0.9474039282753423\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.980891198, Training Accuracy: 46.184\n",
            "Time taken for training worker 3: 0:00:11.489911\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.077493385, Training Accuracy: 44.272\n",
            "0.9572239280288293\n",
            "Worker 4, [epoch: 02]: Training Loss: 2.013497238, Training Accuracy: 45.520\n",
            "Time taken for training worker 4: 0:00:11.631974\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002568\n",
            "Global Model: Test Loss: 1.931460003, Test Accuracy: 48.990\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.980838694, Training Accuracy: 46.312\n",
            "0.9572725988787264\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.929073507, Training Accuracy: 47.616\n",
            "Time taken for training worker 1: 0:00:11.956709\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.005143880, Training Accuracy: 45.704\n",
            "0.9501804283627461\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.930453204, Training Accuracy: 47.208\n",
            "Time taken for training worker 2: 0:00:11.359870\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.996867235, Training Accuracy: 46.128\n",
            "0.9719681685045517\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.955875931, Training Accuracy: 46.976\n",
            "Time taken for training worker 3: 0:00:11.976436\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.054926813, Training Accuracy: 44.840\n",
            "0.9362792872963879\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.969861948, Training Accuracy: 46.736\n",
            "Time taken for training worker 4: 0:00:11.724368\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002676\n",
            "Global Model: Test Loss: 1.923964068, Test Accuracy: 48.520\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.983101822, Training Accuracy: 46.320\n",
            "0.937253892862949\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.890596722, Training Accuracy: 48.248\n",
            "Time taken for training worker 1: 0:00:12.292884\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.969923426, Training Accuracy: 46.168\n",
            "0.9327082752034226\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.906623420, Training Accuracy: 48.232\n",
            "Time taken for training worker 2: 0:00:12.773486\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.997349819, Training Accuracy: 45.720\n",
            "0.9423991233592632\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.912939959, Training Accuracy: 47.464\n",
            "Time taken for training worker 3: 0:00:12.118379\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 2.042614624, Training Accuracy: 45.312\n",
            "0.9405832024242627\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.947624959, Training Accuracy: 47.096\n",
            "Time taken for training worker 4: 0:00:11.987495\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002474\n",
            "Global Model: Test Loss: 1.895008430, Test Accuracy: 49.350\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.938317959, Training Accuracy: 47.472\n",
            "0.9852890598221515\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.894913524, Training Accuracy: 47.928\n",
            "Time taken for training worker 1: 0:00:12.475114\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.955561344, Training Accuracy: 47.312\n",
            "0.942046108424208\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.859635280, Training Accuracy: 49.128\n",
            "Time taken for training worker 2: 0:00:11.857610\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.985572660, Training Accuracy: 46.096\n",
            "0.9313285169860307\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.898739734, Training Accuracy: 48.200\n",
            "Time taken for training worker 3: 0:00:12.400120\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.986021513, Training Accuracy: 46.512\n",
            "0.9721975068225646\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.928530079, Training Accuracy: 47.360\n",
            "Time taken for training worker 4: 0:00:12.152969\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003254\n",
            "Global Model: Test Loss: 1.890713823, Test Accuracy: 49.630\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.907312550, Training Accuracy: 47.936\n",
            "0.9569519334107843\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.853680629, Training Accuracy: 49.296\n",
            "Time taken for training worker 1: 0:00:11.868562\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.912597504, Training Accuracy: 48.152\n",
            "0.9723826866258622\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.848005250, Training Accuracy: 49.024\n",
            "Time taken for training worker 2: 0:00:11.391015\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.926409065, Training Accuracy: 47.656\n",
            "0.9347648412257957\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.859304355, Training Accuracy: 49.720\n",
            "Time taken for training worker 3: 0:00:11.564626\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.979015137, Training Accuracy: 46.960\n",
            "0.9406110686580066\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.869740239, Training Accuracy: 48.808\n",
            "Time taken for training worker 4: 0:00:11.932306\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002486\n",
            "Global Model: Test Loss: 1.863381750, Test Accuracy: 50.260\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.886982510, Training Accuracy: 48.112\n",
            "0.9491666333183992\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.840288472, Training Accuracy: 49.728\n",
            "Time taken for training worker 1: 0:00:11.514074\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.912341434, Training Accuracy: 48.328\n",
            "0.9727325822790449\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.852819237, Training Accuracy: 49.192\n",
            "Time taken for training worker 2: 0:00:11.430165\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.914678075, Training Accuracy: 47.584\n",
            "0.9426203201279522\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.837004789, Training Accuracy: 49.392\n",
            "Time taken for training worker 3: 0:00:11.464656\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.959296059, Training Accuracy: 47.320\n",
            "0.9388033099478149\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.865345737, Training Accuracy: 49.240\n",
            "Time taken for training worker 4: 0:00:12.012241\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002557\n",
            "Global Model: Test Loss: 1.884014278, Test Accuracy: 50.010\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.872652227, Training Accuracy: 48.576\n",
            "0.9469493733571852\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.812526854, Training Accuracy: 50.280\n",
            "Time taken for training worker 1: 0:00:11.894657\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.877432017, Training Accuracy: 48.856\n",
            "0.9506598707883386\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.801418920, Training Accuracy: 50.448\n",
            "Time taken for training worker 2: 0:00:12.497334\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.892988520, Training Accuracy: 48.456\n",
            "0.9653346124604409\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.813719031, Training Accuracy: 49.560\n",
            "Time taken for training worker 3: 0:00:11.454651\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.928213126, Training Accuracy: 48.584\n",
            "0.9786114707985722\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.841804349, Training Accuracy: 49.264\n",
            "Time taken for training worker 4: 0:00:12.215246\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002959\n",
            "Global Model: Test Loss: 1.862380752, Test Accuracy: 50.450\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.848758915, Training Accuracy: 49.456\n",
            "0.9385243935180432\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.756444240, Training Accuracy: 51.472\n",
            "Time taken for training worker 1: 0:00:12.405564\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.841292960, Training Accuracy: 50.104\n",
            "0.9727250707258052\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.786208118, Training Accuracy: 51.000\n",
            "Time taken for training worker 2: 0:00:11.458874\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.865588576, Training Accuracy: 48.832\n",
            "0.9413892327586781\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.775627222, Training Accuracy: 50.728\n",
            "Time taken for training worker 3: 0:00:11.421686\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.886403071, Training Accuracy: 48.968\n",
            "0.9691385445080247\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.821282181, Training Accuracy: 49.960\n",
            "Time taken for training worker 4: 0:00:11.905698\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002916\n",
            "Global Model: Test Loss: 1.857900951, Test Accuracy: 50.570\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.807642231, Training Accuracy: 50.744\n",
            "0.9718746407987727\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.757540286, Training Accuracy: 51.680\n",
            "Time taken for training worker 1: 0:00:11.565778\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.845004764, Training Accuracy: 49.800\n",
            "0.9570986612903077\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.763655997, Training Accuracy: 51.208\n",
            "Time taken for training worker 2: 0:00:11.449706\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.836873082, Training Accuracy: 49.544\n",
            "0.9472480841107057\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.751981132, Training Accuracy: 51.272\n",
            "Time taken for training worker 3: 0:00:11.402597\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.861273982, Training Accuracy: 49.192\n",
            "0.9660922682269828\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.793477913, Training Accuracy: 50.288\n",
            "Time taken for training worker 4: 0:00:11.479933\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003292\n",
            "Global Model: Test Loss: 1.852342431, Test Accuracy: 50.430\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.801206082, Training Accuracy: 50.504\n",
            "0.9669636727573869\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.758481036, Training Accuracy: 51.600\n",
            "Time taken for training worker 1: 0:00:11.877197\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.794988189, Training Accuracy: 50.608\n",
            "0.954937343847736\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.729750855, Training Accuracy: 52.112\n",
            "Time taken for training worker 2: 0:00:11.537546\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.830626968, Training Accuracy: 49.288\n",
            "0.940719562559353\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.764042528, Training Accuracy: 51.224\n",
            "Time taken for training worker 3: 0:00:12.857412\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.848138300, Training Accuracy: 49.464\n",
            "0.9478856353108146\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.772972908, Training Accuracy: 51.168\n",
            "Time taken for training worker 4: 0:00:12.816504\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002782\n",
            "Global Model: Test Loss: 1.805268976, Test Accuracy: 51.800\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.796643177, Training Accuracy: 50.512\n",
            "0.9576950716535009\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.721171628, Training Accuracy: 51.920\n",
            "Time taken for training worker 1: 0:00:11.655111\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.787039074, Training Accuracy: 50.520\n",
            "0.9323167009008041\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.696056554, Training Accuracy: 52.792\n",
            "Time taken for training worker 2: 0:00:11.544766\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.781064433, Training Accuracy: 51.272\n",
            "0.9956841458655546\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.739719762, Training Accuracy: 51.416\n",
            "Time taken for training worker 3: 0:00:13.217364\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.833963488, Training Accuracy: 50.240\n",
            "0.9744842659765639\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.771769781, Training Accuracy: 51.080\n",
            "Time taken for training worker 4: 0:00:12.972549\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002819\n",
            "Global Model: Test Loss: 1.818354026, Test Accuracy: 51.580\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.777562222, Training Accuracy: 50.736\n",
            "0.9409785402700637\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.702945218, Training Accuracy: 52.720\n",
            "Time taken for training worker 1: 0:00:12.116664\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.779213180, Training Accuracy: 51.352\n",
            "0.9663304266345524\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.694797093, Training Accuracy: 52.488\n",
            "Time taken for training worker 2: 0:00:12.487941\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.763868862, Training Accuracy: 51.712\n",
            "0.9716951238147659\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.700171240, Training Accuracy: 52.672\n",
            "Time taken for training worker 3: 0:00:13.059383\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.823465779, Training Accuracy: 50.088\n",
            "0.9475738026867028\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.739878111, Training Accuracy: 51.824\n",
            "Time taken for training worker 4: 0:00:12.165848\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003189\n",
            "Global Model: Test Loss: 1.807168407, Test Accuracy: 51.540\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.758239813, Training Accuracy: 51.256\n",
            "0.9540997381443486\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.701830368, Training Accuracy: 52.808\n",
            "Time taken for training worker 1: 0:00:11.398815\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.747864658, Training Accuracy: 51.256\n",
            "0.9482841483427829\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.694237249, Training Accuracy: 53.008\n",
            "Time taken for training worker 2: 0:00:11.953370\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.751554647, Training Accuracy: 51.648\n",
            "0.9611671037975615\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.692218288, Training Accuracy: 52.968\n",
            "Time taken for training worker 3: 0:00:13.005325\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.801281857, Training Accuracy: 50.784\n",
            "0.965009983146524\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.740149089, Training Accuracy: 51.952\n",
            "Time taken for training worker 4: 0:00:12.656208\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002639\n",
            "Global Model: Test Loss: 1.801259926, Test Accuracy: 51.580\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.746900156, Training Accuracy: 51.640\n",
            "0.9660497602759366\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.679400322, Training Accuracy: 52.792\n",
            "Time taken for training worker 1: 0:00:12.214014\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.716760115, Training Accuracy: 52.456\n",
            "0.9688700487631459\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.657054489, Training Accuracy: 53.528\n",
            "Time taken for training worker 2: 0:00:11.516155\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.745419992, Training Accuracy: 51.520\n",
            "0.9513195925573458\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.670829076, Training Accuracy: 53.176\n",
            "Time taken for training worker 3: 0:00:12.375651\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.804275102, Training Accuracy: 50.464\n",
            "0.9397282987739846\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.718844215, Training Accuracy: 52.480\n",
            "Time taken for training worker 4: 0:00:11.459981\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002431\n",
            "Global Model: Test Loss: 1.805604531, Test Accuracy: 51.830\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.709333665, Training Accuracy: 53.208\n",
            "0.9864094507375749\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.656997708, Training Accuracy: 53.680\n",
            "Time taken for training worker 1: 0:00:12.764904\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.711445335, Training Accuracy: 52.600\n",
            "0.953665314135363\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.640458757, Training Accuracy: 54.208\n",
            "Time taken for training worker 2: 0:00:12.492862\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.727399327, Training Accuracy: 52.152\n",
            "0.985433864175047\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.695673712, Training Accuracy: 52.648\n",
            "Time taken for training worker 3: 0:00:11.395167\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.750893739, Training Accuracy: 51.632\n",
            "0.9623179242460017\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.676673487, Training Accuracy: 52.912\n",
            "Time taken for training worker 4: 0:00:11.465580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002957\n",
            "Global Model: Test Loss: 1.795941436, Test Accuracy: 52.270\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.691427025, Training Accuracy: 53.312\n",
            "0.9795845692172992\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.645404647, Training Accuracy: 54.024\n",
            "Time taken for training worker 1: 0:00:12.032862\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.694949239, Training Accuracy: 52.920\n",
            "0.9525875945373212\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.641929499, Training Accuracy: 54.576\n",
            "Time taken for training worker 2: 0:00:11.726588\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.715773132, Training Accuracy: 52.360\n",
            "0.965591693344406\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.653532313, Training Accuracy: 53.544\n",
            "Time taken for training worker 3: 0:00:12.589591\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.722720402, Training Accuracy: 52.736\n",
            "0.9669761577383719\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.671021047, Training Accuracy: 53.880\n",
            "Time taken for training worker 4: 0:00:12.581433\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003014\n",
            "Global Model: Test Loss: 1.791535629, Test Accuracy: 52.100\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.677825795, Training Accuracy: 53.040\n",
            "0.9448378450596734\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.612071147, Training Accuracy: 54.976\n",
            "Time taken for training worker 1: 0:00:11.540310\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.678421883, Training Accuracy: 53.424\n",
            "0.964917596985196\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.604731642, Training Accuracy: 54.656\n",
            "Time taken for training worker 2: 0:00:11.382562\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.701547866, Training Accuracy: 52.600\n",
            "0.9566198451795571\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.625423220, Training Accuracy: 54.104\n",
            "Time taken for training worker 3: 0:00:11.695009\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.712268453, Training Accuracy: 52.688\n",
            "0.9710678768029649\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.662216471, Training Accuracy: 53.688\n",
            "Time taken for training worker 4: 0:00:12.528327\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003243\n",
            "Global Model: Test Loss: 1.812593112, Test Accuracy: 52.190\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.678652304, Training Accuracy: 53.520\n",
            "0.9676799983520554\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.613639116, Training Accuracy: 54.656\n",
            "Time taken for training worker 1: 0:00:12.287335\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.668440167, Training Accuracy: 53.992\n",
            "0.9843473012407062\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.597773163, Training Accuracy: 54.544\n",
            "Time taken for training worker 2: 0:00:11.869727\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.689104772, Training Accuracy: 53.320\n",
            "0.9779936228532735\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.618879705, Training Accuracy: 54.088\n",
            "Time taken for training worker 3: 0:00:12.932150\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.739220567, Training Accuracy: 52.608\n",
            "0.9514037846029448\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.646116884, Training Accuracy: 54.296\n",
            "Time taken for training worker 4: 0:00:12.493591\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002835\n",
            "Global Model: Test Loss: 1.795762076, Test Accuracy: 51.880\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.660780550, Training Accuracy: 53.976\n",
            "0.9539437093855317\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.584008126, Training Accuracy: 55.616\n",
            "Time taken for training worker 1: 0:00:11.626424\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.657170633, Training Accuracy: 53.568\n",
            "0.9529295459015277\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.583016213, Training Accuracy: 55.232\n",
            "Time taken for training worker 2: 0:00:11.232198\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.664905610, Training Accuracy: 53.712\n",
            "0.9466219540198867\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.595870222, Training Accuracy: 55.608\n",
            "Time taken for training worker 3: 0:00:12.248870\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.701519577, Training Accuracy: 52.688\n",
            "0.9319116782414308\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.613884782, Training Accuracy: 55.072\n",
            "Time taken for training worker 4: 0:00:11.944977\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002862\n",
            "Global Model: Test Loss: 1.780252501, Test Accuracy: 52.410\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.650932496, Training Accuracy: 53.912\n",
            "0.941533197233333\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.558371229, Training Accuracy: 56.000\n",
            "Time taken for training worker 1: 0:00:11.669320\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.646678047, Training Accuracy: 53.584\n",
            "0.9616565781210381\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.600215992, Training Accuracy: 54.936\n",
            "Time taken for training worker 2: 0:00:11.828591\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.642903168, Training Accuracy: 53.832\n",
            "0.9611620272634085\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.595896223, Training Accuracy: 55.208\n",
            "Time taken for training worker 3: 0:00:12.234613\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.661379655, Training Accuracy: 54.200\n",
            "0.9740992603893965\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.626473783, Training Accuracy: 55.120\n",
            "Time taken for training worker 4: 0:00:12.029948\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002581\n",
            "Global Model: Test Loss: 1.787769313, Test Accuracy: 52.790\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.641577833, Training Accuracy: 53.808\n",
            "0.9210850375074922\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.539481439, Training Accuracy: 56.640\n",
            "Time taken for training worker 1: 0:00:11.619666\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.635822445, Training Accuracy: 54.368\n",
            "0.9799785224171118\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.579242580, Training Accuracy: 55.080\n",
            "Time taken for training worker 2: 0:00:11.903661\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.633599945, Training Accuracy: 54.416\n",
            "0.9668777513447675\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.578170489, Training Accuracy: 55.600\n",
            "Time taken for training worker 3: 0:00:11.708694\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.652965640, Training Accuracy: 54.144\n",
            "0.9543051352554298\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.577689873, Training Accuracy: 55.776\n",
            "Time taken for training worker 4: 0:00:11.722180\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002854\n",
            "Global Model: Test Loss: 1.787281147, Test Accuracy: 52.610\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.615497564, Training Accuracy: 54.648\n",
            "0.9362446747721987\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.528008496, Training Accuracy: 56.960\n",
            "Time taken for training worker 1: 0:00:11.831167\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.596787926, Training Accuracy: 55.160\n",
            "0.9677567597278744\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.549477536, Training Accuracy: 56.328\n",
            "Time taken for training worker 2: 0:00:12.861813\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.626817727, Training Accuracy: 54.080\n",
            "0.944342857087316\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.548328586, Training Accuracy: 56.072\n",
            "Time taken for training worker 3: 0:00:11.910326\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.649103174, Training Accuracy: 53.640\n",
            "0.9421259479723125\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.596432289, Training Accuracy: 55.696\n",
            "Time taken for training worker 4: 0:00:12.681265\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003406\n",
            "Global Model: Test Loss: 1.793459661, Test Accuracy: 52.620\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.575582266, Training Accuracy: 56.160\n",
            "0.9797477839835612\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.503610555, Training Accuracy: 56.904\n",
            "Time taken for training worker 1: 0:00:13.395620\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.578989502, Training Accuracy: 55.320\n",
            "0.9643664427510246\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.528060371, Training Accuracy: 56.616\n",
            "Time taken for training worker 2: 0:00:12.169795\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.599752821, Training Accuracy: 54.760\n",
            "0.9541569768869529\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.526632626, Training Accuracy: 56.416\n",
            "Time taken for training worker 3: 0:00:12.270428\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.653390775, Training Accuracy: 54.280\n",
            "0.9432326002178669\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.555783213, Training Accuracy: 56.320\n",
            "Time taken for training worker 4: 0:00:11.821435\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002627\n",
            "Global Model: Test Loss: 1.778088225, Test Accuracy: 52.850\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.568458021, Training Accuracy: 55.896\n",
            "0.9493117998517295\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.502580311, Training Accuracy: 57.768\n",
            "Time taken for training worker 1: 0:00:11.871825\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.592269395, Training Accuracy: 55.128\n",
            "0.940005315225175\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.518518660, Training Accuracy: 57.320\n",
            "Time taken for training worker 2: 0:00:11.795536\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.584904955, Training Accuracy: 55.552\n",
            "0.9655965599614365\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.534327357, Training Accuracy: 56.808\n",
            "Time taken for training worker 3: 0:00:11.851572\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.620415219, Training Accuracy: 55.000\n",
            "0.9724929538263106\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.571978866, Training Accuracy: 55.992\n",
            "Time taken for training worker 4: 0:00:12.420082\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003009\n",
            "Global Model: Test Loss: 1.793046381, Test Accuracy: 52.420\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.568247983, Training Accuracy: 56.088\n",
            "0.9777737382614559\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.514664627, Training Accuracy: 56.904\n",
            "Time taken for training worker 1: 0:00:12.993070\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.540415166, Training Accuracy: 56.472\n",
            "0.9848154886097727\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.507543966, Training Accuracy: 57.032\n",
            "Time taken for training worker 2: 0:00:11.848153\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.587611026, Training Accuracy: 55.144\n",
            "0.9471218555309701\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.510314395, Training Accuracy: 57.072\n",
            "Time taken for training worker 3: 0:00:13.362170\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.603601234, Training Accuracy: 55.504\n",
            "0.9551931592125843\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.521014404, Training Accuracy: 57.144\n",
            "Time taken for training worker 4: 0:00:11.451261\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002442\n",
            "Global Model: Test Loss: 1.778920660, Test Accuracy: 53.120\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.567081635, Training Accuracy: 55.816\n",
            "0.9434910022362829\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.490772101, Training Accuracy: 57.904\n",
            "Time taken for training worker 1: 0:00:12.638292\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.553482096, Training Accuracy: 56.032\n",
            "0.9639554489081066\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.495772311, Training Accuracy: 57.360\n",
            "Time taken for training worker 2: 0:00:11.692159\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.552841482, Training Accuracy: 56.128\n",
            "0.9758450913787693\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.492424645, Training Accuracy: 57.016\n",
            "Time taken for training worker 3: 0:00:11.435510\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.598923472, Training Accuracy: 55.584\n",
            "0.9460360918249764\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.500598945, Training Accuracy: 57.568\n",
            "Time taken for training worker 4: 0:00:11.634582\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002442\n",
            "Global Model: Test Loss: 1.765640049, Test Accuracy: 53.390\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.530351518, Training Accuracy: 56.304\n",
            "0.953697994507055\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.478404862, Training Accuracy: 58.024\n",
            "Time taken for training worker 1: 0:00:12.271401\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.536176325, Training Accuracy: 56.696\n",
            "0.9519078549988361\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.456454285, Training Accuracy: 58.496\n",
            "Time taken for training worker 2: 0:00:12.497671\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.565626067, Training Accuracy: 55.712\n",
            "0.9523564929208661\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.500166372, Training Accuracy: 57.464\n",
            "Time taken for training worker 3: 0:00:12.222235\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.585504032, Training Accuracy: 55.904\n",
            "0.9612952049157136\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.509081867, Training Accuracy: 57.328\n",
            "Time taken for training worker 4: 0:00:13.954850\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003838\n",
            "Global Model: Test Loss: 1.781926353, Test Accuracy: 53.380\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.531992132, Training Accuracy: 57.040\n",
            "0.9709249124020379\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.463136206, Training Accuracy: 58.128\n",
            "Time taken for training worker 1: 0:00:12.563066\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.515773467, Training Accuracy: 57.256\n",
            "0.9718777359191867\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.467502809, Training Accuracy: 58.312\n",
            "Time taken for training worker 2: 0:00:12.069669\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.540670214, Training Accuracy: 56.136\n",
            "0.9364274719460217\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.462453759, Training Accuracy: 58.504\n",
            "Time taken for training worker 3: 0:00:11.539645\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.568242724, Training Accuracy: 56.200\n",
            "0.9604288563683673\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.494889998, Training Accuracy: 57.664\n",
            "Time taken for training worker 4: 0:00:11.471131\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002579\n",
            "Global Model: Test Loss: 1.763891260, Test Accuracy: 53.670\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.525116568, Training Accuracy: 57.240\n",
            "0.9695489693613287\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.452466648, Training Accuracy: 58.384\n",
            "Time taken for training worker 1: 0:00:13.036101\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.523084694, Training Accuracy: 56.960\n",
            "0.9498249989789278\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.434300624, Training Accuracy: 58.848\n",
            "Time taken for training worker 2: 0:00:13.715682\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.529402094, Training Accuracy: 56.728\n",
            "0.9582563615667024\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.457029304, Training Accuracy: 58.288\n",
            "Time taken for training worker 3: 0:00:13.101676\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.549947348, Training Accuracy: 56.248\n",
            "0.9500473918684614\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.478223843, Training Accuracy: 58.104\n",
            "Time taken for training worker 4: 0:00:13.213119\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003757\n",
            "Global Model: Test Loss: 1.765924438, Test Accuracy: 53.750\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.504377825, Training Accuracy: 57.824\n",
            "0.9708976066784516\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.437900707, Training Accuracy: 58.928\n",
            "Time taken for training worker 1: 0:00:11.546113\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.508774004, Training Accuracy: 57.552\n",
            "0.9634338814685152\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.436798351, Training Accuracy: 58.936\n",
            "Time taken for training worker 2: 0:00:11.338913\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.504200569, Training Accuracy: 57.024\n",
            "0.9371901318937184\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.430279671, Training Accuracy: 59.400\n",
            "Time taken for training worker 3: 0:00:12.873677\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.553057915, Training Accuracy: 56.520\n",
            "0.935597254141023\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.470098882, Training Accuracy: 58.936\n",
            "Time taken for training worker 4: 0:00:11.606010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003071\n",
            "Global Model: Test Loss: 1.767914966, Test Accuracy: 53.710\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.492252255, Training Accuracy: 57.840\n",
            "0.9592554520118525\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.426018681, Training Accuracy: 59.392\n",
            "Time taken for training worker 1: 0:00:13.319793\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.489438938, Training Accuracy: 57.904\n",
            "0.9551651275042568\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.432589878, Training Accuracy: 59.616\n",
            "Time taken for training worker 2: 0:00:12.115963\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.507246841, Training Accuracy: 57.280\n",
            "0.9555186825317523\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.421558094, Training Accuracy: 58.960\n",
            "Time taken for training worker 3: 0:00:12.000304\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.554782787, Training Accuracy: 56.568\n",
            "0.9444384307961372\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.461502971, Training Accuracy: 58.648\n",
            "Time taken for training worker 4: 0:00:11.726398\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002509\n",
            "Global Model: Test Loss: 1.762804807, Test Accuracy: 53.510\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.496897908, Training Accuracy: 57.480\n",
            "0.9268213832864991\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.387828079, Training Accuracy: 60.280\n",
            "Time taken for training worker 1: 0:00:12.088125\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.481099295, Training Accuracy: 58.192\n",
            "0.9570284120465765\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.403178284, Training Accuracy: 59.840\n",
            "Time taken for training worker 2: 0:00:12.783916\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.507897927, Training Accuracy: 57.768\n",
            "0.9476299425332186\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.412875024, Training Accuracy: 59.768\n",
            "Time taken for training worker 3: 0:00:13.410372\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.528839662, Training Accuracy: 57.544\n",
            "0.9690791497087571\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.438619376, Training Accuracy: 58.712\n",
            "Time taken for training worker 4: 0:00:12.837751\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002857\n",
            "Global Model: Test Loss: 1.780760245, Test Accuracy: 53.120\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.488033032, Training Accuracy: 57.808\n",
            "0.9197342456432763\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.388260270, Training Accuracy: 60.904\n",
            "Time taken for training worker 1: 0:00:11.477653\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.466026989, Training Accuracy: 58.312\n",
            "0.9585532229881332\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.384977249, Training Accuracy: 59.904\n",
            "Time taken for training worker 2: 0:00:11.347067\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.490469520, Training Accuracy: 57.600\n",
            "0.9281929505697583\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.382291956, Training Accuracy: 60.352\n",
            "Time taken for training worker 3: 0:00:11.916033\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.509387044, Training Accuracy: 57.576\n",
            "0.9667929135219829\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.447276747, Training Accuracy: 58.832\n",
            "Time taken for training worker 4: 0:00:13.052761\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002957\n",
            "Global Model: Test Loss: 1.786617001, Test Accuracy: 52.750\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.430335350, Training Accuracy: 59.584\n",
            "0.9780502473851075\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.385083422, Training Accuracy: 60.440\n",
            "Time taken for training worker 1: 0:00:11.593948\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.482868491, Training Accuracy: 57.872\n",
            "0.9234572201098821\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.375152885, Training Accuracy: 60.824\n",
            "Time taken for training worker 2: 0:00:12.596809\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.465671554, Training Accuracy: 58.112\n",
            "0.9473206187055291\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.391094249, Training Accuracy: 60.136\n",
            "Time taken for training worker 3: 0:00:11.335070\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.511720749, Training Accuracy: 57.632\n",
            "0.947302096470485\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.417487944, Training Accuracy: 59.640\n",
            "Time taken for training worker 4: 0:00:11.284183\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002933\n",
            "Global Model: Test Loss: 1.770979564, Test Accuracy: 53.310\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.431820429, Training Accuracy: 59.216\n",
            "0.9571547419385579\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.361172966, Training Accuracy: 60.888\n",
            "Time taken for training worker 1: 0:00:13.472663\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.443293537, Training Accuracy: 58.760\n",
            "0.9568269105794729\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.380740060, Training Accuracy: 60.432\n",
            "Time taken for training worker 2: 0:00:13.472275\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.461621233, Training Accuracy: 58.072\n",
            "0.954881251253096\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.405057647, Training Accuracy: 59.800\n",
            "Time taken for training worker 3: 0:00:13.084189\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.479332822, Training Accuracy: 57.480\n",
            "0.9496497799033016\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.429948700, Training Accuracy: 59.392\n",
            "Time taken for training worker 4: 0:00:13.128614\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002982\n",
            "Global Model: Test Loss: 1.791638460, Test Accuracy: 53.230\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.421125270, Training Accuracy: 59.288\n",
            "0.9539789462436526\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.365955307, Training Accuracy: 61.088\n",
            "Time taken for training worker 1: 0:00:11.550259\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.444596092, Training Accuracy: 59.008\n",
            "0.9582233528550196\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.372712402, Training Accuracy: 60.632\n",
            "Time taken for training worker 2: 0:00:11.507170\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.435207782, Training Accuracy: 59.072\n",
            "0.9556340333664652\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.372811743, Training Accuracy: 60.800\n",
            "Time taken for training worker 3: 0:00:11.610914\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.479282834, Training Accuracy: 58.440\n",
            "0.933026638795277\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.378578768, Training Accuracy: 61.040\n",
            "Time taken for training worker 4: 0:00:11.439593\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002794\n",
            "Global Model: Test Loss: 1.772876872, Test Accuracy: 53.480\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.417419566, Training Accuracy: 59.656\n",
            "0.9514573770611321\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.344342622, Training Accuracy: 61.568\n",
            "Time taken for training worker 1: 0:00:11.648619\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.432445833, Training Accuracy: 59.208\n",
            "0.957149033423727\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.356085614, Training Accuracy: 60.880\n",
            "Time taken for training worker 2: 0:00:11.489070\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.455640478, Training Accuracy: 58.464\n",
            "0.9401228883811711\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.377650032, Training Accuracy: 60.784\n",
            "Time taken for training worker 3: 0:00:11.880362\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.455974259, Training Accuracy: 59.008\n",
            "0.9618818066289454\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.390983514, Training Accuracy: 60.488\n",
            "Time taken for training worker 4: 0:00:12.331695\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002990\n",
            "Global Model: Test Loss: 1.765855085, Test Accuracy: 53.650\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.413123096, Training Accuracy: 59.656\n",
            "0.9534577342652983\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.329596799, Training Accuracy: 61.488\n",
            "Time taken for training worker 1: 0:00:12.008562\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.406460220, Training Accuracy: 59.800\n",
            "0.9481857828801\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.332400803, Training Accuracy: 61.848\n",
            "Time taken for training worker 2: 0:00:11.651358\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.429753373, Training Accuracy: 58.832\n",
            "0.9368722343193924\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.357160731, Training Accuracy: 61.296\n",
            "Time taken for training worker 3: 0:00:13.329942\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.451456803, Training Accuracy: 58.904\n",
            "0.9462000876201999\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.361441203, Training Accuracy: 61.000\n",
            "Time taken for training worker 4: 0:00:13.481362\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002988\n",
            "Global Model: Test Loss: 1.785212720, Test Accuracy: 53.700\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.389430836, Training Accuracy: 60.024\n",
            "0.9469886194062707\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.322541461, Training Accuracy: 62.128\n",
            "Time taken for training worker 1: 0:00:12.174707\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.384219362, Training Accuracy: 60.160\n",
            "0.963202049417171\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.323611483, Training Accuracy: 61.616\n",
            "Time taken for training worker 2: 0:00:11.885817\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.394586278, Training Accuracy: 60.600\n",
            "0.9796179898198158\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.333304526, Training Accuracy: 61.408\n",
            "Time taken for training worker 3: 0:00:11.928923\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.441388477, Training Accuracy: 58.976\n",
            "0.9474742009649867\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.358463228, Training Accuracy: 61.024\n",
            "Time taken for training worker 4: 0:00:12.884671\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002255\n",
            "Global Model: Test Loss: 1.766865400, Test Accuracy: 53.620\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.390846991, Training Accuracy: 60.232\n",
            "0.9626470212961067\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.320443629, Training Accuracy: 61.712\n",
            "Time taken for training worker 1: 0:00:11.945340\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.386980522, Training Accuracy: 60.032\n",
            "0.9567320800839264\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.327402171, Training Accuracy: 61.744\n",
            "Time taken for training worker 2: 0:00:11.555816\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.408432297, Training Accuracy: 59.624\n",
            "0.9464397855093699\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.309695859, Training Accuracy: 61.736\n",
            "Time taken for training worker 3: 0:00:12.089870\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.417934599, Training Accuracy: 59.824\n",
            "0.9478084427106866\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.339297493, Training Accuracy: 61.888\n",
            "Time taken for training worker 4: 0:00:12.041336\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003007\n",
            "Global Model: Test Loss: 1.750374335, Test Accuracy: 54.070\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.367082060, Training Accuracy: 61.272\n",
            "0.9929838517682359\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.322846269, Training Accuracy: 61.552\n",
            "Time taken for training worker 1: 0:00:12.803878\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.372700697, Training Accuracy: 60.520\n",
            "0.9588571812763222\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.301155756, Training Accuracy: 62.160\n",
            "Time taken for training worker 2: 0:00:12.537616\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.382907983, Training Accuracy: 60.176\n",
            "0.9718238812045127\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.330352072, Training Accuracy: 61.288\n",
            "Time taken for training worker 3: 0:00:13.242069\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.406214637, Training Accuracy: 59.976\n",
            "0.960287954358609\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.335697799, Training Accuracy: 61.544\n",
            "Time taken for training worker 4: 0:00:12.811015\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003065\n",
            "Global Model: Test Loss: 1.767045129, Test Accuracy: 54.170\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.361491405, Training Accuracy: 61.176\n",
            "0.9487481491585662\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.273133824, Training Accuracy: 63.248\n",
            "Time taken for training worker 1: 0:00:11.550031\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.375857838, Training Accuracy: 60.688\n",
            "0.9337065457611631\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.291847978, Training Accuracy: 63.360\n",
            "Time taken for training worker 2: 0:00:11.292274\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.382362548, Training Accuracy: 60.216\n",
            "0.9355479904408874\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.282470122, Training Accuracy: 62.792\n",
            "Time taken for training worker 3: 0:00:11.805334\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.414010475, Training Accuracy: 59.344\n",
            "0.9306517479427415\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.309693221, Training Accuracy: 62.080\n",
            "Time taken for training worker 4: 0:00:11.657439\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002610\n",
            "Global Model: Test Loss: 1.766991392, Test Accuracy: 54.180\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.348136984, Training Accuracy: 61.088\n",
            "0.9478963134672898\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.273923987, Training Accuracy: 63.192\n",
            "Time taken for training worker 1: 0:00:12.237272\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.340242753, Training Accuracy: 61.440\n",
            "0.9590749346565159\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.270470196, Training Accuracy: 63.096\n",
            "Time taken for training worker 2: 0:00:11.781260\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.378629293, Training Accuracy: 59.736\n",
            "0.9279485543865814\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.282866949, Training Accuracy: 62.600\n",
            "Time taken for training worker 3: 0:00:12.376574\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.392300009, Training Accuracy: 60.176\n",
            "0.9404163998076581\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.291662842, Training Accuracy: 62.552\n",
            "Time taken for training worker 4: 0:00:13.130194\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003194\n",
            "Global Model: Test Loss: 1.766894245, Test Accuracy: 53.960\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.325620623, Training Accuracy: 62.536\n",
            "0.982578780461013\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.258513850, Training Accuracy: 63.248\n",
            "Time taken for training worker 1: 0:00:11.423852\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.346045794, Training Accuracy: 61.544\n",
            "0.9536997867458015\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.266482914, Training Accuracy: 63.424\n",
            "Time taken for training worker 2: 0:00:11.437650\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.364807514, Training Accuracy: 60.328\n",
            "0.9498160132858107\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.271756934, Training Accuracy: 62.328\n",
            "Time taken for training worker 3: 0:00:11.826064\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.358270727, Training Accuracy: 60.976\n",
            "0.9733802490441154\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.293315731, Training Accuracy: 62.040\n",
            "Time taken for training worker 4: 0:00:11.686079\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002668\n",
            "Global Model: Test Loss: 1.764140189, Test Accuracy: 54.080\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.316702690, Training Accuracy: 62.088\n",
            "0.9540994078032725\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.257091280, Training Accuracy: 63.968\n",
            "Time taken for training worker 1: 0:00:12.013876\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.312903482, Training Accuracy: 62.488\n",
            "0.9759571244791007\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.263140640, Training Accuracy: 63.472\n",
            "Time taken for training worker 2: 0:00:11.308479\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.352440115, Training Accuracy: 61.312\n",
            "0.9444005920888788\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.245269032, Training Accuracy: 63.568\n",
            "Time taken for training worker 3: 0:00:11.722744\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.352614231, Training Accuracy: 61.744\n",
            "0.9499834744765796\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.262928156, Training Accuracy: 63.784\n",
            "Time taken for training worker 4: 0:00:11.623914\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002830\n",
            "Global Model: Test Loss: 1.784313197, Test Accuracy: 54.110\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.332610487, Training Accuracy: 61.608\n",
            "0.9404294030745463\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.241462416, Training Accuracy: 64.040\n",
            "Time taken for training worker 1: 0:00:12.187149\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.317304202, Training Accuracy: 61.808\n",
            "0.9624216866581417\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.255302674, Training Accuracy: 63.336\n",
            "Time taken for training worker 2: 0:00:11.595497\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.327752876, Training Accuracy: 61.600\n",
            "0.9492883923489837\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.245088836, Training Accuracy: 63.664\n",
            "Time taken for training worker 3: 0:00:12.903635\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.333225925, Training Accuracy: 61.832\n",
            "0.9708126897663236\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.269798161, Training Accuracy: 63.016\n",
            "Time taken for training worker 4: 0:00:12.241399\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003217\n",
            "Global Model: Test Loss: 1.782731768, Test Accuracy: 53.860\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.310464597, Training Accuracy: 62.224\n",
            "0.9572733632552449\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.247518305, Training Accuracy: 63.976\n",
            "Time taken for training worker 1: 0:00:11.501161\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.287587495, Training Accuracy: 63.144\n",
            "0.9716048779282278\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.233267429, Training Accuracy: 64.320\n",
            "Time taken for training worker 2: 0:00:11.644886\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.312040389, Training Accuracy: 62.064\n",
            "0.9639303330671186\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.267614640, Training Accuracy: 63.536\n",
            "Time taken for training worker 3: 0:00:12.607053\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.329171892, Training Accuracy: 61.616\n",
            "0.9385168195264505\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.251502624, Training Accuracy: 64.128\n",
            "Time taken for training worker 4: 0:00:11.870634\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002438\n",
            "Global Model: Test Loss: 1.764975730, Test Accuracy: 54.350\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.303098457, Training Accuracy: 62.416\n",
            "0.9484143780152945\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.223600930, Training Accuracy: 64.544\n",
            "Time taken for training worker 1: 0:00:11.664291\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.292416933, Training Accuracy: 62.704\n",
            "0.9733356956556335\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.222580018, Training Accuracy: 63.800\n",
            "Time taken for training worker 2: 0:00:11.927863\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.319714799, Training Accuracy: 61.848\n",
            "0.9552764916199249\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.244109813, Training Accuracy: 63.672\n",
            "Time taken for training worker 3: 0:00:11.583123\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.320373084, Training Accuracy: 62.184\n",
            "0.9526330293532523\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.229084509, Training Accuracy: 64.128\n",
            "Time taken for training worker 4: 0:00:11.569722\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002550\n",
            "Global Model: Test Loss: 1.775553558, Test Accuracy: 54.060\n",
            "**************************************************\n",
            "nan\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.288201971, Training Accuracy: 63.312\n",
            "0.9737806965008604\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.214991914, Training Accuracy: 64.400\n",
            "Time taken for training worker 1: 0:00:11.557596\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.287438757, Training Accuracy: 62.792\n",
            "0.9364456141483697\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.194451280, Training Accuracy: 65.440\n",
            "Time taken for training worker 2: 0:00:11.552640\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 3, [epoch: 01]: Training Loss: 1.281569612, Training Accuracy: 63.232\n",
            "0.9745135365802114\n",
            "Worker 3, [epoch: 02]: Training Loss: 1.209895489, Training Accuracy: 64.288\n",
            "Time taken for training worker 3: 0:00:11.639190\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.307201854, Training Accuracy: 63.008\n",
            "0.9773086537366367\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.241632876, Training Accuracy: 63.944\n",
            "Time taken for training worker 4: 0:00:11.466690\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002711\n",
            "Global Model: Test Loss: 1.791749354, Test Accuracy: 53.980\n",
            "**************************************************\n",
            "nan\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.270468961, Training Accuracy: 63.592\n",
            "0.9611968351001067\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.193735655, Training Accuracy: 65.216\n",
            "Time taken for training worker 2: 0:00:11.887660\n",
            "--------------------------------------------------\n",
            "nan\n",
            "Worker 4, [epoch: 01]: Training Loss: 1.312506081, Training Accuracy: 62.272\n",
            "0.9440985835825021\n",
            "Worker 4, [epoch: 02]: Training Loss: 1.220145905, Training Accuracy: 64.576\n",
            "Time taken for training worker 4: 0:00:11.371155\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002675\n",
            "Global Model: Test Loss: 1.777332788, Test Accuracy: 54.110\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for Confidence Interval: 1:01:46.467340\n",
            "//////////////////////////////////////////////////\n"
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [4]\n",
        "num_epochs = 150\n",
        "ci_acc_threshold = 0.9\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "    shard_loaders = data.iid_shards(num_shards=k)\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}')\n",
        "    print('='*50)\n",
        "    ConfidenceInetrvalApproach(shard_loaders, loss_fn, k, lr, wd, initial_state_dict, num_epochs, ci_acc_threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Our Approach 3: Heuristic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class HeuristicOptimizer(Optimizer):\n",
        "    def __init__(self, global_model, lr=0.01):\n",
        "        self.global_model = global_model\n",
        "        self.lr = lr\n",
        "        params = list(global_model.parameters())\n",
        "        super(HeuristicOptimizer, self).__init__(params, {'lr': lr})\n",
        "\n",
        "    def step(self, local_models, train_accuracies, local_optimizers):\n",
        "        # Get global model parameters\n",
        "        global_params = list(self.global_model.parameters())\n",
        "        \n",
        "        # Initialize deltas for each parameter, same size as global parameters\n",
        "        deltas = [torch.zeros_like(param) for param in global_params]\n",
        "        \n",
        "        # TODO: Mitoonim begim agar tafavot az ye meghdari bishtar bood average begirim\n",
        "        \n",
        "        # Sum up differences between global model and local models\n",
        "        for i, local_model in enumerate(local_models):\n",
        "            if train_accuracies[i] == max(train_accuracies):\n",
        "                local_params = list(local_models[i].parameters())\n",
        "                for j, param in enumerate(local_params):\n",
        "                    deltas[j] += (global_params[j] - param)/local_optimizers[i].param_groups[0]['lr']\n",
        "        # Average the delta over the number of local models\n",
        "        # for i, delta in enumerate(deltas):\n",
        "        #     deltas[i] /= self.lr\n",
        "        \n",
        "        # Update global model parameters\n",
        "        with torch.no_grad():\n",
        "            for i, param in enumerate(global_params):\n",
        "                param.copy_(param - self.lr * deltas[i])\n",
        "        \n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def local_SGD(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs):\n",
        "  total_start_time = time.time()\n",
        "\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = []\n",
        "  for i, model in enumerate(local_models):\n",
        "    new_lr = lr * (1.5 ** i)\n",
        "    local_optimizers.append(torch.optim.SGD(model.parameters(), lr=new_lr, momentum=0.9, weight_decay=wd))\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "  \n",
        "  global_optimizer = HeuristicOptimizer(global_model, lr)\n",
        "  \n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    train_accuracies = []\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for loca_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      print(f\"lr: {local_optimizers[worker].param_groups[0]['lr']}\")\n",
        "      train_accuracies.append(train_accuracy) # Add last train accuracy to the list\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    global_optimizer.step(local_models, train_accuracies, local_optimizers)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # for local_optimizer in local_optimizers:\n",
        "    #     # print(f\"{local_optimizer.param_groups[0]['lr']}-->{global_optimizer.param_groups[0]['lr']}\") \n",
        "    #     local_optimizer.param_groups[0]['lr'] = global_optimizer.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "    for local_model in local_models:\n",
        "      local_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Local Step {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "# TODO: Schedule ro check konim \n",
        "\n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for local_SGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:1\n",
            "==================================================\n",
            "Worker 1, [01/01]: Training Loss: 4.589787848, Training Accuracy: 1.728\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.746022\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 4.557270807, Training Accuracy: 2.048\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.386824\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 4.485447854, Training Accuracy: 2.952\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.986881\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 4.446730149, Training Accuracy: 2.936\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.970400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002971\n",
            "Local Step 01: Test Loss: 4.500772725, Test Accuracy: 4.680\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 4.231639716, Training Accuracy: 5.424\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.608389\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 4.205415051, Training Accuracy: 5.248\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.388969\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 4.180451768, Training Accuracy: 5.808\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.773666\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 4.211756984, Training Accuracy: 5.256\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.261099\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002191\n",
            "Local Step 02: Test Loss: 4.292123005, Test Accuracy: 7.680\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 4.058339204, Training Accuracy: 7.544\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.517742\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 4.052988528, Training Accuracy: 7.800\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.280649\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 4.018794595, Training Accuracy: 7.560\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.547392\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 4.082402378, Training Accuracy: 6.944\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.664865\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003276\n",
            "Local Step 03: Test Loss: 3.979197036, Test Accuracy: 10.250\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.917438255, Training Accuracy: 9.256\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.158194\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.907348126, Training Accuracy: 9.440\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.185261\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.900125198, Training Accuracy: 9.552\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.774833\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.973917993, Training Accuracy: 8.632\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.050913\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002499\n",
            "Local Step 04: Test Loss: 3.844917720, Test Accuracy: 11.580\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.813078048, Training Accuracy: 10.584\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.883105\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.809908219, Training Accuracy: 10.712\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.153891\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.808217929, Training Accuracy: 10.728\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.789716\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.898001451, Training Accuracy: 9.544\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.585254\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002126\n",
            "Local Step 05: Test Loss: 3.744461834, Test Accuracy: 14.100\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.723714859, Training Accuracy: 12.392\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.986945\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.738407272, Training Accuracy: 12.600\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.120610\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.708212250, Training Accuracy: 12.528\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.005786\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.826257962, Training Accuracy: 10.776\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.784329\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002318\n",
            "Local Step 06: Test Loss: 3.621702698, Test Accuracy: 15.710\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.636695149, Training Accuracy: 14.176\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.916577\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.632109464, Training Accuracy: 13.640\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.324500\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.634877625, Training Accuracy: 13.336\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.984237\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.767925889, Training Accuracy: 11.856\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.167728\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003211\n",
            "Local Step 07: Test Loss: 3.529420834, Test Accuracy: 16.780\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.516599807, Training Accuracy: 16.008\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.242235\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.558906678, Training Accuracy: 14.616\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.362484\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.583551285, Training Accuracy: 14.872\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.057519\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.734859274, Training Accuracy: 12.264\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.864557\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003812\n",
            "Local Step 08: Test Loss: 3.466818296, Test Accuracy: 17.780\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.414356246, Training Accuracy: 17.848\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.093412\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.474345986, Training Accuracy: 16.544\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.347229\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.520872864, Training Accuracy: 15.432\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.375275\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.655085077, Training Accuracy: 13.408\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.763312\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002529\n",
            "Local Step 09: Test Loss: 3.339267907, Test Accuracy: 19.530\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.313855383, Training Accuracy: 19.800\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:08.052443\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.422182255, Training Accuracy: 17.184\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.838917\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.477068553, Training Accuracy: 16.768\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.712236\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.607006564, Training Accuracy: 14.776\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.336923\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002256\n",
            "Local Step 10: Test Loss: 3.271532504, Test Accuracy: 20.530\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.223884398, Training Accuracy: 21.264\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.876837\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.358064015, Training Accuracy: 18.600\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.418422\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.406104517, Training Accuracy: 17.552\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.161748\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.569587559, Training Accuracy: 15.032\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.265262\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002496\n",
            "Local Step 11: Test Loss: 3.165443963, Test Accuracy: 22.380\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.140930611, Training Accuracy: 21.888\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.786521\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.321831113, Training Accuracy: 19.168\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.927488\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.377558065, Training Accuracy: 18.128\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.217449\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.538656684, Training Accuracy: 15.560\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.630215\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002669\n",
            "Local Step 12: Test Loss: 3.132481868, Test Accuracy: 23.080\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.090491065, Training Accuracy: 23.264\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.578330\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.264150854, Training Accuracy: 19.792\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.175619\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.342807123, Training Accuracy: 18.704\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.386190\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.486739414, Training Accuracy: 16.544\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.920359\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002079\n",
            "Local Step 13: Test Loss: 3.032923712, Test Accuracy: 24.840\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.026100083, Training Accuracy: 24.352\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.047773\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.233231743, Training Accuracy: 20.856\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.801449\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.290221830, Training Accuracy: 19.808\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.298313\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.463098168, Training Accuracy: 16.696\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.618788\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002160\n",
            "Local Step 14: Test Loss: 3.105797210, Test Accuracy: 23.590\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.963631872, Training Accuracy: 25.568\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.164110\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.199981086, Training Accuracy: 21.560\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.321585\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.271719210, Training Accuracy: 20.168\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.033863\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.432939785, Training Accuracy: 16.960\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.059986\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002323\n",
            "Local Step 15: Test Loss: 3.009709704, Test Accuracy: 25.540\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.889958806, Training Accuracy: 27.224\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.937228\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.165646088, Training Accuracy: 22.712\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.925502\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.218345855, Training Accuracy: 21.136\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.661133\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.388219659, Training Accuracy: 18.024\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.570728\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002358\n",
            "Local Step 16: Test Loss: 2.947192550, Test Accuracy: 27.260\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.827665202, Training Accuracy: 28.240\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.396236\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.111536665, Training Accuracy: 23.600\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.404536\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.178346033, Training Accuracy: 21.808\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.510904\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.339770969, Training Accuracy: 18.624\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.318248\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002293\n",
            "Local Step 17: Test Loss: 2.926472801, Test Accuracy: 27.470\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.763013174, Training Accuracy: 29.504\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.605952\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.092464403, Training Accuracy: 23.688\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.186312\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.146464260, Training Accuracy: 22.584\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.888457\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.346745029, Training Accuracy: 18.992\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.462689\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004081\n",
            "Local Step 18: Test Loss: 2.907899468, Test Accuracy: 27.120\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.713057738, Training Accuracy: 30.544\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.729917\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.049456980, Training Accuracy: 24.504\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.367651\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.151050698, Training Accuracy: 22.808\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.043068\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.322010310, Training Accuracy: 19.976\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.973361\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002339\n",
            "Local Step 19: Test Loss: 2.914370303, Test Accuracy: 27.590\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.661161530, Training Accuracy: 31.152\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.731905\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.027097389, Training Accuracy: 24.600\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.477779\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.104282294, Training Accuracy: 23.112\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.245046\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.292920038, Training Accuracy: 19.584\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.417168\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002415\n",
            "Local Step 20: Test Loss: 2.761305142, Test Accuracy: 30.500\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.598650473, Training Accuracy: 32.656\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.309558\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.995155026, Training Accuracy: 25.536\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.129035\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.092096859, Training Accuracy: 23.696\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.974579\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.268705148, Training Accuracy: 20.848\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.856725\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002258\n",
            "Local Step 21: Test Loss: 2.822268582, Test Accuracy: 30.000\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.550101486, Training Accuracy: 33.144\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.929680\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.967930259, Training Accuracy: 26.376\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.648053\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.045848395, Training Accuracy: 24.488\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.956805\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.272339876, Training Accuracy: 20.888\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.687631\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003201\n",
            "Local Step 22: Test Loss: 2.756644536, Test Accuracy: 30.490\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.485342156, Training Accuracy: 34.472\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.116915\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.959221054, Training Accuracy: 26.408\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.147470\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.061622957, Training Accuracy: 24.208\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.033657\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.232111723, Training Accuracy: 21.352\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.949024\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002252\n",
            "Local Step 23: Test Loss: 2.748973086, Test Accuracy: 30.610\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.430463351, Training Accuracy: 35.512\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.298309\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.936022600, Training Accuracy: 26.664\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.950089\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.022082038, Training Accuracy: 24.760\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.462507\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.213971615, Training Accuracy: 21.880\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.623963\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002595\n",
            "Local Step 24: Test Loss: 2.746196484, Test Accuracy: 31.630\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.416436481, Training Accuracy: 36.264\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.806817\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.911082866, Training Accuracy: 27.432\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.769124\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.014251209, Training Accuracy: 25.328\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.912035\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.237113258, Training Accuracy: 21.024\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.418314\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002254\n",
            "Local Step 25: Test Loss: 2.727613596, Test Accuracy: 31.400\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.366622279, Training Accuracy: 36.768\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.818800\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.910054226, Training Accuracy: 26.848\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.964041\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.986882407, Training Accuracy: 26.080\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.862178\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.170190454, Training Accuracy: 22.480\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.718707\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002334\n",
            "Local Step 26: Test Loss: 2.749958867, Test Accuracy: 31.150\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.304162094, Training Accuracy: 38.224\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.422384\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.890694815, Training Accuracy: 27.368\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.765903\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.998954008, Training Accuracy: 25.536\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.483752\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.171583091, Training Accuracy: 23.280\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.827540\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002397\n",
            "Local Step 27: Test Loss: 2.690929720, Test Accuracy: 33.140\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.257173156, Training Accuracy: 39.520\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.561779\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.881015117, Training Accuracy: 27.888\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.499114\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.967941811, Training Accuracy: 26.400\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.277443\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.175822906, Training Accuracy: 22.000\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.356079\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002262\n",
            "Local Step 28: Test Loss: 2.717134035, Test Accuracy: 32.550\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.224158562, Training Accuracy: 40.256\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.361974\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.875143439, Training Accuracy: 28.120\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.614228\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.997786877, Training Accuracy: 25.592\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.957266\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.167646117, Training Accuracy: 22.672\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.710475\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002362\n",
            "Local Step 29: Test Loss: 2.707655756, Test Accuracy: 33.510\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.169331031, Training Accuracy: 41.368\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.822488\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.863829281, Training Accuracy: 28.392\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.003799\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.975073229, Training Accuracy: 26.040\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.601039\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.160420987, Training Accuracy: 22.944\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.124087\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002428\n",
            "Local Step 30: Test Loss: 2.676212556, Test Accuracy: 33.220\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.132951577, Training Accuracy: 42.520\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.615819\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.854434982, Training Accuracy: 28.896\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.760223\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.963350222, Training Accuracy: 26.688\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.941346\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.146738274, Training Accuracy: 22.872\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.056648\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003506\n",
            "Local Step 31: Test Loss: 2.682681625, Test Accuracy: 33.330\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.089273798, Training Accuracy: 43.024\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.440252\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.843350526, Training Accuracy: 29.112\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.773524\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.935489844, Training Accuracy: 26.288\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.954905\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.137473747, Training Accuracy: 23.320\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.997612\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002687\n",
            "Local Step 32: Test Loss: 2.648199840, Test Accuracy: 34.020\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.048058316, Training Accuracy: 44.024\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.875091\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.881912042, Training Accuracy: 28.440\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.688161\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.941761156, Training Accuracy: 26.712\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:08.463436\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.122440618, Training Accuracy: 23.232\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:09.052732\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003025\n",
            "Local Step 33: Test Loss: 2.700783102, Test Accuracy: 33.080\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.019318632, Training Accuracy: 44.576\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.242541\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.835129299, Training Accuracy: 29.208\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:08.505503\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.931401778, Training Accuracy: 27.496\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:09.251709\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.114784081, Training Accuracy: 23.864\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.761153\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002218\n",
            "Local Step 34: Test Loss: 2.693163869, Test Accuracy: 34.520\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.986193331, Training Accuracy: 44.960\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.645277\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.841082234, Training Accuracy: 29.136\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.279223\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.940590856, Training Accuracy: 27.640\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:08.485715\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.147334571, Training Accuracy: 23.424\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:08.697841\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002714\n",
            "Local Step 35: Test Loss: 2.634686085, Test Accuracy: 34.870\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.946631952, Training Accuracy: 45.928\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:08.985996\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.827707501, Training Accuracy: 28.968\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.815355\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.930380666, Training Accuracy: 27.096\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.250167\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.156379141, Training Accuracy: 23.416\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.122502\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002336\n",
            "Local Step 36: Test Loss: 2.714101551, Test Accuracy: 34.510\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.919444442, Training Accuracy: 46.976\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.759148\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.839635831, Training Accuracy: 29.112\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.296715\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.938252939, Training Accuracy: 27.112\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.665770\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.120045621, Training Accuracy: 23.896\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.734671\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003232\n",
            "Local Step 37: Test Loss: 2.668068606, Test Accuracy: 34.780\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.866123778, Training Accuracy: 48.376\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.813898\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.820039013, Training Accuracy: 29.592\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.245589\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.929482877, Training Accuracy: 27.792\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.746448\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.109626063, Training Accuracy: 24.472\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.790436\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002068\n",
            "Local Step 38: Test Loss: 2.669131436, Test Accuracy: 34.760\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.831814990, Training Accuracy: 49.200\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.312383\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.836216088, Training Accuracy: 29.488\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.863118\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.940080343, Training Accuracy: 27.512\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.965067\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.125384885, Training Accuracy: 23.744\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.373395\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002299\n",
            "Local Step 39: Test Loss: 2.743684317, Test Accuracy: 35.190\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.787694555, Training Accuracy: 50.368\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.631177\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.830699005, Training Accuracy: 28.920\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.520470\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.904229035, Training Accuracy: 28.256\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.774683\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.139979172, Training Accuracy: 23.624\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.751288\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002233\n",
            "Local Step 40: Test Loss: 2.745167157, Test Accuracy: 34.560\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.758455088, Training Accuracy: 50.696\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.703510\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.818759046, Training Accuracy: 30.056\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.111504\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.943791512, Training Accuracy: 27.376\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.531379\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.126291225, Training Accuracy: 23.632\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.854797\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003061\n",
            "Local Step 41: Test Loss: 2.778776377, Test Accuracy: 35.050\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.718394621, Training Accuracy: 51.952\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.884467\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.824094265, Training Accuracy: 29.904\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.844005\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.925922260, Training Accuracy: 27.880\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.790567\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.164432180, Training Accuracy: 22.920\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.398784\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002306\n",
            "Local Step 42: Test Loss: 2.783299914, Test Accuracy: 34.820\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.680137269, Training Accuracy: 52.608\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.104930\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.820746103, Training Accuracy: 29.832\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.675768\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.945505109, Training Accuracy: 26.992\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.554514\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.131405282, Training Accuracy: 24.376\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.641013\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002150\n",
            "Local Step 43: Test Loss: 2.896891629, Test Accuracy: 34.020\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.663007209, Training Accuracy: 52.952\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.750430\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.868813396, Training Accuracy: 29.728\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.695699\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.900532083, Training Accuracy: 28.128\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.102484\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.163915270, Training Accuracy: 23.728\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.129615\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002247\n",
            "Local Step 44: Test Loss: 2.804090001, Test Accuracy: 35.000\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.642900128, Training Accuracy: 53.392\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.029715\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.827060130, Training Accuracy: 30.208\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.017884\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.936931757, Training Accuracy: 27.768\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.167114\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.124433974, Training Accuracy: 24.048\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.800690\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002569\n",
            "Local Step 45: Test Loss: 2.818094415, Test Accuracy: 34.000\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.627391837, Training Accuracy: 54.176\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.289573\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.856641044, Training Accuracy: 29.856\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.033521\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.917290190, Training Accuracy: 28.168\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.738592\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.133015258, Training Accuracy: 24.576\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.135280\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002835\n",
            "Local Step 46: Test Loss: 2.783011853, Test Accuracy: 35.170\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.568574129, Training Accuracy: 54.816\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.484772\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.838493436, Training Accuracy: 29.416\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.613650\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.946374517, Training Accuracy: 27.592\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.025146\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.144721536, Training Accuracy: 23.904\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.381895\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002844\n",
            "Local Step 47: Test Loss: 2.875326401, Test Accuracy: 34.480\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.537972215, Training Accuracy: 56.160\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:08.379957\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.843739864, Training Accuracy: 29.952\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.058849\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.933190994, Training Accuracy: 28.224\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.067560\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.137811910, Training Accuracy: 24.224\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.684547\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002321\n",
            "Local Step 48: Test Loss: 2.932630954, Test Accuracy: 34.750\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.531879760, Training Accuracy: 55.824\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.784616\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.845875801, Training Accuracy: 30.040\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.439859\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.928408792, Training Accuracy: 28.400\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.854377\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.141863851, Training Accuracy: 24.184\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.002457\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002339\n",
            "Local Step 49: Test Loss: 2.916648935, Test Accuracy: 34.670\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.469292940, Training Accuracy: 57.392\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.778953\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.844461802, Training Accuracy: 29.448\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.831759\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.940814045, Training Accuracy: 27.744\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.753077\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.163285108, Training Accuracy: 23.992\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.899868\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002983\n",
            "Local Step 50: Test Loss: 2.917281653, Test Accuracy: 35.270\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.477859631, Training Accuracy: 57.000\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.902765\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.847389185, Training Accuracy: 29.728\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.945654\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.934633962, Training Accuracy: 28.520\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.796714\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.175899138, Training Accuracy: 23.504\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.012503\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002422\n",
            "Local Step 51: Test Loss: 2.920089195, Test Accuracy: 34.370\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.464753015, Training Accuracy: 57.392\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.949396\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.837016471, Training Accuracy: 29.952\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.972944\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.960583749, Training Accuracy: 27.904\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.811712\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.194385424, Training Accuracy: 23.448\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.897311\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002565\n",
            "Local Step 52: Test Loss: 2.894671835, Test Accuracy: 35.070\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.397185029, Training Accuracy: 59.824\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.652364\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.847827754, Training Accuracy: 29.688\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.889500\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.955624313, Training Accuracy: 27.896\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.715590\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.189883129, Training Accuracy: 23.288\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.931712\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002401\n",
            "Local Step 53: Test Loss: 2.986378430, Test Accuracy: 34.600\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.373409676, Training Accuracy: 60.152\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.781035\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.854562826, Training Accuracy: 29.800\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.980708\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.979599273, Training Accuracy: 27.704\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.722192\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.197973063, Training Accuracy: 23.248\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.840209\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002602\n",
            "Local Step 54: Test Loss: 2.942401577, Test Accuracy: 35.230\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.353966992, Training Accuracy: 60.656\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.919252\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.846919981, Training Accuracy: 29.832\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.778332\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.956689423, Training Accuracy: 27.952\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.741957\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.177274768, Training Accuracy: 23.912\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.634712\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002211\n",
            "Local Step 55: Test Loss: 3.011749849, Test Accuracy: 34.430\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.314813789, Training Accuracy: 61.312\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.813776\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.869151373, Training Accuracy: 29.576\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.634163\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.960105981, Training Accuracy: 28.144\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.686748\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.198249210, Training Accuracy: 23.736\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.729849\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002696\n",
            "Local Step 56: Test Loss: 3.110127827, Test Accuracy: 35.110\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.304395282, Training Accuracy: 62.208\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.560491\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.859132522, Training Accuracy: 30.352\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.702371\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.985762425, Training Accuracy: 26.976\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.678982\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.203278970, Training Accuracy: 23.496\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.135491\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002472\n",
            "Local Step 57: Test Loss: 3.050472316, Test Accuracy: 35.470\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.279940829, Training Accuracy: 62.696\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.676528\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.884408650, Training Accuracy: 29.112\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:08.139177\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.980416417, Training Accuracy: 27.592\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:08.576932\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.187659245, Training Accuracy: 23.824\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:08.362328\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003069\n",
            "Local Step 58: Test Loss: 3.064930044, Test Accuracy: 34.930\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.271239208, Training Accuracy: 62.792\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:08.551237\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.904054302, Training Accuracy: 29.136\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.914376\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.997421367, Training Accuracy: 27.920\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.900016\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.201709138, Training Accuracy: 23.616\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:08.041623\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002430\n",
            "Local Step 59: Test Loss: 3.108161202, Test Accuracy: 35.300\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.227700301, Training Accuracy: 63.912\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.858245\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.875602163, Training Accuracy: 29.824\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.899724\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.991640418, Training Accuracy: 27.832\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:08.565481\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.228053761, Training Accuracy: 23.296\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.955524\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003579\n",
            "Local Step 60: Test Loss: 3.130341480, Test Accuracy: 34.800\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.216399205, Training Accuracy: 64.136\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.932597\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.886568018, Training Accuracy: 29.672\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.931812\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.002176522, Training Accuracy: 27.456\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:08.370763\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.223487751, Training Accuracy: 23.128\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.872351\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002590\n",
            "Local Step 61: Test Loss: 3.190220272, Test Accuracy: 34.200\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.200201267, Training Accuracy: 64.336\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:08.010351\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.875098582, Training Accuracy: 29.936\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:08.020296\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.995968217, Training Accuracy: 27.440\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.969084\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.222627547, Training Accuracy: 23.320\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:08.414571\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002640\n",
            "Local Step 62: Test Loss: 3.185906480, Test Accuracy: 35.400\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.181374572, Training Accuracy: 65.288\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:08.107658\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.885280689, Training Accuracy: 29.688\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.835861\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.005184716, Training Accuracy: 27.512\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.945132\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.224711530, Training Accuracy: 23.544\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.882295\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002888\n",
            "Local Step 63: Test Loss: 3.269294749, Test Accuracy: 34.630\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.167351044, Training Accuracy: 65.968\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.994411\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.918037974, Training Accuracy: 29.088\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:08.085315\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.020654929, Training Accuracy: 27.512\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.919675\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.226510381, Training Accuracy: 23.192\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:08.071681\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003022\n",
            "Local Step 64: Test Loss: 3.202984608, Test Accuracy: 35.200\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.121228214, Training Accuracy: 67.040\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.964146\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.897665750, Training Accuracy: 29.240\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.944362\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.014328032, Training Accuracy: 27.088\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.737966\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.243594812, Training Accuracy: 23.088\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.891946\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002537\n",
            "Local Step 65: Test Loss: 3.300824560, Test Accuracy: 35.000\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.122373345, Training Accuracy: 66.584\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.982037\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.938263701, Training Accuracy: 28.832\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.892864\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.993218985, Training Accuracy: 27.552\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:08.251805\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.284777511, Training Accuracy: 22.384\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.782230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002892\n",
            "Local Step 66: Test Loss: 3.229022951, Test Accuracy: 35.130\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.100709652, Training Accuracy: 67.744\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:08.028525\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.895543867, Training Accuracy: 29.968\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:08.496121\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.997135547, Training Accuracy: 27.672\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:08.097186\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.220194230, Training Accuracy: 23.864\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.808527\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003633\n",
            "Local Step 67: Test Loss: 3.230751467, Test Accuracy: 35.000\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.085275405, Training Accuracy: 67.816\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:08.487554\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.884014124, Training Accuracy: 30.048\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:08.219857\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.005283563, Training Accuracy: 27.672\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.976244\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.277160879, Training Accuracy: 22.360\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:08.054336\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003457\n",
            "Local Step 68: Test Loss: 3.312470823, Test Accuracy: 35.400\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.066900809, Training Accuracy: 68.240\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.945405\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.935698850, Training Accuracy: 29.408\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.913030\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.043256773, Training Accuracy: 26.872\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.638712\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.224550582, Training Accuracy: 23.032\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.853071\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002612\n",
            "Local Step 69: Test Loss: 3.330204305, Test Accuracy: 35.270\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.050967051, Training Accuracy: 68.552\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.857336\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.934148726, Training Accuracy: 29.232\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.900504\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.033816104, Training Accuracy: 27.760\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.970999\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.263718161, Training Accuracy: 23.048\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:08.018910\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002921\n",
            "Local Step 70: Test Loss: 3.309276437, Test Accuracy: 35.140\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.011538475, Training Accuracy: 69.392\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.192000\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.934234451, Training Accuracy: 28.960\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.163019\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.031115411, Training Accuracy: 27.152\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:07.748343\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.241960193, Training Accuracy: 22.864\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.987055\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002907\n",
            "Local Step 71: Test Loss: 3.410999492, Test Accuracy: 34.590\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.998351570, Training Accuracy: 69.736\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.529249\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.947738075, Training Accuracy: 28.896\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.302556\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.057410445, Training Accuracy: 26.920\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:08.198690\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.264395294, Training Accuracy: 23.192\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.174045\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002576\n",
            "Local Step 72: Test Loss: 3.349426330, Test Accuracy: 34.830\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.995938185, Training Accuracy: 70.216\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.272534\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.913419036, Training Accuracy: 30.048\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:08.112953\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[49], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Workers:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Number of Local Steps:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mlocal_SGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[47], line 31\u001b[0m, in \u001b[0;36mlocal_SGD\u001b[0;34m(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m train_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loca_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(j):\n\u001b[0;32m---> 31\u001b[0m   train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mis_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorker \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworker\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloca_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.9f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_optimizers[worker]\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[12], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn, accumulation_steps, device, is_wandb)\u001b[0m\n\u001b[1;32m      5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Initialize gradients to zero at the start of each epoch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     10\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:471\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:664\u001b[0m, in \u001b[0;36mRandomCrop.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m    img (PIL Image or Tensor): Image to be cropped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    PIL Image or Tensor: Cropped image.\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 664\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m width, height \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mget_image_size(img)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# pad the width if needed\u001b[39;00m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:485\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    483\u001b[0m     _log_api_usage_once(pad)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mpad(img, padding\u001b[38;5;241m=\u001b[39mpadding, fill\u001b[38;5;241m=\u001b[39mfill, padding_mode\u001b[38;5;241m=\u001b[39mpadding_mode)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py:201\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# RGB image\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 201\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_top\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_bottom\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_right\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Grayscale image\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/numpy/lib/arraypad.py:866\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m         roi \u001b[38;5;241m=\u001b[39m _view_roi(padded, original_area_slice, axis)\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m left_index \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m right_index \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    863\u001b[0m             \u001b[38;5;66;03m# Iteratively pad until dimension is filled with reflected\u001b[39;00m\n\u001b[1;32m    864\u001b[0m             \u001b[38;5;66;03m# values. This is necessary if the pad area is larger than\u001b[39;00m\n\u001b[1;32m    865\u001b[0m             \u001b[38;5;66;03m# the length of the original values in the current dimension.\u001b[39;00m\n\u001b[0;32m--> 866\u001b[0m             left_index, right_index \u001b[38;5;241m=\u001b[39m \u001b[43m_set_reflect_both\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[43mroi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_edge\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrap\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, (left_index, right_index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axes, pad_width):\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/numpy/lib/arraypad.py:373\u001b[0m, in \u001b[0;36m_set_reflect_both\u001b[0;34m(padded, axis, width_pair, method, include_edge)\u001b[0m\n\u001b[1;32m    371\u001b[0m start \u001b[38;5;241m=\u001b[39m padded\u001b[38;5;241m.\u001b[39mshape[axis] \u001b[38;5;241m-\u001b[39m right_pad\n\u001b[1;32m    372\u001b[0m stop \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m chunk_length\n\u001b[0;32m--> 373\u001b[0m pad_area \u001b[38;5;241m=\u001b[39m \u001b[43m_slice_at_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m padded[pad_area] \u001b[38;5;241m=\u001b[39m right_chunk\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# Adjust pointer to right edge for next iteration\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 5e-03\n",
        "wd = 1e-03\n",
        "K = [4, 8]\n",
        "J = [1, 4, 8, 16, 32, 64] # TODO: 1 ro hazf konim\n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    local_SGD(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Discard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LocalAdaScale (Choice of LR in LocalSGD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def synchronize(models):\n",
        "#   for params in zip(*[model.parameters() for model in models]):\n",
        "#     param_avg = torch.mean(torch.stack([param.data for param in params]), dim=0)\n",
        "#     for param in params:\n",
        "#       param.data = param_avg\n",
        "  \n",
        "#   return models[0]\n",
        "\n",
        "# def synchronize(global_model, local_models):\n",
        "#         # Initialize a state dict with zeros, same shape as the model parameters\n",
        "#         delta = {key: torch.zeros_like(value) for key, value in local_models[0].state_dict().items()}\n",
        "      \n",
        "#         # Sum up all the model parameters\n",
        "#         for local_model in local_models:\n",
        "#             for key, value in local_model.state_dict().items():\n",
        "#                 delta[key] += (global_model.state_dict()[key] - value)\n",
        "      \n",
        "#         # Divide each parameter by the number of models to get the average\n",
        "#         for key in delta:\n",
        "#             delta[key] /= len(local_models)\n",
        "      \n",
        "#         new_weights = {}\n",
        "#         for key, value in global_model.state_dict().items():\n",
        "#             new_weights[key] = value -  delta [key] # TODO: az TA beporsim ke learning rate ro chetor hesab konim.\n",
        "      \n",
        "#         global_model.load_state_dict(new_weights)\n",
        "#         return global_model\n",
        "\n",
        "def average_models(local_models):\n",
        "    \"\"\"Calculate the average model from a list of local models.\"\"\"\n",
        "    num_models = len(local_models)\n",
        "    \n",
        "    # Initialize the averaged model as a copy of the first local model\n",
        "    avg_model = local_models[0]\n",
        "    \n",
        "    # Zero the parameters of the avg_model to start accumulating\n",
        "    for param in avg_model.parameters():\n",
        "        param.data.zero_()\n",
        "    \n",
        "    # Accumulate the parameters from all local models\n",
        "    for local_model in local_models:\n",
        "        for avg_param, local_param in zip(avg_model.parameters(), local_model.parameters()):\n",
        "            avg_param.data.add_(local_param.data)\n",
        "    \n",
        "    # Divide by the number of models to compute the average\n",
        "    for param in avg_model.parameters():\n",
        "        param.data.div_(num_models)\n",
        "    \n",
        "    return avg_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# (data_loader, loss_fn,  num_workers, scale_inv_budget, lr_init, wd,initial_state_dict, num_epochs)\n",
        "# def local_adascale_local_sgd(data_loader, num_epochs, lr_init, num_workers, num_local_steps, scale_inv_budget):\n",
        "#     # Initialize a model and save its initial parameters\n",
        "#     model = LeNet5().to(device='cuda').load_state_dict(initial_state_dict)\n",
        "    \n",
        "#     optimizer = torch.optim.SGD(model.parameters(), lr=lr_init)\n",
        "    \n",
        "#     # Add a learning rate scheduler\n",
        "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "    \n",
        "#     grad_cache = [None] * num_workers\n",
        "#     scale_inv_counter = 0\n",
        "    \n",
        "#     for epoch in range(num_epochs):\n",
        "#         for i, (inputs, targets) in enumerate(data_loader):\n",
        "#             # Simulate local steps on different workers\n",
        "#             for worker_id in range(num_workers):\n",
        "#                 outputs = model(inputs)\n",
        "#                 loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
        "#                 optimizer.zero_grad()\n",
        "#                 loss.backward()\n",
        "                \n",
        "#                 if (i + 1) % num_local_steps == 0:\n",
        "#                     grad_cache[worker_id] = [param.grad.clone() for param in model.parameters()]\n",
        "                    \n",
        "#             # Synchronize and average gradients after num_local_steps\n",
        "#             if (i + 1) % num_local_steps == 0:\n",
        "#                 avg_grad = [torch.mean(torch.stack([grad_cache[worker_id][j] for worker_id in range(num_workers)]), dim=0)\n",
        "#                             for j in range(len(grad_cache[0]))]\n",
        "                \n",
        "#                 # Compute gradient statistics\n",
        "#                 grad_variance = [torch.var(torch.stack([grad_cache[worker_id][j] for worker_id in range(num_workers)]), dim=0)\n",
        "#                                  for j in range(len(grad_cache[0]))]\n",
        "#                 grad_mean = [torch.mean(avg_grad[j]) for j in range(len(avg_grad))]\n",
        "                \n",
        "#                 gain_ratio = compute_gain_ratio(grad_mean, grad_variance, num_workers, num_local_steps)\n",
        "                \n",
        "#                 # Apply scaled gradients\n",
        "#                 for param, grad in zip(model.parameters(), avg_grad):\n",
        "#                     param.grad = gain_ratio * grad\n",
        "                \n",
        "#                 # Scale invariant iteration counter\n",
        "#                 scale_inv_counter += gain_ratio\n",
        "                \n",
        "#                 # Update model parameters\n",
        "#                 optimizer.step()\n",
        "\n",
        "#             # Check if scale invariant budget is exhausted\n",
        "#             if scale_inv_counter >= scale_inv_budget:\n",
        "#                 break\n",
        "        \n",
        "#         # Step the scheduler at the end of each epoch\n",
        "#         scheduler.step()\n",
        "    \n",
        "#     return model\n",
        "\n",
        "# def compute_gain_ratio(grad_mean, grad_variance, num_workers, num_local_steps):\n",
        "#     gain_ratio = []\n",
        "#     for g_mean, g_var in zip(grad_mean, grad_variance):\n",
        "#         g2_mean = g_mean ** 2\n",
        "#         term1 = g2_mean + g_var / num_workers\n",
        "#         term2 = (g2_mean + g_var / num_workers) ** 2\n",
        "#         term3 = 3 * (num_local_steps - 1) * g2_mean * g_var\n",
        "#         gain_ratio.append(2 * term1 / (term1 + torch.sqrt(term2 + term3)))\n",
        "    \n",
        "#     return torch.tensor(gain_ratio).mean().item()\n",
        "\n",
        "# Example usage:\n",
        "# model = MyModel()\n",
        "# data_loader = DataLoader(my_dataset, batch_size=64, shuffle=True)\n",
        "# trained_model = local_adascale_local_sgd(model, data_loader, num_epochs=10, lr_init=0.1, num_workers=4, num_local_steps=5, scale_inv_budget=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "    \n",
        "class CustomSGDLocalAdaScaleOptimizer(Optimizer):\n",
        "    def __init__(self, params, lr=0.01, gain_ratio=1.0, momentum=0.9, weight_decay=0.0):\n",
        "        # Store default settings\n",
        "        defaults = dict(lr=lr, gain_ratio=gain_ratio, momentum=momentum, weight_decay=weight_decay)\n",
        "        super(CustomSGDLocalAdaScaleOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            gain_ratio = group['gain_ratio']\n",
        "            momentum = group['momentum']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for param in group['params']:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                d_p = param.grad.data\n",
        "\n",
        "                # Apply weight decay if specified\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(param.data, alpha=weight_decay)\n",
        "\n",
        "                # If momentum is used\n",
        "                if momentum != 0:\n",
        "                    if 'momentum_buffer' not in self.state[param]:\n",
        "                        buf = self.state[param]['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = self.state[param]['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(d_p)\n",
        "                    d_p = buf\n",
        "\n",
        "                # Custom update rule\n",
        "                try:\n",
        "                    param.data.add_(d_p, alpha=-gain_ratio * lr)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "import math\n",
        "def local_adascale(shard_loaders, loss_fn, K, j, lr, wd, initial_state_dict, scale_invariant_budget):\n",
        "    total_start_time = time.time()\n",
        "    global_model = LeNet5().to(device)\n",
        "    global_model.load_state_dict(initial_state_dict)\n",
        "    local_models = [LeNet5().to(device) for _ in range(K)]\n",
        "    for model in local_models:\n",
        "      model.load_state_dict(initial_state_dict)\n",
        "    # local_optimizers = [CustomOptimizer(model.parameters(), lr=0.01, gain_ratio=1) for model in local_models]\n",
        "    # local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9) for model in local_models]\n",
        "    local_optimizers = [CustomSGDLocalAdaScaleOptimizer(model.parameters(), lr, gain_ratio=1, momentum=0.9, weight_decay= wd) for model in local_models]\n",
        "\n",
        "\n",
        "    num_epochs =150\n",
        "    scale_invariant_budget = 150\n",
        "    # for s in scale_invariant_budget: # Bejaye scale_invariant_budget, num_epochs ro dar nazar gereftim vali goftim age s>scale_invariant_budget shod break kon.\n",
        "    scale_inv_counter = 0\n",
        "    gain_ratio = 1\n",
        "    for epoch in range(num_epochs):\n",
        "        grad_cache = []\n",
        "        for hi, h in enumerate(range(j)):\n",
        "            for k in range(K):\n",
        "                # calculate gradient of all workers g^t_k\n",
        "                train_loss, train_accuracy, gradient =local_update_with_gradient(local_models[k], local_optimizers[k], shard_loaders[k], loss_fn, lr, gain_ratio)\n",
        "                print(f'Worker {k+1}, [{h+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "                if hi == 0: # Only store the gradients for the first local step\n",
        "                    grad_cache.append(gradient)\n",
        "                \n",
        "\n",
        "            # ------------\n",
        "            \n",
        "            # varriance bar va G bar bayad adad bashan va faghat tooye synchronization point (after H steps mohasebe mishan)\n",
        "            # fek konam in mishe g bar_t\n",
        "            # list ba len=10 (ehtemalan tedade layer ha) va dakhele harkodoom 2 ta tensor ke ehtemalen weights and bias\n",
        "            # TA Inja Kar mikone\n",
        "            # TODO: variance bar, G bar, rho, lr tebghe paper dar zamane syncronization update mishan.\n",
        "        \n",
        "        global_model = average_models(local_models)\n",
        "        for local_model in local_models:\n",
        "            local_model.load_state_dict(global_model.state_dict())\n",
        "        # To check if averaging is done correctly (for only two workers)\n",
        "        # with torch.no_grad():\n",
        "        #     for param1, param2, avg_param in zip(local_models[0].parameters(), local_models[1].parameters(), average_model.parameters()):\n",
        "        #         calculated_avg = (param1.data + param2.data) / 2\n",
        "        #         assert torch.allclose(avg_param.data, calculated_avg), \"Averaging failed!\"\n",
        "        \n",
        "        # test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "        # print(f'Local Step {epoch+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "        print(50*'-')\n",
        "        g_bar = calculate_g_bar(grad_cache, K)\n",
        "        variance_bar = calculate_variance_bar(g_bar, grad_cache, K)\n",
        "        G_bar = calculate_G_bar(g_bar=g_bar, num_workers=K, variance_bar=variance_bar)\n",
        "        L = estimate_lipschitz_constant(global_model, shard_loaders[0], loss_fn,device='cuda')\n",
        "        gain_ratio_new = compute_gain_ratio(G_bar,variance_bar, K, j)\n",
        "        lr_new = compute_optimal_learning_rate(G_bar,variance_bar, K, j, L)\n",
        "        if not (lr_new < 0 or math.isnan(lr_new) or gain_ratio_new < 0 or math.isnan(gain_ratio_new)):\n",
        "        # lr = lr_new\n",
        "            gain_ratio = gain_ratio_new\n",
        "        print(f'variance_bar: {variance_bar}, G_bar: {G_bar}, gain_ratio: {gain_ratio}, L: {L}, lr: {lr}')\n",
        "        # Scale invariant iteration counter\n",
        "        scale_inv_counter += gain_ratio\n",
        "        if scale_inv_counter > scale_invariant_budget:\n",
        "            break \n",
        "        print (f'scale_inv_counter: {scale_inv_counter} : scale_invariant_budget: {scale_invariant_budget}')\n",
        "        print (f'gain_ratio: {gain_ratio} lr: {lr}')\n",
        "\n",
        "    \n",
        "    total_end_time = time.time()\n",
        "    print('/'*50)\n",
        "    print(f'Total time taken for LocalAdaScale: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "    print('/'*50)\n",
        "\n",
        "def calculate_g_bar(grad_cache, num_workers):\n",
        "    \"\"\"Calculate ḡ: Element-wise average of all gradients in grad_cache.\"\"\"\n",
        "    gbar = [torch.mean(torch.stack([grad_cache[worker_id][i] for worker_id in range(num_workers)]), dim=0)\n",
        "            for i in range(len(grad_cache[0]))]\n",
        "    \n",
        "    return gbar\n",
        "\n",
        "\n",
        "'''def compute_gain_ratio(grad_mean, grad_variance, num_workers, num_local_steps):\n",
        "    gain_ratio = []\n",
        "    for g_mean, g_var in zip(grad_mean, grad_variance):\n",
        "        g2_mean = g_mean ** 2\n",
        "        term1 = g2_mean + g_var / num_workers\n",
        "        term2 = (g2_mean + g_var / num_workers) ** 2\n",
        "        term3 = 3 * (num_local_steps - 1) * g2_mean * g_var\n",
        "        gain_ratio.append(2 * term1 / (term1 + torch.sqrt(term2 + term3)))\n",
        "    \n",
        "    return torch.tensor(gain_ratio).mean().item()'''\n",
        "\n",
        "def compute_gain_ratio(G_bar, variance_bar, num_workers, num_local_steps):\n",
        "    # Use Ḡ and variancē to compute the gain ratio\n",
        "    term0 = 2 * (G_bar + variance_bar)\n",
        "    term1 = G_bar + variance_bar / num_workers\n",
        "    term2 = term1 ** 2\n",
        "    term3 = 3 * (num_local_steps - 1) * G_bar * variance_bar\n",
        "    \n",
        "    gain_ratio = term0 / (term1 + torch.sqrt(term2 + term3))\n",
        "    # print(f'variance_bar/G_bar: {variance_bar/G_bar}') # Vaghti variance_bar/G_bar miad taghriban zire 10 hame chi be ham mirize va gain ratio nan mishe.\n",
        "    if torch.isnan(gain_ratio):\n",
        "        pass\n",
        "        pass\n",
        "    # Ke variance_bar/G_bar tabe'e tedade worker e, va shayad tedade iteration\n",
        "\n",
        "        # time.sleep(2)\n",
        "\n",
        "    return gain_ratio.item()\n",
        "\n",
        "def compute_optimal_learning_rate(G_bar, variance_bar, num_workers, num_local_steps, L):\n",
        "    # Use Ḡ and variancē to compute the adaptive learning rate (t)\n",
        "    term0 = 2 * (G_bar)\n",
        "    term1 = G_bar + variance_bar / num_workers\n",
        "    term2 = term1 ** 2\n",
        "    term3 = 3 * (num_local_steps - 1) * G_bar * variance_bar\n",
        "    \n",
        "    lr = term0 / (term1 + torch.sqrt(term2 + term3))\n",
        "    lr = lr / L\n",
        "    return lr.item()\n",
        "\n",
        "def calculate_G_bar(g_bar, variance_bar, num_workers):\n",
        "    \"\"\"Calculate Ḡ as a scalar.\"\"\"\n",
        "    g_bar_norm = sum(torch.norm(g_bar[i])**2 for i in range(len(g_bar)))\n",
        "    G_bar = g_bar_norm - (1 / num_workers) * variance_bar\n",
        "    # print(f'G_bar: {G_bar}')\n",
        "    # print(G_bar.item())\n",
        "    if G_bar.item() < 0:\n",
        "        print('Gbaaaaaaaaaaaaar is negative')\n",
        "        # G_bar = 0.00\n",
        "    #     G_bar = 0.02* variance_bar # Choon bazi vaghta adad mishe kamtar az sefr choon taghrib mizanim. vali in kari ke kardam ham dorost nist.\n",
        "    #     print(f'heeeeeeeey: g_bar_norm: {g_bar_norm}, variance_bar: {variance_bar}, (1 / num_workers) * variance_bar: {(1 / num_workers) * variance_bar}')\n",
        "    #     print('-')\n",
        "\n",
        "    return G_bar\n",
        "\n",
        "def calculate_variance_bar(g_bar, grad_cache, num_workers): # Hamishe mosbate.\n",
        "    \"\"\"Calculate variancē as a scalar.\"\"\"\n",
        "    sum_norms = sum(torch.norm(grad_cache[worker_id][i])**2 for worker_id in range(num_workers) for i in range(len(g_bar)))\n",
        "    nomrs_sum = 0\n",
        "    for worker_id in range(num_workers):\n",
        "        for i in range(len(g_bar)):\n",
        "            nomrs_sum(torch.norm(grad_cache[worker_id][i])**2)\n",
        "    g_bar_norm = sum(torch.norm(g_bar[i])**2 for i in range(len(g_bar)))\n",
        "    variance_bar = (1 / (num_workers - 1)) * sum_norms - (num_workers / (num_workers - 1)) * g_bar_norm\n",
        "    print(f'sum(norm(g^t_k)**2): {sum_norms}, g_bar_norm: {g_bar_norm}')\n",
        "    return variance_bar\n",
        "\n",
        "def local_update_with_gradient(model, optimizer, dataloader, loss_fn, lr, gain_ratio, device=device, is_wandb=False):# (X, y, model, criterion, learning_rate): # forward_backward_pass_manual\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Update optimizer parameters\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "        param_group['gain_ratio'] = gain_ratio\n",
        "        \n",
        "        \n",
        "    \n",
        "    ## model.zero_grad()  # Initialize gradients to zero at the start of each epoch\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "    \n",
        "    initial_params = [param.clone() for param in model.parameters()] \n",
        "\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        loss.backward()  # Backpropagation\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        optimizer.step() \n",
        "        if i == (len(dataloader) - 1):\n",
        "            gradient = [param.grad.clone() for param in model.parameters()] \n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    final_params = [param.clone() for param in model.parameters()]  # Capture final parameters\n",
        "\n",
        "    # Calculate delta as the difference between final and initial parameters\n",
        "    # gradient = [(final - initial)/(gain_ratio * lr) for final, initial in zip(final_params, initial_params)]\n",
        "    \n",
        "    # TODO: in faghat gradient e akhar ro hesab mikone  behtare ke \n",
        "    #gradient = [param.grad.clone() for param in model.parameters()] \n",
        "    \n",
        "    # Update parameters using custom rule\n",
        "    # with torch.no_grad():  # Disable gradient tracking for manual update\n",
        "    #     for param in model.parameters():\n",
        "    #         if param.grad is not None:\n",
        "    #             param -= gain_ratio * lr * param.grad\n",
        "\n",
        "    # model.zero_grad()\n",
        "    len_dataloader = len(dataloader)\n",
        "    train_loss = running_loss / len_dataloader\n",
        "    train_accuracy = 100. * correct / total\n",
        "    if math.isnan(train_loss) or train_loss > 10:\n",
        "        pass\n",
        "    if is_wandb:\n",
        "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": train_accuracy})\n",
        "\n",
        "    return train_loss, train_accuracy, gradient\n",
        "\n",
        "def estimate_lipschitz_constant(model, dataloader, loss_fn, device='cuda'):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    max_grad_norm = 0\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        loss.backward()  # Compute gradients\n",
        "        \n",
        "        # Compute the gradient norm\n",
        "        total_norm = 0\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                param_norm = p.grad.data.norm(2)\n",
        "                total_norm += param_norm.item() ** 2\n",
        "        \n",
        "        total_norm = total_norm ** 0.5\n",
        "        max_grad_norm = max(max_grad_norm, total_norm)\n",
        "    \n",
        "    # Estimate L as the maximum observed gradient norm\n",
        "    lipschitz_constant = max_grad_norm\n",
        "    return lipschitz_constant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:2\n",
            "==================================================\n",
            "Worker 1, [01/02]: Training Loss: 4.585575668, Training Accuracy: 1.888\n",
            "Worker 2, [01/02]: Training Loss: 4.587634257, Training Accuracy: 1.920\n",
            "Worker 3, [01/02]: Training Loss: 4.590183788, Training Accuracy: 1.088\n",
            "Worker 4, [01/02]: Training Loss: 4.585954588, Training Accuracy: 1.760\n",
            "Worker 5, [01/02]: Training Loss: 4.590105193, Training Accuracy: 1.456\n",
            "Worker 6, [01/02]: Training Loss: 4.593763317, Training Accuracy: 1.296\n",
            "Worker 7, [01/02]: Training Loss: 4.591303373, Training Accuracy: 1.584\n",
            "Worker 8, [01/02]: Training Loss: 4.588327033, Training Accuracy: 1.648\n",
            "Worker 1, [02/02]: Training Loss: 4.349892193, Training Accuracy: 4.400\n",
            "Worker 2, [02/02]: Training Loss: 4.383812447, Training Accuracy: 3.632\n",
            "Worker 3, [02/02]: Training Loss: 4.370596905, Training Accuracy: 4.336\n",
            "Worker 4, [02/02]: Training Loss: 4.384550151, Training Accuracy: 3.760\n",
            "Worker 5, [02/02]: Training Loss: 4.385340963, Training Accuracy: 3.744\n",
            "Worker 6, [02/02]: Training Loss: 4.420571517, Training Accuracy: 3.488\n",
            "Worker 7, [02/02]: Training Loss: 4.383726125, Training Accuracy: 4.064\n",
            "Worker 8, [02/02]: Training Loss: 4.385157790, Training Accuracy: 4.128\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'int' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# (, loss_fn, , j, lr, wd, initial_state_dict, num_epochs)           \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# local_adascale_local_sgd(shard_loaders, loss_fn,  k, scale_inv_budget, lr, wd,initial_state_dict, num_epochs)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mlocal_adascale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_invariant_budget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# LocalAdaScale(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[40], line 53\u001b[0m, in \u001b[0;36mlocal_adascale\u001b[0;34m(shard_loaders, loss_fn, K, j, lr, wd, initial_state_dict, scale_invariant_budget)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m50\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m g_bar \u001b[38;5;241m=\u001b[39m calculate_g_bar(grad_cache, K)\n\u001b[0;32m---> 53\u001b[0m variance_bar \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_variance_bar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m G_bar \u001b[38;5;241m=\u001b[39m calculate_G_bar(g_bar\u001b[38;5;241m=\u001b[39mg_bar, num_workers\u001b[38;5;241m=\u001b[39mK, variance_bar\u001b[38;5;241m=\u001b[39mvariance_bar)\n\u001b[1;32m     55\u001b[0m L \u001b[38;5;241m=\u001b[39m estimate_lipschitz_constant(global_model, shard_loaders[\u001b[38;5;241m0\u001b[39m], loss_fn,device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[40], line 144\u001b[0m, in \u001b[0;36mcalculate_variance_bar\u001b[0;34m(g_bar, grad_cache, num_workers)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m worker_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_workers):\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(g_bar)):\n\u001b[0;32m--> 144\u001b[0m         \u001b[43mnomrs_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m g_bar_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(torch\u001b[38;5;241m.\u001b[39mnorm(g_bar[i])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(g_bar)))\n\u001b[1;32m    146\u001b[0m variance_bar \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (num_workers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m sum_norms \u001b[38;5;241m-\u001b[39m (num_workers \u001b[38;5;241m/\u001b[39m (num_workers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m g_bar_norm\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [8, 2, 4, 8]\n",
        "J = [2, 4, 8, 16, 32, 64] \n",
        "scale_invariant_budget = 150 # It uses as a unit instead of number of epochs\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    # (, loss_fn, , j, lr, wd, initial_state_dict, num_epochs)           \n",
        "    # local_adascale_local_sgd(shard_loaders, loss_fn,  k, scale_inv_budget, lr, wd,initial_state_dict, num_epochs)\n",
        "    local_adascale(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, scale_invariant_budget)\n",
        "    # LocalAdaScale(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BGJAcU7dTxJ1",
        "V2mj0Wd-T73T",
        "ybM87poAtHnl",
        "5fjzE9q4UOPU",
        "SEseiEKFt0mR",
        "taX_5ElNuKxC",
        "aeMWj_f0sbQE",
        "3GXZaFXruZI_"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06d3de2006b14793bd57a5ca89d44e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68d86018628f420e8d7e516ba5827e35",
            "placeholder": "​",
            "style": "IPY_MODEL_8877fd11846246f989e31835e5d3e7ae",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "07dc9b3c563e4615bec4dbd3233bf4ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07ff6351429c48c2b835305d3111af40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5de35a9063345b1a12e212718a02575",
            "placeholder": "​",
            "style": "IPY_MODEL_e86e517239b54ca4a6767a300aa7e01d",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "09b762f5ba784066a9408ffcfdea5b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a9ee7dc8817456aab4af03cbfa4c6ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b42ea1995bb410b99e653780dad3916": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b4b10bb8bff4d40afa8df981440fe43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d710da02bfe4c8e80d205158d9d64c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ef1563b6ba74b86a7bf7af3fcc3d5d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fdaf43c468043139e5d72397b118371": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "104402d4cba5477499593d972bc48e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13e7a0db2aa74e35b7d1948224358d76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1574e69b88de479da2aadc2dfdd43261": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15a0bbd1e2b14cef8a2f9accf120c1ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "175043eb44e44b4f9aefaf51aae6fb10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71e710b571d142a3a08cd233590b7171",
            "placeholder": "​",
            "style": "IPY_MODEL_2671c3bf3cfd49aa9ad6841c289068cc",
            "value": "0.019 MB of 0.019 MB uploaded\r"
          }
        },
        "17ac4efa9cf148e5be69edbc5b0bdecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18cb73ac1b224757920e7a8187b3ba4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19ed5f5d4f714246bbdf44513ecc50f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f9d7ac8fa8a4bcdb76fe2b2cb443f66",
              "IPY_MODEL_71c0cc303cfc44ba8d989d5f8feca119"
            ],
            "layout": "IPY_MODEL_07dc9b3c563e4615bec4dbd3233bf4ba"
          }
        },
        "1b093d744b374154afe2058e4a6de822": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b1acf7a1f5f43739e1fe0894ba48950": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d534596e41340438fc8d083fff6aa8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d74e9350c2c4c61b2e95924ec133012": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f9d7ac8fa8a4bcdb76fe2b2cb443f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ccc0b32e9a4ceebd6045dff63e6621",
            "placeholder": "​",
            "style": "IPY_MODEL_104402d4cba5477499593d972bc48e9d",
            "value": "0.030 MB of 0.030 MB uploaded\r"
          }
        },
        "1feabb0772134ace893d71cb905e9b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b1acf7a1f5f43739e1fe0894ba48950",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d710da02bfe4c8e80d205158d9d64c7",
            "value": 1
          }
        },
        "1fef0782f0804736881f03c2e31c3d64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2402e482be7e484fa98ae3f60f68d95a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2671c3bf3cfd49aa9ad6841c289068cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2991ead42a2a4637b2902ca26837d74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e0bb90e33ec4e198857c255080ef6f0",
              "IPY_MODEL_77790c4dcd124f73907619689fb8d37c"
            ],
            "layout": "IPY_MODEL_1fef0782f0804736881f03c2e31c3d64"
          }
        },
        "2bc1423c729a4faa9a184ae092eda8ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bd5fda76227433aa9332e6e48cd415b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd6480fe59104c7aa15d39bf6ea4a63b",
              "IPY_MODEL_3cb7b5c42e844747ae133750b7d42882"
            ],
            "layout": "IPY_MODEL_b81cc89707e44dedb51081d13a3ba424"
          }
        },
        "2d65b320aaed40cd87b8f78c99c614e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f39d5f73c7647f1804e4212cb39924d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e7ede02663f4eb0927872bf17858f18",
              "IPY_MODEL_a562d76741f74b10b8095be14da779d1"
            ],
            "layout": "IPY_MODEL_f4e7759ba0f544d8a28a9922e06e890b"
          }
        },
        "30d70d57987c4349b55560e5d9626201": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36ac25551e574241bed4d7e5a4301d95",
              "IPY_MODEL_6898349b9c754ca7be91bea29cabbba5"
            ],
            "layout": "IPY_MODEL_623f947ef06c41569be7fcdda4141174"
          }
        },
        "32c759d7fff64701a6ad61b38ac63187": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67aa8f8c2b244c9a9a3bb11caf491247",
            "placeholder": "​",
            "style": "IPY_MODEL_09b762f5ba784066a9408ffcfdea5b2e",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "33050a2d80c24235aecc36cb34e79684": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33a438b911c04f1f96c67b4f7ba7477d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff7b6956e7e34db68cd38dee19c798f1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d71c8b5d30df474ca6f30976d0a33c71",
            "value": 1
          }
        },
        "3445c5fe76514de0b6b9cc31200c8544": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0748d32c99741d39b18512400e07f30",
            "placeholder": "​",
            "style": "IPY_MODEL_dc43eb867d6b446cb0cb8e5debae57e7",
            "value": "0.030 MB of 0.030 MB uploaded\r"
          }
        },
        "346856a22f0e45b2be0c14aa7902261f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d90a90d5854bcaa2bc5f385cdad931": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3606009883cf4212b7e1ed6e4f182845": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36a546b812d6403cbf5ed693318a9744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a5f301338a4e5185c042a22684ed4a",
            "placeholder": "​",
            "style": "IPY_MODEL_48c7de7682f242fd86a234d1607187f4",
            "value": "0.020 MB of 0.020 MB uploaded\r"
          }
        },
        "36ac25551e574241bed4d7e5a4301d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d268f74266044451b230f20756bab2f6",
            "placeholder": "​",
            "style": "IPY_MODEL_6896790e80d1450c821918c7ef41e42f",
            "value": "0.027 MB of 0.027 MB uploaded\r"
          }
        },
        "371471be80ca47dbb005c853b7832f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1574e69b88de479da2aadc2dfdd43261",
            "placeholder": "​",
            "style": "IPY_MODEL_f523ae2aca8c46878f0a6ddc1f798ef5",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "397b6d4daaf04a1181413adc5e30db0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39aecb8ffe0645fbadccf8b0f8ed2486": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b0b3657d30547d7abe702d6c1e7b7c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cb7b5c42e844747ae133750b7d42882": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fdaf43c468043139e5d72397b118371",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f63fffe56ff64f1eb2b6e8f14974f011",
            "value": 1
          }
        },
        "3ee2c346d4d74faa915ef8315e3303c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "403dcd5f10de42ddbb67c8108e3bc34c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4302f47b1ff043aca2523212b015e080": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_371471be80ca47dbb005c853b7832f04",
              "IPY_MODEL_e75a0d8ae24e462aa3075a813aad302d"
            ],
            "layout": "IPY_MODEL_fd2cd8045a5c4387b98d080782289c48"
          }
        },
        "431cf75dd1cc4840b14f678876f45bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5f3871113994f1eb993c126d793d147",
              "IPY_MODEL_7903555c1e884408b70c9951ecae84ec"
            ],
            "layout": "IPY_MODEL_2d65b320aaed40cd87b8f78c99c614e7"
          }
        },
        "45bdcbbed9ae47ffb24bd2a962345eaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46c9d83bf8af443bb376ed4858ce35f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b0b3657d30547d7abe702d6c1e7b7c4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17ac4efa9cf148e5be69edbc5b0bdecf",
            "value": 1
          }
        },
        "48c7de7682f242fd86a234d1607187f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49727de0590f41dca6addf3f4ffe33f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "497de066b1964cd4b931476e0bd50c66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a4cfb14c56e477cbfef5a652c05fabc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b733fc6800e4a0ab36bba52ff84f1a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e2cc873f917444c833e152f7c2555de",
            "placeholder": "​",
            "style": "IPY_MODEL_49727de0590f41dca6addf3f4ffe33f2",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "5055c91b4ed34c1baf249fa25948e84f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33050a2d80c24235aecc36cb34e79684",
            "placeholder": "​",
            "style": "IPY_MODEL_2bc1423c729a4faa9a184ae092eda8ad",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "50b4ba3147b540abad020ae780353f27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "557cf2de74314915b7204d9055fa6987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a345ff7386643eeb7a5b190d69d2e0e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f4783e2060a4295bb68036a4a7310d9",
            "value": 1
          }
        },
        "5630d5fcb50a46f2887e0cce74a005a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "567cc1542f09433f8cc8b3a39237f198": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "584e5a6aada04535b64c40c3ece566e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cf516644d1a4ebf82c7e9dfa13e560b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36a546b812d6403cbf5ed693318a9744",
              "IPY_MODEL_aed05a015df946c7bae33226c3a8ddc6"
            ],
            "layout": "IPY_MODEL_d601ef3ebb6c4b688ac53a65979aa445"
          }
        },
        "5e0116dc735e4ad995a5b1a039c6a55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76dfea14213f4e66b269b901150c6cba",
            "placeholder": "​",
            "style": "IPY_MODEL_d1e4a03094b94c34ac235ffbd0c7fa06",
            "value": "0.030 MB of 0.030 MB uploaded\r"
          }
        },
        "5f3a33bc6a334c9e8c5886caa5f015ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8040d3a83de41319a2a836def914b1b",
            "placeholder": "​",
            "style": "IPY_MODEL_0b4b10bb8bff4d40afa8df981440fe43",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "60957598b6c64ab0987d583b788dd674": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623f947ef06c41569be7fcdda4141174": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63dd47b50bdc44e88edd97d14585f7ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e0116dc735e4ad995a5b1a039c6a55a",
              "IPY_MODEL_8cf1e952ed294ccdbc3bb2275b5d3efd"
            ],
            "layout": "IPY_MODEL_7421027bfcb34262a1b99d297e49bf8e"
          }
        },
        "64224c777f01418e904dcd30ecd7c5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99929754d3094d03a5f311177ac26cb3",
            "placeholder": "​",
            "style": "IPY_MODEL_9ea517158b974c5da17899e3fea4fc3b",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "67aa8f8c2b244c9a9a3bb11caf491247": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6896790e80d1450c821918c7ef41e42f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6898349b9c754ca7be91bea29cabbba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a30700c8bfb449cda98a40d75828f3d3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77beee57b67d4bad826f616e5483b87a",
            "value": 1
          }
        },
        "68d86018628f420e8d7e516ba5827e35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68ff4108835c462b929d0b5c78497555": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "696917cb66a74c41b87f9b436f8ce42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77a9b4df16ba408a941d67a51eed303d",
              "IPY_MODEL_b3b0d42308df44e6b43a06f7424d489d"
            ],
            "layout": "IPY_MODEL_fd23196beb964eb0a631b1fcb3893545"
          }
        },
        "6a345ff7386643eeb7a5b190d69d2e0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa92cf922724ac08c45385ff8a58447": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6abbcbe71a314356ba04f8349d1fc127": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3326a99729a416abca13d3122196e64",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d534596e41340438fc8d083fff6aa8b",
            "value": 1
          }
        },
        "6c30f9a9e6c44d78ad39f43a5f11a6d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e51a8e2e7e6463486d7b11454333941": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e7ede02663f4eb0927872bf17858f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a9ee7dc8817456aab4af03cbfa4c6ed",
            "placeholder": "​",
            "style": "IPY_MODEL_567cc1542f09433f8cc8b3a39237f198",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "7064cd1e5d794c258dbeeb63a9ae9f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71c0cc303cfc44ba8d989d5f8feca119": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80fe0a51b14b4e548454d4f83215336c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e13a8bef6e14973912966984e83cd4c",
            "value": 1
          }
        },
        "71e710b571d142a3a08cd233590b7171": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7421027bfcb34262a1b99d297e49bf8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76dfea14213f4e66b269b901150c6cba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77790c4dcd124f73907619689fb8d37c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13e7a0db2aa74e35b7d1948224358d76",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7064cd1e5d794c258dbeeb63a9ae9f9b",
            "value": 1
          }
        },
        "77a9b4df16ba408a941d67a51eed303d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e51a8e2e7e6463486d7b11454333941",
            "placeholder": "​",
            "style": "IPY_MODEL_18cb73ac1b224757920e7a8187b3ba4d",
            "value": "0.020 MB of 0.020 MB uploaded\r"
          }
        },
        "77beee57b67d4bad826f616e5483b87a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77c702b6d9c04f00b8e624f31a591f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6d67121f0ab4daf96d0bdd2c433259b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ee2c346d4d74faa915ef8315e3303c8",
            "value": 1
          }
        },
        "7903555c1e884408b70c9951ecae84ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac87babb2f2a4781ab863c3b8f613807",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3970cfd36ed4450b7c8f958a6499dc4",
            "value": 1
          }
        },
        "7e2cc873f917444c833e152f7c2555de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "804c12a3f13840c7bdb2e6df69e62c10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68ff4108835c462b929d0b5c78497555",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a63e1d987556448280ca217f17b60b49",
            "value": 1
          }
        },
        "80fe0a51b14b4e548454d4f83215336c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81408f178c46447f90d36d1d18cdad82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06d3de2006b14793bd57a5ca89d44e4a",
              "IPY_MODEL_804c12a3f13840c7bdb2e6df69e62c10"
            ],
            "layout": "IPY_MODEL_4a4cfb14c56e477cbfef5a652c05fabc"
          }
        },
        "81500a36ef5e4a9e9b36aef01bee3697": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "829bba2a5bb947a0bcf121c527902255": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "829eee71e4d74403a22941b15dfcf9bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8587c890f2a847e293a101405d64fc0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_175043eb44e44b4f9aefaf51aae6fb10",
              "IPY_MODEL_db36d0cc7691440a9792ac779e161e62"
            ],
            "layout": "IPY_MODEL_c6a68af25a2847e98201847781419670"
          }
        },
        "8795a05c814c469083b31be62e79289f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8877fd11846246f989e31835e5d3e7ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "889653c19d8e43cfb6f56ed46af5b76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cf1e952ed294ccdbc3bb2275b5d3efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8e410ff55fc448bbe87c8975a552e88",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f49251ce876f4d9292a4179b48ebb22a",
            "value": 1
          }
        },
        "8dc245e02cac40f1b601fa42a0e6e77a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e13a8bef6e14973912966984e83cd4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f4783e2060a4295bb68036a4a7310d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "904f899f441149e9b707699a0ebf31b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90b585318084475b9543b3226230d373": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f3a33bc6a334c9e8c5886caa5f015ca",
              "IPY_MODEL_46c9d83bf8af443bb376ed4858ce35f7"
            ],
            "layout": "IPY_MODEL_2402e482be7e484fa98ae3f60f68d95a"
          }
        },
        "9387a738135842cf965db860499510f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5630d5fcb50a46f2887e0cce74a005a6",
            "placeholder": "​",
            "style": "IPY_MODEL_fed90f96881140119ee97a0d2120b615",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "97c3921b8e454ccdb7b4db95d4440c64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99929754d3094d03a5f311177ac26cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d88657b39fd4b169d61b1b8d240e6ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e0bb90e33ec4e198857c255080ef6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3606009883cf4212b7e1ed6e4f182845",
            "placeholder": "​",
            "style": "IPY_MODEL_39aecb8ffe0645fbadccf8b0f8ed2486",
            "value": "0.021 MB of 0.021 MB uploaded\r"
          }
        },
        "9ea517158b974c5da17899e3fea4fc3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1d013e4d3e942b6b362ab5ade6d1a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a30700c8bfb449cda98a40d75828f3d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ccc0b32e9a4ceebd6045dff63e6621": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a562d76741f74b10b8095be14da779d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dc245e02cac40f1b601fa42a0e6e77a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b093d744b374154afe2058e4a6de822",
            "value": 1
          }
        },
        "a5758bfac16a4388a036005a15812d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5f3871113994f1eb993c126d793d147": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_346856a22f0e45b2be0c14aa7902261f",
            "placeholder": "​",
            "style": "IPY_MODEL_397b6d4daaf04a1181413adc5e30db0c",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "a63e1d987556448280ca217f17b60b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6ba8ab59be54b8ca096631627ee9713": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa6aa819c1fd4776ac47af588aaaaacd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64224c777f01418e904dcd30ecd7c5ef",
              "IPY_MODEL_c1dbd12595384bc6a0240aaeb16014dc"
            ],
            "layout": "IPY_MODEL_6aa92cf922724ac08c45385ff8a58447"
          }
        },
        "ab0b891f55f648cdbace9bc4540c51c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab580ef13b18428c994fc4f8b80885b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb268a732bc74b2d81ec3c3a6ecd0362",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2915a147a4246dba7984a90f23c34ae",
            "value": 1
          }
        },
        "ab62d64100a84b1faae744ee0f99501a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b4ba3147b540abad020ae780353f27",
            "placeholder": "​",
            "style": "IPY_MODEL_584e5a6aada04535b64c40c3ece566e8",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "ac87babb2f2a4781ab863c3b8f613807": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed05a015df946c7bae33226c3a8ddc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_403dcd5f10de42ddbb67c8108e3bc34c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1d013e4d3e942b6b362ab5ade6d1a80",
            "value": 1
          }
        },
        "b0748d32c99741d39b18512400e07f30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3970cfd36ed4450b7c8f958a6499dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3b0d42308df44e6b43a06f7424d489d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ef1563b6ba74b86a7bf7af3fcc3d5d6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_889653c19d8e43cfb6f56ed46af5b76f",
            "value": 1
          }
        },
        "b81cc89707e44dedb51081d13a3ba424": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba6f9560801c461a90a0fd2f8c20d918": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5055c91b4ed34c1baf249fa25948e84f",
              "IPY_MODEL_1feabb0772134ace893d71cb905e9b12"
            ],
            "layout": "IPY_MODEL_fb5f2e328dff44018f41180c2a2ec871"
          }
        },
        "bb2ee36dd3c04926b10b832928d7d0a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcbeb5bacc824f3aacd20e50d38bb70a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1dbd12595384bc6a0240aaeb16014dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb2ee36dd3c04926b10b832928d7d0a0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d88657b39fd4b169d61b1b8d240e6ff",
            "value": 1
          }
        },
        "c20d73b37f6a497a8847e2fd20a98d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15a0bbd1e2b14cef8a2f9accf120c1ad",
            "placeholder": "​",
            "style": "IPY_MODEL_e181391aa4424c04804ac665abd6f594",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "c59ee0d8b9fb4694ada362115bf965a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab62d64100a84b1faae744ee0f99501a",
              "IPY_MODEL_6abbcbe71a314356ba04f8349d1fc127"
            ],
            "layout": "IPY_MODEL_97c3921b8e454ccdb7b4db95d4440c64"
          }
        },
        "c5bb24dc0f56483b88a0efbfa2a1c8ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5de35a9063345b1a12e212718a02575": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6a68af25a2847e98201847781419670": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8e410ff55fc448bbe87c8975a552e88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbe10db0bb8d45609a0f3d59a30c8f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07ff6351429c48c2b835305d3111af40",
              "IPY_MODEL_ab580ef13b18428c994fc4f8b80885b6"
            ],
            "layout": "IPY_MODEL_497de066b1964cd4b931476e0bd50c66"
          }
        },
        "d1e4a03094b94c34ac235ffbd0c7fa06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d268f74266044451b230f20756bab2f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3326a99729a416abca13d3122196e64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d601ef3ebb6c4b688ac53a65979aa445": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d71c8b5d30df474ca6f30976d0a33c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d770fa4de3bd479886dbf8e1f6369543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c20d73b37f6a497a8847e2fd20a98d16",
              "IPY_MODEL_edd61f4569b54b869669aebf91420508"
            ],
            "layout": "IPY_MODEL_a6ba8ab59be54b8ca096631627ee9713"
          }
        },
        "db36d0cc7691440a9792ac779e161e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45bdcbbed9ae47ffb24bd2a962345eaa",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bcbeb5bacc824f3aacd20e50d38bb70a",
            "value": 1
          }
        },
        "dc43eb867d6b446cb0cb8e5debae57e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcbb358118e644a2a75d44d8adb6747b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60957598b6c64ab0987d583b788dd674",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4ad17a7648245069c70f2f3fb79105a",
            "value": 1
          }
        },
        "df97494c2f654ffbaceb9ca801ff3113": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1e5353cb6a1427a8b0b9d91d6067ef7",
              "IPY_MODEL_77c702b6d9c04f00b8e624f31a591f01"
            ],
            "layout": "IPY_MODEL_8795a05c814c469083b31be62e79289f"
          }
        },
        "e181391aa4424c04804ac665abd6f594": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1e5353cb6a1427a8b0b9d91d6067ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_829eee71e4d74403a22941b15dfcf9bc",
            "placeholder": "​",
            "style": "IPY_MODEL_ab0b891f55f648cdbace9bc4540c51c6",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "e4a5f301338a4e5185c042a22684ed4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4ad17a7648245069c70f2f3fb79105a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6a773393e084de9a117fbbf43b7a59e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6d67121f0ab4daf96d0bdd2c433259b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e75a0d8ae24e462aa3075a813aad302d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b42ea1995bb410b99e653780dad3916",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3ebd9b7bf7a4d24b54ab6673322f74b",
            "value": 1
          }
        },
        "e8040d3a83de41319a2a836def914b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e851c69b66ca4e8a80779a69435b855f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b733fc6800e4a0ab36bba52ff84f1a5",
              "IPY_MODEL_dcbb358118e644a2a75d44d8adb6747b"
            ],
            "layout": "IPY_MODEL_829bba2a5bb947a0bcf121c527902255"
          }
        },
        "e86e517239b54ca4a6767a300aa7e01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eafc1e77174c4758a3780b481d694ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c30f9a9e6c44d78ad39f43a5f11a6d0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81500a36ef5e4a9e9b36aef01bee3697",
            "value": 1
          }
        },
        "eb268a732bc74b2d81ec3c3a6ecd0362": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edd61f4569b54b869669aebf91420508": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8ce66183b2543e7bf19b71d90d1c53e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_904f899f441149e9b707699a0ebf31b5",
            "value": 1
          }
        },
        "f2915a147a4246dba7984a90f23c34ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3ebd9b7bf7a4d24b54ab6673322f74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f49251ce876f4d9292a4179b48ebb22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4e7759ba0f544d8a28a9922e06e890b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f523ae2aca8c46878f0a6ddc1f798ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5fd8db807e144578a2e20762d7369d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32c759d7fff64701a6ad61b38ac63187",
              "IPY_MODEL_557cf2de74314915b7204d9055fa6987"
            ],
            "layout": "IPY_MODEL_c5bb24dc0f56483b88a0efbfa2a1c8ee"
          }
        },
        "f63fffe56ff64f1eb2b6e8f14974f011": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f76ed5acf91a4edf92950dcb88356f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9387a738135842cf965db860499510f5",
              "IPY_MODEL_33a438b911c04f1f96c67b4f7ba7477d"
            ],
            "layout": "IPY_MODEL_35d90a90d5854bcaa2bc5f385cdad931"
          }
        },
        "f8ce66183b2543e7bf19b71d90d1c53e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9dca98d1b8c402db8fa03f1354a051b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3445c5fe76514de0b6b9cc31200c8544",
              "IPY_MODEL_eafc1e77174c4758a3780b481d694ae0"
            ],
            "layout": "IPY_MODEL_e6a773393e084de9a117fbbf43b7a59e"
          }
        },
        "fb5f2e328dff44018f41180c2a2ec871": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd23196beb964eb0a631b1fcb3893545": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2cd8045a5c4387b98d080782289c48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd6480fe59104c7aa15d39bf6ea4a63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d74e9350c2c4c61b2e95924ec133012",
            "placeholder": "​",
            "style": "IPY_MODEL_a5758bfac16a4388a036005a15812d1d",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "fed90f96881140119ee97a0d2120b615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff7b6956e7e34db68cd38dee19c798f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

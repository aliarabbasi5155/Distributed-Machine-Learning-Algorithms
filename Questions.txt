Local SGD
1. Workers calculate gradients and send to PS or they could send whole local model?
2. How to add scheduler?
3. update of the global model using optimizer.
4. LR can apply durin the global model update.

SlowMo

All of the changes of LocalSGD

SHAT

All of the changes of LocalSGD +
5. C starts woth 0? 
6. c_i switches to zero after each model i aggregation?


LocalAdaScale
1. LR will update after each aggregation or not?  it calculates high value for LR (like 0.7 or even 1.1).
    However totally based on the paper specially conclusion section, only update of gain ratio is necessary.
2. G_bar becomes negative. Exactly based on the equation 15.
3. Even without G_bar negative, it causes high values for gain ratio and lr that causes NaN value for the whole model.
4. Gain ratio usually would be near K, so if normally number of epochs in other approaches is 150 for LocalAdaScale would be K*150? otherwise it finishes very fast.

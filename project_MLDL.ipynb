{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVF2_gqB0EDy",
        "outputId": "d30ab9fc-a12a-41d6-f66a-a1fd1c26307d"
      },
      "outputs": [],
      "source": [
        "# Install missing dependencies\n",
        "# !pip install -q torchinfo torchmetrics wandb\n",
        "# !pip install torch\n",
        "# !pip install torchvision\n",
        "# !pip install numpy\n",
        "# !pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGJAcU7dTxJ1"
      },
      "source": [
        "### Import important libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bnVg3M4l0EyA"
      },
      "outputs": [],
      "source": [
        "# Import required libraries/code\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import wandb\n",
        "import pickle\n",
        "\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import  random_split, DataLoader, Subset\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2mj0Wd-T73T"
      },
      "source": [
        "### Build the directory for checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQg3CXCtgB-5",
        "outputId": "7b5a2e28-b427-4b64-f013-8c1977e97423"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybM87poAtHnl"
      },
      "source": [
        "### Set device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FC7BMY9z0H3L",
        "outputId": "3a9750af-17c1-4e3c-f816-20ff79567d9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QH26cxftUt3"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "knmNFhHg0CwT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define a class for handling CIFAR100 data\n",
        "class CIFAR100Data:\n",
        "    \"\"\"\n",
        "    A class used to represent the CIFAR100 dataset.\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    batch_size : int\n",
        "        the number of samples that will be propagated through the network simultaneously\n",
        "    original_train_set : torchvision.datasets.CIFAR100\n",
        "        the original training set downloaded from CIFAR100\n",
        "    original_test_set : torchvision.datasets.CIFAR100\n",
        "        the original test set downloaded from CIFAR100\n",
        "    train_set : torch.utils.data.Subset\n",
        "        the training set after splitting the original training set\n",
        "    validation_set : torch.utils.data.Subset\n",
        "        the validation set after splitting the original training set\n",
        "    test_set : torchvision.datasets.CIFAR100\n",
        "        the test set, same as the original test set\n",
        "    original_train_loader : torch.utils.data.DataLoader\n",
        "        data loader for the original training set\n",
        "    original_test_loader : torch.utils.data.DataLoader\n",
        "        data loader for the original test set\n",
        "    train_loader : torch.utils.data.DataLoader\n",
        "        data loader for the training set\n",
        "    validation_loader : torch.utils.data.DataLoader\n",
        "        data loader for the validation set\n",
        "    test_loader : torch.utils.data.DataLoader\n",
        "        data loader for the test set\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    compute_mean_std(loader)\n",
        "        Computes the mean and standard deviation of the images in the loader.\n",
        "    download_data()\n",
        "        Downloads the CIFAR100 dataset.\n",
        "    split_data(original_train_set, validation_ratio=0.2)\n",
        "        Splits the original training set into a training set and a validation set.\n",
        "    compute_statistics(train_set)\n",
        "        Computes the mean and standard deviation of the training set.\n",
        "    apply_transforms(train_mean, train_std, is_validation_set_available = False)\n",
        "        Defines and applies the transformations for the training set, validation set, and test set.\n",
        "    save_data(data_loader, data_set, file_name: str)\n",
        "        Saves the data loader to Google Drive.\n",
        "    load_data(file_name: str)\n",
        "        Loads the data loader from Google Drive.\n",
        "    create_and_save_data_loaders(train_set, test_set, validation_set=None)\n",
        "        Creates data loaders for the training, validation, and test sets and saves them to Google Drive.\n",
        "    prepare_data(validation_ratio = None)\n",
        "        Prepares the data by downloading it, splitting it, computing statistics, applying transforms, and creating and saving data loaders.\n",
        "    train_valid_test(validation_ratio=0.2)\n",
        "        Loads or prepares the data loaders for the training, validation, and test sets and returns them.\n",
        "    train_test()\n",
        "        Loads or prepares the data loaders for the original training and test sets and returns them.\n",
        "    iid_shards(num_shards=2)\n",
        "        Loads or prepares the data loaders for the shards of the original training set and returns them.\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size=64):\n",
        "        \"\"\"\n",
        "        Initialize the CIFAR100Data object with the given batch size.\n",
        "\n",
        "        Parameters:\n",
        "        batch_size (int): The size of the batches for the data loaders.\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.original_train_set = None\n",
        "        self.original_test_set = None\n",
        "        self.train_set = None\n",
        "        self.validation_set = None\n",
        "        self.test_set = None\n",
        "\n",
        "        self.original_train_loader = None\n",
        "        self.original_test_loader = None\n",
        "        self.train_loader = None\n",
        "        self.validation_loader = None\n",
        "        self.test_loader = None\n",
        "\n",
        "    def compute_mean_std(self, loader):\n",
        "        \"\"\"\n",
        "        Compute the mean and standard deviation of the images in the loader.\n",
        "\n",
        "        Parameters:\n",
        "        loader (DataLoader): The DataLoader object containing the image data.\n",
        "\n",
        "        Returns:\n",
        "        mean (Tensor): The mean of the images.\n",
        "        std (Tensor): The standard deviation of the images.\n",
        "        \"\"\"\n",
        "        channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "        for data, _ in loader:\n",
        "            channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
        "            channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
        "            num_batches += 1\n",
        "        mean = channels_sum / num_batches\n",
        "        std = torch.sqrt((channels_squared_sum / num_batches) - mean**2)\n",
        "        return mean, std\n",
        "\n",
        "    def download_data(self):\n",
        "        \"\"\"\n",
        "        Download the CIFAR100 dataset and store it in instance variables.\n",
        "        \"\"\"\n",
        "        self.original_train_set = CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "        self.original_test_set = CIFAR100(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "    def split_data(self, original_train_set, validation_ratio=0.2):\n",
        "        \"\"\"\n",
        "        Split the original training set into a training set and a validation set.\n",
        "\n",
        "        Parameters:\n",
        "        original_train_set (Dataset): The original training set.\n",
        "        validation_ratio (float): The ratio of the original training set to use for validation.\n",
        "\n",
        "        Returns:\n",
        "        train_set (Subset): The new training set.\n",
        "        validation_set (Subset): The new validation set.\n",
        "        \"\"\"\n",
        "        train_len = int(len(original_train_set) * (1 - validation_ratio))\n",
        "        val_len = len(original_train_set) - train_len\n",
        "        train_set, validation_set = random_split(original_train_set, [train_len, val_len])\n",
        "\n",
        "        return train_set, validation_set\n",
        "\n",
        "    def compute_statistics(self, train_set):\n",
        "        \"\"\"\n",
        "        Compute the mean and standard deviation of the train set.\n",
        "\n",
        "        Parameters:\n",
        "        train_set (Dataset/Subset): The training set.\n",
        "\n",
        "        Returns:\n",
        "        train_mean (Tensor): The mean of the training set.\n",
        "        train_std (Tensor): The standard deviation of the training set.\n",
        "        \"\"\"\n",
        "        trainloader_tmp = DataLoader(train_set, batch_size=self.batch_size, shuffle=True)\n",
        "        train_mean, train_std = self.compute_mean_std(trainloader_tmp)\n",
        "\n",
        "        return train_mean, train_std\n",
        "\n",
        "    def apply_transforms(self, train_mean, train_std, is_validation_set_available = False):\n",
        "        \"\"\"\n",
        "        Define the transformations for the training set, validation set, and test set\n",
        "        and apply them to the datasets.\n",
        "\n",
        "        Parameters:\n",
        "        train_mean (Tensor): The mean of the training set.\n",
        "        train_std (Tensor): The standard deviation of the training set.\n",
        "        is_validation_set_available (bool): Whether a validation set is available.\n",
        "        \"\"\"\n",
        "\n",
        "        # Transformations for the training set\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(train_mean.tolist(), train_std.tolist())\n",
        "        ])\n",
        "\n",
        "        # Transformations for the validation and test sets\n",
        "        test_val_transforms = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(train_mean.tolist(), train_std.tolist())\n",
        "        ])\n",
        "\n",
        "        # Apply the transformations to the datasets\n",
        "        if is_validation_set_available:\n",
        "            self.train_set.transform = train_transforms\n",
        "            self.validation_set.transform = test_val_transforms\n",
        "            self.test_set.transform = test_val_transforms\n",
        "        else:\n",
        "            self.original_train_set.transform = train_transforms\n",
        "            self.original_test_set.transform = test_val_transforms\n",
        "\n",
        "    def save_data(self, data_loader, file_name: str):\n",
        "        \"\"\"\n",
        "        Save the given data loader and data set to Google Drive.\n",
        "\n",
        "        Parameters:\n",
        "        data_loader (DataLoader): The data loader to save.\n",
        "        file_name (str): The name of the file to save the data loader and data set to.\n",
        "        \"\"\"\n",
        "        # Check if the directory exists, if not, create it\n",
        "        # if not os.path.exists('/content/gdrive/MyDrive/data/data_loaders/'):\n",
        "        #     os.makedirs('/content/gdrive/MyDrive/data/data_loaders/')\n",
        "\n",
        "        # Open each file in write-binary mode on Google Drive and dump (pickle) the data loader into it\n",
        "        # with open(f'/content/gdrive/MyDrive/data/data_loaders/{self.batch_size}_{file_name}_loader.pkl', 'wb') as f:\n",
        "        #     pickle.dump(data_loader, f)\n",
        "        if not os.path.exists(f'./data/data_loaders/{self.batch_size}/'):\n",
        "            os.makedirs(f'./data/data_loaders/{self.batch_size}/')\n",
        "        \n",
        "        open(f'./data/data_loaders/{self.batch_size}/{file_name}_loader.pkl', 'wb').write(pickle.dumps(data_loader))\n",
        "\n",
        "    def load_data(self, file_name: str):\n",
        "        \"\"\"\n",
        "        Load a data loader from Google Drive.\n",
        "\n",
        "        Parameters:\n",
        "        file_name (str): The name of the file to load the data loader from.\n",
        "\n",
        "        Returns:\n",
        "        data_loader (DataLoader): The loaded data loader, or None if the file does not exist.\n",
        "        \"\"\"\n",
        "        # Check if the file exists\n",
        "        # if os.path.exists(f'/content/gdrive/MyDrive/data/data_loaders/{self.batch_size}_{file_name}_loader.pkl'):\n",
        "        #     # If it exists, open the file in read-binary mode and load (unpickle) the data loader from it\n",
        "        #     with open(f'/content/gdrive/MyDrive/data/data_loaders/{self.batch_size}_{file_name}_loader.pkl', 'rb') as f:\n",
        "        #         return pickle.load(f)\n",
        "        # else:\n",
        "        #     return None\n",
        "        if os.path.exists(f'./data/data_loaders/{self.batch_size}/{file_name}_loader.pkl'):\n",
        "            return pickle.loads(open(f'./data/data_loaders/{self.batch_size}/{file_name}_loader.pkl', 'rb').read())\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def create_and_save_data_loaders(self, train_set, test_set, train_name: str, test_name: str, validation_set=None):\n",
        "        \"\"\"\n",
        "        Create data loaders for the training, validation, and test sets and save them to Google Drive.\n",
        "\n",
        "        Parameters:\n",
        "        train_set (Subset): The training set.\n",
        "        test_set (Subset): The test set.\n",
        "        validation_set (Subset, optional): The validation set.\n",
        "\n",
        "        Returns:\n",
        "        train_loader (DataLoader): The data loader for the training set.\n",
        "        validation_loader (DataLoader): The data loader for the validation set, if it exists.\n",
        "        test_loader (DataLoader): The data loader for the test set.\n",
        "        \"\"\"\n",
        "        train_loader = DataLoader(train_set, batch_size=self.batch_size, shuffle=True, num_workers =8)\n",
        "        test_loader = DataLoader(test_set, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "        # Save the newly created data loaders to Google Drive\n",
        "        self.save_data(train_loader, train_name)\n",
        "        self.save_data(test_loader, test_name)\n",
        "\n",
        "        if validation_set is not None:\n",
        "            validation_loader = DataLoader(validation_set, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
        "            self.save_data(validation_loader, 'validation')\n",
        "            return train_loader, validation_loader, test_loader\n",
        "\n",
        "        return train_loader, test_loader\n",
        "\n",
        "    def prepare_data(self, validation_ratio = None):\n",
        "        \"\"\"\n",
        "        Prepare the data by downloading it, splitting it into training, validation, and test sets,\n",
        "        computing statistics, applying transformations, and creating and saving data loaders.\n",
        "\n",
        "        Parameters:\n",
        "        validation_ratio (float, optional): The ratio of the original training set to use for validation.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.download_data()\n",
        "        except IOError:\n",
        "            print(\"Error downloading data\")\n",
        "            return\n",
        "\n",
        "        if validation_ratio is not None:\n",
        "            self.train_set, self.validation_set = self.split_data(self.original_train_set, validation_ratio)\n",
        "            if self.validation_set is None:\n",
        "                print(\"Validation set is not available\")\n",
        "                return\n",
        "            self.test_set = self.original_test_set\n",
        "            train_mean, train_std = self.compute_statistics(self.train_set)\n",
        "            self.apply_transforms(train_mean, train_std, is_validation_set_available = True)\n",
        "\n",
        "            # Create and save data loaders\n",
        "            self.train_loader, self.validation_loader, self.test_loader = self.create_and_save_data_loaders(self.train_set, self.test_set, 'train', 'test', self.validation_set)\n",
        "\n",
        "        else:\n",
        "            train_mean, train_std = self.compute_statistics(self.original_train_set)\n",
        "            self.apply_transforms(train_mean, train_std)\n",
        "\n",
        "            # Create and save data loaders\n",
        "            self.original_train_loader, self.original_test_loader = self.create_and_save_data_loaders(self.original_train_set, self.original_test_set, 'original_train', 'original_test')\n",
        "\n",
        "    def train_valid_test(self, validation_ratio=0.2):\n",
        "        \"\"\"\n",
        "        Load the training, validation, and test data loaders from Google Drive, or prepare the data if they do not exist.\n",
        "\n",
        "        Parameters:\n",
        "        validation_ratio (float): The ratio of the original training set to use for validation.\n",
        "\n",
        "        Returns:\n",
        "        train_loader (DataLoader): The data loader for the training set.\n",
        "        validation_loader (DataLoader): The data loader for the validation set.\n",
        "        test_loader (DataLoader): The data loader for the test set.\n",
        "        \"\"\"\n",
        "        self.train_loader = self.load_data('train')\n",
        "        self.validation_loader = self.load_data('validation')\n",
        "        self.test_loader = self.load_data('test')\n",
        "\n",
        "        if self.train_loader is None or self.validation_loader is None or self.test_loader is None:\n",
        "            self.prepare_data(validation_ratio)\n",
        "\n",
        "        # Return the data loaders\n",
        "        return self.train_loader, self.validation_loader, self.test_loader\n",
        "\n",
        "    def train_test(self):\n",
        "        \"\"\"\n",
        "        Load the original training and test data loaders from Google Drive, or prepare the data if they do not exist.\n",
        "\n",
        "        Returns:\n",
        "        original_train_loader (DataLoader): The data loader for the original training set.\n",
        "        original_test_loader (DataLoader): The data loader for the original test set.\n",
        "        \"\"\"\n",
        "        self.original_train_loader = self.load_data('original_train')\n",
        "        self.original_test_loader = self.load_data('original_test')\n",
        "\n",
        "        if self.original_train_loader is None or self.original_test_loader is None:\n",
        "            self.prepare_data()\n",
        "\n",
        "        # Return the data loaders\n",
        "        return self.original_train_loader, self.original_test_loader\n",
        "\n",
        "    def iid_shards(self, num_shards=2):\n",
        "        \"\"\"\n",
        "        Create or load independent and identically distributed (IID) shards of the original training set.\n",
        "\n",
        "        Parameters:\n",
        "        num_shards (int): The number of shards to create.\n",
        "\n",
        "        Returns:\n",
        "        shard_loaders (list of DataLoader): The data loaders for the shards.\n",
        "        \"\"\"\n",
        "        # Try to load the shard datasets and their corresponding data loaders from Google Drive\n",
        "        shard_loaders = []\n",
        "        for i in range(num_shards):\n",
        "            shard_loader = self.load_data(f'iid_sharding/{num_shards}_chunk_{i+1}')\n",
        "            if shard_loader is None:\n",
        "                break\n",
        "            shard_loaders.append(shard_loader)\n",
        "\n",
        "        # If all shard data loaders were successfully loaded, return them\n",
        "        if len(shard_loaders) == num_shards:\n",
        "            return shard_loaders\n",
        "\n",
        "        # If not all shard data loaders were successfully loaded, create them\n",
        "        if self.original_train_set is None:\n",
        "            self.download_data()\n",
        "            train_mean, train_std = self.compute_statistics(self.original_train_set)\n",
        "            self.apply_transforms(train_mean, train_std)\n",
        "\n",
        "        # Shuffle the indices\n",
        "        indices = torch.randperm(len(self.original_train_set))\n",
        "\n",
        "        # Split the indices into K chunks\n",
        "        shard_size = len(indices) // num_shards\n",
        "        shards = [indices[i*shard_size:(i+1)*shard_size] for i in range(num_shards)]\n",
        "\n",
        "        # Create subsets for each shard\n",
        "        shard_datasets = [Subset(self.original_train_set, shard) for shard in shards]\n",
        "\n",
        "        # Create data loaders for each shard\n",
        "        shard_loaders = [DataLoader(shard_dataset, batch_size=self.batch_size, shuffle=True) for shard_dataset in shard_datasets]\n",
        "\n",
        "        # Save each shard dataset and its corresponding data loader\n",
        "        for i, shard_loader in enumerate(shard_loaders):\n",
        "            self.save_data(shard_loader, f'iid_sharding_{num_shards}_chunk_{i+1}')\n",
        "\n",
        "        return shard_loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KE_ErnYB06JF"
      },
      "outputs": [],
      "source": [
        "data = CIFAR100Data()\n",
        "train_loader, validation_loader, test_loader = data.train_valid_test(validation_ratio=0.2)\n",
        "original_train_loader, original_test_loader = data.train_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fjzE9q4UOPU"
      },
      "source": [
        "### Define the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KaLfzyjo0Pzk"
      },
      "outputs": [],
      "source": [
        "# Define the model architecture\n",
        "# Check if it is LeNet-5 or similar to LeNet-5 we want similar.\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=0) # 28x28\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=0) # 10x10\n",
        "        self.pool = nn.MaxPool2d(2, 2) # 14x14 for conv1 and 5x5 for conv2\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(5 * 5 * 64, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x, indexing=None):\n",
        "        intermediate_outputs = {}\n",
        "\n",
        "        # Convolutional and pooling layers\n",
        "        x = F.relu(self.conv1(x))\n",
        "        if indexing == 'conv1':\n",
        "            return x\n",
        "        intermediate_outputs['conv1'] = x\n",
        "\n",
        "        x = self.pool(x)\n",
        "        if indexing == 'pool1':\n",
        "            return x\n",
        "        intermediate_outputs['pool1'] = x\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        if indexing == 'conv2':\n",
        "            return x\n",
        "        intermediate_outputs['conv2'] = x\n",
        "\n",
        "        x = self.pool(x)\n",
        "        if indexing == 'pool2':\n",
        "            return x\n",
        "        intermediate_outputs['pool2'] = x\n",
        "\n",
        "        # Flatten layer\n",
        "        x = self.flatten(x)\n",
        "        if indexing == 'flatten':\n",
        "            return x\n",
        "        intermediate_outputs['flatten'] = x\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        if indexing == 'fc1':\n",
        "            return x\n",
        "        intermediate_outputs['fc1'] = x\n",
        "\n",
        "        x = F.relu(self.fc2(x))\n",
        "        if indexing == 'fc2':\n",
        "            return x\n",
        "        intermediate_outputs['fc2'] = x\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        if indexing == 'fc3':\n",
        "            return x\n",
        "        intermediate_outputs['fc3'] = x\n",
        "\n",
        "        # If no indexing, return final output\n",
        "        return x if indexing is None else intermediate_outputs.get(indexing, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-4HtbSQ20ea5"
      },
      "outputs": [],
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEseiEKFt0mR"
      },
      "source": [
        "### Define some basic functions for train and test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JZNSgcBp0jEl"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, loss_fn, accumulation_steps=1, device=device, is_wandb=False):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    optimizer.zero_grad()  # Initialize gradients to zero at the start of each epoch\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        loss.backward()  # Backpropagation\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # Gradient accumulation\n",
        "        if (i+1) % accumulation_steps == 0 or i+1 == len(dataloader):\n",
        "            optimizer.step()  # Update model parameters\n",
        "            optimizer.zero_grad()  # Reset gradients to zero\n",
        "\n",
        "    train_loss = running_loss / len(dataloader)\n",
        "    train_accuracy = 100. * correct / total\n",
        "\n",
        "    if is_wandb:\n",
        "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": train_accuracy})\n",
        "\n",
        "    return train_loss, train_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m6dkKxXh0i7n"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader, loss_fn, device = device, is_wandb= False):\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if is_wandb:\n",
        "              # Log the loss and accuracy values at each step\n",
        "              wandb.log({\n",
        "                  'Test Loss': test_loss / (batch_idx + 1),\n",
        "                  'Test Accuracy': 100 * correct / total\n",
        "              })\n",
        "\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_accuracy = 100. * correct / total\n",
        "\n",
        "    return test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZD7Vge_wYwvX"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, epoch, batch_size, optimizer_name, hyperparameters):\n",
        "    # dir_path = f\"/content/gdrive/MyDrive/{optimizer_name}/{batch_size}/\"\n",
        "    dir_path = f\"./train/{optimizer_name}/{batch_size}/\"\n",
        "    for key, value in hyperparameters.items():\n",
        "        dir_path = os.path.join(dir_path, f\"{key}_{value}/\")\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "    path = os.path.join(dir_path, f\"epoch_{epoch:03}.pt\")\n",
        "    torch.save(state, path)\n",
        "\n",
        "    # Get list of all files\n",
        "    list_of_files = glob.glob(os.path.join(dir_path, f\"epoch_*.pt\"))\n",
        "    # Sort files by creation time\n",
        "    list_of_files.sort(key=os.path.getctime)\n",
        "    # If there are more than 2 files, delete the second last one\n",
        "    if len(list_of_files) > 1:\n",
        "        os.remove(list_of_files[-2])\n",
        "\n",
        "def load_checkpoint(optimizer_name, batch_size, hyperparameters):\n",
        "    # dir_path = f\"/content/gdrive/MyDrive/{optimizer_name}/{batch_size}/\"\n",
        "    dir_path = f\"./train/{optimizer_name}/{batch_size}/\"\n",
        "    for key, value in hyperparameters.items():\n",
        "        dir_path = os.path.join(dir_path, f\"{key}_{value}/\")\n",
        "    list_of_files = glob.glob(os.path.join(dir_path, f\"epoch_*.pt\")) # * means all if need specific format then *.csv\n",
        "    if not list_of_files:  # I'm using glob which can return an empty list\n",
        "        return None\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    if os.path.isfile(latest_file):\n",
        "        return torch.load(latest_file)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9Yjx-yZf-Sbz"
      },
      "outputs": [],
      "source": [
        "def run_training(\n",
        "    num_epochs,\n",
        "    model,\n",
        "    trainloader,\n",
        "    validationloader,\n",
        "    testloader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    optimizer_name: str,\n",
        "    accumulation_steps=1,\n",
        "    hyperparameters=None,\n",
        "    is_wandb = False,\n",
        "    n_epochs_stop = None,\n",
        "    warmup_ratio = 0\n",
        "  ):\n",
        "\n",
        "  best_accuracy = 0\n",
        "  epochs_no_improve = 0\n",
        "  warmup_steps = int(warmup_ratio * num_epochs)\n",
        "  lr = optimizer.param_groups[0]['lr']\n",
        "  \n",
        "  start_epoch = 0\n",
        "  run_id = None\n",
        "  run_name = None\n",
        "  # Load checkpoint if available\n",
        "  checkpoint = load_checkpoint(optimizer_name, trainloader.batch_size, hyperparameters)\n",
        "  if checkpoint is not None:\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      start_epoch = checkpoint['epoch'] + 1\n",
        "      run_id = checkpoint.get('wandb_run_id', None)\n",
        "      run_name = checkpoint.get('wandb_run_name', None)\n",
        "\n",
        "  else:\n",
        "    run_name = \" \".join([f\"{key}={value}\" for key, value in hyperparameters.items()])\n",
        "\n",
        "  if is_wandb:\n",
        "    # Initialize a wandb run with the given hyperparameters\n",
        "    wandb.init(id=run_id, name=run_name, project=f'cifar100-training-mldl2024-baseline-{optimizer_name}',\n",
        "                   config=hyperparameters if hyperparameters is not None else {},\n",
        "                   resume=\"allow\", reinit=True)\n",
        "\n",
        "  for epoch in range(start_epoch, num_epochs):\n",
        "      if epoch < warmup_steps:\n",
        "        optimizer.param_groups[0]['lr'] = lr * epoch / warmup_steps\n",
        "      elif epoch == warmup_steps:\n",
        "        optimizer.param_groups[0]['lr'] = lr\n",
        "          \n",
        "      # Call the training function for each epoch\n",
        "      train_loss, train_acc = train(model, trainloader, optimizer, loss_fn, accumulation_steps, device, is_wandb=is_wandb)\n",
        "      print(f'[{epoch+1}/{num_epochs}]: Training Loss: {train_loss}, Training Accuracy: {train_acc}')\n",
        "      scheduler.step() # Update learning rate based on scheduler\n",
        "\n",
        "      # Save checkpoint\n",
        "      save_checkpoint({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),\n",
        "                    'loss': criterion,\n",
        "                    'wandb_run_id': wandb.run.id if is_wandb else None,\n",
        "                    'wandb_run_name': wandb.run.name if is_wandb else None,\n",
        "                    }, epoch, trainloader.batch_size, optimizer_name, hyperparameters)\n",
        "\n",
        "      if validationloader is not None:\n",
        "        val_loss, val_acc = test(model, validationloader, criterion)\n",
        "        print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_acc}')\n",
        "\n",
        "\n",
        "        if n_epochs_stop is not None:\n",
        "          if val_acc > best_accuracy:\n",
        "                best_accuracy = val_acc\n",
        "                epochs_no_improve = 0\n",
        "          else:\n",
        "              epochs_no_improve += 1\n",
        "              if epochs_no_improve == n_epochs_stop:\n",
        "                  print('Early stopping!')\n",
        "                  break\n",
        "\n",
        "  print('*'*70)\n",
        "  test_loss, test_acc = test(model, testloader, criterion, is_wandb = is_wandb)\n",
        "  print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')\n",
        "\n",
        "\n",
        "  # Finish the wandb run after all epochs\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taX_5ElNuKxC"
      },
      "source": [
        "# **Centeralised baseline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "8hxWmieIE7jF"
      },
      "outputs": [],
      "source": [
        "\n",
        "learning_rates = [1e-03, 1e-02]\n",
        "weight_decays = [1e-04, 1e-03, 4e-04]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SGDM (Stochastic Gradient Descent with Momentum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8587c890f2a847e293a101405d64fc0d",
            "175043eb44e44b4f9aefaf51aae6fb10",
            "db36d0cc7691440a9792ac779e161e62",
            "c6a68af25a2847e98201847781419670",
            "71e710b571d142a3a08cd233590b7171",
            "2671c3bf3cfd49aa9ad6841c289068cc",
            "45bdcbbed9ae47ffb24bd2a962345eaa",
            "bcbeb5bacc824f3aacd20e50d38bb70a",
            "2991ead42a2a4637b2902ca26837d74b",
            "9e0bb90e33ec4e198857c255080ef6f0",
            "77790c4dcd124f73907619689fb8d37c",
            "1fef0782f0804736881f03c2e31c3d64",
            "3606009883cf4212b7e1ed6e4f182845",
            "39aecb8ffe0645fbadccf8b0f8ed2486",
            "13e7a0db2aa74e35b7d1948224358d76",
            "7064cd1e5d794c258dbeeb63a9ae9f9b",
            "5cf516644d1a4ebf82c7e9dfa13e560b",
            "36a546b812d6403cbf5ed693318a9744",
            "aed05a015df946c7bae33226c3a8ddc6",
            "d601ef3ebb6c4b688ac53a65979aa445",
            "e4a5f301338a4e5185c042a22684ed4a",
            "48c7de7682f242fd86a234d1607187f4",
            "403dcd5f10de42ddbb67c8108e3bc34c",
            "a1d013e4d3e942b6b362ab5ade6d1a80",
            "e851c69b66ca4e8a80779a69435b855f",
            "4b733fc6800e4a0ab36bba52ff84f1a5",
            "dcbb358118e644a2a75d44d8adb6747b",
            "829bba2a5bb947a0bcf121c527902255",
            "7e2cc873f917444c833e152f7c2555de",
            "49727de0590f41dca6addf3f4ffe33f2",
            "60957598b6c64ab0987d583b788dd674",
            "e4ad17a7648245069c70f2f3fb79105a",
            "431cf75dd1cc4840b14f678876f45bc9",
            "a5f3871113994f1eb993c126d793d147",
            "7903555c1e884408b70c9951ecae84ec",
            "2d65b320aaed40cd87b8f78c99c614e7",
            "346856a22f0e45b2be0c14aa7902261f",
            "397b6d4daaf04a1181413adc5e30db0c",
            "ac87babb2f2a4781ab863c3b8f613807",
            "b3970cfd36ed4450b7c8f958a6499dc4",
            "df97494c2f654ffbaceb9ca801ff3113",
            "e1e5353cb6a1427a8b0b9d91d6067ef7",
            "77c702b6d9c04f00b8e624f31a591f01",
            "8795a05c814c469083b31be62e79289f",
            "829eee71e4d74403a22941b15dfcf9bc",
            "ab0b891f55f648cdbace9bc4540c51c6",
            "e6d67121f0ab4daf96d0bdd2c433259b",
            "3ee2c346d4d74faa915ef8315e3303c8"
          ]
        },
        "id": "9rGKhc7FJB8u",
        "outputId": "9b4eddc5-dc51-4af2-fdbe-4ba3175d3642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240611_235508-gw8sn909</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909' target=\"_blank\">learning_rate=0.001 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10/150]: Training Loss: 3.7955315692901612, Training Accuracy: 12.3825\n",
            "Validation Loss: 3.7704276780413974, Validation Accuracy: 12.97\n",
            "[11/150]: Training Loss: 3.693796951675415, Training Accuracy: 14.2675\n",
            "Validation Loss: 3.6661312990127857, Validation Accuracy: 14.76\n",
            "[12/150]: Training Loss: 3.5919596023559572, Training Accuracy: 16.0125\n",
            "Validation Loss: 3.5912161389733575, Validation Accuracy: 16.26\n",
            "[13/150]: Training Loss: 3.5116733703613283, Training Accuracy: 17.3775\n",
            "Validation Loss: 3.5196323546634356, Validation Accuracy: 16.95\n",
            "[14/150]: Training Loss: 3.4402521675109865, Training Accuracy: 18.835\n",
            "Validation Loss: 3.4998342930131656, Validation Accuracy: 17.78\n",
            "[15/150]: Training Loss: 3.374294019317627, Training Accuracy: 19.835\n",
            "Validation Loss: 3.412280123704558, Validation Accuracy: 19.05\n",
            "[16/150]: Training Loss: 3.3126377914428713, Training Accuracy: 20.9225\n",
            "Validation Loss: 3.3567575497232425, Validation Accuracy: 20.21\n",
            "[17/150]: Training Loss: 3.264142190551758, Training Accuracy: 21.7475\n",
            "Validation Loss: 3.3158142976700122, Validation Accuracy: 20.54\n",
            "[18/150]: Training Loss: 3.212448757171631, Training Accuracy: 22.925\n",
            "Validation Loss: 3.288792388454364, Validation Accuracy: 21.5\n",
            "[19/150]: Training Loss: 3.16608261680603, Training Accuracy: 23.6325\n",
            "Validation Loss: 3.247776625262704, Validation Accuracy: 22.28\n",
            "[20/150]: Training Loss: 3.1185284183502198, Training Accuracy: 24.565\n",
            "Validation Loss: 3.2615917640127194, Validation Accuracy: 21.82\n",
            "[21/150]: Training Loss: 3.0692719429016115, Training Accuracy: 25.24\n",
            "Validation Loss: 3.188312981538712, Validation Accuracy: 23.06\n",
            "[22/150]: Training Loss: 3.0227535221099853, Training Accuracy: 26.3125\n",
            "Validation Loss: 3.189725389905796, Validation Accuracy: 23.3\n",
            "[23/150]: Training Loss: 2.9822758083343506, Training Accuracy: 26.9425\n",
            "Validation Loss: 3.1865678501736587, Validation Accuracy: 24.06\n",
            "[24/150]: Training Loss: 2.9383605140686035, Training Accuracy: 27.7575\n",
            "Validation Loss: 3.1210442075304163, Validation Accuracy: 25.28\n",
            "[25/150]: Training Loss: 2.8941220123291016, Training Accuracy: 28.8275\n",
            "Validation Loss: 3.0783458576080904, Validation Accuracy: 26.38\n",
            "[26/150]: Training Loss: 2.846640188598633, Training Accuracy: 29.6625\n",
            "Validation Loss: 3.0489486524253895, Validation Accuracy: 26.15\n",
            "[27/150]: Training Loss: 2.803463589859009, Training Accuracy: 30.53\n",
            "Validation Loss: 3.0650304320511546, Validation Accuracy: 26.3\n",
            "[28/150]: Training Loss: 2.7623793058395387, Training Accuracy: 31.4175\n",
            "Validation Loss: 3.016696295161156, Validation Accuracy: 27.16\n",
            "[29/150]: Training Loss: 2.724277519607544, Training Accuracy: 31.9075\n",
            "Validation Loss: 3.001434786304547, Validation Accuracy: 27.8\n",
            "[30/150]: Training Loss: 2.6789874454498293, Training Accuracy: 33.0275\n",
            "Validation Loss: 3.016883801502787, Validation Accuracy: 27.3\n",
            "[31/150]: Training Loss: 2.639483213806152, Training Accuracy: 33.66\n",
            "Validation Loss: 2.98876147361318, Validation Accuracy: 28.67\n",
            "[32/150]: Training Loss: 2.5927667430877683, Training Accuracy: 34.5325\n",
            "Validation Loss: 2.994789396881298, Validation Accuracy: 27.54\n",
            "[33/150]: Training Loss: 2.5531089670181273, Training Accuracy: 35.2525\n",
            "Validation Loss: 2.997880709399084, Validation Accuracy: 28.54\n",
            "[34/150]: Training Loss: 2.5080887552261353, Training Accuracy: 36.16\n",
            "Validation Loss: 2.9557720460709493, Validation Accuracy: 29.05\n",
            "[35/150]: Training Loss: 2.470765545463562, Training Accuracy: 37.03\n",
            "Validation Loss: 2.960252163516488, Validation Accuracy: 29.2\n",
            "[36/150]: Training Loss: 2.4253519346237185, Training Accuracy: 37.9225\n",
            "Validation Loss: 2.9324972644733016, Validation Accuracy: 29.32\n",
            "[37/150]: Training Loss: 2.3800090463638304, Training Accuracy: 39.06\n",
            "Validation Loss: 2.9341223923264037, Validation Accuracy: 30.21\n",
            "[38/150]: Training Loss: 2.3413858253479005, Training Accuracy: 39.57\n",
            "Validation Loss: 2.9640257130762575, Validation Accuracy: 29.88\n",
            "[39/150]: Training Loss: 2.301597819519043, Training Accuracy: 40.4175\n",
            "Validation Loss: 3.0218158800890493, Validation Accuracy: 28.21\n",
            "[40/150]: Training Loss: 2.2570854791641235, Training Accuracy: 41.3725\n",
            "Validation Loss: 3.0180870347721562, Validation Accuracy: 29.27\n",
            "[41/150]: Training Loss: 2.2198365371704103, Training Accuracy: 42.375\n",
            "Validation Loss: 2.938889775306556, Validation Accuracy: 30.41\n",
            "[42/150]: Training Loss: 2.1788083906173705, Training Accuracy: 43.24\n",
            "Validation Loss: 2.931537726882157, Validation Accuracy: 30.24\n",
            "[43/150]: Training Loss: 2.130757416725159, Training Accuracy: 44.2975\n",
            "Validation Loss: 2.935111481672639, Validation Accuracy: 30.94\n",
            "[44/150]: Training Loss: 2.0930950580596925, Training Accuracy: 45.0275\n",
            "Validation Loss: 2.9872301232283283, Validation Accuracy: 30.88\n",
            "[45/150]: Training Loss: 2.0451603315353393, Training Accuracy: 46.14\n",
            "Validation Loss: 2.9818537857881777, Validation Accuracy: 30.2\n",
            "[46/150]: Training Loss: 2.004084638786316, Training Accuracy: 47.0275\n",
            "Validation Loss: 2.995785168022107, Validation Accuracy: 30.23\n",
            "[47/150]: Training Loss: 1.9692718086242675, Training Accuracy: 47.845\n",
            "Validation Loss: 3.0542795536624396, Validation Accuracy: 30.14\n",
            "[48/150]: Training Loss: 1.9216952280044555, Training Accuracy: 48.8675\n",
            "Validation Loss: 2.9834766873888148, Validation Accuracy: 31.12\n",
            "[49/150]: Training Loss: 1.8784988208770752, Training Accuracy: 49.9625\n",
            "Validation Loss: 3.0277192516691365, Validation Accuracy: 31.11\n",
            "[50/150]: Training Loss: 1.8381427211761474, Training Accuracy: 50.9275\n",
            "Validation Loss: 3.062338437244391, Validation Accuracy: 31.08\n",
            "[51/150]: Training Loss: 1.7901163187026978, Training Accuracy: 52.055\n",
            "Validation Loss: 3.1062804225144114, Validation Accuracy: 30.8\n",
            "[52/150]: Training Loss: 1.7526374937057496, Training Accuracy: 52.695\n",
            "Validation Loss: 3.121828522651818, Validation Accuracy: 31.1\n",
            "[53/150]: Training Loss: 1.7035991048812866, Training Accuracy: 53.87\n",
            "Validation Loss: 3.1324675629852683, Validation Accuracy: 30.77\n",
            "[54/150]: Training Loss: 1.6695509649276734, Training Accuracy: 54.6\n",
            "Validation Loss: 3.1344476976212423, Validation Accuracy: 30.74\n",
            "[55/150]: Training Loss: 1.6191298002243042, Training Accuracy: 55.795\n",
            "Validation Loss: 3.2295576159361823, Validation Accuracy: 29.87\n",
            "[56/150]: Training Loss: 1.5762285459518433, Training Accuracy: 56.7875\n",
            "Validation Loss: 3.242111048121361, Validation Accuracy: 30.56\n",
            "[57/150]: Training Loss: 1.5393764123916627, Training Accuracy: 58.005\n",
            "Validation Loss: 3.303940733526922, Validation Accuracy: 29.53\n",
            "[58/150]: Training Loss: 1.4886947209358214, Training Accuracy: 59.32\n",
            "Validation Loss: 3.306815612088343, Validation Accuracy: 30.63\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 12.757227435992782, Test Accuracy: 12.53\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>██▁▂▂▂▂▁▁▁▁▂▂▂▃▃▃▃▃▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>Test Loss</td><td>▁▄▆▇▇▇███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>Train Loss</td><td>██▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>12.53</td></tr><tr><td>Test Loss</td><td>12.75723</td></tr><tr><td>Train Accuracy</td><td>59.32</td></tr><tr><td>Train Loss</td><td>1.48869</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240611_235508-gw8sn909/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240611_235923-9i8aijvl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl' target=\"_blank\">learning_rate=0.001 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605354942321777, Training Accuracy: 1.0075\n",
            "Validation Loss: 4.604883777108162, Validation Accuracy: 0.9\n",
            "[2/150]: Training Loss: 4.602973918151855, Training Accuracy: 1.0475\n",
            "Validation Loss: 4.6016397293965525, Validation Accuracy: 0.9\n",
            "[3/150]: Training Loss: 4.596875128936768, Training Accuracy: 1.3425\n",
            "Validation Loss: 4.590152442834939, Validation Accuracy: 2.52\n",
            "[4/150]: Training Loss: 4.556022624969483, Training Accuracy: 2.9175\n",
            "Validation Loss: 4.472016604842653, Validation Accuracy: 3.55\n",
            "[5/150]: Training Loss: 4.280242394256592, Training Accuracy: 4.6625\n",
            "Validation Loss: 4.20257385673037, Validation Accuracy: 5.95\n",
            "[6/150]: Training Loss: 4.122820203781128, Training Accuracy: 6.5925\n",
            "Validation Loss: 4.098185727550725, Validation Accuracy: 6.72\n",
            "[7/150]: Training Loss: 4.0448949375152585, Training Accuracy: 7.655\n",
            "Validation Loss: 4.033748802865387, Validation Accuracy: 8.54\n",
            "[8/150]: Training Loss: 3.977578921508789, Training Accuracy: 9.06\n",
            "Validation Loss: 3.970679567118359, Validation Accuracy: 8.73\n",
            "[9/150]: Training Loss: 3.9144488010406495, Training Accuracy: 9.9875\n",
            "Validation Loss: 3.902484763200116, Validation Accuracy: 10.84\n",
            "[10/150]: Training Loss: 3.8504066452026366, Training Accuracy: 11.2025\n",
            "Validation Loss: 3.853308222096437, Validation Accuracy: 11.05\n",
            "[11/150]: Training Loss: 3.776761824798584, Training Accuracy: 12.605\n",
            "Validation Loss: 3.7683646602995076, Validation Accuracy: 13.16\n",
            "[12/150]: Training Loss: 3.6920017036437986, Training Accuracy: 14.09\n",
            "Validation Loss: 3.712872941023225, Validation Accuracy: 13.54\n",
            "[13/150]: Training Loss: 3.617317741394043, Training Accuracy: 15.2475\n",
            "Validation Loss: 3.619570322097487, Validation Accuracy: 14.95\n",
            "[14/150]: Training Loss: 3.5433271072387695, Training Accuracy: 16.7275\n",
            "Validation Loss: 3.5456171734317854, Validation Accuracy: 16.81\n",
            "[15/150]: Training Loss: 3.47977327041626, Training Accuracy: 17.6525\n",
            "Validation Loss: 3.503120666856219, Validation Accuracy: 17.12\n",
            "[16/150]: Training Loss: 3.4149503967285155, Training Accuracy: 18.85\n",
            "Validation Loss: 3.449874220380358, Validation Accuracy: 17.98\n",
            "[17/150]: Training Loss: 3.357447999191284, Training Accuracy: 20.3025\n",
            "Validation Loss: 3.3906334479143667, Validation Accuracy: 19.6\n",
            "[18/150]: Training Loss: 3.3055746353149416, Training Accuracy: 20.985\n",
            "Validation Loss: 3.34384480859064, Validation Accuracy: 20.15\n",
            "[19/150]: Training Loss: 3.249339769363403, Training Accuracy: 21.955\n",
            "Validation Loss: 3.3408105813773576, Validation Accuracy: 20.75\n",
            "[20/150]: Training Loss: 3.2052097175598147, Training Accuracy: 22.84\n",
            "Validation Loss: 3.3127595497544404, Validation Accuracy: 21.29\n",
            "[21/150]: Training Loss: 3.150541979598999, Training Accuracy: 23.8475\n",
            "Validation Loss: 3.267914258750381, Validation Accuracy: 22.13\n",
            "[22/150]: Training Loss: 3.1069109004974367, Training Accuracy: 24.64\n",
            "Validation Loss: 3.2093709350391557, Validation Accuracy: 23.03\n",
            "[23/150]: Training Loss: 3.0725105766296386, Training Accuracy: 25.155\n",
            "Validation Loss: 3.2081045861456805, Validation Accuracy: 23.23\n",
            "[24/150]: Training Loss: 3.0264828220367432, Training Accuracy: 26.165\n",
            "Validation Loss: 3.153175331225061, Validation Accuracy: 23.9\n",
            "[25/150]: Training Loss: 2.9784529972076417, Training Accuracy: 27.0575\n",
            "Validation Loss: 3.120233716478773, Validation Accuracy: 24.62\n",
            "[26/150]: Training Loss: 2.9407661106109617, Training Accuracy: 27.7025\n",
            "Validation Loss: 3.122152067293787, Validation Accuracy: 25.03\n",
            "[27/150]: Training Loss: 2.9034343135833742, Training Accuracy: 28.395\n",
            "Validation Loss: 3.062598638473802, Validation Accuracy: 25.75\n",
            "[28/150]: Training Loss: 2.859911437988281, Training Accuracy: 29.1425\n",
            "Validation Loss: 3.0851597072212558, Validation Accuracy: 26.24\n",
            "[29/150]: Training Loss: 2.825396254348755, Training Accuracy: 29.9575\n",
            "Validation Loss: 3.0798455939930713, Validation Accuracy: 25.39\n",
            "[30/150]: Training Loss: 2.783854722213745, Training Accuracy: 30.46\n",
            "Validation Loss: 2.994018077850342, Validation Accuracy: 27.4\n",
            "[31/150]: Training Loss: 2.7530001544952394, Training Accuracy: 31.445\n",
            "Validation Loss: 3.023551480785297, Validation Accuracy: 26.3\n",
            "[32/150]: Training Loss: 2.715887685775757, Training Accuracy: 31.8725\n",
            "Validation Loss: 2.966492583037941, Validation Accuracy: 27.56\n",
            "[33/150]: Training Loss: 2.673046026611328, Training Accuracy: 32.72\n",
            "Validation Loss: 2.9745204934648646, Validation Accuracy: 27.99\n",
            "[34/150]: Training Loss: 2.6407437208175657, Training Accuracy: 33.34\n",
            "Validation Loss: 2.942354439170497, Validation Accuracy: 29.0\n",
            "[35/150]: Training Loss: 2.5946556980133058, Training Accuracy: 34.7025\n",
            "Validation Loss: 2.930390673837844, Validation Accuracy: 28.88\n",
            "[36/150]: Training Loss: 2.560257712173462, Training Accuracy: 35.2025\n",
            "Validation Loss: 2.9051667292406607, Validation Accuracy: 29.88\n",
            "[37/150]: Training Loss: 2.5238379081726072, Training Accuracy: 35.76\n",
            "Validation Loss: 2.890098687190159, Validation Accuracy: 29.33\n",
            "[38/150]: Training Loss: 2.486947275352478, Training Accuracy: 36.69\n",
            "Validation Loss: 2.9050150206134577, Validation Accuracy: 29.6\n",
            "[39/150]: Training Loss: 2.452709559249878, Training Accuracy: 37.475\n",
            "Validation Loss: 2.8899250182376544, Validation Accuracy: 29.65\n",
            "[40/150]: Training Loss: 2.4208646841049193, Training Accuracy: 37.9575\n",
            "Validation Loss: 2.9285842643421924, Validation Accuracy: 29.49\n",
            "[41/150]: Training Loss: 2.3780534410476686, Training Accuracy: 38.925\n",
            "Validation Loss: 2.896095122501349, Validation Accuracy: 30.27\n",
            "[42/150]: Training Loss: 2.3445595056533812, Training Accuracy: 39.4975\n",
            "Validation Loss: 2.903249968389037, Validation Accuracy: 29.49\n",
            "[43/150]: Training Loss: 2.3128848968505857, Training Accuracy: 40.2375\n",
            "Validation Loss: 2.8930906854617366, Validation Accuracy: 30.18\n",
            "[44/150]: Training Loss: 2.275460436248779, Training Accuracy: 40.965\n",
            "Validation Loss: 2.847154028096776, Validation Accuracy: 30.73\n",
            "[45/150]: Training Loss: 2.2384806400299073, Training Accuracy: 41.705\n",
            "Validation Loss: 2.898577917912963, Validation Accuracy: 30.18\n",
            "[46/150]: Training Loss: 2.214237283706665, Training Accuracy: 42.415\n",
            "Validation Loss: 2.8367908441337053, Validation Accuracy: 31.94\n",
            "[47/150]: Training Loss: 2.174029080581665, Training Accuracy: 43.2825\n",
            "Validation Loss: 2.8599718254842577, Validation Accuracy: 31.4\n",
            "[48/150]: Training Loss: 2.134048504257202, Training Accuracy: 44.0225\n",
            "Validation Loss: 2.8570211207031444, Validation Accuracy: 31.32\n",
            "[49/150]: Training Loss: 2.1011022747039796, Training Accuracy: 44.7675\n",
            "Validation Loss: 2.876888126324696, Validation Accuracy: 31.24\n",
            "[50/150]: Training Loss: 2.066174629211426, Training Accuracy: 45.84\n",
            "Validation Loss: 2.873898560833779, Validation Accuracy: 31.01\n",
            "[51/150]: Training Loss: 2.0310755935668947, Training Accuracy: 46.2775\n",
            "Validation Loss: 2.8654664003165666, Validation Accuracy: 31.98\n",
            "[52/150]: Training Loss: 1.9909400806427002, Training Accuracy: 47.45\n",
            "Validation Loss: 2.8746210936528103, Validation Accuracy: 31.69\n",
            "[53/150]: Training Loss: 1.9579231163024902, Training Accuracy: 48.0525\n",
            "Validation Loss: 2.8748430902031576, Validation Accuracy: 31.57\n",
            "[54/150]: Training Loss: 1.9230639179229736, Training Accuracy: 48.785\n",
            "Validation Loss: 2.910501542364716, Validation Accuracy: 31.56\n",
            "[55/150]: Training Loss: 1.8876561931610107, Training Accuracy: 49.645\n",
            "Validation Loss: 2.8805068404811203, Validation Accuracy: 32.02\n",
            "[56/150]: Training Loss: 1.8495587772369384, Training Accuracy: 50.57\n",
            "Validation Loss: 2.9215301510634695, Validation Accuracy: 31.68\n",
            "[57/150]: Training Loss: 1.8102108228683471, Training Accuracy: 51.2975\n",
            "Validation Loss: 2.926473318391545, Validation Accuracy: 32.35\n",
            "[58/150]: Training Loss: 1.7818908670425415, Training Accuracy: 52.1325\n",
            "Validation Loss: 2.9246792929947, Validation Accuracy: 32.57\n",
            "[59/150]: Training Loss: 1.7456034435272216, Training Accuracy: 52.775\n",
            "Validation Loss: 2.984062427168439, Validation Accuracy: 31.53\n",
            "[60/150]: Training Loss: 1.706142727279663, Training Accuracy: 53.955\n",
            "Validation Loss: 2.9781496843714623, Validation Accuracy: 32.57\n",
            "[61/150]: Training Loss: 1.6744635740280152, Training Accuracy: 54.59\n",
            "Validation Loss: 3.0356672569444982, Validation Accuracy: 31.58\n",
            "[62/150]: Training Loss: 1.6349090488433837, Training Accuracy: 55.5975\n",
            "Validation Loss: 3.0081513763233354, Validation Accuracy: 32.09\n",
            "[63/150]: Training Loss: 1.6007863130569457, Training Accuracy: 56.41\n",
            "Validation Loss: 3.032350327558578, Validation Accuracy: 32.3\n",
            "[64/150]: Training Loss: 1.5609112140655517, Training Accuracy: 57.235\n",
            "Validation Loss: 3.077729872077893, Validation Accuracy: 31.42\n",
            "[65/150]: Training Loss: 1.5253083936691285, Training Accuracy: 58.1475\n",
            "Validation Loss: 3.0732312840261278, Validation Accuracy: 32.72\n",
            "[66/150]: Training Loss: 1.4928287380218506, Training Accuracy: 59.1925\n",
            "Validation Loss: 3.0616249688871346, Validation Accuracy: 31.97\n",
            "[67/150]: Training Loss: 1.4535140877723693, Training Accuracy: 60.1225\n",
            "Validation Loss: 3.124178699627044, Validation Accuracy: 32.15\n",
            "[68/150]: Training Loss: 1.4196137544631957, Training Accuracy: 60.7675\n",
            "Validation Loss: 3.2008153435530935, Validation Accuracy: 31.3\n",
            "[69/150]: Training Loss: 1.3877691086769104, Training Accuracy: 61.4575\n",
            "Validation Loss: 3.1977471834535054, Validation Accuracy: 31.22\n",
            "[70/150]: Training Loss: 1.3564362874031066, Training Accuracy: 62.2125\n",
            "Validation Loss: 3.2483551836317512, Validation Accuracy: 31.49\n",
            "[71/150]: Training Loss: 1.3180213891029358, Training Accuracy: 63.6275\n",
            "Validation Loss: 3.3253093616218323, Validation Accuracy: 31.54\n",
            "[72/150]: Training Loss: 1.2782609523773194, Training Accuracy: 64.6325\n",
            "Validation Loss: 3.300645703722717, Validation Accuracy: 32.03\n",
            "[73/150]: Training Loss: 1.2435202094078064, Training Accuracy: 65.5975\n",
            "Validation Loss: 3.340803805430224, Validation Accuracy: 31.19\n",
            "[74/150]: Training Loss: 1.2111891516685487, Training Accuracy: 66.105\n",
            "Validation Loss: 3.3482626076716526, Validation Accuracy: 31.98\n",
            "[75/150]: Training Loss: 1.1782402634620666, Training Accuracy: 67.3425\n",
            "Validation Loss: 3.442917116128715, Validation Accuracy: 31.68\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 15.762403002210483, Test Accuracy: 12.36\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▄█▂▁▂▁▂▂▁▁▁▂▂▂▂▂▃▂▃▂▂▃▃▃▃▃▃▄▄▃▄▄▃▃▄▃▃▃▄▃</td></tr><tr><td>Test Loss</td><td>▃▁▄▄▆▇▇█▇▇▇▇▇▇█▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>███▇▇▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>12.36</td></tr><tr><td>Test Loss</td><td>15.7624</td></tr><tr><td>Train Accuracy</td><td>67.3425</td></tr><tr><td>Train Loss</td><td>1.17824</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240611_235923-9i8aijvl/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_002202-8lcpcbs2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2' target=\"_blank\">learning_rate=0.001 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605505518341064, Training Accuracy: 1.0675\n",
            "Validation Loss: 4.60469646514601, Validation Accuracy: 0.9\n",
            "[2/150]: Training Loss: 4.603742761993408, Training Accuracy: 1.0775\n",
            "Validation Loss: 4.602513228252435, Validation Accuracy: 0.97\n",
            "[3/150]: Training Loss: 4.60020831451416, Training Accuracy: 1.42\n",
            "Validation Loss: 4.596951505940432, Validation Accuracy: 1.39\n",
            "[4/150]: Training Loss: 4.590583048248291, Training Accuracy: 1.345\n",
            "Validation Loss: 4.579934718502555, Validation Accuracy: 1.81\n",
            "[5/150]: Training Loss: 4.522595316314697, Training Accuracy: 2.9\n",
            "Validation Loss: 4.390095902096694, Validation Accuracy: 3.43\n",
            "[6/150]: Training Loss: 4.228593465805054, Training Accuracy: 5.2625\n",
            "Validation Loss: 4.173950313762495, Validation Accuracy: 5.48\n",
            "[7/150]: Training Loss: 4.1090137573242185, Training Accuracy: 6.88\n",
            "Validation Loss: 4.087003700292794, Validation Accuracy: 7.44\n",
            "[8/150]: Training Loss: 4.035317427825928, Training Accuracy: 8.06\n",
            "Validation Loss: 4.015521998618059, Validation Accuracy: 8.52\n",
            "[9/150]: Training Loss: 3.960421589279175, Training Accuracy: 9.445\n",
            "Validation Loss: 3.9580009849208175, Validation Accuracy: 9.3\n",
            "[10/150]: Training Loss: 3.8591443000793455, Training Accuracy: 11.34\n",
            "Validation Loss: 3.84393062105604, Validation Accuracy: 11.61\n",
            "[11/150]: Training Loss: 3.7639447425842287, Training Accuracy: 12.9725\n",
            "Validation Loss: 3.7741037219952625, Validation Accuracy: 12.76\n",
            "[12/150]: Training Loss: 3.6964004222869873, Training Accuracy: 14.225\n",
            "Validation Loss: 3.6935869903321477, Validation Accuracy: 14.02\n",
            "[13/150]: Training Loss: 3.6246248401641847, Training Accuracy: 15.4325\n",
            "Validation Loss: 3.6313245053503924, Validation Accuracy: 15.35\n",
            "[14/150]: Training Loss: 3.5586987380981445, Training Accuracy: 16.58\n",
            "Validation Loss: 3.570704687932494, Validation Accuracy: 16.63\n",
            "[15/150]: Training Loss: 3.492145662689209, Training Accuracy: 17.56\n",
            "Validation Loss: 3.5174384117126465, Validation Accuracy: 17.05\n",
            "[16/150]: Training Loss: 3.4223767150878905, Training Accuracy: 18.875\n",
            "Validation Loss: 3.4360159157188077, Validation Accuracy: 18.52\n",
            "[17/150]: Training Loss: 3.3623727890014647, Training Accuracy: 19.63\n",
            "Validation Loss: 3.3825773327213944, Validation Accuracy: 19.78\n",
            "[18/150]: Training Loss: 3.3074194034576414, Training Accuracy: 20.8975\n",
            "Validation Loss: 3.3404667316728336, Validation Accuracy: 20.19\n",
            "[19/150]: Training Loss: 3.251063732147217, Training Accuracy: 21.8975\n",
            "Validation Loss: 3.312105529627223, Validation Accuracy: 20.9\n",
            "[20/150]: Training Loss: 3.204967420578003, Training Accuracy: 22.7225\n",
            "Validation Loss: 3.2530917027953326, Validation Accuracy: 22.31\n",
            "[21/150]: Training Loss: 3.15532322807312, Training Accuracy: 23.64\n",
            "Validation Loss: 3.229210142876692, Validation Accuracy: 22.45\n",
            "[22/150]: Training Loss: 3.1103267768859864, Training Accuracy: 24.38\n",
            "Validation Loss: 3.213347544336015, Validation Accuracy: 23.38\n",
            "[23/150]: Training Loss: 3.0626929641723635, Training Accuracy: 25.55\n",
            "Validation Loss: 3.181753307391124, Validation Accuracy: 24.04\n",
            "[24/150]: Training Loss: 3.01508925819397, Training Accuracy: 26.1675\n",
            "Validation Loss: 3.159736381214895, Validation Accuracy: 24.07\n",
            "[25/150]: Training Loss: 2.977297193527222, Training Accuracy: 26.95\n",
            "Validation Loss: 3.139521881273598, Validation Accuracy: 24.65\n",
            "[26/150]: Training Loss: 2.9348321487426756, Training Accuracy: 27.76\n",
            "Validation Loss: 3.129030877617514, Validation Accuracy: 25.02\n",
            "[27/150]: Training Loss: 2.892203674316406, Training Accuracy: 28.71\n",
            "Validation Loss: 3.1089744279339055, Validation Accuracy: 24.88\n",
            "[28/150]: Training Loss: 2.8514965145111084, Training Accuracy: 29.41\n",
            "Validation Loss: 3.0390360476864373, Validation Accuracy: 26.37\n",
            "[29/150]: Training Loss: 2.8064930957794187, Training Accuracy: 30.3275\n",
            "Validation Loss: 3.0866621254356046, Validation Accuracy: 26.25\n",
            "[30/150]: Training Loss: 2.771508701324463, Training Accuracy: 30.8325\n",
            "Validation Loss: 3.034489528388734, Validation Accuracy: 26.79\n",
            "[31/150]: Training Loss: 2.726985428237915, Training Accuracy: 32.06\n",
            "Validation Loss: 2.994590279403006, Validation Accuracy: 27.49\n",
            "[32/150]: Training Loss: 2.693068378448486, Training Accuracy: 32.6375\n",
            "Validation Loss: 2.991791993949064, Validation Accuracy: 27.57\n",
            "[33/150]: Training Loss: 2.6538521224975584, Training Accuracy: 33.305\n",
            "Validation Loss: 2.9930253712234984, Validation Accuracy: 27.59\n",
            "[34/150]: Training Loss: 2.6168819108963013, Training Accuracy: 33.9675\n",
            "Validation Loss: 2.9458029012011875, Validation Accuracy: 28.63\n",
            "[35/150]: Training Loss: 2.5724296060562133, Training Accuracy: 34.885\n",
            "Validation Loss: 2.9706625437280936, Validation Accuracy: 28.37\n",
            "[36/150]: Training Loss: 2.532205513381958, Training Accuracy: 35.7275\n",
            "Validation Loss: 3.0205048026552626, Validation Accuracy: 27.62\n",
            "[37/150]: Training Loss: 2.501378486442566, Training Accuracy: 36.655\n",
            "Validation Loss: 2.9459367466580337, Validation Accuracy: 29.16\n",
            "[38/150]: Training Loss: 2.4599771045684813, Training Accuracy: 37.265\n",
            "Validation Loss: 2.9162613555883907, Validation Accuracy: 29.42\n",
            "[39/150]: Training Loss: 2.416532469558716, Training Accuracy: 38.265\n",
            "Validation Loss: 2.9226647676176327, Validation Accuracy: 29.71\n",
            "[40/150]: Training Loss: 2.377360425758362, Training Accuracy: 39.1\n",
            "Validation Loss: 2.912419487716286, Validation Accuracy: 30.24\n",
            "[41/150]: Training Loss: 2.3416698053359983, Training Accuracy: 39.7725\n",
            "Validation Loss: 2.9336505536061184, Validation Accuracy: 29.72\n",
            "[42/150]: Training Loss: 2.302257305908203, Training Accuracy: 40.6025\n",
            "Validation Loss: 2.9164519552971906, Validation Accuracy: 30.29\n",
            "[43/150]: Training Loss: 2.2648201442718507, Training Accuracy: 41.4425\n",
            "Validation Loss: 2.917926338068239, Validation Accuracy: 30.33\n",
            "[44/150]: Training Loss: 2.2238695373535156, Training Accuracy: 42.1875\n",
            "Validation Loss: 2.920909085091512, Validation Accuracy: 30.59\n",
            "[45/150]: Training Loss: 2.186175981903076, Training Accuracy: 43.1875\n",
            "Validation Loss: 2.979639395027404, Validation Accuracy: 29.65\n",
            "[46/150]: Training Loss: 2.1421424005508425, Training Accuracy: 44.26\n",
            "Validation Loss: 2.955421671745883, Validation Accuracy: 29.75\n",
            "[47/150]: Training Loss: 2.1072659519195556, Training Accuracy: 44.7325\n",
            "Validation Loss: 2.9603501687383957, Validation Accuracy: 30.38\n",
            "[48/150]: Training Loss: 2.0704134420394897, Training Accuracy: 45.5625\n",
            "Validation Loss: 2.9289260366160397, Validation Accuracy: 30.82\n",
            "[49/150]: Training Loss: 2.0253924531936645, Training Accuracy: 46.8725\n",
            "Validation Loss: 2.9537550051500845, Validation Accuracy: 30.64\n",
            "[50/150]: Training Loss: 1.9894214233398437, Training Accuracy: 47.5425\n",
            "Validation Loss: 2.992744015280608, Validation Accuracy: 30.54\n",
            "[51/150]: Training Loss: 1.9478775398254395, Training Accuracy: 48.1625\n",
            "Validation Loss: 3.0440484505550116, Validation Accuracy: 30.87\n",
            "[52/150]: Training Loss: 1.9146845523834228, Training Accuracy: 48.9825\n",
            "Validation Loss: 2.9890038989911414, Validation Accuracy: 31.24\n",
            "[53/150]: Training Loss: 1.8723485668182374, Training Accuracy: 50.01\n",
            "Validation Loss: 2.993290390937951, Validation Accuracy: 31.22\n",
            "[54/150]: Training Loss: 1.8348793195724487, Training Accuracy: 50.52\n",
            "Validation Loss: 3.0035920021640266, Validation Accuracy: 31.61\n",
            "[55/150]: Training Loss: 1.7904879375457763, Training Accuracy: 52.175\n",
            "Validation Loss: 3.0537699574877504, Validation Accuracy: 30.68\n",
            "[56/150]: Training Loss: 1.7518022777557374, Training Accuracy: 52.58\n",
            "Validation Loss: 3.0826165934277188, Validation Accuracy: 30.36\n",
            "[57/150]: Training Loss: 1.723713356781006, Training Accuracy: 53.2725\n",
            "Validation Loss: 3.0921939679771473, Validation Accuracy: 30.79\n",
            "[58/150]: Training Loss: 1.673542138671875, Training Accuracy: 54.7525\n",
            "Validation Loss: 3.114953031965122, Validation Accuracy: 30.97\n",
            "[59/150]: Training Loss: 1.6370127866744995, Training Accuracy: 55.3475\n",
            "Validation Loss: 3.1347598710637183, Validation Accuracy: 31.13\n",
            "[60/150]: Training Loss: 1.595728307723999, Training Accuracy: 56.3775\n",
            "Validation Loss: 3.211446317138186, Validation Accuracy: 30.86\n",
            "[61/150]: Training Loss: 1.5595929719924926, Training Accuracy: 57.445\n",
            "Validation Loss: 3.1916901260424573, Validation Accuracy: 31.69\n",
            "[62/150]: Training Loss: 1.5153404804229735, Training Accuracy: 58.605\n",
            "Validation Loss: 3.2764444821959087, Validation Accuracy: 30.21\n",
            "[63/150]: Training Loss: 1.4795546808242799, Training Accuracy: 59.165\n",
            "Validation Loss: 3.2651574581292024, Validation Accuracy: 31.32\n",
            "[64/150]: Training Loss: 1.443065357875824, Training Accuracy: 60.3525\n",
            "Validation Loss: 3.2850031807164477, Validation Accuracy: 30.88\n",
            "[65/150]: Training Loss: 1.3952457666397096, Training Accuracy: 61.3125\n",
            "Validation Loss: 3.334014499263399, Validation Accuracy: 30.51\n",
            "[66/150]: Training Loss: 1.3616252402305602, Training Accuracy: 62.2875\n",
            "Validation Loss: 3.3781587804199025, Validation Accuracy: 30.85\n",
            "[67/150]: Training Loss: 1.3209700021743775, Training Accuracy: 63.1475\n",
            "Validation Loss: 3.4392695958447304, Validation Accuracy: 30.92\n",
            "[68/150]: Training Loss: 1.2754105567932128, Training Accuracy: 64.4\n",
            "Validation Loss: 3.487312679837464, Validation Accuracy: 30.89\n",
            "[69/150]: Training Loss: 1.2461662047386168, Training Accuracy: 65.2125\n",
            "Validation Loss: 3.470762025019166, Validation Accuracy: 30.79\n",
            "[70/150]: Training Loss: 1.2068843828201294, Training Accuracy: 66.345\n",
            "Validation Loss: 3.5463279022532666, Validation Accuracy: 30.57\n",
            "[71/150]: Training Loss: 1.174416847038269, Training Accuracy: 66.87\n",
            "Validation Loss: 3.5970414428953914, Validation Accuracy: 30.5\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 17.28561769473325, Test Accuracy: 11.15\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▆▁▃▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▂▃▃▃▃▃▃▃▃▃▄▄▃▃▄▄▄▄▄▄</td></tr><tr><td>Test Loss</td><td>▂▁▅▄▆▆▆▇▇▆▆▇█▇██▇█████▇▇▇███████████████</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>███▇▇▇▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>11.15</td></tr><tr><td>Test Loss</td><td>17.28562</td></tr><tr><td>Train Accuracy</td><td>66.87</td></tr><tr><td>Train Loss</td><td>1.17442</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_002202-8lcpcbs2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.0001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_002809-yufpefeq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq' target=\"_blank\">learning_rate=0.01 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.394053651046753, Training Accuracy: 3.235\n",
            "Validation Loss: 4.1179144488778086, Validation Accuracy: 6.51\n",
            "[2/150]: Training Loss: 3.940752759170532, Training Accuracy: 9.1825\n",
            "Validation Loss: 3.793641860318032, Validation Accuracy: 10.96\n",
            "[3/150]: Training Loss: 3.607163610458374, Training Accuracy: 14.85\n",
            "Validation Loss: 3.445422892357893, Validation Accuracy: 18.4\n",
            "[4/150]: Training Loss: 3.32619926071167, Training Accuracy: 19.315\n",
            "Validation Loss: 3.202780070578217, Validation Accuracy: 21.55\n",
            "[5/150]: Training Loss: 3.0974891372680666, Training Accuracy: 23.9725\n",
            "Validation Loss: 3.0388081802684033, Validation Accuracy: 24.97\n",
            "[6/150]: Training Loss: 2.901975968170166, Training Accuracy: 27.42\n",
            "Validation Loss: 3.0498315498327755, Validation Accuracy: 25.89\n",
            "[7/150]: Training Loss: 2.72861491394043, Training Accuracy: 31.1425\n",
            "Validation Loss: 2.817266490049423, Validation Accuracy: 30.26\n",
            "[8/150]: Training Loss: 2.565713974761963, Training Accuracy: 33.8575\n",
            "Validation Loss: 2.7873742094465124, Validation Accuracy: 30.1\n",
            "[9/150]: Training Loss: 2.394580401802063, Training Accuracy: 37.745\n",
            "Validation Loss: 2.714815883879449, Validation Accuracy: 32.43\n",
            "[10/150]: Training Loss: 2.2325665201187133, Training Accuracy: 41.275\n",
            "Validation Loss: 2.6796328748107716, Validation Accuracy: 33.37\n",
            "[11/150]: Training Loss: 2.0732334920883178, Training Accuracy: 44.825\n",
            "Validation Loss: 2.743416508291937, Validation Accuracy: 33.16\n",
            "[12/150]: Training Loss: 1.9020709432601928, Training Accuracy: 48.52\n",
            "Validation Loss: 2.761919692823082, Validation Accuracy: 34.09\n",
            "[13/150]: Training Loss: 1.723027162361145, Training Accuracy: 52.47\n",
            "Validation Loss: 2.781366317894808, Validation Accuracy: 33.87\n",
            "[14/150]: Training Loss: 1.5519161633491516, Training Accuracy: 56.5925\n",
            "Validation Loss: 2.8901124114443544, Validation Accuracy: 32.73\n",
            "[15/150]: Training Loss: 1.3848727501869202, Training Accuracy: 60.6225\n",
            "Validation Loss: 3.061296709024223, Validation Accuracy: 32.5\n",
            "[16/150]: Training Loss: 1.2385452840805053, Training Accuracy: 64.0375\n",
            "Validation Loss: 3.1685607524434474, Validation Accuracy: 33.0\n",
            "[17/150]: Training Loss: 1.0766600145339966, Training Accuracy: 68.1675\n",
            "Validation Loss: 3.470290554556877, Validation Accuracy: 32.52\n",
            "[18/150]: Training Loss: 0.953560617685318, Training Accuracy: 71.2375\n",
            "Validation Loss: 3.737919813508441, Validation Accuracy: 32.63\n",
            "[19/150]: Training Loss: 0.8452390188217163, Training Accuracy: 74.1975\n",
            "Validation Loss: 3.9338995074010956, Validation Accuracy: 32.13\n",
            "[20/150]: Training Loss: 0.7454726006031036, Training Accuracy: 76.935\n",
            "Validation Loss: 4.16999766325495, Validation Accuracy: 32.08\n",
            "[21/150]: Training Loss: 0.6607641342639923, Training Accuracy: 79.455\n",
            "Validation Loss: 4.340858131457287, Validation Accuracy: 31.19\n",
            "[22/150]: Training Loss: 0.5830736963987351, Training Accuracy: 81.7975\n",
            "Validation Loss: 4.7213960696177875, Validation Accuracy: 32.35\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 31.791706814128123, Test Accuracy: 14.4\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▂▃█▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>Test Loss</td><td>▁▅█▇█▇██▇▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇███▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>14.4</td></tr><tr><td>Test Loss</td><td>31.79171</td></tr><tr><td>Train Accuracy</td><td>81.7975</td></tr><tr><td>Train Loss</td><td>0.58307</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_002809-yufpefeq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_003027-5ijvpyn3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3' target=\"_blank\">learning_rate=0.01 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.452998904800415, Training Accuracy: 2.615\n",
            "Validation Loss: 4.1346032179085315, Validation Accuracy: 5.28\n",
            "[2/150]: Training Loss: 3.9527258255004885, Training Accuracy: 8.88\n",
            "Validation Loss: 3.7435999432946465, Validation Accuracy: 11.55\n",
            "[3/150]: Training Loss: 3.6381911922454835, Training Accuracy: 14.3725\n",
            "Validation Loss: 3.50478922181828, Validation Accuracy: 15.58\n",
            "[4/150]: Training Loss: 3.3785652015686036, Training Accuracy: 18.865\n",
            "Validation Loss: 3.3451961089091697, Validation Accuracy: 19.0\n",
            "[5/150]: Training Loss: 3.17810930519104, Training Accuracy: 22.375\n",
            "Validation Loss: 3.1223374703887163, Validation Accuracy: 23.6\n",
            "[6/150]: Training Loss: 3.0013810291290284, Training Accuracy: 25.565\n",
            "Validation Loss: 3.015145599462424, Validation Accuracy: 25.56\n",
            "[7/150]: Training Loss: 2.8544215950012206, Training Accuracy: 28.7175\n",
            "Validation Loss: 2.917889818264421, Validation Accuracy: 28.04\n",
            "[8/150]: Training Loss: 2.6971351528167724, Training Accuracy: 31.3825\n",
            "Validation Loss: 2.8436176351680875, Validation Accuracy: 29.6\n",
            "[9/150]: Training Loss: 2.567851602554321, Training Accuracy: 34.1025\n",
            "Validation Loss: 2.748332465530201, Validation Accuracy: 31.45\n",
            "[10/150]: Training Loss: 2.422238304901123, Training Accuracy: 37.085\n",
            "Validation Loss: 2.7193879473740887, Validation Accuracy: 32.08\n",
            "[11/150]: Training Loss: 2.3000989530563354, Training Accuracy: 39.695\n",
            "Validation Loss: 2.736810536141608, Validation Accuracy: 32.35\n",
            "[12/150]: Training Loss: 2.170335920906067, Training Accuracy: 42.5\n",
            "Validation Loss: 2.668315727239961, Validation Accuracy: 34.0\n",
            "[13/150]: Training Loss: 2.037977534675598, Training Accuracy: 45.47\n",
            "Validation Loss: 2.6606684786498924, Validation Accuracy: 34.72\n",
            "[14/150]: Training Loss: 1.9109795570373536, Training Accuracy: 48.245\n",
            "Validation Loss: 2.677434118690005, Validation Accuracy: 34.85\n",
            "[15/150]: Training Loss: 1.7840767404556275, Training Accuracy: 50.9475\n",
            "Validation Loss: 2.6786925458604363, Validation Accuracy: 35.46\n",
            "[16/150]: Training Loss: 1.6501886499404907, Training Accuracy: 54.325\n",
            "Validation Loss: 2.749260063383989, Validation Accuracy: 34.73\n",
            "[17/150]: Training Loss: 1.5220398645401, Training Accuracy: 57.045\n",
            "Validation Loss: 2.9064030396710536, Validation Accuracy: 34.13\n",
            "[18/150]: Training Loss: 1.3921052120208741, Training Accuracy: 60.4275\n",
            "Validation Loss: 2.978949681968446, Validation Accuracy: 33.67\n",
            "[19/150]: Training Loss: 1.2853214281082153, Training Accuracy: 62.9225\n",
            "Validation Loss: 3.0623086941470006, Validation Accuracy: 33.54\n",
            "[20/150]: Training Loss: 1.1686985822677611, Training Accuracy: 65.5875\n",
            "Validation Loss: 3.099342126755198, Validation Accuracy: 34.34\n",
            "[21/150]: Training Loss: 1.0535273086547852, Training Accuracy: 68.77\n",
            "Validation Loss: 3.2730220791640554, Validation Accuracy: 32.71\n",
            "[22/150]: Training Loss: 0.9473227613449097, Training Accuracy: 71.6425\n",
            "Validation Loss: 3.5567993069909942, Validation Accuracy: 33.18\n",
            "[23/150]: Training Loss: 0.8736372189521789, Training Accuracy: 73.5275\n",
            "Validation Loss: 3.4931609501504592, Validation Accuracy: 33.71\n",
            "[24/150]: Training Loss: 0.7797131684780121, Training Accuracy: 76.18\n",
            "Validation Loss: 3.6084122536288703, Validation Accuracy: 31.79\n",
            "[25/150]: Training Loss: 0.72144877743721, Training Accuracy: 77.925\n",
            "Validation Loss: 3.6706639156220064, Validation Accuracy: 33.34\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 17.253872677019448, Test Accuracy: 18.45\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▄█▄▅▃▄▄▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Test Loss</td><td>▁▂▆▄▃▅▆▇▇▆▆▇▇▇███▇▇▇██▇▇█▇██████████████</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>18.45</td></tr><tr><td>Test Loss</td><td>17.25387</td></tr><tr><td>Train Accuracy</td><td>77.925</td></tr><tr><td>Train Loss</td><td>0.72145</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_003027-5ijvpyn3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_003346-pvvd2my8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8' target=\"_blank\">learning_rate=0.01 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.436210793685913, Training Accuracy: 2.8025\n",
            "Validation Loss: 4.1512272403498365, Validation Accuracy: 5.38\n",
            "[2/150]: Training Loss: 3.943741687011719, Training Accuracy: 9.175\n",
            "Validation Loss: 3.791590502307673, Validation Accuracy: 11.54\n",
            "[3/150]: Training Loss: 3.6135140613555907, Training Accuracy: 14.57\n",
            "Validation Loss: 3.473675949558331, Validation Accuracy: 17.5\n",
            "[4/150]: Training Loss: 3.3465395004272462, Training Accuracy: 19.22\n",
            "Validation Loss: 3.3301091406755385, Validation Accuracy: 19.19\n",
            "[5/150]: Training Loss: 3.1310076709747316, Training Accuracy: 22.94\n",
            "Validation Loss: 3.0678081117617855, Validation Accuracy: 24.65\n",
            "[6/150]: Training Loss: 2.9402838905334474, Training Accuracy: 26.4875\n",
            "Validation Loss: 2.9766739310732313, Validation Accuracy: 25.68\n",
            "[7/150]: Training Loss: 2.7704892574310302, Training Accuracy: 29.855\n",
            "Validation Loss: 2.8771827342403924, Validation Accuracy: 28.79\n",
            "[8/150]: Training Loss: 2.6034393648147582, Training Accuracy: 33.84\n",
            "Validation Loss: 2.7646109807263515, Validation Accuracy: 31.02\n",
            "[9/150]: Training Loss: 2.436837389945984, Training Accuracy: 37.1175\n",
            "Validation Loss: 2.731329696193622, Validation Accuracy: 32.37\n",
            "[10/150]: Training Loss: 2.273189482879639, Training Accuracy: 40.3275\n",
            "Validation Loss: 2.663352646645467, Validation Accuracy: 33.62\n",
            "[11/150]: Training Loss: 2.1113974294662476, Training Accuracy: 44.0125\n",
            "Validation Loss: 2.70243866884025, Validation Accuracy: 33.48\n",
            "[12/150]: Training Loss: 1.9719651878356934, Training Accuracy: 46.755\n",
            "Validation Loss: 2.7540456124931385, Validation Accuracy: 34.46\n",
            "[13/150]: Training Loss: 1.8135078485488891, Training Accuracy: 50.6725\n",
            "Validation Loss: 2.7649777801173507, Validation Accuracy: 34.67\n",
            "[14/150]: Training Loss: 1.663066688156128, Training Accuracy: 53.855\n",
            "Validation Loss: 2.8507347205641924, Validation Accuracy: 34.61\n",
            "[15/150]: Training Loss: 1.5029290641784667, Training Accuracy: 57.7575\n",
            "Validation Loss: 2.961932021341506, Validation Accuracy: 33.61\n",
            "[16/150]: Training Loss: 1.3492597907066346, Training Accuracy: 61.345\n",
            "Validation Loss: 3.0608204777833, Validation Accuracy: 33.71\n",
            "[17/150]: Training Loss: 1.2147304633140563, Training Accuracy: 64.7325\n",
            "Validation Loss: 3.1523648325804694, Validation Accuracy: 33.57\n",
            "[18/150]: Training Loss: 1.0741954045295716, Training Accuracy: 68.3375\n",
            "Validation Loss: 3.4972750517972715, Validation Accuracy: 32.75\n",
            "[19/150]: Training Loss: 0.9752286433696746, Training Accuracy: 70.7775\n",
            "Validation Loss: 3.703600254787761, Validation Accuracy: 32.29\n",
            "[20/150]: Training Loss: 0.8703240002632141, Training Accuracy: 73.66\n",
            "Validation Loss: 3.6860399884023485, Validation Accuracy: 32.6\n",
            "[21/150]: Training Loss: 0.7578078193187714, Training Accuracy: 76.6025\n",
            "Validation Loss: 4.062916931832672, Validation Accuracy: 31.29\n",
            "[22/150]: Training Loss: 0.673084812450409, Training Accuracy: 79.0375\n",
            "Validation Loss: 4.114260547480006, Validation Accuracy: 31.62\n",
            "[23/150]: Training Loss: 0.6178978152275085, Training Accuracy: 80.77\n",
            "Validation Loss: 4.332119194565306, Validation Accuracy: 32.24\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 24.731220123874156, Test Accuracy: 17.1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▆▁▅▆▇█▆▆▆▇▇▆▇▇▇▇▇████████▇██▇▇▇▇███████</td></tr><tr><td>Test Loss</td><td>▁▄▆▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>17.1</td></tr><tr><td>Test Loss</td><td>24.73122</td></tr><tr><td>Train Accuracy</td><td>80.77</td></tr><tr><td>Train Loss</td><td>0.6179</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_003346-pvvd2my8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "\n",
        "for lr in learning_rates:\n",
        "  for wd in weight_decays:\n",
        "\n",
        "    print('='*50)\n",
        "    print(f'Hyperparameter with lr:{lr} and wd:{wd}')\n",
        "    print('='*50)\n",
        "\n",
        "    hyperparameters = {'learning_rate': lr,\n",
        "                       'weight_decay' : wd\n",
        "                       }\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(num_epochs, model, train_loader, validation_loader, test_loader, optimizer, scheduler, criterion, device, 'SGDM-HyperParameterTuning', hyperparameters=hyperparameters, is_wandb = True, n_epochs_stop = 10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training Using Best Hyperparameters Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f9dca98d1b8c402db8fa03f1354a051b",
            "3445c5fe76514de0b6b9cc31200c8544",
            "eafc1e77174c4758a3780b481d694ae0",
            "e6a773393e084de9a117fbbf43b7a59e",
            "b0748d32c99741d39b18512400e07f30",
            "dc43eb867d6b446cb0cb8e5debae57e7",
            "6c30f9a9e6c44d78ad39f43a5f11a6d0",
            "81500a36ef5e4a9e9b36aef01bee3697"
          ]
        },
        "id": "i9Q1MpZmVAUp",
        "outputId": "00a406f8-47bc-405c-d5e8-e10713b7f5fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_013938-4i9h8t3n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n' target=\"_blank\">learning_rate=0.01 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.089274020146226, Training Accuracy: 7.336\n",
            "Validation Loss: 3.675565988394865, Validation Accuracy: 11.94\n",
            "[2/150]: Training Loss: 3.4894355224526445, Training Accuracy: 16.328\n",
            "Validation Loss: 3.239326788361665, Validation Accuracy: 21.43\n",
            "[3/150]: Training Loss: 3.1558719864281852, Training Accuracy: 21.82\n",
            "Validation Loss: 2.9393957374961515, Validation Accuracy: 26.07\n",
            "[4/150]: Training Loss: 2.9186559430778485, Training Accuracy: 26.722\n",
            "Validation Loss: 2.7213429053118277, Validation Accuracy: 30.34\n",
            "[5/150]: Training Loss: 2.7546427048685604, Training Accuracy: 29.83\n",
            "Validation Loss: 2.6445619574018346, Validation Accuracy: 32.87\n",
            "[6/150]: Training Loss: 2.6314107673552334, Training Accuracy: 32.316\n",
            "Validation Loss: 2.463979342940507, Validation Accuracy: 35.97\n",
            "[7/150]: Training Loss: 2.525907842399519, Training Accuracy: 34.748\n",
            "Validation Loss: 2.402360181899587, Validation Accuracy: 37.43\n",
            "[8/150]: Training Loss: 2.4479937202790203, Training Accuracy: 36.512\n",
            "Validation Loss: 2.4091407804732112, Validation Accuracy: 36.77\n",
            "[9/150]: Training Loss: 2.3761757938453303, Training Accuracy: 37.836\n",
            "Validation Loss: 2.266651909062817, Validation Accuracy: 40.12\n",
            "[10/150]: Training Loss: 2.3189178927780114, Training Accuracy: 39.182\n",
            "Validation Loss: 2.248874021943208, Validation Accuracy: 40.42\n",
            "[11/150]: Training Loss: 2.2658593403104015, Training Accuracy: 40.134\n",
            "Validation Loss: 2.259856101054295, Validation Accuracy: 40.67\n",
            "[12/150]: Training Loss: 2.2226978584628583, Training Accuracy: 41.142\n",
            "Validation Loss: 2.1725807235499097, Validation Accuracy: 42.45\n",
            "[13/150]: Training Loss: 2.1647636545893483, Training Accuracy: 42.432\n",
            "Validation Loss: 2.1585155209158637, Validation Accuracy: 43.43\n",
            "[14/150]: Training Loss: 2.130936350206585, Training Accuracy: 43.382\n",
            "Validation Loss: 2.099344393250289, Validation Accuracy: 44.47\n",
            "[15/150]: Training Loss: 2.100127648514555, Training Accuracy: 44.078\n",
            "Validation Loss: 2.1766397307632834, Validation Accuracy: 43.16\n",
            "[16/150]: Training Loss: 2.0619241555633447, Training Accuracy: 44.864\n",
            "Validation Loss: 2.086525283042033, Validation Accuracy: 45.45\n",
            "[17/150]: Training Loss: 2.030578180042374, Training Accuracy: 45.488\n",
            "Validation Loss: 2.056034764666466, Validation Accuracy: 45.56\n",
            "[18/150]: Training Loss: 2.0153322762540538, Training Accuracy: 45.894\n",
            "Validation Loss: 2.0836284441553103, Validation Accuracy: 45.49\n",
            "[19/150]: Training Loss: 1.9813076870520707, Training Accuracy: 46.512\n",
            "Validation Loss: 2.123422172418825, Validation Accuracy: 44.8\n",
            "[20/150]: Training Loss: 1.9613309851692766, Training Accuracy: 47.156\n",
            "Validation Loss: 2.0666351356324117, Validation Accuracy: 45.67\n",
            "[21/150]: Training Loss: 1.9383105931379605, Training Accuracy: 47.68\n",
            "Validation Loss: 2.002842481728572, Validation Accuracy: 47.53\n",
            "[22/150]: Training Loss: 1.9150681045963942, Training Accuracy: 48.088\n",
            "Validation Loss: 2.0622900238462316, Validation Accuracy: 47.17\n",
            "[23/150]: Training Loss: 1.893286463854563, Training Accuracy: 48.534\n",
            "Validation Loss: 2.00721144600279, Validation Accuracy: 46.89\n",
            "[24/150]: Training Loss: 1.890704987908873, Training Accuracy: 48.7\n",
            "Validation Loss: 2.038584551993449, Validation Accuracy: 46.81\n",
            "[25/150]: Training Loss: 1.864747319989802, Training Accuracy: 49.254\n",
            "Validation Loss: 2.0571977568280166, Validation Accuracy: 46.35\n",
            "[26/150]: Training Loss: 1.851232278225062, Training Accuracy: 49.578\n",
            "Validation Loss: 2.067047737206623, Validation Accuracy: 45.89\n",
            "[27/150]: Training Loss: 1.8424793141882132, Training Accuracy: 49.854\n",
            "Validation Loss: 1.9625371656600077, Validation Accuracy: 48.1\n",
            "[28/150]: Training Loss: 1.8046449944186393, Training Accuracy: 50.832\n",
            "Validation Loss: 2.0106217762467207, Validation Accuracy: 47.7\n",
            "[29/150]: Training Loss: 1.8060023488900852, Training Accuracy: 50.666\n",
            "Validation Loss: 1.972709655002424, Validation Accuracy: 48.73\n",
            "[30/150]: Training Loss: 1.794560846770206, Training Accuracy: 50.516\n",
            "Validation Loss: 1.9357355255989512, Validation Accuracy: 49.77\n",
            "[31/150]: Training Loss: 1.7700339468848674, Training Accuracy: 51.572\n",
            "Validation Loss: 2.0069477284789845, Validation Accuracy: 47.09\n",
            "[32/150]: Training Loss: 1.760896964146353, Training Accuracy: 51.7\n",
            "Validation Loss: 1.9989952729765776, Validation Accuracy: 48.32\n",
            "[33/150]: Training Loss: 1.7410227035927346, Training Accuracy: 52.406\n",
            "Validation Loss: 1.912184556578375, Validation Accuracy: 50.02\n",
            "[34/150]: Training Loss: 1.7288355792269987, Training Accuracy: 52.308\n",
            "Validation Loss: 1.960264290973639, Validation Accuracy: 49.04\n",
            "[35/150]: Training Loss: 1.726897613929056, Training Accuracy: 52.332\n",
            "Validation Loss: 1.9354495113822305, Validation Accuracy: 49.72\n",
            "[36/150]: Training Loss: 1.7120373965529225, Training Accuracy: 52.802\n",
            "Validation Loss: 1.9664037690800467, Validation Accuracy: 48.71\n",
            "[37/150]: Training Loss: 1.693096386502161, Training Accuracy: 53.136\n",
            "Validation Loss: 2.0150347925295495, Validation Accuracy: 47.45\n",
            "[38/150]: Training Loss: 1.6896419387949093, Training Accuracy: 53.464\n",
            "Validation Loss: 1.8958269729735746, Validation Accuracy: 50.19\n",
            "[39/150]: Training Loss: 1.6746401542897724, Training Accuracy: 53.896\n",
            "Validation Loss: 1.8924665306783786, Validation Accuracy: 50.21\n",
            "[40/150]: Training Loss: 1.658155962481828, Training Accuracy: 54.248\n",
            "Validation Loss: 2.013761671485415, Validation Accuracy: 47.68\n",
            "[41/150]: Training Loss: 1.6497482628468663, Training Accuracy: 54.48\n",
            "Validation Loss: 1.9099182338471625, Validation Accuracy: 49.93\n",
            "[42/150]: Training Loss: 1.640857491423102, Training Accuracy: 54.522\n",
            "Validation Loss: 1.9500497716247656, Validation Accuracy: 48.99\n",
            "[43/150]: Training Loss: 1.627800954272375, Training Accuracy: 54.828\n",
            "Validation Loss: 1.8859608940258148, Validation Accuracy: 50.08\n",
            "[44/150]: Training Loss: 1.6115469687125261, Training Accuracy: 55.468\n",
            "Validation Loss: 1.8382089783431619, Validation Accuracy: 51.49\n",
            "[45/150]: Training Loss: 1.5990354193141088, Training Accuracy: 55.408\n",
            "Validation Loss: 1.9672614282863155, Validation Accuracy: 49.18\n",
            "[46/150]: Training Loss: 1.597626144090272, Training Accuracy: 55.548\n",
            "Validation Loss: 1.8989077024399095, Validation Accuracy: 50.63\n",
            "[47/150]: Training Loss: 1.583931565284729, Training Accuracy: 55.81\n",
            "Validation Loss: 1.9013854675232225, Validation Accuracy: 50.7\n",
            "[48/150]: Training Loss: 1.5654289053224237, Training Accuracy: 56.032\n",
            "Validation Loss: 1.9721670173535681, Validation Accuracy: 49.36\n",
            "[49/150]: Training Loss: 1.5542657918789808, Training Accuracy: 56.446\n",
            "Validation Loss: 1.9395824830243542, Validation Accuracy: 49.82\n",
            "[50/150]: Training Loss: 1.5429580295482255, Training Accuracy: 56.698\n",
            "Validation Loss: 1.9366691735140078, Validation Accuracy: 50.0\n",
            "[51/150]: Training Loss: 1.539840473086023, Training Accuracy: 56.68\n",
            "Validation Loss: 1.93095024452088, Validation Accuracy: 49.97\n",
            "[52/150]: Training Loss: 1.522039294928846, Training Accuracy: 57.282\n",
            "Validation Loss: 1.901054722488306, Validation Accuracy: 50.91\n",
            "[53/150]: Training Loss: 1.505999029597358, Training Accuracy: 57.376\n",
            "Validation Loss: 1.945236231870712, Validation Accuracy: 49.27\n",
            "[54/150]: Training Loss: 1.507185840469492, Training Accuracy: 57.802\n",
            "Validation Loss: 1.8845598515431592, Validation Accuracy: 50.54\n",
            "[55/150]: Training Loss: 1.4943511856486424, Training Accuracy: 57.754\n",
            "Validation Loss: 1.908029711170561, Validation Accuracy: 50.57\n",
            "[56/150]: Training Loss: 1.4784885358322613, Training Accuracy: 58.39\n",
            "Validation Loss: 1.9093056567914926, Validation Accuracy: 50.32\n",
            "[57/150]: Training Loss: 1.4689392635736929, Training Accuracy: 58.69\n",
            "Validation Loss: 1.9369963088612647, Validation Accuracy: 50.18\n",
            "[58/150]: Training Loss: 1.4634843205704409, Training Accuracy: 58.688\n",
            "Validation Loss: 1.9295313540537646, Validation Accuracy: 50.43\n",
            "[59/150]: Training Loss: 1.4444945978996393, Training Accuracy: 59.07\n",
            "Validation Loss: 1.8427119877687685, Validation Accuracy: 52.33\n",
            "[60/150]: Training Loss: 1.4354293743515258, Training Accuracy: 59.374\n",
            "Validation Loss: 1.910806885950125, Validation Accuracy: 50.94\n",
            "[61/150]: Training Loss: 1.4113788747269174, Training Accuracy: 59.82\n",
            "Validation Loss: 1.8786808958478793, Validation Accuracy: 51.43\n",
            "[62/150]: Training Loss: 1.4087011503898883, Training Accuracy: 59.934\n",
            "Validation Loss: 1.93228034608683, Validation Accuracy: 50.3\n",
            "[63/150]: Training Loss: 1.4004798267046203, Training Accuracy: 60.114\n",
            "Validation Loss: 1.862087261145282, Validation Accuracy: 51.42\n",
            "[64/150]: Training Loss: 1.3885550134627105, Training Accuracy: 60.078\n",
            "Validation Loss: 1.839821860289118, Validation Accuracy: 52.23\n",
            "[65/150]: Training Loss: 1.3659701371741721, Training Accuracy: 61.178\n",
            "Validation Loss: 1.8993338536305033, Validation Accuracy: 51.3\n",
            "[66/150]: Training Loss: 1.359891347720495, Training Accuracy: 61.062\n",
            "Validation Loss: 1.808910168659915, Validation Accuracy: 52.34\n",
            "[67/150]: Training Loss: 1.350066394376023, Training Accuracy: 61.206\n",
            "Validation Loss: 1.842593222666698, Validation Accuracy: 52.23\n",
            "[68/150]: Training Loss: 1.3413548037371672, Training Accuracy: 61.606\n",
            "Validation Loss: 1.8450721175807296, Validation Accuracy: 52.19\n",
            "[69/150]: Training Loss: 1.328629597907176, Training Accuracy: 61.842\n",
            "Validation Loss: 1.9130320769206735, Validation Accuracy: 51.75\n",
            "[70/150]: Training Loss: 1.308861137579774, Training Accuracy: 62.402\n",
            "Validation Loss: 1.8209870947394402, Validation Accuracy: 52.76\n",
            "[71/150]: Training Loss: 1.3000667644736101, Training Accuracy: 62.528\n",
            "Validation Loss: 1.8656981424161583, Validation Accuracy: 52.29\n",
            "[72/150]: Training Loss: 1.2866845129395994, Training Accuracy: 62.902\n",
            "Validation Loss: 1.8237224574301654, Validation Accuracy: 53.36\n",
            "[73/150]: Training Loss: 1.2716818537248675, Training Accuracy: 63.328\n",
            "Validation Loss: 1.872310409879988, Validation Accuracy: 52.51\n",
            "[74/150]: Training Loss: 1.2592318182253777, Training Accuracy: 63.678\n",
            "Validation Loss: 1.8924590835146085, Validation Accuracy: 52.18\n",
            "[75/150]: Training Loss: 1.2489444612694518, Training Accuracy: 63.924\n",
            "Validation Loss: 1.8441433739510311, Validation Accuracy: 53.18\n",
            "[76/150]: Training Loss: 1.2271763168637404, Training Accuracy: 64.204\n",
            "Validation Loss: 1.826472075881472, Validation Accuracy: 53.48\n",
            "[77/150]: Training Loss: 1.2162439644031817, Training Accuracy: 64.628\n",
            "Validation Loss: 1.7922283798266367, Validation Accuracy: 53.91\n",
            "[78/150]: Training Loss: 1.1974031528091187, Training Accuracy: 65.212\n",
            "Validation Loss: 1.854033857394176, Validation Accuracy: 52.77\n",
            "[79/150]: Training Loss: 1.1963484688945438, Training Accuracy: 65.298\n",
            "Validation Loss: 1.8417060686524507, Validation Accuracy: 53.83\n",
            "[80/150]: Training Loss: 1.170646650559457, Training Accuracy: 65.906\n",
            "Validation Loss: 1.8188444110238629, Validation Accuracy: 54.0\n",
            "[81/150]: Training Loss: 1.1599712444998114, Training Accuracy: 66.228\n",
            "Validation Loss: 1.832872725596094, Validation Accuracy: 53.93\n",
            "[82/150]: Training Loss: 1.1514770396987495, Training Accuracy: 66.602\n",
            "Validation Loss: 1.8412558204808813, Validation Accuracy: 53.79\n",
            "[83/150]: Training Loss: 1.1428028439621791, Training Accuracy: 66.712\n",
            "Validation Loss: 1.8457995410178119, Validation Accuracy: 53.19\n",
            "[84/150]: Training Loss: 1.1221959683901208, Training Accuracy: 67.186\n",
            "Validation Loss: 1.8420210499672374, Validation Accuracy: 53.69\n",
            "[85/150]: Training Loss: 1.1132261551859435, Training Accuracy: 67.302\n",
            "Validation Loss: 1.8613816719905587, Validation Accuracy: 53.54\n",
            "[86/150]: Training Loss: 1.095082609473592, Training Accuracy: 68.072\n",
            "Validation Loss: 1.8083385081048224, Validation Accuracy: 53.78\n",
            "[87/150]: Training Loss: 1.0820598827908412, Training Accuracy: 68.22\n",
            "Validation Loss: 1.8537752772592435, Validation Accuracy: 54.26\n",
            "[88/150]: Training Loss: 1.0588698522818973, Training Accuracy: 68.838\n",
            "Validation Loss: 1.8358791383208743, Validation Accuracy: 54.18\n",
            "[89/150]: Training Loss: 1.0537138239806876, Training Accuracy: 68.858\n",
            "Validation Loss: 1.9032178441430354, Validation Accuracy: 53.39\n",
            "[90/150]: Training Loss: 1.0420529545115693, Training Accuracy: 69.19\n",
            "Validation Loss: 1.8331271857972358, Validation Accuracy: 54.72\n",
            "[91/150]: Training Loss: 1.0190018734053883, Training Accuracy: 69.946\n",
            "Validation Loss: 1.81914554811587, Validation Accuracy: 54.61\n",
            "[92/150]: Training Loss: 1.0052901713744453, Training Accuracy: 70.376\n",
            "Validation Loss: 1.8224602520086204, Validation Accuracy: 54.54\n",
            "[93/150]: Training Loss: 0.9959899578100581, Training Accuracy: 70.576\n",
            "Validation Loss: 1.8335816890570769, Validation Accuracy: 54.66\n",
            "[94/150]: Training Loss: 0.9820676271034323, Training Accuracy: 70.882\n",
            "Validation Loss: 1.8163665267312603, Validation Accuracy: 55.18\n",
            "[95/150]: Training Loss: 0.9656730821675352, Training Accuracy: 71.258\n",
            "Validation Loss: 1.8308387478445745, Validation Accuracy: 54.6\n",
            "[96/150]: Training Loss: 0.9520570190666277, Training Accuracy: 71.578\n",
            "Validation Loss: 1.8336243507968393, Validation Accuracy: 55.03\n",
            "[97/150]: Training Loss: 0.938745556661235, Training Accuracy: 71.902\n",
            "Validation Loss: 1.8366246033625997, Validation Accuracy: 54.91\n",
            "[98/150]: Training Loss: 0.9180464198827134, Training Accuracy: 72.722\n",
            "Validation Loss: 1.8760136365890503, Validation Accuracy: 54.5\n",
            "[99/150]: Training Loss: 0.9068292014281768, Training Accuracy: 73.062\n",
            "Validation Loss: 1.7993891132864983, Validation Accuracy: 55.5\n",
            "[100/150]: Training Loss: 0.8922816610625942, Training Accuracy: 73.34\n",
            "Validation Loss: 1.8412930228907591, Validation Accuracy: 54.68\n",
            "[101/150]: Training Loss: 0.8739150777421034, Training Accuracy: 73.706\n",
            "Validation Loss: 1.8339118517128525, Validation Accuracy: 55.67\n",
            "[102/150]: Training Loss: 0.8564372022667199, Training Accuracy: 74.472\n",
            "Validation Loss: 1.831563648904205, Validation Accuracy: 54.7\n",
            "[103/150]: Training Loss: 0.8455693187463619, Training Accuracy: 74.832\n",
            "Validation Loss: 1.8586692977103458, Validation Accuracy: 55.09\n",
            "[104/150]: Training Loss: 0.8356970895815383, Training Accuracy: 74.91\n",
            "Validation Loss: 1.891693490326025, Validation Accuracy: 54.66\n",
            "[105/150]: Training Loss: 0.8192636847038708, Training Accuracy: 75.622\n",
            "Validation Loss: 1.836436649037015, Validation Accuracy: 55.18\n",
            "[106/150]: Training Loss: 0.8021497989783202, Training Accuracy: 75.904\n",
            "Validation Loss: 1.8664625199737064, Validation Accuracy: 55.06\n",
            "[107/150]: Training Loss: 0.7924835441438743, Training Accuracy: 76.238\n",
            "Validation Loss: 1.8400274932764138, Validation Accuracy: 56.23\n",
            "[108/150]: Training Loss: 0.7697334062031773, Training Accuracy: 76.922\n",
            "Validation Loss: 1.852326028665919, Validation Accuracy: 55.58\n",
            "[109/150]: Training Loss: 0.7663251587268337, Training Accuracy: 77.026\n",
            "Validation Loss: 1.851918256206877, Validation Accuracy: 55.79\n",
            "[110/150]: Training Loss: 0.7508895213113111, Training Accuracy: 77.64\n",
            "Validation Loss: 1.8553174369654077, Validation Accuracy: 55.56\n",
            "[111/150]: Training Loss: 0.7446020825973252, Training Accuracy: 77.658\n",
            "Validation Loss: 1.8480755607033992, Validation Accuracy: 55.4\n",
            "[112/150]: Training Loss: 0.7242154034278582, Training Accuracy: 78.302\n",
            "Validation Loss: 1.8599452805367245, Validation Accuracy: 55.63\n",
            "[113/150]: Training Loss: 0.710430224609497, Training Accuracy: 78.58\n",
            "Validation Loss: 1.853841427405169, Validation Accuracy: 55.72\n",
            "[114/150]: Training Loss: 0.6952832066418265, Training Accuracy: 79.15\n",
            "Validation Loss: 1.8361253859890494, Validation Accuracy: 56.53\n",
            "[115/150]: Training Loss: 0.6855566057631427, Training Accuracy: 79.384\n",
            "Validation Loss: 1.8501070935255404, Validation Accuracy: 55.8\n",
            "[116/150]: Training Loss: 0.670332043791366, Training Accuracy: 79.848\n",
            "Validation Loss: 1.871697852565984, Validation Accuracy: 55.87\n",
            "[117/150]: Training Loss: 0.659062210518076, Training Accuracy: 80.28\n",
            "Validation Loss: 1.8932185765284641, Validation Accuracy: 56.33\n",
            "[118/150]: Training Loss: 0.6481513134049027, Training Accuracy: 80.47\n",
            "Validation Loss: 1.8721413916083658, Validation Accuracy: 56.69\n",
            "[119/150]: Training Loss: 0.6378583989256178, Training Accuracy: 80.804\n",
            "Validation Loss: 1.8575806663294507, Validation Accuracy: 56.3\n",
            "[120/150]: Training Loss: 0.6237786744561646, Training Accuracy: 81.468\n",
            "Validation Loss: 1.8885631751103007, Validation Accuracy: 56.27\n",
            "[121/150]: Training Loss: 0.6136706139311157, Training Accuracy: 81.672\n",
            "Validation Loss: 1.8672110297877318, Validation Accuracy: 56.64\n",
            "[122/150]: Training Loss: 0.598588163380885, Training Accuracy: 82.084\n",
            "Validation Loss: 1.8859197356898314, Validation Accuracy: 56.85\n",
            "[123/150]: Training Loss: 0.5942039575494463, Training Accuracy: 82.472\n",
            "Validation Loss: 1.8791207416801696, Validation Accuracy: 56.65\n",
            "[124/150]: Training Loss: 0.5779005947625241, Training Accuracy: 82.892\n",
            "Validation Loss: 1.8723569349118858, Validation Accuracy: 56.69\n",
            "[125/150]: Training Loss: 0.5656420003689463, Training Accuracy: 83.386\n",
            "Validation Loss: 1.8776653800041052, Validation Accuracy: 56.51\n",
            "[126/150]: Training Loss: 0.5588299318423966, Training Accuracy: 83.53\n",
            "Validation Loss: 1.8926861441818772, Validation Accuracy: 56.77\n",
            "[127/150]: Training Loss: 0.5484652115065424, Training Accuracy: 83.916\n",
            "Validation Loss: 1.8761122143192657, Validation Accuracy: 56.82\n",
            "[128/150]: Training Loss: 0.5397156641230254, Training Accuracy: 84.144\n",
            "Validation Loss: 1.883816226272826, Validation Accuracy: 56.91\n",
            "[129/150]: Training Loss: 0.5299459375307688, Training Accuracy: 84.524\n",
            "Validation Loss: 1.885110402942463, Validation Accuracy: 56.93\n",
            "[130/150]: Training Loss: 0.5226550506203985, Training Accuracy: 84.69\n",
            "Validation Loss: 1.8858621705109906, Validation Accuracy: 57.03\n",
            "[131/150]: Training Loss: 0.5145904917622466, Training Accuracy: 85.08\n",
            "Validation Loss: 1.898423175902883, Validation Accuracy: 56.86\n",
            "[132/150]: Training Loss: 0.5080187612444239, Training Accuracy: 85.244\n",
            "Validation Loss: 1.8937967233597093, Validation Accuracy: 57.24\n",
            "[133/150]: Training Loss: 0.501667047438719, Training Accuracy: 85.274\n",
            "Validation Loss: 1.8932398701928983, Validation Accuracy: 56.88\n",
            "[134/150]: Training Loss: 0.49395688687977585, Training Accuracy: 85.592\n",
            "Validation Loss: 1.8888862345628679, Validation Accuracy: 57.11\n",
            "[135/150]: Training Loss: 0.48916787244474796, Training Accuracy: 85.884\n",
            "Validation Loss: 1.8910012594453849, Validation Accuracy: 57.08\n",
            "[136/150]: Training Loss: 0.48000375615894947, Training Accuracy: 86.098\n",
            "Validation Loss: 1.9055567767210067, Validation Accuracy: 56.92\n",
            "[137/150]: Training Loss: 0.48404536522029307, Training Accuracy: 86.152\n",
            "Validation Loss: 1.8957096717919513, Validation Accuracy: 57.23\n",
            "[138/150]: Training Loss: 0.4733923572637236, Training Accuracy: 86.464\n",
            "Validation Loss: 1.8970948966445438, Validation Accuracy: 57.23\n",
            "[139/150]: Training Loss: 0.4687954626424843, Training Accuracy: 86.406\n",
            "Validation Loss: 1.8926855910355878, Validation Accuracy: 57.49\n",
            "[140/150]: Training Loss: 0.4656825878126237, Training Accuracy: 86.552\n",
            "Validation Loss: 1.8944045935466791, Validation Accuracy: 57.25\n",
            "[141/150]: Training Loss: 0.46502622329365567, Training Accuracy: 86.75\n",
            "Validation Loss: 1.892073856797188, Validation Accuracy: 57.21\n",
            "[142/150]: Training Loss: 0.45881695900579245, Training Accuracy: 86.948\n",
            "Validation Loss: 1.8972379872753362, Validation Accuracy: 57.33\n",
            "[143/150]: Training Loss: 0.4567206385152419, Training Accuracy: 86.978\n",
            "Validation Loss: 1.8969501234163904, Validation Accuracy: 57.41\n",
            "[144/150]: Training Loss: 0.45361602742729895, Training Accuracy: 87.108\n",
            "Validation Loss: 1.8974497530870378, Validation Accuracy: 57.36\n",
            "[145/150]: Training Loss: 0.45476321409196807, Training Accuracy: 87.206\n",
            "Validation Loss: 1.9007372218332472, Validation Accuracy: 57.39\n",
            "[146/150]: Training Loss: 0.45061131850685304, Training Accuracy: 87.172\n",
            "Validation Loss: 1.8978507139120893, Validation Accuracy: 57.36\n",
            "[147/150]: Training Loss: 0.4471297430832063, Training Accuracy: 87.32\n",
            "Validation Loss: 1.8995944998066896, Validation Accuracy: 57.45\n",
            "[148/150]: Training Loss: 0.44544086216584494, Training Accuracy: 87.358\n",
            "Validation Loss: 1.8984677867524942, Validation Accuracy: 57.57\n",
            "[149/150]: Training Loss: 0.45087983754589733, Training Accuracy: 87.206\n",
            "Validation Loss: 1.8988231648305418, Validation Accuracy: 57.57\n",
            "[150/150]: Training Loss: 0.445312842879149, Training Accuracy: 87.314\n",
            "Validation Loss: 1.898690512985181, Validation Accuracy: 57.55\n",
            "**********************************************************************\n",
            "Test Loss: 1.898690512985181, Test Accuracy: 57.55\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▆█▄▃▄▃▂▁▂▂▂▂▁▁▁▁▂▂▂▁▁▁▁▂▁▂▁▂▂▂▂▁▂▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>█▁▄▅▅▅▆▇▅▆▅▄▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▄</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>57.55</td></tr><tr><td>Test Loss</td><td>1.89869</td></tr><tr><td>Train Accuracy</td><td>87.314</td></tr><tr><td>Train Loss</td><td>0.44531</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_013938-4i9h8t3n/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "\n",
        "hyperparameters = {'learning_rate': lr,\n",
        "                  'weight_decay' : wd}\n",
        "\n",
        "# Load the model\n",
        "model_0 = LeNet5().to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer_0 = torch.optim.SGD(model_0.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_0, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# Training\n",
        "run_training(num_epochs, model_0, original_train_loader, original_test_loader, original_test_loader, optimizer_0, scheduler, criterion, device, optimizer_name='SGDM', hyperparameters=hyperparameters, is_wandb=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### AdamW (Adam with Weight Decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ba6f9560801c461a90a0fd2f8c20d918",
            "5055c91b4ed34c1baf249fa25948e84f",
            "1feabb0772134ace893d71cb905e9b12",
            "fb5f2e328dff44018f41180c2a2ec871",
            "33050a2d80c24235aecc36cb34e79684",
            "2bc1423c729a4faa9a184ae092eda8ad",
            "1b1acf7a1f5f43739e1fe0894ba48950",
            "0d710da02bfe4c8e80d205158d9d64c7",
            "d770fa4de3bd479886dbf8e1f6369543",
            "c20d73b37f6a497a8847e2fd20a98d16",
            "edd61f4569b54b869669aebf91420508",
            "a6ba8ab59be54b8ca096631627ee9713",
            "15a0bbd1e2b14cef8a2f9accf120c1ad",
            "e181391aa4424c04804ac665abd6f594",
            "f8ce66183b2543e7bf19b71d90d1c53e",
            "904f899f441149e9b707699a0ebf31b5",
            "f76ed5acf91a4edf92950dcb88356f23",
            "9387a738135842cf965db860499510f5",
            "33a438b911c04f1f96c67b4f7ba7477d",
            "35d90a90d5854bcaa2bc5f385cdad931",
            "5630d5fcb50a46f2887e0cce74a005a6",
            "fed90f96881140119ee97a0d2120b615",
            "ff7b6956e7e34db68cd38dee19c798f1",
            "d71c8b5d30df474ca6f30976d0a33c71",
            "cbe10db0bb8d45609a0f3d59a30c8f61",
            "07ff6351429c48c2b835305d3111af40",
            "ab580ef13b18428c994fc4f8b80885b6",
            "497de066b1964cd4b931476e0bd50c66",
            "c5de35a9063345b1a12e212718a02575",
            "e86e517239b54ca4a6767a300aa7e01d",
            "eb268a732bc74b2d81ec3c3a6ecd0362",
            "f2915a147a4246dba7984a90f23c34ae",
            "2f39d5f73c7647f1804e4212cb39924d",
            "6e7ede02663f4eb0927872bf17858f18",
            "a562d76741f74b10b8095be14da779d1",
            "f4e7759ba0f544d8a28a9922e06e890b",
            "0a9ee7dc8817456aab4af03cbfa4c6ed",
            "567cc1542f09433f8cc8b3a39237f198",
            "8dc245e02cac40f1b601fa42a0e6e77a",
            "1b093d744b374154afe2058e4a6de822",
            "4302f47b1ff043aca2523212b015e080",
            "371471be80ca47dbb005c853b7832f04",
            "e75a0d8ae24e462aa3075a813aad302d",
            "fd2cd8045a5c4387b98d080782289c48",
            "1574e69b88de479da2aadc2dfdd43261",
            "f523ae2aca8c46878f0a6ddc1f798ef5",
            "0b42ea1995bb410b99e653780dad3916",
            "f3ebd9b7bf7a4d24b54ab6673322f74b"
          ]
        },
        "id": "Toi1eWRqJuhK",
        "outputId": "bd028d6d-1e8d-4fff-bccf-88ed9aa1c4ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:ymdv8k6d) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁</td></tr><tr><td>Train Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>7.7625</td></tr><tr><td>Train Loss</td><td>4.01706</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.00095 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/ymdv8k6d' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/ymdv8k6d</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_034825-ymdv8k6d/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:ymdv8k6d). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_034959-j16vayol</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol' target=\"_blank\">learning_rate=0.001 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.032126424407959, Training Accuracy: 7.645\n",
            "Validation Loss: 3.633875617555752, Validation Accuracy: 13.81\n",
            "[2/150]: Training Loss: 3.4117616390228274, Training Accuracy: 17.9375\n",
            "Validation Loss: 3.291252329091358, Validation Accuracy: 19.57\n",
            "[3/150]: Training Loss: 3.0980306770324706, Training Accuracy: 23.7325\n",
            "Validation Loss: 3.0133402590539045, Validation Accuracy: 25.49\n",
            "[4/150]: Training Loss: 2.875119793319702, Training Accuracy: 27.905\n",
            "Validation Loss: 2.9483105938905365, Validation Accuracy: 26.77\n",
            "[5/150]: Training Loss: 2.6911334384918213, Training Accuracy: 31.615\n",
            "Validation Loss: 2.851942961383018, Validation Accuracy: 28.77\n",
            "[6/150]: Training Loss: 2.5383362628936768, Training Accuracy: 34.6425\n",
            "Validation Loss: 2.8010447116414454, Validation Accuracy: 30.38\n",
            "[7/150]: Training Loss: 2.3951899276733397, Training Accuracy: 37.7375\n",
            "Validation Loss: 2.772052654035532, Validation Accuracy: 31.08\n",
            "[8/150]: Training Loss: 2.2677825828552245, Training Accuracy: 40.2175\n",
            "Validation Loss: 2.740394227823634, Validation Accuracy: 32.51\n",
            "[9/150]: Training Loss: 2.1415996942520144, Training Accuracy: 43.1725\n",
            "Validation Loss: 2.7748732961666813, Validation Accuracy: 31.82\n",
            "[10/150]: Training Loss: 2.016268748664856, Training Accuracy: 45.7725\n",
            "Validation Loss: 2.766294874203433, Validation Accuracy: 33.51\n",
            "[11/150]: Training Loss: 1.8994886245727538, Training Accuracy: 48.375\n",
            "Validation Loss: 2.840820978401573, Validation Accuracy: 32.8\n",
            "[12/150]: Training Loss: 1.7866277021408081, Training Accuracy: 51.045\n",
            "Validation Loss: 2.8579562796149283, Validation Accuracy: 32.9\n",
            "[13/150]: Training Loss: 1.6882859018325806, Training Accuracy: 53.12\n",
            "Validation Loss: 2.923183766899595, Validation Accuracy: 32.48\n",
            "[14/150]: Training Loss: 1.5779105269432068, Training Accuracy: 55.655\n",
            "Validation Loss: 3.072260461795102, Validation Accuracy: 32.29\n",
            "[15/150]: Training Loss: 1.478017513847351, Training Accuracy: 58.1575\n",
            "Validation Loss: 3.1435716137005265, Validation Accuracy: 32.08\n",
            "[16/150]: Training Loss: 1.3899440537452699, Training Accuracy: 59.9975\n",
            "Validation Loss: 3.176926696376436, Validation Accuracy: 31.97\n",
            "[17/150]: Training Loss: 1.3004323136329652, Training Accuracy: 62.3725\n",
            "Validation Loss: 3.3300343015391354, Validation Accuracy: 32.02\n",
            "[18/150]: Training Loss: 1.2033100715637206, Training Accuracy: 64.74\n",
            "Validation Loss: 3.478040464364799, Validation Accuracy: 31.7\n",
            "[19/150]: Training Loss: 1.1342673721313477, Training Accuracy: 66.51\n",
            "Validation Loss: 3.621978481863714, Validation Accuracy: 31.07\n",
            "[20/150]: Training Loss: 1.057302718448639, Training Accuracy: 68.56\n",
            "Validation Loss: 3.8245642261140667, Validation Accuracy: 31.16\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 198.79228443704594, Test Accuracy: 3.39\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▇▃▃▁▁▂▂▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▁▆▇▅████████▇█▇▇██▇█▇███▇███▇▇█▇███▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>3.39</td></tr><tr><td>Test Loss</td><td>198.79228</td></tr><tr><td>Train Accuracy</td><td>68.56</td></tr><tr><td>Train Loss</td><td>1.0573</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_034959-j16vayol/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_035204-l4zoq5dd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd' target=\"_blank\">learning_rate=0.001 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.108658624267578, Training Accuracy: 6.6075\n",
            "Validation Loss: 3.7040131304674087, Validation Accuracy: 12.1\n",
            "[2/150]: Training Loss: 3.5060665573120118, Training Accuracy: 16.015\n",
            "Validation Loss: 3.3598362321306947, Validation Accuracy: 18.42\n",
            "[3/150]: Training Loss: 3.1865245040893555, Training Accuracy: 21.7275\n",
            "Validation Loss: 3.128495198146553, Validation Accuracy: 22.84\n",
            "[4/150]: Training Loss: 2.9679495239257814, Training Accuracy: 25.8525\n",
            "Validation Loss: 2.9615708293428846, Validation Accuracy: 26.83\n",
            "[5/150]: Training Loss: 2.797335900115967, Training Accuracy: 29.515\n",
            "Validation Loss: 2.9264626108157406, Validation Accuracy: 26.96\n",
            "[6/150]: Training Loss: 2.6617756172180176, Training Accuracy: 32.13\n",
            "Validation Loss: 2.8755724490827816, Validation Accuracy: 28.35\n",
            "[7/150]: Training Loss: 2.54268462638855, Training Accuracy: 34.435\n",
            "Validation Loss: 2.7718013547788, Validation Accuracy: 30.25\n",
            "[8/150]: Training Loss: 2.424749851608276, Training Accuracy: 37.1025\n",
            "Validation Loss: 2.7567353567500024, Validation Accuracy: 31.11\n",
            "[9/150]: Training Loss: 2.3229350467681886, Training Accuracy: 39.08\n",
            "Validation Loss: 2.7088757032042095, Validation Accuracy: 32.08\n",
            "[10/150]: Training Loss: 2.2215481660842897, Training Accuracy: 41.4275\n",
            "Validation Loss: 2.75471066821153, Validation Accuracy: 31.83\n",
            "[11/150]: Training Loss: 2.134213170814514, Training Accuracy: 43.25\n",
            "Validation Loss: 2.8348169676057853, Validation Accuracy: 31.64\n",
            "[12/150]: Training Loss: 2.051020913696289, Training Accuracy: 44.9475\n",
            "Validation Loss: 2.7630925406316282, Validation Accuracy: 32.98\n",
            "[13/150]: Training Loss: 1.960823282814026, Training Accuracy: 46.7775\n",
            "Validation Loss: 2.7420302629470825, Validation Accuracy: 33.4\n",
            "[14/150]: Training Loss: 1.8790180908203125, Training Accuracy: 48.855\n",
            "Validation Loss: 2.7926843044864142, Validation Accuracy: 33.79\n",
            "[15/150]: Training Loss: 1.80182436504364, Training Accuracy: 50.6875\n",
            "Validation Loss: 2.8919099911003356, Validation Accuracy: 33.19\n",
            "[16/150]: Training Loss: 1.727850655937195, Training Accuracy: 52.3025\n",
            "Validation Loss: 2.8964282950018623, Validation Accuracy: 33.5\n",
            "[17/150]: Training Loss: 1.6616035480499267, Training Accuracy: 53.85\n",
            "Validation Loss: 3.033423226350432, Validation Accuracy: 32.53\n",
            "[18/150]: Training Loss: 1.5868734314918518, Training Accuracy: 55.5\n",
            "Validation Loss: 3.032342692089688, Validation Accuracy: 33.04\n",
            "[19/150]: Training Loss: 1.5333876008987426, Training Accuracy: 56.7525\n",
            "Validation Loss: 3.079486154446936, Validation Accuracy: 32.97\n",
            "[20/150]: Training Loss: 1.4575365795135498, Training Accuracy: 58.77\n",
            "Validation Loss: 3.181258452166418, Validation Accuracy: 32.82\n",
            "[21/150]: Training Loss: 1.39640816450119, Training Accuracy: 60.14\n",
            "Validation Loss: 3.216413262543405, Validation Accuracy: 32.76\n",
            "[22/150]: Training Loss: 1.337559293270111, Training Accuracy: 61.5275\n",
            "Validation Loss: 3.3633130872325534, Validation Accuracy: 31.73\n",
            "[23/150]: Training Loss: 1.2836213255882263, Training Accuracy: 62.6875\n",
            "Validation Loss: 3.4558656868661286, Validation Accuracy: 32.33\n",
            "[24/150]: Training Loss: 1.2338986722946168, Training Accuracy: 64.2825\n",
            "Validation Loss: 3.4873077155678134, Validation Accuracy: 32.02\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 156.72609671817463, Test Accuracy: 3.33\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▅▁▂▃▂▃▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>Test Loss</td><td>▇▇▁▂▁▄▄▇█▆▃▄▅▆▅▅▆▆▆▆▅▇▇▆▆▇▆▆▅▆▆▆▅▆▅▅▆▆▅▅</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>3.33</td></tr><tr><td>Test Loss</td><td>156.7261</td></tr><tr><td>Train Accuracy</td><td>64.2825</td></tr><tr><td>Train Loss</td><td>1.2339</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_035204-l4zoq5dd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_035441-y4ask5ys</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys' target=\"_blank\">learning_rate=0.001 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.053023485183716, Training Accuracy: 7.3\n",
            "Validation Loss: 3.71551227873298, Validation Accuracy: 12.6\n",
            "[2/150]: Training Loss: 3.4803551902770997, Training Accuracy: 16.4825\n",
            "Validation Loss: 3.3104240119836894, Validation Accuracy: 19.6\n",
            "[3/150]: Training Loss: 3.1457452793121337, Training Accuracy: 22.71\n",
            "Validation Loss: 3.1647931253834134, Validation Accuracy: 22.36\n",
            "[4/150]: Training Loss: 2.9213612785339356, Training Accuracy: 26.8375\n",
            "Validation Loss: 2.967867093481076, Validation Accuracy: 26.13\n",
            "[5/150]: Training Loss: 2.7346919048309326, Training Accuracy: 30.745\n",
            "Validation Loss: 2.870804744161618, Validation Accuracy: 28.43\n",
            "[6/150]: Training Loss: 2.580346668434143, Training Accuracy: 33.8975\n",
            "Validation Loss: 2.813369401700937, Validation Accuracy: 29.97\n",
            "[7/150]: Training Loss: 2.4525743492126466, Training Accuracy: 36.445\n",
            "Validation Loss: 2.8111886962963517, Validation Accuracy: 31.76\n",
            "[8/150]: Training Loss: 2.3342738655090334, Training Accuracy: 38.8\n",
            "Validation Loss: 2.7751463211266096, Validation Accuracy: 31.29\n",
            "[9/150]: Training Loss: 2.230911629486084, Training Accuracy: 40.89\n",
            "Validation Loss: 2.733239137443008, Validation Accuracy: 32.32\n",
            "[10/150]: Training Loss: 2.127202660560608, Training Accuracy: 43.0575\n",
            "Validation Loss: 2.740311449500406, Validation Accuracy: 32.67\n",
            "[11/150]: Training Loss: 2.0320646129608155, Training Accuracy: 45.3025\n",
            "Validation Loss: 2.7493710031934606, Validation Accuracy: 33.28\n",
            "[12/150]: Training Loss: 1.9452043891906738, Training Accuracy: 47.2675\n",
            "Validation Loss: 2.776471818328663, Validation Accuracy: 33.2\n",
            "[13/150]: Training Loss: 1.8665077058792114, Training Accuracy: 49.035\n",
            "Validation Loss: 2.801089420440091, Validation Accuracy: 32.56\n",
            "[14/150]: Training Loss: 1.782764630126953, Training Accuracy: 50.66\n",
            "Validation Loss: 2.869801163673401, Validation Accuracy: 33.19\n",
            "[15/150]: Training Loss: 1.6995231729507447, Training Accuracy: 52.77\n",
            "Validation Loss: 3.0037558374890856, Validation Accuracy: 32.92\n",
            "[16/150]: Training Loss: 1.6300209772109986, Training Accuracy: 54.165\n",
            "Validation Loss: 2.9989138818850183, Validation Accuracy: 32.86\n",
            "[17/150]: Training Loss: 1.5460361848831177, Training Accuracy: 56.605\n",
            "Validation Loss: 3.0711293569795646, Validation Accuracy: 32.69\n",
            "[18/150]: Training Loss: 1.48129998254776, Training Accuracy: 57.9875\n",
            "Validation Loss: 3.1773584519222284, Validation Accuracy: 31.98\n",
            "[19/150]: Training Loss: 1.4184319323539734, Training Accuracy: 59.43\n",
            "Validation Loss: 3.278900881481778, Validation Accuracy: 31.73\n",
            "[20/150]: Training Loss: 1.3525760270118714, Training Accuracy: 60.9775\n",
            "Validation Loss: 3.3168472605905714, Validation Accuracy: 31.51\n",
            "[21/150]: Training Loss: 1.275265542125702, Training Accuracy: 63.035\n",
            "Validation Loss: 3.5034019506660994, Validation Accuracy: 31.12\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 67.07529235645464, Test Accuracy: 5.39\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▁▂▇▃▁▂▁▂▂▂▃▃▃▄▄▄▄▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>Test Loss</td><td>▁█▅▃▃▃▃▄▅▄▃▂▂▃▃▂▄▄▄▄▃▃▃▃▃▃▃▃▃▂▃▃▃▃▃▃▃▂▂▂</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>5.39</td></tr><tr><td>Test Loss</td><td>67.07529</td></tr><tr><td>Train Accuracy</td><td>63.035</td></tr><tr><td>Train Loss</td><td>1.27527</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_035441-y4ask5ys/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.0001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_035656-za8h22vt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt' target=\"_blank\">learning_rate=0.01 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.622148120880127, Training Accuracy: 0.935\n",
            "Validation Loss: 4.609954472560032, Validation Accuracy: 0.99\n",
            "[2/150]: Training Loss: 4.609106838226318, Training Accuracy: 0.9475\n",
            "Validation Loss: 4.611106529357327, Validation Accuracy: 0.9\n",
            "[3/150]: Training Loss: 4.608888108062744, Training Accuracy: 0.9925\n",
            "Validation Loss: 4.608767968074531, Validation Accuracy: 0.84\n",
            "[4/150]: Training Loss: 4.608837638854981, Training Accuracy: 0.935\n",
            "Validation Loss: 4.610516976399027, Validation Accuracy: 0.89\n",
            "[5/150]: Training Loss: 4.608827792358398, Training Accuracy: 0.9775\n",
            "Validation Loss: 4.610510003035236, Validation Accuracy: 0.9\n",
            "[6/150]: Training Loss: 4.608845700073243, Training Accuracy: 0.935\n",
            "Validation Loss: 4.609803257474474, Validation Accuracy: 0.83\n",
            "[7/150]: Training Loss: 4.608741146087646, Training Accuracy: 0.9625\n",
            "Validation Loss: 4.60897787665106, Validation Accuracy: 0.89\n",
            "[8/150]: Training Loss: 4.608720026397705, Training Accuracy: 0.9775\n",
            "Validation Loss: 4.610442146374162, Validation Accuracy: 0.89\n",
            "[9/150]: Training Loss: 4.60900821762085, Training Accuracy: 1.005\n",
            "Validation Loss: 4.609640285467646, Validation Accuracy: 0.9\n",
            "[10/150]: Training Loss: 4.609175831604004, Training Accuracy: 0.9175\n",
            "Validation Loss: 4.609878120908312, Validation Accuracy: 0.95\n",
            "[11/150]: Training Loss: 4.609065142822265, Training Accuracy: 0.8625\n",
            "Validation Loss: 4.610616553361249, Validation Accuracy: 0.9\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 19.343990089027745, Test Accuracy: 1.01\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▃▃▆▆▆▆▇▇▇▇▇▇▇▇█▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Test Loss</td><td>▅▄█▅▅▄▂▄▅▃▃▃▃▄▃▃▃▃▃▃▂▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▅▅▇▅▇▅▆▇█▄▁</td></tr><tr><td>Train Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.01</td></tr><tr><td>Test Loss</td><td>19.34399</td></tr><tr><td>Train Accuracy</td><td>0.8625</td></tr><tr><td>Train Loss</td><td>4.60907</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_035656-za8h22vt/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_035822-1oq8f1cm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm' target=\"_blank\">learning_rate=0.01 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.625637398529053, Training Accuracy: 0.885\n",
            "Validation Loss: 4.60909015509733, Validation Accuracy: 1.06\n",
            "[2/150]: Training Loss: 4.608977200317383, Training Accuracy: 0.94\n",
            "Validation Loss: 4.609363568056921, Validation Accuracy: 0.92\n",
            "[3/150]: Training Loss: 4.608765270996094, Training Accuracy: 0.9275\n",
            "Validation Loss: 4.610460181145152, Validation Accuracy: 0.9\n",
            "[4/150]: Training Loss: 4.609055269622803, Training Accuracy: 0.855\n",
            "Validation Loss: 4.609649321076217, Validation Accuracy: 0.91\n",
            "[5/150]: Training Loss: 4.608893507385254, Training Accuracy: 0.965\n",
            "Validation Loss: 4.610334041012321, Validation Accuracy: 1.15\n",
            "[6/150]: Training Loss: 4.609225297546387, Training Accuracy: 1.0125\n",
            "Validation Loss: 4.608022522774472, Validation Accuracy: 1.16\n",
            "[7/150]: Training Loss: 4.609016616821289, Training Accuracy: 0.9875\n",
            "Validation Loss: 4.609603547746209, Validation Accuracy: 0.82\n",
            "[8/150]: Training Loss: 4.609039859008789, Training Accuracy: 1.0025\n",
            "Validation Loss: 4.608962860836345, Validation Accuracy: 0.88\n",
            "[9/150]: Training Loss: 4.608584417724609, Training Accuracy: 0.98\n",
            "Validation Loss: 4.609565239803047, Validation Accuracy: 0.92\n",
            "[10/150]: Training Loss: 4.609195093536377, Training Accuracy: 0.9675\n",
            "Validation Loss: 4.609652103132503, Validation Accuracy: 0.9\n",
            "[11/150]: Training Loss: 4.608826132202148, Training Accuracy: 0.975\n",
            "Validation Loss: 4.610933561993253, Validation Accuracy: 0.95\n",
            "[12/150]: Training Loss: 4.609026128387451, Training Accuracy: 1.0425\n",
            "Validation Loss: 4.609673776444356, Validation Accuracy: 0.81\n",
            "[13/150]: Training Loss: 4.60898267364502, Training Accuracy: 0.9275\n",
            "Validation Loss: 4.6093105510541585, Validation Accuracy: 0.89\n",
            "[14/150]: Training Loss: 4.608909271240234, Training Accuracy: 0.885\n",
            "Validation Loss: 4.610904125650977, Validation Accuracy: 0.82\n",
            "[15/150]: Training Loss: 4.609115404510498, Training Accuracy: 0.925\n",
            "Validation Loss: 4.609269661508548, Validation Accuracy: 0.9\n",
            "[16/150]: Training Loss: 4.608642578887939, Training Accuracy: 0.9875\n",
            "Validation Loss: 4.610215545459917, Validation Accuracy: 0.81\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 51.00807974748551, Test Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▆▅▇▇███▇▇▇▆▆▆▆▆▇▇▇▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>Test Loss</td><td>▁▇█▆▅▆▆▇█▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▂▄▄▁▅▇▆▇▆▅▅█▄▂▄▆</td></tr><tr><td>Train Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.0</td></tr><tr><td>Test Loss</td><td>51.00808</td></tr><tr><td>Train Accuracy</td><td>0.9875</td></tr><tr><td>Train Loss</td><td>4.60864</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_035822-1oq8f1cm/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_040031-bgr70v9g</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g' target=\"_blank\">learning_rate=0.01 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.626314550018311, Training Accuracy: 0.9725\n",
            "Validation Loss: 4.610879454643103, Validation Accuracy: 0.88\n",
            "[2/150]: Training Loss: 4.609021481323242, Training Accuracy: 0.9225\n",
            "Validation Loss: 4.6096283493527945, Validation Accuracy: 1.0\n",
            "[3/150]: Training Loss: 4.608779692077637, Training Accuracy: 0.9575\n",
            "Validation Loss: 4.610723838684665, Validation Accuracy: 1.07\n",
            "[4/150]: Training Loss: 4.609119167327881, Training Accuracy: 0.9125\n",
            "Validation Loss: 4.60863508236636, Validation Accuracy: 0.89\n",
            "[5/150]: Training Loss: 4.608814185333252, Training Accuracy: 0.95\n",
            "Validation Loss: 4.611634175488903, Validation Accuracy: 0.93\n",
            "[6/150]: Training Loss: 4.6092648048400875, Training Accuracy: 0.8575\n",
            "Validation Loss: 4.610189516832874, Validation Accuracy: 0.92\n",
            "[7/150]: Training Loss: 4.6090187705993655, Training Accuracy: 1.005\n",
            "Validation Loss: 4.610130288798338, Validation Accuracy: 0.88\n",
            "[8/150]: Training Loss: 4.608886881256104, Training Accuracy: 1.01\n",
            "Validation Loss: 4.610127786162552, Validation Accuracy: 0.84\n",
            "[9/150]: Training Loss: 4.608884384155274, Training Accuracy: 1.0175\n",
            "Validation Loss: 4.610361232879056, Validation Accuracy: 0.95\n",
            "[10/150]: Training Loss: 4.60871681137085, Training Accuracy: 1.0025\n",
            "Validation Loss: 4.6113608263100785, Validation Accuracy: 1.03\n",
            "[11/150]: Training Loss: 4.608806131744385, Training Accuracy: 1.035\n",
            "Validation Loss: 4.609321922253651, Validation Accuracy: 0.94\n",
            "[12/150]: Training Loss: 4.6090567108154294, Training Accuracy: 0.9075\n",
            "Validation Loss: 4.6090145414801915, Validation Accuracy: 0.9\n",
            "[13/150]: Training Loss: 4.608706290435791, Training Accuracy: 0.9225\n",
            "Validation Loss: 4.609259438362851, Validation Accuracy: 0.82\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 7.142075587230123, Test Accuracy: 1.08\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▃█▃▁▁▁▃▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>Test Loss</td><td>▁▅▇█▆▆▆▆▇▆▆▆▆▆▆▆▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▆▄▅▃▅▁▇▇▇▇█▃▄</td></tr><tr><td>Train Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.08</td></tr><tr><td>Test Loss</td><td>7.14208</td></tr><tr><td>Train Accuracy</td><td>0.9225</td></tr><tr><td>Train Loss</td><td>4.60871</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_040031-bgr70v9g/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "\n",
        "for lr in learning_rates:\n",
        "  for wd in weight_decays:\n",
        "\n",
        "    print('='*50)\n",
        "    print(f'Hyperparameter with lr:{lr} and wd:{wd}')\n",
        "    print('='*50)\n",
        "\n",
        "    hyperparameters = {'learning_rate': lr,\n",
        "                       'weight_decay' : wd\n",
        "                       }\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(num_epochs, model, train_loader, validation_loader, test_loader, optimizer, scheduler, criterion, device, 'AdamW-HyperParameterTuning', hyperparameters=hyperparameters, is_wandb = True, n_epochs_stop = 10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training Using Best Hyperparameters Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "63dd47b50bdc44e88edd97d14585f7ea",
            "5e0116dc735e4ad995a5b1a039c6a55a",
            "8cf1e952ed294ccdbc3bb2275b5d3efd",
            "7421027bfcb34262a1b99d297e49bf8e",
            "76dfea14213f4e66b269b901150c6cba",
            "d1e4a03094b94c34ac235ffbd0c7fa06",
            "c8e410ff55fc448bbe87c8975a552e88",
            "f49251ce876f4d9292a4179b48ebb22a"
          ]
        },
        "id": "dC9sTmuvEymk",
        "outputId": "66657750-ed1c-435f-c9e5-a2cc4271560e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:z7dx25wy) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/z7dx25wy' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/z7dx25wy</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_002941-z7dx25wy/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:z7dx25wy). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_003350-ulxra0pg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg' target=\"_blank\">learning_rate=0.001 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 3.863059723773576, Training Accuracy: 10.28\n",
            "Validation Loss: 3.371582930255088, Validation Accuracy: 17.92\n",
            "[2/150]: Training Loss: 3.2885083447941734, Training Accuracy: 19.766\n",
            "Validation Loss: 3.0681341438536434, Validation Accuracy: 23.43\n",
            "[3/150]: Training Loss: 3.0068617961595736, Training Accuracy: 25.216\n",
            "Validation Loss: 2.8509516169311135, Validation Accuracy: 28.09\n",
            "[4/150]: Training Loss: 2.8243453850221756, Training Accuracy: 28.812\n",
            "Validation Loss: 2.6608721253218923, Validation Accuracy: 31.85\n",
            "[5/150]: Training Loss: 2.7198512532826884, Training Accuracy: 30.66\n",
            "Validation Loss: 2.614684703243766, Validation Accuracy: 32.43\n",
            "[6/150]: Training Loss: 2.6302451995937415, Training Accuracy: 32.706\n",
            "Validation Loss: 2.523006957807359, Validation Accuracy: 35.09\n",
            "[7/150]: Training Loss: 2.5562818654053046, Training Accuracy: 34.01\n",
            "Validation Loss: 2.4693225895523265, Validation Accuracy: 36.29\n",
            "[8/150]: Training Loss: 2.503678919409242, Training Accuracy: 35.054\n",
            "Validation Loss: 2.4447873132244036, Validation Accuracy: 36.7\n",
            "[9/150]: Training Loss: 2.4561512488538346, Training Accuracy: 36.162\n",
            "Validation Loss: 2.41026672663962, Validation Accuracy: 37.97\n",
            "[10/150]: Training Loss: 2.4110919961234187, Training Accuracy: 37.32\n",
            "Validation Loss: 2.3787831933635055, Validation Accuracy: 37.75\n",
            "[11/150]: Training Loss: 2.374972592686753, Training Accuracy: 37.942\n",
            "Validation Loss: 2.419552489450783, Validation Accuracy: 38.02\n",
            "[12/150]: Training Loss: 2.341363592220999, Training Accuracy: 38.71\n",
            "Validation Loss: 2.353040744544594, Validation Accuracy: 39.18\n",
            "[13/150]: Training Loss: 2.3113136654314785, Training Accuracy: 39.284\n",
            "Validation Loss: 2.329031374044479, Validation Accuracy: 40.04\n",
            "[14/150]: Training Loss: 2.286358056928191, Training Accuracy: 39.702\n",
            "Validation Loss: 2.353606297711658, Validation Accuracy: 38.89\n",
            "[15/150]: Training Loss: 2.255754057251279, Training Accuracy: 40.382\n",
            "Validation Loss: 2.313938088477797, Validation Accuracy: 40.46\n",
            "[16/150]: Training Loss: 2.2417570858660256, Training Accuracy: 40.766\n",
            "Validation Loss: 2.245134223798278, Validation Accuracy: 41.86\n",
            "[17/150]: Training Loss: 2.229791578886759, Training Accuracy: 40.774\n",
            "Validation Loss: 2.2972328845103074, Validation Accuracy: 40.73\n",
            "[18/150]: Training Loss: 2.198040244981761, Training Accuracy: 41.792\n",
            "Validation Loss: 2.284319670337021, Validation Accuracy: 40.6\n",
            "[19/150]: Training Loss: 2.1809934542307157, Training Accuracy: 41.802\n",
            "Validation Loss: 2.2219127932931206, Validation Accuracy: 41.97\n",
            "[20/150]: Training Loss: 2.172169676827043, Training Accuracy: 41.862\n",
            "Validation Loss: 2.3093747029638596, Validation Accuracy: 40.82\n",
            "[21/150]: Training Loss: 2.1469304264353974, Training Accuracy: 42.644\n",
            "Validation Loss: 2.2553204001894422, Validation Accuracy: 41.67\n",
            "[22/150]: Training Loss: 2.1338748418156754, Training Accuracy: 43.018\n",
            "Validation Loss: 2.300890388002821, Validation Accuracy: 40.66\n",
            "[23/150]: Training Loss: 2.120842968259016, Training Accuracy: 43.254\n",
            "Validation Loss: 2.2334311372914892, Validation Accuracy: 41.97\n",
            "[24/150]: Training Loss: 2.0970673977261614, Training Accuracy: 43.616\n",
            "Validation Loss: 2.2538574728996132, Validation Accuracy: 42.09\n",
            "[25/150]: Training Loss: 2.0897668813500565, Training Accuracy: 43.946\n",
            "Validation Loss: 2.2414238969231866, Validation Accuracy: 41.72\n",
            "[26/150]: Training Loss: 2.0756560088423512, Training Accuracy: 44.038\n",
            "Validation Loss: 2.1973593523547907, Validation Accuracy: 43.01\n",
            "[27/150]: Training Loss: 2.057910572537376, Training Accuracy: 44.59\n",
            "Validation Loss: 2.2248903679999574, Validation Accuracy: 42.7\n",
            "[28/150]: Training Loss: 2.043146767274803, Training Accuracy: 45.032\n",
            "Validation Loss: 2.210173088274184, Validation Accuracy: 42.56\n",
            "[29/150]: Training Loss: 2.03314662345535, Training Accuracy: 45.244\n",
            "Validation Loss: 2.2085875697955966, Validation Accuracy: 42.59\n",
            "[30/150]: Training Loss: 2.028535710576245, Training Accuracy: 45.074\n",
            "Validation Loss: 2.2007097385491536, Validation Accuracy: 43.4\n",
            "[31/150]: Training Loss: 2.009151526576723, Training Accuracy: 45.862\n",
            "Validation Loss: 2.201018148926413, Validation Accuracy: 43.24\n",
            "[32/150]: Training Loss: 1.9960764486466527, Training Accuracy: 46.118\n",
            "Validation Loss: 2.207929955925911, Validation Accuracy: 43.19\n",
            "[33/150]: Training Loss: 1.988575564930811, Training Accuracy: 46.282\n",
            "Validation Loss: 2.2023386712286883, Validation Accuracy: 43.24\n",
            "[34/150]: Training Loss: 1.9779068563905213, Training Accuracy: 46.246\n",
            "Validation Loss: 2.1736220698447744, Validation Accuracy: 43.85\n",
            "[35/150]: Training Loss: 1.9696562569159681, Training Accuracy: 46.612\n",
            "Validation Loss: 2.2412819467532406, Validation Accuracy: 42.97\n",
            "[36/150]: Training Loss: 1.9518849866469499, Training Accuracy: 47.002\n",
            "Validation Loss: 2.142467891334728, Validation Accuracy: 44.48\n",
            "[37/150]: Training Loss: 1.942737179491526, Training Accuracy: 47.244\n",
            "Validation Loss: 2.1808249912444193, Validation Accuracy: 43.96\n",
            "[38/150]: Training Loss: 1.934893620441027, Training Accuracy: 47.278\n",
            "Validation Loss: 2.194337487220764, Validation Accuracy: 44.03\n",
            "[39/150]: Training Loss: 1.9254444508296449, Training Accuracy: 47.672\n",
            "Validation Loss: 2.1627620777506738, Validation Accuracy: 44.64\n",
            "[40/150]: Training Loss: 1.9116085487253525, Training Accuracy: 47.786\n",
            "Validation Loss: 2.143426476770146, Validation Accuracy: 44.69\n",
            "[41/150]: Training Loss: 1.8990542265155432, Training Accuracy: 48.106\n",
            "Validation Loss: 2.1570872994744854, Validation Accuracy: 44.18\n",
            "[42/150]: Training Loss: 1.8952381147448059, Training Accuracy: 48.148\n",
            "Validation Loss: 2.179479962701251, Validation Accuracy: 44.42\n",
            "[43/150]: Training Loss: 1.8831077039699116, Training Accuracy: 48.608\n",
            "Validation Loss: 2.14893990337469, Validation Accuracy: 44.65\n",
            "[44/150]: Training Loss: 1.8653588525169646, Training Accuracy: 49.024\n",
            "Validation Loss: 2.139679863954046, Validation Accuracy: 44.98\n",
            "[45/150]: Training Loss: 1.8571323093855778, Training Accuracy: 49.132\n",
            "Validation Loss: 2.1511670715489966, Validation Accuracy: 45.05\n",
            "[46/150]: Training Loss: 1.851196085248152, Training Accuracy: 49.268\n",
            "Validation Loss: 2.1228003342440176, Validation Accuracy: 45.15\n",
            "[47/150]: Training Loss: 1.8476994010188696, Training Accuracy: 49.152\n",
            "Validation Loss: 2.2052687976011045, Validation Accuracy: 44.33\n",
            "[48/150]: Training Loss: 1.834682262919443, Training Accuracy: 49.48\n",
            "Validation Loss: 2.167970584456328, Validation Accuracy: 44.5\n",
            "[49/150]: Training Loss: 1.8160154127403902, Training Accuracy: 50.016\n",
            "Validation Loss: 2.1706748312445963, Validation Accuracy: 44.33\n",
            "[50/150]: Training Loss: 1.8177930782823002, Training Accuracy: 49.864\n",
            "Validation Loss: 2.1794446357496224, Validation Accuracy: 44.96\n",
            "[51/150]: Training Loss: 1.8067406710913725, Training Accuracy: 50.086\n",
            "Validation Loss: 2.1342575109688338, Validation Accuracy: 45.96\n",
            "[52/150]: Training Loss: 1.7917756205019744, Training Accuracy: 50.474\n",
            "Validation Loss: 2.1779661793617686, Validation Accuracy: 45.2\n",
            "[53/150]: Training Loss: 1.7835827201528622, Training Accuracy: 50.574\n",
            "Validation Loss: 2.105961331136667, Validation Accuracy: 46.26\n",
            "[54/150]: Training Loss: 1.775593501527596, Training Accuracy: 50.768\n",
            "Validation Loss: 2.135809161860472, Validation Accuracy: 45.59\n",
            "[55/150]: Training Loss: 1.774858244087385, Training Accuracy: 50.898\n",
            "Validation Loss: 2.129430113324694, Validation Accuracy: 45.8\n",
            "[56/150]: Training Loss: 1.7574954881997364, Training Accuracy: 51.258\n",
            "Validation Loss: 2.1614714724243065, Validation Accuracy: 45.34\n",
            "[57/150]: Training Loss: 1.7551402684367832, Training Accuracy: 51.536\n",
            "Validation Loss: 2.116780410906312, Validation Accuracy: 45.57\n",
            "[58/150]: Training Loss: 1.7323481986284865, Training Accuracy: 51.87\n",
            "Validation Loss: 2.1469044146264435, Validation Accuracy: 46.29\n",
            "[59/150]: Training Loss: 1.731923724684264, Training Accuracy: 52.004\n",
            "Validation Loss: 2.103028847913074, Validation Accuracy: 46.36\n",
            "[60/150]: Training Loss: 1.7310465676400362, Training Accuracy: 52.0\n",
            "Validation Loss: 2.1536702441561753, Validation Accuracy: 45.7\n",
            "[61/150]: Training Loss: 1.7244189166656845, Training Accuracy: 51.944\n",
            "Validation Loss: 2.140022467655741, Validation Accuracy: 45.48\n",
            "[62/150]: Training Loss: 1.7049367572645397, Training Accuracy: 52.6\n",
            "Validation Loss: 2.1410929146845628, Validation Accuracy: 46.43\n",
            "[63/150]: Training Loss: 1.6979511242998226, Training Accuracy: 52.706\n",
            "Validation Loss: 2.159957699714952, Validation Accuracy: 45.77\n",
            "[64/150]: Training Loss: 1.6934019074110729, Training Accuracy: 52.83\n",
            "Validation Loss: 2.1361425302590535, Validation Accuracy: 46.59\n",
            "[65/150]: Training Loss: 1.6794907693058023, Training Accuracy: 53.024\n",
            "Validation Loss: 2.1693425915043827, Validation Accuracy: 46.15\n",
            "[66/150]: Training Loss: 1.6717879327056964, Training Accuracy: 53.454\n",
            "Validation Loss: 2.1304182694975737, Validation Accuracy: 46.71\n",
            "[67/150]: Training Loss: 1.6595926739828055, Training Accuracy: 53.614\n",
            "Validation Loss: 2.120200866346906, Validation Accuracy: 46.39\n",
            "[68/150]: Training Loss: 1.648870440395287, Training Accuracy: 54.184\n",
            "Validation Loss: 2.098656458459842, Validation Accuracy: 46.87\n",
            "[69/150]: Training Loss: 1.63317440499735, Training Accuracy: 53.856\n",
            "Validation Loss: 2.10397704163934, Validation Accuracy: 47.28\n",
            "[70/150]: Training Loss: 1.6387051284465644, Training Accuracy: 54.282\n",
            "Validation Loss: 2.0964533455052954, Validation Accuracy: 47.14\n",
            "[71/150]: Training Loss: 1.6188073368633495, Training Accuracy: 54.606\n",
            "Validation Loss: 2.12077300032233, Validation Accuracy: 46.67\n",
            "[72/150]: Training Loss: 1.6113011437608762, Training Accuracy: 54.874\n",
            "Validation Loss: 2.1084914108750166, Validation Accuracy: 47.47\n",
            "[73/150]: Training Loss: 1.6181168059253936, Training Accuracy: 54.584\n",
            "Validation Loss: 2.0926199581972353, Validation Accuracy: 47.8\n",
            "[74/150]: Training Loss: 1.6009672808525202, Training Accuracy: 55.14\n",
            "Validation Loss: 2.1137902524061265, Validation Accuracy: 47.2\n",
            "[75/150]: Training Loss: 1.5877168545942477, Training Accuracy: 55.52\n",
            "Validation Loss: 2.0988229885222807, Validation Accuracy: 47.29\n",
            "[76/150]: Training Loss: 1.5841481110933797, Training Accuracy: 55.536\n",
            "Validation Loss: 2.130301916675203, Validation Accuracy: 47.01\n",
            "[77/150]: Training Loss: 1.572215504810938, Training Accuracy: 55.894\n",
            "Validation Loss: 2.149995141727909, Validation Accuracy: 47.05\n",
            "[78/150]: Training Loss: 1.5624580438179738, Training Accuracy: 55.868\n",
            "Validation Loss: 2.111784276688934, Validation Accuracy: 47.42\n",
            "[79/150]: Training Loss: 1.5614525537051813, Training Accuracy: 55.984\n",
            "Validation Loss: 2.114105687019931, Validation Accuracy: 47.26\n",
            "[80/150]: Training Loss: 1.539920823577115, Training Accuracy: 56.65\n",
            "Validation Loss: 2.113597672456389, Validation Accuracy: 48.04\n",
            "[81/150]: Training Loss: 1.5321300439822398, Training Accuracy: 56.72\n",
            "Validation Loss: 2.115057561048277, Validation Accuracy: 47.52\n",
            "[82/150]: Training Loss: 1.5347046963394146, Training Accuracy: 56.472\n",
            "Validation Loss: 2.120380314292422, Validation Accuracy: 47.69\n",
            "[83/150]: Training Loss: 1.5234107173922118, Training Accuracy: 56.854\n",
            "Validation Loss: 2.117662426772391, Validation Accuracy: 47.66\n",
            "[84/150]: Training Loss: 1.5176254797469624, Training Accuracy: 57.066\n",
            "Validation Loss: 2.1148886255397916, Validation Accuracy: 47.15\n",
            "[85/150]: Training Loss: 1.5007459478610008, Training Accuracy: 57.452\n",
            "Validation Loss: 2.114618365931663, Validation Accuracy: 47.39\n",
            "[86/150]: Training Loss: 1.4968964526872806, Training Accuracy: 57.466\n",
            "Validation Loss: 2.1021699586491676, Validation Accuracy: 47.78\n",
            "[87/150]: Training Loss: 1.4844805052518235, Training Accuracy: 57.862\n",
            "Validation Loss: 2.1039314968570784, Validation Accuracy: 47.72\n",
            "[88/150]: Training Loss: 1.4738873809652255, Training Accuracy: 58.114\n",
            "Validation Loss: 2.097645890181232, Validation Accuracy: 48.24\n",
            "[89/150]: Training Loss: 1.483021430271056, Training Accuracy: 57.876\n",
            "Validation Loss: 2.108822873443555, Validation Accuracy: 47.97\n",
            "[90/150]: Training Loss: 1.4626602787343437, Training Accuracy: 58.702\n",
            "Validation Loss: 2.0972186729406856, Validation Accuracy: 47.88\n",
            "[91/150]: Training Loss: 1.4539960583152673, Training Accuracy: 58.564\n",
            "Validation Loss: 2.1077453277672933, Validation Accuracy: 48.02\n",
            "[92/150]: Training Loss: 1.4446301146236527, Training Accuracy: 58.7\n",
            "Validation Loss: 2.0844076651676446, Validation Accuracy: 48.44\n",
            "[93/150]: Training Loss: 1.4391108949471008, Training Accuracy: 59.026\n",
            "Validation Loss: 2.093005408147338, Validation Accuracy: 48.37\n",
            "[94/150]: Training Loss: 1.423930914581889, Training Accuracy: 59.616\n",
            "Validation Loss: 2.1331047563795833, Validation Accuracy: 47.89\n",
            "[95/150]: Training Loss: 1.41947230582347, Training Accuracy: 59.502\n",
            "Validation Loss: 2.1126899483856882, Validation Accuracy: 48.0\n",
            "[96/150]: Training Loss: 1.4149078359384366, Training Accuracy: 59.56\n",
            "Validation Loss: 2.0845493116196554, Validation Accuracy: 48.7\n",
            "[97/150]: Training Loss: 1.3990580768841308, Training Accuracy: 60.132\n",
            "Validation Loss: 2.1009285654991294, Validation Accuracy: 48.44\n",
            "[98/150]: Training Loss: 1.4016986463380896, Training Accuracy: 60.082\n",
            "Validation Loss: 2.1022102696121117, Validation Accuracy: 48.23\n",
            "[99/150]: Training Loss: 1.391786497572194, Training Accuracy: 60.44\n",
            "Validation Loss: 2.087019986407772, Validation Accuracy: 48.33\n",
            "[100/150]: Training Loss: 1.3808096985682807, Training Accuracy: 60.598\n",
            "Validation Loss: 2.1079373640619266, Validation Accuracy: 48.46\n",
            "[101/150]: Training Loss: 1.375501683377244, Training Accuracy: 60.572\n",
            "Validation Loss: 2.1078098131592866, Validation Accuracy: 48.82\n",
            "[102/150]: Training Loss: 1.3738657930470488, Training Accuracy: 60.484\n",
            "Validation Loss: 2.118699251466496, Validation Accuracy: 47.93\n",
            "[103/150]: Training Loss: 1.366849816973557, Training Accuracy: 60.848\n",
            "Validation Loss: 2.07916833564734, Validation Accuracy: 49.02\n",
            "[104/150]: Training Loss: 1.3610880574606874, Training Accuracy: 60.932\n",
            "Validation Loss: 2.087844172860407, Validation Accuracy: 48.91\n",
            "[105/150]: Training Loss: 1.34294292833799, Training Accuracy: 61.462\n",
            "Validation Loss: 2.1156119442289802, Validation Accuracy: 49.17\n",
            "[106/150]: Training Loss: 1.3525332610320557, Training Accuracy: 61.288\n",
            "Validation Loss: 2.0910707043994003, Validation Accuracy: 48.55\n",
            "[107/150]: Training Loss: 1.331598934538834, Training Accuracy: 61.91\n",
            "Validation Loss: 2.1001909211942347, Validation Accuracy: 49.0\n",
            "[108/150]: Training Loss: 1.3267096242179042, Training Accuracy: 61.816\n",
            "Validation Loss: 2.1083290622492505, Validation Accuracy: 48.74\n",
            "[109/150]: Training Loss: 1.3154139440230397, Training Accuracy: 62.34\n",
            "Validation Loss: 2.1171723535865734, Validation Accuracy: 49.04\n",
            "[110/150]: Training Loss: 1.3305041518662593, Training Accuracy: 61.83\n",
            "Validation Loss: 2.0921388515241586, Validation Accuracy: 49.21\n",
            "[111/150]: Training Loss: 1.3154786750483696, Training Accuracy: 61.994\n",
            "Validation Loss: 2.1219316781706112, Validation Accuracy: 48.77\n",
            "[112/150]: Training Loss: 1.3147668346876988, Training Accuracy: 61.946\n",
            "Validation Loss: 2.0896254671607046, Validation Accuracy: 48.89\n",
            "[113/150]: Training Loss: 1.3001932930915863, Training Accuracy: 62.724\n",
            "Validation Loss: 2.103232970662937, Validation Accuracy: 49.02\n",
            "[114/150]: Training Loss: 1.2976091802120209, Training Accuracy: 62.64\n",
            "Validation Loss: 2.089232644457726, Validation Accuracy: 48.88\n",
            "[115/150]: Training Loss: 1.2918279953015126, Training Accuracy: 62.894\n",
            "Validation Loss: 2.098377283971021, Validation Accuracy: 49.17\n",
            "[116/150]: Training Loss: 1.2901173764482483, Training Accuracy: 63.008\n",
            "Validation Loss: 2.096894028080497, Validation Accuracy: 49.39\n",
            "[117/150]: Training Loss: 1.275385139619603, Training Accuracy: 63.266\n",
            "Validation Loss: 2.1110763952230953, Validation Accuracy: 49.35\n",
            "[118/150]: Training Loss: 1.2755424442803462, Training Accuracy: 63.438\n",
            "Validation Loss: 2.1062107443050215, Validation Accuracy: 49.34\n",
            "[119/150]: Training Loss: 1.2707537008673333, Training Accuracy: 63.144\n",
            "Validation Loss: 2.094398064977804, Validation Accuracy: 49.44\n",
            "[120/150]: Training Loss: 1.2619870495613275, Training Accuracy: 63.662\n",
            "Validation Loss: 2.109912445590754, Validation Accuracy: 49.31\n",
            "[121/150]: Training Loss: 1.2651302716921053, Training Accuracy: 63.564\n",
            "Validation Loss: 2.1148870439286442, Validation Accuracy: 48.87\n",
            "[122/150]: Training Loss: 1.2517427335614744, Training Accuracy: 63.992\n",
            "Validation Loss: 2.111772822726304, Validation Accuracy: 49.14\n",
            "[123/150]: Training Loss: 1.2525572213522917, Training Accuracy: 63.8\n",
            "Validation Loss: 2.097744130784539, Validation Accuracy: 49.4\n",
            "[124/150]: Training Loss: 1.2481763019891041, Training Accuracy: 63.982\n",
            "Validation Loss: 2.110331610509544, Validation Accuracy: 49.5\n",
            "[125/150]: Training Loss: 1.242631159322646, Training Accuracy: 64.258\n",
            "Validation Loss: 2.113812481521801, Validation Accuracy: 49.25\n",
            "[126/150]: Training Loss: 1.2439294564144692, Training Accuracy: 64.134\n",
            "Validation Loss: 2.106003999710083, Validation Accuracy: 49.44\n",
            "[127/150]: Training Loss: 1.2376054860746768, Training Accuracy: 64.1\n",
            "Validation Loss: 2.1138451304405357, Validation Accuracy: 49.45\n",
            "[128/150]: Training Loss: 1.2359750416425184, Training Accuracy: 64.344\n",
            "Validation Loss: 2.0956578482488157, Validation Accuracy: 49.5\n",
            "[129/150]: Training Loss: 1.2426931114910205, Training Accuracy: 64.084\n",
            "Validation Loss: 2.102140469915548, Validation Accuracy: 49.39\n",
            "[130/150]: Training Loss: 1.2271092739099128, Training Accuracy: 64.588\n",
            "Validation Loss: 2.1065317156967844, Validation Accuracy: 49.55\n",
            "[131/150]: Training Loss: 1.2293265251552357, Training Accuracy: 64.606\n",
            "Validation Loss: 2.1087451816364458, Validation Accuracy: 49.71\n",
            "[132/150]: Training Loss: 1.2165279118606196, Training Accuracy: 64.626\n",
            "Validation Loss: 2.1100895678161815, Validation Accuracy: 49.58\n",
            "[133/150]: Training Loss: 1.2260109276112998, Training Accuracy: 64.716\n",
            "Validation Loss: 2.107479982315355, Validation Accuracy: 49.48\n",
            "[134/150]: Training Loss: 1.2170597407823938, Training Accuracy: 64.91\n",
            "Validation Loss: 2.109668661075033, Validation Accuracy: 49.21\n",
            "[135/150]: Training Loss: 1.2119625063655932, Training Accuracy: 64.864\n",
            "Validation Loss: 2.1121888259413897, Validation Accuracy: 49.6\n",
            "[136/150]: Training Loss: 1.2143927402508534, Training Accuracy: 64.752\n",
            "Validation Loss: 2.1146486792594765, Validation Accuracy: 49.42\n",
            "[137/150]: Training Loss: 1.20208662275768, Training Accuracy: 65.196\n",
            "Validation Loss: 2.1086552408850117, Validation Accuracy: 49.53\n",
            "[138/150]: Training Loss: 1.2066055967679719, Training Accuracy: 65.132\n",
            "Validation Loss: 2.106570492884156, Validation Accuracy: 49.36\n",
            "[139/150]: Training Loss: 1.2045007688767464, Training Accuracy: 65.206\n",
            "Validation Loss: 2.1134595893750525, Validation Accuracy: 49.61\n",
            "[140/150]: Training Loss: 1.209533206413469, Training Accuracy: 65.066\n",
            "Validation Loss: 2.111029122285782, Validation Accuracy: 49.59\n",
            "[141/150]: Training Loss: 1.2018247985321542, Training Accuracy: 64.974\n",
            "Validation Loss: 2.112992768834351, Validation Accuracy: 49.59\n",
            "[142/150]: Training Loss: 1.2081030414384955, Training Accuracy: 65.38\n",
            "Validation Loss: 2.1114872906618056, Validation Accuracy: 49.54\n",
            "[143/150]: Training Loss: 1.1987551257128606, Training Accuracy: 65.484\n",
            "Validation Loss: 2.11025989283422, Validation Accuracy: 49.53\n",
            "[144/150]: Training Loss: 1.204268764961711, Training Accuracy: 65.084\n",
            "Validation Loss: 2.1116166456489807, Validation Accuracy: 49.52\n",
            "[145/150]: Training Loss: 1.2022794322741917, Training Accuracy: 65.088\n",
            "Validation Loss: 2.1134905511406576, Validation Accuracy: 49.47\n",
            "[146/150]: Training Loss: 1.1945363251144623, Training Accuracy: 65.452\n",
            "Validation Loss: 2.1135922700736174, Validation Accuracy: 49.52\n",
            "[147/150]: Training Loss: 1.1900141044803287, Training Accuracy: 65.494\n",
            "Validation Loss: 2.113608984430884, Validation Accuracy: 49.49\n",
            "[148/150]: Training Loss: 1.2009751156467916, Training Accuracy: 65.298\n",
            "Validation Loss: 2.1133065656491907, Validation Accuracy: 49.47\n",
            "[149/150]: Training Loss: 1.2031257037464005, Training Accuracy: 65.434\n",
            "Validation Loss: 2.113412342253764, Validation Accuracy: 49.47\n",
            "[150/150]: Training Loss: 1.1963014528726983, Training Accuracy: 65.458\n",
            "Validation Loss: 2.113395432757724, Validation Accuracy: 49.47\n",
            "**********************************************************************\n",
            "Test Loss: 2.113395432757724, Test Accuracy: 49.47\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▅▃▂▁▂▁▁▂▁▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>Test Loss</td><td>▁▃▆▃█▆▅▇▅▅▃▃▃▄▄▃▃▃▄▅▆▅▅▄▄▅▅▅▄▄▄▄▄▄▄▄▄▄▃▃</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>Train Loss</td><td>█▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>49.47</td></tr><tr><td>Test Loss</td><td>2.1134</td></tr><tr><td>Train Accuracy</td><td>65.458</td></tr><tr><td>Train Loss</td><td>1.1963</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_003350-ulxra0pg/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 1e-03\n",
        "wd = 4e-04\n",
        "\n",
        "hyperparameters = {'learning_rate': lr,\n",
        "                    'weight_decay' : wd\n",
        "                  }\n",
        "\n",
        "# Load the model\n",
        "model_1 = LeNet5().to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer_1 = torch.optim.AdamW(model_1.parameters(), lr=lr, weight_decay=wd)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_1, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# Training\n",
        "run_training(num_epochs, model_1, original_train_loader, original_test_loader, original_test_loader, optimizer_1, scheduler, criterion, device, optimizer_name='AdamW', hyperparameters=hyperparameters, is_wandb = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeMWj_f0sbQE"
      },
      "source": [
        "# **Large Batch Optimizers**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LARS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WkZmVFG0q90m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class LARS(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements LARS (Layer-wise Adaptive Rate Scaling).\n",
        "\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "        lr (float): learning rate (default: 1e-3)\n",
        "        momentum (float, optional): momentum factor (default: 0)\n",
        "        trust_coef (float, optional): LARS coefficient as used in the paper (default: 1e-3)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        dampening (float, optional): dampening for momentum (default: 0)\n",
        "        nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
        "        epsilon (float, optional): epsilon to prevent zero division (default: 0)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            params,\n",
        "            lr: float = 1e-3,\n",
        "            momentum: float = 0,\n",
        "            trust_coef: float = 1e-3,\n",
        "            dampening: float = 0,\n",
        "            weight_decay: float = 0,\n",
        "            nesterov=False,\n",
        "            epsilon: float = 1e-9\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a new instance of the LARS optimizer.\n",
        "\n",
        "        Args:\n",
        "            params: iterable of parameters to optimize or dicts defining\n",
        "            lr: learning rate\n",
        "            momentum: momentum factor\n",
        "            trust_coef: LARS coefficient as used in the paper\n",
        "            weight_decay: weight decay (L2 penalty)\n",
        "            dampening: dampening for momentum\n",
        "            nesterov: enables Nesterov momentum\n",
        "            epsilon: epsilon to prevent zero division\n",
        "        \"\"\"\n",
        "\n",
        "        if lr <= 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            trust_coef=trust_coef,\n",
        "            dampening=dampening,\n",
        "            weight_decay=weight_decay,\n",
        "            nesterov=nesterov,\n",
        "            epsilon=epsilon)\n",
        "\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(LARS, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        \"\"\"\n",
        "        Sets the state of the optimizer.\n",
        "\n",
        "        Args:\n",
        "            state: The state to set the optimizer to.\n",
        "        \"\"\"\n",
        "        super(LARS, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def _compute_local_lr(self, p, weight_decay, trust_coef, epsilon):\n",
        "        \"\"\"\n",
        "        Computes the local learning rate for a given parameter.\n",
        "\n",
        "        Args:\n",
        "            p: The parameter to compute the local learning rate for.\n",
        "            weight_decay: The weight decay factor.\n",
        "            trust_coef: The trust coefficient.\n",
        "            epsilon: A small constant for numerical stability.\n",
        "\n",
        "        Returns:\n",
        "            float: The computed local learning rate.\n",
        "        \"\"\"\n",
        "        w_norm = torch.norm(p.data)\n",
        "        g_norm = torch.norm(p.grad.data)\n",
        "        if w_norm * g_norm > 0:\n",
        "            return trust_coef * w_norm / (g_norm + weight_decay * w_norm + epsilon)\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    def _update_params(self, p, d_p, local_lr, lr, momentum, buf,\n",
        "                       dampening, nesterov, weight_decay):\n",
        "        \"\"\"\n",
        "        Updates the parameters with the computed update.\n",
        "\n",
        "        Args:\n",
        "            p: The parameter to be updated.\n",
        "            d_p: The computed update for the parameter.\n",
        "            local_lr: The local learning rate.\n",
        "            lr: The global learning rate.\n",
        "            momentum: The momentum factor.\n",
        "            buf: The buffer for the momentum.\n",
        "            dampening: The dampening for the momentum.\n",
        "            nesterov: A flag indicating whether to use Nesterov momentum.\n",
        "            weight_decay: The weight decay factor.\n",
        "        \"\"\"\n",
        "        if weight_decay != 0:\n",
        "            d_p.add_(weight_decay, p.data)\n",
        "        if momentum != 0:\n",
        "            param_state = self.state[p]\n",
        "            if 'momentum_buffer' not in param_state:\n",
        "                buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "            else:\n",
        "                buf = param_state['momentum_buffer']\n",
        "            buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "            if nesterov:\n",
        "                d_p = d_p.add(momentum, buf)\n",
        "            else:\n",
        "                d_p = buf\n",
        "\n",
        "        p.data.add_(-local_lr * lr, d_p)\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        \"\"\"\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            trust_coef = group['trust_coef']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "            epsilon = group['epsilon']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                local_lr = self._compute_local_lr(p, weight_decay, trust_coef, epsilon)\n",
        "                self._update_params(p, p.grad.data, local_lr, group['lr'], momentum, None, dampening, nesterov, weight_decay)\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "dh421LdmM9XP"
      },
      "outputs": [],
      "source": [
        "learning_rates = [1e-02, 5e-02, 1e-01, 5e-01, 1, 1.5, 2]\n",
        "wd = 1e-03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LARS Hyperparameter Tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "30d70d57987c4349b55560e5d9626201",
            "36ac25551e574241bed4d7e5a4301d95",
            "6898349b9c754ca7be91bea29cabbba5",
            "623f947ef06c41569be7fcdda4141174",
            "d268f74266044451b230f20756bab2f6",
            "6896790e80d1450c821918c7ef41e42f",
            "a30700c8bfb449cda98a40d75828f3d3",
            "77beee57b67d4bad826f616e5483b87a",
            "c59ee0d8b9fb4694ada362115bf965a9",
            "ab62d64100a84b1faae744ee0f99501a",
            "6abbcbe71a314356ba04f8349d1fc127",
            "97c3921b8e454ccdb7b4db95d4440c64",
            "50b4ba3147b540abad020ae780353f27",
            "584e5a6aada04535b64c40c3ece566e8",
            "d3326a99729a416abca13d3122196e64",
            "1d534596e41340438fc8d083fff6aa8b",
            "19ed5f5d4f714246bbdf44513ecc50f0",
            "1f9d7ac8fa8a4bcdb76fe2b2cb443f66",
            "71c0cc303cfc44ba8d989d5f8feca119",
            "07dc9b3c563e4615bec4dbd3233bf4ba",
            "a3ccc0b32e9a4ceebd6045dff63e6621",
            "104402d4cba5477499593d972bc48e9d",
            "80fe0a51b14b4e548454d4f83215336c",
            "8e13a8bef6e14973912966984e83cd4c"
          ]
        },
        "id": "dzsCB_Q9NnCD",
        "outputId": "56d144bd-faa8-4df2-a19c-7cc15394622d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_022555-nh2g7isx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx' target=\"_blank\">learning_rate=0.01 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605613048553467, Training Accuracy: 1.0325\n",
            "Validation Loss: 4.605481047539195, Validation Accuracy: 1.01\n",
            "[2/150]: Training Loss: 4.603893094635009, Training Accuracy: 1.2825\n",
            "Validation Loss: 4.603514659176966, Validation Accuracy: 1.37\n",
            "[3/150]: Training Loss: 4.601452378845215, Training Accuracy: 1.7625\n",
            "Validation Loss: 4.600479241389378, Validation Accuracy: 1.96\n",
            "[4/150]: Training Loss: 4.597398109436035, Training Accuracy: 1.815\n",
            "Validation Loss: 4.595512341541849, Validation Accuracy: 1.41\n",
            "[5/150]: Training Loss: 4.590549831390381, Training Accuracy: 1.5175\n",
            "Validation Loss: 4.587287993947411, Validation Accuracy: 1.68\n",
            "[6/150]: Training Loss: 4.579249038696289, Training Accuracy: 1.93\n",
            "Validation Loss: 4.573939116897097, Validation Accuracy: 2.37\n",
            "[7/150]: Training Loss: 4.560530513763427, Training Accuracy: 2.69\n",
            "Validation Loss: 4.5522112117451465, Validation Accuracy: 2.64\n",
            "[8/150]: Training Loss: 4.5309873321533205, Training Accuracy: 2.92\n",
            "Validation Loss: 4.519049043108703, Validation Accuracy: 2.85\n",
            "[9/150]: Training Loss: 4.488997451782226, Training Accuracy: 2.9475\n",
            "Validation Loss: 4.475813519423175, Validation Accuracy: 3.12\n",
            "[10/150]: Training Loss: 4.439574425506592, Training Accuracy: 3.3375\n",
            "Validation Loss: 4.429785430811013, Validation Accuracy: 3.32\n",
            "[11/150]: Training Loss: 4.391272030639648, Training Accuracy: 3.61\n",
            "Validation Loss: 4.386288050633327, Validation Accuracy: 3.95\n",
            "[12/150]: Training Loss: 4.346567308807373, Training Accuracy: 4.215\n",
            "Validation Loss: 4.348948326839763, Validation Accuracy: 4.02\n",
            "[13/150]: Training Loss: 4.3075446601867675, Training Accuracy: 4.48\n",
            "Validation Loss: 4.312188755934406, Validation Accuracy: 4.38\n",
            "[14/150]: Training Loss: 4.274280471801758, Training Accuracy: 4.9575\n",
            "Validation Loss: 4.284171402074729, Validation Accuracy: 4.91\n",
            "[15/150]: Training Loss: 4.24782478981018, Training Accuracy: 5.3275\n",
            "Validation Loss: 4.261963683328811, Validation Accuracy: 5.05\n",
            "[16/150]: Training Loss: 4.226929823303223, Training Accuracy: 5.7425\n",
            "Validation Loss: 4.2434815540435205, Validation Accuracy: 5.46\n",
            "[17/150]: Training Loss: 4.210706592941285, Training Accuracy: 5.9375\n",
            "Validation Loss: 4.230875344792748, Validation Accuracy: 5.78\n",
            "[18/150]: Training Loss: 4.197167670059204, Training Accuracy: 6.23\n",
            "Validation Loss: 4.218225898256727, Validation Accuracy: 5.69\n",
            "[19/150]: Training Loss: 4.1860190193176265, Training Accuracy: 6.505\n",
            "Validation Loss: 4.208940963076937, Validation Accuracy: 5.86\n",
            "[20/150]: Training Loss: 4.175828193664551, Training Accuracy: 6.6025\n",
            "Validation Loss: 4.201019479970264, Validation Accuracy: 6.0\n",
            "[21/150]: Training Loss: 4.167100285339355, Training Accuracy: 6.8025\n",
            "Validation Loss: 4.1936384735593375, Validation Accuracy: 5.97\n",
            "[22/150]: Training Loss: 4.158846283340454, Training Accuracy: 6.92\n",
            "Validation Loss: 4.185141735016161, Validation Accuracy: 6.28\n",
            "[23/150]: Training Loss: 4.151738377380371, Training Accuracy: 7.0375\n",
            "Validation Loss: 4.178911400448745, Validation Accuracy: 6.83\n",
            "[24/150]: Training Loss: 4.144738430023193, Training Accuracy: 7.23\n",
            "Validation Loss: 4.172337419667821, Validation Accuracy: 6.52\n",
            "[25/150]: Training Loss: 4.1379581127166745, Training Accuracy: 7.265\n",
            "Validation Loss: 4.165943450988478, Validation Accuracy: 6.63\n",
            "[26/150]: Training Loss: 4.132068264770508, Training Accuracy: 7.4525\n",
            "Validation Loss: 4.160335552920202, Validation Accuracy: 7.0\n",
            "[27/150]: Training Loss: 4.126062253570557, Training Accuracy: 7.4725\n",
            "Validation Loss: 4.157630066962758, Validation Accuracy: 6.66\n",
            "[28/150]: Training Loss: 4.120529958724975, Training Accuracy: 7.52\n",
            "Validation Loss: 4.150649539983956, Validation Accuracy: 7.07\n",
            "[29/150]: Training Loss: 4.115550531768799, Training Accuracy: 7.6175\n",
            "Validation Loss: 4.145238416210102, Validation Accuracy: 7.01\n",
            "[30/150]: Training Loss: 4.109972083282471, Training Accuracy: 7.74\n",
            "Validation Loss: 4.140822541182208, Validation Accuracy: 6.87\n",
            "[31/150]: Training Loss: 4.1054605201721195, Training Accuracy: 7.7775\n",
            "Validation Loss: 4.135137576206475, Validation Accuracy: 7.26\n",
            "[32/150]: Training Loss: 4.100883611297608, Training Accuracy: 7.9175\n",
            "Validation Loss: 4.13245949623691, Validation Accuracy: 7.37\n",
            "[33/150]: Training Loss: 4.096106577682495, Training Accuracy: 7.965\n",
            "Validation Loss: 4.1272796597450405, Validation Accuracy: 7.21\n",
            "[34/150]: Training Loss: 4.092031303024292, Training Accuracy: 8.075\n",
            "Validation Loss: 4.1231977316983945, Validation Accuracy: 7.19\n",
            "[35/150]: Training Loss: 4.087661064910889, Training Accuracy: 8.0775\n",
            "Validation Loss: 4.119425037104612, Validation Accuracy: 7.42\n",
            "[36/150]: Training Loss: 4.083959820175171, Training Accuracy: 8.14\n",
            "Validation Loss: 4.117144297642313, Validation Accuracy: 7.43\n",
            "[37/150]: Training Loss: 4.079987169265747, Training Accuracy: 8.2575\n",
            "Validation Loss: 4.112940530108798, Validation Accuracy: 7.33\n",
            "[38/150]: Training Loss: 4.076256622695923, Training Accuracy: 8.275\n",
            "Validation Loss: 4.10887487071335, Validation Accuracy: 7.72\n",
            "[39/150]: Training Loss: 4.072722610473633, Training Accuracy: 8.2625\n",
            "Validation Loss: 4.1064621779569395, Validation Accuracy: 7.62\n",
            "[40/150]: Training Loss: 4.06919368019104, Training Accuracy: 8.4025\n",
            "Validation Loss: 4.102874081605559, Validation Accuracy: 7.77\n",
            "[41/150]: Training Loss: 4.065893580627441, Training Accuracy: 8.425\n",
            "Validation Loss: 4.101947505003328, Validation Accuracy: 7.99\n",
            "[42/150]: Training Loss: 4.062419548797608, Training Accuracy: 8.64\n",
            "Validation Loss: 4.096677272942416, Validation Accuracy: 7.69\n",
            "[43/150]: Training Loss: 4.059447618484497, Training Accuracy: 8.6025\n",
            "Validation Loss: 4.096899234565201, Validation Accuracy: 7.72\n",
            "[44/150]: Training Loss: 4.056013444137573, Training Accuracy: 8.5725\n",
            "Validation Loss: 4.091373089772121, Validation Accuracy: 7.88\n",
            "[45/150]: Training Loss: 4.053135622024536, Training Accuracy: 8.6275\n",
            "Validation Loss: 4.088051226488345, Validation Accuracy: 8.07\n",
            "[46/150]: Training Loss: 4.049987061309815, Training Accuracy: 8.63\n",
            "Validation Loss: 4.085329578180981, Validation Accuracy: 8.03\n",
            "[47/150]: Training Loss: 4.047172613143921, Training Accuracy: 8.7125\n",
            "Validation Loss: 4.082841596785625, Validation Accuracy: 7.78\n",
            "[48/150]: Training Loss: 4.0441587448120115, Training Accuracy: 8.745\n",
            "Validation Loss: 4.081506820241357, Validation Accuracy: 8.21\n",
            "[49/150]: Training Loss: 4.04135802230835, Training Accuracy: 8.82\n",
            "Validation Loss: 4.079341964357218, Validation Accuracy: 8.03\n",
            "[50/150]: Training Loss: 4.0387205871582035, Training Accuracy: 8.7775\n",
            "Validation Loss: 4.075337004509701, Validation Accuracy: 8.12\n",
            "[51/150]: Training Loss: 4.036031353759766, Training Accuracy: 8.9025\n",
            "Validation Loss: 4.074757421092623, Validation Accuracy: 8.17\n",
            "[52/150]: Training Loss: 4.033370276260376, Training Accuracy: 8.91\n",
            "Validation Loss: 4.071030196110914, Validation Accuracy: 8.29\n",
            "[53/150]: Training Loss: 4.031033205032348, Training Accuracy: 8.965\n",
            "Validation Loss: 4.069437462812776, Validation Accuracy: 8.36\n",
            "[54/150]: Training Loss: 4.028417000579834, Training Accuracy: 9.095\n",
            "Validation Loss: 4.067116659917649, Validation Accuracy: 8.11\n",
            "[55/150]: Training Loss: 4.0260539081573485, Training Accuracy: 9.045\n",
            "Validation Loss: 4.065322780305413, Validation Accuracy: 8.45\n",
            "[56/150]: Training Loss: 4.0235095344543454, Training Accuracy: 9.085\n",
            "Validation Loss: 4.064038521165301, Validation Accuracy: 8.38\n",
            "[57/150]: Training Loss: 4.02125683631897, Training Accuracy: 9.18\n",
            "Validation Loss: 4.0599597381178745, Validation Accuracy: 8.62\n",
            "[58/150]: Training Loss: 4.019049766159058, Training Accuracy: 9.2225\n",
            "Validation Loss: 4.058757024206174, Validation Accuracy: 8.53\n",
            "[59/150]: Training Loss: 4.016744113540649, Training Accuracy: 9.3325\n",
            "Validation Loss: 4.056453751910264, Validation Accuracy: 8.65\n",
            "[60/150]: Training Loss: 4.014383445358276, Training Accuracy: 9.33\n",
            "Validation Loss: 4.054738797959248, Validation Accuracy: 8.49\n",
            "[61/150]: Training Loss: 4.012673494720459, Training Accuracy: 9.4\n",
            "Validation Loss: 4.05312654015365, Validation Accuracy: 8.45\n",
            "[62/150]: Training Loss: 4.0104282833099365, Training Accuracy: 9.3375\n",
            "Validation Loss: 4.050798089640915, Validation Accuracy: 8.61\n",
            "[63/150]: Training Loss: 4.0085177280426025, Training Accuracy: 9.345\n",
            "Validation Loss: 4.047672698452215, Validation Accuracy: 9.08\n",
            "[64/150]: Training Loss: 4.007076532363891, Training Accuracy: 9.44\n",
            "Validation Loss: 4.047075613289122, Validation Accuracy: 8.55\n",
            "[65/150]: Training Loss: 4.004718017959595, Training Accuracy: 9.47\n",
            "Validation Loss: 4.044435259642874, Validation Accuracy: 8.66\n",
            "[66/150]: Training Loss: 4.002854734802246, Training Accuracy: 9.58\n",
            "Validation Loss: 4.043384468479521, Validation Accuracy: 8.68\n",
            "[67/150]: Training Loss: 4.000918030929565, Training Accuracy: 9.59\n",
            "Validation Loss: 4.042230905241268, Validation Accuracy: 8.87\n",
            "[68/150]: Training Loss: 3.9992192554473878, Training Accuracy: 9.5775\n",
            "Validation Loss: 4.040329585409468, Validation Accuracy: 8.83\n",
            "[69/150]: Training Loss: 3.9973626461029053, Training Accuracy: 9.615\n",
            "Validation Loss: 4.037929029221747, Validation Accuracy: 9.01\n",
            "[70/150]: Training Loss: 3.9957307056427003, Training Accuracy: 9.69\n",
            "Validation Loss: 4.037050957892351, Validation Accuracy: 8.67\n",
            "[71/150]: Training Loss: 3.9943285522460936, Training Accuracy: 9.7375\n",
            "Validation Loss: 4.0364253475407885, Validation Accuracy: 8.88\n",
            "[72/150]: Training Loss: 3.9925530368804933, Training Accuracy: 9.7025\n",
            "Validation Loss: 4.033815119676529, Validation Accuracy: 8.93\n",
            "[73/150]: Training Loss: 3.9908373458862303, Training Accuracy: 9.7625\n",
            "Validation Loss: 4.0321422671056855, Validation Accuracy: 8.97\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 4.837270994854581, Test Accuracy: 5.7\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▇▂▂▂▂▂▁▁▁▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▁▁▇██▇▇█▇▇▆▇▇▇▇▆▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▃▃▄▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>████▇▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>5.7</td></tr><tr><td>Test Loss</td><td>4.83727</td></tr><tr><td>Train Accuracy</td><td>9.7625</td></tr><tr><td>Train Loss</td><td>3.99084</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_022555-nh2g7isx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.05 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_023355-cthymqs8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8' target=\"_blank\">learning_rate=0.05 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.600790646362305, Training Accuracy: 1.1675\n",
            "Validation Loss: 4.590667490746565, Validation Accuracy: 1.26\n",
            "[2/150]: Training Loss: 4.559383809661865, Training Accuracy: 2.1575\n",
            "Validation Loss: 4.506907958133965, Validation Accuracy: 2.54\n",
            "[3/150]: Training Loss: 4.4036731437683105, Training Accuracy: 3.8525\n",
            "Validation Loss: 4.321598778864381, Validation Accuracy: 4.84\n",
            "[4/150]: Training Loss: 4.246265041351318, Training Accuracy: 5.2175\n",
            "Validation Loss: 4.227905522486207, Validation Accuracy: 6.02\n",
            "[5/150]: Training Loss: 4.176178216934204, Training Accuracy: 6.1675\n",
            "Validation Loss: 4.179626170237353, Validation Accuracy: 6.38\n",
            "[6/150]: Training Loss: 4.135982835769654, Training Accuracy: 6.6675\n",
            "Validation Loss: 4.148335417364813, Validation Accuracy: 6.64\n",
            "[7/150]: Training Loss: 4.108450821304321, Training Accuracy: 7.2275\n",
            "Validation Loss: 4.130975275282648, Validation Accuracy: 6.89\n",
            "[8/150]: Training Loss: 4.0844139430999755, Training Accuracy: 7.6\n",
            "Validation Loss: 4.101699337078507, Validation Accuracy: 7.5\n",
            "[9/150]: Training Loss: 4.065156806564331, Training Accuracy: 7.93\n",
            "Validation Loss: 4.086441223788413, Validation Accuracy: 7.13\n",
            "[10/150]: Training Loss: 4.045691847991943, Training Accuracy: 8.4825\n",
            "Validation Loss: 4.07577043733779, Validation Accuracy: 8.03\n",
            "[11/150]: Training Loss: 4.029001393127442, Training Accuracy: 8.6075\n",
            "Validation Loss: 4.055504823186595, Validation Accuracy: 8.33\n",
            "[12/150]: Training Loss: 4.011618738937378, Training Accuracy: 9.0575\n",
            "Validation Loss: 4.039901239856793, Validation Accuracy: 8.48\n",
            "[13/150]: Training Loss: 3.9964084663391115, Training Accuracy: 9.2925\n",
            "Validation Loss: 4.02936831097694, Validation Accuracy: 8.91\n",
            "[14/150]: Training Loss: 3.9814702980041505, Training Accuracy: 9.4925\n",
            "Validation Loss: 4.011358877655807, Validation Accuracy: 9.08\n",
            "[15/150]: Training Loss: 3.967844100570679, Training Accuracy: 9.735\n",
            "Validation Loss: 4.00843292133064, Validation Accuracy: 9.01\n",
            "[16/150]: Training Loss: 3.9526515048980713, Training Accuracy: 10.11\n",
            "Validation Loss: 3.983490101091421, Validation Accuracy: 9.73\n",
            "[17/150]: Training Loss: 3.9376510147094725, Training Accuracy: 10.2675\n",
            "Validation Loss: 3.9689508137429597, Validation Accuracy: 9.61\n",
            "[18/150]: Training Loss: 3.9245626121520996, Training Accuracy: 10.52\n",
            "Validation Loss: 3.9620496361118973, Validation Accuracy: 9.98\n",
            "[19/150]: Training Loss: 3.910297552108765, Training Accuracy: 10.8175\n",
            "Validation Loss: 3.952874835129756, Validation Accuracy: 10.09\n",
            "[20/150]: Training Loss: 3.8983414665222167, Training Accuracy: 11.1225\n",
            "Validation Loss: 3.930051308528633, Validation Accuracy: 10.44\n",
            "[21/150]: Training Loss: 3.8838251056671145, Training Accuracy: 11.2975\n",
            "Validation Loss: 3.9188870700301637, Validation Accuracy: 10.53\n",
            "[22/150]: Training Loss: 3.870637998199463, Training Accuracy: 11.44\n",
            "Validation Loss: 3.903455512538837, Validation Accuracy: 10.81\n",
            "[23/150]: Training Loss: 3.857129457092285, Training Accuracy: 11.84\n",
            "Validation Loss: 3.8918681099156665, Validation Accuracy: 11.32\n",
            "[24/150]: Training Loss: 3.8432881324768067, Training Accuracy: 12.0075\n",
            "Validation Loss: 3.8916967568124177, Validation Accuracy: 11.36\n",
            "[25/150]: Training Loss: 3.829104020690918, Training Accuracy: 12.395\n",
            "Validation Loss: 3.874841636912838, Validation Accuracy: 11.71\n",
            "[26/150]: Training Loss: 3.814932382965088, Training Accuracy: 12.6375\n",
            "Validation Loss: 3.862990617752075, Validation Accuracy: 11.7\n",
            "[27/150]: Training Loss: 3.801380111312866, Training Accuracy: 12.8\n",
            "Validation Loss: 3.850984105638638, Validation Accuracy: 12.17\n",
            "[28/150]: Training Loss: 3.7865677406311034, Training Accuracy: 13.18\n",
            "Validation Loss: 3.830880017796899, Validation Accuracy: 12.18\n",
            "[29/150]: Training Loss: 3.7712997707366944, Training Accuracy: 13.4425\n",
            "Validation Loss: 3.816112823547072, Validation Accuracy: 12.61\n",
            "[30/150]: Training Loss: 3.7579914638519285, Training Accuracy: 13.83\n",
            "Validation Loss: 3.817626942494872, Validation Accuracy: 12.9\n",
            "[31/150]: Training Loss: 3.7429426765441893, Training Accuracy: 14.14\n",
            "Validation Loss: 3.795013807381794, Validation Accuracy: 12.82\n",
            "[32/150]: Training Loss: 3.7270426654815676, Training Accuracy: 14.46\n",
            "Validation Loss: 3.774455428882769, Validation Accuracy: 13.53\n",
            "[33/150]: Training Loss: 3.7124608081817625, Training Accuracy: 14.53\n",
            "Validation Loss: 3.7734299799439253, Validation Accuracy: 13.65\n",
            "[34/150]: Training Loss: 3.695247130584717, Training Accuracy: 15.0575\n",
            "Validation Loss: 3.749705171888801, Validation Accuracy: 14.01\n",
            "[35/150]: Training Loss: 3.679954329299927, Training Accuracy: 15.12\n",
            "Validation Loss: 3.7417220279669308, Validation Accuracy: 13.98\n",
            "[36/150]: Training Loss: 3.6635797924041746, Training Accuracy: 15.48\n",
            "Validation Loss: 3.7247597214522634, Validation Accuracy: 14.42\n",
            "[37/150]: Training Loss: 3.6480526805877687, Training Accuracy: 15.7375\n",
            "Validation Loss: 3.7059438805671254, Validation Accuracy: 14.5\n",
            "[38/150]: Training Loss: 3.6313486305236817, Training Accuracy: 16.3075\n",
            "Validation Loss: 3.6903846977622647, Validation Accuracy: 14.92\n",
            "[39/150]: Training Loss: 3.6153011852264405, Training Accuracy: 16.3475\n",
            "Validation Loss: 3.6778608826315327, Validation Accuracy: 14.79\n",
            "[40/150]: Training Loss: 3.60027672958374, Training Accuracy: 16.67\n",
            "Validation Loss: 3.6614270756958396, Validation Accuracy: 15.31\n",
            "[41/150]: Training Loss: 3.585800159072876, Training Accuracy: 16.7725\n",
            "Validation Loss: 3.655038889805982, Validation Accuracy: 15.05\n",
            "[42/150]: Training Loss: 3.5694461936950685, Training Accuracy: 17.0375\n",
            "Validation Loss: 3.636945672855256, Validation Accuracy: 15.93\n",
            "[43/150]: Training Loss: 3.5568551288604735, Training Accuracy: 17.3975\n",
            "Validation Loss: 3.627324414101376, Validation Accuracy: 16.21\n",
            "[44/150]: Training Loss: 3.5419315227508545, Training Accuracy: 17.7\n",
            "Validation Loss: 3.6199980754001886, Validation Accuracy: 16.01\n",
            "[45/150]: Training Loss: 3.529905016708374, Training Accuracy: 17.7675\n",
            "Validation Loss: 3.6078376861134913, Validation Accuracy: 16.36\n",
            "[46/150]: Training Loss: 3.5179548221588135, Training Accuracy: 18.195\n",
            "Validation Loss: 3.591066009679418, Validation Accuracy: 16.38\n",
            "[47/150]: Training Loss: 3.5055387844085693, Training Accuracy: 18.2875\n",
            "Validation Loss: 3.5880215836178726, Validation Accuracy: 16.91\n",
            "[48/150]: Training Loss: 3.4944853992462157, Training Accuracy: 18.445\n",
            "Validation Loss: 3.573359035382605, Validation Accuracy: 17.28\n",
            "[49/150]: Training Loss: 3.4836856788635253, Training Accuracy: 18.615\n",
            "Validation Loss: 3.5583009431316595, Validation Accuracy: 17.54\n",
            "[50/150]: Training Loss: 3.4725227031707764, Training Accuracy: 18.9225\n",
            "Validation Loss: 3.5552009609854145, Validation Accuracy: 17.27\n",
            "[51/150]: Training Loss: 3.461646089553833, Training Accuracy: 19.095\n",
            "Validation Loss: 3.5449378551191586, Validation Accuracy: 17.49\n",
            "[52/150]: Training Loss: 3.451679636383057, Training Accuracy: 19.2575\n",
            "Validation Loss: 3.5295108943987805, Validation Accuracy: 17.59\n",
            "[53/150]: Training Loss: 3.44256662979126, Training Accuracy: 19.475\n",
            "Validation Loss: 3.5264732868048796, Validation Accuracy: 17.73\n",
            "[54/150]: Training Loss: 3.4335047622680666, Training Accuracy: 19.5175\n",
            "Validation Loss: 3.521607499213735, Validation Accuracy: 17.96\n",
            "[55/150]: Training Loss: 3.425080271530151, Training Accuracy: 19.8125\n",
            "Validation Loss: 3.513597962203299, Validation Accuracy: 18.01\n",
            "[56/150]: Training Loss: 3.4155618144989015, Training Accuracy: 19.8425\n",
            "Validation Loss: 3.5052308413633115, Validation Accuracy: 18.2\n",
            "[57/150]: Training Loss: 3.4078545150756834, Training Accuracy: 20.0575\n",
            "Validation Loss: 3.498734108202017, Validation Accuracy: 18.36\n",
            "[58/150]: Training Loss: 3.399192015457153, Training Accuracy: 20.1425\n",
            "Validation Loss: 3.4935538814326, Validation Accuracy: 18.5\n",
            "[59/150]: Training Loss: 3.3918968856811524, Training Accuracy: 20.275\n",
            "Validation Loss: 3.4842984858591843, Validation Accuracy: 18.83\n",
            "[60/150]: Training Loss: 3.3846318199157714, Training Accuracy: 20.45\n",
            "Validation Loss: 3.475666542721402, Validation Accuracy: 18.75\n",
            "[61/150]: Training Loss: 3.378842526626587, Training Accuracy: 20.395\n",
            "Validation Loss: 3.47740289360095, Validation Accuracy: 18.53\n",
            "[62/150]: Training Loss: 3.369043716430664, Training Accuracy: 20.85\n",
            "Validation Loss: 3.469373791081131, Validation Accuracy: 18.88\n",
            "[63/150]: Training Loss: 3.363127135467529, Training Accuracy: 20.8125\n",
            "Validation Loss: 3.453160047531128, Validation Accuracy: 19.11\n",
            "[64/150]: Training Loss: 3.355681411361694, Training Accuracy: 21.0925\n",
            "Validation Loss: 3.466009464992839, Validation Accuracy: 19.12\n",
            "[65/150]: Training Loss: 3.3498121349334715, Training Accuracy: 21.1625\n",
            "Validation Loss: 3.4505164547331013, Validation Accuracy: 19.11\n",
            "[66/150]: Training Loss: 3.343057912063599, Training Accuracy: 21.225\n",
            "Validation Loss: 3.4455762957311737, Validation Accuracy: 19.05\n",
            "[67/150]: Training Loss: 3.3380538593292237, Training Accuracy: 21.2275\n",
            "Validation Loss: 3.448872314137258, Validation Accuracy: 18.99\n",
            "[68/150]: Training Loss: 3.3309446399688722, Training Accuracy: 21.4425\n",
            "Validation Loss: 3.434380317189891, Validation Accuracy: 19.65\n",
            "[69/150]: Training Loss: 3.325505153656006, Training Accuracy: 21.5125\n",
            "Validation Loss: 3.428423096419899, Validation Accuracy: 19.77\n",
            "[70/150]: Training Loss: 3.319794240951538, Training Accuracy: 21.7375\n",
            "Validation Loss: 3.4293241318623733, Validation Accuracy: 19.58\n",
            "[71/150]: Training Loss: 3.3128502590179445, Training Accuracy: 21.7775\n",
            "Validation Loss: 3.422350405128139, Validation Accuracy: 19.89\n",
            "[72/150]: Training Loss: 3.308202914047241, Training Accuracy: 21.9175\n",
            "Validation Loss: 3.4211007197191763, Validation Accuracy: 20.03\n",
            "[73/150]: Training Loss: 3.3036910511016844, Training Accuracy: 21.85\n",
            "Validation Loss: 3.412458062931231, Validation Accuracy: 19.99\n",
            "[74/150]: Training Loss: 3.2999632190704347, Training Accuracy: 22.05\n",
            "Validation Loss: 3.410503987294094, Validation Accuracy: 19.96\n",
            "[75/150]: Training Loss: 3.2934399642944334, Training Accuracy: 21.9825\n",
            "Validation Loss: 3.4070342604521735, Validation Accuracy: 20.1\n",
            "[76/150]: Training Loss: 3.288022666168213, Training Accuracy: 22.4075\n",
            "Validation Loss: 3.3981059220186465, Validation Accuracy: 20.33\n",
            "[77/150]: Training Loss: 3.283489876937866, Training Accuracy: 22.37\n",
            "Validation Loss: 3.403488172846995, Validation Accuracy: 20.14\n",
            "[78/150]: Training Loss: 3.279419002151489, Training Accuracy: 22.4725\n",
            "Validation Loss: 3.39810815434547, Validation Accuracy: 20.23\n",
            "[79/150]: Training Loss: 3.275748169708252, Training Accuracy: 22.3675\n",
            "Validation Loss: 3.3924497191313727, Validation Accuracy: 20.38\n",
            "[80/150]: Training Loss: 3.2700943855285645, Training Accuracy: 22.625\n",
            "Validation Loss: 3.394149239655513, Validation Accuracy: 20.3\n",
            "[81/150]: Training Loss: 3.266976011657715, Training Accuracy: 22.6325\n",
            "Validation Loss: 3.389360663237845, Validation Accuracy: 20.25\n",
            "[82/150]: Training Loss: 3.2631465213775637, Training Accuracy: 22.7725\n",
            "Validation Loss: 3.3781265529098023, Validation Accuracy: 20.61\n",
            "[83/150]: Training Loss: 3.2582495727539063, Training Accuracy: 22.885\n",
            "Validation Loss: 3.380337715148926, Validation Accuracy: 20.88\n",
            "[84/150]: Training Loss: 3.2537473976135254, Training Accuracy: 23.075\n",
            "Validation Loss: 3.3771434042863784, Validation Accuracy: 20.89\n",
            "[85/150]: Training Loss: 3.2507594604492187, Training Accuracy: 22.9875\n",
            "Validation Loss: 3.3780695936482426, Validation Accuracy: 20.92\n",
            "[86/150]: Training Loss: 3.246923331832886, Training Accuracy: 23.055\n",
            "Validation Loss: 3.3708357325025426, Validation Accuracy: 20.88\n",
            "[87/150]: Training Loss: 3.2429458431243896, Training Accuracy: 23.1625\n",
            "Validation Loss: 3.36568513493629, Validation Accuracy: 21.0\n",
            "[88/150]: Training Loss: 3.239496036148071, Training Accuracy: 23.09\n",
            "Validation Loss: 3.369001481183775, Validation Accuracy: 20.84\n",
            "[89/150]: Training Loss: 3.235803797531128, Training Accuracy: 23.2875\n",
            "Validation Loss: 3.362776779065466, Validation Accuracy: 21.19\n",
            "[90/150]: Training Loss: 3.2327476997375486, Training Accuracy: 23.39\n",
            "Validation Loss: 3.3616829556264696, Validation Accuracy: 21.16\n",
            "[91/150]: Training Loss: 3.229936269760132, Training Accuracy: 23.3875\n",
            "Validation Loss: 3.360888686149743, Validation Accuracy: 21.41\n",
            "[92/150]: Training Loss: 3.2263083686828615, Training Accuracy: 23.4225\n",
            "Validation Loss: 3.359251875786265, Validation Accuracy: 21.08\n",
            "[93/150]: Training Loss: 3.224231428909302, Training Accuracy: 23.5575\n",
            "Validation Loss: 3.3559617753241473, Validation Accuracy: 21.38\n",
            "[94/150]: Training Loss: 3.2209563293457033, Training Accuracy: 23.52\n",
            "Validation Loss: 3.3566071850479027, Validation Accuracy: 21.38\n",
            "[95/150]: Training Loss: 3.2181186485290527, Training Accuracy: 23.5925\n",
            "Validation Loss: 3.3525171613996956, Validation Accuracy: 21.11\n",
            "[96/150]: Training Loss: 3.2156093044281007, Training Accuracy: 23.6025\n",
            "Validation Loss: 3.3487717983828986, Validation Accuracy: 21.48\n",
            "[97/150]: Training Loss: 3.211993044281006, Training Accuracy: 23.67\n",
            "Validation Loss: 3.3509595333390934, Validation Accuracy: 21.34\n",
            "[98/150]: Training Loss: 3.2086189796447755, Training Accuracy: 23.7725\n",
            "Validation Loss: 3.35127718585312, Validation Accuracy: 21.49\n",
            "[99/150]: Training Loss: 3.2077421699523927, Training Accuracy: 23.7925\n",
            "Validation Loss: 3.345605060553095, Validation Accuracy: 21.5\n",
            "[100/150]: Training Loss: 3.2047137928009035, Training Accuracy: 23.7025\n",
            "Validation Loss: 3.3447778088271996, Validation Accuracy: 21.56\n",
            "[101/150]: Training Loss: 3.202946053314209, Training Accuracy: 23.8775\n",
            "Validation Loss: 3.342098644584607, Validation Accuracy: 21.49\n",
            "[102/150]: Training Loss: 3.2004322288513185, Training Accuracy: 23.8375\n",
            "Validation Loss: 3.343631648713616, Validation Accuracy: 21.61\n",
            "[103/150]: Training Loss: 3.1983728965759277, Training Accuracy: 23.9425\n",
            "Validation Loss: 3.3422155471364405, Validation Accuracy: 21.24\n",
            "[104/150]: Training Loss: 3.196205286026001, Training Accuracy: 23.9975\n",
            "Validation Loss: 3.3361381269564294, Validation Accuracy: 21.8\n",
            "[105/150]: Training Loss: 3.194296379852295, Training Accuracy: 23.9125\n",
            "Validation Loss: 3.3376316750884816, Validation Accuracy: 21.59\n",
            "[106/150]: Training Loss: 3.1917529289245605, Training Accuracy: 23.9425\n",
            "Validation Loss: 3.3328404289901634, Validation Accuracy: 21.83\n",
            "[107/150]: Training Loss: 3.1901755290985108, Training Accuracy: 24.06\n",
            "Validation Loss: 3.335774242498313, Validation Accuracy: 21.85\n",
            "[108/150]: Training Loss: 3.188256001663208, Training Accuracy: 24.01\n",
            "Validation Loss: 3.335664199416045, Validation Accuracy: 21.84\n",
            "[109/150]: Training Loss: 3.186541044998169, Training Accuracy: 24.1\n",
            "Validation Loss: 3.331627411447513, Validation Accuracy: 21.98\n",
            "[110/150]: Training Loss: 3.184773151397705, Training Accuracy: 24.0825\n",
            "Validation Loss: 3.3308697946512016, Validation Accuracy: 21.79\n",
            "[111/150]: Training Loss: 3.1835228912353517, Training Accuracy: 24.1275\n",
            "Validation Loss: 3.3312593797209917, Validation Accuracy: 21.68\n",
            "[112/150]: Training Loss: 3.181557007980347, Training Accuracy: 24.2275\n",
            "Validation Loss: 3.3289258191539983, Validation Accuracy: 21.87\n",
            "[113/150]: Training Loss: 3.180008267211914, Training Accuracy: 24.3675\n",
            "Validation Loss: 3.3277820234845397, Validation Accuracy: 21.73\n",
            "[114/150]: Training Loss: 3.1787550163269045, Training Accuracy: 24.2975\n",
            "Validation Loss: 3.326631183077575, Validation Accuracy: 21.95\n",
            "[115/150]: Training Loss: 3.177647034072876, Training Accuracy: 24.285\n",
            "Validation Loss: 3.3255794716488785, Validation Accuracy: 22.03\n",
            "[116/150]: Training Loss: 3.176307448196411, Training Accuracy: 24.385\n",
            "Validation Loss: 3.3254039242009448, Validation Accuracy: 21.9\n",
            "[117/150]: Training Loss: 3.175141114425659, Training Accuracy: 24.3075\n",
            "Validation Loss: 3.3223649164673628, Validation Accuracy: 22.03\n",
            "[118/150]: Training Loss: 3.173803305053711, Training Accuracy: 24.4125\n",
            "Validation Loss: 3.323437298938727, Validation Accuracy: 21.99\n",
            "[119/150]: Training Loss: 3.1727485507965087, Training Accuracy: 24.345\n",
            "Validation Loss: 3.3220133781433105, Validation Accuracy: 22.09\n",
            "[120/150]: Training Loss: 3.171308903121948, Training Accuracy: 24.475\n",
            "Validation Loss: 3.320924275999616, Validation Accuracy: 22.15\n",
            "[121/150]: Training Loss: 3.1707864654541016, Training Accuracy: 24.485\n",
            "Validation Loss: 3.3217681638754097, Validation Accuracy: 21.98\n",
            "[122/150]: Training Loss: 3.169487755203247, Training Accuracy: 24.5\n",
            "Validation Loss: 3.3205978354071357, Validation Accuracy: 22.2\n",
            "[123/150]: Training Loss: 3.1687754665374754, Training Accuracy: 24.4775\n",
            "Validation Loss: 3.3199286217902118, Validation Accuracy: 22.03\n",
            "[124/150]: Training Loss: 3.1677217502593993, Training Accuracy: 24.5225\n",
            "Validation Loss: 3.3194276329818044, Validation Accuracy: 22.2\n",
            "[125/150]: Training Loss: 3.1672953506469725, Training Accuracy: 24.53\n",
            "Validation Loss: 3.3200536381666828, Validation Accuracy: 22.05\n",
            "[126/150]: Training Loss: 3.166370540237427, Training Accuracy: 24.53\n",
            "Validation Loss: 3.3180840486174175, Validation Accuracy: 22.01\n",
            "[127/150]: Training Loss: 3.165336986541748, Training Accuracy: 24.565\n",
            "Validation Loss: 3.318746434655159, Validation Accuracy: 22.24\n",
            "[128/150]: Training Loss: 3.164968849182129, Training Accuracy: 24.535\n",
            "Validation Loss: 3.3186944214401732, Validation Accuracy: 22.05\n",
            "[129/150]: Training Loss: 3.1642740074157714, Training Accuracy: 24.5175\n",
            "Validation Loss: 3.3177084133123897, Validation Accuracy: 22.21\n",
            "[130/150]: Training Loss: 3.16352066116333, Training Accuracy: 24.655\n",
            "Validation Loss: 3.318562794642843, Validation Accuracy: 22.01\n",
            "[131/150]: Training Loss: 3.1630500473022463, Training Accuracy: 24.5975\n",
            "Validation Loss: 3.317300656798539, Validation Accuracy: 22.06\n",
            "[132/150]: Training Loss: 3.1626014945983885, Training Accuracy: 24.66\n",
            "Validation Loss: 3.317058933768303, Validation Accuracy: 22.16\n",
            "[133/150]: Training Loss: 3.161920825958252, Training Accuracy: 24.64\n",
            "Validation Loss: 3.3171579837799072, Validation Accuracy: 22.02\n",
            "[134/150]: Training Loss: 3.161545040512085, Training Accuracy: 24.6375\n",
            "Validation Loss: 3.316502076045723, Validation Accuracy: 22.18\n",
            "[135/150]: Training Loss: 3.1611102500915527, Training Accuracy: 24.585\n",
            "Validation Loss: 3.3166633107859615, Validation Accuracy: 22.06\n",
            "[136/150]: Training Loss: 3.160765283203125, Training Accuracy: 24.6325\n",
            "Validation Loss: 3.3164391669498126, Validation Accuracy: 22.14\n",
            "[137/150]: Training Loss: 3.160325936508179, Training Accuracy: 24.6675\n",
            "Validation Loss: 3.315987286294342, Validation Accuracy: 22.11\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 5.462109195198982, Test Accuracy: 10.88\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▆█▂▁▂▂▁▁▂▂▃▃▂▂▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▃▁▄▆▆▇██▇▇▆▇▇▇▇▇▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Train Loss</td><td>█▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>10.88</td></tr><tr><td>Test Loss</td><td>5.46211</td></tr><tr><td>Train Accuracy</td><td>24.6675</td></tr><tr><td>Train Loss</td><td>3.16033</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.05 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_023355-cthymqs8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.1 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_024913-26wygp07</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07' target=\"_blank\">learning_rate=0.1 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.554393922424317, Training Accuracy: 2.2\n",
            "Validation Loss: 4.421840394378468, Validation Accuracy: 3.74\n",
            "[2/150]: Training Loss: 4.284681567382813, Training Accuracy: 4.52\n",
            "Validation Loss: 4.239770348664302, Validation Accuracy: 4.95\n",
            "[3/150]: Training Loss: 4.181965134429932, Training Accuracy: 5.585\n",
            "Validation Loss: 4.184003517126581, Validation Accuracy: 5.53\n",
            "[4/150]: Training Loss: 4.125413941192627, Training Accuracy: 6.6775\n",
            "Validation Loss: 4.123335443484556, Validation Accuracy: 6.52\n",
            "[5/150]: Training Loss: 4.084817845535278, Training Accuracy: 7.4525\n",
            "Validation Loss: 4.096477279237881, Validation Accuracy: 6.87\n",
            "[6/150]: Training Loss: 4.048351853179931, Training Accuracy: 8.1125\n",
            "Validation Loss: 4.069233142646255, Validation Accuracy: 7.8\n",
            "[7/150]: Training Loss: 4.018422792816162, Training Accuracy: 8.56\n",
            "Validation Loss: 4.0364193445558, Validation Accuracy: 8.44\n",
            "[8/150]: Training Loss: 3.989247378540039, Training Accuracy: 8.9625\n",
            "Validation Loss: 4.010960247865908, Validation Accuracy: 8.51\n",
            "[9/150]: Training Loss: 3.9593318000793456, Training Accuracy: 9.5725\n",
            "Validation Loss: 3.9864684988738626, Validation Accuracy: 8.73\n",
            "[10/150]: Training Loss: 3.935969552612305, Training Accuracy: 9.97\n",
            "Validation Loss: 3.959391053315181, Validation Accuracy: 9.43\n",
            "[11/150]: Training Loss: 3.910384812927246, Training Accuracy: 10.53\n",
            "Validation Loss: 3.9335520723063473, Validation Accuracy: 10.05\n",
            "[12/150]: Training Loss: 3.8837633472442628, Training Accuracy: 11.015\n",
            "Validation Loss: 3.9218593843423637, Validation Accuracy: 9.91\n",
            "[13/150]: Training Loss: 3.85968777885437, Training Accuracy: 11.265\n",
            "Validation Loss: 3.884523991566555, Validation Accuracy: 10.94\n",
            "[14/150]: Training Loss: 3.83401321182251, Training Accuracy: 12.19\n",
            "Validation Loss: 3.8702170302154153, Validation Accuracy: 11.29\n",
            "[15/150]: Training Loss: 3.8098957332611083, Training Accuracy: 12.6325\n",
            "Validation Loss: 3.840197701363047, Validation Accuracy: 11.62\n",
            "[16/150]: Training Loss: 3.7832756935119627, Training Accuracy: 13.2\n",
            "Validation Loss: 3.8233511766810326, Validation Accuracy: 12.45\n",
            "[17/150]: Training Loss: 3.755026846694946, Training Accuracy: 13.5\n",
            "Validation Loss: 3.7900661996975065, Validation Accuracy: 12.91\n",
            "[18/150]: Training Loss: 3.7291974296569825, Training Accuracy: 14.06\n",
            "Validation Loss: 3.7731176181963293, Validation Accuracy: 13.22\n",
            "[19/150]: Training Loss: 3.703822989273071, Training Accuracy: 14.6625\n",
            "Validation Loss: 3.742501725057128, Validation Accuracy: 13.6\n",
            "[20/150]: Training Loss: 3.676954047012329, Training Accuracy: 15.14\n",
            "Validation Loss: 3.718170521365609, Validation Accuracy: 14.24\n",
            "[21/150]: Training Loss: 3.6502103706359863, Training Accuracy: 15.59\n",
            "Validation Loss: 3.6871351967951296, Validation Accuracy: 14.81\n",
            "[22/150]: Training Loss: 3.622364068222046, Training Accuracy: 16.105\n",
            "Validation Loss: 3.67376760464565, Validation Accuracy: 14.79\n",
            "[23/150]: Training Loss: 3.5969187736511232, Training Accuracy: 16.585\n",
            "Validation Loss: 3.6408486214413007, Validation Accuracy: 15.59\n",
            "[24/150]: Training Loss: 3.568799534988403, Training Accuracy: 17.215\n",
            "Validation Loss: 3.6294803862359113, Validation Accuracy: 15.79\n",
            "[25/150]: Training Loss: 3.5421108798980714, Training Accuracy: 17.57\n",
            "Validation Loss: 3.598143538092352, Validation Accuracy: 16.37\n",
            "[26/150]: Training Loss: 3.5124191093444823, Training Accuracy: 18.205\n",
            "Validation Loss: 3.5703049495721317, Validation Accuracy: 16.98\n",
            "[27/150]: Training Loss: 3.484529112625122, Training Accuracy: 18.7275\n",
            "Validation Loss: 3.5376776236637384, Validation Accuracy: 17.48\n",
            "[28/150]: Training Loss: 3.455819899749756, Training Accuracy: 19.0425\n",
            "Validation Loss: 3.5291991643844898, Validation Accuracy: 17.68\n",
            "[29/150]: Training Loss: 3.4284223934173585, Training Accuracy: 19.5625\n",
            "Validation Loss: 3.4991380257211673, Validation Accuracy: 18.43\n",
            "[30/150]: Training Loss: 3.4028552402496337, Training Accuracy: 20.0525\n",
            "Validation Loss: 3.4806957472661497, Validation Accuracy: 18.35\n",
            "[31/150]: Training Loss: 3.376880758666992, Training Accuracy: 20.5925\n",
            "Validation Loss: 3.450026891793415, Validation Accuracy: 19.53\n",
            "[32/150]: Training Loss: 3.354429434585571, Training Accuracy: 21.165\n",
            "Validation Loss: 3.435450357995975, Validation Accuracy: 19.72\n",
            "[33/150]: Training Loss: 3.3340842437744143, Training Accuracy: 21.175\n",
            "Validation Loss: 3.418211444927629, Validation Accuracy: 19.67\n",
            "[34/150]: Training Loss: 3.314141171646118, Training Accuracy: 21.5925\n",
            "Validation Loss: 3.4085868847597935, Validation Accuracy: 20.29\n",
            "[35/150]: Training Loss: 3.2981444416046144, Training Accuracy: 21.85\n",
            "Validation Loss: 3.3930132495369882, Validation Accuracy: 20.07\n",
            "[36/150]: Training Loss: 3.280233634567261, Training Accuracy: 22.4375\n",
            "Validation Loss: 3.3805624132703063, Validation Accuracy: 20.71\n",
            "[37/150]: Training Loss: 3.2627798179626466, Training Accuracy: 22.555\n",
            "Validation Loss: 3.365730774630407, Validation Accuracy: 21.1\n",
            "[38/150]: Training Loss: 3.246723106765747, Training Accuracy: 22.8925\n",
            "Validation Loss: 3.3649645671722994, Validation Accuracy: 21.17\n",
            "[39/150]: Training Loss: 3.23333472366333, Training Accuracy: 23.1725\n",
            "Validation Loss: 3.340107489543356, Validation Accuracy: 21.48\n",
            "[40/150]: Training Loss: 3.2185105766296385, Training Accuracy: 23.425\n",
            "Validation Loss: 3.3351985269291387, Validation Accuracy: 21.9\n",
            "[41/150]: Training Loss: 3.2041201099395753, Training Accuracy: 23.65\n",
            "Validation Loss: 3.320353848159693, Validation Accuracy: 22.37\n",
            "[42/150]: Training Loss: 3.1920853351593017, Training Accuracy: 23.97\n",
            "Validation Loss: 3.3151075627393785, Validation Accuracy: 21.98\n",
            "[43/150]: Training Loss: 3.178889651107788, Training Accuracy: 24.015\n",
            "Validation Loss: 3.3039617690311114, Validation Accuracy: 22.11\n",
            "[44/150]: Training Loss: 3.164986518096924, Training Accuracy: 24.4275\n",
            "Validation Loss: 3.2982496850809473, Validation Accuracy: 22.16\n",
            "[45/150]: Training Loss: 3.1540319355010986, Training Accuracy: 24.705\n",
            "Validation Loss: 3.2881052099215755, Validation Accuracy: 22.49\n",
            "[46/150]: Training Loss: 3.1423869262695314, Training Accuracy: 24.745\n",
            "Validation Loss: 3.285053641932785, Validation Accuracy: 22.32\n",
            "[47/150]: Training Loss: 3.1313994647979735, Training Accuracy: 25.0425\n",
            "Validation Loss: 3.2773830009873506, Validation Accuracy: 22.64\n",
            "[48/150]: Training Loss: 3.1199681632995606, Training Accuracy: 25.2175\n",
            "Validation Loss: 3.2647413463349553, Validation Accuracy: 22.66\n",
            "[49/150]: Training Loss: 3.107323546600342, Training Accuracy: 25.5575\n",
            "Validation Loss: 3.277184949559011, Validation Accuracy: 22.72\n",
            "[50/150]: Training Loss: 3.0971016899108887, Training Accuracy: 25.77\n",
            "Validation Loss: 3.2499756160055755, Validation Accuracy: 22.69\n",
            "[51/150]: Training Loss: 3.0865638957977293, Training Accuracy: 25.915\n",
            "Validation Loss: 3.28796663102071, Validation Accuracy: 22.45\n",
            "[52/150]: Training Loss: 3.075482699203491, Training Accuracy: 26.0775\n",
            "Validation Loss: 3.256385016593204, Validation Accuracy: 22.97\n",
            "[53/150]: Training Loss: 3.0676188301086427, Training Accuracy: 26.045\n",
            "Validation Loss: 3.2297865843317313, Validation Accuracy: 23.53\n",
            "[54/150]: Training Loss: 3.0562880493164064, Training Accuracy: 26.5925\n",
            "Validation Loss: 3.2346752266974965, Validation Accuracy: 23.32\n",
            "[55/150]: Training Loss: 3.046115529251099, Training Accuracy: 26.525\n",
            "Validation Loss: 3.229158471344383, Validation Accuracy: 23.27\n",
            "[56/150]: Training Loss: 3.0363171295166014, Training Accuracy: 26.5875\n",
            "Validation Loss: 3.2251109287237667, Validation Accuracy: 23.85\n",
            "[57/150]: Training Loss: 3.0262594100952147, Training Accuracy: 26.96\n",
            "Validation Loss: 3.20999311793382, Validation Accuracy: 23.73\n",
            "[58/150]: Training Loss: 3.018730586242676, Training Accuracy: 26.925\n",
            "Validation Loss: 3.206504849111958, Validation Accuracy: 23.66\n",
            "[59/150]: Training Loss: 3.007856759262085, Training Accuracy: 27.315\n",
            "Validation Loss: 3.2090084021258507, Validation Accuracy: 24.02\n",
            "[60/150]: Training Loss: 3.0000602096557616, Training Accuracy: 27.39\n",
            "Validation Loss: 3.2029919001706846, Validation Accuracy: 23.96\n",
            "[61/150]: Training Loss: 2.9893322017669677, Training Accuracy: 27.6525\n",
            "Validation Loss: 3.207233451733923, Validation Accuracy: 23.61\n",
            "[62/150]: Training Loss: 2.9812552192687987, Training Accuracy: 27.855\n",
            "Validation Loss: 3.191280894978031, Validation Accuracy: 23.71\n",
            "[63/150]: Training Loss: 2.972245023727417, Training Accuracy: 27.83\n",
            "Validation Loss: 3.191427630224046, Validation Accuracy: 24.21\n",
            "[64/150]: Training Loss: 2.9653648654937745, Training Accuracy: 28.0175\n",
            "Validation Loss: 3.189020594214178, Validation Accuracy: 24.07\n",
            "[65/150]: Training Loss: 2.957701969909668, Training Accuracy: 28.245\n",
            "Validation Loss: 3.183226536793314, Validation Accuracy: 23.98\n",
            "[66/150]: Training Loss: 2.9480487686157226, Training Accuracy: 28.3725\n",
            "Validation Loss: 3.1869888670125586, Validation Accuracy: 24.08\n",
            "[67/150]: Training Loss: 2.9404784973144533, Training Accuracy: 28.5625\n",
            "Validation Loss: 3.1826100653144205, Validation Accuracy: 24.61\n",
            "[68/150]: Training Loss: 2.9308408599853517, Training Accuracy: 28.73\n",
            "Validation Loss: 3.1697433404861743, Validation Accuracy: 24.11\n",
            "[69/150]: Training Loss: 2.924504146194458, Training Accuracy: 28.735\n",
            "Validation Loss: 3.164285852650928, Validation Accuracy: 24.76\n",
            "[70/150]: Training Loss: 2.9181696743011476, Training Accuracy: 29.045\n",
            "Validation Loss: 3.1617295559804153, Validation Accuracy: 24.5\n",
            "[71/150]: Training Loss: 2.909669787979126, Training Accuracy: 28.97\n",
            "Validation Loss: 3.16842480222131, Validation Accuracy: 24.55\n",
            "[72/150]: Training Loss: 2.901284480667114, Training Accuracy: 29.165\n",
            "Validation Loss: 3.16541398710506, Validation Accuracy: 24.39\n",
            "[73/150]: Training Loss: 2.8958903297424317, Training Accuracy: 29.41\n",
            "Validation Loss: 3.156350550378204, Validation Accuracy: 24.81\n",
            "[74/150]: Training Loss: 2.8880927780151366, Training Accuracy: 29.51\n",
            "Validation Loss: 3.1484538536922186, Validation Accuracy: 24.79\n",
            "[75/150]: Training Loss: 2.8816709129333495, Training Accuracy: 29.6575\n",
            "Validation Loss: 3.1521760521421007, Validation Accuracy: 24.75\n",
            "[76/150]: Training Loss: 2.874811888885498, Training Accuracy: 29.6025\n",
            "Validation Loss: 3.1412497159022434, Validation Accuracy: 24.86\n",
            "[77/150]: Training Loss: 2.868447174453735, Training Accuracy: 29.75\n",
            "Validation Loss: 3.148860450003557, Validation Accuracy: 25.12\n",
            "[78/150]: Training Loss: 2.8620141311645506, Training Accuracy: 30.02\n",
            "Validation Loss: 3.1491585096735863, Validation Accuracy: 25.07\n",
            "[79/150]: Training Loss: 2.8542796688079832, Training Accuracy: 30.2175\n",
            "Validation Loss: 3.1315311428847585, Validation Accuracy: 25.26\n",
            "[80/150]: Training Loss: 2.8495312446594236, Training Accuracy: 30.2625\n",
            "Validation Loss: 3.1400332511610287, Validation Accuracy: 25.12\n",
            "[81/150]: Training Loss: 2.8422299156188964, Training Accuracy: 30.4475\n",
            "Validation Loss: 3.129277274866772, Validation Accuracy: 25.51\n",
            "[82/150]: Training Loss: 2.8352813999176028, Training Accuracy: 30.645\n",
            "Validation Loss: 3.1319479532302563, Validation Accuracy: 25.37\n",
            "[83/150]: Training Loss: 2.830805637359619, Training Accuracy: 30.595\n",
            "Validation Loss: 3.1387126217981813, Validation Accuracy: 25.34\n",
            "[84/150]: Training Loss: 2.824944213485718, Training Accuracy: 30.7875\n",
            "Validation Loss: 3.1277991054923673, Validation Accuracy: 25.58\n",
            "[85/150]: Training Loss: 2.8196789070129396, Training Accuracy: 31.02\n",
            "Validation Loss: 3.1215793433462737, Validation Accuracy: 25.59\n",
            "[86/150]: Training Loss: 2.814448028564453, Training Accuracy: 31.1025\n",
            "Validation Loss: 3.1227446027622103, Validation Accuracy: 25.41\n",
            "[87/150]: Training Loss: 2.8087932884216307, Training Accuracy: 31.3075\n",
            "Validation Loss: 3.123080830665151, Validation Accuracy: 25.3\n",
            "[88/150]: Training Loss: 2.8027573429107666, Training Accuracy: 31.35\n",
            "Validation Loss: 3.1130096623851995, Validation Accuracy: 25.94\n",
            "[89/150]: Training Loss: 2.7979101371765136, Training Accuracy: 31.195\n",
            "Validation Loss: 3.1245606795997376, Validation Accuracy: 25.59\n",
            "[90/150]: Training Loss: 2.79326137008667, Training Accuracy: 31.37\n",
            "Validation Loss: 3.1252101788854905, Validation Accuracy: 25.47\n",
            "[91/150]: Training Loss: 2.7884928009033203, Training Accuracy: 31.3925\n",
            "Validation Loss: 3.115745410797702, Validation Accuracy: 25.52\n",
            "[92/150]: Training Loss: 2.7844152694702147, Training Accuracy: 31.5825\n",
            "Validation Loss: 3.1132962870749696, Validation Accuracy: 25.67\n",
            "[93/150]: Training Loss: 2.7792159679412842, Training Accuracy: 31.665\n",
            "Validation Loss: 3.111494478906036, Validation Accuracy: 25.84\n",
            "[94/150]: Training Loss: 2.773549281311035, Training Accuracy: 31.6525\n",
            "Validation Loss: 3.1067730256706287, Validation Accuracy: 25.89\n",
            "[95/150]: Training Loss: 2.76964068031311, Training Accuracy: 31.895\n",
            "Validation Loss: 3.1083280220153227, Validation Accuracy: 25.78\n",
            "[96/150]: Training Loss: 2.7651722465515136, Training Accuracy: 31.8825\n",
            "Validation Loss: 3.1057740730844485, Validation Accuracy: 25.67\n",
            "[97/150]: Training Loss: 2.761124114608765, Training Accuracy: 32.1025\n",
            "Validation Loss: 3.1073889534944183, Validation Accuracy: 25.55\n",
            "[98/150]: Training Loss: 2.756427172470093, Training Accuracy: 32.1475\n",
            "Validation Loss: 3.1095221422280477, Validation Accuracy: 25.75\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 6.147166403995198, Test Accuracy: 12.53\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▄█▆▄▅▃▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▅▁▆▆▇███▇▇▆▇█▇▇▇▇▇▇▇▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>12.53</td></tr><tr><td>Test Loss</td><td>6.14717</td></tr><tr><td>Train Accuracy</td><td>32.1475</td></tr><tr><td>Train Loss</td><td>2.75643</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.1 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_024913-26wygp07/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.5 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_030015-a8qnvlbp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp' target=\"_blank\">learning_rate=0.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.377298123168945, Training Accuracy: 3.2725\n",
            "Validation Loss: 4.2101050941807445, Validation Accuracy: 5.2\n",
            "[2/150]: Training Loss: 4.07323479423523, Training Accuracy: 7.05\n",
            "Validation Loss: 3.997956647994412, Validation Accuracy: 8.69\n",
            "[3/150]: Training Loss: 3.9166299026489257, Training Accuracy: 9.98\n",
            "Validation Loss: 3.8700176090191882, Validation Accuracy: 10.85\n",
            "[4/150]: Training Loss: 3.76255333404541, Training Accuracy: 12.6475\n",
            "Validation Loss: 3.7763638131937403, Validation Accuracy: 12.03\n",
            "[5/150]: Training Loss: 3.6385873168945313, Training Accuracy: 14.8\n",
            "Validation Loss: 3.5995885247637514, Validation Accuracy: 15.3\n",
            "[6/150]: Training Loss: 3.5313883621215822, Training Accuracy: 16.6625\n",
            "Validation Loss: 3.547201700271315, Validation Accuracy: 16.33\n",
            "[7/150]: Training Loss: 3.4311797870635985, Training Accuracy: 18.0425\n",
            "Validation Loss: 3.447270431336324, Validation Accuracy: 18.49\n",
            "[8/150]: Training Loss: 3.3406018447875976, Training Accuracy: 20.2325\n",
            "Validation Loss: 3.3745645914867426, Validation Accuracy: 19.81\n",
            "[9/150]: Training Loss: 3.2601512760162352, Training Accuracy: 21.73\n",
            "Validation Loss: 3.3083346163391307, Validation Accuracy: 21.21\n",
            "[10/150]: Training Loss: 3.190315942764282, Training Accuracy: 23.0925\n",
            "Validation Loss: 3.2459553244766917, Validation Accuracy: 22.4\n",
            "[11/150]: Training Loss: 3.120881378555298, Training Accuracy: 24.2625\n",
            "Validation Loss: 3.2441095774340782, Validation Accuracy: 22.07\n",
            "[12/150]: Training Loss: 3.065099639892578, Training Accuracy: 25.1825\n",
            "Validation Loss: 3.205546066259882, Validation Accuracy: 22.44\n",
            "[13/150]: Training Loss: 3.002047216796875, Training Accuracy: 26.4075\n",
            "Validation Loss: 3.1507282090035216, Validation Accuracy: 24.42\n",
            "[14/150]: Training Loss: 2.947496301269531, Training Accuracy: 27.6075\n",
            "Validation Loss: 3.1272822519776167, Validation Accuracy: 24.2\n",
            "[15/150]: Training Loss: 2.8929669227600097, Training Accuracy: 28.1625\n",
            "Validation Loss: 3.08642372052381, Validation Accuracy: 25.55\n",
            "[16/150]: Training Loss: 2.8405557304382323, Training Accuracy: 29.4725\n",
            "Validation Loss: 3.021532262206837, Validation Accuracy: 26.7\n",
            "[17/150]: Training Loss: 2.7890852066040037, Training Accuracy: 30.505\n",
            "Validation Loss: 3.042905480998337, Validation Accuracy: 26.24\n",
            "[18/150]: Training Loss: 2.74124369392395, Training Accuracy: 31.39\n",
            "Validation Loss: 2.9991517309929914, Validation Accuracy: 26.63\n",
            "[19/150]: Training Loss: 2.692549164581299, Training Accuracy: 32.36\n",
            "Validation Loss: 2.9798509998685994, Validation Accuracy: 27.88\n",
            "[20/150]: Training Loss: 2.6449439025878907, Training Accuracy: 33.1025\n",
            "Validation Loss: 3.0654538998937912, Validation Accuracy: 26.5\n",
            "[21/150]: Training Loss: 2.6013897836685183, Training Accuracy: 34.135\n",
            "Validation Loss: 2.937944315041706, Validation Accuracy: 29.27\n",
            "[22/150]: Training Loss: 2.5583987575531006, Training Accuracy: 35.0375\n",
            "Validation Loss: 2.965156334980278, Validation Accuracy: 28.48\n",
            "[23/150]: Training Loss: 2.5067957414627076, Training Accuracy: 36.145\n",
            "Validation Loss: 2.940655703757219, Validation Accuracy: 28.99\n",
            "[24/150]: Training Loss: 2.4618710725784303, Training Accuracy: 37.005\n",
            "Validation Loss: 2.9297352444594074, Validation Accuracy: 28.88\n",
            "[25/150]: Training Loss: 2.4159774208068847, Training Accuracy: 37.9375\n",
            "Validation Loss: 2.9579435579336373, Validation Accuracy: 28.85\n",
            "[26/150]: Training Loss: 2.3772740001678465, Training Accuracy: 38.8375\n",
            "Validation Loss: 2.892203370476984, Validation Accuracy: 29.97\n",
            "[27/150]: Training Loss: 2.331508861351013, Training Accuracy: 39.695\n",
            "Validation Loss: 2.902201515853785, Validation Accuracy: 29.95\n",
            "[28/150]: Training Loss: 2.292740362548828, Training Accuracy: 40.25\n",
            "Validation Loss: 2.884886794788822, Validation Accuracy: 30.54\n",
            "[29/150]: Training Loss: 2.246493480873108, Training Accuracy: 41.2375\n",
            "Validation Loss: 2.9104482884619647, Validation Accuracy: 30.14\n",
            "[30/150]: Training Loss: 2.203635430717468, Training Accuracy: 42.4475\n",
            "Validation Loss: 2.9059300164508213, Validation Accuracy: 31.3\n",
            "[31/150]: Training Loss: 2.1624910346984865, Training Accuracy: 43.2625\n",
            "Validation Loss: 2.884638991325524, Validation Accuracy: 31.26\n",
            "[32/150]: Training Loss: 2.122718844985962, Training Accuracy: 44.17\n",
            "Validation Loss: 2.9034518907024602, Validation Accuracy: 30.71\n",
            "[33/150]: Training Loss: 2.0803083070755006, Training Accuracy: 45.055\n",
            "Validation Loss: 2.8911248453103813, Validation Accuracy: 31.08\n",
            "[34/150]: Training Loss: 2.0393711433410644, Training Accuracy: 45.9575\n",
            "Validation Loss: 2.9181400818429934, Validation Accuracy: 31.06\n",
            "[35/150]: Training Loss: 2.001698533630371, Training Accuracy: 46.8225\n",
            "Validation Loss: 2.9216579084943053, Validation Accuracy: 30.6\n",
            "[36/150]: Training Loss: 1.9612565605163574, Training Accuracy: 47.845\n",
            "Validation Loss: 2.95554546945414, Validation Accuracy: 31.47\n",
            "[37/150]: Training Loss: 1.9143695009231567, Training Accuracy: 48.7825\n",
            "Validation Loss: 2.9769522748934993, Validation Accuracy: 30.95\n",
            "[38/150]: Training Loss: 1.8757219652175903, Training Accuracy: 49.6925\n",
            "Validation Loss: 2.954901927595685, Validation Accuracy: 31.79\n",
            "[39/150]: Training Loss: 1.8348310264587402, Training Accuracy: 50.46\n",
            "Validation Loss: 2.97764066374226, Validation Accuracy: 31.41\n",
            "[40/150]: Training Loss: 1.7973475383758546, Training Accuracy: 51.6725\n",
            "Validation Loss: 3.0162993069666966, Validation Accuracy: 31.12\n",
            "[41/150]: Training Loss: 1.7525395877838135, Training Accuracy: 52.525\n",
            "Validation Loss: 3.0236745138836514, Validation Accuracy: 30.91\n",
            "[42/150]: Training Loss: 1.7114470052719115, Training Accuracy: 53.885\n",
            "Validation Loss: 3.051269774224348, Validation Accuracy: 31.17\n",
            "[43/150]: Training Loss: 1.6800853149414063, Training Accuracy: 54.0875\n",
            "Validation Loss: 3.0829398237216243, Validation Accuracy: 31.28\n",
            "[44/150]: Training Loss: 1.636157625389099, Training Accuracy: 55.3375\n",
            "Validation Loss: 3.110968621673098, Validation Accuracy: 31.14\n",
            "[45/150]: Training Loss: 1.589929871749878, Training Accuracy: 56.7225\n",
            "Validation Loss: 3.142918248085459, Validation Accuracy: 31.26\n",
            "[46/150]: Training Loss: 1.5445106566429139, Training Accuracy: 57.9225\n",
            "Validation Loss: 3.1339099574240907, Validation Accuracy: 31.8\n",
            "[47/150]: Training Loss: 1.5067133940696715, Training Accuracy: 58.8375\n",
            "Validation Loss: 3.1682807533604325, Validation Accuracy: 31.84\n",
            "[48/150]: Training Loss: 1.4696476486206054, Training Accuracy: 59.655\n",
            "Validation Loss: 3.2345377184023523, Validation Accuracy: 30.89\n",
            "[49/150]: Training Loss: 1.4380368849754332, Training Accuracy: 60.32\n",
            "Validation Loss: 3.2836993484740047, Validation Accuracy: 31.58\n",
            "[50/150]: Training Loss: 1.3966082113265992, Training Accuracy: 61.33\n",
            "Validation Loss: 3.369933787424853, Validation Accuracy: 30.61\n",
            "[51/150]: Training Loss: 1.3530357138633728, Training Accuracy: 62.43\n",
            "Validation Loss: 3.357933571384211, Validation Accuracy: 31.19\n",
            "[52/150]: Training Loss: 1.31355241689682, Training Accuracy: 63.4325\n",
            "Validation Loss: 3.41107276138986, Validation Accuracy: 30.17\n",
            "[53/150]: Training Loss: 1.2827554839134216, Training Accuracy: 64.0425\n",
            "Validation Loss: 3.376580183673057, Validation Accuracy: 30.85\n",
            "[54/150]: Training Loss: 1.2412437489509582, Training Accuracy: 65.2825\n",
            "Validation Loss: 3.48424918332677, Validation Accuracy: 30.12\n",
            "[55/150]: Training Loss: 1.1999455925941467, Training Accuracy: 66.39\n",
            "Validation Loss: 3.4844332743602195, Validation Accuracy: 30.82\n",
            "[56/150]: Training Loss: 1.1624719073295593, Training Accuracy: 67.315\n",
            "Validation Loss: 3.535801676428242, Validation Accuracy: 30.86\n",
            "[57/150]: Training Loss: 1.1305819693565369, Training Accuracy: 67.9275\n",
            "Validation Loss: 3.6139490315868597, Validation Accuracy: 30.44\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 16.0335865749675, Test Accuracy: 13.22\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▄▂▂▂▁▂▁▁▁▁▁▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>█▁▅▄▄▅▆▇▅▄▃▄▅▄▅▅▄▅▄▅▅▄▄▄▄▄▄▄▄▅▅▅▅▅▄▅▅▅▄▄</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>13.22</td></tr><tr><td>Test Loss</td><td>16.03359</td></tr><tr><td>Train Accuracy</td><td>67.9275</td></tr><tr><td>Train Loss</td><td>1.13058</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_030015-a8qnvlbp/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:1 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_030647-hgowk59s</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s' target=\"_blank\">learning_rate=1 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.390103232955933, Training Accuracy: 2.935\n",
            "Validation Loss: 4.160131826522244, Validation Accuracy: 5.54\n",
            "[2/150]: Training Loss: 4.031327721786499, Training Accuracy: 7.5025\n",
            "Validation Loss: 3.9958771960750505, Validation Accuracy: 8.6\n",
            "[3/150]: Training Loss: 3.800543330383301, Training Accuracy: 11.665\n",
            "Validation Loss: 3.7379826664165328, Validation Accuracy: 12.06\n",
            "[4/150]: Training Loss: 3.6242603660583494, Training Accuracy: 14.345\n",
            "Validation Loss: 3.559469526740396, Validation Accuracy: 15.28\n",
            "[5/150]: Training Loss: 3.48455090675354, Training Accuracy: 16.9325\n",
            "Validation Loss: 3.491581111956554, Validation Accuracy: 16.56\n",
            "[6/150]: Training Loss: 3.3448009601593016, Training Accuracy: 19.435\n",
            "Validation Loss: 3.3586603653658726, Validation Accuracy: 19.41\n",
            "[7/150]: Training Loss: 3.229361641693115, Training Accuracy: 21.4375\n",
            "Validation Loss: 3.2139996130754995, Validation Accuracy: 21.78\n",
            "[8/150]: Training Loss: 3.1254781677246095, Training Accuracy: 23.4825\n",
            "Validation Loss: 3.168266389020689, Validation Accuracy: 22.86\n",
            "[9/150]: Training Loss: 3.0173661666870117, Training Accuracy: 25.6925\n",
            "Validation Loss: 3.1283769850518293, Validation Accuracy: 24.17\n",
            "[10/150]: Training Loss: 2.931245880126953, Training Accuracy: 26.99\n",
            "Validation Loss: 3.016676799506898, Validation Accuracy: 25.79\n",
            "[11/150]: Training Loss: 2.8445968715667727, Training Accuracy: 28.6675\n",
            "Validation Loss: 2.968725839238258, Validation Accuracy: 26.94\n",
            "[12/150]: Training Loss: 2.769311902618408, Training Accuracy: 30.345\n",
            "Validation Loss: 2.935349525160091, Validation Accuracy: 27.73\n",
            "[13/150]: Training Loss: 2.698523984146118, Training Accuracy: 31.47\n",
            "Validation Loss: 2.9356894462731233, Validation Accuracy: 28.11\n",
            "[14/150]: Training Loss: 2.6179113666534426, Training Accuracy: 33.2625\n",
            "Validation Loss: 2.8293940914664297, Validation Accuracy: 30.0\n",
            "[15/150]: Training Loss: 2.5539515073776244, Training Accuracy: 34.565\n",
            "Validation Loss: 2.8150154101620815, Validation Accuracy: 30.54\n",
            "[16/150]: Training Loss: 2.482784992599487, Training Accuracy: 36.2825\n",
            "Validation Loss: 2.801185814438352, Validation Accuracy: 30.73\n",
            "[17/150]: Training Loss: 2.416481968307495, Training Accuracy: 37.2175\n",
            "Validation Loss: 2.778297721200688, Validation Accuracy: 31.36\n",
            "[18/150]: Training Loss: 2.3516669242858885, Training Accuracy: 38.785\n",
            "Validation Loss: 2.803610542017943, Validation Accuracy: 31.44\n",
            "[19/150]: Training Loss: 2.2785828254699707, Training Accuracy: 40.36\n",
            "Validation Loss: 2.770737362515395, Validation Accuracy: 31.54\n",
            "[20/150]: Training Loss: 2.2117202407836913, Training Accuracy: 42.02\n",
            "Validation Loss: 2.751694551698721, Validation Accuracy: 32.2\n",
            "[21/150]: Training Loss: 2.1421232624053954, Training Accuracy: 43.1125\n",
            "Validation Loss: 2.7352929327897963, Validation Accuracy: 32.64\n",
            "[22/150]: Training Loss: 2.0760102186203, Training Accuracy: 44.805\n",
            "Validation Loss: 2.7648525397489023, Validation Accuracy: 32.85\n",
            "[23/150]: Training Loss: 2.0119320999145507, Training Accuracy: 46.1375\n",
            "Validation Loss: 2.7834541448362313, Validation Accuracy: 33.19\n",
            "[24/150]: Training Loss: 1.946039645767212, Training Accuracy: 47.555\n",
            "Validation Loss: 2.805889447023914, Validation Accuracy: 33.52\n",
            "[25/150]: Training Loss: 1.8788161462783814, Training Accuracy: 49.055\n",
            "Validation Loss: 2.7862223508251702, Validation Accuracy: 34.25\n",
            "[26/150]: Training Loss: 1.799899059486389, Training Accuracy: 50.915\n",
            "Validation Loss: 2.804932374863108, Validation Accuracy: 33.77\n",
            "[27/150]: Training Loss: 1.7454933450698853, Training Accuracy: 52.16\n",
            "Validation Loss: 2.861256376193587, Validation Accuracy: 33.83\n",
            "[28/150]: Training Loss: 1.6650921211242675, Training Accuracy: 53.9225\n",
            "Validation Loss: 2.877333830116661, Validation Accuracy: 34.25\n",
            "[29/150]: Training Loss: 1.6064131809234619, Training Accuracy: 55.555\n",
            "Validation Loss: 2.973996728089205, Validation Accuracy: 33.39\n",
            "[30/150]: Training Loss: 1.543812055683136, Training Accuracy: 56.7775\n",
            "Validation Loss: 2.943861024394916, Validation Accuracy: 34.15\n",
            "[31/150]: Training Loss: 1.4620859157562256, Training Accuracy: 58.88\n",
            "Validation Loss: 2.9692332015675342, Validation Accuracy: 33.88\n",
            "[32/150]: Training Loss: 1.4009520672798157, Training Accuracy: 60.3925\n",
            "Validation Loss: 3.024310210707841, Validation Accuracy: 33.42\n",
            "[33/150]: Training Loss: 1.3313846939086913, Training Accuracy: 62.155\n",
            "Validation Loss: 3.1067024993289047, Validation Accuracy: 33.18\n",
            "[34/150]: Training Loss: 1.2518226915359496, Training Accuracy: 64.225\n",
            "Validation Loss: 3.307321997964458, Validation Accuracy: 33.31\n",
            "[35/150]: Training Loss: 1.1938729809761048, Training Accuracy: 65.5075\n",
            "Validation Loss: 3.3076086089869214, Validation Accuracy: 32.76\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 18.36642926210051, Test Accuracy: 13.77\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▅▁▂▁▁▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▁▂▅▆▇▆▇█▇▇▇▇▇██▇███████▇▇███████████████</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>13.77</td></tr><tr><td>Test Loss</td><td>18.36643</td></tr><tr><td>Train Accuracy</td><td>65.5075</td></tr><tr><td>Train Loss</td><td>1.19387</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=1 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_030647-hgowk59s/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:1.5 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_031055-hfm8dgbz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz' target=\"_blank\">learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.426865048217773, Training Accuracy: 2.45\n",
            "Validation Loss: 4.219578746018136, Validation Accuracy: 4.19\n",
            "[2/150]: Training Loss: 4.060279036712647, Training Accuracy: 6.7225\n",
            "Validation Loss: 3.8940811293899635, Validation Accuracy: 9.43\n",
            "[3/150]: Training Loss: 3.7707011901855467, Training Accuracy: 11.6925\n",
            "Validation Loss: 3.8030508642743346, Validation Accuracy: 11.0\n",
            "[4/150]: Training Loss: 3.58028080368042, Training Accuracy: 15.05\n",
            "Validation Loss: 3.516739834645751, Validation Accuracy: 16.1\n",
            "[5/150]: Training Loss: 3.4152388591766356, Training Accuracy: 17.82\n",
            "Validation Loss: 3.411277610025588, Validation Accuracy: 17.62\n",
            "[6/150]: Training Loss: 3.2448327434539794, Training Accuracy: 20.7725\n",
            "Validation Loss: 3.2892891753251385, Validation Accuracy: 19.86\n",
            "[7/150]: Training Loss: 3.1129616333007815, Training Accuracy: 23.41\n",
            "Validation Loss: 3.155244686041668, Validation Accuracy: 22.75\n",
            "[8/150]: Training Loss: 2.9936462223052978, Training Accuracy: 25.73\n",
            "Validation Loss: 3.020396041262681, Validation Accuracy: 25.53\n",
            "[9/150]: Training Loss: 2.8851455352783204, Training Accuracy: 28.0475\n",
            "Validation Loss: 2.9794276216227535, Validation Accuracy: 26.31\n",
            "[10/150]: Training Loss: 2.7872766895294188, Training Accuracy: 29.955\n",
            "Validation Loss: 2.873067584007409, Validation Accuracy: 28.57\n",
            "[11/150]: Training Loss: 2.6929493949890135, Training Accuracy: 31.84\n",
            "Validation Loss: 2.8678578267431565, Validation Accuracy: 28.72\n",
            "[12/150]: Training Loss: 2.60749265499115, Training Accuracy: 33.46\n",
            "Validation Loss: 2.831394789325204, Validation Accuracy: 29.21\n",
            "[13/150]: Training Loss: 2.5269983169555665, Training Accuracy: 35.2325\n",
            "Validation Loss: 2.8061546094857963, Validation Accuracy: 29.73\n",
            "[14/150]: Training Loss: 2.430880810165405, Training Accuracy: 37.0475\n",
            "Validation Loss: 2.777110814288923, Validation Accuracy: 31.06\n",
            "[15/150]: Training Loss: 2.357077407836914, Training Accuracy: 38.8175\n",
            "Validation Loss: 2.7613005273661035, Validation Accuracy: 32.32\n",
            "[16/150]: Training Loss: 2.2730004482269286, Training Accuracy: 40.5\n",
            "Validation Loss: 2.7338021667140304, Validation Accuracy: 32.77\n",
            "[17/150]: Training Loss: 2.183533114242554, Training Accuracy: 42.27\n",
            "Validation Loss: 2.764839885341134, Validation Accuracy: 32.81\n",
            "[18/150]: Training Loss: 2.1031003602981566, Training Accuracy: 43.9825\n",
            "Validation Loss: 2.7428993381512394, Validation Accuracy: 32.82\n",
            "[19/150]: Training Loss: 2.0166405237197877, Training Accuracy: 46.195\n",
            "Validation Loss: 2.792896580544247, Validation Accuracy: 32.59\n",
            "[20/150]: Training Loss: 1.9404266941070556, Training Accuracy: 47.745\n",
            "Validation Loss: 2.742409686374057, Validation Accuracy: 34.58\n",
            "[21/150]: Training Loss: 1.8538161834716798, Training Accuracy: 49.4875\n",
            "Validation Loss: 2.809636064395783, Validation Accuracy: 33.93\n",
            "[22/150]: Training Loss: 1.768288578414917, Training Accuracy: 51.4775\n",
            "Validation Loss: 2.7697540058451855, Validation Accuracy: 34.68\n",
            "[23/150]: Training Loss: 1.684787001991272, Training Accuracy: 53.0825\n",
            "Validation Loss: 2.8915080067458425, Validation Accuracy: 33.94\n",
            "[24/150]: Training Loss: 1.6049392827987672, Training Accuracy: 55.145\n",
            "Validation Loss: 2.8930992533446878, Validation Accuracy: 33.94\n",
            "[25/150]: Training Loss: 1.511135014438629, Training Accuracy: 57.31\n",
            "Validation Loss: 2.9712191205115834, Validation Accuracy: 33.37\n",
            "[26/150]: Training Loss: 1.431588893699646, Training Accuracy: 59.12\n",
            "Validation Loss: 3.04860136311525, Validation Accuracy: 34.26\n",
            "[27/150]: Training Loss: 1.359563471508026, Training Accuracy: 60.8075\n",
            "Validation Loss: 3.205587582983029, Validation Accuracy: 33.21\n",
            "[28/150]: Training Loss: 1.2744348917961121, Training Accuracy: 63.0075\n",
            "Validation Loss: 3.1817259317750386, Validation Accuracy: 32.84\n",
            "[29/150]: Training Loss: 1.2077363389968871, Training Accuracy: 64.38\n",
            "Validation Loss: 3.2625666909916387, Validation Accuracy: 33.74\n",
            "[30/150]: Training Loss: 1.146615783405304, Training Accuracy: 66.3775\n",
            "Validation Loss: 3.439337396317986, Validation Accuracy: 32.76\n",
            "[31/150]: Training Loss: 1.0637209503173828, Training Accuracy: 68.4275\n",
            "Validation Loss: 3.464182601612844, Validation Accuracy: 32.3\n",
            "[32/150]: Training Loss: 1.0153738078117371, Training Accuracy: 69.42\n",
            "Validation Loss: 3.585658008125937, Validation Accuracy: 32.99\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 20.296875631733304, Test Accuracy: 15.72\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▆▁▄▃▄▄▄▄▄▄▄▄▄▄▅▅▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>Test Loss</td><td>▁▅▇▇▇▆▇██▇▇█████████████████████████████</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>15.72</td></tr><tr><td>Test Loss</td><td>20.29688</td></tr><tr><td>Train Accuracy</td><td>69.42</td></tr><tr><td>Train Loss</td><td>1.01537</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_031055-hfm8dgbz/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:2 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_031444-ws7fqwm1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1' target=\"_blank\">learning_rate=2 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.6014143745422365, Training Accuracy: 0.98\n",
            "Validation Loss: 4.606205284215842, Validation Accuracy: 0.91\n",
            "[2/150]: Training Loss: 4.605726162719726, Training Accuracy: 0.93\n",
            "Validation Loss: 4.606422876856129, Validation Accuracy: 0.91\n",
            "[3/150]: Training Loss: 4.605443458557129, Training Accuracy: 0.9375\n",
            "Validation Loss: 4.606579792727331, Validation Accuracy: 0.82\n",
            "[4/150]: Training Loss: 4.6053140991210935, Training Accuracy: 1.015\n",
            "Validation Loss: 4.606754381945179, Validation Accuracy: 0.82\n",
            "[5/150]: Training Loss: 4.6052259376525875, Training Accuracy: 1.0275\n",
            "Validation Loss: 4.60688726765335, Validation Accuracy: 0.82\n",
            "[6/150]: Training Loss: 4.605191477966309, Training Accuracy: 1.0175\n",
            "Validation Loss: 4.607019655264107, Validation Accuracy: 0.82\n",
            "[7/150]: Training Loss: 4.6051658882141115, Training Accuracy: 1.015\n",
            "Validation Loss: 4.607097844409335, Validation Accuracy: 0.82\n",
            "[8/150]: Training Loss: 4.605149390411377, Training Accuracy: 0.9775\n",
            "Validation Loss: 4.607179025176224, Validation Accuracy: 0.82\n",
            "[9/150]: Training Loss: 4.605146067810058, Training Accuracy: 0.9425\n",
            "Validation Loss: 4.6072455181437695, Validation Accuracy: 0.82\n",
            "[10/150]: Training Loss: 4.605130741882324, Training Accuracy: 1.02\n",
            "Validation Loss: 4.607305599625703, Validation Accuracy: 0.82\n",
            "[11/150]: Training Loss: 4.605136211395264, Training Accuracy: 0.9625\n",
            "Validation Loss: 4.607342504392005, Validation Accuracy: 0.82\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 4.605385412835771, Test Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▆███▆▇▇▇▇▇▇▇▇▆▇███▇████████████████████</td></tr><tr><td>Test Loss</td><td>█▅▁▁▃▃▅▄▄▄▄▄▄▄▄▅▅▅▅▅▆▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅</td></tr><tr><td>Train Accuracy</td><td>▅▁▂▇█▇▇▄▂▇▃</td></tr><tr><td>Train Loss</td><td>▁██▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.0</td></tr><tr><td>Test Loss</td><td>4.60539</td></tr><tr><td>Train Accuracy</td><td>0.9625</td></tr><tr><td>Train Loss</td><td>4.60514</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=2 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_031444-ws7fqwm1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "\n",
        "for lr in learning_rates:\n",
        "\n",
        "  print('='*50)\n",
        "  print(f'Hyperparameter with lr:{lr} and wd:{wd}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {'learning_rate': lr,\n",
        "                      'weight_decay' : wd\n",
        "                      }\n",
        "  # Load the model\n",
        "  model = LeNet5().to(device)\n",
        "\n",
        "  # Optimizer and scheduler setup\n",
        "  optimizer = LARS(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "  # Training\n",
        "  run_training(num_epochs,\n",
        "                model,\n",
        "                train_loader,\n",
        "                validation_loader,\n",
        "                test_loader,\n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                criterion,\n",
        "                device,\n",
        "                'LARS-HyperParameterTuning',\n",
        "                hyperparameters=hyperparameters,\n",
        "                is_wandb = True,\n",
        "                n_epochs_stop = 10\n",
        "  )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LARS BaseLine B-Size 64 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2bd5fda76227433aa9332e6e48cd415b",
            "fd6480fe59104c7aa15d39bf6ea4a63b",
            "3cb7b5c42e844747ae133750b7d42882",
            "b81cc89707e44dedb51081d13a3ba424",
            "1d74e9350c2c4c61b2e95924ec133012",
            "a5758bfac16a4388a036005a15812d1d",
            "0fdaf43c468043139e5d72397b118371",
            "f63fffe56ff64f1eb2b6e8f14974f011"
          ]
        },
        "id": "czSHzFs7SQug",
        "outputId": "fc513f95-8814-4002-e0d6-2a2eedad7dd7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_005802-t2cjgem5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5' target=\"_blank\">learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1584951/4263216451.py:113: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1578.)\n",
            "  d_p.add_(weight_decay, p.data)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.004822687724667, Training Accuracy: 8.156\n",
            "Validation Loss: 3.6169370329304105, Validation Accuracy: 15.1\n",
            "[2/150]: Training Loss: 3.489779775099986, Training Accuracy: 16.288\n",
            "Validation Loss: 3.195649910884298, Validation Accuracy: 21.71\n",
            "[3/150]: Training Loss: 3.188335987003258, Training Accuracy: 21.346\n",
            "Validation Loss: 2.9737777011409685, Validation Accuracy: 26.15\n",
            "[4/150]: Training Loss: 3.0118914528576006, Training Accuracy: 24.982\n",
            "Validation Loss: 2.7995891707717995, Validation Accuracy: 29.11\n",
            "[5/150]: Training Loss: 2.8688232484071152, Training Accuracy: 28.08\n",
            "Validation Loss: 2.7174989600090464, Validation Accuracy: 30.23\n",
            "[6/150]: Training Loss: 2.7214920926276984, Training Accuracy: 30.662\n",
            "Validation Loss: 2.5305145515757763, Validation Accuracy: 34.82\n",
            "[7/150]: Training Loss: 2.6200312084858983, Training Accuracy: 32.576\n",
            "Validation Loss: 2.474041129373441, Validation Accuracy: 35.97\n",
            "[8/150]: Training Loss: 2.53857664821093, Training Accuracy: 34.358\n",
            "Validation Loss: 2.3864964094890913, Validation Accuracy: 37.73\n",
            "[9/150]: Training Loss: 2.460603771764604, Training Accuracy: 36.126\n",
            "Validation Loss: 2.3855689604570913, Validation Accuracy: 38.2\n",
            "[10/150]: Training Loss: 2.3902963816052507, Training Accuracy: 37.29\n",
            "Validation Loss: 2.301407885399594, Validation Accuracy: 40.45\n",
            "[11/150]: Training Loss: 2.3503475908733087, Training Accuracy: 38.624\n",
            "Validation Loss: 2.3037940666174435, Validation Accuracy: 39.96\n",
            "[12/150]: Training Loss: 2.303130599696313, Training Accuracy: 39.49\n",
            "Validation Loss: 2.2026211046109534, Validation Accuracy: 42.0\n",
            "[13/150]: Training Loss: 2.2486851939459895, Training Accuracy: 40.568\n",
            "Validation Loss: 2.212206517055536, Validation Accuracy: 42.21\n",
            "[14/150]: Training Loss: 2.203081512085312, Training Accuracy: 41.506\n",
            "Validation Loss: 2.1992271386893694, Validation Accuracy: 43.09\n",
            "[15/150]: Training Loss: 2.159660982963679, Training Accuracy: 42.428\n",
            "Validation Loss: 2.1746684677281958, Validation Accuracy: 43.09\n",
            "[16/150]: Training Loss: 2.1303664485511877, Training Accuracy: 43.166\n",
            "Validation Loss: 2.146143364298875, Validation Accuracy: 44.21\n",
            "[17/150]: Training Loss: 2.1123076631589925, Training Accuracy: 43.67\n",
            "Validation Loss: 2.1201481705258605, Validation Accuracy: 44.27\n",
            "[18/150]: Training Loss: 2.0810553551939748, Training Accuracy: 44.61\n",
            "Validation Loss: 2.0423277001472036, Validation Accuracy: 46.15\n",
            "[19/150]: Training Loss: 2.0513661890993338, Training Accuracy: 44.866\n",
            "Validation Loss: 2.091191877225402, Validation Accuracy: 44.81\n",
            "[20/150]: Training Loss: 2.018301057541157, Training Accuracy: 45.826\n",
            "Validation Loss: 2.1546491832490178, Validation Accuracy: 43.92\n",
            "[21/150]: Training Loss: 1.99891439834824, Training Accuracy: 46.082\n",
            "Validation Loss: 2.149173748720983, Validation Accuracy: 44.11\n",
            "[22/150]: Training Loss: 1.9834696277023276, Training Accuracy: 46.51\n",
            "Validation Loss: 2.0435469044241934, Validation Accuracy: 46.15\n",
            "[23/150]: Training Loss: 1.954583641970554, Training Accuracy: 46.984\n",
            "Validation Loss: 2.0593765806999937, Validation Accuracy: 46.4\n",
            "[24/150]: Training Loss: 1.9357828209772134, Training Accuracy: 47.47\n",
            "Validation Loss: 1.9959042148225625, Validation Accuracy: 47.77\n",
            "[25/150]: Training Loss: 1.9152823200311198, Training Accuracy: 48.11\n",
            "Validation Loss: 2.007097185037698, Validation Accuracy: 46.74\n",
            "[26/150]: Training Loss: 1.889640983870572, Training Accuracy: 48.486\n",
            "Validation Loss: 2.0237637371014636, Validation Accuracy: 46.53\n",
            "[27/150]: Training Loss: 1.8898505502954468, Training Accuracy: 48.75\n",
            "Validation Loss: 1.9383376062295998, Validation Accuracy: 48.79\n",
            "[28/150]: Training Loss: 1.8655293333865797, Training Accuracy: 49.182\n",
            "Validation Loss: 2.000895071181522, Validation Accuracy: 47.12\n",
            "[29/150]: Training Loss: 1.843536861564802, Training Accuracy: 49.576\n",
            "Validation Loss: 2.0557514102595627, Validation Accuracy: 46.42\n",
            "[30/150]: Training Loss: 1.8280195388037834, Training Accuracy: 49.938\n",
            "Validation Loss: 1.9708276941518115, Validation Accuracy: 48.17\n",
            "[31/150]: Training Loss: 1.8121484304632982, Training Accuracy: 50.256\n",
            "Validation Loss: 2.0148930929269, Validation Accuracy: 48.07\n",
            "[32/150]: Training Loss: 1.7905213011195287, Training Accuracy: 51.016\n",
            "Validation Loss: 1.9494955805456562, Validation Accuracy: 48.66\n",
            "[33/150]: Training Loss: 1.7819690168513667, Training Accuracy: 51.152\n",
            "Validation Loss: 1.9355155472542829, Validation Accuracy: 49.85\n",
            "[34/150]: Training Loss: 1.7735850943628784, Training Accuracy: 51.054\n",
            "Validation Loss: 1.9679190664534356, Validation Accuracy: 48.93\n",
            "[35/150]: Training Loss: 1.7615117981000934, Training Accuracy: 51.618\n",
            "Validation Loss: 2.019153520559809, Validation Accuracy: 47.96\n",
            "[36/150]: Training Loss: 1.7415476721875809, Training Accuracy: 51.984\n",
            "Validation Loss: 1.9617887530357214, Validation Accuracy: 48.72\n",
            "[37/150]: Training Loss: 1.7273670095007132, Training Accuracy: 52.228\n",
            "Validation Loss: 1.9595575993228111, Validation Accuracy: 49.25\n",
            "[38/150]: Training Loss: 1.7174837630423134, Training Accuracy: 52.45\n",
            "Validation Loss: 1.9162586555359469, Validation Accuracy: 50.15\n",
            "[39/150]: Training Loss: 1.6948718257877222, Training Accuracy: 53.104\n",
            "Validation Loss: 1.9148104600845628, Validation Accuracy: 50.11\n",
            "[40/150]: Training Loss: 1.6948116973537923, Training Accuracy: 53.164\n",
            "Validation Loss: 1.9015048544877653, Validation Accuracy: 50.46\n",
            "[41/150]: Training Loss: 1.669160524292675, Training Accuracy: 53.832\n",
            "Validation Loss: 1.9202428220943282, Validation Accuracy: 49.95\n",
            "[42/150]: Training Loss: 1.6587599679027372, Training Accuracy: 53.99\n",
            "Validation Loss: 1.9989394374713776, Validation Accuracy: 48.53\n",
            "[43/150]: Training Loss: 1.6486307676033596, Training Accuracy: 54.324\n",
            "Validation Loss: 1.9677664428759531, Validation Accuracy: 49.26\n",
            "[44/150]: Training Loss: 1.6284820756034168, Training Accuracy: 54.798\n",
            "Validation Loss: 1.917847274215358, Validation Accuracy: 50.53\n",
            "[45/150]: Training Loss: 1.6195188324774623, Training Accuracy: 54.902\n",
            "Validation Loss: 1.9071525859225327, Validation Accuracy: 50.67\n",
            "[46/150]: Training Loss: 1.6109208017206558, Training Accuracy: 54.96\n",
            "Validation Loss: 1.8967621220145257, Validation Accuracy: 51.23\n",
            "[47/150]: Training Loss: 1.602793920375502, Training Accuracy: 55.188\n",
            "Validation Loss: 1.8559222494720653, Validation Accuracy: 51.43\n",
            "[48/150]: Training Loss: 1.5858894056066528, Training Accuracy: 55.652\n",
            "Validation Loss: 1.9233948349193404, Validation Accuracy: 50.32\n",
            "[49/150]: Training Loss: 1.5650417865694637, Training Accuracy: 56.188\n",
            "Validation Loss: 1.9384330412384811, Validation Accuracy: 49.91\n",
            "[50/150]: Training Loss: 1.5663130182744291, Training Accuracy: 56.228\n",
            "Validation Loss: 1.8889641389725313, Validation Accuracy: 51.35\n",
            "[51/150]: Training Loss: 1.542809939445437, Training Accuracy: 56.846\n",
            "Validation Loss: 1.8398898855136459, Validation Accuracy: 51.84\n",
            "[52/150]: Training Loss: 1.5279571914002108, Training Accuracy: 57.064\n",
            "Validation Loss: 1.9208611204366015, Validation Accuracy: 50.34\n",
            "[53/150]: Training Loss: 1.510901224735143, Training Accuracy: 57.122\n",
            "Validation Loss: 1.9133090471765797, Validation Accuracy: 50.8\n",
            "[54/150]: Training Loss: 1.511483409596831, Training Accuracy: 57.642\n",
            "Validation Loss: 1.8751734176259132, Validation Accuracy: 51.71\n",
            "[55/150]: Training Loss: 1.4843521323960152, Training Accuracy: 58.136\n",
            "Validation Loss: 1.8774340836105832, Validation Accuracy: 51.56\n",
            "[56/150]: Training Loss: 1.4896384542403014, Training Accuracy: 57.742\n",
            "Validation Loss: 1.8484339972210537, Validation Accuracy: 52.7\n",
            "[57/150]: Training Loss: 1.4539345915207778, Training Accuracy: 58.774\n",
            "Validation Loss: 1.8711960125880636, Validation Accuracy: 50.9\n",
            "[58/150]: Training Loss: 1.4413522976591153, Training Accuracy: 59.254\n",
            "Validation Loss: 1.8433482806394055, Validation Accuracy: 52.14\n",
            "[59/150]: Training Loss: 1.435598407407551, Training Accuracy: 59.348\n",
            "Validation Loss: 1.842708926291982, Validation Accuracy: 52.32\n",
            "[60/150]: Training Loss: 1.415708273572995, Training Accuracy: 59.856\n",
            "Validation Loss: 1.8708495941891032, Validation Accuracy: 51.34\n",
            "[61/150]: Training Loss: 1.4131718484489508, Training Accuracy: 59.934\n",
            "Validation Loss: 1.8397565495436359, Validation Accuracy: 52.21\n",
            "[62/150]: Training Loss: 1.3917764421466672, Training Accuracy: 60.466\n",
            "Validation Loss: 1.8553483759521678, Validation Accuracy: 52.56\n",
            "[63/150]: Training Loss: 1.3845181973541485, Training Accuracy: 60.468\n",
            "Validation Loss: 1.8767407699755043, Validation Accuracy: 51.4\n",
            "[64/150]: Training Loss: 1.3718093946156904, Training Accuracy: 60.9\n",
            "Validation Loss: 1.833029270931414, Validation Accuracy: 53.34\n",
            "[65/150]: Training Loss: 1.3485638827771482, Training Accuracy: 61.372\n",
            "Validation Loss: 1.8435825352456159, Validation Accuracy: 52.78\n",
            "[66/150]: Training Loss: 1.3356790865778618, Training Accuracy: 61.914\n",
            "Validation Loss: 1.8920623299422537, Validation Accuracy: 52.03\n",
            "[67/150]: Training Loss: 1.3243823959242047, Training Accuracy: 61.984\n",
            "Validation Loss: 1.8541991725848739, Validation Accuracy: 53.25\n",
            "[68/150]: Training Loss: 1.3088703974128684, Training Accuracy: 62.324\n",
            "Validation Loss: 1.8384279855497323, Validation Accuracy: 52.97\n",
            "[69/150]: Training Loss: 1.2935843141487493, Training Accuracy: 62.726\n",
            "Validation Loss: 1.8648313618010017, Validation Accuracy: 52.35\n",
            "[70/150]: Training Loss: 1.2788333388240747, Training Accuracy: 63.162\n",
            "Validation Loss: 1.8200989673092107, Validation Accuracy: 53.57\n",
            "[71/150]: Training Loss: 1.275286832810058, Training Accuracy: 63.326\n",
            "Validation Loss: 1.8271277934122996, Validation Accuracy: 52.6\n",
            "[72/150]: Training Loss: 1.2548354720063222, Training Accuracy: 63.724\n",
            "Validation Loss: 1.832458599357848, Validation Accuracy: 53.43\n",
            "[73/150]: Training Loss: 1.2409771790589823, Training Accuracy: 64.15\n",
            "Validation Loss: 1.8489187993821066, Validation Accuracy: 53.54\n",
            "[74/150]: Training Loss: 1.2177538118703897, Training Accuracy: 64.682\n",
            "Validation Loss: 1.8377945020699957, Validation Accuracy: 53.86\n",
            "[75/150]: Training Loss: 1.2167601452764039, Training Accuracy: 64.92\n",
            "Validation Loss: 1.856788229031168, Validation Accuracy: 53.15\n",
            "[76/150]: Training Loss: 1.1967681403964987, Training Accuracy: 65.394\n",
            "Validation Loss: 1.8202834759548212, Validation Accuracy: 54.44\n",
            "[77/150]: Training Loss: 1.1781836492021371, Training Accuracy: 65.698\n",
            "Validation Loss: 1.8182067392738002, Validation Accuracy: 54.42\n",
            "[78/150]: Training Loss: 1.1718934528967913, Training Accuracy: 65.94\n",
            "Validation Loss: 1.8361693392893312, Validation Accuracy: 53.34\n",
            "[79/150]: Training Loss: 1.1540917455387847, Training Accuracy: 66.372\n",
            "Validation Loss: 1.8517911365837048, Validation Accuracy: 53.86\n",
            "[80/150]: Training Loss: 1.1260635425215182, Training Accuracy: 67.01\n",
            "Validation Loss: 1.8486420083197819, Validation Accuracy: 53.93\n",
            "[81/150]: Training Loss: 1.1187372058248886, Training Accuracy: 67.426\n",
            "Validation Loss: 1.822678742894701, Validation Accuracy: 54.82\n",
            "[82/150]: Training Loss: 1.1072917601184162, Training Accuracy: 67.646\n",
            "Validation Loss: 1.8640035948935587, Validation Accuracy: 53.57\n",
            "[83/150]: Training Loss: 1.082973983510376, Training Accuracy: 68.116\n",
            "Validation Loss: 1.867675439567323, Validation Accuracy: 53.9\n",
            "[84/150]: Training Loss: 1.0828096682915602, Training Accuracy: 68.366\n",
            "Validation Loss: 1.826197045244229, Validation Accuracy: 54.16\n",
            "[85/150]: Training Loss: 1.0598852286100997, Training Accuracy: 68.856\n",
            "Validation Loss: 1.8198747148938998, Validation Accuracy: 54.4\n",
            "[86/150]: Training Loss: 1.0407975543185572, Training Accuracy: 69.42\n",
            "Validation Loss: 1.8279076829837386, Validation Accuracy: 54.55\n",
            "[87/150]: Training Loss: 1.030418163827618, Training Accuracy: 69.804\n",
            "Validation Loss: 1.854318435784358, Validation Accuracy: 54.88\n",
            "[88/150]: Training Loss: 1.0299987217120807, Training Accuracy: 69.594\n",
            "Validation Loss: 1.8434015595988862, Validation Accuracy: 54.47\n",
            "[89/150]: Training Loss: 1.0026646399741892, Training Accuracy: 70.49\n",
            "Validation Loss: 1.8365007965428055, Validation Accuracy: 54.24\n",
            "[90/150]: Training Loss: 0.9944430979164055, Training Accuracy: 70.764\n",
            "Validation Loss: 1.845389035097353, Validation Accuracy: 54.42\n",
            "[91/150]: Training Loss: 0.9791042178945468, Training Accuracy: 70.958\n",
            "Validation Loss: 1.8592563897940764, Validation Accuracy: 54.71\n",
            "[92/150]: Training Loss: 0.9622275731371491, Training Accuracy: 71.348\n",
            "Validation Loss: 1.8250258181505143, Validation Accuracy: 55.29\n",
            "[93/150]: Training Loss: 0.9501752741349018, Training Accuracy: 71.708\n",
            "Validation Loss: 1.849954966526882, Validation Accuracy: 55.02\n",
            "[94/150]: Training Loss: 0.9297837285358278, Training Accuracy: 72.38\n",
            "Validation Loss: 1.8463909307103248, Validation Accuracy: 55.77\n",
            "[95/150]: Training Loss: 0.9163948694991944, Training Accuracy: 72.898\n",
            "Validation Loss: 1.8669461308011583, Validation Accuracy: 55.06\n",
            "[96/150]: Training Loss: 0.9114100606468938, Training Accuracy: 72.956\n",
            "Validation Loss: 1.8656909784693627, Validation Accuracy: 54.75\n",
            "[97/150]: Training Loss: 0.8974789249165284, Training Accuracy: 73.276\n",
            "Validation Loss: 1.834348332350421, Validation Accuracy: 55.01\n",
            "[98/150]: Training Loss: 0.8797091352360328, Training Accuracy: 73.696\n",
            "Validation Loss: 1.8680614483584264, Validation Accuracy: 55.11\n",
            "[99/150]: Training Loss: 0.8685366097085007, Training Accuracy: 74.026\n",
            "Validation Loss: 1.8696094318559975, Validation Accuracy: 55.43\n",
            "[100/150]: Training Loss: 0.8596895751745804, Training Accuracy: 74.396\n",
            "Validation Loss: 1.8580050965782944, Validation Accuracy: 55.31\n",
            "[101/150]: Training Loss: 0.8450928368531835, Training Accuracy: 74.934\n",
            "Validation Loss: 1.8560257132645626, Validation Accuracy: 55.71\n",
            "[102/150]: Training Loss: 0.834670692567935, Training Accuracy: 75.278\n",
            "Validation Loss: 1.8615986776959366, Validation Accuracy: 55.84\n",
            "[103/150]: Training Loss: 0.8168128573757303, Training Accuracy: 75.56\n",
            "Validation Loss: 1.8920900590100866, Validation Accuracy: 55.38\n",
            "[104/150]: Training Loss: 0.8022570627577165, Training Accuracy: 76.022\n",
            "Validation Loss: 1.9013077168707635, Validation Accuracy: 55.71\n",
            "[105/150]: Training Loss: 0.7898137537414766, Training Accuracy: 76.588\n",
            "Validation Loss: 1.8971044345266501, Validation Accuracy: 55.23\n",
            "[106/150]: Training Loss: 0.7799203321528252, Training Accuracy: 76.854\n",
            "Validation Loss: 1.8587115873956377, Validation Accuracy: 55.33\n",
            "[107/150]: Training Loss: 0.7687876791600377, Training Accuracy: 76.832\n",
            "Validation Loss: 1.9066706402286602, Validation Accuracy: 55.7\n",
            "[108/150]: Training Loss: 0.7502601898234823, Training Accuracy: 77.674\n",
            "Validation Loss: 1.891866291784177, Validation Accuracy: 56.06\n",
            "[109/150]: Training Loss: 0.7453718431236799, Training Accuracy: 77.84\n",
            "Validation Loss: 1.9089836011267012, Validation Accuracy: 55.88\n",
            "[110/150]: Training Loss: 0.7324799866322667, Training Accuracy: 78.2\n",
            "Validation Loss: 1.8889893972949616, Validation Accuracy: 55.7\n",
            "[111/150]: Training Loss: 0.716928729620736, Training Accuracy: 78.53\n",
            "Validation Loss: 1.9158697025791096, Validation Accuracy: 56.07\n",
            "[112/150]: Training Loss: 0.7084723916809882, Training Accuracy: 78.768\n",
            "Validation Loss: 1.9187719480247254, Validation Accuracy: 55.71\n",
            "[113/150]: Training Loss: 0.6928996008146754, Training Accuracy: 79.508\n",
            "Validation Loss: 1.9122050970223299, Validation Accuracy: 56.4\n",
            "[114/150]: Training Loss: 0.686598046775669, Training Accuracy: 79.428\n",
            "Validation Loss: 1.895180571990408, Validation Accuracy: 56.18\n",
            "[115/150]: Training Loss: 0.6837174600881079, Training Accuracy: 79.66\n",
            "Validation Loss: 1.9321321894408792, Validation Accuracy: 55.4\n",
            "[116/150]: Training Loss: 0.6646802525233735, Training Accuracy: 80.106\n",
            "Validation Loss: 1.9081798047776435, Validation Accuracy: 56.21\n",
            "[117/150]: Training Loss: 0.6522155370172638, Training Accuracy: 80.528\n",
            "Validation Loss: 1.9046855428416258, Validation Accuracy: 56.69\n",
            "[118/150]: Training Loss: 0.6456939719064766, Training Accuracy: 80.752\n",
            "Validation Loss: 1.9132253895899294, Validation Accuracy: 56.47\n",
            "[119/150]: Training Loss: 0.6339363064378729, Training Accuracy: 81.286\n",
            "Validation Loss: 1.914469665782467, Validation Accuracy: 56.51\n",
            "[120/150]: Training Loss: 0.6339401570351227, Training Accuracy: 81.134\n",
            "Validation Loss: 1.913045568830648, Validation Accuracy: 55.93\n",
            "[121/150]: Training Loss: 0.6202226441610804, Training Accuracy: 81.422\n",
            "Validation Loss: 1.9112963069016766, Validation Accuracy: 56.58\n",
            "[122/150]: Training Loss: 0.6146376765216403, Training Accuracy: 81.858\n",
            "Validation Loss: 1.916867652516456, Validation Accuracy: 56.28\n",
            "[123/150]: Training Loss: 0.6090771163363591, Training Accuracy: 81.96\n",
            "Validation Loss: 1.9238637059357515, Validation Accuracy: 56.33\n",
            "[124/150]: Training Loss: 0.5977434807497523, Training Accuracy: 82.352\n",
            "Validation Loss: 1.9187599549627607, Validation Accuracy: 56.09\n",
            "[125/150]: Training Loss: 0.5865007763933343, Training Accuracy: 82.728\n",
            "Validation Loss: 1.9290249192031326, Validation Accuracy: 55.85\n",
            "[126/150]: Training Loss: 0.5832711922390686, Training Accuracy: 82.85\n",
            "Validation Loss: 1.925058329560954, Validation Accuracy: 56.54\n",
            "[127/150]: Training Loss: 0.5710540865845692, Training Accuracy: 83.058\n",
            "Validation Loss: 1.9433572846613112, Validation Accuracy: 56.46\n",
            "[128/150]: Training Loss: 0.566251437987208, Training Accuracy: 83.328\n",
            "Validation Loss: 1.9474792632327718, Validation Accuracy: 56.24\n",
            "[129/150]: Training Loss: 0.5592512233787791, Training Accuracy: 83.606\n",
            "Validation Loss: 1.9336790825910628, Validation Accuracy: 56.49\n",
            "[130/150]: Training Loss: 0.5560948127675849, Training Accuracy: 83.726\n",
            "Validation Loss: 1.9358682472994373, Validation Accuracy: 56.75\n",
            "[131/150]: Training Loss: 0.5547005703191623, Training Accuracy: 83.68\n",
            "Validation Loss: 1.941912688647106, Validation Accuracy: 56.32\n",
            "[132/150]: Training Loss: 0.5438491536299591, Training Accuracy: 84.188\n",
            "Validation Loss: 1.9333096700868788, Validation Accuracy: 56.52\n",
            "[133/150]: Training Loss: 0.5359534242421465, Training Accuracy: 84.398\n",
            "Validation Loss: 1.9515662831106004, Validation Accuracy: 56.36\n",
            "[134/150]: Training Loss: 0.5345852662763937, Training Accuracy: 84.37\n",
            "Validation Loss: 1.939547040280263, Validation Accuracy: 56.31\n",
            "[135/150]: Training Loss: 0.5338244077266024, Training Accuracy: 84.354\n",
            "Validation Loss: 1.9529236646214867, Validation Accuracy: 56.4\n",
            "[136/150]: Training Loss: 0.5233329969751256, Training Accuracy: 84.792\n",
            "Validation Loss: 1.9439837556735726, Validation Accuracy: 56.43\n",
            "[137/150]: Training Loss: 0.519332280549247, Training Accuracy: 84.868\n",
            "Validation Loss: 1.9427648835880742, Validation Accuracy: 56.25\n",
            "[138/150]: Training Loss: 0.513342816468395, Training Accuracy: 85.202\n",
            "Validation Loss: 1.9522086123751987, Validation Accuracy: 56.69\n",
            "[139/150]: Training Loss: 0.5097889236515135, Training Accuracy: 85.132\n",
            "Validation Loss: 1.950890618904381, Validation Accuracy: 56.41\n",
            "[140/150]: Training Loss: 0.5080071812319329, Training Accuracy: 85.386\n",
            "Validation Loss: 1.9511753282729227, Validation Accuracy: 56.69\n",
            "[141/150]: Training Loss: 0.5083310039299528, Training Accuracy: 85.398\n",
            "Validation Loss: 1.9493300193434309, Validation Accuracy: 56.5\n",
            "[142/150]: Training Loss: 0.5110094372726157, Training Accuracy: 85.232\n",
            "Validation Loss: 1.9510933920076698, Validation Accuracy: 56.46\n",
            "[143/150]: Training Loss: 0.5043484553161179, Training Accuracy: 85.402\n",
            "Validation Loss: 1.9473720959797027, Validation Accuracy: 56.53\n",
            "[144/150]: Training Loss: 0.5001817758926346, Training Accuracy: 85.59\n",
            "Validation Loss: 1.9499852835752403, Validation Accuracy: 56.64\n",
            "[145/150]: Training Loss: 0.5050505888088584, Training Accuracy: 85.566\n",
            "Validation Loss: 1.9509570135432444, Validation Accuracy: 56.58\n",
            "[146/150]: Training Loss: 0.4944563165230824, Training Accuracy: 85.7\n",
            "Validation Loss: 1.9510046653686814, Validation Accuracy: 56.56\n",
            "[147/150]: Training Loss: 0.4947198845655717, Training Accuracy: 85.914\n",
            "Validation Loss: 1.9500796012817674, Validation Accuracy: 56.62\n",
            "[148/150]: Training Loss: 0.4965038236487857, Training Accuracy: 85.548\n",
            "Validation Loss: 1.9505202922092122, Validation Accuracy: 56.65\n",
            "[149/150]: Training Loss: 0.4984786443964905, Training Accuracy: 85.54\n",
            "Validation Loss: 1.94986033325742, Validation Accuracy: 56.62\n",
            "[150/150]: Training Loss: 0.498654707256333, Training Accuracy: 85.464\n",
            "Validation Loss: 1.9498052581860001, Validation Accuracy: 56.63\n",
            "**********************************************************************\n",
            "Test Loss: 1.9498052581860001, Test Accuracy: 56.63\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▆█▃▄▃▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Test Loss</td><td>▁▃▆▅██▇█▆▇▆▅▆▇█▇▆▇▆▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▆▆▇▆▆▆</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>56.63</td></tr><tr><td>Test Loss</td><td>1.94981</td></tr><tr><td>Train Accuracy</td><td>85.464</td></tr><tr><td>Train Loss</td><td>0.49865</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_005802-t2cjgem5/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 1.5\n",
        "wd = 1e-03\n",
        "\n",
        "hyperparameters = {'learning_rate': lr,\n",
        "                    'weight_decay' : wd\n",
        "                  }\n",
        "\n",
        "# Load the model\n",
        "model = LeNet5().to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer = LARS(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# Training\n",
        "run_training(\n",
        "    num_epochs,\n",
        "    model,\n",
        "    original_train_loader,\n",
        "    original_test_loader,\n",
        "    original_test_loader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    criterion,\n",
        "    device,\n",
        "    optimizer_name='LARS',\n",
        "    hyperparameters=hyperparameters,\n",
        "    is_wandb = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LARS Test Large Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "81408f178c46447f90d36d1d18cdad82",
            "06d3de2006b14793bd57a5ca89d44e4a",
            "804c12a3f13840c7bdb2e6df69e62c10",
            "4a4cfb14c56e477cbfef5a652c05fabc",
            "68d86018628f420e8d7e516ba5827e35",
            "8877fd11846246f989e31835e5d3e7ae",
            "68ff4108835c462b929d0b5c78497555",
            "a63e1d987556448280ca217f17b60b49"
          ]
        },
        "id": "RFWHh4OL50WX",
        "outputId": "099a0039-9168-4d4e-84ec-4d6300dd3498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 512, Learning rate: 4.242640687119286, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:9hoyxk41) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=512 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/9hoyxk41' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/9hoyxk41</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_012745-9hoyxk41/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:9hoyxk41). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_012749-momu4e5u</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u' target=\"_blank\">batch_size=512 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.194531238808924, Training Accuracy: 5.802\n",
            "Validation Loss: 3.8219263911247254, Validation Accuracy: 11.06\n",
            "[2/150]: Training Loss: 3.737521487839368, Training Accuracy: 12.162\n",
            "Validation Loss: 3.5326468467712404, Validation Accuracy: 15.5\n",
            "[3/150]: Training Loss: 3.469568571265863, Training Accuracy: 16.654\n",
            "Validation Loss: 3.2325081706047056, Validation Accuracy: 21.14\n",
            "[4/150]: Training Loss: 3.2623822543085836, Training Accuracy: 20.338\n",
            "Validation Loss: 3.041664385795593, Validation Accuracy: 24.34\n",
            "[5/150]: Training Loss: 3.1202199824002324, Training Accuracy: 23.134\n",
            "Validation Loss: 2.9670259952545166, Validation Accuracy: 26.23\n",
            "[6/150]: Training Loss: 2.973516508024566, Training Accuracy: 25.864\n",
            "Validation Loss: 2.835892844200134, Validation Accuracy: 28.97\n",
            "[7/150]: Training Loss: 2.8618772808386357, Training Accuracy: 27.73\n",
            "Validation Loss: 2.731530475616455, Validation Accuracy: 31.08\n",
            "[8/150]: Training Loss: 2.7622045083921782, Training Accuracy: 29.856\n",
            "Validation Loss: 2.6019123077392576, Validation Accuracy: 33.01\n",
            "[9/150]: Training Loss: 2.719049726213728, Training Accuracy: 30.804\n",
            "Validation Loss: 2.6247976064682006, Validation Accuracy: 33.43\n",
            "[10/150]: Training Loss: 2.6084506000791277, Training Accuracy: 33.166\n",
            "Validation Loss: 2.4777934432029722, Validation Accuracy: 36.28\n",
            "[11/150]: Training Loss: 2.5361861233808556, Training Accuracy: 34.45\n",
            "Validation Loss: 2.3982665181159972, Validation Accuracy: 37.66\n",
            "[12/150]: Training Loss: 2.486152622164512, Training Accuracy: 35.61\n",
            "Validation Loss: 2.4073413372039796, Validation Accuracy: 37.69\n",
            "[13/150]: Training Loss: 2.419785971544227, Training Accuracy: 36.994\n",
            "Validation Loss: 2.3586077094078064, Validation Accuracy: 39.62\n",
            "[14/150]: Training Loss: 2.378681479668131, Training Accuracy: 37.686\n",
            "Validation Loss: 2.2874060750007628, Validation Accuracy: 40.44\n",
            "[15/150]: Training Loss: 2.3229291341742693, Training Accuracy: 39.04\n",
            "Validation Loss: 2.391434836387634, Validation Accuracy: 38.28\n",
            "[16/150]: Training Loss: 2.3006048445798912, Training Accuracy: 39.566\n",
            "Validation Loss: 2.233357620239258, Validation Accuracy: 40.92\n",
            "[17/150]: Training Loss: 2.270021514016755, Training Accuracy: 40.304\n",
            "Validation Loss: 2.2809773087501526, Validation Accuracy: 40.35\n",
            "[18/150]: Training Loss: 2.219994238444737, Training Accuracy: 41.128\n",
            "Validation Loss: 2.208327281475067, Validation Accuracy: 42.26\n",
            "[19/150]: Training Loss: 2.1639412665853697, Training Accuracy: 42.49\n",
            "Validation Loss: 2.1477415204048156, Validation Accuracy: 43.47\n",
            "[20/150]: Training Loss: 2.1379866052647025, Training Accuracy: 43.012\n",
            "Validation Loss: 2.202856254577637, Validation Accuracy: 42.27\n",
            "[21/150]: Training Loss: 2.139409989726787, Training Accuracy: 43.086\n",
            "Validation Loss: 2.1385614931583405, Validation Accuracy: 43.93\n",
            "[22/150]: Training Loss: 2.07097029564332, Training Accuracy: 44.974\n",
            "Validation Loss: 2.128925609588623, Validation Accuracy: 44.07\n",
            "[23/150]: Training Loss: 2.0605698106240253, Training Accuracy: 44.808\n",
            "Validation Loss: 2.15288764834404, Validation Accuracy: 44.03\n",
            "[24/150]: Training Loss: 2.0457090200210106, Training Accuracy: 45.174\n",
            "Validation Loss: 2.1201067209243774, Validation Accuracy: 44.63\n",
            "[25/150]: Training Loss: 2.0101604510326774, Training Accuracy: 45.978\n",
            "Validation Loss: 2.0930452048778534, Validation Accuracy: 45.37\n",
            "[26/150]: Training Loss: 1.9699756795046282, Training Accuracy: 46.9\n",
            "Validation Loss: 2.089737904071808, Validation Accuracy: 45.61\n",
            "[27/150]: Training Loss: 1.967079225851565, Training Accuracy: 47.01\n",
            "Validation Loss: 2.1089664578437803, Validation Accuracy: 44.55\n",
            "[28/150]: Training Loss: 1.9404766535272404, Training Accuracy: 47.568\n",
            "Validation Loss: 2.0624644994735717, Validation Accuracy: 45.57\n",
            "[29/150]: Training Loss: 1.928514483023663, Training Accuracy: 47.712\n",
            "Validation Loss: 2.0849966049194335, Validation Accuracy: 45.11\n",
            "[30/150]: Training Loss: 1.9005685375661265, Training Accuracy: 48.44\n",
            "Validation Loss: 2.015596163272858, Validation Accuracy: 46.79\n",
            "[31/150]: Training Loss: 1.868617719533492, Training Accuracy: 49.064\n",
            "Validation Loss: 2.044619733095169, Validation Accuracy: 45.67\n",
            "[32/150]: Training Loss: 1.8784857465296376, Training Accuracy: 48.794\n",
            "Validation Loss: 2.029283958673477, Validation Accuracy: 46.27\n",
            "[33/150]: Training Loss: 1.8382798968529215, Training Accuracy: 49.868\n",
            "Validation Loss: 1.9727658331394196, Validation Accuracy: 48.14\n",
            "[34/150]: Training Loss: 1.8194199094966965, Training Accuracy: 50.486\n",
            "Validation Loss: 1.9934200942516327, Validation Accuracy: 47.5\n",
            "[35/150]: Training Loss: 1.7988280094399745, Training Accuracy: 50.79\n",
            "Validation Loss: 2.0087626039981843, Validation Accuracy: 46.76\n",
            "[36/150]: Training Loss: 1.8009161900500863, Training Accuracy: 50.804\n",
            "Validation Loss: 1.9823269903659821, Validation Accuracy: 48.14\n",
            "[37/150]: Training Loss: 1.767955198579905, Training Accuracy: 51.498\n",
            "Validation Loss: 1.973670369386673, Validation Accuracy: 48.18\n",
            "[38/150]: Training Loss: 1.761318182458683, Training Accuracy: 51.526\n",
            "Validation Loss: 1.9882086098194123, Validation Accuracy: 47.85\n",
            "[39/150]: Training Loss: 1.7494291188765545, Training Accuracy: 52.066\n",
            "Validation Loss: 1.945462554693222, Validation Accuracy: 48.63\n",
            "[40/150]: Training Loss: 1.7158451688532927, Training Accuracy: 52.788\n",
            "Validation Loss: 1.9138611912727357, Validation Accuracy: 49.24\n",
            "[41/150]: Training Loss: 1.7024361783144426, Training Accuracy: 52.972\n",
            "Validation Loss: 1.9541066110134124, Validation Accuracy: 49.38\n",
            "[42/150]: Training Loss: 1.6934348527266054, Training Accuracy: 53.184\n",
            "Validation Loss: 1.9292299151420593, Validation Accuracy: 49.48\n",
            "[43/150]: Training Loss: 1.6734726489806662, Training Accuracy: 53.69\n",
            "Validation Loss: 1.9281952559947968, Validation Accuracy: 49.57\n",
            "[44/150]: Training Loss: 1.6466177933070125, Training Accuracy: 54.438\n",
            "Validation Loss: 1.92038534283638, Validation Accuracy: 49.7\n",
            "[45/150]: Training Loss: 1.6545339092916371, Training Accuracy: 54.198\n",
            "Validation Loss: 1.9556318461894988, Validation Accuracy: 49.26\n",
            "[46/150]: Training Loss: 1.619915745696243, Training Accuracy: 54.986\n",
            "Validation Loss: 1.899458384513855, Validation Accuracy: 50.32\n",
            "[47/150]: Training Loss: 1.6211930987786274, Training Accuracy: 54.738\n",
            "Validation Loss: 1.9299038767814636, Validation Accuracy: 49.14\n",
            "[48/150]: Training Loss: 1.5995109361045214, Training Accuracy: 55.258\n",
            "Validation Loss: 1.9302596151828766, Validation Accuracy: 49.32\n",
            "[49/150]: Training Loss: 1.5750943154704815, Training Accuracy: 55.898\n",
            "Validation Loss: 1.8993667602539062, Validation Accuracy: 49.91\n",
            "[50/150]: Training Loss: 1.5744633698950008, Training Accuracy: 55.76\n",
            "Validation Loss: 1.9295048713684082, Validation Accuracy: 50.3\n",
            "[51/150]: Training Loss: 1.5583884862004493, Training Accuracy: 56.348\n",
            "Validation Loss: 1.925916600227356, Validation Accuracy: 50.14\n",
            "[52/150]: Training Loss: 1.5439508910081825, Training Accuracy: 56.592\n",
            "Validation Loss: 1.8903591930866241, Validation Accuracy: 50.65\n",
            "[53/150]: Training Loss: 1.5187961057740815, Training Accuracy: 57.12\n",
            "Validation Loss: 1.893898105621338, Validation Accuracy: 51.28\n",
            "[54/150]: Training Loss: 1.500082329827912, Training Accuracy: 57.754\n",
            "Validation Loss: 1.8901280999183654, Validation Accuracy: 50.42\n",
            "[55/150]: Training Loss: 1.5054621915428006, Training Accuracy: 57.632\n",
            "Validation Loss: 1.890578955411911, Validation Accuracy: 51.07\n",
            "[56/150]: Training Loss: 1.4812841756003243, Training Accuracy: 57.986\n",
            "Validation Loss: 1.9068008363246918, Validation Accuracy: 51.44\n",
            "[57/150]: Training Loss: 1.4542514341218131, Training Accuracy: 58.654\n",
            "Validation Loss: 1.8665942788124084, Validation Accuracy: 51.26\n",
            "[58/150]: Training Loss: 1.4304890097404013, Training Accuracy: 59.206\n",
            "Validation Loss: 1.872557681798935, Validation Accuracy: 51.35\n",
            "[59/150]: Training Loss: 1.4232833567930727, Training Accuracy: 59.584\n",
            "Validation Loss: 1.9096780002117157, Validation Accuracy: 50.55\n",
            "[60/150]: Training Loss: 1.4309481491847915, Training Accuracy: 59.184\n",
            "Validation Loss: 1.8791188061237336, Validation Accuracy: 51.2\n",
            "[61/150]: Training Loss: 1.40531452334657, Training Accuracy: 59.96\n",
            "Validation Loss: 1.8862035810947417, Validation Accuracy: 51.85\n",
            "[62/150]: Training Loss: 1.3938273799662688, Training Accuracy: 60.274\n",
            "Validation Loss: 1.8729142725467682, Validation Accuracy: 51.37\n",
            "[63/150]: Training Loss: 1.3863425692733453, Training Accuracy: 60.462\n",
            "Validation Loss: 1.8725131154060364, Validation Accuracy: 52.01\n",
            "[64/150]: Training Loss: 1.366844469187211, Training Accuracy: 60.966\n",
            "Validation Loss: 1.8498412251472474, Validation Accuracy: 51.87\n",
            "[65/150]: Training Loss: 1.3531476259231567, Training Accuracy: 61.048\n",
            "Validation Loss: 1.8571744859218597, Validation Accuracy: 52.36\n",
            "[66/150]: Training Loss: 1.3398733954040372, Training Accuracy: 61.472\n",
            "Validation Loss: 1.869799542427063, Validation Accuracy: 52.02\n",
            "[67/150]: Training Loss: 1.3146201080205488, Training Accuracy: 62.184\n",
            "Validation Loss: 1.85557941198349, Validation Accuracy: 52.16\n",
            "[68/150]: Training Loss: 1.3349930096645743, Training Accuracy: 61.624\n",
            "Validation Loss: 1.899474561214447, Validation Accuracy: 51.8\n",
            "[69/150]: Training Loss: 1.2890492300597989, Training Accuracy: 63.07\n",
            "Validation Loss: 1.8615913569927216, Validation Accuracy: 52.55\n",
            "[70/150]: Training Loss: 1.281143541238746, Training Accuracy: 63.172\n",
            "Validation Loss: 1.8803191304206848, Validation Accuracy: 51.97\n",
            "[71/150]: Training Loss: 1.2725608847579177, Training Accuracy: 63.378\n",
            "Validation Loss: 1.859445983171463, Validation Accuracy: 52.42\n",
            "[72/150]: Training Loss: 1.2559378949963316, Training Accuracy: 63.878\n",
            "Validation Loss: 1.8568916201591492, Validation Accuracy: 52.05\n",
            "[73/150]: Training Loss: 1.236931935865052, Training Accuracy: 64.244\n",
            "Validation Loss: 1.8739505887031556, Validation Accuracy: 52.77\n",
            "[74/150]: Training Loss: 1.2173650824293798, Training Accuracy: 64.762\n",
            "Validation Loss: 1.8348273098468781, Validation Accuracy: 53.4\n",
            "[75/150]: Training Loss: 1.2067504488691991, Training Accuracy: 65.066\n",
            "Validation Loss: 1.8692607581615448, Validation Accuracy: 53.1\n",
            "[76/150]: Training Loss: 1.1903777292796545, Training Accuracy: 65.334\n",
            "Validation Loss: 1.8275393545627594, Validation Accuracy: 54.25\n",
            "[77/150]: Training Loss: 1.1792806843105628, Training Accuracy: 65.694\n",
            "Validation Loss: 1.8515967845916748, Validation Accuracy: 53.52\n",
            "[78/150]: Training Loss: 1.1699374263383904, Training Accuracy: 65.942\n",
            "Validation Loss: 1.8811356365680694, Validation Accuracy: 53.09\n",
            "[79/150]: Training Loss: 1.1490371531369734, Training Accuracy: 66.792\n",
            "Validation Loss: 1.8353333652019501, Validation Accuracy: 53.82\n",
            "[80/150]: Training Loss: 1.13668700383634, Training Accuracy: 66.562\n",
            "Validation Loss: 1.8432445168495177, Validation Accuracy: 53.57\n",
            "[81/150]: Training Loss: 1.1307378818794174, Training Accuracy: 67.21\n",
            "Validation Loss: 1.8481176435947417, Validation Accuracy: 53.14\n",
            "[82/150]: Training Loss: 1.113474543605532, Training Accuracy: 67.312\n",
            "Validation Loss: 1.8410939693450927, Validation Accuracy: 53.83\n",
            "[83/150]: Training Loss: 1.087531100122296, Training Accuracy: 68.23\n",
            "Validation Loss: 1.8605490565299987, Validation Accuracy: 54.45\n",
            "[84/150]: Training Loss: 1.0892518618885352, Training Accuracy: 68.016\n",
            "Validation Loss: 1.8244962751865388, Validation Accuracy: 54.28\n",
            "[85/150]: Training Loss: 1.0597951071602958, Training Accuracy: 68.916\n",
            "Validation Loss: 1.848558533191681, Validation Accuracy: 54.32\n",
            "[86/150]: Training Loss: 1.0610668713949165, Training Accuracy: 69.08\n",
            "Validation Loss: 1.834687203168869, Validation Accuracy: 54.36\n",
            "[87/150]: Training Loss: 1.029247879373784, Training Accuracy: 69.662\n",
            "Validation Loss: 1.8577094554901123, Validation Accuracy: 54.1\n",
            "[88/150]: Training Loss: 1.0305843438420976, Training Accuracy: 69.63\n",
            "Validation Loss: 1.88287433385849, Validation Accuracy: 53.74\n",
            "[89/150]: Training Loss: 1.0086117690923262, Training Accuracy: 70.346\n",
            "Validation Loss: 1.8663456857204437, Validation Accuracy: 53.54\n",
            "[90/150]: Training Loss: 0.9981355539390019, Training Accuracy: 70.802\n",
            "Validation Loss: 1.8623527228832244, Validation Accuracy: 53.93\n",
            "[91/150]: Training Loss: 0.986907957159743, Training Accuracy: 70.956\n",
            "Validation Loss: 1.8775681614875794, Validation Accuracy: 54.56\n",
            "[92/150]: Training Loss: 0.9677880217834395, Training Accuracy: 71.402\n",
            "Validation Loss: 1.886433583498001, Validation Accuracy: 54.06\n",
            "[93/150]: Training Loss: 0.9601449692735866, Training Accuracy: 71.714\n",
            "Validation Loss: 1.848057508468628, Validation Accuracy: 55.12\n",
            "[94/150]: Training Loss: 0.9483164567120221, Training Accuracy: 71.99\n",
            "Validation Loss: 1.8797329545021058, Validation Accuracy: 54.52\n",
            "[95/150]: Training Loss: 0.9349474110165421, Training Accuracy: 72.398\n",
            "Validation Loss: 1.8802269518375396, Validation Accuracy: 54.13\n",
            "[96/150]: Training Loss: 0.9153923319310558, Training Accuracy: 72.582\n",
            "Validation Loss: 1.8897387385368347, Validation Accuracy: 54.28\n",
            "[97/150]: Training Loss: 0.9160392132340646, Training Accuracy: 72.794\n",
            "Validation Loss: 1.9283715963363648, Validation Accuracy: 53.5\n",
            "[98/150]: Training Loss: 0.9007182382807439, Training Accuracy: 73.294\n",
            "Validation Loss: 1.8609421133995057, Validation Accuracy: 54.96\n",
            "[99/150]: Training Loss: 0.8783578246223683, Training Accuracy: 73.912\n",
            "Validation Loss: 1.8637104988098145, Validation Accuracy: 54.56\n",
            "[100/150]: Training Loss: 0.8765367439814976, Training Accuracy: 74.022\n",
            "Validation Loss: 1.8750475943088531, Validation Accuracy: 54.59\n",
            "[101/150]: Training Loss: 0.8546222393610039, Training Accuracy: 74.918\n",
            "Validation Loss: 1.8807278335094453, Validation Accuracy: 55.13\n",
            "[102/150]: Training Loss: 0.8389398102857628, Training Accuracy: 74.972\n",
            "Validation Loss: 1.8944664776325226, Validation Accuracy: 54.78\n",
            "[103/150]: Training Loss: 0.8364474201688961, Training Accuracy: 75.32\n",
            "Validation Loss: 1.8999429523944855, Validation Accuracy: 54.68\n",
            "[104/150]: Training Loss: 0.8233104719191181, Training Accuracy: 75.606\n",
            "Validation Loss: 1.9150757372379303, Validation Accuracy: 54.49\n",
            "[105/150]: Training Loss: 0.8071710479502775, Training Accuracy: 76.094\n",
            "Validation Loss: 1.8923523843288421, Validation Accuracy: 55.54\n",
            "[106/150]: Training Loss: 0.7909371567015745, Training Accuracy: 76.448\n",
            "Validation Loss: 1.883741980791092, Validation Accuracy: 54.92\n",
            "[107/150]: Training Loss: 0.7867257595062256, Training Accuracy: 76.748\n",
            "Validation Loss: 1.8950099110603333, Validation Accuracy: 55.47\n",
            "[108/150]: Training Loss: 0.7650761269793218, Training Accuracy: 77.31\n",
            "Validation Loss: 1.9024061024188996, Validation Accuracy: 54.91\n",
            "[109/150]: Training Loss: 0.7652728691393015, Training Accuracy: 77.178\n",
            "Validation Loss: 1.9312968671321868, Validation Accuracy: 54.57\n",
            "[110/150]: Training Loss: 0.7572217492424712, Training Accuracy: 77.548\n",
            "Validation Loss: 1.8906243860721588, Validation Accuracy: 55.71\n",
            "[111/150]: Training Loss: 0.738684381149253, Training Accuracy: 77.932\n",
            "Validation Loss: 1.945603609085083, Validation Accuracy: 55.18\n",
            "[112/150]: Training Loss: 0.7319968044757843, Training Accuracy: 78.382\n",
            "Validation Loss: 1.9286673545837403, Validation Accuracy: 55.27\n",
            "[113/150]: Training Loss: 0.7233879006638819, Training Accuracy: 78.482\n",
            "Validation Loss: 1.9232488691806793, Validation Accuracy: 55.58\n",
            "[114/150]: Training Loss: 0.7083694521261721, Training Accuracy: 78.814\n",
            "Validation Loss: 1.907062864303589, Validation Accuracy: 55.51\n",
            "[115/150]: Training Loss: 0.7031112666032753, Training Accuracy: 79.204\n",
            "Validation Loss: 1.9117585182189942, Validation Accuracy: 56.02\n",
            "[116/150]: Training Loss: 0.6852655745282465, Training Accuracy: 79.458\n",
            "Validation Loss: 1.9387612223625184, Validation Accuracy: 55.18\n",
            "[117/150]: Training Loss: 0.6826438423322172, Training Accuracy: 79.792\n",
            "Validation Loss: 1.923887985944748, Validation Accuracy: 55.49\n",
            "[118/150]: Training Loss: 0.6681522337757811, Training Accuracy: 80.242\n",
            "Validation Loss: 1.940861666202545, Validation Accuracy: 55.59\n",
            "[119/150]: Training Loss: 0.6691557758924912, Training Accuracy: 80.146\n",
            "Validation Loss: 1.953080016374588, Validation Accuracy: 55.29\n",
            "[120/150]: Training Loss: 0.6531465272514188, Training Accuracy: 80.714\n",
            "Validation Loss: 1.9404853343963624, Validation Accuracy: 55.54\n",
            "[121/150]: Training Loss: 0.6419856098233437, Training Accuracy: 81.21\n",
            "Validation Loss: 1.9526274442672729, Validation Accuracy: 56.17\n",
            "[122/150]: Training Loss: 0.6383003540793244, Training Accuracy: 81.27\n",
            "Validation Loss: 1.9685742020606996, Validation Accuracy: 55.67\n",
            "[123/150]: Training Loss: 0.6263646182357049, Training Accuracy: 81.396\n",
            "Validation Loss: 1.9449464201927185, Validation Accuracy: 56.05\n",
            "[124/150]: Training Loss: 0.62737323982375, Training Accuracy: 81.438\n",
            "Validation Loss: 1.9453241765499114, Validation Accuracy: 56.28\n",
            "[125/150]: Training Loss: 0.6171576502371807, Training Accuracy: 81.866\n",
            "Validation Loss: 1.9553956925868987, Validation Accuracy: 55.63\n",
            "[126/150]: Training Loss: 0.6041063827519514, Training Accuracy: 82.16\n",
            "Validation Loss: 1.9582968175411224, Validation Accuracy: 56.15\n",
            "[127/150]: Training Loss: 0.6006453934372687, Training Accuracy: 82.38\n",
            "Validation Loss: 1.9529106080532075, Validation Accuracy: 56.03\n",
            "[128/150]: Training Loss: 0.6008156434613832, Training Accuracy: 82.348\n",
            "Validation Loss: 1.9448323190212249, Validation Accuracy: 56.3\n",
            "[129/150]: Training Loss: 0.5887871582289131, Training Accuracy: 82.762\n",
            "Validation Loss: 1.9513534665107728, Validation Accuracy: 56.25\n",
            "[130/150]: Training Loss: 0.5888028677020755, Training Accuracy: 82.86\n",
            "Validation Loss: 1.9529175937175751, Validation Accuracy: 56.18\n",
            "[131/150]: Training Loss: 0.5761450562550097, Training Accuracy: 83.106\n",
            "Validation Loss: 1.9645096361637115, Validation Accuracy: 56.02\n",
            "[132/150]: Training Loss: 0.5727416809116092, Training Accuracy: 83.094\n",
            "Validation Loss: 1.963749361038208, Validation Accuracy: 56.38\n",
            "[133/150]: Training Loss: 0.5652356436666177, Training Accuracy: 83.672\n",
            "Validation Loss: 1.9716902256011963, Validation Accuracy: 56.28\n",
            "[134/150]: Training Loss: 0.5695076165150623, Training Accuracy: 83.448\n",
            "Validation Loss: 1.964329320192337, Validation Accuracy: 55.99\n",
            "[135/150]: Training Loss: 0.5608592775403237, Training Accuracy: 83.678\n",
            "Validation Loss: 1.9603359639644622, Validation Accuracy: 56.25\n",
            "[136/150]: Training Loss: 0.5542723040799705, Training Accuracy: 83.854\n",
            "Validation Loss: 1.9673468470573425, Validation Accuracy: 56.04\n",
            "[137/150]: Training Loss: 0.5495593383604166, Training Accuracy: 84.128\n",
            "Validation Loss: 1.9731853008270264, Validation Accuracy: 56.19\n",
            "[138/150]: Training Loss: 0.5444910410715609, Training Accuracy: 84.204\n",
            "Validation Loss: 1.9700454473495483, Validation Accuracy: 56.12\n",
            "[139/150]: Training Loss: 0.5433347140039716, Training Accuracy: 84.306\n",
            "Validation Loss: 1.9688788115978242, Validation Accuracy: 56.13\n",
            "[140/150]: Training Loss: 0.5362506448006144, Training Accuracy: 84.586\n",
            "Validation Loss: 1.966701751947403, Validation Accuracy: 56.24\n",
            "[141/150]: Training Loss: 0.5390761190531205, Training Accuracy: 84.338\n",
            "Validation Loss: 1.9651995241641997, Validation Accuracy: 56.21\n",
            "[142/150]: Training Loss: 0.5341207445884237, Training Accuracy: 84.41\n",
            "Validation Loss: 1.9675312757492065, Validation Accuracy: 56.34\n",
            "[143/150]: Training Loss: 0.5282508870776819, Training Accuracy: 84.872\n",
            "Validation Loss: 1.967752468585968, Validation Accuracy: 56.25\n",
            "[144/150]: Training Loss: 0.5355107656547001, Training Accuracy: 84.532\n",
            "Validation Loss: 1.967381328344345, Validation Accuracy: 56.19\n",
            "[145/150]: Training Loss: 0.5333734960580359, Training Accuracy: 84.51\n",
            "Validation Loss: 1.969217723608017, Validation Accuracy: 56.19\n",
            "[146/150]: Training Loss: 0.5298109002867524, Training Accuracy: 84.75\n",
            "Validation Loss: 1.9694741308689117, Validation Accuracy: 56.12\n",
            "[147/150]: Training Loss: 0.529324583253082, Training Accuracy: 84.722\n",
            "Validation Loss: 1.9707287430763245, Validation Accuracy: 56.17\n",
            "[148/150]: Training Loss: 0.5251490242627203, Training Accuracy: 84.936\n",
            "Validation Loss: 1.9699740409851074, Validation Accuracy: 56.26\n",
            "[149/150]: Training Loss: 0.5242341507454308, Training Accuracy: 84.942\n",
            "Validation Loss: 1.97024707198143, Validation Accuracy: 56.16\n",
            "[150/150]: Training Loss: 0.5341996495821038, Training Accuracy: 84.57\n",
            "Validation Loss: 1.9700913786888123, Validation Accuracy: 56.18\n",
            "**********************************************************************\n",
            "Test Loss: 1.9700913786888123, Test Accuracy: 56.18\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▆▁▅▄▅▃▆▄▃▃▅▅▅▅▅▆▆▆▆</td></tr><tr><td>Test Loss</td><td>▆▆█▄▃▃▄▂▂▃▂▃▃▂▃▃▁▂▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>56.18</td></tr><tr><td>Test Loss</td><td>1.97009</td></tr><tr><td>Train Accuracy</td><td>84.57</td></tr><tr><td>Train Loss</td><td>0.5342</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=512 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_012749-momu4e5u/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 1024, Learning rate: 6.0, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_014707-w80ujlhs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs' target=\"_blank\">batch_size=1024 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.271560571631607, Training Accuracy: 4.982\n",
            "Validation Loss: 3.9391962051391602, Validation Accuracy: 8.84\n",
            "[2/150]: Training Loss: 3.82728639914065, Training Accuracy: 10.948\n",
            "Validation Loss: 3.6827336311340333, Validation Accuracy: 13.37\n",
            "[3/150]: Training Loss: 3.6269987845907408, Training Accuracy: 14.39\n",
            "Validation Loss: 3.4175909042358397, Validation Accuracy: 18.27\n",
            "[4/150]: Training Loss: 3.4358463238696664, Training Accuracy: 17.696\n",
            "Validation Loss: 3.2815504550933836, Validation Accuracy: 20.38\n",
            "[5/150]: Training Loss: 3.3039202203555984, Training Accuracy: 19.798\n",
            "Validation Loss: 3.171510195732117, Validation Accuracy: 23.09\n",
            "[6/150]: Training Loss: 3.1788000671231016, Training Accuracy: 22.106\n",
            "Validation Loss: 3.083156633377075, Validation Accuracy: 24.25\n",
            "[7/150]: Training Loss: 3.0781905553778826, Training Accuracy: 24.04\n",
            "Validation Loss: 2.9470993995666506, Validation Accuracy: 26.99\n",
            "[8/150]: Training Loss: 2.9905799846259917, Training Accuracy: 25.462\n",
            "Validation Loss: 2.8303237915039063, Validation Accuracy: 29.04\n",
            "[9/150]: Training Loss: 2.8789747199233697, Training Accuracy: 28.024\n",
            "Validation Loss: 2.7251497507095337, Validation Accuracy: 31.63\n",
            "[10/150]: Training Loss: 2.752841701312941, Training Accuracy: 30.378\n",
            "Validation Loss: 2.6787135124206545, Validation Accuracy: 32.86\n",
            "[11/150]: Training Loss: 2.664300650966411, Training Accuracy: 32.018\n",
            "Validation Loss: 2.5810091733932494, Validation Accuracy: 34.34\n",
            "[12/150]: Training Loss: 2.6055227445096385, Training Accuracy: 33.34\n",
            "Validation Loss: 2.511804437637329, Validation Accuracy: 35.24\n",
            "[13/150]: Training Loss: 2.569062106463374, Training Accuracy: 34.12\n",
            "Validation Loss: 2.452015995979309, Validation Accuracy: 36.63\n",
            "[14/150]: Training Loss: 2.4965496793085213, Training Accuracy: 35.586\n",
            "Validation Loss: 2.353745174407959, Validation Accuracy: 38.35\n",
            "[15/150]: Training Loss: 2.4792360870205625, Training Accuracy: 35.788\n",
            "Validation Loss: 2.362164545059204, Validation Accuracy: 38.51\n",
            "[16/150]: Training Loss: 2.406777240792099, Training Accuracy: 37.498\n",
            "Validation Loss: 2.322283720970154, Validation Accuracy: 39.52\n",
            "[17/150]: Training Loss: 2.3524391651153564, Training Accuracy: 38.372\n",
            "Validation Loss: 2.2932183027267454, Validation Accuracy: 40.0\n",
            "[18/150]: Training Loss: 2.3162564647441006, Training Accuracy: 39.304\n",
            "Validation Loss: 2.390507221221924, Validation Accuracy: 38.54\n",
            "[19/150]: Training Loss: 2.3010421821049283, Training Accuracy: 39.48\n",
            "Validation Loss: 2.2039053201675416, Validation Accuracy: 42.83\n",
            "[20/150]: Training Loss: 2.2905601968570632, Training Accuracy: 40.014\n",
            "Validation Loss: 2.327000045776367, Validation Accuracy: 39.01\n",
            "[21/150]: Training Loss: 2.2232315151058897, Training Accuracy: 41.472\n",
            "Validation Loss: 2.1848330736160277, Validation Accuracy: 42.37\n",
            "[22/150]: Training Loss: 2.175964046497734, Training Accuracy: 42.316\n",
            "Validation Loss: 2.192854475975037, Validation Accuracy: 42.34\n",
            "[23/150]: Training Loss: 2.12663603315548, Training Accuracy: 43.39\n",
            "Validation Loss: 2.184653973579407, Validation Accuracy: 42.91\n",
            "[24/150]: Training Loss: 2.1151150294712613, Training Accuracy: 43.664\n",
            "Validation Loss: 2.1352186679840086, Validation Accuracy: 43.8\n",
            "[25/150]: Training Loss: 2.0990794507824644, Training Accuracy: 44.256\n",
            "Validation Loss: 2.088001215457916, Validation Accuracy: 44.67\n",
            "[26/150]: Training Loss: 2.0608093349301084, Training Accuracy: 44.852\n",
            "Validation Loss: 2.1832612276077272, Validation Accuracy: 42.92\n",
            "[27/150]: Training Loss: 2.0558675892499028, Training Accuracy: 45.23\n",
            "Validation Loss: 2.176005721092224, Validation Accuracy: 43.08\n",
            "[28/150]: Training Loss: 2.0222370770512796, Training Accuracy: 45.868\n",
            "Validation Loss: 2.1084380388259887, Validation Accuracy: 44.53\n",
            "[29/150]: Training Loss: 1.9681626996215509, Training Accuracy: 47.006\n",
            "Validation Loss: 2.07066353559494, Validation Accuracy: 45.38\n",
            "[30/150]: Training Loss: 1.9436734793137531, Training Accuracy: 47.268\n",
            "Validation Loss: 2.0732940912246702, Validation Accuracy: 45.56\n",
            "[31/150]: Training Loss: 1.9339864083698817, Training Accuracy: 48.042\n",
            "Validation Loss: 2.095534420013428, Validation Accuracy: 45.23\n",
            "[32/150]: Training Loss: 1.9033171717001467, Training Accuracy: 48.32\n",
            "Validation Loss: 2.111021065711975, Validation Accuracy: 45.35\n",
            "[33/150]: Training Loss: 1.904011852887212, Training Accuracy: 48.414\n",
            "Validation Loss: 2.050978398323059, Validation Accuracy: 46.52\n",
            "[34/150]: Training Loss: 1.8750450124545974, Training Accuracy: 48.976\n",
            "Validation Loss: 2.080773401260376, Validation Accuracy: 45.2\n",
            "[35/150]: Training Loss: 1.8805813740710824, Training Accuracy: 48.968\n",
            "Validation Loss: 2.036470913887024, Validation Accuracy: 46.34\n",
            "[36/150]: Training Loss: 1.851276424466347, Training Accuracy: 49.646\n",
            "Validation Loss: 2.1005828619003295, Validation Accuracy: 45.2\n",
            "[37/150]: Training Loss: 1.8493371909978438, Training Accuracy: 49.736\n",
            "Validation Loss: 2.003628730773926, Validation Accuracy: 46.62\n",
            "[38/150]: Training Loss: 1.7895233460835047, Training Accuracy: 50.876\n",
            "Validation Loss: 1.9916603326797486, Validation Accuracy: 47.91\n",
            "[39/150]: Training Loss: 1.783764887829216, Training Accuracy: 51.294\n",
            "Validation Loss: 2.066832160949707, Validation Accuracy: 45.61\n",
            "[40/150]: Training Loss: 1.7760933297021049, Training Accuracy: 51.504\n",
            "Validation Loss: 1.9901643991470337, Validation Accuracy: 47.5\n",
            "[41/150]: Training Loss: 1.7614454274274864, Training Accuracy: 51.798\n",
            "Validation Loss: 2.0182290196418764, Validation Accuracy: 47.1\n",
            "[42/150]: Training Loss: 1.7892101711156416, Training Accuracy: 51.024\n",
            "Validation Loss: 1.9716030478477478, Validation Accuracy: 48.76\n",
            "[43/150]: Training Loss: 1.7074257597631337, Training Accuracy: 52.916\n",
            "Validation Loss: 1.9718806385993957, Validation Accuracy: 48.61\n",
            "[44/150]: Training Loss: 1.714418238523055, Training Accuracy: 52.714\n",
            "Validation Loss: 2.0062466263771057, Validation Accuracy: 48.27\n",
            "[45/150]: Training Loss: 1.6869578483153362, Training Accuracy: 53.334\n",
            "Validation Loss: 1.95201518535614, Validation Accuracy: 49.06\n",
            "[46/150]: Training Loss: 1.6628945092765652, Training Accuracy: 53.922\n",
            "Validation Loss: 2.012867844104767, Validation Accuracy: 47.15\n",
            "[47/150]: Training Loss: 1.6412470778640436, Training Accuracy: 54.254\n",
            "Validation Loss: 1.923640739917755, Validation Accuracy: 49.27\n",
            "[48/150]: Training Loss: 1.6261077705694704, Training Accuracy: 55.032\n",
            "Validation Loss: 1.9601864099502564, Validation Accuracy: 48.98\n",
            "[49/150]: Training Loss: 1.6493362480280351, Training Accuracy: 54.152\n",
            "Validation Loss: 1.9593440890312195, Validation Accuracy: 48.65\n",
            "[50/150]: Training Loss: 1.612999371119908, Training Accuracy: 55.134\n",
            "Validation Loss: 1.9344399094581604, Validation Accuracy: 49.38\n",
            "[51/150]: Training Loss: 1.5939155281806479, Training Accuracy: 55.666\n",
            "Validation Loss: 1.9557547926902772, Validation Accuracy: 48.7\n",
            "[52/150]: Training Loss: 1.6124721181635955, Training Accuracy: 55.024\n",
            "Validation Loss: 1.9627613067626952, Validation Accuracy: 48.75\n",
            "[53/150]: Training Loss: 1.5704505808499394, Training Accuracy: 56.09\n",
            "Validation Loss: 1.9339674592018128, Validation Accuracy: 49.77\n",
            "[54/150]: Training Loss: 1.556705151285444, Training Accuracy: 56.304\n",
            "Validation Loss: 1.910308563709259, Validation Accuracy: 50.74\n",
            "[55/150]: Training Loss: 1.5271518887305746, Training Accuracy: 57.15\n",
            "Validation Loss: 1.8914035081863403, Validation Accuracy: 50.77\n",
            "[56/150]: Training Loss: 1.5157563102488616, Training Accuracy: 57.53\n",
            "Validation Loss: 1.8953699111938476, Validation Accuracy: 50.82\n",
            "[57/150]: Training Loss: 1.4868425179500968, Training Accuracy: 58.284\n",
            "Validation Loss: 1.960522997379303, Validation Accuracy: 49.2\n",
            "[58/150]: Training Loss: 1.5009924586938352, Training Accuracy: 57.544\n",
            "Validation Loss: 1.9257111549377441, Validation Accuracy: 50.88\n",
            "[59/150]: Training Loss: 1.4776659741693614, Training Accuracy: 58.214\n",
            "Validation Loss: 1.9358840227127074, Validation Accuracy: 50.04\n",
            "[60/150]: Training Loss: 1.4657867806298392, Training Accuracy: 58.49\n",
            "Validation Loss: 2.0199026823043824, Validation Accuracy: 48.59\n",
            "[61/150]: Training Loss: 1.4666189709488227, Training Accuracy: 58.868\n",
            "Validation Loss: 1.9441120982170106, Validation Accuracy: 49.79\n",
            "[62/150]: Training Loss: 1.4370595435706937, Training Accuracy: 59.142\n",
            "Validation Loss: 1.8950949430465698, Validation Accuracy: 51.17\n",
            "[63/150]: Training Loss: 1.4119965835493438, Training Accuracy: 59.966\n",
            "Validation Loss: 1.946419107913971, Validation Accuracy: 49.96\n",
            "[64/150]: Training Loss: 1.3992952692265412, Training Accuracy: 60.37\n",
            "Validation Loss: 1.929420042037964, Validation Accuracy: 50.74\n",
            "[65/150]: Training Loss: 1.3843509275086072, Training Accuracy: 60.93\n",
            "Validation Loss: 1.8808708190917969, Validation Accuracy: 51.52\n",
            "[66/150]: Training Loss: 1.3892741276293386, Training Accuracy: 60.538\n",
            "Validation Loss: 1.9178170323371888, Validation Accuracy: 51.05\n",
            "[67/150]: Training Loss: 1.3926233807388617, Training Accuracy: 60.324\n",
            "Validation Loss: 1.879079854488373, Validation Accuracy: 52.14\n",
            "[68/150]: Training Loss: 1.3284603332986638, Training Accuracy: 62.108\n",
            "Validation Loss: 1.8741150736808776, Validation Accuracy: 51.87\n",
            "[69/150]: Training Loss: 1.3297611377677139, Training Accuracy: 62.144\n",
            "Validation Loss: 1.8850895166397095, Validation Accuracy: 51.78\n",
            "[70/150]: Training Loss: 1.3190836857776254, Training Accuracy: 62.086\n",
            "Validation Loss: 1.8964969992637635, Validation Accuracy: 51.72\n",
            "[71/150]: Training Loss: 1.297834805079869, Training Accuracy: 63.002\n",
            "Validation Loss: 1.8922056078910827, Validation Accuracy: 51.94\n",
            "[72/150]: Training Loss: 1.3022558105235198, Training Accuracy: 62.866\n",
            "Validation Loss: 1.8891796469688416, Validation Accuracy: 52.07\n",
            "[73/150]: Training Loss: 1.2681196271156778, Training Accuracy: 63.622\n",
            "Validation Loss: 1.9051270723342895, Validation Accuracy: 52.41\n",
            "[74/150]: Training Loss: 1.2562547800492267, Training Accuracy: 64.092\n",
            "Validation Loss: 1.8912514567375183, Validation Accuracy: 52.28\n",
            "[75/150]: Training Loss: 1.2469108761573324, Training Accuracy: 64.088\n",
            "Validation Loss: 1.9140023827552795, Validation Accuracy: 52.11\n",
            "[76/150]: Training Loss: 1.2424099080416622, Training Accuracy: 64.206\n",
            "Validation Loss: 1.896690595149994, Validation Accuracy: 52.53\n",
            "[77/150]: Training Loss: 1.2172678052162638, Training Accuracy: 64.804\n",
            "Validation Loss: 1.8944836139678956, Validation Accuracy: 52.54\n",
            "[78/150]: Training Loss: 1.198543380717842, Training Accuracy: 65.358\n",
            "Validation Loss: 1.8942208647727967, Validation Accuracy: 52.32\n",
            "[79/150]: Training Loss: 1.198562699921277, Training Accuracy: 65.602\n",
            "Validation Loss: 1.8823209404945374, Validation Accuracy: 52.42\n",
            "[80/150]: Training Loss: 1.1809428224758225, Training Accuracy: 66.04\n",
            "Validation Loss: 1.8809239506721496, Validation Accuracy: 52.57\n",
            "[81/150]: Training Loss: 1.1573098119424314, Training Accuracy: 66.476\n",
            "Validation Loss: 1.904803991317749, Validation Accuracy: 52.64\n",
            "[82/150]: Training Loss: 1.1748385405053898, Training Accuracy: 65.876\n",
            "Validation Loss: 1.8975732803344727, Validation Accuracy: 52.68\n",
            "[83/150]: Training Loss: 1.1381618465696062, Training Accuracy: 67.06\n",
            "Validation Loss: 1.87950758934021, Validation Accuracy: 53.8\n",
            "[84/150]: Training Loss: 1.1169312851769584, Training Accuracy: 67.614\n",
            "Validation Loss: 1.9594017744064331, Validation Accuracy: 51.52\n",
            "[85/150]: Training Loss: 1.1326112601221825, Training Accuracy: 67.212\n",
            "Validation Loss: 1.9392709732055664, Validation Accuracy: 52.51\n",
            "[86/150]: Training Loss: 1.1153452299079116, Training Accuracy: 67.558\n",
            "Validation Loss: 1.8824020862579345, Validation Accuracy: 53.05\n",
            "[87/150]: Training Loss: 1.075961629955136, Training Accuracy: 68.556\n",
            "Validation Loss: 1.8995132923126221, Validation Accuracy: 53.22\n",
            "[88/150]: Training Loss: 1.0561599804430593, Training Accuracy: 69.222\n",
            "Validation Loss: 1.8934579133987426, Validation Accuracy: 53.25\n",
            "[89/150]: Training Loss: 1.0379744488365796, Training Accuracy: 69.548\n",
            "Validation Loss: 1.8907739281654359, Validation Accuracy: 54.05\n",
            "[90/150]: Training Loss: 1.0645462481343015, Training Accuracy: 69.002\n",
            "Validation Loss: 1.9286641478538513, Validation Accuracy: 52.38\n",
            "[91/150]: Training Loss: 1.0510404596523362, Training Accuracy: 69.392\n",
            "Validation Loss: 1.897424077987671, Validation Accuracy: 53.61\n",
            "[92/150]: Training Loss: 1.017185703832276, Training Accuracy: 70.506\n",
            "Validation Loss: 1.9066467523574828, Validation Accuracy: 53.56\n",
            "[93/150]: Training Loss: 0.9912244945156331, Training Accuracy: 70.89\n",
            "Validation Loss: 1.9277787804603577, Validation Accuracy: 53.53\n",
            "[94/150]: Training Loss: 1.0054571652898983, Training Accuracy: 70.53\n",
            "Validation Loss: 1.8809196829795838, Validation Accuracy: 54.1\n",
            "[95/150]: Training Loss: 0.9765667416611497, Training Accuracy: 71.32\n",
            "Validation Loss: 1.8944197297096252, Validation Accuracy: 53.48\n",
            "[96/150]: Training Loss: 0.9578249174721387, Training Accuracy: 71.926\n",
            "Validation Loss: 1.942376160621643, Validation Accuracy: 53.53\n",
            "[97/150]: Training Loss: 0.9575933753227701, Training Accuracy: 71.676\n",
            "Validation Loss: 1.8917027354240417, Validation Accuracy: 53.65\n",
            "[98/150]: Training Loss: 0.944321874453097, Training Accuracy: 72.258\n",
            "Validation Loss: 1.8984474778175353, Validation Accuracy: 54.22\n",
            "[99/150]: Training Loss: 0.9205025398001379, Training Accuracy: 72.942\n",
            "Validation Loss: 1.9325331091880797, Validation Accuracy: 54.27\n",
            "[100/150]: Training Loss: 0.9185542439927861, Training Accuracy: 72.924\n",
            "Validation Loss: 1.9163445711135865, Validation Accuracy: 54.26\n",
            "[101/150]: Training Loss: 0.8990846799344433, Training Accuracy: 73.314\n",
            "Validation Loss: 1.9365061402320862, Validation Accuracy: 54.02\n",
            "[102/150]: Training Loss: 0.8797871519108208, Training Accuracy: 74.004\n",
            "Validation Loss: 1.937075173854828, Validation Accuracy: 54.45\n",
            "[103/150]: Training Loss: 0.8798652291297913, Training Accuracy: 74.044\n",
            "Validation Loss: 1.9165674328804017, Validation Accuracy: 54.24\n",
            "[104/150]: Training Loss: 0.8599669252123151, Training Accuracy: 74.54\n",
            "Validation Loss: 1.9492801547050476, Validation Accuracy: 53.91\n",
            "[105/150]: Training Loss: 0.8473539729507602, Training Accuracy: 74.856\n",
            "Validation Loss: 1.9327858686447144, Validation Accuracy: 53.8\n",
            "[106/150]: Training Loss: 0.8454914895855651, Training Accuracy: 74.948\n",
            "Validation Loss: 1.918694531917572, Validation Accuracy: 54.58\n",
            "[107/150]: Training Loss: 0.8351617601453042, Training Accuracy: 75.38\n",
            "Validation Loss: 1.925265645980835, Validation Accuracy: 54.65\n",
            "[108/150]: Training Loss: 0.8257343720416633, Training Accuracy: 75.846\n",
            "Validation Loss: 1.9380712270736695, Validation Accuracy: 54.61\n",
            "[109/150]: Training Loss: 0.803754199524315, Training Accuracy: 76.216\n",
            "Validation Loss: 1.952760434150696, Validation Accuracy: 54.15\n",
            "[110/150]: Training Loss: 0.8017950021490758, Training Accuracy: 76.334\n",
            "Validation Loss: 1.938999843597412, Validation Accuracy: 54.87\n",
            "[111/150]: Training Loss: 0.7898118264821111, Training Accuracy: 76.776\n",
            "Validation Loss: 1.9549705505371093, Validation Accuracy: 54.67\n",
            "[112/150]: Training Loss: 0.7774463660862981, Training Accuracy: 76.952\n",
            "Validation Loss: 1.9725535273551942, Validation Accuracy: 54.17\n",
            "[113/150]: Training Loss: 0.7767588861134588, Training Accuracy: 76.824\n",
            "Validation Loss: 1.9652800679206848, Validation Accuracy: 54.49\n",
            "[114/150]: Training Loss: 0.7543022997525274, Training Accuracy: 77.61\n",
            "Validation Loss: 1.9479739904403686, Validation Accuracy: 54.85\n",
            "[115/150]: Training Loss: 0.7407014296979321, Training Accuracy: 78.064\n",
            "Validation Loss: 1.974105668067932, Validation Accuracy: 54.73\n",
            "[116/150]: Training Loss: 0.7379214763641357, Training Accuracy: 78.122\n",
            "Validation Loss: 1.9709696292877197, Validation Accuracy: 54.92\n",
            "[117/150]: Training Loss: 0.7280089295640284, Training Accuracy: 78.332\n",
            "Validation Loss: 1.9550812244415283, Validation Accuracy: 54.87\n",
            "[118/150]: Training Loss: 0.719780373330019, Training Accuracy: 78.916\n",
            "Validation Loss: 1.9715718150138855, Validation Accuracy: 54.88\n",
            "[119/150]: Training Loss: 0.7140980338563725, Training Accuracy: 78.938\n",
            "Validation Loss: 1.9744232773780823, Validation Accuracy: 54.88\n",
            "[120/150]: Training Loss: 0.7100381559255172, Training Accuracy: 79.112\n",
            "Validation Loss: 1.9600295305252076, Validation Accuracy: 55.01\n",
            "[121/150]: Training Loss: 0.695380872609664, Training Accuracy: 79.644\n",
            "Validation Loss: 1.9685936331748963, Validation Accuracy: 55.25\n",
            "[122/150]: Training Loss: 0.693044870483632, Training Accuracy: 79.566\n",
            "Validation Loss: 1.973974621295929, Validation Accuracy: 54.99\n",
            "[123/150]: Training Loss: 0.685412128360904, Training Accuracy: 79.762\n",
            "Validation Loss: 1.981511080265045, Validation Accuracy: 55.03\n",
            "[124/150]: Training Loss: 0.6729757055944326, Training Accuracy: 80.388\n",
            "Validation Loss: 1.9723920106887818, Validation Accuracy: 54.88\n",
            "[125/150]: Training Loss: 0.6728029567368177, Training Accuracy: 80.238\n",
            "Validation Loss: 1.972815704345703, Validation Accuracy: 55.06\n",
            "[126/150]: Training Loss: 0.6577607624384821, Training Accuracy: 80.906\n",
            "Validation Loss: 1.9941662311553956, Validation Accuracy: 55.01\n",
            "[127/150]: Training Loss: 0.6531734880135984, Training Accuracy: 80.798\n",
            "Validation Loss: 1.9830293536186219, Validation Accuracy: 55.29\n",
            "[128/150]: Training Loss: 0.6451807484334829, Training Accuracy: 80.998\n",
            "Validation Loss: 1.98456552028656, Validation Accuracy: 55.25\n",
            "[129/150]: Training Loss: 0.6361871045462939, Training Accuracy: 81.438\n",
            "Validation Loss: 1.991374099254608, Validation Accuracy: 55.33\n",
            "[130/150]: Training Loss: 0.6329771852006718, Training Accuracy: 81.326\n",
            "Validation Loss: 1.982979428768158, Validation Accuracy: 55.15\n",
            "[131/150]: Training Loss: 0.6337368731596031, Training Accuracy: 81.498\n",
            "Validation Loss: 1.9814332127571106, Validation Accuracy: 55.25\n",
            "[132/150]: Training Loss: 0.6237865613431347, Training Accuracy: 81.828\n",
            "Validation Loss: 1.9956311464309693, Validation Accuracy: 55.37\n",
            "[133/150]: Training Loss: 0.6184223087466493, Training Accuracy: 81.804\n",
            "Validation Loss: 1.997809612751007, Validation Accuracy: 55.31\n",
            "[134/150]: Training Loss: 0.6162658759525844, Training Accuracy: 82.05\n",
            "Validation Loss: 1.9944164514541627, Validation Accuracy: 55.37\n",
            "[135/150]: Training Loss: 0.6049274242654139, Training Accuracy: 82.308\n",
            "Validation Loss: 2.0007421493530275, Validation Accuracy: 55.18\n",
            "[136/150]: Training Loss: 0.6065428415123297, Training Accuracy: 82.346\n",
            "Validation Loss: 2.002026319503784, Validation Accuracy: 55.35\n",
            "[137/150]: Training Loss: 0.6054561515243686, Training Accuracy: 82.446\n",
            "Validation Loss: 2.0026141166687013, Validation Accuracy: 55.51\n",
            "[138/150]: Training Loss: 0.6016628170499996, Training Accuracy: 82.674\n",
            "Validation Loss: 1.9972286820411682, Validation Accuracy: 55.54\n",
            "[139/150]: Training Loss: 0.599141927397981, Training Accuracy: 82.822\n",
            "Validation Loss: 1.9984585762023925, Validation Accuracy: 55.21\n",
            "[140/150]: Training Loss: 0.5853999877462581, Training Accuracy: 82.924\n",
            "Validation Loss: 2.0003657698631288, Validation Accuracy: 55.39\n",
            "[141/150]: Training Loss: 0.5930902678139356, Training Accuracy: 82.696\n",
            "Validation Loss: 1.9994045495986938, Validation Accuracy: 55.54\n",
            "[142/150]: Training Loss: 0.5897825044028613, Training Accuracy: 83.098\n",
            "Validation Loss: 2.0004444122314453, Validation Accuracy: 55.4\n",
            "[143/150]: Training Loss: 0.5907509253949536, Training Accuracy: 82.836\n",
            "Validation Loss: 1.9981922507286072, Validation Accuracy: 55.3\n",
            "[144/150]: Training Loss: 0.5838782507546094, Training Accuracy: 83.046\n",
            "Validation Loss: 1.9986275434494019, Validation Accuracy: 55.59\n",
            "[145/150]: Training Loss: 0.5819402069461589, Training Accuracy: 83.02\n",
            "Validation Loss: 2.0001697182655334, Validation Accuracy: 55.45\n",
            "[146/150]: Training Loss: 0.5872244786243049, Training Accuracy: 82.898\n",
            "Validation Loss: 2.000009226799011, Validation Accuracy: 55.41\n",
            "[147/150]: Training Loss: 0.5821643848808444, Training Accuracy: 83.186\n",
            "Validation Loss: 2.0006944298744203, Validation Accuracy: 55.35\n",
            "[148/150]: Training Loss: 0.5849178761852031, Training Accuracy: 83.14\n",
            "Validation Loss: 2.000812566280365, Validation Accuracy: 55.33\n",
            "[149/150]: Training Loss: 0.5800890168365167, Training Accuracy: 83.304\n",
            "Validation Loss: 2.000700604915619, Validation Accuracy: 55.4\n",
            "[150/150]: Training Loss: 0.5826626444349483, Training Accuracy: 83.206\n",
            "Validation Loss: 2.000621163845062, Validation Accuracy: 55.35\n",
            "**********************************************************************\n",
            "Test Loss: 2.000621163845062, Test Accuracy: 55.35\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▁▁▃▂▂▅▄▃▄</td></tr><tr><td>Test Loss</td><td>█▂▂▁▄▄▃▄▃▂</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>55.35</td></tr><tr><td>Test Loss</td><td>2.00062</td></tr><tr><td>Train Accuracy</td><td>83.206</td></tr><tr><td>Train Loss</td><td>0.58266</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=1024 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_014707-w80ujlhs/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 2048, Learning rate: 8.485281374238571, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_022321-j8jszxxs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs' target=\"_blank\">batch_size=2048 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.36701738357544, Training Accuracy: 3.886\n",
            "Validation Loss: 4.163058471679688, Validation Accuracy: 5.67\n",
            "[2/150]: Training Loss: 4.113679485321045, Training Accuracy: 6.492\n",
            "Validation Loss: 3.9081267356872558, Validation Accuracy: 9.58\n",
            "[3/150]: Training Loss: 3.8604202461242676, Training Accuracy: 10.016\n",
            "Validation Loss: 3.6436347484588625, Validation Accuracy: 14.06\n",
            "[4/150]: Training Loss: 3.7227595615386964, Training Accuracy: 12.524\n",
            "Validation Loss: 3.58444561958313, Validation Accuracy: 15.49\n",
            "[5/150]: Training Loss: 3.5834127140045164, Training Accuracy: 14.702\n",
            "Validation Loss: 3.4575596332550047, Validation Accuracy: 17.38\n",
            "[6/150]: Training Loss: 3.4569161987304686, Training Accuracy: 16.904\n",
            "Validation Loss: 3.262946367263794, Validation Accuracy: 20.16\n",
            "[7/150]: Training Loss: 3.3158952045440673, Training Accuracy: 19.406\n",
            "Validation Loss: 3.2002731800079345, Validation Accuracy: 22.26\n",
            "[8/150]: Training Loss: 3.1832161426544188, Training Accuracy: 22.068\n",
            "Validation Loss: 3.0290011405944823, Validation Accuracy: 24.81\n",
            "[9/150]: Training Loss: 3.185646390914917, Training Accuracy: 21.936\n",
            "Validation Loss: 3.1018139362335204, Validation Accuracy: 24.11\n",
            "[10/150]: Training Loss: 3.0637048053741456, Training Accuracy: 23.926\n",
            "Validation Loss: 2.9448835372924806, Validation Accuracy: 26.91\n",
            "[11/150]: Training Loss: 2.951322078704834, Training Accuracy: 26.03\n",
            "Validation Loss: 2.8756459236145018, Validation Accuracy: 27.94\n",
            "[12/150]: Training Loss: 2.9279331874847414, Training Accuracy: 26.79\n",
            "Validation Loss: 2.9318973541259767, Validation Accuracy: 27.51\n",
            "[13/150]: Training Loss: 2.828905839920044, Training Accuracy: 28.722\n",
            "Validation Loss: 2.634994125366211, Validation Accuracy: 33.35\n",
            "[14/150]: Training Loss: 2.8078646850585938, Training Accuracy: 29.266\n",
            "Validation Loss: 2.7456551074981688, Validation Accuracy: 30.56\n",
            "[15/150]: Training Loss: 2.712659044265747, Training Accuracy: 30.836\n",
            "Validation Loss: 2.5297746658325195, Validation Accuracy: 34.77\n",
            "[16/150]: Training Loss: 2.651611213684082, Training Accuracy: 32.136\n",
            "Validation Loss: 2.547566270828247, Validation Accuracy: 34.69\n",
            "[17/150]: Training Loss: 2.599648675918579, Training Accuracy: 33.174\n",
            "Validation Loss: 2.5292351245880127, Validation Accuracy: 34.72\n",
            "[18/150]: Training Loss: 2.577428216934204, Training Accuracy: 33.64\n",
            "Validation Loss: 2.5313771247863768, Validation Accuracy: 35.11\n",
            "[19/150]: Training Loss: 2.575561113357544, Training Accuracy: 33.86\n",
            "Validation Loss: 2.50340313911438, Validation Accuracy: 36.18\n",
            "[20/150]: Training Loss: 2.617272434234619, Training Accuracy: 32.958\n",
            "Validation Loss: 2.601680040359497, Validation Accuracy: 34.74\n",
            "[21/150]: Training Loss: 2.503620433807373, Training Accuracy: 35.3\n",
            "Validation Loss: 2.3930795192718506, Validation Accuracy: 38.69\n",
            "[22/150]: Training Loss: 2.4627874088287354, Training Accuracy: 36.662\n",
            "Validation Loss: 2.669213056564331, Validation Accuracy: 33.56\n",
            "[23/150]: Training Loss: 2.548478717803955, Training Accuracy: 34.642\n",
            "Validation Loss: 2.3424915313720702, Validation Accuracy: 39.2\n",
            "[24/150]: Training Loss: 2.3940266799926757, Training Accuracy: 37.736\n",
            "Validation Loss: 2.3067100048065186, Validation Accuracy: 39.53\n",
            "[25/150]: Training Loss: 2.3650291538238526, Training Accuracy: 38.102\n",
            "Validation Loss: 2.4807125091552735, Validation Accuracy: 36.63\n",
            "[26/150]: Training Loss: 2.3525609493255617, Training Accuracy: 38.382\n",
            "Validation Loss: 2.3692517280578613, Validation Accuracy: 39.11\n",
            "[27/150]: Training Loss: 2.2789812183380125, Training Accuracy: 40.074\n",
            "Validation Loss: 2.2349360942840577, Validation Accuracy: 41.75\n",
            "[28/150]: Training Loss: 2.2222459888458252, Training Accuracy: 40.97\n",
            "Validation Loss: 2.19801549911499, Validation Accuracy: 42.61\n",
            "[29/150]: Training Loss: 2.214301881790161, Training Accuracy: 41.694\n",
            "Validation Loss: 2.605668783187866, Validation Accuracy: 35.32\n",
            "[30/150]: Training Loss: 2.338454341888428, Training Accuracy: 38.786\n",
            "Validation Loss: 2.288765287399292, Validation Accuracy: 40.36\n",
            "[31/150]: Training Loss: 2.201904296875, Training Accuracy: 41.99\n",
            "Validation Loss: 2.205613946914673, Validation Accuracy: 42.56\n",
            "[32/150]: Training Loss: 2.1113436079025267, Training Accuracy: 43.934\n",
            "Validation Loss: 2.321266937255859, Validation Accuracy: 41.03\n",
            "[33/150]: Training Loss: 2.1466791677474975, Training Accuracy: 43.006\n",
            "Validation Loss: 2.1349957466125487, Validation Accuracy: 43.8\n",
            "[34/150]: Training Loss: 2.1290991640090944, Training Accuracy: 43.29\n",
            "Validation Loss: 2.217708683013916, Validation Accuracy: 42.79\n",
            "[35/150]: Training Loss: 2.0663461446762086, Training Accuracy: 44.68\n",
            "Validation Loss: 2.3302943229675295, Validation Accuracy: 40.26\n",
            "[36/150]: Training Loss: 2.0297911977767944, Training Accuracy: 45.408\n",
            "Validation Loss: 2.21708025932312, Validation Accuracy: 43.36\n",
            "[37/150]: Training Loss: 2.0129797410964967, Training Accuracy: 45.814\n",
            "Validation Loss: 2.0887409687042235, Validation Accuracy: 45.25\n",
            "[38/150]: Training Loss: 1.937804069519043, Training Accuracy: 47.634\n",
            "Validation Loss: 2.0723501682281493, Validation Accuracy: 45.71\n",
            "[39/150]: Training Loss: 1.943020739555359, Training Accuracy: 47.708\n",
            "Validation Loss: 2.0943463325500487, Validation Accuracy: 44.8\n",
            "[40/150]: Training Loss: 1.9138334608078003, Training Accuracy: 48.346\n",
            "Validation Loss: 2.2366223335266113, Validation Accuracy: 42.21\n",
            "[41/150]: Training Loss: 1.9987104320526123, Training Accuracy: 46.376\n",
            "Validation Loss: 2.12234206199646, Validation Accuracy: 45.35\n",
            "[42/150]: Training Loss: 1.9329777145385743, Training Accuracy: 47.608\n",
            "Validation Loss: 2.097510814666748, Validation Accuracy: 45.23\n",
            "[43/150]: Training Loss: 1.9182713079452514, Training Accuracy: 47.922\n",
            "Validation Loss: 2.085854196548462, Validation Accuracy: 45.54\n",
            "[44/150]: Training Loss: 1.900009379386902, Training Accuracy: 48.732\n",
            "Validation Loss: 2.051387619972229, Validation Accuracy: 46.24\n",
            "[45/150]: Training Loss: 1.8408135080337524, Training Accuracy: 49.714\n",
            "Validation Loss: 2.0023281574249268, Validation Accuracy: 47.57\n",
            "[46/150]: Training Loss: 1.785364203453064, Training Accuracy: 51.326\n",
            "Validation Loss: 2.049905252456665, Validation Accuracy: 46.49\n",
            "[47/150]: Training Loss: 1.8102792501449585, Training Accuracy: 50.858\n",
            "Validation Loss: 2.0309868812561036, Validation Accuracy: 46.64\n",
            "[48/150]: Training Loss: 1.7956236791610718, Training Accuracy: 50.794\n",
            "Validation Loss: 2.0011850118637087, Validation Accuracy: 47.81\n",
            "[49/150]: Training Loss: 1.7439770412445068, Training Accuracy: 52.0\n",
            "Validation Loss: 1.9770531415939332, Validation Accuracy: 48.07\n",
            "[50/150]: Training Loss: 1.7183953332901, Training Accuracy: 52.358\n",
            "Validation Loss: 2.0561673641204834, Validation Accuracy: 46.4\n",
            "[51/150]: Training Loss: 1.717338604927063, Training Accuracy: 53.112\n",
            "Validation Loss: 1.9954840183258056, Validation Accuracy: 47.55\n",
            "[52/150]: Training Loss: 1.7013448238372804, Training Accuracy: 53.098\n",
            "Validation Loss: 2.0024028778076173, Validation Accuracy: 47.72\n",
            "[53/150]: Training Loss: 1.7069044065475465, Training Accuracy: 53.0\n",
            "Validation Loss: 2.08711462020874, Validation Accuracy: 45.49\n",
            "[54/150]: Training Loss: 1.6761716079711915, Training Accuracy: 53.252\n",
            "Validation Loss: 1.9557607173919678, Validation Accuracy: 49.03\n",
            "[55/150]: Training Loss: 1.6440558958053588, Training Accuracy: 54.564\n",
            "Validation Loss: 1.9891844987869263, Validation Accuracy: 48.39\n",
            "[56/150]: Training Loss: 1.6161942625045775, Training Accuracy: 54.926\n",
            "Validation Loss: 2.029493975639343, Validation Accuracy: 47.58\n",
            "[57/150]: Training Loss: 1.6047872447967528, Training Accuracy: 55.192\n",
            "Validation Loss: 1.967372179031372, Validation Accuracy: 48.91\n",
            "[58/150]: Training Loss: 1.713731393814087, Training Accuracy: 52.926\n",
            "Validation Loss: 2.0652761220932008, Validation Accuracy: 46.68\n",
            "[59/150]: Training Loss: 1.6507894086837769, Training Accuracy: 54.226\n",
            "Validation Loss: 1.9929080486297608, Validation Accuracy: 48.66\n",
            "[60/150]: Training Loss: 1.6021452569961547, Training Accuracy: 55.432\n",
            "Validation Loss: 1.9558722257614136, Validation Accuracy: 49.28\n",
            "[61/150]: Training Loss: 1.543088173866272, Training Accuracy: 56.656\n",
            "Validation Loss: 1.9447553634643555, Validation Accuracy: 49.2\n",
            "[62/150]: Training Loss: 1.5494691371917724, Training Accuracy: 56.864\n",
            "Validation Loss: 1.885341501235962, Validation Accuracy: 50.76\n",
            "[63/150]: Training Loss: 1.4937063312530519, Training Accuracy: 58.158\n",
            "Validation Loss: 1.9500130414962769, Validation Accuracy: 49.94\n",
            "[64/150]: Training Loss: 1.4539641427993775, Training Accuracy: 59.004\n",
            "Validation Loss: 1.9285942554473876, Validation Accuracy: 49.98\n",
            "[65/150]: Training Loss: 1.4972302389144898, Training Accuracy: 57.904\n",
            "Validation Loss: 1.9630061149597169, Validation Accuracy: 49.26\n",
            "[66/150]: Training Loss: 1.518665623664856, Training Accuracy: 57.152\n",
            "Validation Loss: 1.9333840608596802, Validation Accuracy: 50.24\n",
            "[67/150]: Training Loss: 1.5036478281021117, Training Accuracy: 57.512\n",
            "Validation Loss: 1.961331272125244, Validation Accuracy: 50.09\n",
            "[68/150]: Training Loss: 1.4539181280136109, Training Accuracy: 59.142\n",
            "Validation Loss: 1.9611144304275512, Validation Accuracy: 49.25\n",
            "[69/150]: Training Loss: 1.46078604221344, Training Accuracy: 58.794\n",
            "Validation Loss: 1.916247057914734, Validation Accuracy: 50.02\n",
            "[70/150]: Training Loss: 1.4112844705581664, Training Accuracy: 59.934\n",
            "Validation Loss: 1.8900265216827392, Validation Accuracy: 51.2\n",
            "[71/150]: Training Loss: 1.3915088891983032, Training Accuracy: 60.376\n",
            "Validation Loss: 1.9043931245803833, Validation Accuracy: 50.87\n",
            "[72/150]: Training Loss: 1.3921622323989868, Training Accuracy: 60.564\n",
            "Validation Loss: 1.9693795204162599, Validation Accuracy: 50.0\n",
            "[73/150]: Training Loss: 1.3862884998321534, Training Accuracy: 60.752\n",
            "Validation Loss: 1.9339972734451294, Validation Accuracy: 50.94\n",
            "[74/150]: Training Loss: 1.358318910598755, Training Accuracy: 61.528\n",
            "Validation Loss: 1.9029748439788818, Validation Accuracy: 51.01\n",
            "[75/150]: Training Loss: 1.3378327751159669, Training Accuracy: 61.9\n",
            "Validation Loss: 1.9114508390426637, Validation Accuracy: 51.04\n",
            "[76/150]: Training Loss: 1.307257251739502, Training Accuracy: 62.684\n",
            "Validation Loss: 1.897865653038025, Validation Accuracy: 51.33\n",
            "[77/150]: Training Loss: 1.307571873664856, Training Accuracy: 62.566\n",
            "Validation Loss: 1.9430776834487915, Validation Accuracy: 50.79\n",
            "[78/150]: Training Loss: 1.2927326250076294, Training Accuracy: 62.904\n",
            "Validation Loss: 1.910500168800354, Validation Accuracy: 52.01\n",
            "[79/150]: Training Loss: 1.2737730932235718, Training Accuracy: 63.4\n",
            "Validation Loss: 1.8831387996673583, Validation Accuracy: 52.08\n",
            "[80/150]: Training Loss: 1.254057068824768, Training Accuracy: 64.394\n",
            "Validation Loss: 1.8864023208618164, Validation Accuracy: 51.51\n",
            "[81/150]: Training Loss: 1.2196114826202393, Training Accuracy: 64.948\n",
            "Validation Loss: 1.9068121194839478, Validation Accuracy: 51.97\n",
            "[82/150]: Training Loss: 1.2375918960571288, Training Accuracy: 64.48\n",
            "Validation Loss: 1.9304296493530273, Validation Accuracy: 51.61\n",
            "[83/150]: Training Loss: 1.2156933164596557, Training Accuracy: 64.806\n",
            "Validation Loss: 1.9103889226913453, Validation Accuracy: 51.82\n",
            "[84/150]: Training Loss: 1.1918401718139648, Training Accuracy: 65.572\n",
            "Validation Loss: 1.9341503858566285, Validation Accuracy: 51.6\n",
            "[85/150]: Training Loss: 1.2064945554733277, Training Accuracy: 65.146\n",
            "Validation Loss: 1.9983864784240724, Validation Accuracy: 50.34\n",
            "[86/150]: Training Loss: 1.201401376724243, Training Accuracy: 65.426\n",
            "Validation Loss: 1.9176611185073853, Validation Accuracy: 51.45\n",
            "[87/150]: Training Loss: 1.1660136127471923, Training Accuracy: 66.176\n",
            "Validation Loss: 1.943800139427185, Validation Accuracy: 51.49\n",
            "[88/150]: Training Loss: 1.1312262058258056, Training Accuracy: 67.438\n",
            "Validation Loss: 1.9152551412582397, Validation Accuracy: 51.68\n",
            "[89/150]: Training Loss: 1.1454276323318482, Training Accuracy: 66.958\n",
            "Validation Loss: 1.9066155195236205, Validation Accuracy: 52.43\n",
            "[90/150]: Training Loss: 1.1536525392532349, Training Accuracy: 66.622\n",
            "Validation Loss: 1.912238073348999, Validation Accuracy: 51.99\n",
            "[91/150]: Training Loss: 1.1152591037750244, Training Accuracy: 67.662\n",
            "Validation Loss: 1.9254616022109985, Validation Accuracy: 52.11\n",
            "[92/150]: Training Loss: 1.0796469306945802, Training Accuracy: 68.538\n",
            "Validation Loss: 1.918501377105713, Validation Accuracy: 52.25\n",
            "[93/150]: Training Loss: 1.0836839008331298, Training Accuracy: 68.43\n",
            "Validation Loss: 1.9036231279373168, Validation Accuracy: 52.52\n",
            "[94/150]: Training Loss: 1.111613211631775, Training Accuracy: 67.66\n",
            "Validation Loss: 1.9749577283859252, Validation Accuracy: 51.0\n",
            "[95/150]: Training Loss: 1.1183206748962402, Training Accuracy: 67.53\n",
            "Validation Loss: 1.9319661378860473, Validation Accuracy: 52.53\n",
            "[96/150]: Training Loss: 1.0428566884994508, Training Accuracy: 69.502\n",
            "Validation Loss: 1.9246442794799805, Validation Accuracy: 52.17\n",
            "[97/150]: Training Loss: 1.029050705432892, Training Accuracy: 70.002\n",
            "Validation Loss: 1.9244839429855347, Validation Accuracy: 52.21\n",
            "[98/150]: Training Loss: 1.0212417674064636, Training Accuracy: 70.272\n",
            "Validation Loss: 1.9145327806472778, Validation Accuracy: 52.78\n",
            "[99/150]: Training Loss: 1.0124672603607179, Training Accuracy: 70.44\n",
            "Validation Loss: 1.9107223749160767, Validation Accuracy: 53.04\n",
            "[100/150]: Training Loss: 0.9880559778213501, Training Accuracy: 71.17\n",
            "Validation Loss: 1.9347419500350953, Validation Accuracy: 52.57\n",
            "[101/150]: Training Loss: 0.9812371230125427, Training Accuracy: 71.144\n",
            "Validation Loss: 1.9186458826065063, Validation Accuracy: 53.22\n",
            "[102/150]: Training Loss: 0.9656218528747559, Training Accuracy: 72.014\n",
            "Validation Loss: 1.9120693922042846, Validation Accuracy: 53.07\n",
            "[103/150]: Training Loss: 0.9567889213562012, Training Accuracy: 71.89\n",
            "Validation Loss: 1.9197413206100464, Validation Accuracy: 52.84\n",
            "[104/150]: Training Loss: 0.9529655456542969, Training Accuracy: 72.086\n",
            "Validation Loss: 1.9144711256027223, Validation Accuracy: 53.0\n",
            "[105/150]: Training Loss: 0.9852134394645691, Training Accuracy: 71.252\n",
            "Validation Loss: 1.9319961071014404, Validation Accuracy: 53.06\n",
            "[106/150]: Training Loss: 0.9273789191246032, Training Accuracy: 72.91\n",
            "Validation Loss: 1.9424444437026978, Validation Accuracy: 52.84\n",
            "[107/150]: Training Loss: 0.9234069514274598, Training Accuracy: 73.084\n",
            "Validation Loss: 1.9234145641326905, Validation Accuracy: 52.94\n",
            "[108/150]: Training Loss: 0.9229924130439758, Training Accuracy: 73.21\n",
            "Validation Loss: 1.9324826002120972, Validation Accuracy: 52.73\n",
            "[109/150]: Training Loss: 0.9058748340606689, Training Accuracy: 73.67\n",
            "Validation Loss: 1.9500421285629272, Validation Accuracy: 53.29\n",
            "[110/150]: Training Loss: 0.8911954641342164, Training Accuracy: 73.872\n",
            "Validation Loss: 1.941013789176941, Validation Accuracy: 53.28\n",
            "[111/150]: Training Loss: 0.8825567650794983, Training Accuracy: 74.012\n",
            "Validation Loss: 1.9361898183822632, Validation Accuracy: 53.09\n",
            "[112/150]: Training Loss: 0.8620765686035157, Training Accuracy: 74.656\n",
            "Validation Loss: 1.9341821908950805, Validation Accuracy: 53.36\n",
            "[113/150]: Training Loss: 0.8601221704483032, Training Accuracy: 74.8\n",
            "Validation Loss: 1.9615466117858886, Validation Accuracy: 53.34\n",
            "[114/150]: Training Loss: 0.8472113418579101, Training Accuracy: 75.216\n",
            "Validation Loss: 1.950052309036255, Validation Accuracy: 53.42\n",
            "[115/150]: Training Loss: 0.84150705575943, Training Accuracy: 75.34\n",
            "Validation Loss: 1.9719900608062744, Validation Accuracy: 53.33\n",
            "[116/150]: Training Loss: 0.8295826292037964, Training Accuracy: 75.764\n",
            "Validation Loss: 1.9504849910736084, Validation Accuracy: 53.65\n",
            "[117/150]: Training Loss: 0.8204774522781372, Training Accuracy: 76.024\n",
            "Validation Loss: 1.9509764194488526, Validation Accuracy: 54.02\n",
            "[118/150]: Training Loss: 0.8103749394416809, Training Accuracy: 76.418\n",
            "Validation Loss: 1.9664921760559082, Validation Accuracy: 53.3\n",
            "[119/150]: Training Loss: 0.8074698400497436, Training Accuracy: 76.44\n",
            "Validation Loss: 1.955758023262024, Validation Accuracy: 53.71\n",
            "[120/150]: Training Loss: 0.8068988251686097, Training Accuracy: 76.406\n",
            "Validation Loss: 1.9637468814849854, Validation Accuracy: 53.55\n",
            "[121/150]: Training Loss: 0.7903911852836609, Training Accuracy: 76.678\n",
            "Validation Loss: 1.9706329584121705, Validation Accuracy: 53.57\n",
            "[122/150]: Training Loss: 0.7799955105781555, Training Accuracy: 77.276\n",
            "Validation Loss: 1.976036286354065, Validation Accuracy: 53.7\n",
            "[123/150]: Training Loss: 0.7765986251831055, Training Accuracy: 77.324\n",
            "Validation Loss: 1.9730368375778198, Validation Accuracy: 53.3\n",
            "[124/150]: Training Loss: 0.7681253123283386, Training Accuracy: 77.54\n",
            "Validation Loss: 1.9741400957107544, Validation Accuracy: 53.63\n",
            "[125/150]: Training Loss: 0.7622663331031799, Training Accuracy: 77.518\n",
            "Validation Loss: 1.9705329179763793, Validation Accuracy: 54.09\n",
            "[126/150]: Training Loss: 0.7574337005615235, Training Accuracy: 78.034\n",
            "Validation Loss: 1.9730172395706176, Validation Accuracy: 53.71\n",
            "[127/150]: Training Loss: 0.7504327082633973, Training Accuracy: 78.258\n",
            "Validation Loss: 1.9689469575881957, Validation Accuracy: 53.87\n",
            "[128/150]: Training Loss: 0.7432142400741577, Training Accuracy: 78.082\n",
            "Validation Loss: 1.9787657976150512, Validation Accuracy: 53.54\n",
            "[129/150]: Training Loss: 0.7408276867866516, Training Accuracy: 78.396\n",
            "Validation Loss: 1.9802783012390137, Validation Accuracy: 53.89\n",
            "[130/150]: Training Loss: 0.7272537612915039, Training Accuracy: 78.856\n",
            "Validation Loss: 1.9677628517150878, Validation Accuracy: 53.76\n",
            "[131/150]: Training Loss: 0.7158604049682618, Training Accuracy: 78.946\n",
            "Validation Loss: 1.97785542011261, Validation Accuracy: 53.9\n",
            "[132/150]: Training Loss: 0.7163805866241455, Training Accuracy: 79.138\n",
            "Validation Loss: 1.976898455619812, Validation Accuracy: 53.7\n",
            "[133/150]: Training Loss: 0.721850438117981, Training Accuracy: 78.816\n",
            "Validation Loss: 1.9874832153320312, Validation Accuracy: 53.66\n",
            "[134/150]: Training Loss: 0.719142906665802, Training Accuracy: 79.046\n",
            "Validation Loss: 1.9858264207839966, Validation Accuracy: 53.92\n",
            "[135/150]: Training Loss: 0.7152685523033142, Training Accuracy: 79.11\n",
            "Validation Loss: 1.9745909452438355, Validation Accuracy: 53.99\n",
            "[136/150]: Training Loss: 0.7077970743179322, Training Accuracy: 79.564\n",
            "Validation Loss: 1.9791152715682983, Validation Accuracy: 53.54\n",
            "[137/150]: Training Loss: 0.7019647455215454, Training Accuracy: 79.626\n",
            "Validation Loss: 1.9777050018310547, Validation Accuracy: 54.06\n",
            "[138/150]: Training Loss: 0.6962298607826233, Training Accuracy: 79.744\n",
            "Validation Loss: 1.9783851623535156, Validation Accuracy: 53.9\n",
            "[139/150]: Training Loss: 0.6945947551727295, Training Accuracy: 79.802\n",
            "Validation Loss: 1.9834458827972412, Validation Accuracy: 53.84\n",
            "[140/150]: Training Loss: 0.6916244769096375, Training Accuracy: 79.876\n",
            "Validation Loss: 1.9782796621322631, Validation Accuracy: 54.08\n",
            "[141/150]: Training Loss: 0.686495201587677, Training Accuracy: 80.022\n",
            "Validation Loss: 1.9823485612869263, Validation Accuracy: 54.07\n",
            "[142/150]: Training Loss: 0.6907814073562623, Training Accuracy: 80.056\n",
            "Validation Loss: 1.9827837467193603, Validation Accuracy: 53.98\n",
            "[143/150]: Training Loss: 0.6871487998962402, Training Accuracy: 80.084\n",
            "Validation Loss: 1.9844423055648803, Validation Accuracy: 54.1\n",
            "[144/150]: Training Loss: 0.6854921150207519, Training Accuracy: 80.314\n",
            "Validation Loss: 1.9852968454360962, Validation Accuracy: 53.98\n",
            "[145/150]: Training Loss: 0.6804221367835999, Training Accuracy: 80.244\n",
            "Validation Loss: 1.9844240427017212, Validation Accuracy: 54.1\n",
            "[146/150]: Training Loss: 0.682576208114624, Training Accuracy: 80.136\n",
            "Validation Loss: 1.9841588735580444, Validation Accuracy: 54.07\n",
            "[147/150]: Training Loss: 0.6786355781555176, Training Accuracy: 80.254\n",
            "Validation Loss: 1.9839539051055908, Validation Accuracy: 54.12\n",
            "[148/150]: Training Loss: 0.6753697633743286, Training Accuracy: 80.27\n",
            "Validation Loss: 1.9842885971069335, Validation Accuracy: 54.16\n",
            "[149/150]: Training Loss: 0.6753653740882873, Training Accuracy: 80.488\n",
            "Validation Loss: 1.9842547655105591, Validation Accuracy: 54.12\n",
            "[150/150]: Training Loss: 0.6740122199058532, Training Accuracy: 80.532\n",
            "Validation Loss: 1.984296441078186, Validation Accuracy: 54.15\n",
            "**********************************************************************\n",
            "Test Loss: 1.984296441078186, Test Accuracy: 54.15\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁█▆▃▇</td></tr><tr><td>Test Loss</td><td>▆▄▆█▁</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>54.15</td></tr><tr><td>Test Loss</td><td>1.9843</td></tr><tr><td>Train Accuracy</td><td>80.532</td></tr><tr><td>Train Loss</td><td>0.67401</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=2048 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_022321-j8jszxxs/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 4096, Learning rate: 12.0, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_024410-fcrci1zx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx' target=\"_blank\">batch_size=4096 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.448218419001653, Training Accuracy: 3.104\n",
            "Validation Loss: 4.19553820292155, Validation Accuracy: 5.83\n",
            "[2/150]: Training Loss: 4.162900081047645, Training Accuracy: 5.992\n",
            "Validation Loss: 4.001244783401489, Validation Accuracy: 8.19\n",
            "[3/150]: Training Loss: 3.9526728116548977, Training Accuracy: 9.02\n",
            "Validation Loss: 3.849735975265503, Validation Accuracy: 10.99\n",
            "[4/150]: Training Loss: 3.9088880098783054, Training Accuracy: 9.748\n",
            "Validation Loss: 3.7720150152842202, Validation Accuracy: 11.82\n",
            "[5/150]: Training Loss: 3.7579474999354434, Training Accuracy: 11.674\n",
            "Validation Loss: 3.6346964836120605, Validation Accuracy: 13.58\n",
            "[6/150]: Training Loss: 3.8317384169651914, Training Accuracy: 10.848\n",
            "Validation Loss: 3.6928911209106445, Validation Accuracy: 13.8\n",
            "[7/150]: Training Loss: 3.6279670825371375, Training Accuracy: 13.764\n",
            "Validation Loss: 3.426232655843099, Validation Accuracy: 17.9\n",
            "[8/150]: Training Loss: 3.559702047934899, Training Accuracy: 15.198\n",
            "Validation Loss: 3.5184563795725503, Validation Accuracy: 15.74\n",
            "[9/150]: Training Loss: 3.619346306874202, Training Accuracy: 14.12\n",
            "Validation Loss: 3.4724766413370767, Validation Accuracy: 16.53\n",
            "[10/150]: Training Loss: 3.4825954070458045, Training Accuracy: 16.518\n",
            "Validation Loss: 3.491668939590454, Validation Accuracy: 17.05\n",
            "[11/150]: Training Loss: 3.5322888447688174, Training Accuracy: 15.738\n",
            "Validation Loss: 3.3563063939412436, Validation Accuracy: 18.53\n",
            "[12/150]: Training Loss: 3.381067386040321, Training Accuracy: 18.274\n",
            "Validation Loss: 3.371330420176188, Validation Accuracy: 18.51\n",
            "[13/150]: Training Loss: 3.3759262011601376, Training Accuracy: 18.364\n",
            "Validation Loss: 3.1718979676564536, Validation Accuracy: 22.38\n",
            "[14/150]: Training Loss: 3.24786586027879, Training Accuracy: 20.88\n",
            "Validation Loss: 3.149214585622152, Validation Accuracy: 22.67\n",
            "[15/150]: Training Loss: 3.155352940926185, Training Accuracy: 22.24\n",
            "Validation Loss: 2.999330917994181, Validation Accuracy: 25.44\n",
            "[16/150]: Training Loss: 2.9997272124657264, Training Accuracy: 25.252\n",
            "Validation Loss: 2.8626222610473633, Validation Accuracy: 28.18\n",
            "[17/150]: Training Loss: 2.952247326190655, Training Accuracy: 26.39\n",
            "Validation Loss: 3.0151708920796714, Validation Accuracy: 25.48\n",
            "[18/150]: Training Loss: 3.035908350577721, Training Accuracy: 24.716\n",
            "Validation Loss: 2.84928830464681, Validation Accuracy: 28.54\n",
            "[19/150]: Training Loss: 2.8744187355041504, Training Accuracy: 27.56\n",
            "Validation Loss: 2.732924222946167, Validation Accuracy: 30.87\n",
            "[20/150]: Training Loss: 2.7774770443256083, Training Accuracy: 29.672\n",
            "Validation Loss: 2.800260861714681, Validation Accuracy: 29.69\n",
            "[21/150]: Training Loss: 2.9696661142202525, Training Accuracy: 26.582\n",
            "Validation Loss: 3.213308095932007, Validation Accuracy: 22.23\n",
            "[22/150]: Training Loss: 3.2055968137887807, Training Accuracy: 22.87\n",
            "Validation Loss: 3.0360422134399414, Validation Accuracy: 25.56\n",
            "[23/150]: Training Loss: 3.0269867456876316, Training Accuracy: 25.102\n",
            "Validation Loss: 2.861661911010742, Validation Accuracy: 29.24\n",
            "[24/150]: Training Loss: 2.8519647121429443, Training Accuracy: 28.248\n",
            "Validation Loss: 2.682717482248942, Validation Accuracy: 31.6\n",
            "[25/150]: Training Loss: 2.705435276031494, Training Accuracy: 31.156\n",
            "Validation Loss: 2.509896198908488, Validation Accuracy: 35.11\n",
            "[26/150]: Training Loss: 2.613956708174485, Training Accuracy: 33.41\n",
            "Validation Loss: 2.505950371424357, Validation Accuracy: 34.77\n",
            "[27/150]: Training Loss: 3.002190589904785, Training Accuracy: 26.466\n",
            "Validation Loss: 2.775864839553833, Validation Accuracy: 29.88\n",
            "[28/150]: Training Loss: 2.742469017322247, Training Accuracy: 30.316\n",
            "Validation Loss: 2.589009443918864, Validation Accuracy: 33.99\n",
            "[29/150]: Training Loss: 2.8341226394359884, Training Accuracy: 29.282\n",
            "Validation Loss: 2.8079089323679605, Validation Accuracy: 29.56\n",
            "[30/150]: Training Loss: 2.745966214400071, Training Accuracy: 30.384\n",
            "Validation Loss: 2.5663379033406577, Validation Accuracy: 34.18\n",
            "[31/150]: Training Loss: 2.5754335476801944, Training Accuracy: 33.944\n",
            "Validation Loss: 2.4243160088857016, Validation Accuracy: 37.33\n",
            "[32/150]: Training Loss: 2.4728290117703953, Training Accuracy: 35.998\n",
            "Validation Loss: 2.4164366722106934, Validation Accuracy: 37.02\n",
            "[33/150]: Training Loss: 2.550288127018855, Training Accuracy: 34.558\n",
            "Validation Loss: 2.4929703871409097, Validation Accuracy: 35.7\n",
            "[34/150]: Training Loss: 2.5172606064723086, Training Accuracy: 35.468\n",
            "Validation Loss: 2.5022354125976562, Validation Accuracy: 35.92\n",
            "[35/150]: Training Loss: 2.5095452528733473, Training Accuracy: 35.052\n",
            "Validation Loss: 2.3538503646850586, Validation Accuracy: 38.43\n",
            "[36/150]: Training Loss: 2.595434335561899, Training Accuracy: 34.342\n",
            "Validation Loss: 2.7459415594736734, Validation Accuracy: 30.95\n",
            "[37/150]: Training Loss: 2.5996641562535214, Training Accuracy: 33.51\n",
            "Validation Loss: 2.4413878122965493, Validation Accuracy: 37.55\n",
            "[38/150]: Training Loss: 2.4308731005742, Training Accuracy: 36.788\n",
            "Validation Loss: 2.3517263730367026, Validation Accuracy: 38.8\n",
            "[39/150]: Training Loss: 2.3385920707996073, Training Accuracy: 39.318\n",
            "Validation Loss: 2.4507851600646973, Validation Accuracy: 37.58\n",
            "[40/150]: Training Loss: 2.4006691529200626, Training Accuracy: 37.842\n",
            "Validation Loss: 2.3381757736206055, Validation Accuracy: 39.0\n",
            "[41/150]: Training Loss: 2.4301048792325535, Training Accuracy: 36.898\n",
            "Validation Loss: 2.3649702866872153, Validation Accuracy: 38.64\n",
            "[42/150]: Training Loss: 2.282798070173997, Training Accuracy: 39.856\n",
            "Validation Loss: 2.2728551228841147, Validation Accuracy: 40.36\n",
            "[43/150]: Training Loss: 2.235896715751061, Training Accuracy: 40.962\n",
            "Validation Loss: 2.2269629637400308, Validation Accuracy: 41.47\n",
            "[44/150]: Training Loss: 2.277051045344426, Training Accuracy: 40.666\n",
            "Validation Loss: 2.393709977467855, Validation Accuracy: 38.68\n",
            "[45/150]: Training Loss: 2.266980061164269, Training Accuracy: 40.186\n",
            "Validation Loss: 2.260339101155599, Validation Accuracy: 41.24\n",
            "[46/150]: Training Loss: 2.2269545518434963, Training Accuracy: 41.5\n",
            "Validation Loss: 2.184856653213501, Validation Accuracy: 42.59\n",
            "[47/150]: Training Loss: 2.1506412762861986, Training Accuracy: 42.82\n",
            "Validation Loss: 2.151843468348185, Validation Accuracy: 43.6\n",
            "[48/150]: Training Loss: 2.0945023756760817, Training Accuracy: 45.142\n",
            "Validation Loss: 2.1851927439371743, Validation Accuracy: 42.94\n",
            "[49/150]: Training Loss: 2.368942279082078, Training Accuracy: 38.482\n",
            "Validation Loss: 2.311251401901245, Validation Accuracy: 39.8\n",
            "[50/150]: Training Loss: 2.1795968275803785, Training Accuracy: 42.274\n",
            "Validation Loss: 2.1368969281514487, Validation Accuracy: 43.53\n",
            "[51/150]: Training Loss: 2.287435614145719, Training Accuracy: 40.578\n",
            "Validation Loss: 2.3101561864217124, Validation Accuracy: 40.31\n",
            "[52/150]: Training Loss: 2.133154667340792, Training Accuracy: 43.412\n",
            "Validation Loss: 2.101039091746012, Validation Accuracy: 44.48\n",
            "[53/150]: Training Loss: 2.009332299232483, Training Accuracy: 46.076\n",
            "Validation Loss: 2.053215821584066, Validation Accuracy: 46.02\n",
            "[54/150]: Training Loss: 2.0092475414276123, Training Accuracy: 46.312\n",
            "Validation Loss: 2.068614880243937, Validation Accuracy: 45.15\n",
            "[55/150]: Training Loss: 2.0747447105554433, Training Accuracy: 44.708\n",
            "Validation Loss: 2.0757156213124595, Validation Accuracy: 45.38\n",
            "[56/150]: Training Loss: 2.0832339341823873, Training Accuracy: 44.52\n",
            "Validation Loss: 2.2822861671447754, Validation Accuracy: 40.59\n",
            "[57/150]: Training Loss: 2.035889350450956, Training Accuracy: 45.234\n",
            "Validation Loss: 2.077279488245646, Validation Accuracy: 45.32\n",
            "[58/150]: Training Loss: 1.9101605140245879, Training Accuracy: 48.098\n",
            "Validation Loss: 2.075277328491211, Validation Accuracy: 45.52\n",
            "[59/150]: Training Loss: 1.930518700526311, Training Accuracy: 47.694\n",
            "Validation Loss: 2.0554560820261636, Validation Accuracy: 45.94\n",
            "[60/150]: Training Loss: 1.9262570784642146, Training Accuracy: 47.71\n",
            "Validation Loss: 1.9856750170389812, Validation Accuracy: 46.93\n",
            "[61/150]: Training Loss: 1.8526782714403593, Training Accuracy: 49.478\n",
            "Validation Loss: 1.9915226300557454, Validation Accuracy: 46.83\n",
            "[62/150]: Training Loss: 1.8636801151128917, Training Accuracy: 49.23\n",
            "Validation Loss: 2.094877322514852, Validation Accuracy: 45.09\n",
            "[63/150]: Training Loss: 1.8440089867665217, Training Accuracy: 49.998\n",
            "Validation Loss: 1.9725687503814697, Validation Accuracy: 47.58\n",
            "[64/150]: Training Loss: 1.8951456546783447, Training Accuracy: 48.798\n",
            "Validation Loss: 2.079833189646403, Validation Accuracy: 45.33\n",
            "[65/150]: Training Loss: 1.9134588700074415, Training Accuracy: 48.274\n",
            "Validation Loss: 2.0578166246414185, Validation Accuracy: 45.69\n",
            "[66/150]: Training Loss: 1.850765347480774, Training Accuracy: 49.748\n",
            "Validation Loss: 2.0475390752156577, Validation Accuracy: 47.14\n",
            "[67/150]: Training Loss: 1.7821540557421172, Training Accuracy: 51.288\n",
            "Validation Loss: 1.9497700134913127, Validation Accuracy: 48.19\n",
            "[68/150]: Training Loss: 1.7319823136696448, Training Accuracy: 52.606\n",
            "Validation Loss: 2.018282691637675, Validation Accuracy: 47.2\n",
            "[69/150]: Training Loss: 1.7186179894667406, Training Accuracy: 52.626\n",
            "Validation Loss: 1.9749605258305867, Validation Accuracy: 48.19\n",
            "[70/150]: Training Loss: 1.6911903069569514, Training Accuracy: 53.452\n",
            "Validation Loss: 1.9826482931772869, Validation Accuracy: 47.74\n",
            "[71/150]: Training Loss: 1.660032529097337, Training Accuracy: 54.11\n",
            "Validation Loss: 1.908194661140442, Validation Accuracy: 49.51\n",
            "[72/150]: Training Loss: 1.6769285110326915, Training Accuracy: 53.518\n",
            "Validation Loss: 1.954678734143575, Validation Accuracy: 48.03\n",
            "[73/150]: Training Loss: 1.6593820590239305, Training Accuracy: 53.96\n",
            "Validation Loss: 1.9272022247314453, Validation Accuracy: 48.63\n",
            "[74/150]: Training Loss: 1.6419056562276988, Training Accuracy: 54.992\n",
            "Validation Loss: 1.992493987083435, Validation Accuracy: 47.79\n",
            "[75/150]: Training Loss: 1.6689472198486328, Training Accuracy: 53.936\n",
            "Validation Loss: 1.9214499394098918, Validation Accuracy: 49.0\n",
            "[76/150]: Training Loss: 1.6087861702992365, Training Accuracy: 55.276\n",
            "Validation Loss: 1.896010160446167, Validation Accuracy: 49.84\n",
            "[77/150]: Training Loss: 1.5652516438410833, Training Accuracy: 56.72\n",
            "Validation Loss: 1.873304804166158, Validation Accuracy: 51.03\n",
            "[78/150]: Training Loss: 1.6746907876088069, Training Accuracy: 54.032\n",
            "Validation Loss: 1.954778750737508, Validation Accuracy: 49.08\n",
            "[79/150]: Training Loss: 1.5788640517454882, Training Accuracy: 55.932\n",
            "Validation Loss: 1.9272086222966511, Validation Accuracy: 50.3\n",
            "[80/150]: Training Loss: 1.6021267634171705, Training Accuracy: 55.644\n",
            "Validation Loss: 2.1279262701670327, Validation Accuracy: 45.34\n",
            "[81/150]: Training Loss: 1.7035195552385771, Training Accuracy: 53.114\n",
            "Validation Loss: 1.9278326431910198, Validation Accuracy: 49.04\n",
            "[82/150]: Training Loss: 1.559159095470722, Training Accuracy: 56.4\n",
            "Validation Loss: 1.9029817183812459, Validation Accuracy: 49.79\n",
            "[83/150]: Training Loss: 1.5267010835500865, Training Accuracy: 57.53\n",
            "Validation Loss: 1.9380817810694377, Validation Accuracy: 48.88\n",
            "[84/150]: Training Loss: 1.5102592706680298, Training Accuracy: 57.916\n",
            "Validation Loss: 1.8722127676010132, Validation Accuracy: 51.08\n",
            "[85/150]: Training Loss: 1.4807829398375292, Training Accuracy: 58.592\n",
            "Validation Loss: 1.9032895962397258, Validation Accuracy: 49.92\n",
            "[86/150]: Training Loss: 1.4740843681188731, Training Accuracy: 58.914\n",
            "Validation Loss: 1.8672935565312703, Validation Accuracy: 50.45\n",
            "[87/150]: Training Loss: 1.515398117212149, Training Accuracy: 57.934\n",
            "Validation Loss: 1.9412325620651245, Validation Accuracy: 49.15\n",
            "[88/150]: Training Loss: 1.576969366807204, Training Accuracy: 55.868\n",
            "Validation Loss: 1.935785969098409, Validation Accuracy: 49.25\n",
            "[89/150]: Training Loss: 1.4918665610826933, Training Accuracy: 58.11\n",
            "Validation Loss: 1.8986582358678181, Validation Accuracy: 51.05\n",
            "[90/150]: Training Loss: 1.4492073059082031, Training Accuracy: 59.446\n",
            "Validation Loss: 1.8367600440979004, Validation Accuracy: 51.57\n",
            "[91/150]: Training Loss: 1.4048503912412202, Training Accuracy: 60.418\n",
            "Validation Loss: 1.837977687517802, Validation Accuracy: 51.68\n",
            "[92/150]: Training Loss: 1.3744991559248705, Training Accuracy: 61.112\n",
            "Validation Loss: 1.8420219818751018, Validation Accuracy: 51.77\n",
            "[93/150]: Training Loss: 1.420577076765207, Training Accuracy: 60.392\n",
            "Validation Loss: 1.8663844267527263, Validation Accuracy: 51.21\n",
            "[94/150]: Training Loss: 1.4505155269916241, Training Accuracy: 59.012\n",
            "Validation Loss: 1.8934478759765625, Validation Accuracy: 50.84\n",
            "[95/150]: Training Loss: 1.4003274349065928, Training Accuracy: 60.672\n",
            "Validation Loss: 1.8551263411839802, Validation Accuracy: 51.69\n",
            "[96/150]: Training Loss: 1.3484498354104848, Training Accuracy: 61.974\n",
            "Validation Loss: 1.8562318483988445, Validation Accuracy: 51.96\n",
            "[97/150]: Training Loss: 1.314302426118117, Training Accuracy: 62.828\n",
            "Validation Loss: 1.8313268423080444, Validation Accuracy: 52.72\n",
            "[98/150]: Training Loss: 1.2976581683525672, Training Accuracy: 63.268\n",
            "Validation Loss: 1.8362118005752563, Validation Accuracy: 52.11\n",
            "[99/150]: Training Loss: 1.317265354670011, Training Accuracy: 62.994\n",
            "Validation Loss: 1.8350664774576824, Validation Accuracy: 52.02\n",
            "[100/150]: Training Loss: 1.2960394254097571, Training Accuracy: 63.262\n",
            "Validation Loss: 1.800865610440572, Validation Accuracy: 53.11\n",
            "[101/150]: Training Loss: 1.298880622937129, Training Accuracy: 62.896\n",
            "Validation Loss: 1.883919596672058, Validation Accuracy: 51.53\n",
            "[102/150]: Training Loss: 1.2974212628144484, Training Accuracy: 62.852\n",
            "Validation Loss: 1.8424270550409954, Validation Accuracy: 52.02\n",
            "[103/150]: Training Loss: 1.256823787322411, Training Accuracy: 63.952\n",
            "Validation Loss: 1.8408455053965251, Validation Accuracy: 52.54\n",
            "[104/150]: Training Loss: 1.252828497153062, Training Accuracy: 64.42\n",
            "Validation Loss: 1.8253253698349, Validation Accuracy: 52.37\n",
            "[105/150]: Training Loss: 1.2491283691846407, Training Accuracy: 64.358\n",
            "Validation Loss: 1.8558521668116252, Validation Accuracy: 52.17\n",
            "[106/150]: Training Loss: 1.229967685846182, Training Accuracy: 64.958\n",
            "Validation Loss: 1.8309193849563599, Validation Accuracy: 52.71\n",
            "[107/150]: Training Loss: 1.2172498794702382, Training Accuracy: 65.282\n",
            "Validation Loss: 1.8394190073013306, Validation Accuracy: 52.48\n",
            "[108/150]: Training Loss: 1.207243589254526, Training Accuracy: 65.544\n",
            "Validation Loss: 1.82985524336497, Validation Accuracy: 52.3\n",
            "[109/150]: Training Loss: 1.1976150732774, Training Accuracy: 65.54\n",
            "Validation Loss: 1.8410567442576091, Validation Accuracy: 52.6\n",
            "[110/150]: Training Loss: 1.1797684522775502, Training Accuracy: 66.22\n",
            "Validation Loss: 1.828009049097697, Validation Accuracy: 53.27\n",
            "[111/150]: Training Loss: 1.1586786141762366, Training Accuracy: 66.72\n",
            "Validation Loss: 1.8170452117919922, Validation Accuracy: 53.66\n",
            "[112/150]: Training Loss: 1.171253332724938, Training Accuracy: 66.59\n",
            "Validation Loss: 1.8729171355565388, Validation Accuracy: 52.55\n",
            "[113/150]: Training Loss: 1.2010775346022387, Training Accuracy: 65.75\n",
            "Validation Loss: 1.8387389580408733, Validation Accuracy: 53.17\n",
            "[114/150]: Training Loss: 1.1887562825129583, Training Accuracy: 66.114\n",
            "Validation Loss: 1.8573131561279297, Validation Accuracy: 52.59\n",
            "[115/150]: Training Loss: 1.1683265062478871, Training Accuracy: 66.588\n",
            "Validation Loss: 1.8474773168563843, Validation Accuracy: 53.17\n",
            "[116/150]: Training Loss: 1.1644465373112605, Training Accuracy: 66.768\n",
            "Validation Loss: 1.8591439326604207, Validation Accuracy: 52.41\n",
            "[117/150]: Training Loss: 1.1479460184390728, Training Accuracy: 67.308\n",
            "Validation Loss: 1.8460925817489624, Validation Accuracy: 53.29\n",
            "[118/150]: Training Loss: 1.1312938928604126, Training Accuracy: 67.496\n",
            "Validation Loss: 1.8282337188720703, Validation Accuracy: 53.62\n",
            "[119/150]: Training Loss: 1.1176557632593007, Training Accuracy: 68.078\n",
            "Validation Loss: 1.8184215625127156, Validation Accuracy: 53.58\n",
            "[120/150]: Training Loss: 1.1109853432728694, Training Accuracy: 68.212\n",
            "Validation Loss: 1.8269612789154053, Validation Accuracy: 53.4\n",
            "[121/150]: Training Loss: 1.092060116621164, Training Accuracy: 68.912\n",
            "Validation Loss: 1.8181384801864624, Validation Accuracy: 53.59\n",
            "[122/150]: Training Loss: 1.0812291502952576, Training Accuracy: 69.18\n",
            "Validation Loss: 1.8252775271733601, Validation Accuracy: 53.6\n",
            "[123/150]: Training Loss: 1.0699465458209698, Training Accuracy: 69.43\n",
            "Validation Loss: 1.834611177444458, Validation Accuracy: 54.01\n",
            "[124/150]: Training Loss: 1.0735741762014537, Training Accuracy: 69.112\n",
            "Validation Loss: 1.8203496138254802, Validation Accuracy: 53.93\n",
            "[125/150]: Training Loss: 1.0661220917334924, Training Accuracy: 69.538\n",
            "Validation Loss: 1.828150749206543, Validation Accuracy: 53.85\n",
            "[126/150]: Training Loss: 1.059658169746399, Training Accuracy: 69.666\n",
            "Validation Loss: 1.8271387418111165, Validation Accuracy: 53.8\n",
            "[127/150]: Training Loss: 1.0502580862778883, Training Accuracy: 70.108\n",
            "Validation Loss: 1.824654181798299, Validation Accuracy: 53.91\n",
            "[128/150]: Training Loss: 1.045825692323538, Training Accuracy: 69.762\n",
            "Validation Loss: 1.828839858373006, Validation Accuracy: 54.18\n",
            "[129/150]: Training Loss: 1.041814657358023, Training Accuracy: 70.04\n",
            "Validation Loss: 1.8183866739273071, Validation Accuracy: 53.89\n",
            "[130/150]: Training Loss: 1.0349373588195214, Training Accuracy: 70.334\n",
            "Validation Loss: 1.817612926165263, Validation Accuracy: 54.08\n",
            "[131/150]: Training Loss: 1.0304582439936125, Training Accuracy: 70.738\n",
            "Validation Loss: 1.8261488676071167, Validation Accuracy: 54.44\n",
            "[132/150]: Training Loss: 1.0328317513832679, Training Accuracy: 70.262\n",
            "Validation Loss: 1.8170503377914429, Validation Accuracy: 54.11\n",
            "[133/150]: Training Loss: 1.0127528584920442, Training Accuracy: 71.006\n",
            "Validation Loss: 1.823221206665039, Validation Accuracy: 54.24\n",
            "[134/150]: Training Loss: 1.0145881175994873, Training Accuracy: 70.876\n",
            "Validation Loss: 1.8177992900212605, Validation Accuracy: 54.42\n",
            "[135/150]: Training Loss: 1.0154722057856047, Training Accuracy: 70.856\n",
            "Validation Loss: 1.8151982227961223, Validation Accuracy: 54.36\n",
            "[136/150]: Training Loss: 1.0053791770568261, Training Accuracy: 70.962\n",
            "Validation Loss: 1.815675139427185, Validation Accuracy: 54.52\n",
            "[137/150]: Training Loss: 1.0002271212064302, Training Accuracy: 71.094\n",
            "Validation Loss: 1.8183471361796062, Validation Accuracy: 54.62\n",
            "[138/150]: Training Loss: 1.003823459148407, Training Accuracy: 71.112\n",
            "Validation Loss: 1.8167388836542766, Validation Accuracy: 54.32\n",
            "[139/150]: Training Loss: 1.0023488723314726, Training Accuracy: 71.28\n",
            "Validation Loss: 1.8161654869715373, Validation Accuracy: 54.5\n",
            "[140/150]: Training Loss: 0.996692391542288, Training Accuracy: 71.326\n",
            "Validation Loss: 1.8168489535649617, Validation Accuracy: 54.36\n",
            "[141/150]: Training Loss: 0.9932475502674396, Training Accuracy: 71.382\n",
            "Validation Loss: 1.8148254950841267, Validation Accuracy: 54.41\n",
            "[142/150]: Training Loss: 0.9950209443385785, Training Accuracy: 71.754\n",
            "Validation Loss: 1.8171526590983074, Validation Accuracy: 54.22\n",
            "[143/150]: Training Loss: 0.9949191717001108, Training Accuracy: 71.676\n",
            "Validation Loss: 1.8166076342264812, Validation Accuracy: 54.39\n",
            "[144/150]: Training Loss: 0.9898046300961421, Training Accuracy: 71.61\n",
            "Validation Loss: 1.8168938557306926, Validation Accuracy: 54.34\n",
            "[145/150]: Training Loss: 0.9879593665783222, Training Accuracy: 71.72\n",
            "Validation Loss: 1.8167452017466228, Validation Accuracy: 54.36\n",
            "[146/150]: Training Loss: 0.9881623249787551, Training Accuracy: 71.81\n",
            "Validation Loss: 1.8165217638015747, Validation Accuracy: 54.38\n",
            "[147/150]: Training Loss: 0.9901151886353126, Training Accuracy: 71.69\n",
            "Validation Loss: 1.81660795211792, Validation Accuracy: 54.48\n",
            "[148/150]: Training Loss: 0.9835631480583777, Training Accuracy: 71.944\n",
            "Validation Loss: 1.8168546358744304, Validation Accuracy: 54.43\n",
            "[149/150]: Training Loss: 0.9820910050318792, Training Accuracy: 72.082\n",
            "Validation Loss: 1.8165034850438435, Validation Accuracy: 54.45\n",
            "[150/150]: Training Loss: 0.9884678217080923, Training Accuracy: 71.694\n",
            "Validation Loss: 1.8165420691172283, Validation Accuracy: 54.48\n",
            "**********************************************************************\n",
            "Test Loss: 1.8165420691172283, Test Accuracy: 54.48\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▂▁█</td></tr><tr><td>Test Loss</td><td>▇█▁</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▃▄▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▇▆▅▅▅▅▄▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>54.48</td></tr><tr><td>Test Loss</td><td>1.81654</td></tr><tr><td>Train Accuracy</td><td>71.694</td></tr><tr><td>Train Loss</td><td>0.98847</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=4096 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_024410-fcrci1zx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 8192, Learning rate: 16.970562748477143, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_030841-573n6uz6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6' target=\"_blank\">batch_size=8192 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.543815356034499, Training Accuracy: 1.934\n",
            "Validation Loss: 4.557299613952637, Validation Accuracy: 4.11\n",
            "[2/150]: Training Loss: 4.4831575613755446, Training Accuracy: 3.478\n",
            "Validation Loss: 4.407679557800293, Validation Accuracy: 3.73\n",
            "[3/150]: Training Loss: 4.383443318880522, Training Accuracy: 4.272\n",
            "Validation Loss: 4.315660317738851, Validation Accuracy: 4.56\n",
            "[4/150]: Training Loss: 4.753879473759578, Training Accuracy: 2.854\n",
            "Validation Loss: 4.617130438486735, Validation Accuracy: 1.55\n",
            "[5/150]: Training Loss: 4.834007776700533, Training Accuracy: 1.152\n",
            "Validation Loss: 4.623517354329427, Validation Accuracy: 1.21\n",
            "[6/150]: Training Loss: 4.748236729548528, Training Accuracy: 1.046\n",
            "Validation Loss: 4.616909344991048, Validation Accuracy: 1.0\n",
            "[7/150]: Training Loss: 4.6143631201524, Training Accuracy: 1.022\n",
            "Validation Loss: 4.6133114496866865, Validation Accuracy: 0.95\n",
            "[8/150]: Training Loss: 86586261001215.23, Training Accuracy: 1.076\n",
            "Validation Loss: 199153685277354.66, Validation Accuracy: 1.08\n",
            "[9/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[10/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[11/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[12/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[13/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[14/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[15/150]: Training Loss: inf, Training Accuracy: 1.006\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[16/150]: Training Loss: inf, Training Accuracy: 1.004\n",
            "Validation Loss: inf, Validation Accuracy: 0.99\n",
            "[17/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[18/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[19/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[20/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[21/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[22/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[23/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[24/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[25/150]: Training Loss: inf, Training Accuracy: 1.014\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[26/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[27/150]: Training Loss: inf, Training Accuracy: 0.998\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[28/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[29/150]: Training Loss: inf, Training Accuracy: 1.004\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[30/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[31/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[32/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[33/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3707833002946235e+34, Validation Accuracy: 1.04\n",
            "[34/150]: Training Loss: 7.178876870446214e+34, Training Accuracy: 1.03\n",
            "Validation Loss: 6.908918765509594e+34, Validation Accuracy: 1.04\n",
            "[35/150]: Training Loss: 7.0300555279696e+34, Training Accuracy: 1.006\n",
            "Validation Loss: 6.850623178707614e+34, Validation Accuracy: 1.04\n",
            "[36/150]: Training Loss: 7.161850738321898e+34, Training Accuracy: 1.024\n",
            "Validation Loss: 6.997600003222513e+34, Validation Accuracy: 1.04\n",
            "[37/150]: Training Loss: 7.295138713909261e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.183176128279674e+34, Validation Accuracy: 1.04\n",
            "[38/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.320277327395843e+34, Validation Accuracy: 1.04\n",
            "[39/150]: Training Loss: 7.652232993118385e+34, Training Accuracy: 1.024\n",
            "Validation Loss: 7.385754947129741e+34, Validation Accuracy: 1.04\n",
            "[40/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.395794805907017e+34, Validation Accuracy: 1.04\n",
            "[41/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.389329127611166e+34, Validation Accuracy: 1.0\n",
            "[42/150]: Training Loss: 7.64618030431646e+34, Training Accuracy: 1.012\n",
            "Validation Loss: 7.36814170130946e+34, Validation Accuracy: 1.04\n",
            "[43/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.351131744934992e+34, Validation Accuracy: 1.04\n",
            "[44/150]: Training Loss: inf, Training Accuracy: 1.026\n",
            "Validation Loss: 7.339262375838323e+34, Validation Accuracy: 1.04\n",
            "[45/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.334463460011365e+34, Validation Accuracy: 1.04\n",
            "[46/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.33473861281743e+34, Validation Accuracy: 1.04\n",
            "[47/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.342335768309189e+34, Validation Accuracy: 1.04\n",
            "[48/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.343183509648092e+34, Validation Accuracy: 1.04\n",
            "[49/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.353242350172637e+34, Validation Accuracy: 1.04\n",
            "[50/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.35132750451987e+34, Validation Accuracy: 1.04\n",
            "[51/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.338507727590375e+34, Validation Accuracy: 1.04\n",
            "[52/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.345233868470492e+34, Validation Accuracy: 1.04\n",
            "[53/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.35075210998961e+34, Validation Accuracy: 1.0\n",
            "[54/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.35572186154199e+34, Validation Accuracy: 1.04\n",
            "[55/150]: Training Loss: 7.620612957278016e+34, Training Accuracy: 1.026\n",
            "Validation Loss: 7.362447672304763e+34, Validation Accuracy: 1.04\n",
            "[56/150]: Training Loss: 7.605070410587243e+34, Training Accuracy: 1.012\n",
            "Validation Loss: 7.363398575313606e+34, Validation Accuracy: 1.04\n",
            "[57/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3600082701926835e+34, Validation Accuracy: 1.04\n",
            "[58/150]: Training Loss: 7.453089464225746e+34, Training Accuracy: 1.02\n",
            "Validation Loss: 7.363201660318024e+34, Validation Accuracy: 1.04\n",
            "[59/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.36800305202506e+34, Validation Accuracy: 1.04\n",
            "[60/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.36741461785972e+34, Validation Accuracy: 1.04\n",
            "[61/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.367097540150991e+34, Validation Accuracy: 1.04\n",
            "[62/150]: Training Loss: inf, Training Accuracy: 1.024\n",
            "Validation Loss: 7.360266091838198e+34, Validation Accuracy: 1.04\n",
            "[63/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.357579927011621e+34, Validation Accuracy: 1.04\n",
            "[64/150]: Training Loss: inf, Training Accuracy: 1.002\n",
            "Validation Loss: 7.3600130568941685e+34, Validation Accuracy: 1.04\n",
            "[65/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.361191245694225e+34, Validation Accuracy: 1.04\n",
            "[66/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.358085336664993e+34, Validation Accuracy: 1.04\n",
            "[67/150]: Training Loss: 7.58769795997563e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.343106922424328e+34, Validation Accuracy: 1.04\n",
            "[68/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.340167062419033e+34, Validation Accuracy: 1.04\n",
            "[69/150]: Training Loss: 7.539564832453689e+34, Training Accuracy: 1.01\n",
            "Validation Loss: 7.3399107263015645e+34, Validation Accuracy: 1.04\n",
            "[70/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.343239299479195e+34, Validation Accuracy: 1.04\n",
            "[71/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.344283460637665e+34, Validation Accuracy: 1.04\n",
            "[72/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3470638739659e+34, Validation Accuracy: 1.04\n",
            "[73/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.337876213111667e+34, Validation Accuracy: 1.04\n",
            "[74/150]: Training Loss: 7.520041527682127e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.339984672586578e+34, Validation Accuracy: 1.04\n",
            "[75/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.341706399593216e+34, Validation Accuracy: 1.04\n",
            "[76/150]: Training Loss: 7.50774223153243e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.344978357646383e+34, Validation Accuracy: 1.04\n",
            "[77/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.34611412636776e+34, Validation Accuracy: 1.04\n",
            "[78/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.34531260145699e+34, Validation Accuracy: 1.04\n",
            "[79/150]: Training Loss: 7.570326728258824e+34, Training Accuracy: 1.008\n",
            "Validation Loss: 7.35118538900336e+34, Validation Accuracy: 1.04\n",
            "[80/150]: Training Loss: 7.392040813047783e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.351906530340912e+34, Validation Accuracy: 1.04\n",
            "[81/150]: Training Loss: 7.55008671334021e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.357009484241519e+34, Validation Accuracy: 1.04\n",
            "[82/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.355688354631593e+34, Validation Accuracy: 1.04\n",
            "[83/150]: Training Loss: inf, Training Accuracy: 1.024\n",
            "Validation Loss: 7.351709120169314e+34, Validation Accuracy: 1.04\n",
            "[84/150]: Training Loss: 7.518704819072938e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.352724726177544e+34, Validation Accuracy: 1.04\n",
            "[85/150]: Training Loss: 7.480363492538042e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.353025628136426e+34, Validation Accuracy: 1.04\n",
            "[86/150]: Training Loss: 7.609761708160119e+34, Training Accuracy: 1.02\n",
            "Validation Loss: 7.355622331162831e+34, Validation Accuracy: 1.04\n",
            "[87/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.351417956672074e+34, Validation Accuracy: 1.04\n",
            "[88/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.3432506885275565e+34, Validation Accuracy: 1.04\n",
            "[89/150]: Training Loss: 7.571858879032369e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.342947970923284e+34, Validation Accuracy: 1.04\n",
            "[90/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.341380243657533e+34, Validation Accuracy: 1.04\n",
            "[91/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3435405315554215e+34, Validation Accuracy: 1.04\n",
            "[92/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.347343978532122e+34, Validation Accuracy: 1.04\n",
            "[93/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.349835869301868e+34, Validation Accuracy: 1.04\n",
            "[94/150]: Training Loss: 7.553363102583877e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.350179851574117e+34, Validation Accuracy: 1.04\n",
            "[95/150]: Training Loss: 7.518843468357338e+34, Training Accuracy: 1.006\n",
            "Validation Loss: 7.349381297719442e+34, Validation Accuracy: 1.04\n",
            "[96/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.350629966572401e+34, Validation Accuracy: 1.04\n",
            "[97/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.350986328245043e+34, Validation Accuracy: 1.04\n",
            "[98/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.349232414797383e+34, Validation Accuracy: 1.04\n",
            "[99/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.3498634341000755e+34, Validation Accuracy: 1.04\n",
            "[100/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.346068900291658e+34, Validation Accuracy: 1.04\n",
            "[101/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.343747350071318e+34, Validation Accuracy: 1.04\n",
            "[102/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.343838462458209e+34, Validation Accuracy: 1.04\n",
            "[103/150]: Training Loss: 7.587162712793105e+34, Training Accuracy: 1.036\n",
            "Validation Loss: 7.34444604342949e+34, Validation Accuracy: 1.04\n",
            "[104/150]: Training Loss: 7.563224723389138e+34, Training Accuracy: 1.024\n",
            "Validation Loss: 7.3440896817568485e+34, Validation Accuracy: 1.04\n",
            "[105/150]: Training Loss: 7.502001541710868e+34, Training Accuracy: 1.012\n",
            "Validation Loss: 7.343770788402729e+34, Validation Accuracy: 1.04\n",
            "[106/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.344186901314601e+34, Validation Accuracy: 1.04\n",
            "[107/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.347785180362123e+34, Validation Accuracy: 1.04\n",
            "[108/150]: Training Loss: 7.541743759284682e+34, Training Accuracy: 1.008\n",
            "Validation Loss: 7.349218714927616e+34, Validation Accuracy: 1.04\n",
            "[109/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.350196357441308e+34, Validation Accuracy: 1.04\n",
            "[110/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.35085609695291e+34, Validation Accuracy: 1.04\n",
            "[111/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.3504860354105e+34, Validation Accuracy: 1.04\n",
            "[112/150]: Training Loss: inf, Training Accuracy: 1.006\n",
            "Validation Loss: 7.350065630973159e+34, Validation Accuracy: 1.04\n",
            "[113/150]: Training Loss: 7.572177010577234e+34, Training Accuracy: 1.014\n",
            "Validation Loss: 7.34878494073785e+34, Validation Accuracy: 1.04\n",
            "[114/150]: Training Loss: 7.602740036076367e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.345721781904642e+34, Validation Accuracy: 1.04\n",
            "[115/150]: Training Loss: 7.637903627666111e+34, Training Accuracy: 1.014\n",
            "Validation Loss: 7.342419287997172e+34, Validation Accuracy: 1.04\n",
            "[116/150]: Training Loss: 7.578869428701e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.340205025913571e+34, Validation Accuracy: 1.04\n",
            "[117/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.33890419852029e+34, Validation Accuracy: 1.04\n",
            "[118/150]: Training Loss: 7.53878203535377e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.3400515213487e+34, Validation Accuracy: 1.04\n",
            "[119/150]: Training Loss: 7.55180291722975e+34, Training Accuracy: 1.006\n",
            "Validation Loss: 7.341673717976179e+34, Validation Accuracy: 1.04\n",
            "[120/150]: Training Loss: 7.52984426063198e+34, Training Accuracy: 1.014\n",
            "Validation Loss: 7.341424314322931e+34, Validation Accuracy: 1.04\n",
            "[121/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.342221382649559e+34, Validation Accuracy: 1.04\n",
            "[122/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.341639550831095e+34, Validation Accuracy: 1.04\n",
            "[123/150]: Training Loss: 7.564221817432473e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.34171366217478e+34, Validation Accuracy: 1.04\n",
            "[124/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.341365883553077e+34, Validation Accuracy: 1.04\n",
            "[125/150]: Training Loss: 7.467476193914916e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.341843068173553e+34, Validation Accuracy: 1.04\n",
            "[126/150]: Training Loss: 7.557965611288089e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.34209494770688e+34, Validation Accuracy: 1.04\n",
            "[127/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.342893501561555e+34, Validation Accuracy: 1.04\n",
            "[128/150]: Training Loss: inf, Training Accuracy: 1.014\n",
            "Validation Loss: 7.343795217086171e+34, Validation Accuracy: 1.04\n",
            "[129/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3440480869715285e+34, Validation Accuracy: 1.04\n",
            "[130/150]: Training Loss: inf, Training Accuracy: 1.026\n",
            "Validation Loss: 7.343806276017188e+34, Validation Accuracy: 1.04\n",
            "[131/150]: Training Loss: 7.511219776509865e+34, Training Accuracy: 1.006\n",
            "Validation Loss: 7.343705425168654e+34, Validation Accuracy: 1.04\n",
            "[132/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.343869988664544e+34, Validation Accuracy: 1.04\n",
            "[133/150]: Training Loss: 7.572846983726496e+34, Training Accuracy: 1.004\n",
            "Validation Loss: 7.34368512295201e+34, Validation Accuracy: 1.04\n",
            "[134/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.3437027842299035e+34, Validation Accuracy: 1.04\n",
            "[135/150]: Training Loss: inf, Training Accuracy: 1.006\n",
            "Validation Loss: 7.34337316206211e+34, Validation Accuracy: 1.04\n",
            "[136/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.343067143284399e+34, Validation Accuracy: 1.04\n",
            "[137/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.343213385267707e+34, Validation Accuracy: 1.04\n",
            "[138/150]: Training Loss: 7.591718865404154e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.343415086964774e+34, Validation Accuracy: 1.04\n",
            "[139/150]: Training Loss: inf, Training Accuracy: 1.024\n",
            "Validation Loss: 7.343418718255556e+34, Validation Accuracy: 1.04\n",
            "[140/150]: Training Loss: 7.5901811534230485e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.343325625164602e+34, Validation Accuracy: 1.04\n",
            "[141/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.343275777445687e+34, Validation Accuracy: 1.04\n",
            "[142/150]: Training Loss: inf, Training Accuracy: 1.014\n",
            "Validation Loss: 7.3432812243818595e+34, Validation Accuracy: 1.04\n",
            "[143/150]: Training Loss: 7.521443789977704e+34, Training Accuracy: 1.008\n",
            "Validation Loss: 7.343387192049222e+34, Validation Accuracy: 1.04\n",
            "[144/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.343500752415492e+34, Validation Accuracy: 1.04\n",
            "[145/150]: Training Loss: 7.489635587251364e+34, Training Accuracy: 1.014\n",
            "Validation Loss: 7.343563144593472e+34, Validation Accuracy: 1.04\n",
            "[146/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.343603253850745e+34, Validation Accuracy: 1.04\n",
            "[147/150]: Training Loss: 7.537443193678667e+34, Training Accuracy: 1.026\n",
            "Validation Loss: 7.343630983707625e+34, Validation Accuracy: 1.04\n",
            "[148/150]: Training Loss: 7.421953062990879e+34, Training Accuracy: 1.024\n",
            "Validation Loss: 7.343639566758564e+34, Validation Accuracy: 1.04\n",
            "[149/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.343644683577394e+34, Validation Accuracy: 1.04\n",
            "[150/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.343644848636065e+34, Validation Accuracy: 1.04\n",
            "**********************************************************************\n",
            "Test Loss: 7.343644848636065e+34, Test Accuracy: 1.04\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁██</td></tr><tr><td>Test Loss</td><td>█▅▁</td></tr><tr><td>Train Accuracy</td><td>█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Loss</td><td>▁▁▁      █ █   █   ████    ██ ██ █  █   </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.04</td></tr><tr><td>Test Loss</td><td>7.343644848636065e+34</td></tr><tr><td>Train Accuracy</td><td>1.01</td></tr><tr><td>Train Loss</td><td>inf</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=8192 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_030841-573n6uz6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 16384, Learning rate: 24.0, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_033254-cneyt8y0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0' target=\"_blank\">batch_size=16384 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.5802256144010105, Training Accuracy: 1.524\n",
            "Validation Loss: 4.505903720855713, Validation Accuracy: 3.24\n",
            "[2/150]: Training Loss: 4.487952305720403, Training Accuracy: 2.782\n",
            "Validation Loss: 4.534458001454671, Validation Accuracy: 1.82\n",
            "[3/150]: Training Loss: 4.498784615443303, Training Accuracy: 2.232\n",
            "Validation Loss: 4.477282365163167, Validation Accuracy: 2.08\n",
            "[4/150]: Training Loss: 4.4941191306481, Training Accuracy: 2.178\n",
            "Validation Loss: 4.612959861755371, Validation Accuracy: 1.29\n",
            "[5/150]: Training Loss: 4.5912819642287035, Training Accuracy: 1.504\n",
            "Validation Loss: 5.598563989003499, Validation Accuracy: 2.19\n",
            "[6/150]: Training Loss: 5.028410141284649, Training Accuracy: 1.632\n",
            "Validation Loss: 7.806542873382568, Validation Accuracy: 1.07\n",
            "[7/150]: Training Loss: 14089.29747926272, Training Accuracy: 1.044\n",
            "Validation Loss: 7.282219409942627, Validation Accuracy: 0.93\n",
            "[8/150]: Training Loss: 461533512.8043683, Training Accuracy: 0.978\n",
            "Validation Loss: 145186464.0, Validation Accuracy: 0.98\n",
            "[9/150]: Training Loss: 1.1580541048823784e+29, Training Accuracy: 0.952\n",
            "Validation Loss: 2.748474842936338e+29, Validation Accuracy: 1.0\n",
            "[10/150]: Training Loss: 2.8978003478935013e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7484746540416787e+29, Validation Accuracy: 1.0\n",
            "[11/150]: Training Loss: 2.828666588105865e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.748474402182133e+29, Validation Accuracy: 1.0\n",
            "[12/150]: Training Loss: 2.8943495766660634e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7484738354981552e+29, Validation Accuracy: 1.0\n",
            "[13/150]: Training Loss: 2.848512485818888e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.748472828059972e+29, Validation Accuracy: 1.0\n",
            "[14/150]: Training Loss: 2.8171856724252214e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.748468672377467e+29, Validation Accuracy: 1.0\n",
            "[15/150]: Training Loss: 2.9237725789099303e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7484595424689335e+29, Validation Accuracy: 1.0\n",
            "[16/150]: Training Loss: 2.8024521069540443e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.748430578621172e+29, Validation Accuracy: 1.0\n",
            "[17/150]: Training Loss: 2.818569224092193e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7483436870778873e+29, Validation Accuracy: 1.0\n",
            "[18/150]: Training Loss: 2.8163175416317275e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7480828865182598e+29, Validation Accuracy: 1.0\n",
            "[19/150]: Training Loss: 2.818046581630586e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7474053213752977e+29, Validation Accuracy: 1.0\n",
            "[20/150]: Training Loss: 2.8566013910414994e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7454263979594298e+29, Validation Accuracy: 1.0\n",
            "[21/150]: Training Loss: 2.8411686828451232e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7395975495271803e+29, Validation Accuracy: 1.0\n",
            "[22/150]: Training Loss: 2.8607326044228643e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.722675988156443e+29, Validation Accuracy: 1.0\n",
            "[23/150]: Training Loss: 2.7987878847615806e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.6741031117411477e+29, Validation Accuracy: 1.0\n",
            "[24/150]: Training Loss: 2.723279453314973e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.535316349808136e+29, Validation Accuracy: 1.0\n",
            "[25/150]: Training Loss: 2.573586814635347e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.1297902431238725e+29, Validation Accuracy: 1.0\n",
            "[26/150]: Training Loss: 2.051805262233517e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 1.0476803956799425e+29, Validation Accuracy: 1.0\n",
            "[27/150]: Training Loss: 8.450376531000046e+28, Training Accuracy: 1.0\n",
            "Validation Loss: 9.264404581011073e+18, Validation Accuracy: 1.0\n",
            "[28/150]: Training Loss: nan, Training Accuracy: 1.044\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[29/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[30/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[31/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[32/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[33/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[34/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[35/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[36/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[37/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[38/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[39/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[40/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[41/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[42/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[43/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[44/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[45/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[46/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[47/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[48/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[49/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[50/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[51/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[52/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[53/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[54/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[55/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[56/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[57/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[58/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[59/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[60/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[61/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[62/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[63/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[64/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[65/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[66/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[67/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[68/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[69/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[70/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[71/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[72/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[73/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[74/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[75/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[76/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[77/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[78/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[79/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[80/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[81/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[82/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[83/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[84/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[85/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[86/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[87/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[88/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[89/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[90/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[91/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[92/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[93/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[94/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[95/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[96/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[97/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[98/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[99/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[100/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[101/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[102/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[103/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[104/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[105/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[106/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[107/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[108/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[109/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[110/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[111/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[112/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[113/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[114/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[115/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[116/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[117/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[118/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[119/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[120/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[121/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[122/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[123/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[124/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[125/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[126/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[127/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[128/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[129/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[130/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[131/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[132/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[133/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[134/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[135/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[136/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[137/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[138/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[139/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[140/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[141/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[142/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[143/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[144/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[145/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[146/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[147/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[148/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[149/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[150/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "**********************************************************************\n",
            "Test Loss: nan, Test Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▃▁</td></tr><tr><td>Train Accuracy</td><td>█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Loss</td><td>▁▁▁████                                 </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.0</td></tr><tr><td>Test Loss</td><td>nan</td></tr><tr><td>Train Accuracy</td><td>1.0</td></tr><tr><td>Train Loss</td><td>nan</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=16384 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_033254-cneyt8y0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 32768, Learning rate: 33.941125496954285, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_035739-zqj7yms0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0' target=\"_blank\">batch_size=32768 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.600700745215783, Training Accuracy: 1.18\n",
            "Validation Loss: 4.5345486005147295, Validation Accuracy: 2.25\n",
            "[2/150]: Training Loss: 4.525542259216309, Training Accuracy: 2.194\n",
            "Validation Loss: 4.480770270029704, Validation Accuracy: 3.63\n",
            "[3/150]: Training Loss: 4.467245468726525, Training Accuracy: 3.348\n",
            "Validation Loss: 4.682004292805989, Validation Accuracy: 2.02\n",
            "[4/150]: Training Loss: 4.584587500645564, Training Accuracy: 2.316\n",
            "Validation Loss: 4.589324951171875, Validation Accuracy: 1.07\n",
            "[5/150]: Training Loss: 4.593533919407771, Training Accuracy: 1.128\n",
            "Validation Loss: 4.565906047821045, Validation Accuracy: 2.24\n",
            "[6/150]: Training Loss: 4.610266098609338, Training Accuracy: 1.65\n",
            "Validation Loss: 5.121236960093181, Validation Accuracy: 1.01\n",
            "[7/150]: Training Loss: 5.261244590465839, Training Accuracy: 1.226\n",
            "Validation Loss: 7.05544392267863, Validation Accuracy: 1.19\n",
            "[8/150]: Training Loss: 6.352289456587571, Training Accuracy: 1.176\n",
            "Validation Loss: 22.820095698038738, Validation Accuracy: 0.83\n",
            "[9/150]: Training Loss: 33.29087433448205, Training Accuracy: 0.906\n",
            "Validation Loss: 54.22376505533854, Validation Accuracy: 1.0\n",
            "[10/150]: Training Loss: 35.99252113929162, Training Accuracy: 0.988\n",
            "Validation Loss: 69.11057027180989, Validation Accuracy: 0.95\n",
            "[11/150]: Training Loss: 76.55324378380409, Training Accuracy: 1.196\n",
            "Validation Loss: 7.983708381652832, Validation Accuracy: 1.06\n",
            "[12/150]: Training Loss: 58.39160236945519, Training Accuracy: 1.032\n",
            "Validation Loss: 164.86418660481772, Validation Accuracy: 0.92\n",
            "[13/150]: Training Loss: 131.58557803814227, Training Accuracy: 0.95\n",
            "Validation Loss: 281.80524190266925, Validation Accuracy: 1.0\n",
            "[14/150]: Training Loss: 178.63248865421002, Training Accuracy: 0.962\n",
            "Validation Loss: 7817948401325397.0, Validation Accuracy: 1.39\n",
            "[15/150]: Training Loss: 4967409836682319.0, Training Accuracy: 1.34\n",
            "Validation Loss: 2.8247563537114726e+17, Validation Accuracy: 1.0\n",
            "[16/150]: Training Loss: 1.7259852552660077e+17, Training Accuracy: 0.982\n",
            "Validation Loss: 7061819945735509.0, Validation Accuracy: 0.96\n",
            "[17/150]: Training Loss: 7.113184439761936e+16, Training Accuracy: 0.924\n",
            "Validation Loss: 64496.6328125, Validation Accuracy: 1.0\n",
            "[18/150]: Training Loss: nan, Training Accuracy: 1.014\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[19/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[20/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[21/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[22/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[23/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[24/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[25/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[26/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[27/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[28/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[29/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[30/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[31/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[32/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[33/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[34/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[35/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[36/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[37/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[38/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[39/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[40/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[41/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[42/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[43/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[44/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[45/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[46/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[47/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[48/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[49/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[50/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[51/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[52/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[53/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[54/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[55/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[56/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[57/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[58/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[59/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[60/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[61/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[62/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[63/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[64/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[65/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[66/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[67/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[68/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[69/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[70/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[71/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[72/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[73/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[74/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[75/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[76/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[77/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[78/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[79/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[80/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[81/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[82/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[83/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[84/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[85/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[86/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[87/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[88/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[89/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[90/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[91/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[92/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[93/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[94/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[95/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[96/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[97/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[98/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[99/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[100/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[101/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[102/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[103/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[104/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[105/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[106/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[107/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[108/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[109/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[110/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[111/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[112/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[113/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[114/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[115/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[116/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[117/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[118/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[119/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[120/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[121/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[122/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[123/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[124/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[125/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[126/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[127/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[128/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[129/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[130/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[131/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[132/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[133/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[134/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[135/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[136/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[137/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[138/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[139/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[140/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[141/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[142/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[143/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[144/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[145/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[146/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[147/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[148/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[149/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[150/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "**********************************************************************\n",
            "Test Loss: nan, Test Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▃▁</td></tr><tr><td>Train Accuracy</td><td>▇█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Loss</td><td>▁▁▁▁█                                   </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.0</td></tr><tr><td>Test Loss</td><td>nan</td></tr><tr><td>Train Accuracy</td><td>1.0</td></tr><tr><td>Train Loss</td><td>nan</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=32768 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_035739-zqj7yms0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 1.5\n",
        "wd = 1e-03\n",
        "batch_sizes = [512, 1024, 2048, 4096, 8192, 16384 , 32768]\n",
        "learning_rates = [lr * (batch_size / 64.0) ** 0.5 for batch_size in batch_sizes] # Root square scale-up of learning rate\n",
        "\n",
        "\n",
        "for i, batch_size in enumerate(batch_sizes):\n",
        "\n",
        "  print('='*50)\n",
        "  print(f'Batch size: {batch_size}, Weight decay: {wd}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {\n",
        "    'batch_size': batch_size,\n",
        "    'learning_rate': learning_rates[i],\n",
        "    'weight_decay' : wd\n",
        "  }\n",
        "  if batch_size <= 4096:\n",
        "  \n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= batch_size)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LARS(model.parameters(), lr= learning_rates[i], weight_decay=wd, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LARS_Large_Batches',\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True\n",
        "    )\n",
        "  else:\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= 4096)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LARS(model.parameters(), lr= learning_rates[i], weight_decay=wd, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    accumulation_steps = batch_size // 4096\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LARS_Large_Batches',\n",
        "        accumulation_steps=accumulation_steps,\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LAMB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from typing import Optional, Union, Callable, List\n",
        "\n",
        "class LAMB(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements LAMB (Layer-wise Adaptive Moments) optimizer.\n",
        "\n",
        "    Args:\n",
        "        params (iterable): Iterable of parameters to optimize or dicts defining parameter groups.\n",
        "        learning_rate (Union[float, Callable], optional): The learning rate. Default is 0.001.\n",
        "        beta_1 (float, optional): The exponential decay rate for the 1st moment estimates. Default is 0.9.\n",
        "        beta_2 (float, optional): The exponential decay rate for the 2nd moment estimates. Default is 0.999.\n",
        "        epsilon (float, optional): A small constant for numerical stability. Default is 1e-6.\n",
        "        weight_decay (float, optional): Weight decay. Default is 0.0.\n",
        "        exclude_from_weight_decay (Optional[List[str]], optional): List of regex patterns of variables excluded from weight decay. Variables whose name contain a substring matching the pattern will be excluded. Default is None.\n",
        "        exclude_from_layer_adaptation (Optional[List[str]], optional): List of regex patterns of variables excluded from layer adaptation. Variables whose name contain a substring matching the pattern will be excluded. Default is None.\n",
        "        name (str, optional): Optional name for the operations created when applying gradients. Defaults to \"LAMB\".\n",
        "        **kwargs: Keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip gradients by value, `decay` is included for backward compatibility to allow time inverse decay of learning rate. `lr` is included for backward compatibility, recommended to use `learning_rate` instead.\n",
        "\n",
        "    Note:\n",
        "        - If \"weight_decay_rate\" is found in kwargs, it will be renamed to \"weight_decay\", and will be deprecated in Addons 0.18.\n",
        "        - If exclude_from_layer_adaptation is None, it will be set to exclude_from_weight_decay.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr: Union[float, Callable] = 0.001,\n",
        "        beta_1: float = 0.9,\n",
        "        beta_2: float = 0.999,\n",
        "        epsilon: float = 1e-6,\n",
        "        wd: float = 0.0,\n",
        "        exclude_from_weight_decay: Optional[List[str]] = None,\n",
        "        exclude_from_layer_adaptation: Optional[List[str]] = None,\n",
        "        name: str = \"LAMB\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a new instance of the LAMB optimizer.\n",
        "        \"\"\"\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            betas=(beta_1, beta_2),\n",
        "            eps=epsilon,\n",
        "            wd=wd,\n",
        "            **kwargs)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if\n",
        "        # the arg is None.\n",
        "        if exclude_from_layer_adaptation:\n",
        "            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
        "        else:\n",
        "            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
        "\n",
        "    def _compute_update(self, p, grad, state, group):\n",
        "        \"\"\"\n",
        "        Computes the update for a given parameter.\n",
        "\n",
        "        Args:\n",
        "            p (Tensor): The parameter to be updated.\n",
        "            grad (Tensor): The gradient of the parameter.\n",
        "            state (dict): A dictionary containing information about the optimization state.\n",
        "            group (dict): A dictionary containing the optimization parameters.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The computed update for the parameter.\n",
        "        \"\"\"\n",
        "        # State initialization\n",
        "        if len(state) == 0:\n",
        "            state['step'] = 0\n",
        "            # Exponential moving average of gradient values\n",
        "            state['exp_avg'] = torch.zeros_like(p.data)\n",
        "            # Exponential moving average of squared gradient values\n",
        "            state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "        exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "        beta1, beta2 = group['betas']\n",
        "\n",
        "        state['step'] += 1\n",
        "\n",
        "        # Decay the first and second moment running average coefficient\n",
        "        exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "        exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "        denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "        update = exp_avg / denom\n",
        "\n",
        "        # LAMB layer-wise adaptation\n",
        "        r1 = p.data.pow(2).sum().sqrt()\n",
        "        r2 = update.pow(2).sum().sqrt()\n",
        "        r = torch.where(r1 == 0, torch.zeros_like(r1), r1 / r2)\n",
        "\n",
        "        return r * update\n",
        "\n",
        "    def _update_params(self, p, update, step_size, weight_decay):\n",
        "        \"\"\"\n",
        "        Updates the parameters with the computed update.\n",
        "\n",
        "        Args:\n",
        "            p (Tensor): The parameter to be updated.\n",
        "            update (Tensor): The computed update for the parameter.\n",
        "            step_size (float): The step size for the update.\n",
        "            weight_decay (float): The weight decay factor.\n",
        "        \"\"\"\n",
        "        if weight_decay != 0:\n",
        "            p.data.add_(-weight_decay * step_size, p.data)\n",
        "\n",
        "        p.data.add_(-step_size, update)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model and returns the loss. Default is None.\n",
        "        \"\"\"\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('LAMB does not support sparse gradients.')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                update = self._compute_update(p, grad, state, group)\n",
        "                self._update_params(p, update, group['lr'], group['weight_decay'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LAMB Hyperparameter Tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ftxikW3Lr2ya"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.0009 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_045009-kw5xm0f9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9' target=\"_blank\">learning_rate=0.0009 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.366185345458985, Training Accuracy: 3.775\n",
            "Validation Loss: 4.140061999582182, Validation Accuracy: 6.19\n",
            "[2/150]: Training Loss: 3.9872212955474855, Training Accuracy: 9.2825\n",
            "Validation Loss: 3.9175391182018693, Validation Accuracy: 10.13\n",
            "[3/150]: Training Loss: 3.7751855819702147, Training Accuracy: 12.73\n",
            "Validation Loss: 3.754828929901123, Validation Accuracy: 13.44\n",
            "[4/150]: Training Loss: 3.6031822479248046, Training Accuracy: 15.7675\n",
            "Validation Loss: 3.5609892857302525, Validation Accuracy: 16.33\n",
            "[5/150]: Training Loss: 3.4478089206695555, Training Accuracy: 18.66\n",
            "Validation Loss: 3.4119288389849816, Validation Accuracy: 19.45\n",
            "[6/150]: Training Loss: 3.3203197284698485, Training Accuracy: 20.7075\n",
            "Validation Loss: 3.349654642639646, Validation Accuracy: 20.47\n",
            "[7/150]: Training Loss: 3.20732957611084, Training Accuracy: 22.7475\n",
            "Validation Loss: 3.2386614349996967, Validation Accuracy: 22.79\n",
            "[8/150]: Training Loss: 3.115479539489746, Training Accuracy: 24.5875\n",
            "Validation Loss: 3.1584747019846726, Validation Accuracy: 24.53\n",
            "[9/150]: Training Loss: 3.0234272315979003, Training Accuracy: 26.28\n",
            "Validation Loss: 3.0975672788680737, Validation Accuracy: 24.97\n",
            "[10/150]: Training Loss: 2.9408509815216064, Training Accuracy: 27.665\n",
            "Validation Loss: 3.0618672537955507, Validation Accuracy: 25.33\n",
            "[11/150]: Training Loss: 2.862503829956055, Training Accuracy: 29.3325\n",
            "Validation Loss: 2.9638838965422027, Validation Accuracy: 27.41\n",
            "[12/150]: Training Loss: 2.790407749938965, Training Accuracy: 30.48\n",
            "Validation Loss: 2.937404105617742, Validation Accuracy: 28.32\n",
            "[13/150]: Training Loss: 2.724927402114868, Training Accuracy: 31.9425\n",
            "Validation Loss: 2.9041963838467932, Validation Accuracy: 28.5\n",
            "[14/150]: Training Loss: 2.659171875, Training Accuracy: 33.2275\n",
            "Validation Loss: 2.8527053769227044, Validation Accuracy: 29.6\n",
            "[15/150]: Training Loss: 2.604087335395813, Training Accuracy: 34.1075\n",
            "Validation Loss: 2.8513991179739593, Validation Accuracy: 30.15\n",
            "[16/150]: Training Loss: 2.5427039728164673, Training Accuracy: 35.57\n",
            "Validation Loss: 2.8172519510718668, Validation Accuracy: 30.73\n",
            "[17/150]: Training Loss: 2.4821195234298705, Training Accuracy: 36.73\n",
            "Validation Loss: 2.826693563704278, Validation Accuracy: 31.28\n",
            "[18/150]: Training Loss: 2.4249122804641723, Training Accuracy: 37.97\n",
            "Validation Loss: 2.78142261353268, Validation Accuracy: 32.01\n",
            "[19/150]: Training Loss: 2.366035011482239, Training Accuracy: 39.02\n",
            "Validation Loss: 2.7907780021618884, Validation Accuracy: 31.84\n",
            "[20/150]: Training Loss: 2.3027314464569093, Training Accuracy: 40.4925\n",
            "Validation Loss: 2.740103484718663, Validation Accuracy: 32.92\n",
            "[21/150]: Training Loss: 2.24520486240387, Training Accuracy: 41.56\n",
            "Validation Loss: 2.720843028111063, Validation Accuracy: 33.71\n",
            "[22/150]: Training Loss: 2.194983146095276, Training Accuracy: 42.9375\n",
            "Validation Loss: 2.7611854592705987, Validation Accuracy: 32.67\n",
            "[23/150]: Training Loss: 2.138135533905029, Training Accuracy: 44.1975\n",
            "Validation Loss: 2.7068749218230037, Validation Accuracy: 33.35\n",
            "[24/150]: Training Loss: 2.078735979270935, Training Accuracy: 45.3275\n",
            "Validation Loss: 2.6949741673317686, Validation Accuracy: 34.58\n",
            "[25/150]: Training Loss: 2.0237128454208375, Training Accuracy: 46.48\n",
            "Validation Loss: 2.700145792809262, Validation Accuracy: 33.95\n",
            "[26/150]: Training Loss: 1.9667144546508788, Training Accuracy: 47.8375\n",
            "Validation Loss: 2.749180986623096, Validation Accuracy: 34.02\n",
            "[27/150]: Training Loss: 1.9086554452896118, Training Accuracy: 49.35\n",
            "Validation Loss: 2.719582610828861, Validation Accuracy: 34.45\n",
            "[28/150]: Training Loss: 1.845149391746521, Training Accuracy: 50.64\n",
            "Validation Loss: 2.735584658422288, Validation Accuracy: 34.52\n",
            "[29/150]: Training Loss: 1.7947315074920653, Training Accuracy: 51.665\n",
            "Validation Loss: 2.7848551835224127, Validation Accuracy: 33.98\n",
            "[30/150]: Training Loss: 1.7361212677001954, Training Accuracy: 53.24\n",
            "Validation Loss: 2.7989868426778513, Validation Accuracy: 34.65\n",
            "[31/150]: Training Loss: 1.675447853088379, Training Accuracy: 54.63\n",
            "Validation Loss: 2.805454480420252, Validation Accuracy: 35.16\n",
            "[32/150]: Training Loss: 1.6181456073760987, Training Accuracy: 55.7275\n",
            "Validation Loss: 2.825571666097945, Validation Accuracy: 35.24\n",
            "[33/150]: Training Loss: 1.5572484771728516, Training Accuracy: 57.33\n",
            "Validation Loss: 2.895953078178843, Validation Accuracy: 34.87\n",
            "[34/150]: Training Loss: 1.493730352783203, Training Accuracy: 58.9\n",
            "Validation Loss: 2.8819164500874317, Validation Accuracy: 34.54\n",
            "[35/150]: Training Loss: 1.4407835181236268, Training Accuracy: 60.1325\n",
            "Validation Loss: 2.9298907556351583, Validation Accuracy: 35.13\n",
            "[36/150]: Training Loss: 1.378046295261383, Training Accuracy: 61.6725\n",
            "Validation Loss: 2.9877944615236514, Validation Accuracy: 34.88\n",
            "[37/150]: Training Loss: 1.3214307213783265, Training Accuracy: 63.3825\n",
            "Validation Loss: 3.01296287281498, Validation Accuracy: 34.97\n",
            "[38/150]: Training Loss: 1.2566315541267394, Training Accuracy: 64.46\n",
            "Validation Loss: 3.1075331208052908, Validation Accuracy: 34.96\n",
            "[39/150]: Training Loss: 1.2033088095664979, Training Accuracy: 66.1\n",
            "Validation Loss: 3.1721465040923684, Validation Accuracy: 34.01\n",
            "[40/150]: Training Loss: 1.1419879460334779, Training Accuracy: 67.7475\n",
            "Validation Loss: 3.260179937265481, Validation Accuracy: 34.51\n",
            "[41/150]: Training Loss: 1.0848600325584412, Training Accuracy: 69.2325\n",
            "Validation Loss: 3.281414782165722, Validation Accuracy: 34.07\n",
            "[42/150]: Training Loss: 1.0264957049369812, Training Accuracy: 70.73\n",
            "Validation Loss: 3.4151626255861514, Validation Accuracy: 34.15\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 17.67243060336751, Test Accuracy: 14.49\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▃█▁▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▂▃▃▃▃▃▃▃▂▃▃</td></tr><tr><td>Test Loss</td><td>▆▁▂▂▄▄▆█▇▇▆▇█▇▇▇▆▇▇▇▇▇▆▆▆▆▆▆▆▇▇▆▆▆▆▆▆▆▆▆</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>14.49</td></tr><tr><td>Test Loss</td><td>17.67243</td></tr><tr><td>Train Accuracy</td><td>70.73</td></tr><tr><td>Train Loss</td><td>1.0265</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.0009 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_045009-kw5xm0f9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.00095 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_045608-cwudz2wj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj' target=\"_blank\">learning_rate=0.00095 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.36243955039978, Training Accuracy: 3.8475\n",
            "Validation Loss: 4.152171432592307, Validation Accuracy: 6.23\n",
            "[2/150]: Training Loss: 3.988539717102051, Training Accuracy: 9.035\n",
            "Validation Loss: 3.8899211443153914, Validation Accuracy: 10.42\n",
            "[3/150]: Training Loss: 3.784552033996582, Training Accuracy: 12.44\n",
            "Validation Loss: 3.716481655266634, Validation Accuracy: 13.63\n",
            "[4/150]: Training Loss: 3.6115343948364256, Training Accuracy: 15.38\n",
            "Validation Loss: 3.575136796684022, Validation Accuracy: 15.25\n",
            "[5/150]: Training Loss: 3.464410584640503, Training Accuracy: 18.1625\n",
            "Validation Loss: 3.448981456695848, Validation Accuracy: 17.92\n",
            "[6/150]: Training Loss: 3.340861389923096, Training Accuracy: 20.2875\n",
            "Validation Loss: 3.3443257702384024, Validation Accuracy: 19.79\n",
            "[7/150]: Training Loss: 3.240240854263306, Training Accuracy: 22.0975\n",
            "Validation Loss: 3.2861741910314866, Validation Accuracy: 21.07\n",
            "[8/150]: Training Loss: 3.1493673683166503, Training Accuracy: 23.8725\n",
            "Validation Loss: 3.1970221844448408, Validation Accuracy: 22.88\n",
            "[9/150]: Training Loss: 3.058048994064331, Training Accuracy: 25.4\n",
            "Validation Loss: 3.149918096080707, Validation Accuracy: 23.89\n",
            "[10/150]: Training Loss: 2.980970076370239, Training Accuracy: 26.93\n",
            "Validation Loss: 3.05195216008812, Validation Accuracy: 25.52\n",
            "[11/150]: Training Loss: 2.8978964962005613, Training Accuracy: 28.4275\n",
            "Validation Loss: 3.035321120243923, Validation Accuracy: 26.01\n",
            "[12/150]: Training Loss: 2.828321962738037, Training Accuracy: 29.7825\n",
            "Validation Loss: 3.033455335410537, Validation Accuracy: 26.14\n",
            "[13/150]: Training Loss: 2.764588646316528, Training Accuracy: 31.0525\n",
            "Validation Loss: 2.9178414527018357, Validation Accuracy: 28.5\n",
            "[14/150]: Training Loss: 2.6963481563568115, Training Accuracy: 32.6275\n",
            "Validation Loss: 2.8597835826266342, Validation Accuracy: 29.48\n",
            "[15/150]: Training Loss: 2.6279782932281495, Training Accuracy: 33.5125\n",
            "Validation Loss: 2.820923639710542, Validation Accuracy: 30.76\n",
            "[16/150]: Training Loss: 2.567752400970459, Training Accuracy: 35.0025\n",
            "Validation Loss: 2.8166574232137886, Validation Accuracy: 30.67\n",
            "[17/150]: Training Loss: 2.5122242719650267, Training Accuracy: 36.11\n",
            "Validation Loss: 2.77275866611748, Validation Accuracy: 31.51\n",
            "[18/150]: Training Loss: 2.448093600654602, Training Accuracy: 37.37\n",
            "Validation Loss: 2.7573567454222663, Validation Accuracy: 31.87\n",
            "[19/150]: Training Loss: 2.390553472328186, Training Accuracy: 38.6925\n",
            "Validation Loss: 2.779167422823086, Validation Accuracy: 31.58\n",
            "[20/150]: Training Loss: 2.329650112724304, Training Accuracy: 39.8\n",
            "Validation Loss: 2.7296412940237933, Validation Accuracy: 32.77\n",
            "[21/150]: Training Loss: 2.2680850820541383, Training Accuracy: 41.2075\n",
            "Validation Loss: 2.7711537537301423, Validation Accuracy: 32.31\n",
            "[22/150]: Training Loss: 2.205855764579773, Training Accuracy: 42.4775\n",
            "Validation Loss: 2.710599136959975, Validation Accuracy: 33.08\n",
            "[23/150]: Training Loss: 2.153862490653992, Training Accuracy: 43.845\n",
            "Validation Loss: 2.6846724543601845, Validation Accuracy: 34.22\n",
            "[24/150]: Training Loss: 2.0879069725036623, Training Accuracy: 45.07\n",
            "Validation Loss: 2.6652191763470885, Validation Accuracy: 34.62\n",
            "[25/150]: Training Loss: 2.027932558250427, Training Accuracy: 46.0825\n",
            "Validation Loss: 2.7143203483265674, Validation Accuracy: 34.04\n",
            "[26/150]: Training Loss: 1.9607762811660767, Training Accuracy: 47.975\n",
            "Validation Loss: 2.6897484147624606, Validation Accuracy: 34.96\n",
            "[27/150]: Training Loss: 1.9115407361984253, Training Accuracy: 48.8575\n",
            "Validation Loss: 2.7138490722437574, Validation Accuracy: 34.41\n",
            "[28/150]: Training Loss: 1.8502281684875488, Training Accuracy: 50.4525\n",
            "Validation Loss: 2.699836605673383, Validation Accuracy: 35.02\n",
            "[29/150]: Training Loss: 1.790220089149475, Training Accuracy: 51.585\n",
            "Validation Loss: 2.7410409693505353, Validation Accuracy: 34.77\n",
            "[30/150]: Training Loss: 1.7332203540802003, Training Accuracy: 53.0625\n",
            "Validation Loss: 2.6978999216845083, Validation Accuracy: 35.51\n",
            "[31/150]: Training Loss: 1.6659312999725342, Training Accuracy: 54.48\n",
            "Validation Loss: 2.7799882926758688, Validation Accuracy: 35.39\n",
            "[32/150]: Training Loss: 1.6054936313629151, Training Accuracy: 55.9275\n",
            "Validation Loss: 2.783534232218554, Validation Accuracy: 35.38\n",
            "[33/150]: Training Loss: 1.5524930331230165, Training Accuracy: 57.2425\n",
            "Validation Loss: 2.7876193948612094, Validation Accuracy: 35.43\n",
            "[34/150]: Training Loss: 1.4864114666938781, Training Accuracy: 58.62\n",
            "Validation Loss: 2.8515792378954066, Validation Accuracy: 34.97\n",
            "[35/150]: Training Loss: 1.4250763600349425, Training Accuracy: 60.31\n",
            "Validation Loss: 2.89135210453325, Validation Accuracy: 34.83\n",
            "[36/150]: Training Loss: 1.3677435276031493, Training Accuracy: 61.51\n",
            "Validation Loss: 2.919705820691054, Validation Accuracy: 36.01\n",
            "[37/150]: Training Loss: 1.3075296778678893, Training Accuracy: 63.435\n",
            "Validation Loss: 3.000214212259669, Validation Accuracy: 35.23\n",
            "[38/150]: Training Loss: 1.2455093726158142, Training Accuracy: 64.7925\n",
            "Validation Loss: 3.0685715766469386, Validation Accuracy: 34.47\n",
            "[39/150]: Training Loss: 1.1875538174629212, Training Accuracy: 66.195\n",
            "Validation Loss: 3.0860758040361342, Validation Accuracy: 35.17\n",
            "[40/150]: Training Loss: 1.1235356809616088, Training Accuracy: 67.92\n",
            "Validation Loss: 3.142808488979461, Validation Accuracy: 35.12\n",
            "[41/150]: Training Loss: 1.0638824738502501, Training Accuracy: 69.31\n",
            "Validation Loss: 3.2775286580346954, Validation Accuracy: 35.6\n",
            "[42/150]: Training Loss: 1.0043726112365723, Training Accuracy: 71.1\n",
            "Validation Loss: 3.332795017084498, Validation Accuracy: 34.53\n",
            "[43/150]: Training Loss: 0.9537097842216492, Training Accuracy: 72.4\n",
            "Validation Loss: 3.416984627960594, Validation Accuracy: 34.84\n",
            "[44/150]: Training Loss: 0.8938642192840576, Training Accuracy: 74.145\n",
            "Validation Loss: 3.5402958104564886, Validation Accuracy: 34.94\n",
            "[45/150]: Training Loss: 0.8376890470981598, Training Accuracy: 75.4875\n",
            "Validation Loss: 3.6570517561238285, Validation Accuracy: 34.43\n",
            "[46/150]: Training Loss: 0.7843643071174622, Training Accuracy: 76.875\n",
            "Validation Loss: 3.745219696858886, Validation Accuracy: 34.67\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 24.405659256467395, Test Accuracy: 13.44\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>██▁▂▂▁▁▁▂▃▃▄▃▃▃▄▄▄▅▅▄▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>Test Loss</td><td>▄▁▄▇▆▇█▇▄▂▂▂▄▃▄▃▃▄▃▃▃▂▂▁▂▂▂▃▃▃▃▂▃▃▂▂▂▂▂▂</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>Train Loss</td><td>█▇▇▇▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>13.44</td></tr><tr><td>Test Loss</td><td>24.40566</td></tr><tr><td>Train Accuracy</td><td>76.875</td></tr><tr><td>Train Loss</td><td>0.78436</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.00095 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_045608-cwudz2wj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_050220-wgvfp3rx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx' target=\"_blank\">learning_rate=0.001 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.351185768127442, Training Accuracy: 3.94\n",
            "Validation Loss: 4.125218055810139, Validation Accuracy: 6.94\n",
            "[2/150]: Training Loss: 3.934326037979126, Training Accuracy: 9.675\n",
            "Validation Loss: 3.8352680054439863, Validation Accuracy: 11.53\n",
            "[3/150]: Training Loss: 3.6920593730926514, Training Accuracy: 14.25\n",
            "Validation Loss: 3.6194094821905636, Validation Accuracy: 15.04\n",
            "[4/150]: Training Loss: 3.5119912174224854, Training Accuracy: 17.235\n",
            "Validation Loss: 3.498675950773203, Validation Accuracy: 17.26\n",
            "[5/150]: Training Loss: 3.3595370391845703, Training Accuracy: 19.9025\n",
            "Validation Loss: 3.3320715184424334, Validation Accuracy: 20.65\n",
            "[6/150]: Training Loss: 3.2359037544250486, Training Accuracy: 22.135\n",
            "Validation Loss: 3.2282794827868226, Validation Accuracy: 21.83\n",
            "[7/150]: Training Loss: 3.1236792266845703, Training Accuracy: 24.1875\n",
            "Validation Loss: 3.1484436852157494, Validation Accuracy: 23.69\n",
            "[8/150]: Training Loss: 3.0264555549621583, Training Accuracy: 25.99\n",
            "Validation Loss: 3.0823915505864816, Validation Accuracy: 24.76\n",
            "[9/150]: Training Loss: 2.9406655250549316, Training Accuracy: 27.4325\n",
            "Validation Loss: 3.018990922126041, Validation Accuracy: 25.75\n",
            "[10/150]: Training Loss: 2.8602982753753663, Training Accuracy: 28.8825\n",
            "Validation Loss: 2.982261516486004, Validation Accuracy: 26.83\n",
            "[11/150]: Training Loss: 2.7865394050598145, Training Accuracy: 30.62\n",
            "Validation Loss: 2.881638531472273, Validation Accuracy: 28.86\n",
            "[12/150]: Training Loss: 2.713952407836914, Training Accuracy: 32.0525\n",
            "Validation Loss: 2.8773576226204063, Validation Accuracy: 28.51\n",
            "[13/150]: Training Loss: 2.6428218954086304, Training Accuracy: 33.4175\n",
            "Validation Loss: 2.852285245421586, Validation Accuracy: 29.44\n",
            "[14/150]: Training Loss: 2.5819866678237915, Training Accuracy: 34.2325\n",
            "Validation Loss: 2.8152025581165483, Validation Accuracy: 29.99\n",
            "[15/150]: Training Loss: 2.5168129930496215, Training Accuracy: 36.01\n",
            "Validation Loss: 2.7263220571408606, Validation Accuracy: 32.4\n",
            "[16/150]: Training Loss: 2.456671890640259, Training Accuracy: 37.025\n",
            "Validation Loss: 2.7360152697107596, Validation Accuracy: 31.89\n",
            "[17/150]: Training Loss: 2.394518960952759, Training Accuracy: 38.405\n",
            "Validation Loss: 2.693733840231683, Validation Accuracy: 33.43\n",
            "[18/150]: Training Loss: 2.329904039955139, Training Accuracy: 40.0125\n",
            "Validation Loss: 2.679514690569252, Validation Accuracy: 33.49\n",
            "[19/150]: Training Loss: 2.271217462730408, Training Accuracy: 41.0875\n",
            "Validation Loss: 2.7137394892941615, Validation Accuracy: 32.98\n",
            "[20/150]: Training Loss: 2.2047285720825194, Training Accuracy: 42.575\n",
            "Validation Loss: 2.6841572416815787, Validation Accuracy: 32.96\n",
            "[21/150]: Training Loss: 2.1471473873138427, Training Accuracy: 43.83\n",
            "Validation Loss: 2.6751675947456603, Validation Accuracy: 33.61\n",
            "[22/150]: Training Loss: 2.085570357322693, Training Accuracy: 45.055\n",
            "Validation Loss: 2.6481618797703153, Validation Accuracy: 34.55\n",
            "[23/150]: Training Loss: 2.0214510499954224, Training Accuracy: 46.6625\n",
            "Validation Loss: 2.638173457163914, Validation Accuracy: 35.26\n",
            "[24/150]: Training Loss: 1.9607698831558227, Training Accuracy: 48.02\n",
            "Validation Loss: 2.621039168849872, Validation Accuracy: 35.84\n",
            "[25/150]: Training Loss: 1.8981156831741333, Training Accuracy: 49.45\n",
            "Validation Loss: 2.62651793896013, Validation Accuracy: 36.22\n",
            "[26/150]: Training Loss: 1.8291910402297973, Training Accuracy: 51.235\n",
            "Validation Loss: 2.6563384396255394, Validation Accuracy: 35.62\n",
            "[27/150]: Training Loss: 1.7651165187835693, Training Accuracy: 52.31\n",
            "Validation Loss: 2.639584611935221, Validation Accuracy: 36.55\n",
            "[28/150]: Training Loss: 1.7005936141967772, Training Accuracy: 54.275\n",
            "Validation Loss: 2.7095768307424652, Validation Accuracy: 36.06\n",
            "[29/150]: Training Loss: 1.6383595853805542, Training Accuracy: 55.6225\n",
            "Validation Loss: 2.7259993150735355, Validation Accuracy: 35.64\n",
            "[30/150]: Training Loss: 1.5726151014328003, Training Accuracy: 57.1225\n",
            "Validation Loss: 2.7611397246646274, Validation Accuracy: 36.11\n",
            "[31/150]: Training Loss: 1.508023483467102, Training Accuracy: 58.3025\n",
            "Validation Loss: 2.7436208489594187, Validation Accuracy: 35.66\n",
            "[32/150]: Training Loss: 1.4352122190475465, Training Accuracy: 60.295\n",
            "Validation Loss: 2.8204059456564057, Validation Accuracy: 36.43\n",
            "[33/150]: Training Loss: 1.3806078368186951, Training Accuracy: 61.585\n",
            "Validation Loss: 2.876485233853577, Validation Accuracy: 34.85\n",
            "[34/150]: Training Loss: 1.3117412397384645, Training Accuracy: 63.325\n",
            "Validation Loss: 2.8764415660481544, Validation Accuracy: 36.16\n",
            "[35/150]: Training Loss: 1.2477456188201905, Training Accuracy: 64.815\n",
            "Validation Loss: 2.959255384032134, Validation Accuracy: 35.83\n",
            "[36/150]: Training Loss: 1.178989047718048, Training Accuracy: 66.845\n",
            "Validation Loss: 3.055402055667464, Validation Accuracy: 35.68\n",
            "[37/150]: Training Loss: 1.1195641567230226, Training Accuracy: 68.255\n",
            "Validation Loss: 3.092683092044417, Validation Accuracy: 35.74\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 19.867750665944094, Test Accuracy: 12.24\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>██▂▃▁▁▂▁▁▁▂▂▁▁▂▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>Test Loss</td><td>▂▁▂▃▅▅▆▇▇▆▆▆█▇██▇█▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>12.24</td></tr><tr><td>Test Loss</td><td>19.86775</td></tr><tr><td>Train Accuracy</td><td>68.255</td></tr><tr><td>Train Loss</td><td>1.11956</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_050220-wgvfp3rx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.0015 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_050736-gnfp4sxh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh' target=\"_blank\">learning_rate=0.0015 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.262849729156494, Training Accuracy: 4.9725\n",
            "Validation Loss: 3.9988777105975304, Validation Accuracy: 9.32\n",
            "[2/150]: Training Loss: 3.787733755874634, Training Accuracy: 12.5275\n",
            "Validation Loss: 3.666901140455987, Validation Accuracy: 14.13\n",
            "[3/150]: Training Loss: 3.5222632232666014, Training Accuracy: 16.975\n",
            "Validation Loss: 3.430799526773441, Validation Accuracy: 18.28\n",
            "[4/150]: Training Loss: 3.3261954177856445, Training Accuracy: 20.0275\n",
            "Validation Loss: 3.268884590476941, Validation Accuracy: 20.25\n",
            "[5/150]: Training Loss: 3.1651099575042725, Training Accuracy: 22.825\n",
            "Validation Loss: 3.140599861266507, Validation Accuracy: 23.63\n",
            "[6/150]: Training Loss: 3.0220409996032713, Training Accuracy: 25.57\n",
            "Validation Loss: 3.062152488975768, Validation Accuracy: 24.92\n",
            "[7/150]: Training Loss: 2.9046311878204345, Training Accuracy: 27.855\n",
            "Validation Loss: 2.9628464735237654, Validation Accuracy: 26.95\n",
            "[8/150]: Training Loss: 2.7910710647583006, Training Accuracy: 30.235\n",
            "Validation Loss: 2.8832454301749064, Validation Accuracy: 28.14\n",
            "[9/150]: Training Loss: 2.687530333709717, Training Accuracy: 32.2625\n",
            "Validation Loss: 2.8348786056421367, Validation Accuracy: 30.07\n",
            "[10/150]: Training Loss: 2.590290707015991, Training Accuracy: 34.1175\n",
            "Validation Loss: 2.7776184780582502, Validation Accuracy: 30.52\n",
            "[11/150]: Training Loss: 2.499223603057861, Training Accuracy: 36.12\n",
            "Validation Loss: 2.726046642680077, Validation Accuracy: 31.91\n",
            "[12/150]: Training Loss: 2.4020754039764403, Training Accuracy: 37.9725\n",
            "Validation Loss: 2.7060916757887337, Validation Accuracy: 32.51\n",
            "[13/150]: Training Loss: 2.302425242996216, Training Accuracy: 40.3075\n",
            "Validation Loss: 2.660059646436363, Validation Accuracy: 33.06\n",
            "[14/150]: Training Loss: 2.2182941759109496, Training Accuracy: 42.03\n",
            "Validation Loss: 2.647583461870813, Validation Accuracy: 34.15\n",
            "[15/150]: Training Loss: 2.1231776348114013, Training Accuracy: 43.9025\n",
            "Validation Loss: 2.6469775620539475, Validation Accuracy: 34.32\n",
            "[16/150]: Training Loss: 2.030972394180298, Training Accuracy: 45.9525\n",
            "Validation Loss: 2.7037209963342947, Validation Accuracy: 32.86\n",
            "[17/150]: Training Loss: 1.9431676984786987, Training Accuracy: 47.91\n",
            "Validation Loss: 2.5850457507333937, Validation Accuracy: 36.27\n",
            "[18/150]: Training Loss: 1.8313071418762208, Training Accuracy: 50.255\n",
            "Validation Loss: 2.699979861071155, Validation Accuracy: 35.5\n",
            "[19/150]: Training Loss: 1.7440621503829956, Training Accuracy: 52.4725\n",
            "Validation Loss: 2.661453550028953, Validation Accuracy: 35.96\n",
            "[20/150]: Training Loss: 1.637422308921814, Training Accuracy: 54.8325\n",
            "Validation Loss: 2.7208387426509977, Validation Accuracy: 35.5\n",
            "[21/150]: Training Loss: 1.5425473594665526, Training Accuracy: 57.03\n",
            "Validation Loss: 2.766743406368669, Validation Accuracy: 35.3\n",
            "[22/150]: Training Loss: 1.4425379981994628, Training Accuracy: 59.42\n",
            "Validation Loss: 2.7848968460301684, Validation Accuracy: 36.36\n",
            "[23/150]: Training Loss: 1.346246865272522, Training Accuracy: 61.82\n",
            "Validation Loss: 2.918702983552483, Validation Accuracy: 35.2\n",
            "[24/150]: Training Loss: 1.2415513612747193, Training Accuracy: 64.48\n",
            "Validation Loss: 3.0769596373199657, Validation Accuracy: 34.71\n",
            "[25/150]: Training Loss: 1.1484121152877809, Training Accuracy: 66.9425\n",
            "Validation Loss: 3.111480196570135, Validation Accuracy: 35.56\n",
            "[26/150]: Training Loss: 1.049374456501007, Training Accuracy: 69.2\n",
            "Validation Loss: 3.2441035076311437, Validation Accuracy: 35.12\n",
            "[27/150]: Training Loss: 0.9539197593688965, Training Accuracy: 71.64\n",
            "Validation Loss: 3.3398004808243673, Validation Accuracy: 34.83\n",
            "[28/150]: Training Loss: 0.8686957097053528, Training Accuracy: 73.885\n",
            "Validation Loss: 3.526861637261263, Validation Accuracy: 35.38\n",
            "[29/150]: Training Loss: 0.7777377708911896, Training Accuracy: 76.4\n",
            "Validation Loss: 3.74925019938475, Validation Accuracy: 34.95\n",
            "[30/150]: Training Loss: 0.6971015272140503, Training Accuracy: 78.4225\n",
            "Validation Loss: 3.921683767039305, Validation Accuracy: 34.49\n",
            "[31/150]: Training Loss: 0.6319020359992981, Training Accuracy: 80.37\n",
            "Validation Loss: 4.265976667404175, Validation Accuracy: 33.7\n",
            "[32/150]: Training Loss: 0.5592198015213012, Training Accuracy: 82.41\n",
            "Validation Loss: 4.3743809985507065, Validation Accuracy: 34.21\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 32.802743437943185, Test Accuracy: 15.7\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▅█▂▅▄▂▁▁▁▁▂▃▃▂▂▃▃▃▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▄▄▄▄▄▄</td></tr><tr><td>Test Loss</td><td>▁▄▇▆▇▇███▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>15.7</td></tr><tr><td>Test Loss</td><td>32.80274</td></tr><tr><td>Train Accuracy</td><td>82.41</td></tr><tr><td>Train Loss</td><td>0.55922</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.0015 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_050736-gnfp4sxh/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.002 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_051217-y92gs5m0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0' target=\"_blank\">learning_rate=0.002 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.222224911499024, Training Accuracy: 5.3975\n",
            "Validation Loss: 3.937301506662065, Validation Accuracy: 9.22\n",
            "[2/150]: Training Loss: 3.7261308227539063, Training Accuracy: 13.09\n",
            "Validation Loss: 3.593827167134376, Validation Accuracy: 15.34\n",
            "[3/150]: Training Loss: 3.4419474758148194, Training Accuracy: 18.2775\n",
            "Validation Loss: 3.3379262875599465, Validation Accuracy: 19.89\n",
            "[4/150]: Training Loss: 3.242010182952881, Training Accuracy: 21.865\n",
            "Validation Loss: 3.2397833553848754, Validation Accuracy: 21.54\n",
            "[5/150]: Training Loss: 3.074250465774536, Training Accuracy: 24.8525\n",
            "Validation Loss: 3.2218855049959414, Validation Accuracy: 22.29\n",
            "[6/150]: Training Loss: 2.9360343181610107, Training Accuracy: 27.3175\n",
            "Validation Loss: 2.9865669110778033, Validation Accuracy: 26.78\n",
            "[7/150]: Training Loss: 2.801424619293213, Training Accuracy: 30.0175\n",
            "Validation Loss: 2.881270451150882, Validation Accuracy: 28.82\n",
            "[8/150]: Training Loss: 2.688298546409607, Training Accuracy: 32.05\n",
            "Validation Loss: 2.8182313017025113, Validation Accuracy: 30.07\n",
            "[9/150]: Training Loss: 2.5692677375793456, Training Accuracy: 34.84\n",
            "Validation Loss: 2.7818179950592623, Validation Accuracy: 30.94\n",
            "[10/150]: Training Loss: 2.454072025489807, Training Accuracy: 37.1425\n",
            "Validation Loss: 2.741547807766374, Validation Accuracy: 32.13\n",
            "[11/150]: Training Loss: 2.341491235733032, Training Accuracy: 39.3875\n",
            "Validation Loss: 2.735630521349087, Validation Accuracy: 32.51\n",
            "[12/150]: Training Loss: 2.2293454483032225, Training Accuracy: 41.4725\n",
            "Validation Loss: 2.690371492106444, Validation Accuracy: 33.37\n",
            "[13/150]: Training Loss: 2.1135470266342162, Training Accuracy: 43.935\n",
            "Validation Loss: 2.7198572918108312, Validation Accuracy: 34.01\n",
            "[14/150]: Training Loss: 1.9979984771728516, Training Accuracy: 46.1675\n",
            "Validation Loss: 2.7182075719165195, Validation Accuracy: 34.3\n",
            "[15/150]: Training Loss: 1.8890921760559083, Training Accuracy: 49.265\n",
            "Validation Loss: 2.7245476724235873, Validation Accuracy: 34.8\n",
            "[16/150]: Training Loss: 1.7773400522232055, Training Accuracy: 51.53\n",
            "Validation Loss: 2.749075835677469, Validation Accuracy: 34.12\n",
            "[17/150]: Training Loss: 1.676476739501953, Training Accuracy: 53.69\n",
            "Validation Loss: 2.8863297662917216, Validation Accuracy: 34.72\n",
            "[18/150]: Training Loss: 1.5655937635421753, Training Accuracy: 56.19\n",
            "Validation Loss: 2.8975180911410385, Validation Accuracy: 35.07\n",
            "[19/150]: Training Loss: 1.4489710484504699, Training Accuracy: 59.165\n",
            "Validation Loss: 2.9350054689273715, Validation Accuracy: 34.48\n",
            "[20/150]: Training Loss: 1.3497021337509156, Training Accuracy: 61.3125\n",
            "Validation Loss: 3.0767477864672426, Validation Accuracy: 34.78\n",
            "[21/150]: Training Loss: 1.2329449357032776, Training Accuracy: 64.19\n",
            "Validation Loss: 3.22720639113408, Validation Accuracy: 33.72\n",
            "[22/150]: Training Loss: 1.1406824831962585, Training Accuracy: 66.7375\n",
            "Validation Loss: 3.406786183642734, Validation Accuracy: 33.27\n",
            "[23/150]: Training Loss: 1.044043209552765, Training Accuracy: 69.1625\n",
            "Validation Loss: 3.664752894905722, Validation Accuracy: 32.75\n",
            "[24/150]: Training Loss: 0.9549756371498108, Training Accuracy: 71.1525\n",
            "Validation Loss: 3.7399451838936777, Validation Accuracy: 32.73\n",
            "[25/150]: Training Loss: 0.8772646800518036, Training Accuracy: 73.25\n",
            "Validation Loss: 3.9628275534149946, Validation Accuracy: 33.22\n",
            "[26/150]: Training Loss: 0.7976884460449218, Training Accuracy: 75.4625\n",
            "Validation Loss: 4.143224073823091, Validation Accuracy: 32.26\n",
            "[27/150]: Training Loss: 0.7417024869441986, Training Accuracy: 76.86\n",
            "Validation Loss: 4.497031521645321, Validation Accuracy: 32.38\n",
            "[28/150]: Training Loss: 0.682380113697052, Training Accuracy: 78.7575\n",
            "Validation Loss: 4.677568254956774, Validation Accuracy: 32.16\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 35.528370875461846, Test Accuracy: 11.85\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▅▁▂▁▁▁▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▁▃▆▅▆▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>11.85</td></tr><tr><td>Test Loss</td><td>35.52837</td></tr><tr><td>Train Accuracy</td><td>78.7575</td></tr><tr><td>Train Loss</td><td>0.68238</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.002 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_051217-y92gs5m0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "learning_rates = [9e-04, 95e-05, 1e-03, 15e-04, 2e-03]\n",
        "wd = 4e-04\n",
        "for lr in learning_rates:\n",
        "\n",
        "  print('='*50)\n",
        "  print(f'Hyperparameter with lr:{lr} and wd:{wd}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {'learning_rate': lr,\n",
        "                      'weight_decay' : wd\n",
        "                      }\n",
        "  # Load the model\n",
        "  model = LeNet5().to(device)\n",
        "\n",
        "  # Optimizer and scheduler setup\n",
        "  optimizer = LAMB(model.parameters(), lr=lr, weight_decay=wd)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "  # Training\n",
        "  run_training(num_epochs,\n",
        "                model,\n",
        "                train_loader,\n",
        "                validation_loader,\n",
        "                test_loader,\n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                criterion,\n",
        "                device,\n",
        "                'LAMB-HyperParameterTuning',\n",
        "                hyperparameters=hyperparameters,\n",
        "                is_wandb = True,\n",
        "                n_epochs_stop = 10\n",
        "  )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LAMB BaseLine B-Size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "90b585318084475b9543b3226230d373",
            "5f3a33bc6a334c9e8c5886caa5f015ca",
            "46c9d83bf8af443bb376ed4858ce35f7",
            "2402e482be7e484fa98ae3f60f68d95a",
            "e8040d3a83de41319a2a836def914b1b",
            "0b4b10bb8bff4d40afa8df981440fe43",
            "3b0b3657d30547d7abe702d6c1e7b7c4",
            "17ac4efa9cf148e5be69edbc5b0bdecf",
            "f5fd8db807e144578a2e20762d7369d0",
            "32c759d7fff64701a6ad61b38ac63187",
            "557cf2de74314915b7204d9055fa6987",
            "c5bb24dc0f56483b88a0efbfa2a1c8ee",
            "67aa8f8c2b244c9a9a3bb11caf491247",
            "09b762f5ba784066a9408ffcfdea5b2e",
            "6a345ff7386643eeb7a5b190d69d2e0e",
            "8f4783e2060a4295bb68036a4a7310d9",
            "aa6aa819c1fd4776ac47af588aaaaacd",
            "64224c777f01418e904dcd30ecd7c5ef",
            "c1dbd12595384bc6a0240aaeb16014dc",
            "6aa92cf922724ac08c45385ff8a58447",
            "99929754d3094d03a5f311177ac26cb3",
            "9ea517158b974c5da17899e3fea4fc3b",
            "bb2ee36dd3c04926b10b832928d7d0a0",
            "9d88657b39fd4b169d61b1b8d240e6ff"
          ]
        },
        "id": "7sUsHkiHgURG",
        "outputId": "3829f79a-3d06-48ee-fea5-a00c18f8744b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:uqgfhn52) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁▄▆▇█</td></tr><tr><td>Train Loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>22.668</td></tr><tr><td>Train Loss</td><td>3.18342</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.0015026019100214134 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/uqgfhn52' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/uqgfhn52</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_051725-uqgfhn52/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:uqgfhn52). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_052010-62vz5e00</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00' target=\"_blank\">learning_rate=0.0015 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.170614353226274, Training Accuracy: 6.554\n",
            "Validation Loss: 3.8419871087286883, Validation Accuracy: 11.55\n",
            "[2/150]: Training Loss: 3.73113738697813, Training Accuracy: 13.326\n",
            "Validation Loss: 3.5141605601948536, Validation Accuracy: 17.24\n",
            "[3/150]: Training Loss: 3.4963422099037853, Training Accuracy: 17.148\n",
            "Validation Loss: 3.329459586720558, Validation Accuracy: 21.14\n",
            "[4/150]: Training Loss: 3.32537763838268, Training Accuracy: 20.17\n",
            "Validation Loss: 3.1594036445496188, Validation Accuracy: 23.38\n",
            "[5/150]: Training Loss: 3.1821046982274948, Training Accuracy: 22.724\n",
            "Validation Loss: 3.024734082495331, Validation Accuracy: 25.42\n",
            "[6/150]: Training Loss: 3.0576654736648132, Training Accuracy: 24.838\n",
            "Validation Loss: 2.917888655024729, Validation Accuracy: 27.48\n",
            "[7/150]: Training Loss: 2.9461568444586166, Training Accuracy: 26.996\n",
            "Validation Loss: 2.8047209317517128, Validation Accuracy: 29.77\n",
            "[8/150]: Training Loss: 2.860208951298843, Training Accuracy: 28.704\n",
            "Validation Loss: 2.7236505541831826, Validation Accuracy: 30.98\n",
            "[9/150]: Training Loss: 2.7667418916512023, Training Accuracy: 30.53\n",
            "Validation Loss: 2.6368031137308496, Validation Accuracy: 33.13\n",
            "[10/150]: Training Loss: 2.6879202682343895, Training Accuracy: 31.998\n",
            "Validation Loss: 2.536134794259527, Validation Accuracy: 35.32\n",
            "[11/150]: Training Loss: 2.61040174458033, Training Accuracy: 33.4\n",
            "Validation Loss: 2.4932953764678567, Validation Accuracy: 36.12\n",
            "[12/150]: Training Loss: 2.546117087306879, Training Accuracy: 34.894\n",
            "Validation Loss: 2.4478625689342524, Validation Accuracy: 37.12\n",
            "[13/150]: Training Loss: 2.4885423736803975, Training Accuracy: 35.89\n",
            "Validation Loss: 2.4252931342762745, Validation Accuracy: 37.55\n",
            "[14/150]: Training Loss: 2.4414472836057852, Training Accuracy: 37.026\n",
            "Validation Loss: 2.3621389410298343, Validation Accuracy: 39.2\n",
            "[15/150]: Training Loss: 2.3877986550636, Training Accuracy: 37.84\n",
            "Validation Loss: 2.3034424402151896, Validation Accuracy: 40.21\n",
            "[16/150]: Training Loss: 2.3379326612138382, Training Accuracy: 39.018\n",
            "Validation Loss: 2.272260831419829, Validation Accuracy: 41.24\n",
            "[17/150]: Training Loss: 2.300395258247395, Training Accuracy: 39.968\n",
            "Validation Loss: 2.2585707120834644, Validation Accuracy: 40.6\n",
            "[18/150]: Training Loss: 2.254751528315532, Training Accuracy: 40.976\n",
            "Validation Loss: 2.2186143770339384, Validation Accuracy: 42.07\n",
            "[19/150]: Training Loss: 2.2090765830805843, Training Accuracy: 42.026\n",
            "Validation Loss: 2.1868586600965756, Validation Accuracy: 42.61\n",
            "[20/150]: Training Loss: 2.183374183562101, Training Accuracy: 42.532\n",
            "Validation Loss: 2.184412774007032, Validation Accuracy: 42.58\n",
            "[21/150]: Training Loss: 2.1446489868566507, Training Accuracy: 43.298\n",
            "Validation Loss: 2.144960983543639, Validation Accuracy: 43.98\n",
            "[22/150]: Training Loss: 2.1038296910198144, Training Accuracy: 44.234\n",
            "Validation Loss: 2.14452546493263, Validation Accuracy: 44.01\n",
            "[23/150]: Training Loss: 2.0707620507311026, Training Accuracy: 44.894\n",
            "Validation Loss: 2.098228795513226, Validation Accuracy: 45.23\n",
            "[24/150]: Training Loss: 2.043932166093451, Training Accuracy: 45.518\n",
            "Validation Loss: 2.0813077680624215, Validation Accuracy: 45.38\n",
            "[25/150]: Training Loss: 2.0066164360021994, Training Accuracy: 46.274\n",
            "Validation Loss: 2.075542102953431, Validation Accuracy: 45.85\n",
            "[26/150]: Training Loss: 1.9791695739302184, Training Accuracy: 46.988\n",
            "Validation Loss: 2.0774263060016995, Validation Accuracy: 45.28\n",
            "[27/150]: Training Loss: 1.948626570232079, Training Accuracy: 47.584\n",
            "Validation Loss: 2.0519798697939344, Validation Accuracy: 46.31\n",
            "[28/150]: Training Loss: 1.9167836584398508, Training Accuracy: 48.236\n",
            "Validation Loss: 2.0201504807563344, Validation Accuracy: 46.88\n",
            "[29/150]: Training Loss: 1.8909891124271676, Training Accuracy: 48.752\n",
            "Validation Loss: 2.040525470569635, Validation Accuracy: 46.57\n",
            "[30/150]: Training Loss: 1.8726914470153087, Training Accuracy: 49.618\n",
            "Validation Loss: 2.0113253760489687, Validation Accuracy: 47.44\n",
            "[31/150]: Training Loss: 1.8444365990131408, Training Accuracy: 50.096\n",
            "Validation Loss: 2.0178289990516225, Validation Accuracy: 47.23\n",
            "[32/150]: Training Loss: 1.8157384923047117, Training Accuracy: 50.462\n",
            "Validation Loss: 1.9997166759648901, Validation Accuracy: 47.55\n",
            "[33/150]: Training Loss: 1.7931588938474046, Training Accuracy: 51.29\n",
            "Validation Loss: 1.9950028824958073, Validation Accuracy: 47.93\n",
            "[34/150]: Training Loss: 1.770590110355631, Training Accuracy: 51.824\n",
            "Validation Loss: 1.993164770162789, Validation Accuracy: 48.12\n",
            "[35/150]: Training Loss: 1.7467985225607976, Training Accuracy: 52.03\n",
            "Validation Loss: 1.9869080242837311, Validation Accuracy: 47.9\n",
            "[36/150]: Training Loss: 1.7260596163742377, Training Accuracy: 52.556\n",
            "Validation Loss: 1.9724081488931255, Validation Accuracy: 48.61\n",
            "[37/150]: Training Loss: 1.7053224419998696, Training Accuracy: 53.142\n",
            "Validation Loss: 1.9826935643603087, Validation Accuracy: 48.29\n",
            "[38/150]: Training Loss: 1.67785135681367, Training Accuracy: 53.82\n",
            "Validation Loss: 1.9470138079041888, Validation Accuracy: 49.88\n",
            "[39/150]: Training Loss: 1.6623152171254463, Training Accuracy: 54.074\n",
            "Validation Loss: 1.9769204634769706, Validation Accuracy: 48.87\n",
            "[40/150]: Training Loss: 1.6364040380853522, Training Accuracy: 54.652\n",
            "Validation Loss: 1.9554897744184847, Validation Accuracy: 49.57\n",
            "[41/150]: Training Loss: 1.6296177715291758, Training Accuracy: 54.87\n",
            "Validation Loss: 1.9755018660976629, Validation Accuracy: 49.42\n",
            "[42/150]: Training Loss: 1.6172699540319955, Training Accuracy: 55.052\n",
            "Validation Loss: 1.9689236112460968, Validation Accuracy: 49.6\n",
            "[43/150]: Training Loss: 1.5901159763793506, Training Accuracy: 55.706\n",
            "Validation Loss: 1.9709602108426914, Validation Accuracy: 49.39\n",
            "[44/150]: Training Loss: 1.5688080241917954, Training Accuracy: 56.494\n",
            "Validation Loss: 1.944002225140857, Validation Accuracy: 50.24\n",
            "[45/150]: Training Loss: 1.5468424783490808, Training Accuracy: 57.02\n",
            "Validation Loss: 1.9809717410688947, Validation Accuracy: 49.68\n",
            "[46/150]: Training Loss: 1.5288271441331605, Training Accuracy: 57.372\n",
            "Validation Loss: 1.9420389149599016, Validation Accuracy: 50.41\n",
            "[47/150]: Training Loss: 1.527083154453341, Training Accuracy: 57.472\n",
            "Validation Loss: 1.9570772792123685, Validation Accuracy: 49.71\n",
            "[48/150]: Training Loss: 1.5049020071773578, Training Accuracy: 57.818\n",
            "Validation Loss: 1.954159548328181, Validation Accuracy: 50.38\n",
            "[49/150]: Training Loss: 1.48557850992893, Training Accuracy: 58.67\n",
            "Validation Loss: 1.934345255232161, Validation Accuracy: 50.33\n",
            "[50/150]: Training Loss: 1.461377340052134, Training Accuracy: 58.866\n",
            "Validation Loss: 1.9670750221629052, Validation Accuracy: 50.09\n",
            "[51/150]: Training Loss: 1.4531892761397545, Training Accuracy: 59.148\n",
            "Validation Loss: 1.9439144878630426, Validation Accuracy: 50.57\n",
            "[52/150]: Training Loss: 1.4365854823528348, Training Accuracy: 59.4\n",
            "Validation Loss: 1.9623823484797387, Validation Accuracy: 50.49\n",
            "[53/150]: Training Loss: 1.4231898410393453, Training Accuracy: 59.886\n",
            "Validation Loss: 1.9759472274476555, Validation Accuracy: 50.14\n",
            "[54/150]: Training Loss: 1.408926703664653, Training Accuracy: 60.114\n",
            "Validation Loss: 1.924273559242297, Validation Accuracy: 51.21\n",
            "[55/150]: Training Loss: 1.3876726690613095, Training Accuracy: 60.616\n",
            "Validation Loss: 1.9458329692767684, Validation Accuracy: 50.63\n",
            "[56/150]: Training Loss: 1.375196378478004, Training Accuracy: 61.12\n",
            "Validation Loss: 1.9410056468028172, Validation Accuracy: 51.4\n",
            "[57/150]: Training Loss: 1.3586041162081082, Training Accuracy: 61.634\n",
            "Validation Loss: 1.9649552509283563, Validation Accuracy: 50.61\n",
            "[58/150]: Training Loss: 1.3581253530271828, Training Accuracy: 61.412\n",
            "Validation Loss: 1.9388997919240576, Validation Accuracy: 51.27\n",
            "[59/150]: Training Loss: 1.3380669855400729, Training Accuracy: 61.932\n",
            "Validation Loss: 1.9778220661126884, Validation Accuracy: 50.91\n",
            "[60/150]: Training Loss: 1.3257979125622898, Training Accuracy: 62.056\n",
            "Validation Loss: 1.9304483665782175, Validation Accuracy: 51.44\n",
            "[61/150]: Training Loss: 1.316665162896866, Training Accuracy: 62.458\n",
            "Validation Loss: 1.9412351808730204, Validation Accuracy: 51.61\n",
            "[62/150]: Training Loss: 1.2979835793947625, Training Accuracy: 62.832\n",
            "Validation Loss: 1.9522758213577756, Validation Accuracy: 51.6\n",
            "[63/150]: Training Loss: 1.2900130991130838, Training Accuracy: 63.092\n",
            "Validation Loss: 1.9639577113898696, Validation Accuracy: 51.23\n",
            "[64/150]: Training Loss: 1.2804708784955847, Training Accuracy: 63.436\n",
            "Validation Loss: 1.9787959422275518, Validation Accuracy: 51.42\n",
            "[65/150]: Training Loss: 1.255796139959789, Training Accuracy: 63.948\n",
            "Validation Loss: 1.9797948439409778, Validation Accuracy: 51.63\n",
            "[66/150]: Training Loss: 1.25389591042343, Training Accuracy: 64.23\n",
            "Validation Loss: 1.97708031554131, Validation Accuracy: 52.19\n",
            "[67/150]: Training Loss: 1.2357215599330795, Training Accuracy: 64.366\n",
            "Validation Loss: 1.9641305322100402, Validation Accuracy: 52.22\n",
            "[68/150]: Training Loss: 1.2240538022402303, Training Accuracy: 64.862\n",
            "Validation Loss: 1.9815536661512534, Validation Accuracy: 51.47\n",
            "[69/150]: Training Loss: 1.2087419308969736, Training Accuracy: 65.136\n",
            "Validation Loss: 1.979879951021474, Validation Accuracy: 51.77\n",
            "[70/150]: Training Loss: 1.1984577734604516, Training Accuracy: 65.552\n",
            "Validation Loss: 2.005889118856685, Validation Accuracy: 51.82\n",
            "[71/150]: Training Loss: 1.184933677506264, Training Accuracy: 66.026\n",
            "Validation Loss: 1.9805337274150483, Validation Accuracy: 51.89\n",
            "[72/150]: Training Loss: 1.1769923466398282, Training Accuracy: 65.964\n",
            "Validation Loss: 2.0164508561419834, Validation Accuracy: 51.97\n",
            "[73/150]: Training Loss: 1.1639809905720488, Training Accuracy: 66.116\n",
            "Validation Loss: 2.0093113024523306, Validation Accuracy: 51.75\n",
            "[74/150]: Training Loss: 1.1529584921077085, Training Accuracy: 66.55\n",
            "Validation Loss: 2.017468781987573, Validation Accuracy: 51.53\n",
            "[75/150]: Training Loss: 1.1453676276347216, Training Accuracy: 66.758\n",
            "Validation Loss: 2.011715882902692, Validation Accuracy: 51.48\n",
            "[76/150]: Training Loss: 1.1254654065574832, Training Accuracy: 67.396\n",
            "Validation Loss: 2.0016076625532406, Validation Accuracy: 51.93\n",
            "[77/150]: Training Loss: 1.1255306048161537, Training Accuracy: 67.168\n",
            "Validation Loss: 2.037281950567938, Validation Accuracy: 51.54\n",
            "[78/150]: Training Loss: 1.107588133391212, Training Accuracy: 67.898\n",
            "Validation Loss: 2.0207215995545598, Validation Accuracy: 52.25\n",
            "[79/150]: Training Loss: 1.0974091778478354, Training Accuracy: 67.988\n",
            "Validation Loss: 2.0399385941256383, Validation Accuracy: 52.27\n",
            "[80/150]: Training Loss: 1.0841643994726489, Training Accuracy: 68.482\n",
            "Validation Loss: 2.0396120669735467, Validation Accuracy: 51.88\n",
            "[81/150]: Training Loss: 1.086065025128367, Training Accuracy: 68.472\n",
            "Validation Loss: 2.0602660406926634, Validation Accuracy: 52.23\n",
            "[82/150]: Training Loss: 1.068733287741766, Training Accuracy: 68.792\n",
            "Validation Loss: 2.038099098357425, Validation Accuracy: 52.32\n",
            "[83/150]: Training Loss: 1.0567113834116466, Training Accuracy: 69.072\n",
            "Validation Loss: 2.0566319333519907, Validation Accuracy: 52.0\n",
            "[84/150]: Training Loss: 1.0568410671123154, Training Accuracy: 69.164\n",
            "Validation Loss: 2.031417142054078, Validation Accuracy: 52.23\n",
            "[85/150]: Training Loss: 1.0462324290019471, Training Accuracy: 69.518\n",
            "Validation Loss: 2.039376434247205, Validation Accuracy: 52.34\n",
            "[86/150]: Training Loss: 1.0314530159353905, Training Accuracy: 69.866\n",
            "Validation Loss: 2.0698205103540115, Validation Accuracy: 52.02\n",
            "[87/150]: Training Loss: 1.0182559825956363, Training Accuracy: 70.136\n",
            "Validation Loss: 2.063336102825821, Validation Accuracy: 52.83\n",
            "[88/150]: Training Loss: 1.010449574350396, Training Accuracy: 70.25\n",
            "Validation Loss: 2.077486888618226, Validation Accuracy: 52.3\n",
            "[89/150]: Training Loss: 1.0035706778316547, Training Accuracy: 70.786\n",
            "Validation Loss: 2.095621291998845, Validation Accuracy: 52.44\n",
            "[90/150]: Training Loss: 0.9991849294251494, Training Accuracy: 70.812\n",
            "Validation Loss: 2.069785719464539, Validation Accuracy: 52.4\n",
            "[91/150]: Training Loss: 0.993351522995078, Training Accuracy: 70.858\n",
            "Validation Loss: 2.1024031069627993, Validation Accuracy: 52.29\n",
            "[92/150]: Training Loss: 0.9726321048215222, Training Accuracy: 71.618\n",
            "Validation Loss: 2.084312925672835, Validation Accuracy: 52.53\n",
            "[93/150]: Training Loss: 0.97342309279515, Training Accuracy: 71.502\n",
            "Validation Loss: 2.0946866744642803, Validation Accuracy: 52.66\n",
            "[94/150]: Training Loss: 0.963803626920866, Training Accuracy: 71.718\n",
            "Validation Loss: 2.110511908105984, Validation Accuracy: 52.0\n",
            "[95/150]: Training Loss: 0.9580129440635672, Training Accuracy: 71.984\n",
            "Validation Loss: 2.111149025570815, Validation Accuracy: 52.13\n",
            "[96/150]: Training Loss: 0.9513878270869365, Training Accuracy: 72.204\n",
            "Validation Loss: 2.1115560136782894, Validation Accuracy: 52.52\n",
            "[97/150]: Training Loss: 0.938226883673607, Training Accuracy: 72.542\n",
            "Validation Loss: 2.1242388023692333, Validation Accuracy: 52.65\n",
            "[98/150]: Training Loss: 0.938509788811969, Training Accuracy: 72.476\n",
            "Validation Loss: 2.1135271635784467, Validation Accuracy: 52.82\n",
            "[99/150]: Training Loss: 0.9283110121326983, Training Accuracy: 72.674\n",
            "Validation Loss: 2.125285923860635, Validation Accuracy: 52.06\n",
            "[100/150]: Training Loss: 0.9165082442790956, Training Accuracy: 73.184\n",
            "Validation Loss: 2.1202975777304096, Validation Accuracy: 52.38\n",
            "[101/150]: Training Loss: 0.9109463012584335, Training Accuracy: 72.998\n",
            "Validation Loss: 2.136450658937928, Validation Accuracy: 52.42\n",
            "[102/150]: Training Loss: 0.9098046744418571, Training Accuracy: 73.26\n",
            "Validation Loss: 2.1267288725846893, Validation Accuracy: 52.86\n",
            "[103/150]: Training Loss: 0.8908002294237961, Training Accuracy: 73.838\n",
            "Validation Loss: 2.125086438883642, Validation Accuracy: 52.34\n",
            "[104/150]: Training Loss: 0.8947355315432219, Training Accuracy: 73.496\n",
            "Validation Loss: 2.13778414771815, Validation Accuracy: 52.62\n",
            "[105/150]: Training Loss: 0.8788276913830692, Training Accuracy: 74.106\n",
            "Validation Loss: 2.153025217876313, Validation Accuracy: 52.42\n",
            "[106/150]: Training Loss: 0.8798901442524112, Training Accuracy: 74.082\n",
            "Validation Loss: 2.1569059441803367, Validation Accuracy: 52.12\n",
            "[107/150]: Training Loss: 0.8666944346388282, Training Accuracy: 74.51\n",
            "Validation Loss: 2.1608687851839004, Validation Accuracy: 52.57\n",
            "[108/150]: Training Loss: 0.863055142485882, Training Accuracy: 74.528\n",
            "Validation Loss: 2.152739950805713, Validation Accuracy: 52.64\n",
            "[109/150]: Training Loss: 0.8662346141874943, Training Accuracy: 74.474\n",
            "Validation Loss: 2.14941197823567, Validation Accuracy: 52.71\n",
            "[110/150]: Training Loss: 0.8472307874342365, Training Accuracy: 75.102\n",
            "Validation Loss: 2.1724644390640746, Validation Accuracy: 52.69\n",
            "[111/150]: Training Loss: 0.8462071400469221, Training Accuracy: 75.26\n",
            "Validation Loss: 2.166171987345264, Validation Accuracy: 52.5\n",
            "[112/150]: Training Loss: 0.8393054546221442, Training Accuracy: 75.156\n",
            "Validation Loss: 2.16822886315121, Validation Accuracy: 52.79\n",
            "[113/150]: Training Loss: 0.835232794132379, Training Accuracy: 75.426\n",
            "Validation Loss: 2.1755759412316, Validation Accuracy: 52.8\n",
            "[114/150]: Training Loss: 0.8176318519484357, Training Accuracy: 75.594\n",
            "Validation Loss: 2.1797922325741714, Validation Accuracy: 52.63\n",
            "[115/150]: Training Loss: 0.8166115129618998, Training Accuracy: 75.654\n",
            "Validation Loss: 2.1856938577761316, Validation Accuracy: 52.44\n",
            "[116/150]: Training Loss: 0.8185412460733252, Training Accuracy: 75.768\n",
            "Validation Loss: 2.178807595732865, Validation Accuracy: 52.51\n",
            "[117/150]: Training Loss: 0.819738648203023, Training Accuracy: 75.68\n",
            "Validation Loss: 2.196637267519714, Validation Accuracy: 52.53\n",
            "[118/150]: Training Loss: 0.8139445048463924, Training Accuracy: 76.114\n",
            "Validation Loss: 2.1727140124436395, Validation Accuracy: 52.14\n",
            "[119/150]: Training Loss: 0.804041659359432, Training Accuracy: 76.238\n",
            "Validation Loss: 2.189874280789855, Validation Accuracy: 52.54\n",
            "[120/150]: Training Loss: 0.8070289515473349, Training Accuracy: 75.962\n",
            "Validation Loss: 2.1908302922157725, Validation Accuracy: 52.74\n",
            "[121/150]: Training Loss: 0.7972686002626443, Training Accuracy: 76.462\n",
            "Validation Loss: 2.183206063167305, Validation Accuracy: 52.9\n",
            "[122/150]: Training Loss: 0.7931420375090426, Training Accuracy: 76.7\n",
            "Validation Loss: 2.1991693654637428, Validation Accuracy: 52.88\n",
            "[123/150]: Training Loss: 0.7942516611284002, Training Accuracy: 76.382\n",
            "Validation Loss: 2.1899024916302627, Validation Accuracy: 52.96\n",
            "[124/150]: Training Loss: 0.7901588624243236, Training Accuracy: 76.912\n",
            "Validation Loss: 2.1976229795225106, Validation Accuracy: 52.76\n",
            "[125/150]: Training Loss: 0.784064800881059, Training Accuracy: 76.852\n",
            "Validation Loss: 2.197301295152895, Validation Accuracy: 52.94\n",
            "[126/150]: Training Loss: 0.7810554346236427, Training Accuracy: 77.018\n",
            "Validation Loss: 2.1978831883448704, Validation Accuracy: 53.05\n",
            "[127/150]: Training Loss: 0.7694210947855659, Training Accuracy: 77.242\n",
            "Validation Loss: 2.2037578973041216, Validation Accuracy: 52.8\n",
            "[128/150]: Training Loss: 0.771505500425768, Training Accuracy: 77.276\n",
            "Validation Loss: 2.204603326548437, Validation Accuracy: 52.61\n",
            "[129/150]: Training Loss: 0.7755528422039183, Training Accuracy: 77.05\n",
            "Validation Loss: 2.2066474340523885, Validation Accuracy: 52.48\n",
            "[130/150]: Training Loss: 0.7633897851190299, Training Accuracy: 77.462\n",
            "Validation Loss: 2.20657627294018, Validation Accuracy: 52.87\n",
            "[131/150]: Training Loss: 0.7619557766734487, Training Accuracy: 77.74\n",
            "Validation Loss: 2.2119038287241746, Validation Accuracy: 52.87\n",
            "[132/150]: Training Loss: 0.7639070795015301, Training Accuracy: 77.628\n",
            "Validation Loss: 2.214540675946861, Validation Accuracy: 52.93\n",
            "[133/150]: Training Loss: 0.7627752876800039, Training Accuracy: 77.624\n",
            "Validation Loss: 2.2126305232382126, Validation Accuracy: 52.92\n",
            "[134/150]: Training Loss: 0.762916150681503, Training Accuracy: 77.468\n",
            "Validation Loss: 2.213455740813237, Validation Accuracy: 52.87\n",
            "[135/150]: Training Loss: 0.7556408758434798, Training Accuracy: 77.858\n",
            "Validation Loss: 2.2093218823147427, Validation Accuracy: 52.78\n",
            "[136/150]: Training Loss: 0.7546808309567249, Training Accuracy: 77.614\n",
            "Validation Loss: 2.215512813276546, Validation Accuracy: 52.82\n",
            "[137/150]: Training Loss: 0.7569507613130237, Training Accuracy: 77.856\n",
            "Validation Loss: 2.214977786799145, Validation Accuracy: 52.94\n",
            "[138/150]: Training Loss: 0.7538443226414873, Training Accuracy: 77.938\n",
            "Validation Loss: 2.215168529255375, Validation Accuracy: 52.96\n",
            "[139/150]: Training Loss: 0.7538767649084711, Training Accuracy: 77.666\n",
            "Validation Loss: 2.215025094664021, Validation Accuracy: 52.95\n",
            "[140/150]: Training Loss: 0.7476015420216123, Training Accuracy: 77.98\n",
            "Validation Loss: 2.2151690637989407, Validation Accuracy: 53.11\n",
            "[141/150]: Training Loss: 0.7429782649135346, Training Accuracy: 78.256\n",
            "Validation Loss: 2.214695118794775, Validation Accuracy: 52.91\n",
            "[142/150]: Training Loss: 0.7340887488077974, Training Accuracy: 78.538\n",
            "Validation Loss: 2.216404147968171, Validation Accuracy: 53.08\n",
            "[143/150]: Training Loss: 0.7499228092029576, Training Accuracy: 78.16\n",
            "Validation Loss: 2.216663356799229, Validation Accuracy: 52.99\n",
            "[144/150]: Training Loss: 0.739901978646398, Training Accuracy: 78.208\n",
            "Validation Loss: 2.216373413231722, Validation Accuracy: 53.03\n",
            "[145/150]: Training Loss: 0.7356343204560487, Training Accuracy: 78.332\n",
            "Validation Loss: 2.21599667087482, Validation Accuracy: 52.82\n",
            "[146/150]: Training Loss: 0.7417128108575216, Training Accuracy: 78.1\n",
            "Validation Loss: 2.216853774277268, Validation Accuracy: 53.06\n",
            "[147/150]: Training Loss: 0.7406589664385447, Training Accuracy: 78.444\n",
            "Validation Loss: 2.2168655562552675, Validation Accuracy: 52.95\n",
            "[148/150]: Training Loss: 0.7310213162694745, Training Accuracy: 78.62\n",
            "Validation Loss: 2.216719139913085, Validation Accuracy: 52.99\n",
            "[149/150]: Training Loss: 0.7304428781923431, Training Accuracy: 78.478\n",
            "Validation Loss: 2.216782635943905, Validation Accuracy: 53.01\n",
            "[150/150]: Training Loss: 0.7461780087493569, Training Accuracy: 78.148\n",
            "Validation Loss: 2.2168066478838586, Validation Accuracy: 53.01\n",
            "**********************************************************************\n",
            "Test Loss: 2.2168066478838586, Test Accuracy: 53.01\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▆█▁▁▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂</td></tr><tr><td>Test Loss</td><td>▂▁█▆▇▅▆▇▄▅▅▅▅▅▆▅▅▅▅▆▆▆▅▅▅▅▆▆▅▅▅▅▅▅▄▅▅▅▄▄</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>53.01</td></tr><tr><td>Test Loss</td><td>2.21681</td></tr><tr><td>Train Accuracy</td><td>78.148</td></tr><tr><td>Train Loss</td><td>0.74618</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.0015 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_052010-62vz5e00/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 4.8/(2**5 *1e02) # approximately 15e-04\n",
        "wd = 4e-04\n",
        "\n",
        "hyperparameters = {'learning_rate': lr,\n",
        "                    'weight_decay' : wd\n",
        "                  }\n",
        "\n",
        "# Load the model\n",
        "model = LeNet5().to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer = LAMB(model.parameters(), learning_rate=lr, weight_decay=wd)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# Training\n",
        "run_training(num_epochs, model, original_train_loader, original_test_loader, original_test_loader, optimizer, scheduler, criterion, device, optimizer_name='LAMB', hyperparameters=hyperparameters, is_wandb = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### LAMB Test Large Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 512\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_030434-cxzhvadl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/cxzhvadl' target=\"_blank\">batch_size=512 learning_rate=0.012 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/cxzhvadl' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/cxzhvadl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.548103619594963, Training Accuracy: 2.248\n",
            "Validation Loss: 4.408652305603027, Validation Accuracy: 4.64\n",
            "[2/150]: Training Loss: 4.2417654796522495, Training Accuracy: 5.812\n",
            "Validation Loss: 4.078771162033081, Validation Accuracy: 7.52\n",
            "[3/150]: Training Loss: 4.044979589326041, Training Accuracy: 8.312\n",
            "Validation Loss: 3.9360867619514464, Validation Accuracy: 10.3\n",
            "[4/150]: Training Loss: 3.919014852874133, Training Accuracy: 10.582\n",
            "Validation Loss: 3.8062320232391356, Validation Accuracy: 12.69\n",
            "[5/150]: Training Loss: 3.802256394405754, Training Accuracy: 12.248\n",
            "Validation Loss: 3.6829858899116514, Validation Accuracy: 14.81\n",
            "[6/150]: Training Loss: 3.7044574873788014, Training Accuracy: 13.958\n",
            "Validation Loss: 3.598350536823273, Validation Accuracy: 16.05\n",
            "[7/150]: Training Loss: 3.619959531998148, Training Accuracy: 15.254\n",
            "Validation Loss: 3.504871332645416, Validation Accuracy: 17.62\n",
            "[8/150]: Training Loss: 3.5404407418504054, Training Accuracy: 16.608\n",
            "Validation Loss: 3.430806314945221, Validation Accuracy: 19.07\n",
            "[9/150]: Training Loss: 3.4704261118051956, Training Accuracy: 17.92\n",
            "Validation Loss: 3.355990946292877, Validation Accuracy: 20.39\n",
            "[10/150]: Training Loss: 3.4050637994493758, Training Accuracy: 18.696\n",
            "Validation Loss: 3.2953290343284607, Validation Accuracy: 21.2\n",
            "[11/150]: Training Loss: 3.347317642095138, Training Accuracy: 20.048\n",
            "Validation Loss: 3.2452304482460024, Validation Accuracy: 21.79\n",
            "[12/150]: Training Loss: 3.297038664623183, Training Accuracy: 20.922\n",
            "Validation Loss: 3.19344140291214, Validation Accuracy: 23.07\n",
            "[13/150]: Training Loss: 3.252234322684152, Training Accuracy: 21.52\n",
            "Validation Loss: 3.141191840171814, Validation Accuracy: 24.41\n",
            "[14/150]: Training Loss: 3.2010493010890726, Training Accuracy: 22.678\n",
            "Validation Loss: 3.0985478281974794, Validation Accuracy: 25.22\n",
            "[15/150]: Training Loss: 3.157341334284568, Training Accuracy: 23.544\n",
            "Validation Loss: 3.0466434955596924, Validation Accuracy: 25.34\n",
            "[16/150]: Training Loss: 3.119534066745213, Training Accuracy: 24.012\n",
            "Validation Loss: 3.001296067237854, Validation Accuracy: 26.5\n",
            "[17/150]: Training Loss: 3.072014643221485, Training Accuracy: 25.244\n",
            "Validation Loss: 2.9735819339752196, Validation Accuracy: 27.2\n",
            "[18/150]: Training Loss: 3.036018361850661, Training Accuracy: 25.664\n",
            "Validation Loss: 2.9088571310043334, Validation Accuracy: 28.33\n",
            "[19/150]: Training Loss: 2.998151776741962, Training Accuracy: 26.456\n",
            "Validation Loss: 2.87474707365036, Validation Accuracy: 29.39\n",
            "[20/150]: Training Loss: 2.954772486978648, Training Accuracy: 27.418\n",
            "Validation Loss: 2.8360928535461425, Validation Accuracy: 30.02\n",
            "[21/150]: Training Loss: 2.9239249545700696, Training Accuracy: 27.92\n",
            "Validation Loss: 2.8242129921913146, Validation Accuracy: 30.11\n",
            "[22/150]: Training Loss: 2.895316936531845, Training Accuracy: 28.508\n",
            "Validation Loss: 2.774155652523041, Validation Accuracy: 31.66\n",
            "[23/150]: Training Loss: 2.859412969375143, Training Accuracy: 29.08\n",
            "Validation Loss: 2.744171452522278, Validation Accuracy: 31.68\n",
            "[24/150]: Training Loss: 2.8252378531864712, Training Accuracy: 29.67\n",
            "Validation Loss: 2.714933896064758, Validation Accuracy: 32.09\n",
            "[25/150]: Training Loss: 2.7994687800504723, Training Accuracy: 30.014\n",
            "Validation Loss: 2.677684450149536, Validation Accuracy: 32.99\n",
            "[26/150]: Training Loss: 2.765696126587537, Training Accuracy: 31.108\n",
            "Validation Loss: 2.6599324345588684, Validation Accuracy: 33.18\n",
            "[27/150]: Training Loss: 2.740787510969201, Training Accuracy: 31.346\n",
            "Validation Loss: 2.6425792574882507, Validation Accuracy: 34.19\n",
            "[28/150]: Training Loss: 2.708871919281629, Training Accuracy: 31.902\n",
            "Validation Loss: 2.595020318031311, Validation Accuracy: 34.92\n",
            "[29/150]: Training Loss: 2.685730184827532, Training Accuracy: 32.37\n",
            "Validation Loss: 2.5777265787124635, Validation Accuracy: 35.0\n",
            "[30/150]: Training Loss: 2.65871207324826, Training Accuracy: 32.814\n",
            "Validation Loss: 2.5629518866539, Validation Accuracy: 35.52\n",
            "[31/150]: Training Loss: 2.6434246131352017, Training Accuracy: 33.334\n",
            "Validation Loss: 2.536830258369446, Validation Accuracy: 36.09\n",
            "[32/150]: Training Loss: 2.6128752523539016, Training Accuracy: 33.888\n",
            "Validation Loss: 2.5102296233177186, Validation Accuracy: 36.3\n",
            "[33/150]: Training Loss: 2.5891901619580326, Training Accuracy: 34.278\n",
            "Validation Loss: 2.5139454007148743, Validation Accuracy: 35.79\n",
            "[34/150]: Training Loss: 2.5790800391411293, Training Accuracy: 34.66\n",
            "Validation Loss: 2.4993165135383606, Validation Accuracy: 36.84\n",
            "[35/150]: Training Loss: 2.5574506521224976, Training Accuracy: 35.194\n",
            "Validation Loss: 2.4735957622528075, Validation Accuracy: 37.42\n",
            "[36/150]: Training Loss: 2.5401810237339566, Training Accuracy: 35.368\n",
            "Validation Loss: 2.4526307821273803, Validation Accuracy: 37.83\n",
            "[37/150]: Training Loss: 2.525405545623935, Training Accuracy: 35.55\n",
            "Validation Loss: 2.4341293573379517, Validation Accuracy: 38.12\n",
            "[38/150]: Training Loss: 2.511008401306308, Training Accuracy: 36.018\n",
            "Validation Loss: 2.4294227361679077, Validation Accuracy: 38.29\n",
            "[39/150]: Training Loss: 2.485745899531306, Training Accuracy: 36.542\n",
            "Validation Loss: 2.424438309669495, Validation Accuracy: 38.25\n",
            "[40/150]: Training Loss: 2.4709256230568397, Training Accuracy: 37.07\n",
            "Validation Loss: 2.392498195171356, Validation Accuracy: 39.18\n",
            "[41/150]: Training Loss: 2.453389508383615, Training Accuracy: 37.188\n",
            "Validation Loss: 2.3836654901504515, Validation Accuracy: 39.34\n",
            "[42/150]: Training Loss: 2.436603519381309, Training Accuracy: 37.772\n",
            "Validation Loss: 2.366767430305481, Validation Accuracy: 39.42\n",
            "[43/150]: Training Loss: 2.4228299369617385, Training Accuracy: 37.662\n",
            "Validation Loss: 2.376271140575409, Validation Accuracy: 39.35\n",
            "[44/150]: Training Loss: 2.410697212024611, Training Accuracy: 38.148\n",
            "Validation Loss: 2.346989369392395, Validation Accuracy: 39.91\n",
            "[45/150]: Training Loss: 2.3892664276823705, Training Accuracy: 38.722\n",
            "Validation Loss: 2.343613922595978, Validation Accuracy: 40.27\n",
            "[46/150]: Training Loss: 2.3732383518802878, Training Accuracy: 38.978\n",
            "Validation Loss: 2.3365381956100464, Validation Accuracy: 40.04\n",
            "[47/150]: Training Loss: 2.365978574266239, Training Accuracy: 39.05\n",
            "Validation Loss: 2.327448809146881, Validation Accuracy: 40.27\n",
            "[48/150]: Training Loss: 2.3588604537808164, Training Accuracy: 39.06\n",
            "Validation Loss: 2.306267297267914, Validation Accuracy: 40.56\n",
            "[49/150]: Training Loss: 2.3424654785467656, Training Accuracy: 39.404\n",
            "Validation Loss: 2.3065796971321104, Validation Accuracy: 40.84\n",
            "[50/150]: Training Loss: 2.3214932753115285, Training Accuracy: 39.88\n",
            "Validation Loss: 2.291364300251007, Validation Accuracy: 40.85\n",
            "[51/150]: Training Loss: 2.319426546291429, Training Accuracy: 40.212\n",
            "Validation Loss: 2.2994829177856446, Validation Accuracy: 40.73\n",
            "[52/150]: Training Loss: 2.3011135446782016, Training Accuracy: 40.49\n",
            "Validation Loss: 2.272173082828522, Validation Accuracy: 41.48\n",
            "[53/150]: Training Loss: 2.291705601069392, Training Accuracy: 40.65\n",
            "Validation Loss: 2.266998326778412, Validation Accuracy: 41.39\n",
            "[54/150]: Training Loss: 2.2740079359132417, Training Accuracy: 40.958\n",
            "Validation Loss: 2.262247622013092, Validation Accuracy: 41.51\n",
            "[55/150]: Training Loss: 2.2702779356314213, Training Accuracy: 41.14\n",
            "Validation Loss: 2.247134339809418, Validation Accuracy: 42.18\n",
            "[56/150]: Training Loss: 2.2560372790511773, Training Accuracy: 41.404\n",
            "Validation Loss: 2.2417269587516784, Validation Accuracy: 42.2\n",
            "[57/150]: Training Loss: 2.2483739342008318, Training Accuracy: 41.564\n",
            "Validation Loss: 2.2450056076049805, Validation Accuracy: 41.86\n",
            "[58/150]: Training Loss: 2.2375453880855014, Training Accuracy: 41.784\n",
            "Validation Loss: 2.240702271461487, Validation Accuracy: 42.3\n",
            "[59/150]: Training Loss: 2.2237761117974104, Training Accuracy: 42.04\n",
            "Validation Loss: 2.2335373759269714, Validation Accuracy: 42.3\n",
            "[60/150]: Training Loss: 2.205965436234766, Training Accuracy: 42.788\n",
            "Validation Loss: 2.2246406435966493, Validation Accuracy: 42.3\n",
            "[61/150]: Training Loss: 2.209139476017076, Training Accuracy: 42.374\n",
            "Validation Loss: 2.2219802379608153, Validation Accuracy: 42.67\n",
            "[62/150]: Training Loss: 2.1963447551338042, Training Accuracy: 42.81\n",
            "Validation Loss: 2.2133118152618407, Validation Accuracy: 43.14\n",
            "[63/150]: Training Loss: 2.1860338960375105, Training Accuracy: 42.898\n",
            "Validation Loss: 2.1990301489830015, Validation Accuracy: 43.06\n",
            "[64/150]: Training Loss: 2.1734118291309903, Training Accuracy: 43.52\n",
            "Validation Loss: 2.184101331233978, Validation Accuracy: 43.49\n",
            "[65/150]: Training Loss: 2.1634396545741024, Training Accuracy: 43.672\n",
            "Validation Loss: 2.184304988384247, Validation Accuracy: 43.2\n",
            "[66/150]: Training Loss: 2.1568861445602105, Training Accuracy: 43.418\n",
            "Validation Loss: 2.185279929637909, Validation Accuracy: 43.78\n",
            "[67/150]: Training Loss: 2.148503156340852, Training Accuracy: 43.668\n",
            "Validation Loss: 2.167687404155731, Validation Accuracy: 43.87\n",
            "[68/150]: Training Loss: 2.1439749002456665, Training Accuracy: 44.14\n",
            "Validation Loss: 2.1670034646987917, Validation Accuracy: 43.46\n",
            "[69/150]: Training Loss: 2.1294457790802936, Training Accuracy: 44.21\n",
            "Validation Loss: 2.16845703125, Validation Accuracy: 43.74\n",
            "[70/150]: Training Loss: 2.124348041962604, Training Accuracy: 44.398\n",
            "Validation Loss: 2.1625195741653442, Validation Accuracy: 43.96\n",
            "[71/150]: Training Loss: 2.1167217590370955, Training Accuracy: 44.5\n",
            "Validation Loss: 2.156108921766281, Validation Accuracy: 44.18\n",
            "[72/150]: Training Loss: 2.110376476025095, Training Accuracy: 44.732\n",
            "Validation Loss: 2.163306188583374, Validation Accuracy: 44.44\n",
            "[73/150]: Training Loss: 2.0973582632687626, Training Accuracy: 45.182\n",
            "Validation Loss: 2.1542032241821287, Validation Accuracy: 43.94\n",
            "[74/150]: Training Loss: 2.0909664728203596, Training Accuracy: 44.946\n",
            "Validation Loss: 2.1355461835861207, Validation Accuracy: 44.96\n",
            "[75/150]: Training Loss: 2.0853602423959847, Training Accuracy: 45.484\n",
            "Validation Loss: 2.133738082647324, Validation Accuracy: 44.85\n",
            "[76/150]: Training Loss: 2.0740136200067947, Training Accuracy: 45.472\n",
            "Validation Loss: 2.133003461360931, Validation Accuracy: 44.84\n",
            "[77/150]: Training Loss: 2.0717564541466382, Training Accuracy: 45.53\n",
            "Validation Loss: 2.1258037745952607, Validation Accuracy: 44.82\n",
            "[78/150]: Training Loss: 2.0652840003675346, Training Accuracy: 45.638\n",
            "Validation Loss: 2.128880649805069, Validation Accuracy: 45.05\n",
            "[79/150]: Training Loss: 2.058366948244523, Training Accuracy: 45.926\n",
            "Validation Loss: 2.1184136271476746, Validation Accuracy: 45.13\n",
            "[80/150]: Training Loss: 2.0537973958618787, Training Accuracy: 46.032\n",
            "Validation Loss: 2.1158532202243805, Validation Accuracy: 45.41\n",
            "[81/150]: Training Loss: 2.045546445311332, Training Accuracy: 46.22\n",
            "Validation Loss: 2.1055331349372866, Validation Accuracy: 45.66\n",
            "[82/150]: Training Loss: 2.0298990595097446, Training Accuracy: 46.356\n",
            "Validation Loss: 2.1113824367523195, Validation Accuracy: 44.91\n",
            "[83/150]: Training Loss: 2.0376479844657744, Training Accuracy: 46.254\n",
            "Validation Loss: 2.107465136051178, Validation Accuracy: 45.22\n",
            "[84/150]: Training Loss: 2.024351028763518, Training Accuracy: 46.772\n",
            "Validation Loss: 2.111053156852722, Validation Accuracy: 45.25\n",
            "[85/150]: Training Loss: 2.0180776496322785, Training Accuracy: 46.708\n",
            "Validation Loss: 2.1116692423820496, Validation Accuracy: 45.4\n",
            "[86/150]: Training Loss: 2.0185614410711796, Training Accuracy: 46.768\n",
            "Validation Loss: 2.0962437868118284, Validation Accuracy: 45.53\n",
            "[87/150]: Training Loss: 2.008148268777497, Training Accuracy: 47.038\n",
            "Validation Loss: 2.1018231213092804, Validation Accuracy: 45.87\n",
            "[88/150]: Training Loss: 2.0017905855665403, Training Accuracy: 47.142\n",
            "Validation Loss: 2.0960277736186983, Validation Accuracy: 45.43\n",
            "[89/150]: Training Loss: 1.997242003071065, Training Accuracy: 47.234\n",
            "Validation Loss: 2.0983319759368895, Validation Accuracy: 45.81\n",
            "[90/150]: Training Loss: 1.990279420297973, Training Accuracy: 47.292\n",
            "Validation Loss: 2.0858993887901307, Validation Accuracy: 45.95\n",
            "[91/150]: Training Loss: 1.9810005718347978, Training Accuracy: 47.574\n",
            "Validation Loss: 2.084125190973282, Validation Accuracy: 45.77\n",
            "[92/150]: Training Loss: 1.9804937997642829, Training Accuracy: 47.424\n",
            "Validation Loss: 2.082464426755905, Validation Accuracy: 46.53\n",
            "[93/150]: Training Loss: 1.9814110872696857, Training Accuracy: 47.53\n",
            "Validation Loss: 2.0802278578281403, Validation Accuracy: 46.11\n",
            "[94/150]: Training Loss: 1.9699082763827578, Training Accuracy: 47.858\n",
            "Validation Loss: 2.0711540818214416, Validation Accuracy: 46.49\n",
            "[95/150]: Training Loss: 1.963157915339178, Training Accuracy: 47.982\n",
            "Validation Loss: 2.0740585386753083, Validation Accuracy: 46.44\n",
            "[96/150]: Training Loss: 1.958235819729007, Training Accuracy: 48.274\n",
            "Validation Loss: 2.07954381108284, Validation Accuracy: 46.17\n",
            "[97/150]: Training Loss: 1.954954169234451, Training Accuracy: 48.15\n",
            "Validation Loss: 2.074056273698807, Validation Accuracy: 46.2\n",
            "[98/150]: Training Loss: 1.9549918174743652, Training Accuracy: 48.224\n",
            "Validation Loss: 2.076428145170212, Validation Accuracy: 46.58\n",
            "[99/150]: Training Loss: 1.9507698878950002, Training Accuracy: 48.31\n",
            "Validation Loss: 2.0744490921497345, Validation Accuracy: 46.39\n",
            "[100/150]: Training Loss: 1.9442955656927459, Training Accuracy: 48.386\n",
            "Validation Loss: 2.070361965894699, Validation Accuracy: 46.32\n",
            "[101/150]: Training Loss: 1.9397471109215094, Training Accuracy: 48.69\n",
            "Validation Loss: 2.0684597194194794, Validation Accuracy: 46.38\n",
            "[102/150]: Training Loss: 1.9359576568311574, Training Accuracy: 48.714\n",
            "Validation Loss: 2.0585030019283295, Validation Accuracy: 46.51\n",
            "[103/150]: Training Loss: 1.9329783028485823, Training Accuracy: 48.922\n",
            "Validation Loss: 2.0640245974063873, Validation Accuracy: 46.74\n",
            "[104/150]: Training Loss: 1.9296102134548887, Training Accuracy: 48.876\n",
            "Validation Loss: 2.0570806205272674, Validation Accuracy: 46.91\n",
            "[105/150]: Training Loss: 1.9274067817902079, Training Accuracy: 48.836\n",
            "Validation Loss: 2.0609201788902283, Validation Accuracy: 46.8\n",
            "[106/150]: Training Loss: 1.9235176504874716, Training Accuracy: 48.79\n",
            "Validation Loss: 2.0552598774433135, Validation Accuracy: 46.81\n",
            "[107/150]: Training Loss: 1.9150361941785228, Training Accuracy: 48.932\n",
            "Validation Loss: 2.0611153185367583, Validation Accuracy: 46.99\n",
            "[108/150]: Training Loss: 1.9190492228585847, Training Accuracy: 49.058\n",
            "Validation Loss: 2.0567619800567627, Validation Accuracy: 46.75\n",
            "[109/150]: Training Loss: 1.913926816716486, Training Accuracy: 49.228\n",
            "Validation Loss: 2.050940066576004, Validation Accuracy: 47.03\n",
            "[110/150]: Training Loss: 1.9050643358911787, Training Accuracy: 49.476\n",
            "Validation Loss: 2.0542674362659454, Validation Accuracy: 47.17\n",
            "[111/150]: Training Loss: 1.9050180559255638, Training Accuracy: 49.556\n",
            "Validation Loss: 2.0528355121612547, Validation Accuracy: 46.76\n",
            "[112/150]: Training Loss: 1.9088959620923411, Training Accuracy: 49.342\n",
            "Validation Loss: 2.0460824489593508, Validation Accuracy: 47.29\n",
            "[113/150]: Training Loss: 1.9044355105380624, Training Accuracy: 49.372\n",
            "Validation Loss: 2.0464745998382567, Validation Accuracy: 47.08\n",
            "[114/150]: Training Loss: 1.8996793262812557, Training Accuracy: 49.554\n",
            "Validation Loss: 2.0468083798885344, Validation Accuracy: 46.91\n",
            "[115/150]: Training Loss: 1.8977701590985667, Training Accuracy: 49.58\n",
            "Validation Loss: 2.0489026606082916, Validation Accuracy: 47.02\n",
            "[116/150]: Training Loss: 1.9024124863196392, Training Accuracy: 49.632\n",
            "Validation Loss: 2.045408546924591, Validation Accuracy: 47.0\n",
            "[117/150]: Training Loss: 1.896902701076196, Training Accuracy: 49.664\n",
            "Validation Loss: 2.0449475407600404, Validation Accuracy: 47.28\n",
            "[118/150]: Training Loss: 1.887433451049182, Training Accuracy: 49.708\n",
            "Validation Loss: 2.0426290810108183, Validation Accuracy: 47.38\n",
            "[119/150]: Training Loss: 1.890810448296216, Training Accuracy: 49.792\n",
            "Validation Loss: 2.0400512278079987, Validation Accuracy: 47.37\n",
            "[120/150]: Training Loss: 1.8811560346155751, Training Accuracy: 49.924\n",
            "Validation Loss: 2.0446768164634705, Validation Accuracy: 47.22\n",
            "[121/150]: Training Loss: 1.8865068250772905, Training Accuracy: 49.69\n",
            "Validation Loss: 2.043155688047409, Validation Accuracy: 47.38\n",
            "[122/150]: Training Loss: 1.8877364591676362, Training Accuracy: 49.846\n",
            "Validation Loss: 2.0397587597370146, Validation Accuracy: 47.32\n",
            "[123/150]: Training Loss: 1.8801838646129685, Training Accuracy: 50.076\n",
            "Validation Loss: 2.0414818286895753, Validation Accuracy: 47.09\n",
            "[124/150]: Training Loss: 1.885428019932338, Training Accuracy: 49.846\n",
            "Validation Loss: 2.039280617237091, Validation Accuracy: 47.25\n",
            "[125/150]: Training Loss: 1.8782189658709936, Training Accuracy: 49.912\n",
            "Validation Loss: 2.041011613607407, Validation Accuracy: 47.37\n",
            "[126/150]: Training Loss: 1.879096979997596, Training Accuracy: 50.194\n",
            "Validation Loss: 2.039641273021698, Validation Accuracy: 47.31\n",
            "[127/150]: Training Loss: 1.8750834902938531, Training Accuracy: 50.116\n",
            "Validation Loss: 2.0357660591602325, Validation Accuracy: 47.24\n",
            "[128/150]: Training Loss: 1.8704910095857115, Training Accuracy: 50.072\n",
            "Validation Loss: 2.036173564195633, Validation Accuracy: 47.35\n",
            "[129/150]: Training Loss: 1.877774753132645, Training Accuracy: 49.962\n",
            "Validation Loss: 2.036165338754654, Validation Accuracy: 47.39\n",
            "[130/150]: Training Loss: 1.873654185509195, Training Accuracy: 50.22\n",
            "Validation Loss: 2.0360648393630982, Validation Accuracy: 47.45\n",
            "[131/150]: Training Loss: 1.8686100913553823, Training Accuracy: 50.242\n",
            "Validation Loss: 2.0372694969177245, Validation Accuracy: 47.45\n",
            "[132/150]: Training Loss: 1.8713702170216306, Training Accuracy: 50.18\n",
            "Validation Loss: 2.0337652981281282, Validation Accuracy: 47.49\n",
            "[133/150]: Training Loss: 1.8645332942203598, Training Accuracy: 50.246\n",
            "Validation Loss: 2.0328580677509307, Validation Accuracy: 47.49\n",
            "[134/150]: Training Loss: 1.8704569595200675, Training Accuracy: 50.214\n",
            "Validation Loss: 2.034824085235596, Validation Accuracy: 47.51\n",
            "[135/150]: Training Loss: 1.8728308142447958, Training Accuracy: 50.154\n",
            "Validation Loss: 2.0340331494808197, Validation Accuracy: 47.43\n",
            "[136/150]: Training Loss: 1.8635819201566735, Training Accuracy: 50.458\n",
            "Validation Loss: 2.0333786249160766, Validation Accuracy: 47.49\n",
            "[137/150]: Training Loss: 1.8670503034883617, Training Accuracy: 50.31\n",
            "Validation Loss: 2.0328627586364747, Validation Accuracy: 47.54\n",
            "[138/150]: Training Loss: 1.8680279534690234, Training Accuracy: 50.094\n",
            "Validation Loss: 2.033160853385925, Validation Accuracy: 47.58\n",
            "[139/150]: Training Loss: 1.8607495062205257, Training Accuracy: 50.598\n",
            "Validation Loss: 2.0337081253528595, Validation Accuracy: 47.43\n",
            "[140/150]: Training Loss: 1.8683336133859596, Training Accuracy: 50.274\n",
            "Validation Loss: 2.0336008071899414, Validation Accuracy: 47.43\n",
            "[141/150]: Training Loss: 1.8690763091554448, Training Accuracy: 50.138\n",
            "Validation Loss: 2.0335811972618103, Validation Accuracy: 47.35\n",
            "[142/150]: Training Loss: 1.8656006175644544, Training Accuracy: 50.446\n",
            "Validation Loss: 2.03268141746521, Validation Accuracy: 47.33\n",
            "[143/150]: Training Loss: 1.8692044360297067, Training Accuracy: 50.272\n",
            "Validation Loss: 2.0328011751174926, Validation Accuracy: 47.47\n",
            "[144/150]: Training Loss: 1.8644137017580928, Training Accuracy: 50.484\n",
            "Validation Loss: 2.0326188147068023, Validation Accuracy: 47.35\n",
            "[145/150]: Training Loss: 1.8648517861658214, Training Accuracy: 50.596\n",
            "Validation Loss: 2.0326371788978577, Validation Accuracy: 47.46\n",
            "[146/150]: Training Loss: 1.8628962891442435, Training Accuracy: 50.572\n",
            "Validation Loss: 2.0327242374420167, Validation Accuracy: 47.44\n",
            "[147/150]: Training Loss: 1.8659434099586643, Training Accuracy: 50.224\n",
            "Validation Loss: 2.0326199173927306, Validation Accuracy: 47.48\n",
            "[148/150]: Training Loss: 1.8660591877236659, Training Accuracy: 50.486\n",
            "Validation Loss: 2.0326066315174103, Validation Accuracy: 47.47\n",
            "[149/150]: Training Loss: 1.8515786005526174, Training Accuracy: 50.66\n",
            "Validation Loss: 2.0325814962387083, Validation Accuracy: 47.47\n",
            "[150/150]: Training Loss: 1.8628681095278994, Training Accuracy: 50.284\n",
            "Validation Loss: 2.032588768005371, Validation Accuracy: 47.49\n",
            "**********************************************************************\n",
            "Test Loss: 2.032588768005371, Test Accuracy: 47.49\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▅▄▁▃▅▄▄▅▆▆▆▆▇▇▇▇███▇</td></tr><tr><td>Test Loss</td><td>██▇▃▃▃▃▁▁▂▂▁▂▁▂▂▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>47.49</td></tr><tr><td>Test Loss</td><td>2.03259</td></tr><tr><td>Train Accuracy</td><td>50.284</td></tr><tr><td>Train Loss</td><td>1.86287</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=512 learning_rate=0.012 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/cxzhvadl' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/cxzhvadl</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_030434-cxzhvadl/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 1024\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_032944-gya1gfdd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/gya1gfdd' target=\"_blank\">batch_size=1024 learning_rate=0.024 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/gya1gfdd' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/gya1gfdd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.590565778771225, Training Accuracy: 2.506\n",
            "Validation Loss: 4.557540512084961, Validation Accuracy: 3.82\n",
            "[2/150]: Training Loss: 4.493393129231978, Training Accuracy: 3.93\n",
            "Validation Loss: 4.3973547458648685, Validation Accuracy: 4.08\n",
            "[3/150]: Training Loss: 4.306708112054942, Training Accuracy: 4.95\n",
            "Validation Loss: 4.188614082336426, Validation Accuracy: 7.08\n",
            "[4/150]: Training Loss: 4.141291871362803, Training Accuracy: 7.376\n",
            "Validation Loss: 4.052172231674194, Validation Accuracy: 9.03\n",
            "[5/150]: Training Loss: 4.041586535317557, Training Accuracy: 8.738\n",
            "Validation Loss: 3.958093047142029, Validation Accuracy: 10.53\n",
            "[6/150]: Training Loss: 3.964396841672002, Training Accuracy: 9.848\n",
            "Validation Loss: 3.877572846412659, Validation Accuracy: 11.97\n",
            "[7/150]: Training Loss: 3.884106631181678, Training Accuracy: 11.158\n",
            "Validation Loss: 3.7878554344177244, Validation Accuracy: 13.45\n",
            "[8/150]: Training Loss: 3.8150518037834944, Training Accuracy: 12.512\n",
            "Validation Loss: 3.717964506149292, Validation Accuracy: 14.81\n",
            "[9/150]: Training Loss: 3.7486333409134223, Training Accuracy: 13.622\n",
            "Validation Loss: 3.6480979919433594, Validation Accuracy: 15.64\n",
            "[10/150]: Training Loss: 3.6901821408952986, Training Accuracy: 14.558\n",
            "Validation Loss: 3.596638226509094, Validation Accuracy: 17.04\n",
            "[11/150]: Training Loss: 3.6437562047218788, Training Accuracy: 15.138\n",
            "Validation Loss: 3.5412201166152952, Validation Accuracy: 17.54\n",
            "[12/150]: Training Loss: 3.5933454912536, Training Accuracy: 16.05\n",
            "Validation Loss: 3.4941962718963624, Validation Accuracy: 18.44\n",
            "[13/150]: Training Loss: 3.5444304748457305, Training Accuracy: 16.992\n",
            "Validation Loss: 3.4401322841644286, Validation Accuracy: 19.71\n",
            "[14/150]: Training Loss: 3.4985976121863542, Training Accuracy: 17.586\n",
            "Validation Loss: 3.395567035675049, Validation Accuracy: 20.55\n",
            "[15/150]: Training Loss: 3.453222994901696, Training Accuracy: 18.504\n",
            "Validation Loss: 3.345482683181763, Validation Accuracy: 21.11\n",
            "[16/150]: Training Loss: 3.409157923289708, Training Accuracy: 19.202\n",
            "Validation Loss: 3.309169292449951, Validation Accuracy: 21.17\n",
            "[17/150]: Training Loss: 3.362671049273744, Training Accuracy: 19.936\n",
            "Validation Loss: 3.2603885412216185, Validation Accuracy: 22.74\n",
            "[18/150]: Training Loss: 3.325447690730192, Training Accuracy: 20.694\n",
            "Validation Loss: 3.2284779787063598, Validation Accuracy: 22.9\n",
            "[19/150]: Training Loss: 3.2953158884632345, Training Accuracy: 21.242\n",
            "Validation Loss: 3.1922855138778687, Validation Accuracy: 23.83\n",
            "[20/150]: Training Loss: 3.2581767743947556, Training Accuracy: 21.716\n",
            "Validation Loss: 3.1589023590087892, Validation Accuracy: 24.16\n",
            "[21/150]: Training Loss: 3.2280877463671627, Training Accuracy: 22.26\n",
            "Validation Loss: 3.120241904258728, Validation Accuracy: 25.39\n",
            "[22/150]: Training Loss: 3.197068000326351, Training Accuracy: 22.91\n",
            "Validation Loss: 3.1016393423080446, Validation Accuracy: 25.24\n",
            "[23/150]: Training Loss: 3.168019095245673, Training Accuracy: 23.574\n",
            "Validation Loss: 3.070971941947937, Validation Accuracy: 25.89\n",
            "[24/150]: Training Loss: 3.1401676158515777, Training Accuracy: 23.778\n",
            "Validation Loss: 3.043724036216736, Validation Accuracy: 26.42\n",
            "[25/150]: Training Loss: 3.1160317245794804, Training Accuracy: 24.456\n",
            "Validation Loss: 3.0154256582260133, Validation Accuracy: 27.1\n",
            "[26/150]: Training Loss: 3.0870673218551947, Training Accuracy: 24.898\n",
            "Validation Loss: 2.9828964948654173, Validation Accuracy: 27.83\n",
            "[27/150]: Training Loss: 3.063414077369534, Training Accuracy: 25.346\n",
            "Validation Loss: 2.9679323196411134, Validation Accuracy: 27.77\n",
            "[28/150]: Training Loss: 3.0381831246979383, Training Accuracy: 25.734\n",
            "Validation Loss: 2.9393006801605224, Validation Accuracy: 27.88\n",
            "[29/150]: Training Loss: 3.0093668480308686, Training Accuracy: 26.254\n",
            "Validation Loss: 2.9122467756271364, Validation Accuracy: 28.61\n",
            "[30/150]: Training Loss: 2.993134167729592, Training Accuracy: 26.534\n",
            "Validation Loss: 2.8969119787216187, Validation Accuracy: 29.41\n",
            "[31/150]: Training Loss: 2.96768881836716, Training Accuracy: 27.274\n",
            "Validation Loss: 2.8658586025238035, Validation Accuracy: 29.67\n",
            "[32/150]: Training Loss: 2.9414292695570965, Training Accuracy: 27.478\n",
            "Validation Loss: 2.862342190742493, Validation Accuracy: 29.54\n",
            "[33/150]: Training Loss: 2.9305670018098793, Training Accuracy: 27.892\n",
            "Validation Loss: 2.822964382171631, Validation Accuracy: 30.79\n",
            "[34/150]: Training Loss: 2.9122587028814824, Training Accuracy: 28.452\n",
            "Validation Loss: 2.827067565917969, Validation Accuracy: 30.5\n",
            "[35/150]: Training Loss: 2.8878965669748733, Training Accuracy: 28.686\n",
            "Validation Loss: 2.7999042510986327, Validation Accuracy: 31.18\n",
            "[36/150]: Training Loss: 2.877296729963653, Training Accuracy: 29.068\n",
            "Validation Loss: 2.779293489456177, Validation Accuracy: 31.43\n",
            "[37/150]: Training Loss: 2.8623253374683615, Training Accuracy: 29.252\n",
            "Validation Loss: 2.771078372001648, Validation Accuracy: 31.46\n",
            "[38/150]: Training Loss: 2.845758058586899, Training Accuracy: 29.536\n",
            "Validation Loss: 2.7440542459487913, Validation Accuracy: 32.21\n",
            "[39/150]: Training Loss: 2.826752618867524, Training Accuracy: 30.078\n",
            "Validation Loss: 2.732813310623169, Validation Accuracy: 32.48\n",
            "[40/150]: Training Loss: 2.8119893901202144, Training Accuracy: 30.44\n",
            "Validation Loss: 2.7104941606521606, Validation Accuracy: 32.62\n",
            "[41/150]: Training Loss: 2.798588991165161, Training Accuracy: 30.704\n",
            "Validation Loss: 2.71542112827301, Validation Accuracy: 32.54\n",
            "[42/150]: Training Loss: 2.789037441720768, Training Accuracy: 30.904\n",
            "Validation Loss: 2.699553608894348, Validation Accuracy: 32.91\n",
            "[43/150]: Training Loss: 2.7693806959658254, Training Accuracy: 31.302\n",
            "Validation Loss: 2.6739187002182008, Validation Accuracy: 33.55\n",
            "[44/150]: Training Loss: 2.759038647826837, Training Accuracy: 31.434\n",
            "Validation Loss: 2.6662899017333985, Validation Accuracy: 33.41\n",
            "[45/150]: Training Loss: 2.741479440611236, Training Accuracy: 31.38\n",
            "Validation Loss: 2.668094348907471, Validation Accuracy: 33.54\n",
            "[46/150]: Training Loss: 2.7325507524062176, Training Accuracy: 31.794\n",
            "Validation Loss: 2.642213749885559, Validation Accuracy: 33.91\n",
            "[47/150]: Training Loss: 2.713993953198803, Training Accuracy: 32.126\n",
            "Validation Loss: 2.638446068763733, Validation Accuracy: 33.93\n",
            "[48/150]: Training Loss: 2.7059789968996633, Training Accuracy: 32.314\n",
            "Validation Loss: 2.625475454330444, Validation Accuracy: 34.3\n",
            "[49/150]: Training Loss: 2.6864885018796336, Training Accuracy: 32.77\n",
            "Validation Loss: 2.609480929374695, Validation Accuracy: 34.55\n",
            "[50/150]: Training Loss: 2.672593793090509, Training Accuracy: 32.9\n",
            "Validation Loss: 2.6012288331985474, Validation Accuracy: 34.62\n",
            "[51/150]: Training Loss: 2.6611643907975178, Training Accuracy: 33.552\n",
            "Validation Loss: 2.5788160800933837, Validation Accuracy: 35.31\n",
            "[52/150]: Training Loss: 2.6511389333374646, Training Accuracy: 33.566\n",
            "Validation Loss: 2.5761229038238525, Validation Accuracy: 35.32\n",
            "[53/150]: Training Loss: 2.6390009899528657, Training Accuracy: 33.73\n",
            "Validation Loss: 2.5618453502655028, Validation Accuracy: 35.69\n",
            "[54/150]: Training Loss: 2.6149472947023353, Training Accuracy: 34.264\n",
            "Validation Loss: 2.5448814392089845, Validation Accuracy: 36.26\n",
            "[55/150]: Training Loss: 2.6032056467873708, Training Accuracy: 34.406\n",
            "Validation Loss: 2.540690517425537, Validation Accuracy: 36.11\n",
            "[56/150]: Training Loss: 2.5915229174555563, Training Accuracy: 34.672\n",
            "Validation Loss: 2.5265331268310547, Validation Accuracy: 36.11\n",
            "[57/150]: Training Loss: 2.5826624850837554, Training Accuracy: 34.72\n",
            "Validation Loss: 2.508814811706543, Validation Accuracy: 36.63\n",
            "[58/150]: Training Loss: 2.5663072041102817, Training Accuracy: 35.1\n",
            "Validation Loss: 2.5033692836761476, Validation Accuracy: 36.62\n",
            "[59/150]: Training Loss: 2.5570305464219074, Training Accuracy: 35.432\n",
            "Validation Loss: 2.5039987564086914, Validation Accuracy: 36.86\n",
            "[60/150]: Training Loss: 2.5427569759135342, Training Accuracy: 35.544\n",
            "Validation Loss: 2.480252814292908, Validation Accuracy: 36.95\n",
            "[61/150]: Training Loss: 2.5343954806425133, Training Accuracy: 35.818\n",
            "Validation Loss: 2.4878724813461304, Validation Accuracy: 36.76\n",
            "[62/150]: Training Loss: 2.525669209811152, Training Accuracy: 36.168\n",
            "Validation Loss: 2.463840389251709, Validation Accuracy: 37.41\n",
            "[63/150]: Training Loss: 2.5151758339940287, Training Accuracy: 36.232\n",
            "Validation Loss: 2.4627907991409304, Validation Accuracy: 37.57\n",
            "[64/150]: Training Loss: 2.500540592232529, Training Accuracy: 36.386\n",
            "Validation Loss: 2.436687207221985, Validation Accuracy: 37.62\n",
            "[65/150]: Training Loss: 2.495049014383433, Training Accuracy: 36.56\n",
            "Validation Loss: 2.4480030298233033, Validation Accuracy: 37.49\n",
            "[66/150]: Training Loss: 2.4877421953240217, Training Accuracy: 36.736\n",
            "Validation Loss: 2.438114333152771, Validation Accuracy: 37.91\n",
            "[67/150]: Training Loss: 2.473500212844537, Training Accuracy: 37.184\n",
            "Validation Loss: 2.4267699241638185, Validation Accuracy: 38.53\n",
            "[68/150]: Training Loss: 2.4613005725704893, Training Accuracy: 37.244\n",
            "Validation Loss: 2.4308807611465455, Validation Accuracy: 38.0\n",
            "[69/150]: Training Loss: 2.4629757258356832, Training Accuracy: 37.416\n",
            "Validation Loss: 2.4256237030029295, Validation Accuracy: 38.22\n",
            "[70/150]: Training Loss: 2.4537137041286545, Training Accuracy: 37.484\n",
            "Validation Loss: 2.4168110609054567, Validation Accuracy: 38.54\n",
            "[71/150]: Training Loss: 2.438401460647583, Training Accuracy: 37.964\n",
            "Validation Loss: 2.401189923286438, Validation Accuracy: 38.86\n",
            "[72/150]: Training Loss: 2.431730114683813, Training Accuracy: 37.806\n",
            "Validation Loss: 2.3951660871505736, Validation Accuracy: 38.77\n",
            "[73/150]: Training Loss: 2.425863484947049, Training Accuracy: 38.066\n",
            "Validation Loss: 2.3934099197387697, Validation Accuracy: 38.84\n",
            "[74/150]: Training Loss: 2.418164759266133, Training Accuracy: 38.102\n",
            "Validation Loss: 2.3899000883102417, Validation Accuracy: 39.18\n",
            "[75/150]: Training Loss: 2.4137614357228183, Training Accuracy: 38.604\n",
            "Validation Loss: 2.38698296546936, Validation Accuracy: 38.79\n",
            "[76/150]: Training Loss: 2.4025228948009256, Training Accuracy: 38.79\n",
            "Validation Loss: 2.379944348335266, Validation Accuracy: 39.32\n",
            "[77/150]: Training Loss: 2.402056786478782, Training Accuracy: 38.59\n",
            "Validation Loss: 2.367121386528015, Validation Accuracy: 39.44\n",
            "[78/150]: Training Loss: 2.39358198886015, Training Accuracy: 38.706\n",
            "Validation Loss: 2.360290360450745, Validation Accuracy: 39.64\n",
            "[79/150]: Training Loss: 2.385896707067684, Training Accuracy: 38.97\n",
            "Validation Loss: 2.362123727798462, Validation Accuracy: 39.74\n",
            "[80/150]: Training Loss: 2.3822805248961156, Training Accuracy: 38.96\n",
            "Validation Loss: 2.355213189125061, Validation Accuracy: 39.86\n",
            "[81/150]: Training Loss: 2.3766152566793015, Training Accuracy: 39.064\n",
            "Validation Loss: 2.354436731338501, Validation Accuracy: 39.82\n",
            "[82/150]: Training Loss: 2.3680763390599466, Training Accuracy: 39.428\n",
            "Validation Loss: 2.3397852420806884, Validation Accuracy: 40.27\n",
            "[83/150]: Training Loss: 2.3645690363280627, Training Accuracy: 39.386\n",
            "Validation Loss: 2.340263819694519, Validation Accuracy: 40.03\n",
            "[84/150]: Training Loss: 2.3626577708185934, Training Accuracy: 39.516\n",
            "Validation Loss: 2.3479089021682737, Validation Accuracy: 39.5\n",
            "[85/150]: Training Loss: 2.356617353400406, Training Accuracy: 39.298\n",
            "Validation Loss: 2.3384116172790526, Validation Accuracy: 39.85\n",
            "[86/150]: Training Loss: 2.350528507816548, Training Accuracy: 39.64\n",
            "Validation Loss: 2.3308178663253782, Validation Accuracy: 40.42\n",
            "[87/150]: Training Loss: 2.3518636421281465, Training Accuracy: 39.588\n",
            "Validation Loss: 2.332779359817505, Validation Accuracy: 40.17\n",
            "[88/150]: Training Loss: 2.339253683479465, Training Accuracy: 39.942\n",
            "Validation Loss: 2.3242568492889406, Validation Accuracy: 40.35\n",
            "[89/150]: Training Loss: 2.331887250043908, Training Accuracy: 39.994\n",
            "Validation Loss: 2.3254419326782227, Validation Accuracy: 40.36\n",
            "[90/150]: Training Loss: 2.330076134934717, Training Accuracy: 40.046\n",
            "Validation Loss: 2.3118756771087647, Validation Accuracy: 40.79\n",
            "[91/150]: Training Loss: 2.3228837275991636, Training Accuracy: 40.382\n",
            "Validation Loss: 2.308647394180298, Validation Accuracy: 40.81\n",
            "[92/150]: Training Loss: 2.3223668945078946, Training Accuracy: 40.278\n",
            "Validation Loss: 2.3109630823135374, Validation Accuracy: 40.91\n",
            "[93/150]: Training Loss: 2.3131886890956332, Training Accuracy: 40.396\n",
            "Validation Loss: 2.3050234794616697, Validation Accuracy: 40.87\n",
            "[94/150]: Training Loss: 2.3118463730325503, Training Accuracy: 40.442\n",
            "Validation Loss: 2.309488868713379, Validation Accuracy: 40.79\n",
            "[95/150]: Training Loss: 2.31477233341762, Training Accuracy: 40.426\n",
            "Validation Loss: 2.3041553497314453, Validation Accuracy: 41.03\n",
            "[96/150]: Training Loss: 2.3052495450389627, Training Accuracy: 40.74\n",
            "Validation Loss: 2.2959155321121214, Validation Accuracy: 41.33\n",
            "[97/150]: Training Loss: 2.2986154750901826, Training Accuracy: 41.036\n",
            "Validation Loss: 2.291999101638794, Validation Accuracy: 41.16\n",
            "[98/150]: Training Loss: 2.296130083045181, Training Accuracy: 41.05\n",
            "Validation Loss: 2.296833896636963, Validation Accuracy: 40.89\n",
            "[99/150]: Training Loss: 2.293913335216289, Training Accuracy: 40.574\n",
            "Validation Loss: 2.2895299196243286, Validation Accuracy: 41.51\n",
            "[100/150]: Training Loss: 2.289002457443549, Training Accuracy: 40.874\n",
            "Validation Loss: 2.296149969100952, Validation Accuracy: 41.25\n",
            "[101/150]: Training Loss: 2.28773240653836, Training Accuracy: 40.816\n",
            "Validation Loss: 2.2874104261398314, Validation Accuracy: 41.22\n",
            "[102/150]: Training Loss: 2.2810704367501393, Training Accuracy: 41.238\n",
            "Validation Loss: 2.2846805810928346, Validation Accuracy: 41.36\n",
            "[103/150]: Training Loss: 2.2808507607907664, Training Accuracy: 41.062\n",
            "Validation Loss: 2.2863134384155273, Validation Accuracy: 41.45\n",
            "[104/150]: Training Loss: 2.281994581222534, Training Accuracy: 41.224\n",
            "Validation Loss: 2.2800295829772947, Validation Accuracy: 41.63\n",
            "[105/150]: Training Loss: 2.2791298846809234, Training Accuracy: 41.27\n",
            "Validation Loss: 2.276352620124817, Validation Accuracy: 41.69\n",
            "[106/150]: Training Loss: 2.274389768133358, Training Accuracy: 41.284\n",
            "Validation Loss: 2.277020573616028, Validation Accuracy: 41.82\n",
            "[107/150]: Training Loss: 2.266129561832973, Training Accuracy: 41.632\n",
            "Validation Loss: 2.2738038301467896, Validation Accuracy: 41.79\n",
            "[108/150]: Training Loss: 2.264082616689254, Training Accuracy: 41.726\n",
            "Validation Loss: 2.272693967819214, Validation Accuracy: 41.83\n",
            "[109/150]: Training Loss: 2.266490250217671, Training Accuracy: 41.69\n",
            "Validation Loss: 2.269743800163269, Validation Accuracy: 41.79\n",
            "[110/150]: Training Loss: 2.264426902848847, Training Accuracy: 41.53\n",
            "Validation Loss: 2.2705984592437742, Validation Accuracy: 41.75\n",
            "[111/150]: Training Loss: 2.259202057001542, Training Accuracy: 41.662\n",
            "Validation Loss: 2.266038990020752, Validation Accuracy: 41.82\n",
            "[112/150]: Training Loss: 2.25746505114497, Training Accuracy: 41.906\n",
            "Validation Loss: 2.265566897392273, Validation Accuracy: 41.83\n",
            "[113/150]: Training Loss: 2.253900381983543, Training Accuracy: 41.718\n",
            "Validation Loss: 2.268652153015137, Validation Accuracy: 41.67\n",
            "[114/150]: Training Loss: 2.258889456184543, Training Accuracy: 41.682\n",
            "Validation Loss: 2.264122176170349, Validation Accuracy: 41.95\n",
            "[115/150]: Training Loss: 2.2516084106601015, Training Accuracy: 41.906\n",
            "Validation Loss: 2.26185884475708, Validation Accuracy: 42.08\n",
            "[116/150]: Training Loss: 2.244454252476595, Training Accuracy: 41.716\n",
            "Validation Loss: 2.25951509475708, Validation Accuracy: 42.01\n",
            "[117/150]: Training Loss: 2.2538246329949825, Training Accuracy: 41.758\n",
            "Validation Loss: 2.257939171791077, Validation Accuracy: 42.16\n",
            "[118/150]: Training Loss: 2.2463515972604555, Training Accuracy: 41.924\n",
            "Validation Loss: 2.256657099723816, Validation Accuracy: 42.13\n",
            "[119/150]: Training Loss: 2.245320835892035, Training Accuracy: 41.94\n",
            "Validation Loss: 2.255589818954468, Validation Accuracy: 41.97\n",
            "[120/150]: Training Loss: 2.250691910179294, Training Accuracy: 42.04\n",
            "Validation Loss: 2.259244465827942, Validation Accuracy: 42.07\n",
            "[121/150]: Training Loss: 2.237687383379255, Training Accuracy: 42.29\n",
            "Validation Loss: 2.2565879344940187, Validation Accuracy: 42.27\n",
            "[122/150]: Training Loss: 2.2319457725602754, Training Accuracy: 42.16\n",
            "Validation Loss: 2.2570300817489626, Validation Accuracy: 42.15\n",
            "[123/150]: Training Loss: 2.2342230300514068, Training Accuracy: 42.226\n",
            "Validation Loss: 2.255305027961731, Validation Accuracy: 42.29\n",
            "[124/150]: Training Loss: 2.239549919050567, Training Accuracy: 42.174\n",
            "Validation Loss: 2.256370449066162, Validation Accuracy: 42.23\n",
            "[125/150]: Training Loss: 2.237409543017952, Training Accuracy: 42.23\n",
            "Validation Loss: 2.2512456655502318, Validation Accuracy: 42.3\n",
            "[126/150]: Training Loss: 2.2329092658295924, Training Accuracy: 42.402\n",
            "Validation Loss: 2.251137447357178, Validation Accuracy: 42.43\n",
            "[127/150]: Training Loss: 2.2352922595277125, Training Accuracy: 42.266\n",
            "Validation Loss: 2.2498720407485964, Validation Accuracy: 42.34\n",
            "[128/150]: Training Loss: 2.2299188545772006, Training Accuracy: 42.322\n",
            "Validation Loss: 2.248763990402222, Validation Accuracy: 42.27\n",
            "[129/150]: Training Loss: 2.2254644364726786, Training Accuracy: 42.516\n",
            "Validation Loss: 2.247484731674194, Validation Accuracy: 42.49\n",
            "[130/150]: Training Loss: 2.229749392489998, Training Accuracy: 42.22\n",
            "Validation Loss: 2.249401307106018, Validation Accuracy: 42.39\n",
            "[131/150]: Training Loss: 2.2227475886442223, Training Accuracy: 42.588\n",
            "Validation Loss: 2.248521161079407, Validation Accuracy: 42.35\n",
            "[132/150]: Training Loss: 2.2250091251061885, Training Accuracy: 42.428\n",
            "Validation Loss: 2.2475123167037965, Validation Accuracy: 42.33\n",
            "[133/150]: Training Loss: 2.2305113004178416, Training Accuracy: 42.204\n",
            "Validation Loss: 2.248475170135498, Validation Accuracy: 42.25\n",
            "[134/150]: Training Loss: 2.227376563208444, Training Accuracy: 42.406\n",
            "Validation Loss: 2.248474097251892, Validation Accuracy: 42.42\n",
            "[135/150]: Training Loss: 2.228417683620842, Training Accuracy: 42.378\n",
            "Validation Loss: 2.24667911529541, Validation Accuracy: 42.48\n",
            "[136/150]: Training Loss: 2.2207712056685467, Training Accuracy: 42.538\n",
            "Validation Loss: 2.2469127416610717, Validation Accuracy: 42.44\n",
            "[137/150]: Training Loss: 2.2226368310500164, Training Accuracy: 42.52\n",
            "Validation Loss: 2.246177053451538, Validation Accuracy: 42.41\n",
            "[138/150]: Training Loss: 2.2275044090893803, Training Accuracy: 42.468\n",
            "Validation Loss: 2.246805429458618, Validation Accuracy: 42.4\n",
            "[139/150]: Training Loss: 2.223565267056835, Training Accuracy: 42.634\n",
            "Validation Loss: 2.246001434326172, Validation Accuracy: 42.32\n",
            "[140/150]: Training Loss: 2.224436273380202, Training Accuracy: 42.37\n",
            "Validation Loss: 2.2460646629333496, Validation Accuracy: 42.42\n",
            "[141/150]: Training Loss: 2.228755454627835, Training Accuracy: 42.58\n",
            "Validation Loss: 2.2457078218460085, Validation Accuracy: 42.42\n",
            "[142/150]: Training Loss: 2.2257018916460933, Training Accuracy: 42.484\n",
            "Validation Loss: 2.24568395614624, Validation Accuracy: 42.43\n",
            "[143/150]: Training Loss: 2.224288507383697, Training Accuracy: 42.534\n",
            "Validation Loss: 2.245313835144043, Validation Accuracy: 42.47\n",
            "[144/150]: Training Loss: 2.2213949767910703, Training Accuracy: 42.712\n",
            "Validation Loss: 2.245689034461975, Validation Accuracy: 42.39\n",
            "[145/150]: Training Loss: 2.2182185795842386, Training Accuracy: 42.612\n",
            "Validation Loss: 2.2455665349960325, Validation Accuracy: 42.39\n",
            "[146/150]: Training Loss: 2.2254402540167986, Training Accuracy: 42.488\n",
            "Validation Loss: 2.245345687866211, Validation Accuracy: 42.39\n",
            "[147/150]: Training Loss: 2.2218092509678433, Training Accuracy: 42.64\n",
            "Validation Loss: 2.2454842805862425, Validation Accuracy: 42.42\n",
            "[148/150]: Training Loss: 2.2220640085181413, Training Accuracy: 42.318\n",
            "Validation Loss: 2.2455402135849, Validation Accuracy: 42.39\n",
            "[149/150]: Training Loss: 2.2183865965629113, Training Accuracy: 42.668\n",
            "Validation Loss: 2.2455331802368166, Validation Accuracy: 42.42\n",
            "[150/150]: Training Loss: 2.218842954051738, Training Accuracy: 42.57\n",
            "Validation Loss: 2.2455447912216187, Validation Accuracy: 42.42\n",
            "**********************************************************************\n",
            "Test Loss: 2.2455447912216187, Test Accuracy: 42.42\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▃▃▄▂▃▂▁▁▂</td></tr><tr><td>Test Loss</td><td>█▄▂▁▃▂▃▄▃▂</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>42.42</td></tr><tr><td>Test Loss</td><td>2.24554</td></tr><tr><td>Train Accuracy</td><td>42.57</td></tr><tr><td>Train Loss</td><td>2.21884</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=1024 learning_rate=0.024 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/gya1gfdd' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/gya1gfdd</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_032944-gya1gfdd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 2048\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_035348-1r088cdb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/1r088cdb' target=\"_blank\">batch_size=2048 learning_rate=0.048 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/1r088cdb' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/1r088cdb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.606891899108887, Training Accuracy: 0.96\n",
            "Validation Loss: 4.60687141418457, Validation Accuracy: 1.1\n",
            "[2/150]: Training Loss: 4.600610694885254, Training Accuracy: 1.736\n",
            "Validation Loss: 4.590463447570801, Validation Accuracy: 2.39\n",
            "[3/150]: Training Loss: 4.575713977813721, Training Accuracy: 2.896\n",
            "Validation Loss: 4.549876403808594, Validation Accuracy: 3.48\n",
            "[4/150]: Training Loss: 4.521224956512452, Training Accuracy: 3.332\n",
            "Validation Loss: 4.476098251342774, Validation Accuracy: 3.51\n",
            "[5/150]: Training Loss: 4.436832466125488, Training Accuracy: 3.676\n",
            "Validation Loss: 4.373760604858399, Validation Accuracy: 4.44\n",
            "[6/150]: Training Loss: 4.333190250396728, Training Accuracy: 4.484\n",
            "Validation Loss: 4.2619599342346195, Validation Accuracy: 5.74\n",
            "[7/150]: Training Loss: 4.228750267028809, Training Accuracy: 6.184\n",
            "Validation Loss: 4.158302211761475, Validation Accuracy: 7.0\n",
            "[8/150]: Training Loss: 4.14489444732666, Training Accuracy: 7.23\n",
            "Validation Loss: 4.080980014801026, Validation Accuracy: 8.09\n",
            "[9/150]: Training Loss: 4.086068439483642, Training Accuracy: 8.046\n",
            "Validation Loss: 4.022517251968384, Validation Accuracy: 9.23\n",
            "[10/150]: Training Loss: 4.038323402404785, Training Accuracy: 8.906\n",
            "Validation Loss: 3.973035955429077, Validation Accuracy: 10.09\n",
            "[11/150]: Training Loss: 3.9903813552856446, Training Accuracy: 9.57\n",
            "Validation Loss: 3.9220180988311766, Validation Accuracy: 11.14\n",
            "[12/150]: Training Loss: 3.9483608531951906, Training Accuracy: 10.134\n",
            "Validation Loss: 3.8770122051239015, Validation Accuracy: 11.85\n",
            "[13/150]: Training Loss: 3.906474552154541, Training Accuracy: 11.124\n",
            "Validation Loss: 3.830677843093872, Validation Accuracy: 12.24\n",
            "[14/150]: Training Loss: 3.8657492351531983, Training Accuracy: 11.712\n",
            "Validation Loss: 3.784984064102173, Validation Accuracy: 13.01\n",
            "[15/150]: Training Loss: 3.827094488143921, Training Accuracy: 12.232\n",
            "Validation Loss: 3.747487115859985, Validation Accuracy: 13.69\n",
            "[16/150]: Training Loss: 3.7882041549682617, Training Accuracy: 12.894\n",
            "Validation Loss: 3.706352472305298, Validation Accuracy: 14.75\n",
            "[17/150]: Training Loss: 3.7569018363952638, Training Accuracy: 13.466\n",
            "Validation Loss: 3.6668768405914305, Validation Accuracy: 15.0\n",
            "[18/150]: Training Loss: 3.724953260421753, Training Accuracy: 13.934\n",
            "Validation Loss: 3.6342330455780028, Validation Accuracy: 15.84\n",
            "[19/150]: Training Loss: 3.6888757514953614, Training Accuracy: 14.426\n",
            "Validation Loss: 3.602110433578491, Validation Accuracy: 16.34\n",
            "[20/150]: Training Loss: 3.6559412384033205, Training Accuracy: 15.034\n",
            "Validation Loss: 3.5667267799377442, Validation Accuracy: 16.84\n",
            "[21/150]: Training Loss: 3.621606273651123, Training Accuracy: 15.512\n",
            "Validation Loss: 3.532243776321411, Validation Accuracy: 17.51\n",
            "[22/150]: Training Loss: 3.586877536773682, Training Accuracy: 16.244\n",
            "Validation Loss: 3.498146724700928, Validation Accuracy: 17.93\n",
            "[23/150]: Training Loss: 3.5530471801757812, Training Accuracy: 16.71\n",
            "Validation Loss: 3.469740056991577, Validation Accuracy: 18.54\n",
            "[24/150]: Training Loss: 3.523145809173584, Training Accuracy: 17.322\n",
            "Validation Loss: 3.429482364654541, Validation Accuracy: 19.66\n",
            "[25/150]: Training Loss: 3.492525005340576, Training Accuracy: 17.878\n",
            "Validation Loss: 3.4018688201904297, Validation Accuracy: 19.37\n",
            "[26/150]: Training Loss: 3.462628803253174, Training Accuracy: 18.3\n",
            "Validation Loss: 3.3740827560424806, Validation Accuracy: 20.22\n",
            "[27/150]: Training Loss: 3.4355313301086428, Training Accuracy: 18.766\n",
            "Validation Loss: 3.345688581466675, Validation Accuracy: 20.63\n",
            "[28/150]: Training Loss: 3.4118361473083496, Training Accuracy: 19.086\n",
            "Validation Loss: 3.3152890682220457, Validation Accuracy: 21.47\n",
            "[29/150]: Training Loss: 3.3853207206726075, Training Accuracy: 19.684\n",
            "Validation Loss: 3.288516616821289, Validation Accuracy: 21.77\n",
            "[30/150]: Training Loss: 3.357349433898926, Training Accuracy: 20.182\n",
            "Validation Loss: 3.272135925292969, Validation Accuracy: 21.96\n",
            "[31/150]: Training Loss: 3.33790997505188, Training Accuracy: 20.556\n",
            "Validation Loss: 3.2512518882751467, Validation Accuracy: 22.86\n",
            "[32/150]: Training Loss: 3.3162687015533447, Training Accuracy: 20.618\n",
            "Validation Loss: 3.2230645179748536, Validation Accuracy: 22.77\n",
            "[33/150]: Training Loss: 3.2954384326934814, Training Accuracy: 21.236\n",
            "Validation Loss: 3.209771013259888, Validation Accuracy: 23.22\n",
            "[34/150]: Training Loss: 3.2834206104278563, Training Accuracy: 21.342\n",
            "Validation Loss: 3.1870476245880126, Validation Accuracy: 23.97\n",
            "[35/150]: Training Loss: 3.255247573852539, Training Accuracy: 21.83\n",
            "Validation Loss: 3.1760802268981934, Validation Accuracy: 24.2\n",
            "[36/150]: Training Loss: 3.244738836288452, Training Accuracy: 22.18\n",
            "Validation Loss: 3.149981164932251, Validation Accuracy: 24.63\n",
            "[37/150]: Training Loss: 3.2233246517181398, Training Accuracy: 22.646\n",
            "Validation Loss: 3.1303664207458497, Validation Accuracy: 24.92\n",
            "[38/150]: Training Loss: 3.2000336933135984, Training Accuracy: 23.05\n",
            "Validation Loss: 3.1102230072021486, Validation Accuracy: 25.24\n",
            "[39/150]: Training Loss: 3.1809381294250487, Training Accuracy: 23.32\n",
            "Validation Loss: 3.0988600730895994, Validation Accuracy: 25.57\n",
            "[40/150]: Training Loss: 3.1758554553985596, Training Accuracy: 23.448\n",
            "Validation Loss: 3.0785787105560303, Validation Accuracy: 25.66\n",
            "[41/150]: Training Loss: 3.1560576248168943, Training Accuracy: 23.69\n",
            "Validation Loss: 3.056659126281738, Validation Accuracy: 26.3\n",
            "[42/150]: Training Loss: 3.136617965698242, Training Accuracy: 24.108\n",
            "Validation Loss: 3.044490671157837, Validation Accuracy: 26.49\n",
            "[43/150]: Training Loss: 3.1227911949157714, Training Accuracy: 24.27\n",
            "Validation Loss: 3.029969644546509, Validation Accuracy: 26.68\n",
            "[44/150]: Training Loss: 3.102838659286499, Training Accuracy: 24.826\n",
            "Validation Loss: 3.0146406173706053, Validation Accuracy: 26.97\n",
            "[45/150]: Training Loss: 3.094334297180176, Training Accuracy: 24.89\n",
            "Validation Loss: 2.998156213760376, Validation Accuracy: 27.54\n",
            "[46/150]: Training Loss: 3.075647096633911, Training Accuracy: 25.27\n",
            "Validation Loss: 2.9830405712127686, Validation Accuracy: 27.7\n",
            "[47/150]: Training Loss: 3.0576992225646973, Training Accuracy: 25.456\n",
            "Validation Loss: 2.972663164138794, Validation Accuracy: 27.56\n",
            "[48/150]: Training Loss: 3.0457812404632567, Training Accuracy: 25.736\n",
            "Validation Loss: 2.957260274887085, Validation Accuracy: 28.21\n",
            "[49/150]: Training Loss: 3.0373900318145752, Training Accuracy: 25.894\n",
            "Validation Loss: 2.9418825626373293, Validation Accuracy: 28.5\n",
            "[50/150]: Training Loss: 3.020056085586548, Training Accuracy: 26.326\n",
            "Validation Loss: 2.929175853729248, Validation Accuracy: 28.88\n",
            "[51/150]: Training Loss: 3.0056448650360106, Training Accuracy: 26.46\n",
            "Validation Loss: 2.9070215702056883, Validation Accuracy: 29.27\n",
            "[52/150]: Training Loss: 2.9939701843261717, Training Accuracy: 26.762\n",
            "Validation Loss: 2.8945343494415283, Validation Accuracy: 29.48\n",
            "[53/150]: Training Loss: 2.9771877002716063, Training Accuracy: 27.252\n",
            "Validation Loss: 2.885859823226929, Validation Accuracy: 29.39\n",
            "[54/150]: Training Loss: 2.9624980735778808, Training Accuracy: 27.476\n",
            "Validation Loss: 2.87581090927124, Validation Accuracy: 29.49\n",
            "[55/150]: Training Loss: 2.9567051315307618, Training Accuracy: 27.696\n",
            "Validation Loss: 2.8627774238586428, Validation Accuracy: 29.88\n",
            "[56/150]: Training Loss: 2.945523920059204, Training Accuracy: 27.744\n",
            "Validation Loss: 2.855295705795288, Validation Accuracy: 29.62\n",
            "[57/150]: Training Loss: 2.93146315574646, Training Accuracy: 27.892\n",
            "Validation Loss: 2.8434480667114257, Validation Accuracy: 30.07\n",
            "[58/150]: Training Loss: 2.924803171157837, Training Accuracy: 27.968\n",
            "Validation Loss: 2.823704719543457, Validation Accuracy: 30.61\n",
            "[59/150]: Training Loss: 2.9088122272491455, Training Accuracy: 28.24\n",
            "Validation Loss: 2.8173577785491943, Validation Accuracy: 30.69\n",
            "[60/150]: Training Loss: 2.8987307167053222, Training Accuracy: 28.58\n",
            "Validation Loss: 2.809761810302734, Validation Accuracy: 30.93\n",
            "[61/150]: Training Loss: 2.8892831802368164, Training Accuracy: 28.828\n",
            "Validation Loss: 2.7993114948272706, Validation Accuracy: 31.22\n",
            "[62/150]: Training Loss: 2.874851484298706, Training Accuracy: 29.142\n",
            "Validation Loss: 2.7836734771728517, Validation Accuracy: 31.41\n",
            "[63/150]: Training Loss: 2.869437532424927, Training Accuracy: 29.33\n",
            "Validation Loss: 2.7782646656036376, Validation Accuracy: 31.29\n",
            "[64/150]: Training Loss: 2.8525683212280275, Training Accuracy: 29.672\n",
            "Validation Loss: 2.766908121109009, Validation Accuracy: 31.82\n",
            "[65/150]: Training Loss: 2.847346153259277, Training Accuracy: 29.614\n",
            "Validation Loss: 2.7592673778533934, Validation Accuracy: 31.8\n",
            "[66/150]: Training Loss: 2.838597478866577, Training Accuracy: 29.832\n",
            "Validation Loss: 2.746263265609741, Validation Accuracy: 31.86\n",
            "[67/150]: Training Loss: 2.8289484119415285, Training Accuracy: 29.956\n",
            "Validation Loss: 2.7383673667907713, Validation Accuracy: 32.15\n",
            "[68/150]: Training Loss: 2.818249168395996, Training Accuracy: 29.954\n",
            "Validation Loss: 2.723827075958252, Validation Accuracy: 32.35\n",
            "[69/150]: Training Loss: 2.8115413284301756, Training Accuracy: 30.47\n",
            "Validation Loss: 2.7157896995544433, Validation Accuracy: 33.16\n",
            "[70/150]: Training Loss: 2.796581230163574, Training Accuracy: 30.382\n",
            "Validation Loss: 2.7060704708099363, Validation Accuracy: 33.03\n",
            "[71/150]: Training Loss: 2.788346128463745, Training Accuracy: 30.692\n",
            "Validation Loss: 2.6914602279663087, Validation Accuracy: 33.27\n",
            "[72/150]: Training Loss: 2.779642858505249, Training Accuracy: 30.64\n",
            "Validation Loss: 2.691270637512207, Validation Accuracy: 33.74\n",
            "[73/150]: Training Loss: 2.773522415161133, Training Accuracy: 30.952\n",
            "Validation Loss: 2.6753844261169433, Validation Accuracy: 33.72\n",
            "[74/150]: Training Loss: 2.751913080215454, Training Accuracy: 31.466\n",
            "Validation Loss: 2.662951850891113, Validation Accuracy: 33.7\n",
            "[75/150]: Training Loss: 2.745556564331055, Training Accuracy: 31.714\n",
            "Validation Loss: 2.6605172634124754, Validation Accuracy: 33.77\n",
            "[76/150]: Training Loss: 2.7446984481811523, Training Accuracy: 31.672\n",
            "Validation Loss: 2.657077407836914, Validation Accuracy: 33.73\n",
            "[77/150]: Training Loss: 2.7300216293334962, Training Accuracy: 31.946\n",
            "Validation Loss: 2.643053579330444, Validation Accuracy: 34.25\n",
            "[78/150]: Training Loss: 2.7262183570861818, Training Accuracy: 31.986\n",
            "Validation Loss: 2.6381571292877197, Validation Accuracy: 34.28\n",
            "[79/150]: Training Loss: 2.7191726398468017, Training Accuracy: 32.246\n",
            "Validation Loss: 2.627451467514038, Validation Accuracy: 34.45\n",
            "[80/150]: Training Loss: 2.7006086349487304, Training Accuracy: 32.402\n",
            "Validation Loss: 2.6252679347991945, Validation Accuracy: 34.61\n",
            "[81/150]: Training Loss: 2.7075089263916015, Training Accuracy: 32.438\n",
            "Validation Loss: 2.6192337036132813, Validation Accuracy: 34.21\n",
            "[82/150]: Training Loss: 2.702421941757202, Training Accuracy: 32.624\n",
            "Validation Loss: 2.6089219570159914, Validation Accuracy: 34.72\n",
            "[83/150]: Training Loss: 2.685064287185669, Training Accuracy: 32.76\n",
            "Validation Loss: 2.601409578323364, Validation Accuracy: 34.82\n",
            "[84/150]: Training Loss: 2.6799155330657958, Training Accuracy: 32.842\n",
            "Validation Loss: 2.59562029838562, Validation Accuracy: 35.02\n",
            "[85/150]: Training Loss: 2.6843585395812988, Training Accuracy: 32.746\n",
            "Validation Loss: 2.594114303588867, Validation Accuracy: 35.06\n",
            "[86/150]: Training Loss: 2.6704106616973875, Training Accuracy: 33.22\n",
            "Validation Loss: 2.593081569671631, Validation Accuracy: 35.13\n",
            "[87/150]: Training Loss: 2.668564338684082, Training Accuracy: 33.186\n",
            "Validation Loss: 2.5836416244506837, Validation Accuracy: 35.4\n",
            "[88/150]: Training Loss: 2.662708683013916, Training Accuracy: 33.212\n",
            "Validation Loss: 2.5814727306365968, Validation Accuracy: 35.44\n",
            "[89/150]: Training Loss: 2.6585529041290283, Training Accuracy: 33.444\n",
            "Validation Loss: 2.5715579986572266, Validation Accuracy: 35.67\n",
            "[90/150]: Training Loss: 2.6515896797180174, Training Accuracy: 33.474\n",
            "Validation Loss: 2.5656859397888185, Validation Accuracy: 35.65\n",
            "[91/150]: Training Loss: 2.6523977279663087, Training Accuracy: 33.45\n",
            "Validation Loss: 2.566520643234253, Validation Accuracy: 35.62\n",
            "[92/150]: Training Loss: 2.6442916107177736, Training Accuracy: 33.936\n",
            "Validation Loss: 2.5638641357421874, Validation Accuracy: 35.49\n",
            "[93/150]: Training Loss: 2.6416356277465822, Training Accuracy: 33.602\n",
            "Validation Loss: 2.562070035934448, Validation Accuracy: 35.91\n",
            "[94/150]: Training Loss: 2.628076629638672, Training Accuracy: 33.744\n",
            "Validation Loss: 2.5538381576538085, Validation Accuracy: 35.99\n",
            "[95/150]: Training Loss: 2.6320537185668944, Training Accuracy: 34.032\n",
            "Validation Loss: 2.5495237827301027, Validation Accuracy: 35.75\n",
            "[96/150]: Training Loss: 2.628206911087036, Training Accuracy: 34.152\n",
            "Validation Loss: 2.545219898223877, Validation Accuracy: 36.1\n",
            "[97/150]: Training Loss: 2.6205778026580813, Training Accuracy: 34.186\n",
            "Validation Loss: 2.542753982543945, Validation Accuracy: 36.02\n",
            "[98/150]: Training Loss: 2.617592248916626, Training Accuracy: 34.176\n",
            "Validation Loss: 2.542101335525513, Validation Accuracy: 35.98\n",
            "[99/150]: Training Loss: 2.6197222805023195, Training Accuracy: 34.094\n",
            "Validation Loss: 2.538325071334839, Validation Accuracy: 36.14\n",
            "[100/150]: Training Loss: 2.6128591823577882, Training Accuracy: 34.212\n",
            "Validation Loss: 2.535270118713379, Validation Accuracy: 36.06\n",
            "[101/150]: Training Loss: 2.609950532913208, Training Accuracy: 34.26\n",
            "Validation Loss: 2.5293672561645506, Validation Accuracy: 36.23\n",
            "[102/150]: Training Loss: 2.6042222595214843, Training Accuracy: 34.486\n",
            "Validation Loss: 2.5314455032348633, Validation Accuracy: 36.4\n",
            "[103/150]: Training Loss: 2.60733717918396, Training Accuracy: 34.668\n",
            "Validation Loss: 2.5286536693572996, Validation Accuracy: 36.22\n",
            "[104/150]: Training Loss: 2.600928258895874, Training Accuracy: 34.68\n",
            "Validation Loss: 2.5272301197052003, Validation Accuracy: 36.12\n",
            "[105/150]: Training Loss: 2.5978956508636473, Training Accuracy: 34.822\n",
            "Validation Loss: 2.521611976623535, Validation Accuracy: 36.39\n",
            "[106/150]: Training Loss: 2.5903251361846924, Training Accuracy: 34.832\n",
            "Validation Loss: 2.5163689136505125, Validation Accuracy: 36.51\n",
            "[107/150]: Training Loss: 2.594358024597168, Training Accuracy: 34.894\n",
            "Validation Loss: 2.5197267055511476, Validation Accuracy: 36.45\n",
            "[108/150]: Training Loss: 2.590383281707764, Training Accuracy: 34.85\n",
            "Validation Loss: 2.515224838256836, Validation Accuracy: 36.61\n",
            "[109/150]: Training Loss: 2.58540961265564, Training Accuracy: 34.91\n",
            "Validation Loss: 2.5137331008911135, Validation Accuracy: 36.72\n",
            "[110/150]: Training Loss: 2.5731867122650147, Training Accuracy: 35.144\n",
            "Validation Loss: 2.5086013793945314, Validation Accuracy: 36.91\n",
            "[111/150]: Training Loss: 2.5846965503692627, Training Accuracy: 34.988\n",
            "Validation Loss: 2.51100435256958, Validation Accuracy: 36.72\n",
            "[112/150]: Training Loss: 2.5743587589263917, Training Accuracy: 34.872\n",
            "Validation Loss: 2.5043835639953613, Validation Accuracy: 36.84\n",
            "[113/150]: Training Loss: 2.577282676696777, Training Accuracy: 35.01\n",
            "Validation Loss: 2.507812738418579, Validation Accuracy: 36.84\n",
            "[114/150]: Training Loss: 2.5752318000793455, Training Accuracy: 35.168\n",
            "Validation Loss: 2.5039757251739503, Validation Accuracy: 36.75\n",
            "[115/150]: Training Loss: 2.5756383037567137, Training Accuracy: 35.106\n",
            "Validation Loss: 2.5018725395202637, Validation Accuracy: 36.89\n",
            "[116/150]: Training Loss: 2.573223829269409, Training Accuracy: 35.25\n",
            "Validation Loss: 2.5003193855285644, Validation Accuracy: 37.2\n",
            "[117/150]: Training Loss: 2.5703605079650877, Training Accuracy: 35.122\n",
            "Validation Loss: 2.49833083152771, Validation Accuracy: 37.18\n",
            "[118/150]: Training Loss: 2.56836067199707, Training Accuracy: 35.334\n",
            "Validation Loss: 2.4982669830322264, Validation Accuracy: 36.95\n",
            "[119/150]: Training Loss: 2.568598871231079, Training Accuracy: 35.246\n",
            "Validation Loss: 2.4962736129760743, Validation Accuracy: 36.98\n",
            "[120/150]: Training Loss: 2.565032138824463, Training Accuracy: 35.226\n",
            "Validation Loss: 2.4972612857818604, Validation Accuracy: 37.05\n",
            "[121/150]: Training Loss: 2.564799222946167, Training Accuracy: 35.332\n",
            "Validation Loss: 2.494395542144775, Validation Accuracy: 37.02\n",
            "[122/150]: Training Loss: 2.5654745578765867, Training Accuracy: 35.408\n",
            "Validation Loss: 2.4940038681030274, Validation Accuracy: 37.03\n",
            "[123/150]: Training Loss: 2.567827205657959, Training Accuracy: 35.318\n",
            "Validation Loss: 2.4918994426727297, Validation Accuracy: 37.44\n",
            "[124/150]: Training Loss: 2.564124135971069, Training Accuracy: 35.482\n",
            "Validation Loss: 2.4921056747436525, Validation Accuracy: 37.23\n",
            "[125/150]: Training Loss: 2.5578205394744873, Training Accuracy: 35.414\n",
            "Validation Loss: 2.4925122261047363, Validation Accuracy: 37.1\n",
            "[126/150]: Training Loss: 2.553278617858887, Training Accuracy: 35.388\n",
            "Validation Loss: 2.491460990905762, Validation Accuracy: 37.3\n",
            "[127/150]: Training Loss: 2.556161298751831, Training Accuracy: 35.484\n",
            "Validation Loss: 2.4900985240936278, Validation Accuracy: 37.34\n",
            "[128/150]: Training Loss: 2.5557092952728273, Training Accuracy: 35.646\n",
            "Validation Loss: 2.487586164474487, Validation Accuracy: 37.36\n",
            "[129/150]: Training Loss: 2.560698127746582, Training Accuracy: 35.54\n",
            "Validation Loss: 2.4893627643585203, Validation Accuracy: 37.25\n",
            "[130/150]: Training Loss: 2.5523008346557616, Training Accuracy: 35.798\n",
            "Validation Loss: 2.4873109817504884, Validation Accuracy: 37.43\n",
            "[131/150]: Training Loss: 2.547942838668823, Training Accuracy: 35.822\n",
            "Validation Loss: 2.486782455444336, Validation Accuracy: 37.36\n",
            "[132/150]: Training Loss: 2.5490105152130127, Training Accuracy: 35.614\n",
            "Validation Loss: 2.4866137981414793, Validation Accuracy: 37.34\n",
            "[133/150]: Training Loss: 2.554526605606079, Training Accuracy: 35.53\n",
            "Validation Loss: 2.48647084236145, Validation Accuracy: 37.39\n",
            "[134/150]: Training Loss: 2.549386672973633, Training Accuracy: 35.682\n",
            "Validation Loss: 2.4857484340667724, Validation Accuracy: 37.31\n",
            "[135/150]: Training Loss: 2.5567483425140383, Training Accuracy: 35.614\n",
            "Validation Loss: 2.485041952133179, Validation Accuracy: 37.4\n",
            "[136/150]: Training Loss: 2.548656129837036, Training Accuracy: 35.698\n",
            "Validation Loss: 2.485407066345215, Validation Accuracy: 37.43\n",
            "[137/150]: Training Loss: 2.542435131072998, Training Accuracy: 35.692\n",
            "Validation Loss: 2.485075092315674, Validation Accuracy: 37.25\n",
            "[138/150]: Training Loss: 2.55237006187439, Training Accuracy: 35.72\n",
            "Validation Loss: 2.484128713607788, Validation Accuracy: 37.39\n",
            "[139/150]: Training Loss: 2.5439516735076904, Training Accuracy: 35.732\n",
            "Validation Loss: 2.4843750953674317, Validation Accuracy: 37.4\n",
            "[140/150]: Training Loss: 2.546170501708984, Training Accuracy: 35.786\n",
            "Validation Loss: 2.4839417934417725, Validation Accuracy: 37.32\n",
            "[141/150]: Training Loss: 2.546264410018921, Training Accuracy: 35.748\n",
            "Validation Loss: 2.4843177318573, Validation Accuracy: 37.35\n",
            "[142/150]: Training Loss: 2.546672296524048, Training Accuracy: 35.486\n",
            "Validation Loss: 2.48406925201416, Validation Accuracy: 37.43\n",
            "[143/150]: Training Loss: 2.543995885848999, Training Accuracy: 35.828\n",
            "Validation Loss: 2.4839701652526855, Validation Accuracy: 37.31\n",
            "[144/150]: Training Loss: 2.54662841796875, Training Accuracy: 35.866\n",
            "Validation Loss: 2.4839941024780274, Validation Accuracy: 37.33\n",
            "[145/150]: Training Loss: 2.55161771774292, Training Accuracy: 35.778\n",
            "Validation Loss: 2.483786201477051, Validation Accuracy: 37.34\n",
            "[146/150]: Training Loss: 2.551975059509277, Training Accuracy: 35.694\n",
            "Validation Loss: 2.4838438034057617, Validation Accuracy: 37.34\n",
            "[147/150]: Training Loss: 2.5511072635650636, Training Accuracy: 35.556\n",
            "Validation Loss: 2.4838553428649903, Validation Accuracy: 37.33\n",
            "[148/150]: Training Loss: 2.54339656829834, Training Accuracy: 35.836\n",
            "Validation Loss: 2.4838125705718994, Validation Accuracy: 37.35\n",
            "[149/150]: Training Loss: 2.541078643798828, Training Accuracy: 35.776\n",
            "Validation Loss: 2.483800745010376, Validation Accuracy: 37.32\n",
            "[150/150]: Training Loss: 2.5504231452941895, Training Accuracy: 35.95\n",
            "Validation Loss: 2.4837899684906004, Validation Accuracy: 37.34\n",
            "**********************************************************************\n",
            "Test Loss: 2.4837899684906004, Test Accuracy: 37.34\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▆▇█▁▃</td></tr><tr><td>Test Loss</td><td>█▁▁▅▂</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▃▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Train Loss</td><td>██▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>37.34</td></tr><tr><td>Test Loss</td><td>2.48379</td></tr><tr><td>Train Accuracy</td><td>35.95</td></tr><tr><td>Train Loss</td><td>2.55042</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=2048 learning_rate=0.048 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/1r088cdb' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/1r088cdb</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_035348-1r088cdb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 4096\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_041810-9zk0s9ui</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/9zk0s9ui' target=\"_blank\">batch_size=4096 learning_rate=0.096 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/9zk0s9ui' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/9zk0s9ui</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.606667481935942, Training Accuracy: 1.056\n",
            "Validation Loss: 4.606671492258708, Validation Accuracy: 1.19\n",
            "[2/150]: Training Loss: 4.605730753678542, Training Accuracy: 1.08\n",
            "Validation Loss: 4.6042531331380205, Validation Accuracy: 1.28\n",
            "[3/150]: Training Loss: 4.602307979877178, Training Accuracy: 1.302\n",
            "Validation Loss: 4.599176406860352, Validation Accuracy: 1.68\n",
            "[4/150]: Training Loss: 4.5949787726769085, Training Accuracy: 1.758\n",
            "Validation Loss: 4.587771574656169, Validation Accuracy: 2.21\n",
            "[5/150]: Training Loss: 4.581083957965557, Training Accuracy: 2.472\n",
            "Validation Loss: 4.569830258687337, Validation Accuracy: 2.95\n",
            "[6/150]: Training Loss: 4.561565582568829, Training Accuracy: 2.844\n",
            "Validation Loss: 4.543959299723308, Validation Accuracy: 3.23\n",
            "[7/150]: Training Loss: 4.530556715451754, Training Accuracy: 3.226\n",
            "Validation Loss: 4.509588241577148, Validation Accuracy: 3.39\n",
            "[8/150]: Training Loss: 4.495710409604586, Training Accuracy: 3.406\n",
            "Validation Loss: 4.4675553639729815, Validation Accuracy: 3.84\n",
            "[9/150]: Training Loss: 4.451528292435866, Training Accuracy: 3.844\n",
            "Validation Loss: 4.417957146962483, Validation Accuracy: 4.41\n",
            "[10/150]: Training Loss: 4.401393670302171, Training Accuracy: 4.492\n",
            "Validation Loss: 4.361032485961914, Validation Accuracy: 5.12\n",
            "[11/150]: Training Loss: 4.34503434254573, Training Accuracy: 5.242\n",
            "Validation Loss: 4.297556241353353, Validation Accuracy: 6.11\n",
            "[12/150]: Training Loss: 4.284046503213736, Training Accuracy: 6.066\n",
            "Validation Loss: 4.233114242553711, Validation Accuracy: 7.08\n",
            "[13/150]: Training Loss: 4.224347151242769, Training Accuracy: 6.906\n",
            "Validation Loss: 4.170989990234375, Validation Accuracy: 7.67\n",
            "[14/150]: Training Loss: 4.16760961826031, Training Accuracy: 7.38\n",
            "Validation Loss: 4.115687370300293, Validation Accuracy: 8.27\n",
            "[15/150]: Training Loss: 4.123450425954966, Training Accuracy: 7.966\n",
            "Validation Loss: 4.068022092183431, Validation Accuracy: 8.87\n",
            "[16/150]: Training Loss: 4.084108205942007, Training Accuracy: 8.202\n",
            "Validation Loss: 4.029605388641357, Validation Accuracy: 9.54\n",
            "[17/150]: Training Loss: 4.048959145179162, Training Accuracy: 8.832\n",
            "Validation Loss: 3.9942894776662192, Validation Accuracy: 10.03\n",
            "[18/150]: Training Loss: 4.015094628700843, Training Accuracy: 9.204\n",
            "Validation Loss: 3.961328665415446, Validation Accuracy: 10.5\n",
            "[19/150]: Training Loss: 3.9947778628422665, Training Accuracy: 9.404\n",
            "Validation Loss: 3.9311509132385254, Validation Accuracy: 10.79\n",
            "[20/150]: Training Loss: 3.967010424687312, Training Accuracy: 10.074\n",
            "Validation Loss: 3.904151280721029, Validation Accuracy: 11.22\n",
            "[21/150]: Training Loss: 3.940983533859253, Training Accuracy: 10.394\n",
            "Validation Loss: 3.8733415603637695, Validation Accuracy: 12.06\n",
            "[22/150]: Training Loss: 3.9169222758366513, Training Accuracy: 10.968\n",
            "Validation Loss: 3.849616289138794, Validation Accuracy: 12.37\n",
            "[23/150]: Training Loss: 3.891700506210327, Training Accuracy: 11.278\n",
            "Validation Loss: 3.8228482405344644, Validation Accuracy: 12.65\n",
            "[24/150]: Training Loss: 3.8753539415506215, Training Accuracy: 11.308\n",
            "Validation Loss: 3.7991214593251548, Validation Accuracy: 13.44\n",
            "[25/150]: Training Loss: 3.8501394345210147, Training Accuracy: 11.864\n",
            "Validation Loss: 3.7803680896759033, Validation Accuracy: 13.28\n",
            "[26/150]: Training Loss: 3.8272896363185, Training Accuracy: 12.248\n",
            "Validation Loss: 3.7514286041259766, Validation Accuracy: 13.69\n",
            "[27/150]: Training Loss: 3.8028186651376576, Training Accuracy: 12.736\n",
            "Validation Loss: 3.7285123666127524, Validation Accuracy: 14.23\n",
            "[28/150]: Training Loss: 3.784411540398231, Training Accuracy: 12.956\n",
            "Validation Loss: 3.7080076535542807, Validation Accuracy: 14.9\n",
            "[29/150]: Training Loss: 3.769466730264517, Training Accuracy: 13.47\n",
            "Validation Loss: 3.685424486796061, Validation Accuracy: 15.03\n",
            "[30/150]: Training Loss: 3.7380496722001295, Training Accuracy: 13.826\n",
            "Validation Loss: 3.6666505336761475, Validation Accuracy: 15.27\n",
            "[31/150]: Training Loss: 3.72714613034175, Training Accuracy: 14.114\n",
            "Validation Loss: 3.647770563761393, Validation Accuracy: 15.3\n",
            "[32/150]: Training Loss: 3.704881869829618, Training Accuracy: 14.22\n",
            "Validation Loss: 3.6258915265401206, Validation Accuracy: 15.94\n",
            "[33/150]: Training Loss: 3.6916760481320896, Training Accuracy: 14.512\n",
            "Validation Loss: 3.605382760365804, Validation Accuracy: 16.3\n",
            "[34/150]: Training Loss: 3.669787131823026, Training Accuracy: 15.118\n",
            "Validation Loss: 3.585141976674398, Validation Accuracy: 16.42\n",
            "[35/150]: Training Loss: 3.6509481026576114, Training Accuracy: 15.094\n",
            "Validation Loss: 3.565377871195475, Validation Accuracy: 16.71\n",
            "[36/150]: Training Loss: 3.627447036596445, Training Accuracy: 15.684\n",
            "Validation Loss: 3.5500471591949463, Validation Accuracy: 17.2\n",
            "[37/150]: Training Loss: 3.6156828036675086, Training Accuracy: 15.97\n",
            "Validation Loss: 3.5334328015645347, Validation Accuracy: 17.48\n",
            "[38/150]: Training Loss: 3.60913295012254, Training Accuracy: 15.936\n",
            "Validation Loss: 3.518000523249308, Validation Accuracy: 17.68\n",
            "[39/150]: Training Loss: 3.588004075563871, Training Accuracy: 16.056\n",
            "Validation Loss: 3.5036707719167075, Validation Accuracy: 17.79\n",
            "[40/150]: Training Loss: 3.572803992491502, Training Accuracy: 16.404\n",
            "Validation Loss: 3.4900471369425454, Validation Accuracy: 18.34\n",
            "[41/150]: Training Loss: 3.553792586693397, Training Accuracy: 16.82\n",
            "Validation Loss: 3.4707635243733725, Validation Accuracy: 18.5\n",
            "[42/150]: Training Loss: 3.5353387685922475, Training Accuracy: 17.062\n",
            "Validation Loss: 3.451948642730713, Validation Accuracy: 19.1\n",
            "[43/150]: Training Loss: 3.5242697642399716, Training Accuracy: 17.448\n",
            "Validation Loss: 3.440772612889608, Validation Accuracy: 18.73\n",
            "[44/150]: Training Loss: 3.508471268873948, Training Accuracy: 17.51\n",
            "Validation Loss: 3.4259134928385415, Validation Accuracy: 19.32\n",
            "[45/150]: Training Loss: 3.495282558294443, Training Accuracy: 17.764\n",
            "Validation Loss: 3.4083244800567627, Validation Accuracy: 19.93\n",
            "[46/150]: Training Loss: 3.4788230932675877, Training Accuracy: 17.988\n",
            "Validation Loss: 3.3957042694091797, Validation Accuracy: 20.13\n",
            "[47/150]: Training Loss: 3.4677468629983754, Training Accuracy: 18.18\n",
            "Validation Loss: 3.381241957346598, Validation Accuracy: 20.12\n",
            "[48/150]: Training Loss: 3.4516031558697042, Training Accuracy: 18.396\n",
            "Validation Loss: 3.3734585444132485, Validation Accuracy: 20.41\n",
            "[49/150]: Training Loss: 3.444180488586426, Training Accuracy: 18.762\n",
            "Validation Loss: 3.355231444040934, Validation Accuracy: 20.56\n",
            "[50/150]: Training Loss: 3.431383829850417, Training Accuracy: 18.72\n",
            "Validation Loss: 3.3463191191355386, Validation Accuracy: 21.08\n",
            "[51/150]: Training Loss: 3.4191522414867697, Training Accuracy: 18.926\n",
            "Validation Loss: 3.3322107791900635, Validation Accuracy: 21.28\n",
            "[52/150]: Training Loss: 3.4109142010028544, Training Accuracy: 19.188\n",
            "Validation Loss: 3.3198678493499756, Validation Accuracy: 21.38\n",
            "[53/150]: Training Loss: 3.389728619502141, Training Accuracy: 19.502\n",
            "Validation Loss: 3.3070958455403647, Validation Accuracy: 21.67\n",
            "[54/150]: Training Loss: 3.3762773000277004, Training Accuracy: 19.762\n",
            "Validation Loss: 3.298886696497599, Validation Accuracy: 21.9\n",
            "[55/150]: Training Loss: 3.3614331025343676, Training Accuracy: 20.024\n",
            "Validation Loss: 3.2792559464772544, Validation Accuracy: 22.34\n",
            "[56/150]: Training Loss: 3.3627426624298096, Training Accuracy: 19.906\n",
            "Validation Loss: 3.2689783573150635, Validation Accuracy: 22.21\n",
            "[57/150]: Training Loss: 3.3459411401015062, Training Accuracy: 20.326\n",
            "Validation Loss: 3.262605905532837, Validation Accuracy: 22.4\n",
            "[58/150]: Training Loss: 3.328116068473229, Training Accuracy: 20.592\n",
            "Validation Loss: 3.2577860355377197, Validation Accuracy: 22.57\n",
            "[59/150]: Training Loss: 3.319796947332529, Training Accuracy: 20.718\n",
            "Validation Loss: 3.244077761967977, Validation Accuracy: 22.72\n",
            "[60/150]: Training Loss: 3.3160109519958496, Training Accuracy: 20.926\n",
            "Validation Loss: 3.232391436894735, Validation Accuracy: 23.21\n",
            "[61/150]: Training Loss: 3.3058263888725867, Training Accuracy: 20.962\n",
            "Validation Loss: 3.2259323596954346, Validation Accuracy: 23.38\n",
            "[62/150]: Training Loss: 3.2940543431502123, Training Accuracy: 21.21\n",
            "Validation Loss: 3.2195600668589273, Validation Accuracy: 23.22\n",
            "[63/150]: Training Loss: 3.2899109033437877, Training Accuracy: 21.408\n",
            "Validation Loss: 3.20307191212972, Validation Accuracy: 23.73\n",
            "[64/150]: Training Loss: 3.2760523832761326, Training Accuracy: 21.614\n",
            "Validation Loss: 3.19364595413208, Validation Accuracy: 23.82\n",
            "[65/150]: Training Loss: 3.2687179492070126, Training Accuracy: 21.762\n",
            "Validation Loss: 3.188945452372233, Validation Accuracy: 23.98\n",
            "[66/150]: Training Loss: 3.263365232027494, Training Accuracy: 21.714\n",
            "Validation Loss: 3.180399020512899, Validation Accuracy: 24.13\n",
            "[67/150]: Training Loss: 3.2552486749795766, Training Accuracy: 21.902\n",
            "Validation Loss: 3.169262409210205, Validation Accuracy: 24.32\n",
            "[68/150]: Training Loss: 3.2468105462881236, Training Accuracy: 22.02\n",
            "Validation Loss: 3.1592602729797363, Validation Accuracy: 24.7\n",
            "[69/150]: Training Loss: 3.236006791775043, Training Accuracy: 22.332\n",
            "Validation Loss: 3.1546688079833984, Validation Accuracy: 24.36\n",
            "[70/150]: Training Loss: 3.2279206055861254, Training Accuracy: 22.532\n",
            "Validation Loss: 3.1467626094818115, Validation Accuracy: 24.62\n",
            "[71/150]: Training Loss: 3.2204081278580885, Training Accuracy: 22.422\n",
            "Validation Loss: 3.142202456792196, Validation Accuracy: 25.23\n",
            "[72/150]: Training Loss: 3.2053526181441088, Training Accuracy: 22.976\n",
            "Validation Loss: 3.1304725805918374, Validation Accuracy: 25.06\n",
            "[73/150]: Training Loss: 3.2087103036733775, Training Accuracy: 22.836\n",
            "Validation Loss: 3.121542453765869, Validation Accuracy: 25.43\n",
            "[74/150]: Training Loss: 3.21028942328233, Training Accuracy: 22.914\n",
            "Validation Loss: 3.1152079900105796, Validation Accuracy: 25.46\n",
            "[75/150]: Training Loss: 3.1951534748077393, Training Accuracy: 23.172\n",
            "Validation Loss: 3.1097386678059897, Validation Accuracy: 25.91\n",
            "[76/150]: Training Loss: 3.187066591702975, Training Accuracy: 23.242\n",
            "Validation Loss: 3.1020663579305015, Validation Accuracy: 25.53\n",
            "[77/150]: Training Loss: 3.181211379858164, Training Accuracy: 23.288\n",
            "Validation Loss: 3.0956215858459473, Validation Accuracy: 26.07\n",
            "[78/150]: Training Loss: 3.1798458099365234, Training Accuracy: 23.354\n",
            "Validation Loss: 3.092714786529541, Validation Accuracy: 25.77\n",
            "[79/150]: Training Loss: 3.1642683102534366, Training Accuracy: 23.748\n",
            "Validation Loss: 3.0828441778818765, Validation Accuracy: 26.06\n",
            "[80/150]: Training Loss: 3.159773588180542, Training Accuracy: 23.54\n",
            "Validation Loss: 3.081282138824463, Validation Accuracy: 26.08\n",
            "[81/150]: Training Loss: 3.1536663678976207, Training Accuracy: 23.836\n",
            "Validation Loss: 3.0736547311147056, Validation Accuracy: 26.57\n",
            "[82/150]: Training Loss: 3.152631429525522, Training Accuracy: 23.908\n",
            "Validation Loss: 3.0681962966918945, Validation Accuracy: 26.27\n",
            "[83/150]: Training Loss: 3.148173992450421, Training Accuracy: 24.21\n",
            "Validation Loss: 3.060869057973226, Validation Accuracy: 26.52\n",
            "[84/150]: Training Loss: 3.144138354521531, Training Accuracy: 24.266\n",
            "Validation Loss: 3.057458480199178, Validation Accuracy: 26.17\n",
            "[85/150]: Training Loss: 3.1297646302443285, Training Accuracy: 24.25\n",
            "Validation Loss: 3.051652987798055, Validation Accuracy: 26.52\n",
            "[86/150]: Training Loss: 3.1335733120258036, Training Accuracy: 24.256\n",
            "Validation Loss: 3.049645980199178, Validation Accuracy: 26.69\n",
            "[87/150]: Training Loss: 3.1256027221679688, Training Accuracy: 24.65\n",
            "Validation Loss: 3.0414721171061196, Validation Accuracy: 26.59\n",
            "[88/150]: Training Loss: 3.1209965302393985, Training Accuracy: 24.62\n",
            "Validation Loss: 3.038153092066447, Validation Accuracy: 26.63\n",
            "[89/150]: Training Loss: 3.1104712302868185, Training Accuracy: 24.64\n",
            "Validation Loss: 3.0309388637542725, Validation Accuracy: 27.11\n",
            "[90/150]: Training Loss: 3.1125464806189904, Training Accuracy: 24.67\n",
            "Validation Loss: 3.0235498746236167, Validation Accuracy: 27.1\n",
            "[91/150]: Training Loss: 3.1025814093076267, Training Accuracy: 24.932\n",
            "Validation Loss: 3.0206593672434487, Validation Accuracy: 26.91\n",
            "[92/150]: Training Loss: 3.0976960842426005, Training Accuracy: 25.086\n",
            "Validation Loss: 3.0188937981923423, Validation Accuracy: 27.18\n",
            "[93/150]: Training Loss: 3.1000913106478176, Training Accuracy: 25.098\n",
            "Validation Loss: 3.0101601282755532, Validation Accuracy: 27.45\n",
            "[94/150]: Training Loss: 3.0842209595900316, Training Accuracy: 25.102\n",
            "Validation Loss: 3.0081886450449624, Validation Accuracy: 27.32\n",
            "[95/150]: Training Loss: 3.089082112679115, Training Accuracy: 25.238\n",
            "Validation Loss: 3.004807790120443, Validation Accuracy: 27.51\n",
            "[96/150]: Training Loss: 3.0870832113119273, Training Accuracy: 25.588\n",
            "Validation Loss: 2.9993834495544434, Validation Accuracy: 27.42\n",
            "[97/150]: Training Loss: 3.0780730064098654, Training Accuracy: 25.422\n",
            "Validation Loss: 2.9940427939097085, Validation Accuracy: 27.54\n",
            "[98/150]: Training Loss: 3.0749858892880955, Training Accuracy: 25.39\n",
            "Validation Loss: 2.992240826288859, Validation Accuracy: 27.52\n",
            "[99/150]: Training Loss: 3.0701116231771617, Training Accuracy: 25.484\n",
            "Validation Loss: 2.991929848988851, Validation Accuracy: 27.38\n",
            "[100/150]: Training Loss: 3.075368826205914, Training Accuracy: 25.532\n",
            "Validation Loss: 2.985892375310262, Validation Accuracy: 27.66\n",
            "[101/150]: Training Loss: 3.0686124104719896, Training Accuracy: 25.752\n",
            "Validation Loss: 2.9830666383107505, Validation Accuracy: 27.62\n",
            "[102/150]: Training Loss: 3.065825737439669, Training Accuracy: 25.708\n",
            "Validation Loss: 2.9795896212259927, Validation Accuracy: 27.75\n",
            "[103/150]: Training Loss: 3.0559082948244534, Training Accuracy: 25.714\n",
            "Validation Loss: 2.9758129914601645, Validation Accuracy: 27.99\n",
            "[104/150]: Training Loss: 3.055809112695547, Training Accuracy: 25.83\n",
            "Validation Loss: 2.9747377236684165, Validation Accuracy: 28.08\n",
            "[105/150]: Training Loss: 3.051142399127667, Training Accuracy: 26.058\n",
            "Validation Loss: 2.9694248040517173, Validation Accuracy: 28.1\n",
            "[106/150]: Training Loss: 3.052405449060293, Training Accuracy: 25.962\n",
            "Validation Loss: 2.967036406199137, Validation Accuracy: 28.21\n",
            "[107/150]: Training Loss: 3.050378047502958, Training Accuracy: 26.184\n",
            "Validation Loss: 2.9633421103159585, Validation Accuracy: 28.25\n",
            "[108/150]: Training Loss: 3.049203927700336, Training Accuracy: 25.912\n",
            "Validation Loss: 2.960350831349691, Validation Accuracy: 28.27\n",
            "[109/150]: Training Loss: 3.039809043590839, Training Accuracy: 26.25\n",
            "Validation Loss: 2.9594175815582275, Validation Accuracy: 28.54\n",
            "[110/150]: Training Loss: 3.0399123521951528, Training Accuracy: 26.228\n",
            "Validation Loss: 2.956749359766642, Validation Accuracy: 28.35\n",
            "[111/150]: Training Loss: 3.035701183172373, Training Accuracy: 26.196\n",
            "Validation Loss: 2.953205664952596, Validation Accuracy: 28.27\n",
            "[112/150]: Training Loss: 3.033349422308115, Training Accuracy: 26.232\n",
            "Validation Loss: 2.9511603514353433, Validation Accuracy: 28.56\n",
            "[113/150]: Training Loss: 3.0366667050581713, Training Accuracy: 26.474\n",
            "Validation Loss: 2.9514257113138833, Validation Accuracy: 28.41\n",
            "[114/150]: Training Loss: 3.028848886489868, Training Accuracy: 26.362\n",
            "Validation Loss: 2.948322137196859, Validation Accuracy: 28.45\n",
            "[115/150]: Training Loss: 3.028550863265991, Training Accuracy: 26.532\n",
            "Validation Loss: 2.9465808073679605, Validation Accuracy: 28.55\n",
            "[116/150]: Training Loss: 3.0293847230764537, Training Accuracy: 26.382\n",
            "Validation Loss: 2.943213860193888, Validation Accuracy: 28.71\n",
            "[117/150]: Training Loss: 3.0208603785588193, Training Accuracy: 26.396\n",
            "Validation Loss: 2.944130261739095, Validation Accuracy: 28.47\n",
            "[118/150]: Training Loss: 3.027190428513747, Training Accuracy: 26.268\n",
            "Validation Loss: 2.9417835076649985, Validation Accuracy: 28.72\n",
            "[119/150]: Training Loss: 3.018063325148362, Training Accuracy: 26.58\n",
            "Validation Loss: 2.9406763712565103, Validation Accuracy: 28.57\n",
            "[120/150]: Training Loss: 3.0236737177922177, Training Accuracy: 26.436\n",
            "Validation Loss: 2.938833475112915, Validation Accuracy: 28.71\n",
            "[121/150]: Training Loss: 3.025139331817627, Training Accuracy: 26.506\n",
            "Validation Loss: 2.9386914571126304, Validation Accuracy: 28.59\n",
            "[122/150]: Training Loss: 3.019059492991521, Training Accuracy: 26.644\n",
            "Validation Loss: 2.9362939993540444, Validation Accuracy: 28.92\n",
            "[123/150]: Training Loss: 3.026591025865995, Training Accuracy: 26.702\n",
            "Validation Loss: 2.93481437365214, Validation Accuracy: 28.77\n",
            "[124/150]: Training Loss: 3.0196935580326962, Training Accuracy: 26.838\n",
            "Validation Loss: 2.9332067171732583, Validation Accuracy: 28.81\n",
            "[125/150]: Training Loss: 3.011383111660297, Training Accuracy: 26.388\n",
            "Validation Loss: 2.9336490631103516, Validation Accuracy: 28.87\n",
            "[126/150]: Training Loss: 3.0112004463489237, Training Accuracy: 26.728\n",
            "Validation Loss: 2.932124614715576, Validation Accuracy: 28.88\n",
            "[127/150]: Training Loss: 3.0043031985943136, Training Accuracy: 26.746\n",
            "Validation Loss: 2.9308346112569175, Validation Accuracy: 28.92\n",
            "[128/150]: Training Loss: 3.0122800973745494, Training Accuracy: 26.708\n",
            "Validation Loss: 2.9301002820332847, Validation Accuracy: 28.88\n",
            "[129/150]: Training Loss: 3.0128011336693397, Training Accuracy: 26.618\n",
            "Validation Loss: 2.9301366806030273, Validation Accuracy: 28.94\n",
            "[130/150]: Training Loss: 3.021453068806575, Training Accuracy: 26.574\n",
            "Validation Loss: 2.9286012649536133, Validation Accuracy: 28.95\n",
            "[131/150]: Training Loss: 3.0087369772104116, Training Accuracy: 26.808\n",
            "Validation Loss: 2.9288355509440103, Validation Accuracy: 28.86\n",
            "[132/150]: Training Loss: 3.009090882081252, Training Accuracy: 26.898\n",
            "Validation Loss: 2.927795886993408, Validation Accuracy: 28.9\n",
            "[133/150]: Training Loss: 3.0040540328392615, Training Accuracy: 26.832\n",
            "Validation Loss: 2.926760117212931, Validation Accuracy: 28.94\n",
            "[134/150]: Training Loss: 3.0087718413426328, Training Accuracy: 26.818\n",
            "Validation Loss: 2.927534898122152, Validation Accuracy: 28.89\n",
            "[135/150]: Training Loss: 3.0036952312176046, Training Accuracy: 26.822\n",
            "Validation Loss: 2.926776965459188, Validation Accuracy: 28.95\n",
            "[136/150]: Training Loss: 3.006104597678551, Training Accuracy: 26.948\n",
            "Validation Loss: 2.925653616587321, Validation Accuracy: 29.02\n",
            "[137/150]: Training Loss: 3.005603166726919, Training Accuracy: 26.876\n",
            "Validation Loss: 2.925398667653402, Validation Accuracy: 28.93\n",
            "[138/150]: Training Loss: 3.008038960970365, Training Accuracy: 26.988\n",
            "Validation Loss: 2.925060510635376, Validation Accuracy: 28.97\n",
            "[139/150]: Training Loss: 3.006507231638982, Training Accuracy: 26.72\n",
            "Validation Loss: 2.9248908360799155, Validation Accuracy: 28.91\n",
            "[140/150]: Training Loss: 3.0022271413069506, Training Accuracy: 26.81\n",
            "Validation Loss: 2.924588123957316, Validation Accuracy: 29.03\n",
            "[141/150]: Training Loss: 3.011791944503784, Training Accuracy: 26.896\n",
            "Validation Loss: 2.925060828526815, Validation Accuracy: 28.91\n",
            "[142/150]: Training Loss: 3.007263256953313, Training Accuracy: 26.982\n",
            "Validation Loss: 2.924833297729492, Validation Accuracy: 28.9\n",
            "[143/150]: Training Loss: 3.0038411617279053, Training Accuracy: 26.82\n",
            "Validation Loss: 2.924312194188436, Validation Accuracy: 29.02\n",
            "[144/150]: Training Loss: 3.0007228667919454, Training Accuracy: 26.974\n",
            "Validation Loss: 2.9242995580037436, Validation Accuracy: 28.92\n",
            "[145/150]: Training Loss: 3.0014155277839074, Training Accuracy: 26.706\n",
            "Validation Loss: 2.92417041460673, Validation Accuracy: 28.95\n",
            "[146/150]: Training Loss: 3.0123214538280783, Training Accuracy: 26.89\n",
            "Validation Loss: 2.924126386642456, Validation Accuracy: 28.91\n",
            "[147/150]: Training Loss: 3.008448545749371, Training Accuracy: 26.812\n",
            "Validation Loss: 2.9240718682607016, Validation Accuracy: 28.92\n",
            "[148/150]: Training Loss: 3.0003258815178504, Training Accuracy: 26.644\n",
            "Validation Loss: 2.9240763187408447, Validation Accuracy: 28.91\n",
            "[149/150]: Training Loss: 3.0013261208167443, Training Accuracy: 26.96\n",
            "Validation Loss: 2.924076795578003, Validation Accuracy: 28.9\n",
            "[150/150]: Training Loss: 3.001100833599384, Training Accuracy: 26.888\n",
            "Validation Loss: 2.9240763187408447, Validation Accuracy: 28.9\n",
            "**********************************************************************\n",
            "Test Loss: 2.9240763187408447, Test Accuracy: 28.9\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▁▇</td></tr><tr><td>Test Loss</td><td>▁█▂</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Train Loss</td><td>███▇▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>28.9</td></tr><tr><td>Test Loss</td><td>2.92408</td></tr><tr><td>Train Accuracy</td><td>26.888</td></tr><tr><td>Train Loss</td><td>3.0011</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=4096 learning_rate=0.096 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/9zk0s9ui' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/9zk0s9ui</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_041810-9zk0s9ui/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 8192\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_044434-2i4ynggq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/2i4ynggq' target=\"_blank\">batch_size=8192 learning_rate=0.192 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/2i4ynggq' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/2i4ynggq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.606760025024414, Training Accuracy: 1.154\n",
            "Validation Loss: 4.6062242190043134, Validation Accuracy: 1.16\n",
            "[2/150]: Training Loss: 4.606538149026724, Training Accuracy: 1.112\n",
            "Validation Loss: 4.605688412984212, Validation Accuracy: 1.25\n",
            "[3/150]: Training Loss: 4.605847468742957, Training Accuracy: 1.244\n",
            "Validation Loss: 4.6046427090962725, Validation Accuracy: 1.37\n",
            "[4/150]: Training Loss: 4.604880149547871, Training Accuracy: 1.37\n",
            "Validation Loss: 4.603097438812256, Validation Accuracy: 1.75\n",
            "[5/150]: Training Loss: 4.602994515345647, Training Accuracy: 1.682\n",
            "Validation Loss: 4.600901285807292, Validation Accuracy: 2.09\n",
            "[6/150]: Training Loss: 4.600499519935021, Training Accuracy: 1.924\n",
            "Validation Loss: 4.597598870595296, Validation Accuracy: 2.29\n",
            "[7/150]: Training Loss: 4.596665969261756, Training Accuracy: 2.098\n",
            "Validation Loss: 4.592558860778809, Validation Accuracy: 2.64\n",
            "[8/150]: Training Loss: 4.591101573063777, Training Accuracy: 2.486\n",
            "Validation Loss: 4.584865570068359, Validation Accuracy: 3.16\n",
            "[9/150]: Training Loss: 4.5825383479778585, Training Accuracy: 3.076\n",
            "Validation Loss: 4.575003465016683, Validation Accuracy: 3.84\n",
            "[10/150]: Training Loss: 4.571999696584848, Training Accuracy: 3.336\n",
            "Validation Loss: 4.562810103098552, Validation Accuracy: 3.58\n",
            "[11/150]: Training Loss: 4.559818267822266, Training Accuracy: 3.298\n",
            "Validation Loss: 4.548019091288249, Validation Accuracy: 3.45\n",
            "[12/150]: Training Loss: 4.544124970069299, Training Accuracy: 3.284\n",
            "Validation Loss: 4.530599912007649, Validation Accuracy: 3.53\n",
            "[13/150]: Training Loss: 4.526562433976394, Training Accuracy: 3.516\n",
            "Validation Loss: 4.510464509328206, Validation Accuracy: 3.87\n",
            "[14/150]: Training Loss: 4.505583102886494, Training Accuracy: 3.706\n",
            "Validation Loss: 4.487647533416748, Validation Accuracy: 3.96\n",
            "[15/150]: Training Loss: 4.4825364626370945, Training Accuracy: 3.97\n",
            "Validation Loss: 4.462220668792725, Validation Accuracy: 4.06\n",
            "[16/150]: Training Loss: 4.456124305725098, Training Accuracy: 4.04\n",
            "Validation Loss: 4.434208234151204, Validation Accuracy: 4.26\n",
            "[17/150]: Training Loss: 4.428849366995005, Training Accuracy: 4.238\n",
            "Validation Loss: 4.404198328653972, Validation Accuracy: 4.68\n",
            "[18/150]: Training Loss: 4.398712635040283, Training Accuracy: 4.382\n",
            "Validation Loss: 4.372480710347493, Validation Accuracy: 5.08\n",
            "[19/150]: Training Loss: 4.369481490208552, Training Accuracy: 4.73\n",
            "Validation Loss: 4.339550971984863, Validation Accuracy: 5.32\n",
            "[20/150]: Training Loss: 4.3372171842134914, Training Accuracy: 5.082\n",
            "Validation Loss: 4.306232770284017, Validation Accuracy: 5.64\n",
            "[21/150]: Training Loss: 4.306336256173941, Training Accuracy: 5.376\n",
            "Validation Loss: 4.272884527842204, Validation Accuracy: 5.85\n",
            "[22/150]: Training Loss: 4.27412722660945, Training Accuracy: 5.544\n",
            "Validation Loss: 4.240369955698649, Validation Accuracy: 6.19\n",
            "[23/150]: Training Loss: 4.245465902181772, Training Accuracy: 6.004\n",
            "Validation Loss: 4.209603627522786, Validation Accuracy: 6.6\n",
            "[24/150]: Training Loss: 4.2155220325176535, Training Accuracy: 6.35\n",
            "Validation Loss: 4.179777940114339, Validation Accuracy: 6.96\n",
            "[25/150]: Training Loss: 4.185919578258808, Training Accuracy: 6.624\n",
            "Validation Loss: 4.151111761728923, Validation Accuracy: 7.2\n",
            "[26/150]: Training Loss: 4.160709711221548, Training Accuracy: 6.94\n",
            "Validation Loss: 4.125725905100505, Validation Accuracy: 7.66\n",
            "[27/150]: Training Loss: 4.137477397918701, Training Accuracy: 7.344\n",
            "Validation Loss: 4.100640455881755, Validation Accuracy: 8.21\n",
            "[28/150]: Training Loss: 4.1160736450782185, Training Accuracy: 7.532\n",
            "Validation Loss: 4.079355557759603, Validation Accuracy: 8.25\n",
            "[29/150]: Training Loss: 4.098053125234751, Training Accuracy: 7.884\n",
            "Validation Loss: 4.058371702829997, Validation Accuracy: 8.55\n",
            "[30/150]: Training Loss: 4.086406047527607, Training Accuracy: 7.96\n",
            "Validation Loss: 4.0379958152771, Validation Accuracy: 9.04\n",
            "[31/150]: Training Loss: 4.063176705287053, Training Accuracy: 8.412\n",
            "Validation Loss: 4.020632902781169, Validation Accuracy: 9.28\n",
            "[32/150]: Training Loss: 4.054692195012019, Training Accuracy: 8.59\n",
            "Validation Loss: 4.0030783812205, Validation Accuracy: 9.63\n",
            "[33/150]: Training Loss: 4.032094441927397, Training Accuracy: 8.758\n",
            "Validation Loss: 3.987109978993734, Validation Accuracy: 9.71\n",
            "[34/150]: Training Loss: 4.022676577934852, Training Accuracy: 8.996\n",
            "Validation Loss: 3.9713465372721353, Validation Accuracy: 9.76\n",
            "[35/150]: Training Loss: 4.000812567197359, Training Accuracy: 9.384\n",
            "Validation Loss: 3.9538060824076333, Validation Accuracy: 10.19\n",
            "[36/150]: Training Loss: 3.9866452400500956, Training Accuracy: 9.608\n",
            "Validation Loss: 3.93888529141744, Validation Accuracy: 10.44\n",
            "[37/150]: Training Loss: 3.9776483315687914, Training Accuracy: 9.874\n",
            "Validation Loss: 3.922476847966512, Validation Accuracy: 10.56\n",
            "[38/150]: Training Loss: 3.9651166108938365, Training Accuracy: 9.862\n",
            "Validation Loss: 3.90744686126709, Validation Accuracy: 10.81\n",
            "[39/150]: Training Loss: 3.948905064509465, Training Accuracy: 10.218\n",
            "Validation Loss: 3.89294163386027, Validation Accuracy: 11.35\n",
            "[40/150]: Training Loss: 3.937441715827355, Training Accuracy: 10.372\n",
            "Validation Loss: 3.8783599535624185, Validation Accuracy: 11.22\n",
            "[41/150]: Training Loss: 3.9230183821458082, Training Accuracy: 10.612\n",
            "Validation Loss: 3.8631796836853027, Validation Accuracy: 11.66\n",
            "[42/150]: Training Loss: 3.912040582069984, Training Accuracy: 10.784\n",
            "Validation Loss: 3.8491604328155518, Validation Accuracy: 11.86\n",
            "[43/150]: Training Loss: 3.8964080076951246, Training Accuracy: 11.06\n",
            "Validation Loss: 3.8363358974456787, Validation Accuracy: 12.08\n",
            "[44/150]: Training Loss: 3.8831426180326023, Training Accuracy: 11.18\n",
            "Validation Loss: 3.825343132019043, Validation Accuracy: 12.27\n",
            "[45/150]: Training Loss: 3.8707955250373254, Training Accuracy: 11.322\n",
            "Validation Loss: 3.811360756556193, Validation Accuracy: 12.62\n",
            "[46/150]: Training Loss: 3.860361337661743, Training Accuracy: 11.54\n",
            "Validation Loss: 3.795260190963745, Validation Accuracy: 12.84\n",
            "[47/150]: Training Loss: 3.8526153014256406, Training Accuracy: 11.646\n",
            "Validation Loss: 3.784214735031128, Validation Accuracy: 13.13\n",
            "[48/150]: Training Loss: 3.8345638788663425, Training Accuracy: 11.768\n",
            "Validation Loss: 3.7713340123494468, Validation Accuracy: 13.34\n",
            "[49/150]: Training Loss: 3.8285101377047024, Training Accuracy: 12.096\n",
            "Validation Loss: 3.76099960009257, Validation Accuracy: 13.47\n",
            "[50/150]: Training Loss: 3.816150261805608, Training Accuracy: 12.372\n",
            "Validation Loss: 3.7498886585235596, Validation Accuracy: 13.5\n",
            "[51/150]: Training Loss: 3.8035758091853213, Training Accuracy: 12.228\n",
            "Validation Loss: 3.7394703229268393, Validation Accuracy: 13.84\n",
            "[52/150]: Training Loss: 3.7984210527860203, Training Accuracy: 12.59\n",
            "Validation Loss: 3.726769765218099, Validation Accuracy: 14.28\n",
            "[53/150]: Training Loss: 3.786878989293025, Training Accuracy: 12.82\n",
            "Validation Loss: 3.719003756841024, Validation Accuracy: 14.37\n",
            "[54/150]: Training Loss: 3.774993529686561, Training Accuracy: 12.926\n",
            "Validation Loss: 3.707072099049886, Validation Accuracy: 14.61\n",
            "[55/150]: Training Loss: 3.765552227313702, Training Accuracy: 13.176\n",
            "Validation Loss: 3.698981682459513, Validation Accuracy: 14.82\n",
            "[56/150]: Training Loss: 3.759232466037457, Training Accuracy: 13.316\n",
            "Validation Loss: 3.6923623085021973, Validation Accuracy: 14.6\n",
            "[57/150]: Training Loss: 3.746529505803035, Training Accuracy: 13.31\n",
            "Validation Loss: 3.6810197035471597, Validation Accuracy: 14.95\n",
            "[58/150]: Training Loss: 3.739341607460609, Training Accuracy: 13.734\n",
            "Validation Loss: 3.673978885014852, Validation Accuracy: 14.96\n",
            "[59/150]: Training Loss: 3.734126567840576, Training Accuracy: 13.616\n",
            "Validation Loss: 3.6645074685414634, Validation Accuracy: 15.33\n",
            "[60/150]: Training Loss: 3.724345353933481, Training Accuracy: 13.872\n",
            "Validation Loss: 3.654491583506266, Validation Accuracy: 15.44\n",
            "[61/150]: Training Loss: 3.717476936487051, Training Accuracy: 13.9\n",
            "Validation Loss: 3.645918528238932, Validation Accuracy: 15.77\n",
            "[62/150]: Training Loss: 3.7088359869443455, Training Accuracy: 14.286\n",
            "Validation Loss: 3.638837734858195, Validation Accuracy: 15.73\n",
            "[63/150]: Training Loss: 3.7012051068819485, Training Accuracy: 14.23\n",
            "Validation Loss: 3.629307826360067, Validation Accuracy: 16.17\n",
            "[64/150]: Training Loss: 3.6978440284729004, Training Accuracy: 14.368\n",
            "Validation Loss: 3.6212432384490967, Validation Accuracy: 16.28\n",
            "[65/150]: Training Loss: 3.685521070773785, Training Accuracy: 14.538\n",
            "Validation Loss: 3.6163320541381836, Validation Accuracy: 16.03\n",
            "[66/150]: Training Loss: 3.681316999288706, Training Accuracy: 14.432\n",
            "Validation Loss: 3.606062889099121, Validation Accuracy: 16.41\n",
            "[67/150]: Training Loss: 3.6727612752180834, Training Accuracy: 14.784\n",
            "Validation Loss: 3.5995655059814453, Validation Accuracy: 16.47\n",
            "[68/150]: Training Loss: 3.670240182142991, Training Accuracy: 14.814\n",
            "Validation Loss: 3.5927998224894204, Validation Accuracy: 16.65\n",
            "[69/150]: Training Loss: 3.655353234364436, Training Accuracy: 14.904\n",
            "Validation Loss: 3.5843842029571533, Validation Accuracy: 16.92\n",
            "[70/150]: Training Loss: 3.6477879744309645, Training Accuracy: 15.132\n",
            "Validation Loss: 3.5797649224599204, Validation Accuracy: 16.89\n",
            "[71/150]: Training Loss: 3.6427008555485654, Training Accuracy: 15.29\n",
            "Validation Loss: 3.5710333983103433, Validation Accuracy: 17.1\n",
            "[72/150]: Training Loss: 3.6349054116469164, Training Accuracy: 15.326\n",
            "Validation Loss: 3.564692815144857, Validation Accuracy: 17.25\n",
            "[73/150]: Training Loss: 3.631576318007249, Training Accuracy: 15.41\n",
            "Validation Loss: 3.557377735773722, Validation Accuracy: 17.21\n",
            "[74/150]: Training Loss: 3.625403587634747, Training Accuracy: 15.382\n",
            "Validation Loss: 3.5509705543518066, Validation Accuracy: 17.52\n",
            "[75/150]: Training Loss: 3.615908696101262, Training Accuracy: 15.832\n",
            "Validation Loss: 3.54543407758077, Validation Accuracy: 17.53\n",
            "[76/150]: Training Loss: 3.617959187580989, Training Accuracy: 15.9\n",
            "Validation Loss: 3.5409313837687173, Validation Accuracy: 17.63\n",
            "[77/150]: Training Loss: 3.61479036624615, Training Accuracy: 15.924\n",
            "Validation Loss: 3.5348545710245767, Validation Accuracy: 17.74\n",
            "[78/150]: Training Loss: 3.599782026731051, Training Accuracy: 15.856\n",
            "Validation Loss: 3.52974534034729, Validation Accuracy: 17.95\n",
            "[79/150]: Training Loss: 3.5990573993095984, Training Accuracy: 16.238\n",
            "Validation Loss: 3.5212018489837646, Validation Accuracy: 17.94\n",
            "[80/150]: Training Loss: 3.592515175159161, Training Accuracy: 16.154\n",
            "Validation Loss: 3.5165327390034995, Validation Accuracy: 18.21\n",
            "[81/150]: Training Loss: 3.58927504832928, Training Accuracy: 16.29\n",
            "Validation Loss: 3.5114110310872397, Validation Accuracy: 18.24\n",
            "[82/150]: Training Loss: 3.5821184745201697, Training Accuracy: 16.33\n",
            "Validation Loss: 3.5056587855021157, Validation Accuracy: 18.24\n",
            "[83/150]: Training Loss: 3.571472461407001, Training Accuracy: 16.528\n",
            "Validation Loss: 3.4996766249338784, Validation Accuracy: 18.35\n",
            "[84/150]: Training Loss: 3.5723203145540676, Training Accuracy: 16.702\n",
            "Validation Loss: 3.4941263993581138, Validation Accuracy: 18.52\n",
            "[85/150]: Training Loss: 3.5644964621617246, Training Accuracy: 16.56\n",
            "Validation Loss: 3.4899566968282065, Validation Accuracy: 18.69\n",
            "[86/150]: Training Loss: 3.5610409883352427, Training Accuracy: 16.728\n",
            "Validation Loss: 3.486067454020182, Validation Accuracy: 18.62\n",
            "[87/150]: Training Loss: 3.5533294677734375, Training Accuracy: 16.754\n",
            "Validation Loss: 3.478351672490438, Validation Accuracy: 18.82\n",
            "[88/150]: Training Loss: 3.5531911116379957, Training Accuracy: 16.91\n",
            "Validation Loss: 3.476896047592163, Validation Accuracy: 19.08\n",
            "[89/150]: Training Loss: 3.5495745952312765, Training Accuracy: 16.96\n",
            "Validation Loss: 3.4712963104248047, Validation Accuracy: 18.83\n",
            "[90/150]: Training Loss: 3.54185702250554, Training Accuracy: 16.884\n",
            "Validation Loss: 3.4682395458221436, Validation Accuracy: 19.06\n",
            "[91/150]: Training Loss: 3.5386377297914944, Training Accuracy: 17.084\n",
            "Validation Loss: 3.4618451595306396, Validation Accuracy: 19.09\n",
            "[92/150]: Training Loss: 3.5369215928591213, Training Accuracy: 17.188\n",
            "Validation Loss: 3.45944881439209, Validation Accuracy: 19.09\n",
            "[93/150]: Training Loss: 3.534542725636409, Training Accuracy: 17.522\n",
            "Validation Loss: 3.4543633460998535, Validation Accuracy: 19.27\n",
            "[94/150]: Training Loss: 3.527037657224215, Training Accuracy: 17.302\n",
            "Validation Loss: 3.4519523779551187, Validation Accuracy: 19.39\n",
            "[95/150]: Training Loss: 3.5207606095534105, Training Accuracy: 17.612\n",
            "Validation Loss: 3.4471476078033447, Validation Accuracy: 19.3\n",
            "[96/150]: Training Loss: 3.523169627556434, Training Accuracy: 17.366\n",
            "Validation Loss: 3.44489852587382, Validation Accuracy: 19.46\n",
            "[97/150]: Training Loss: 3.514947872895461, Training Accuracy: 17.576\n",
            "Validation Loss: 3.440441687901815, Validation Accuracy: 19.53\n",
            "[98/150]: Training Loss: 3.516437218739436, Training Accuracy: 17.636\n",
            "Validation Loss: 3.4375240008036294, Validation Accuracy: 19.7\n",
            "[99/150]: Training Loss: 3.515314450630775, Training Accuracy: 17.722\n",
            "Validation Loss: 3.4350737730662027, Validation Accuracy: 19.64\n",
            "[100/150]: Training Loss: 3.5090638674222507, Training Accuracy: 17.904\n",
            "Validation Loss: 3.430509169896444, Validation Accuracy: 19.79\n",
            "[101/150]: Training Loss: 3.504866893474872, Training Accuracy: 17.752\n",
            "Validation Loss: 3.4284051259358725, Validation Accuracy: 19.74\n",
            "[102/150]: Training Loss: 3.4917823534745436, Training Accuracy: 17.848\n",
            "Validation Loss: 3.425445636113485, Validation Accuracy: 19.83\n",
            "[103/150]: Training Loss: 3.5007056456345778, Training Accuracy: 18.006\n",
            "Validation Loss: 3.4235105514526367, Validation Accuracy: 19.77\n",
            "[104/150]: Training Loss: 3.488600785915668, Training Accuracy: 18.088\n",
            "Validation Loss: 3.419638713200887, Validation Accuracy: 19.93\n",
            "[105/150]: Training Loss: 3.4892408664409933, Training Accuracy: 17.904\n",
            "Validation Loss: 3.4154659112294516, Validation Accuracy: 20.02\n",
            "[106/150]: Training Loss: 3.483979060099675, Training Accuracy: 18.014\n",
            "Validation Loss: 3.413444995880127, Validation Accuracy: 20.12\n",
            "[107/150]: Training Loss: 3.4866304030785193, Training Accuracy: 18.084\n",
            "Validation Loss: 3.4109641710917153, Validation Accuracy: 20.04\n",
            "[108/150]: Training Loss: 3.486282256933359, Training Accuracy: 18.144\n",
            "Validation Loss: 3.4091246922810874, Validation Accuracy: 20.08\n",
            "[109/150]: Training Loss: 3.481999470637395, Training Accuracy: 17.968\n",
            "Validation Loss: 3.4066150983174643, Validation Accuracy: 20.18\n",
            "[110/150]: Training Loss: 3.4790745148291955, Training Accuracy: 18.248\n",
            "Validation Loss: 3.4039930502573648, Validation Accuracy: 20.22\n",
            "[111/150]: Training Loss: 3.476212684924786, Training Accuracy: 18.252\n",
            "Validation Loss: 3.402389129002889, Validation Accuracy: 20.29\n",
            "[112/150]: Training Loss: 3.474406755887545, Training Accuracy: 18.33\n",
            "Validation Loss: 3.400217294692993, Validation Accuracy: 20.3\n",
            "[113/150]: Training Loss: 3.4745553456819973, Training Accuracy: 18.042\n",
            "Validation Loss: 3.397853215535482, Validation Accuracy: 20.22\n",
            "[114/150]: Training Loss: 3.474104697887714, Training Accuracy: 18.246\n",
            "Validation Loss: 3.3972776730855307, Validation Accuracy: 20.24\n",
            "[115/150]: Training Loss: 3.4714555190159726, Training Accuracy: 18.372\n",
            "Validation Loss: 3.394416252772013, Validation Accuracy: 20.37\n",
            "[116/150]: Training Loss: 3.4606630985553446, Training Accuracy: 18.416\n",
            "Validation Loss: 3.3930967648824057, Validation Accuracy: 20.55\n",
            "[117/150]: Training Loss: 3.4698875684004564, Training Accuracy: 18.464\n",
            "Validation Loss: 3.3917219638824463, Validation Accuracy: 20.42\n",
            "[118/150]: Training Loss: 3.46042590874892, Training Accuracy: 18.584\n",
            "Validation Loss: 3.3900171915690103, Validation Accuracy: 20.36\n",
            "[119/150]: Training Loss: 3.459270037137545, Training Accuracy: 18.616\n",
            "Validation Loss: 3.38869571685791, Validation Accuracy: 20.44\n",
            "[120/150]: Training Loss: 3.4700822096604567, Training Accuracy: 18.456\n",
            "Validation Loss: 3.3867367108662925, Validation Accuracy: 20.59\n",
            "[121/150]: Training Loss: 3.4599712445185733, Training Accuracy: 18.566\n",
            "Validation Loss: 3.385423421859741, Validation Accuracy: 20.63\n",
            "[122/150]: Training Loss: 3.461164199388944, Training Accuracy: 18.588\n",
            "Validation Loss: 3.3854024410247803, Validation Accuracy: 20.65\n",
            "[123/150]: Training Loss: 3.4520862102508545, Training Accuracy: 18.478\n",
            "Validation Loss: 3.38422425587972, Validation Accuracy: 20.61\n",
            "[124/150]: Training Loss: 3.4461356676541843, Training Accuracy: 18.644\n",
            "Validation Loss: 3.3832034269968667, Validation Accuracy: 20.59\n",
            "[125/150]: Training Loss: 3.4624485419346738, Training Accuracy: 18.532\n",
            "Validation Loss: 3.3818772633870444, Validation Accuracy: 20.62\n",
            "[126/150]: Training Loss: 3.4651179497058573, Training Accuracy: 18.372\n",
            "Validation Loss: 3.381105343500773, Validation Accuracy: 20.62\n",
            "[127/150]: Training Loss: 3.4483938950758715, Training Accuracy: 18.59\n",
            "Validation Loss: 3.37996514638265, Validation Accuracy: 20.77\n",
            "[128/150]: Training Loss: 3.443562305890597, Training Accuracy: 18.652\n",
            "Validation Loss: 3.379426956176758, Validation Accuracy: 20.72\n",
            "[129/150]: Training Loss: 3.4467421678396373, Training Accuracy: 18.606\n",
            "Validation Loss: 3.3791234493255615, Validation Accuracy: 20.52\n",
            "[130/150]: Training Loss: 3.4486422538757324, Training Accuracy: 18.642\n",
            "Validation Loss: 3.3781088987986245, Validation Accuracy: 20.66\n",
            "[131/150]: Training Loss: 3.4504753993107724, Training Accuracy: 18.628\n",
            "Validation Loss: 3.3772608439127603, Validation Accuracy: 20.72\n",
            "[132/150]: Training Loss: 3.456728091606727, Training Accuracy: 18.674\n",
            "Validation Loss: 3.3773043950398765, Validation Accuracy: 20.71\n",
            "[133/150]: Training Loss: 3.44931200834421, Training Accuracy: 18.624\n",
            "Validation Loss: 3.3772084712982178, Validation Accuracy: 20.7\n",
            "[134/150]: Training Loss: 3.449618541277372, Training Accuracy: 18.704\n",
            "Validation Loss: 3.376373608907064, Validation Accuracy: 20.79\n",
            "[135/150]: Training Loss: 3.4503632875589223, Training Accuracy: 18.66\n",
            "Validation Loss: 3.3759363492329917, Validation Accuracy: 20.83\n",
            "[136/150]: Training Loss: 3.449677210587722, Training Accuracy: 18.596\n",
            "Validation Loss: 3.375778595606486, Validation Accuracy: 20.83\n",
            "[137/150]: Training Loss: 3.449537864098182, Training Accuracy: 18.88\n",
            "Validation Loss: 3.375666777292887, Validation Accuracy: 20.86\n",
            "[138/150]: Training Loss: 3.450434923171997, Training Accuracy: 18.604\n",
            "Validation Loss: 3.3753153483072915, Validation Accuracy: 20.78\n",
            "[139/150]: Training Loss: 3.4475225118490367, Training Accuracy: 18.636\n",
            "Validation Loss: 3.37508487701416, Validation Accuracy: 20.75\n",
            "[140/150]: Training Loss: 3.4522607143108663, Training Accuracy: 18.838\n",
            "Validation Loss: 3.3748606046040854, Validation Accuracy: 20.76\n",
            "[141/150]: Training Loss: 3.4529799681443434, Training Accuracy: 18.474\n",
            "Validation Loss: 3.3746429284413657, Validation Accuracy: 20.81\n",
            "[142/150]: Training Loss: 3.4617789341853213, Training Accuracy: 18.554\n",
            "Validation Loss: 3.374579668045044, Validation Accuracy: 20.83\n",
            "[143/150]: Training Loss: 3.4485555245326114, Training Accuracy: 18.682\n",
            "Validation Loss: 3.374601682027181, Validation Accuracy: 20.8\n",
            "[144/150]: Training Loss: 3.444190428807185, Training Accuracy: 18.658\n",
            "Validation Loss: 3.3745707670847573, Validation Accuracy: 20.8\n",
            "[145/150]: Training Loss: 3.4463418263655443, Training Accuracy: 18.744\n",
            "Validation Loss: 3.3744784196217856, Validation Accuracy: 20.77\n",
            "[146/150]: Training Loss: 3.4481177146618185, Training Accuracy: 18.63\n",
            "Validation Loss: 3.3744141260782876, Validation Accuracy: 20.8\n",
            "[147/150]: Training Loss: 3.45231503706712, Training Accuracy: 18.596\n",
            "Validation Loss: 3.3743797143300376, Validation Accuracy: 20.78\n",
            "[148/150]: Training Loss: 3.4454290316655087, Training Accuracy: 18.72\n",
            "Validation Loss: 3.3743703365325928, Validation Accuracy: 20.79\n",
            "[149/150]: Training Loss: 3.4413316616645226, Training Accuracy: 18.828\n",
            "Validation Loss: 3.3743655681610107, Validation Accuracy: 20.78\n",
            "[150/150]: Training Loss: 3.446342101463905, Training Accuracy: 18.644\n",
            "Validation Loss: 3.374370892842611, Validation Accuracy: 20.78\n",
            "**********************************************************************\n",
            "Test Loss: 3.374370892842611, Test Accuracy: 20.78\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▃▁█</td></tr><tr><td>Test Loss</td><td>▃█▁</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>Train Loss</td><td>████▇▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>20.78</td></tr><tr><td>Test Loss</td><td>3.37437</td></tr><tr><td>Train Accuracy</td><td>18.644</td></tr><tr><td>Train Loss</td><td>3.44634</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=8192 learning_rate=0.192 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/2i4ynggq' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/2i4ynggq</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_044434-2i4ynggq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 16384\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_051107-0sspuo81</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/0sspuo81' target=\"_blank\">batch_size=16384 learning_rate=0.384 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/0sspuo81' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/0sspuo81</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.606681016775278, Training Accuracy: 0.952\n",
            "Validation Loss: 4.606822967529297, Validation Accuracy: 1.1\n",
            "[2/150]: Training Loss: 4.606707572937012, Training Accuracy: 0.922\n",
            "Validation Loss: 4.606650352478027, Validation Accuracy: 1.1\n",
            "[3/150]: Training Loss: 4.606582201444185, Training Accuracy: 0.856\n",
            "Validation Loss: 4.606306711832683, Validation Accuracy: 1.12\n",
            "[4/150]: Training Loss: 4.606221565833459, Training Accuracy: 0.884\n",
            "Validation Loss: 4.605796655019124, Validation Accuracy: 1.13\n",
            "[5/150]: Training Loss: 4.605512875777024, Training Accuracy: 0.93\n",
            "Validation Loss: 4.605127016703288, Validation Accuracy: 1.15\n",
            "[6/150]: Training Loss: 4.605027785668006, Training Accuracy: 1.008\n",
            "Validation Loss: 4.604293505350749, Validation Accuracy: 1.14\n",
            "[7/150]: Training Loss: 4.604158364809477, Training Accuracy: 1.026\n",
            "Validation Loss: 4.603291670481364, Validation Accuracy: 1.15\n",
            "[8/150]: Training Loss: 4.602965281559871, Training Accuracy: 1.028\n",
            "Validation Loss: 4.602102597554524, Validation Accuracy: 1.17\n",
            "[9/150]: Training Loss: 4.601937660804162, Training Accuracy: 1.122\n",
            "Validation Loss: 4.600688298543294, Validation Accuracy: 1.27\n",
            "[10/150]: Training Loss: 4.6005659103393555, Training Accuracy: 1.248\n",
            "Validation Loss: 4.598995208740234, Validation Accuracy: 1.39\n",
            "[11/150]: Training Loss: 4.598669895758996, Training Accuracy: 1.406\n",
            "Validation Loss: 4.596963723500569, Validation Accuracy: 1.69\n",
            "[12/150]: Training Loss: 4.596671361189622, Training Accuracy: 1.644\n",
            "Validation Loss: 4.594494024912517, Validation Accuracy: 1.97\n",
            "[13/150]: Training Loss: 4.5939992024348335, Training Accuracy: 1.998\n",
            "Validation Loss: 4.591485818227132, Validation Accuracy: 2.25\n",
            "[14/150]: Training Loss: 4.590727952810434, Training Accuracy: 2.41\n",
            "Validation Loss: 4.587808132171631, Validation Accuracy: 2.67\n",
            "[15/150]: Training Loss: 4.587152261000413, Training Accuracy: 2.794\n",
            "Validation Loss: 4.583325386047363, Validation Accuracy: 2.81\n",
            "[16/150]: Training Loss: 4.582096283252422, Training Accuracy: 2.964\n",
            "Validation Loss: 4.577853520711263, Validation Accuracy: 2.97\n",
            "[17/150]: Training Loss: 4.577282648820144, Training Accuracy: 3.076\n",
            "Validation Loss: 4.571693579355876, Validation Accuracy: 3.11\n",
            "[18/150]: Training Loss: 4.570924282073975, Training Accuracy: 3.242\n",
            "Validation Loss: 4.564822355906169, Validation Accuracy: 3.27\n",
            "[19/150]: Training Loss: 4.563632451570951, Training Accuracy: 3.34\n",
            "Validation Loss: 4.5572309494018555, Validation Accuracy: 3.35\n",
            "[20/150]: Training Loss: 4.555748756115253, Training Accuracy: 3.342\n",
            "Validation Loss: 4.5488667488098145, Validation Accuracy: 3.36\n",
            "[21/150]: Training Loss: 4.5476059180039625, Training Accuracy: 3.322\n",
            "Validation Loss: 4.539740244547526, Validation Accuracy: 3.35\n",
            "[22/150]: Training Loss: 4.538436192732591, Training Accuracy: 3.37\n",
            "Validation Loss: 4.529847621917725, Validation Accuracy: 3.39\n",
            "[23/150]: Training Loss: 4.528222891000601, Training Accuracy: 3.476\n",
            "Validation Loss: 4.519181410471599, Validation Accuracy: 3.46\n",
            "[24/150]: Training Loss: 4.518055218916673, Training Accuracy: 3.4\n",
            "Validation Loss: 4.5077924728393555, Validation Accuracy: 3.55\n",
            "[25/150]: Training Loss: 4.507573017707238, Training Accuracy: 3.426\n",
            "Validation Loss: 4.4956404368082685, Validation Accuracy: 3.58\n",
            "[26/150]: Training Loss: 4.493582468766433, Training Accuracy: 3.424\n",
            "Validation Loss: 4.482788721720378, Validation Accuracy: 3.57\n",
            "[27/150]: Training Loss: 4.482110867133508, Training Accuracy: 3.468\n",
            "Validation Loss: 4.469227155049642, Validation Accuracy: 3.49\n",
            "[28/150]: Training Loss: 4.468239380763127, Training Accuracy: 3.518\n",
            "Validation Loss: 4.454997380574544, Validation Accuracy: 3.61\n",
            "[29/150]: Training Loss: 4.454548689035269, Training Accuracy: 3.53\n",
            "Validation Loss: 4.440215110778809, Validation Accuracy: 3.63\n",
            "[30/150]: Training Loss: 4.440642576951247, Training Accuracy: 3.57\n",
            "Validation Loss: 4.424936135609944, Validation Accuracy: 3.75\n",
            "[31/150]: Training Loss: 4.425176070286677, Training Accuracy: 3.614\n",
            "Validation Loss: 4.409231026967366, Validation Accuracy: 3.93\n",
            "[32/150]: Training Loss: 4.408273770258977, Training Accuracy: 3.716\n",
            "Validation Loss: 4.393052260080974, Validation Accuracy: 3.92\n",
            "[33/150]: Training Loss: 4.392657866844764, Training Accuracy: 3.734\n",
            "Validation Loss: 4.376562436421712, Validation Accuracy: 3.98\n",
            "[34/150]: Training Loss: 4.376563952519343, Training Accuracy: 3.866\n",
            "Validation Loss: 4.3598737716674805, Validation Accuracy: 4.08\n",
            "[35/150]: Training Loss: 4.3609442344078655, Training Accuracy: 3.982\n",
            "Validation Loss: 4.343031565348308, Validation Accuracy: 4.25\n",
            "[36/150]: Training Loss: 4.346464450542744, Training Accuracy: 4.026\n",
            "Validation Loss: 4.326065540313721, Validation Accuracy: 4.41\n",
            "[37/150]: Training Loss: 4.329667274768536, Training Accuracy: 4.286\n",
            "Validation Loss: 4.308992862701416, Validation Accuracy: 4.58\n",
            "[38/150]: Training Loss: 4.311468894665058, Training Accuracy: 4.522\n",
            "Validation Loss: 4.291934331258138, Validation Accuracy: 4.63\n",
            "[39/150]: Training Loss: 4.297071236830491, Training Accuracy: 4.664\n",
            "Validation Loss: 4.274834314982097, Validation Accuracy: 4.93\n",
            "[40/150]: Training Loss: 4.27970842214731, Training Accuracy: 4.878\n",
            "Validation Loss: 4.257642904917399, Validation Accuracy: 5.28\n",
            "[41/150]: Training Loss: 4.261493022625263, Training Accuracy: 5.156\n",
            "Validation Loss: 4.240853945414226, Validation Accuracy: 5.64\n",
            "[42/150]: Training Loss: 4.25109129685622, Training Accuracy: 5.306\n",
            "Validation Loss: 4.224237600962321, Validation Accuracy: 5.99\n",
            "[43/150]: Training Loss: 4.231884186084454, Training Accuracy: 5.772\n",
            "Validation Loss: 4.207854111989339, Validation Accuracy: 6.25\n",
            "[44/150]: Training Loss: 4.219960396106426, Training Accuracy: 6.004\n",
            "Validation Loss: 4.1916913986206055, Validation Accuracy: 6.58\n",
            "[45/150]: Training Loss: 4.202443342942458, Training Accuracy: 6.26\n",
            "Validation Loss: 4.17600154876709, Validation Accuracy: 6.89\n",
            "[46/150]: Training Loss: 4.190515664907602, Training Accuracy: 6.364\n",
            "Validation Loss: 4.161019643147786, Validation Accuracy: 7.02\n",
            "[47/150]: Training Loss: 4.178160373981182, Training Accuracy: 6.606\n",
            "Validation Loss: 4.146643161773682, Validation Accuracy: 7.07\n",
            "[48/150]: Training Loss: 4.163040858048659, Training Accuracy: 6.732\n",
            "Validation Loss: 4.132482687632243, Validation Accuracy: 7.23\n",
            "[49/150]: Training Loss: 4.155646397517278, Training Accuracy: 6.992\n",
            "Validation Loss: 4.119060039520264, Validation Accuracy: 7.49\n",
            "[50/150]: Training Loss: 4.138869872459998, Training Accuracy: 7.174\n",
            "Validation Loss: 4.106416384379069, Validation Accuracy: 7.62\n",
            "[51/150]: Training Loss: 4.130591539236216, Training Accuracy: 7.38\n",
            "Validation Loss: 4.094014008839925, Validation Accuracy: 7.87\n",
            "[52/150]: Training Loss: 4.117375740638146, Training Accuracy: 7.462\n",
            "Validation Loss: 4.0823713938395185, Validation Accuracy: 8.04\n",
            "[53/150]: Training Loss: 4.105965027442346, Training Accuracy: 7.604\n",
            "Validation Loss: 4.07077423731486, Validation Accuracy: 8.32\n",
            "[54/150]: Training Loss: 4.097295357630803, Training Accuracy: 7.75\n",
            "Validation Loss: 4.059831301371257, Validation Accuracy: 8.44\n",
            "[55/150]: Training Loss: 4.088907241821289, Training Accuracy: 7.992\n",
            "Validation Loss: 4.04987112681071, Validation Accuracy: 8.46\n",
            "[56/150]: Training Loss: 4.084290797893818, Training Accuracy: 8.136\n",
            "Validation Loss: 4.039998849232991, Validation Accuracy: 8.65\n",
            "[57/150]: Training Loss: 4.0709899022029, Training Accuracy: 8.344\n",
            "Validation Loss: 4.030025164286296, Validation Accuracy: 8.9\n",
            "[58/150]: Training Loss: 4.060490058018611, Training Accuracy: 8.506\n",
            "Validation Loss: 4.020981788635254, Validation Accuracy: 9.02\n",
            "[59/150]: Training Loss: 4.054420214432937, Training Accuracy: 8.518\n",
            "Validation Loss: 4.012075821558635, Validation Accuracy: 9.21\n",
            "[60/150]: Training Loss: 4.045688995948205, Training Accuracy: 8.586\n",
            "Validation Loss: 4.003910303115845, Validation Accuracy: 9.58\n",
            "[61/150]: Training Loss: 4.043613470517672, Training Accuracy: 8.84\n",
            "Validation Loss: 3.9957558314005532, Validation Accuracy: 9.69\n",
            "[62/150]: Training Loss: 4.030606673314021, Training Accuracy: 8.906\n",
            "Validation Loss: 3.9876187642415366, Validation Accuracy: 9.77\n",
            "[63/150]: Training Loss: 4.024190425872803, Training Accuracy: 9.034\n",
            "Validation Loss: 3.979825814565023, Validation Accuracy: 9.9\n",
            "[64/150]: Training Loss: 4.017007460960975, Training Accuracy: 9.224\n",
            "Validation Loss: 3.9720873832702637, Validation Accuracy: 10.1\n",
            "[65/150]: Training Loss: 4.008704735682561, Training Accuracy: 9.386\n",
            "Validation Loss: 3.964705228805542, Validation Accuracy: 10.18\n",
            "[66/150]: Training Loss: 4.002646042750432, Training Accuracy: 9.524\n",
            "Validation Loss: 3.9574963251749673, Validation Accuracy: 10.36\n",
            "[67/150]: Training Loss: 3.9969605115743785, Training Accuracy: 9.504\n",
            "Validation Loss: 3.9497090180714927, Validation Accuracy: 10.69\n",
            "[68/150]: Training Loss: 3.9883175996633677, Training Accuracy: 9.578\n",
            "Validation Loss: 3.943034569422404, Validation Accuracy: 10.55\n",
            "[69/150]: Training Loss: 3.9852346090170054, Training Accuracy: 9.846\n",
            "Validation Loss: 3.9364415804545083, Validation Accuracy: 10.77\n",
            "[70/150]: Training Loss: 3.975684587772076, Training Accuracy: 9.944\n",
            "Validation Loss: 3.929186741511027, Validation Accuracy: 11.05\n",
            "[71/150]: Training Loss: 3.971948128480178, Training Accuracy: 10.058\n",
            "Validation Loss: 3.9228974978129068, Validation Accuracy: 11.05\n",
            "[72/150]: Training Loss: 3.971257521555974, Training Accuracy: 9.956\n",
            "Validation Loss: 3.916752497355143, Validation Accuracy: 11.14\n",
            "[73/150]: Training Loss: 3.958688259124756, Training Accuracy: 10.334\n",
            "Validation Loss: 3.9100093046824136, Validation Accuracy: 11.16\n",
            "[74/150]: Training Loss: 3.951256807033832, Training Accuracy: 10.262\n",
            "Validation Loss: 3.903977711995443, Validation Accuracy: 11.43\n",
            "[75/150]: Training Loss: 3.952374531672551, Training Accuracy: 10.376\n",
            "Validation Loss: 3.8979154427846274, Validation Accuracy: 11.6\n",
            "[76/150]: Training Loss: 3.9451453318962684, Training Accuracy: 10.552\n",
            "Validation Loss: 3.8917051951090493, Validation Accuracy: 11.8\n",
            "[77/150]: Training Loss: 3.9374708212338962, Training Accuracy: 10.576\n",
            "Validation Loss: 3.8862176736195884, Validation Accuracy: 11.91\n",
            "[78/150]: Training Loss: 3.9311011204352746, Training Accuracy: 10.524\n",
            "Validation Loss: 3.8805812199910483, Validation Accuracy: 11.98\n",
            "[79/150]: Training Loss: 3.9276427305661716, Training Accuracy: 10.858\n",
            "Validation Loss: 3.874942938486735, Validation Accuracy: 12.24\n",
            "[80/150]: Training Loss: 3.9206688770881066, Training Accuracy: 10.978\n",
            "Validation Loss: 3.8699731826782227, Validation Accuracy: 12.27\n",
            "[81/150]: Training Loss: 3.915351335818951, Training Accuracy: 10.92\n",
            "Validation Loss: 3.8641488552093506, Validation Accuracy: 12.18\n",
            "[82/150]: Training Loss: 3.916545317723201, Training Accuracy: 11.03\n",
            "Validation Loss: 3.8589042027791343, Validation Accuracy: 12.32\n",
            "[83/150]: Training Loss: 3.9109069384061375, Training Accuracy: 11.152\n",
            "Validation Loss: 3.8542784055074057, Validation Accuracy: 12.36\n",
            "[84/150]: Training Loss: 3.90489517725431, Training Accuracy: 11.19\n",
            "Validation Loss: 3.849609613418579, Validation Accuracy: 12.51\n",
            "[85/150]: Training Loss: 3.899632453918457, Training Accuracy: 11.386\n",
            "Validation Loss: 3.844412406285604, Validation Accuracy: 12.58\n",
            "[86/150]: Training Loss: 3.891819275342501, Training Accuracy: 11.476\n",
            "Validation Loss: 3.83998433748881, Validation Accuracy: 12.58\n",
            "[87/150]: Training Loss: 3.8915958404541016, Training Accuracy: 11.318\n",
            "Validation Loss: 3.8357551097869873, Validation Accuracy: 12.7\n",
            "[88/150]: Training Loss: 3.8887247305649977, Training Accuracy: 11.456\n",
            "Validation Loss: 3.8317208290100098, Validation Accuracy: 12.82\n",
            "[89/150]: Training Loss: 3.8861560087937574, Training Accuracy: 11.436\n",
            "Validation Loss: 3.8272467454274497, Validation Accuracy: 12.95\n",
            "[90/150]: Training Loss: 3.882909444662241, Training Accuracy: 11.526\n",
            "Validation Loss: 3.8237024943033853, Validation Accuracy: 12.93\n",
            "[91/150]: Training Loss: 3.8792625757364125, Training Accuracy: 11.726\n",
            "Validation Loss: 3.8198357423146567, Validation Accuracy: 12.98\n",
            "[92/150]: Training Loss: 3.8738321707798886, Training Accuracy: 11.788\n",
            "Validation Loss: 3.816159645716349, Validation Accuracy: 13.03\n",
            "[93/150]: Training Loss: 3.8671187254098744, Training Accuracy: 11.858\n",
            "Validation Loss: 3.81238055229187, Validation Accuracy: 13.17\n",
            "[94/150]: Training Loss: 3.863940477371216, Training Accuracy: 11.886\n",
            "Validation Loss: 3.80879545211792, Validation Accuracy: 13.17\n",
            "[95/150]: Training Loss: 3.861557960510254, Training Accuracy: 11.898\n",
            "Validation Loss: 3.8057108720143638, Validation Accuracy: 13.01\n",
            "[96/150]: Training Loss: 3.8610231692974386, Training Accuracy: 11.998\n",
            "Validation Loss: 3.80320143699646, Validation Accuracy: 13.06\n",
            "[97/150]: Training Loss: 3.8616612874544582, Training Accuracy: 11.81\n",
            "Validation Loss: 3.8002119859059653, Validation Accuracy: 13.2\n",
            "[98/150]: Training Loss: 3.8542607930990367, Training Accuracy: 11.916\n",
            "Validation Loss: 3.7972320715586343, Validation Accuracy: 13.3\n",
            "[99/150]: Training Loss: 3.8545856842627892, Training Accuracy: 12.306\n",
            "Validation Loss: 3.7939964135487876, Validation Accuracy: 13.34\n",
            "[100/150]: Training Loss: 3.8537971056424656, Training Accuracy: 12.136\n",
            "Validation Loss: 3.791252930959066, Validation Accuracy: 13.37\n",
            "[101/150]: Training Loss: 3.845060165111835, Training Accuracy: 12.294\n",
            "Validation Loss: 3.788733164469401, Validation Accuracy: 13.44\n",
            "[102/150]: Training Loss: 3.8459858160752516, Training Accuracy: 12.318\n",
            "Validation Loss: 3.7863372961680093, Validation Accuracy: 13.55\n",
            "[103/150]: Training Loss: 3.845681208830613, Training Accuracy: 12.304\n",
            "Validation Loss: 3.7838097413380942, Validation Accuracy: 13.5\n",
            "[104/150]: Training Loss: 3.8407657329852762, Training Accuracy: 12.284\n",
            "Validation Loss: 3.7814003626505532, Validation Accuracy: 13.47\n",
            "[105/150]: Training Loss: 3.843270485217755, Training Accuracy: 12.246\n",
            "Validation Loss: 3.7793458302815757, Validation Accuracy: 13.54\n",
            "[106/150]: Training Loss: 3.8365319875570445, Training Accuracy: 12.352\n",
            "Validation Loss: 3.7774136066436768, Validation Accuracy: 13.68\n",
            "[107/150]: Training Loss: 3.8353553185096154, Training Accuracy: 12.4\n",
            "Validation Loss: 3.775336424509684, Validation Accuracy: 13.7\n",
            "[108/150]: Training Loss: 3.8264750884129453, Training Accuracy: 12.538\n",
            "Validation Loss: 3.773082176844279, Validation Accuracy: 13.78\n",
            "[109/150]: Training Loss: 3.828341557429387, Training Accuracy: 12.494\n",
            "Validation Loss: 3.7709336280822754, Validation Accuracy: 13.82\n",
            "[110/150]: Training Loss: 3.8278093338012695, Training Accuracy: 12.49\n",
            "Validation Loss: 3.769374450047811, Validation Accuracy: 13.73\n",
            "[111/150]: Training Loss: 3.8284831047058105, Training Accuracy: 12.502\n",
            "Validation Loss: 3.76780104637146, Validation Accuracy: 13.69\n",
            "[112/150]: Training Loss: 3.826379500902616, Training Accuracy: 12.588\n",
            "Validation Loss: 3.7659263610839844, Validation Accuracy: 13.75\n",
            "[113/150]: Training Loss: 3.824379774240347, Training Accuracy: 12.478\n",
            "Validation Loss: 3.7643912633260093, Validation Accuracy: 13.82\n",
            "[114/150]: Training Loss: 3.8296838907095103, Training Accuracy: 12.514\n",
            "Validation Loss: 3.763111432393392, Validation Accuracy: 13.91\n",
            "[115/150]: Training Loss: 3.8198436223543606, Training Accuracy: 12.7\n",
            "Validation Loss: 3.761921485265096, Validation Accuracy: 13.98\n",
            "[116/150]: Training Loss: 3.825025430092445, Training Accuracy: 12.724\n",
            "Validation Loss: 3.7602592309316, Validation Accuracy: 14.05\n",
            "[117/150]: Training Loss: 3.8199995847848744, Training Accuracy: 12.67\n",
            "Validation Loss: 3.7588558991750083, Validation Accuracy: 13.97\n",
            "[118/150]: Training Loss: 3.8171204236837535, Training Accuracy: 12.626\n",
            "Validation Loss: 3.757905960083008, Validation Accuracy: 13.89\n",
            "[119/150]: Training Loss: 3.8168038588303785, Training Accuracy: 12.574\n",
            "Validation Loss: 3.7570317586263022, Validation Accuracy: 13.95\n",
            "[120/150]: Training Loss: 3.8181789104755106, Training Accuracy: 12.786\n",
            "Validation Loss: 3.7561088403066, Validation Accuracy: 14.05\n",
            "[121/150]: Training Loss: 3.814372722919171, Training Accuracy: 12.866\n",
            "Validation Loss: 3.7551366488138833, Validation Accuracy: 14.12\n",
            "[122/150]: Training Loss: 3.8122683671804576, Training Accuracy: 12.62\n",
            "Validation Loss: 3.754225571950277, Validation Accuracy: 14.09\n",
            "[123/150]: Training Loss: 3.8116220327524037, Training Accuracy: 12.65\n",
            "Validation Loss: 3.7533694903055825, Validation Accuracy: 14.1\n",
            "[124/150]: Training Loss: 3.8165716391343336, Training Accuracy: 12.532\n",
            "Validation Loss: 3.7525957425435386, Validation Accuracy: 14.11\n",
            "[125/150]: Training Loss: 3.8071923439319315, Training Accuracy: 12.776\n",
            "Validation Loss: 3.7518486976623535, Validation Accuracy: 14.13\n",
            "[126/150]: Training Loss: 3.804902773637038, Training Accuracy: 12.66\n",
            "Validation Loss: 3.751216491063436, Validation Accuracy: 14.18\n",
            "[127/150]: Training Loss: 3.812517001078679, Training Accuracy: 12.756\n",
            "Validation Loss: 3.7504872481028237, Validation Accuracy: 14.06\n",
            "[128/150]: Training Loss: 3.808407214971689, Training Accuracy: 12.782\n",
            "Validation Loss: 3.749931971232096, Validation Accuracy: 14.08\n",
            "[129/150]: Training Loss: 3.809405345183152, Training Accuracy: 12.982\n",
            "Validation Loss: 3.7494645913441977, Validation Accuracy: 14.12\n",
            "[130/150]: Training Loss: 3.8093962852771464, Training Accuracy: 12.79\n",
            "Validation Loss: 3.749009291330973, Validation Accuracy: 14.19\n",
            "[131/150]: Training Loss: 3.8160287050100474, Training Accuracy: 12.826\n",
            "Validation Loss: 3.748585859934489, Validation Accuracy: 14.15\n",
            "[132/150]: Training Loss: 3.805815549997183, Training Accuracy: 12.954\n",
            "Validation Loss: 3.7482031981150308, Validation Accuracy: 14.18\n",
            "[133/150]: Training Loss: 3.8074949154487023, Training Accuracy: 13.01\n",
            "Validation Loss: 3.7478596369425454, Validation Accuracy: 14.27\n",
            "[134/150]: Training Loss: 3.8079946224506083, Training Accuracy: 13.006\n",
            "Validation Loss: 3.7474911212921143, Validation Accuracy: 14.29\n",
            "[135/150]: Training Loss: 3.803782499753512, Training Accuracy: 13.028\n",
            "Validation Loss: 3.7471730709075928, Validation Accuracy: 14.26\n",
            "[136/150]: Training Loss: 3.8073516809023342, Training Accuracy: 12.738\n",
            "Validation Loss: 3.746917645136515, Validation Accuracy: 14.27\n",
            "[137/150]: Training Loss: 3.808643799561721, Training Accuracy: 13.072\n",
            "Validation Loss: 3.746670722961426, Validation Accuracy: 14.26\n",
            "[138/150]: Training Loss: 3.8111466811253476, Training Accuracy: 12.808\n",
            "Validation Loss: 3.7464906374613443, Validation Accuracy: 14.26\n",
            "[139/150]: Training Loss: 3.800331610899705, Training Accuracy: 12.784\n",
            "Validation Loss: 3.7463111877441406, Validation Accuracy: 14.25\n",
            "[140/150]: Training Loss: 3.8085801968207726, Training Accuracy: 13.02\n",
            "Validation Loss: 3.7461959520975747, Validation Accuracy: 14.24\n",
            "[141/150]: Training Loss: 3.8026637114011326, Training Accuracy: 13.04\n",
            "Validation Loss: 3.746067841847738, Validation Accuracy: 14.24\n",
            "[142/150]: Training Loss: 3.8049483666053185, Training Accuracy: 13.014\n",
            "Validation Loss: 3.745962699254354, Validation Accuracy: 14.26\n",
            "[143/150]: Training Loss: 3.802163234123817, Training Accuracy: 12.892\n",
            "Validation Loss: 3.745898723602295, Validation Accuracy: 14.26\n",
            "[144/150]: Training Loss: 3.80464847271259, Training Accuracy: 13.098\n",
            "Validation Loss: 3.7458243370056152, Validation Accuracy: 14.23\n",
            "[145/150]: Training Loss: 3.8023996353149414, Training Accuracy: 12.958\n",
            "Validation Loss: 3.7457852363586426, Validation Accuracy: 14.24\n",
            "[146/150]: Training Loss: 3.808238928134625, Training Accuracy: 12.754\n",
            "Validation Loss: 3.74574605623881, Validation Accuracy: 14.25\n",
            "[147/150]: Training Loss: 3.8094704701350284, Training Accuracy: 12.834\n",
            "Validation Loss: 3.7457223733266196, Validation Accuracy: 14.24\n",
            "[148/150]: Training Loss: 3.810224698140071, Training Accuracy: 12.922\n",
            "Validation Loss: 3.7457141081492105, Validation Accuracy: 14.24\n",
            "[149/150]: Training Loss: 3.808670997619629, Training Accuracy: 12.826\n",
            "Validation Loss: 3.745707352956136, Validation Accuracy: 14.22\n",
            "[150/150]: Training Loss: 3.8055135103372426, Training Accuracy: 12.94\n",
            "Validation Loss: 3.745703140894572, Validation Accuracy: 14.2\n",
            "**********************************************************************\n",
            "Test Loss: 3.745703140894572, Test Accuracy: 14.2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▂▁█</td></tr><tr><td>Test Loss</td><td>▁█▄</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▁▂▂▂▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇█████████████</td></tr><tr><td>Train Loss</td><td>██████▇▇▆▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>14.2</td></tr><tr><td>Test Loss</td><td>3.7457</td></tr><tr><td>Train Accuracy</td><td>12.94</td></tr><tr><td>Train Loss</td><td>3.80551</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=16384 learning_rate=0.384 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/0sspuo81' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/0sspuo81</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_051107-0sspuo81/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 32768\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240908_053723-99jlxrtf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/99jlxrtf' target=\"_blank\">batch_size=32768 learning_rate=0.768 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/99jlxrtf' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/99jlxrtf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.606651562910813, Training Accuracy: 1.002\n",
            "Validation Loss: 4.6062649091084795, Validation Accuracy: 1.0\n",
            "[2/150]: Training Loss: 4.6064779208256645, Training Accuracy: 1.0\n",
            "Validation Loss: 4.606228033701579, Validation Accuracy: 1.0\n",
            "[3/150]: Training Loss: 4.606321224799523, Training Accuracy: 1.0\n",
            "Validation Loss: 4.606154441833496, Validation Accuracy: 1.0\n",
            "[4/150]: Training Loss: 4.606344589820275, Training Accuracy: 1.0\n",
            "Validation Loss: 4.606043815612793, Validation Accuracy: 1.0\n",
            "[5/150]: Training Loss: 4.606256888462947, Training Accuracy: 1.006\n",
            "Validation Loss: 4.605896631876628, Validation Accuracy: 1.0\n",
            "[6/150]: Training Loss: 4.606011354006254, Training Accuracy: 1.002\n",
            "Validation Loss: 4.605713526407878, Validation Accuracy: 1.0\n",
            "[7/150]: Training Loss: 4.605957177969126, Training Accuracy: 1.0\n",
            "Validation Loss: 4.605494976043701, Validation Accuracy: 1.0\n",
            "[8/150]: Training Loss: 4.60562016413762, Training Accuracy: 1.004\n",
            "Validation Loss: 4.6052422523498535, Validation Accuracy: 1.0\n",
            "[9/150]: Training Loss: 4.605379141294039, Training Accuracy: 1.0\n",
            "Validation Loss: 4.6049544016520185, Validation Accuracy: 1.0\n",
            "[10/150]: Training Loss: 4.605238987849309, Training Accuracy: 0.998\n",
            "Validation Loss: 4.6046319007873535, Validation Accuracy: 1.0\n",
            "[11/150]: Training Loss: 4.604823736044077, Training Accuracy: 0.992\n",
            "Validation Loss: 4.6042758623758955, Validation Accuracy: 1.0\n",
            "[12/150]: Training Loss: 4.6045523790212775, Training Accuracy: 1.002\n",
            "Validation Loss: 4.603885491689046, Validation Accuracy: 1.01\n",
            "[13/150]: Training Loss: 4.6041412720313435, Training Accuracy: 1.006\n",
            "Validation Loss: 4.603460311889648, Validation Accuracy: 1.01\n",
            "[14/150]: Training Loss: 4.603790209843562, Training Accuracy: 1.002\n",
            "Validation Loss: 4.60299825668335, Validation Accuracy: 1.01\n",
            "[15/150]: Training Loss: 4.603167937352107, Training Accuracy: 1.0\n",
            "Validation Loss: 4.60249678293864, Validation Accuracy: 1.01\n",
            "[16/150]: Training Loss: 4.602945987994854, Training Accuracy: 1.002\n",
            "Validation Loss: 4.601947625478108, Validation Accuracy: 1.01\n",
            "[17/150]: Training Loss: 4.602274197798509, Training Accuracy: 0.998\n",
            "Validation Loss: 4.601348876953125, Validation Accuracy: 1.01\n",
            "[18/150]: Training Loss: 4.601670815394475, Training Accuracy: 0.998\n",
            "Validation Loss: 4.600697199503581, Validation Accuracy: 1.02\n",
            "[19/150]: Training Loss: 4.600979328155518, Training Accuracy: 1.014\n",
            "Validation Loss: 4.599982261657715, Validation Accuracy: 1.02\n",
            "[20/150]: Training Loss: 4.600179158724272, Training Accuracy: 1.006\n",
            "Validation Loss: 4.59920072555542, Validation Accuracy: 1.03\n",
            "[21/150]: Training Loss: 4.599789472726675, Training Accuracy: 1.014\n",
            "Validation Loss: 4.5983381271362305, Validation Accuracy: 1.04\n",
            "[22/150]: Training Loss: 4.5985127962552586, Training Accuracy: 1.016\n",
            "Validation Loss: 4.5973836580912275, Validation Accuracy: 1.04\n",
            "[23/150]: Training Loss: 4.597778356992281, Training Accuracy: 1.012\n",
            "Validation Loss: 4.596320788065593, Validation Accuracy: 1.04\n",
            "[24/150]: Training Loss: 4.596712589263916, Training Accuracy: 1.01\n",
            "Validation Loss: 4.595138390858968, Validation Accuracy: 1.04\n",
            "[25/150]: Training Loss: 4.595335960388184, Training Accuracy: 1.016\n",
            "Validation Loss: 4.593827406565349, Validation Accuracy: 1.04\n",
            "[26/150]: Training Loss: 4.5942942545964165, Training Accuracy: 1.026\n",
            "Validation Loss: 4.592372576395671, Validation Accuracy: 1.04\n",
            "[27/150]: Training Loss: 4.5928042851961575, Training Accuracy: 1.04\n",
            "Validation Loss: 4.590754985809326, Validation Accuracy: 1.05\n",
            "[28/150]: Training Loss: 4.591340908637414, Training Accuracy: 1.034\n",
            "Validation Loss: 4.588959534962972, Validation Accuracy: 1.08\n",
            "[29/150]: Training Loss: 4.589395119593694, Training Accuracy: 1.06\n",
            "Validation Loss: 4.586968739827474, Validation Accuracy: 1.1\n",
            "[30/150]: Training Loss: 4.587504900418795, Training Accuracy: 1.074\n",
            "Validation Loss: 4.5847554206848145, Validation Accuracy: 1.13\n",
            "[31/150]: Training Loss: 4.585566887488732, Training Accuracy: 1.084\n",
            "Validation Loss: 4.582296371459961, Validation Accuracy: 1.16\n",
            "[32/150]: Training Loss: 4.58308194233821, Training Accuracy: 1.166\n",
            "Validation Loss: 4.579681237538655, Validation Accuracy: 1.29\n",
            "[33/150]: Training Loss: 4.580507901998667, Training Accuracy: 1.31\n",
            "Validation Loss: 4.576923529307048, Validation Accuracy: 1.44\n",
            "[34/150]: Training Loss: 4.577524295220008, Training Accuracy: 1.46\n",
            "Validation Loss: 4.574008464813232, Validation Accuracy: 1.63\n",
            "[35/150]: Training Loss: 4.574791504786565, Training Accuracy: 1.666\n",
            "Validation Loss: 4.570941766103108, Validation Accuracy: 1.84\n",
            "[36/150]: Training Loss: 4.571814023531401, Training Accuracy: 1.898\n",
            "Validation Loss: 4.5677235921223955, Validation Accuracy: 1.95\n",
            "[37/150]: Training Loss: 4.56871957045335, Training Accuracy: 2.106\n",
            "Validation Loss: 4.564344724019368, Validation Accuracy: 2.12\n",
            "[38/150]: Training Loss: 4.564909861637996, Training Accuracy: 2.244\n",
            "Validation Loss: 4.560825665791829, Validation Accuracy: 2.26\n",
            "[39/150]: Training Loss: 4.562269687652588, Training Accuracy: 2.412\n",
            "Validation Loss: 4.5571645100911455, Validation Accuracy: 2.47\n",
            "[40/150]: Training Loss: 4.558965609623836, Training Accuracy: 2.522\n",
            "Validation Loss: 4.553352991739909, Validation Accuracy: 2.64\n",
            "[41/150]: Training Loss: 4.554804985339825, Training Accuracy: 2.674\n",
            "Validation Loss: 4.549396355946858, Validation Accuracy: 2.8\n",
            "[42/150]: Training Loss: 4.550996633676382, Training Accuracy: 2.828\n",
            "Validation Loss: 4.545290470123291, Validation Accuracy: 2.91\n",
            "[43/150]: Training Loss: 4.5466168110187235, Training Accuracy: 2.882\n",
            "Validation Loss: 4.541029135386149, Validation Accuracy: 3.07\n",
            "[44/150]: Training Loss: 4.542607417473426, Training Accuracy: 3.024\n",
            "Validation Loss: 4.536633332570394, Validation Accuracy: 3.24\n",
            "[45/150]: Training Loss: 4.538647064795861, Training Accuracy: 3.174\n",
            "Validation Loss: 4.532093207041423, Validation Accuracy: 3.27\n",
            "[46/150]: Training Loss: 4.534129582918608, Training Accuracy: 3.226\n",
            "Validation Loss: 4.527428309122722, Validation Accuracy: 3.35\n",
            "[47/150]: Training Loss: 4.530724268693191, Training Accuracy: 3.344\n",
            "Validation Loss: 4.522626082102458, Validation Accuracy: 3.42\n",
            "[48/150]: Training Loss: 4.524971155019907, Training Accuracy: 3.418\n",
            "Validation Loss: 4.517709096272786, Validation Accuracy: 3.53\n",
            "[49/150]: Training Loss: 4.521018285017747, Training Accuracy: 3.492\n",
            "Validation Loss: 4.5126776695251465, Validation Accuracy: 3.61\n",
            "[50/150]: Training Loss: 4.516248776362493, Training Accuracy: 3.598\n",
            "Validation Loss: 4.507528463999431, Validation Accuracy: 3.69\n",
            "[51/150]: Training Loss: 4.51058134665856, Training Accuracy: 3.602\n",
            "Validation Loss: 4.50228230158488, Validation Accuracy: 3.74\n",
            "[52/150]: Training Loss: 4.506296891432542, Training Accuracy: 3.646\n",
            "Validation Loss: 4.496960957845052, Validation Accuracy: 3.76\n",
            "[53/150]: Training Loss: 4.500542383927566, Training Accuracy: 3.72\n",
            "Validation Loss: 4.491568565368652, Validation Accuracy: 3.77\n",
            "[54/150]: Training Loss: 4.4958164141728325, Training Accuracy: 3.792\n",
            "Validation Loss: 4.4861117998758955, Validation Accuracy: 3.79\n",
            "[55/150]: Training Loss: 4.4898162988516, Training Accuracy: 3.77\n",
            "Validation Loss: 4.480595429738362, Validation Accuracy: 3.81\n",
            "[56/150]: Training Loss: 4.48429430448092, Training Accuracy: 3.86\n",
            "Validation Loss: 4.475037892659505, Validation Accuracy: 3.9\n",
            "[57/150]: Training Loss: 4.480384533221905, Training Accuracy: 3.908\n",
            "Validation Loss: 4.4694546063741045, Validation Accuracy: 3.92\n",
            "[58/150]: Training Loss: 4.474462802593525, Training Accuracy: 3.888\n",
            "Validation Loss: 4.463854630788167, Validation Accuracy: 4.05\n",
            "[59/150]: Training Loss: 4.468657420231746, Training Accuracy: 3.994\n",
            "Validation Loss: 4.458210150400798, Validation Accuracy: 4.21\n",
            "[60/150]: Training Loss: 4.4632906180161696, Training Accuracy: 4.054\n",
            "Validation Loss: 4.452545960744222, Validation Accuracy: 4.31\n",
            "[61/150]: Training Loss: 4.457904448876014, Training Accuracy: 4.174\n",
            "Validation Loss: 4.446877797444661, Validation Accuracy: 4.34\n",
            "[62/150]: Training Loss: 4.45219505750216, Training Accuracy: 4.154\n",
            "Validation Loss: 4.441228866577148, Validation Accuracy: 4.39\n",
            "[63/150]: Training Loss: 4.448141024662898, Training Accuracy: 4.228\n",
            "Validation Loss: 4.4355998039245605, Validation Accuracy: 4.51\n",
            "[64/150]: Training Loss: 4.440924974588247, Training Accuracy: 4.254\n",
            "Validation Loss: 4.430009365081787, Validation Accuracy: 4.58\n",
            "[65/150]: Training Loss: 4.436653247246375, Training Accuracy: 4.264\n",
            "Validation Loss: 4.424450397491455, Validation Accuracy: 4.56\n",
            "[66/150]: Training Loss: 4.432394211108868, Training Accuracy: 4.304\n",
            "Validation Loss: 4.418941179911296, Validation Accuracy: 4.6\n",
            "[67/150]: Training Loss: 4.426660941197322, Training Accuracy: 4.248\n",
            "Validation Loss: 4.4134782155354815, Validation Accuracy: 4.69\n",
            "[68/150]: Training Loss: 4.423159122467041, Training Accuracy: 4.292\n",
            "Validation Loss: 4.408063252766927, Validation Accuracy: 4.71\n",
            "[69/150]: Training Loss: 4.415358873514029, Training Accuracy: 4.38\n",
            "Validation Loss: 4.4027144114176435, Validation Accuracy: 4.76\n",
            "[70/150]: Training Loss: 4.4111648339491625, Training Accuracy: 4.352\n",
            "Validation Loss: 4.397424379984538, Validation Accuracy: 4.8\n",
            "[71/150]: Training Loss: 4.404525976914626, Training Accuracy: 4.32\n",
            "Validation Loss: 4.392204602559407, Validation Accuracy: 4.8\n",
            "[72/150]: Training Loss: 4.402150594271147, Training Accuracy: 4.372\n",
            "Validation Loss: 4.3870619138081866, Validation Accuracy: 4.87\n",
            "[73/150]: Training Loss: 4.396322653843806, Training Accuracy: 4.44\n",
            "Validation Loss: 4.381992657979329, Validation Accuracy: 4.89\n",
            "[74/150]: Training Loss: 4.389365893143874, Training Accuracy: 4.388\n",
            "Validation Loss: 4.377000013987224, Validation Accuracy: 4.93\n",
            "[75/150]: Training Loss: 4.385019522446853, Training Accuracy: 4.56\n",
            "Validation Loss: 4.372077624003093, Validation Accuracy: 4.92\n",
            "[76/150]: Training Loss: 4.38299454175509, Training Accuracy: 4.474\n",
            "Validation Loss: 4.367240905761719, Validation Accuracy: 4.89\n",
            "[77/150]: Training Loss: 4.375784213726337, Training Accuracy: 4.58\n",
            "Validation Loss: 4.362499078114827, Validation Accuracy: 4.87\n",
            "[78/150]: Training Loss: 4.372459705059345, Training Accuracy: 4.58\n",
            "Validation Loss: 4.357850233713786, Validation Accuracy: 4.88\n",
            "[79/150]: Training Loss: 4.368399766775278, Training Accuracy: 4.494\n",
            "Validation Loss: 4.3532694180806475, Validation Accuracy: 4.87\n",
            "[80/150]: Training Loss: 4.363157015580398, Training Accuracy: 4.656\n",
            "Validation Loss: 4.348786989847819, Validation Accuracy: 4.92\n",
            "[81/150]: Training Loss: 4.360122277186467, Training Accuracy: 4.664\n",
            "Validation Loss: 4.344403266906738, Validation Accuracy: 4.94\n",
            "[82/150]: Training Loss: 4.353952591235821, Training Accuracy: 4.588\n",
            "Validation Loss: 4.340079625447591, Validation Accuracy: 4.99\n",
            "[83/150]: Training Loss: 4.351973606989934, Training Accuracy: 4.668\n",
            "Validation Loss: 4.335822423299153, Validation Accuracy: 5.03\n",
            "[84/150]: Training Loss: 4.347680421975943, Training Accuracy: 4.772\n",
            "Validation Loss: 4.331654230753581, Validation Accuracy: 5.01\n",
            "[85/150]: Training Loss: 4.344626720135029, Training Accuracy: 4.73\n",
            "Validation Loss: 4.327584743499756, Validation Accuracy: 5.01\n",
            "[86/150]: Training Loss: 4.340798157912034, Training Accuracy: 4.81\n",
            "Validation Loss: 4.323618253072103, Validation Accuracy: 5.05\n",
            "[87/150]: Training Loss: 4.33451535151555, Training Accuracy: 4.846\n",
            "Validation Loss: 4.319734573364258, Validation Accuracy: 5.03\n",
            "[88/150]: Training Loss: 4.335264719449556, Training Accuracy: 4.872\n",
            "Validation Loss: 4.315944194793701, Validation Accuracy: 5.07\n",
            "[89/150]: Training Loss: 4.326850634354812, Training Accuracy: 4.928\n",
            "Validation Loss: 4.312259356180827, Validation Accuracy: 5.16\n",
            "[90/150]: Training Loss: 4.325683997227595, Training Accuracy: 4.968\n",
            "Validation Loss: 4.308659394582112, Validation Accuracy: 5.22\n",
            "[91/150]: Training Loss: 4.320460429558387, Training Accuracy: 5.014\n",
            "Validation Loss: 4.3051651318868, Validation Accuracy: 5.26\n",
            "[92/150]: Training Loss: 4.319362420302171, Training Accuracy: 5.026\n",
            "Validation Loss: 4.301756381988525, Validation Accuracy: 5.29\n",
            "[93/150]: Training Loss: 4.314967118776762, Training Accuracy: 5.114\n",
            "Validation Loss: 4.298442999521892, Validation Accuracy: 5.37\n",
            "[94/150]: Training Loss: 4.312812475057749, Training Accuracy: 5.194\n",
            "Validation Loss: 4.295233090718587, Validation Accuracy: 5.44\n",
            "[95/150]: Training Loss: 4.307969496800349, Training Accuracy: 5.212\n",
            "Validation Loss: 4.292148113250732, Validation Accuracy: 5.49\n",
            "[96/150]: Training Loss: 4.304759869208703, Training Accuracy: 5.198\n",
            "Validation Loss: 4.2891364097595215, Validation Accuracy: 5.49\n",
            "[97/150]: Training Loss: 4.303560513716477, Training Accuracy: 5.25\n",
            "Validation Loss: 4.286212921142578, Validation Accuracy: 5.56\n",
            "[98/150]: Training Loss: 4.299245760991023, Training Accuracy: 5.218\n",
            "Validation Loss: 4.283364772796631, Validation Accuracy: 5.6\n",
            "[99/150]: Training Loss: 4.29710762317364, Training Accuracy: 5.426\n",
            "Validation Loss: 4.2806094487508135, Validation Accuracy: 5.61\n",
            "[100/150]: Training Loss: 4.293303159567026, Training Accuracy: 5.38\n",
            "Validation Loss: 4.277939637502034, Validation Accuracy: 5.67\n",
            "[101/150]: Training Loss: 4.291012140420767, Training Accuracy: 5.472\n",
            "Validation Loss: 4.2753651936848955, Validation Accuracy: 5.72\n",
            "[102/150]: Training Loss: 4.290883467747615, Training Accuracy: 5.438\n",
            "Validation Loss: 4.272902647654216, Validation Accuracy: 5.72\n",
            "[103/150]: Training Loss: 4.289033926450289, Training Accuracy: 5.448\n",
            "Validation Loss: 4.2705206871032715, Validation Accuracy: 5.73\n",
            "[104/150]: Training Loss: 4.286309095529409, Training Accuracy: 5.464\n",
            "Validation Loss: 4.268236955006917, Validation Accuracy: 5.77\n",
            "[105/150]: Training Loss: 4.283603448134202, Training Accuracy: 5.542\n",
            "Validation Loss: 4.266055901845296, Validation Accuracy: 5.79\n",
            "[106/150]: Training Loss: 4.281673284677359, Training Accuracy: 5.566\n",
            "Validation Loss: 4.263958930969238, Validation Accuracy: 5.78\n",
            "[107/150]: Training Loss: 4.2781767478356, Training Accuracy: 5.594\n",
            "Validation Loss: 4.261946201324463, Validation Accuracy: 5.8\n",
            "[108/150]: Training Loss: 4.275523039010855, Training Accuracy: 5.586\n",
            "Validation Loss: 4.2600124677022295, Validation Accuracy: 5.78\n",
            "[109/150]: Training Loss: 4.274957400101882, Training Accuracy: 5.594\n",
            "Validation Loss: 4.258159160614014, Validation Accuracy: 5.79\n",
            "[110/150]: Training Loss: 4.2723074692946215, Training Accuracy: 5.732\n",
            "Validation Loss: 4.2563802401224775, Validation Accuracy: 5.82\n",
            "[111/150]: Training Loss: 4.270333436819223, Training Accuracy: 5.656\n",
            "Validation Loss: 4.254659652709961, Validation Accuracy: 5.83\n",
            "[112/150]: Training Loss: 4.269529599409837, Training Accuracy: 5.63\n",
            "Validation Loss: 4.253021240234375, Validation Accuracy: 5.84\n",
            "[113/150]: Training Loss: 4.26841218654926, Training Accuracy: 5.734\n",
            "Validation Loss: 4.2514543533325195, Validation Accuracy: 5.88\n",
            "[114/150]: Training Loss: 4.264081184680645, Training Accuracy: 5.674\n",
            "Validation Loss: 4.249970277150472, Validation Accuracy: 5.89\n",
            "[115/150]: Training Loss: 4.264317292433518, Training Accuracy: 5.816\n",
            "Validation Loss: 4.248543898264567, Validation Accuracy: 5.9\n",
            "[116/150]: Training Loss: 4.265165035541241, Training Accuracy: 5.812\n",
            "Validation Loss: 4.24720033009847, Validation Accuracy: 5.93\n",
            "[117/150]: Training Loss: 4.264072968409612, Training Accuracy: 5.836\n",
            "Validation Loss: 4.245920976003011, Validation Accuracy: 5.97\n",
            "[118/150]: Training Loss: 4.263286517216609, Training Accuracy: 5.73\n",
            "Validation Loss: 4.244728883107503, Validation Accuracy: 5.98\n",
            "[119/150]: Training Loss: 4.258808466104361, Training Accuracy: 5.826\n",
            "Validation Loss: 4.243605295817058, Validation Accuracy: 6.01\n",
            "[120/150]: Training Loss: 4.259444530193623, Training Accuracy: 5.79\n",
            "Validation Loss: 4.242557366689046, Validation Accuracy: 6.03\n",
            "[121/150]: Training Loss: 4.259942898383508, Training Accuracy: 5.87\n",
            "Validation Loss: 4.241562366485596, Validation Accuracy: 6.04\n",
            "[122/150]: Training Loss: 4.256737085489126, Training Accuracy: 5.956\n",
            "Validation Loss: 4.240633487701416, Validation Accuracy: 6.08\n",
            "[123/150]: Training Loss: 4.25673587505634, Training Accuracy: 5.888\n",
            "Validation Loss: 4.23976484934489, Validation Accuracy: 6.08\n",
            "[124/150]: Training Loss: 4.256068853231577, Training Accuracy: 5.852\n",
            "Validation Loss: 4.23895804087321, Validation Accuracy: 6.09\n",
            "[125/150]: Training Loss: 4.252622494330773, Training Accuracy: 5.926\n",
            "Validation Loss: 4.238210360209147, Validation Accuracy: 6.12\n",
            "[126/150]: Training Loss: 4.253502772404597, Training Accuracy: 5.974\n",
            "Validation Loss: 4.237521489461263, Validation Accuracy: 6.15\n",
            "[127/150]: Training Loss: 4.255204897660476, Training Accuracy: 5.94\n",
            "Validation Loss: 4.236883481343587, Validation Accuracy: 6.16\n",
            "[128/150]: Training Loss: 4.254409569960374, Training Accuracy: 5.962\n",
            "Validation Loss: 4.236288070678711, Validation Accuracy: 6.16\n",
            "[129/150]: Training Loss: 4.2498477055476265, Training Accuracy: 5.95\n",
            "Validation Loss: 4.235741297403972, Validation Accuracy: 6.16\n",
            "[130/150]: Training Loss: 4.249825147482065, Training Accuracy: 5.97\n",
            "Validation Loss: 4.235249042510986, Validation Accuracy: 6.17\n",
            "[131/150]: Training Loss: 4.253135497753437, Training Accuracy: 5.892\n",
            "Validation Loss: 4.234799226125081, Validation Accuracy: 6.19\n",
            "[132/150]: Training Loss: 4.2529898423414965, Training Accuracy: 5.85\n",
            "Validation Loss: 4.234389940897624, Validation Accuracy: 6.2\n",
            "[133/150]: Training Loss: 4.250357114351713, Training Accuracy: 5.972\n",
            "Validation Loss: 4.234029134114583, Validation Accuracy: 6.19\n",
            "[134/150]: Training Loss: 4.250753989586463, Training Accuracy: 5.952\n",
            "Validation Loss: 4.233706633249919, Validation Accuracy: 6.22\n",
            "[135/150]: Training Loss: 4.249126434326172, Training Accuracy: 5.988\n",
            "Validation Loss: 4.23341433207194, Validation Accuracy: 6.23\n",
            "[136/150]: Training Loss: 4.252225215618427, Training Accuracy: 5.98\n",
            "Validation Loss: 4.2331617673238116, Validation Accuracy: 6.2\n",
            "[137/150]: Training Loss: 4.2500433921813965, Training Accuracy: 5.95\n",
            "Validation Loss: 4.232938925425212, Validation Accuracy: 6.21\n",
            "[138/150]: Training Loss: 4.24816370010376, Training Accuracy: 5.874\n",
            "Validation Loss: 4.232744057973226, Validation Accuracy: 6.21\n",
            "[139/150]: Training Loss: 4.250017312856821, Training Accuracy: 5.968\n",
            "Validation Loss: 4.232572714487712, Validation Accuracy: 6.21\n",
            "[140/150]: Training Loss: 4.248001502110408, Training Accuracy: 5.962\n",
            "Validation Loss: 4.232438087463379, Validation Accuracy: 6.21\n",
            "[141/150]: Training Loss: 4.248642994807317, Training Accuracy: 5.936\n",
            "Validation Loss: 4.232326825459798, Validation Accuracy: 6.21\n",
            "[142/150]: Training Loss: 4.247634997734656, Training Accuracy: 6.008\n",
            "Validation Loss: 4.232233047485352, Validation Accuracy: 6.21\n",
            "[143/150]: Training Loss: 4.247423832233135, Training Accuracy: 5.942\n",
            "Validation Loss: 4.232162952423096, Validation Accuracy: 6.21\n",
            "[144/150]: Training Loss: 4.247767558464637, Training Accuracy: 5.964\n",
            "Validation Loss: 4.232110182444255, Validation Accuracy: 6.21\n",
            "[145/150]: Training Loss: 4.250305909376878, Training Accuracy: 5.908\n",
            "Validation Loss: 4.2320709228515625, Validation Accuracy: 6.21\n",
            "[146/150]: Training Loss: 4.250130616701567, Training Accuracy: 6.024\n",
            "Validation Loss: 4.232042153676351, Validation Accuracy: 6.21\n",
            "[147/150]: Training Loss: 4.247591238755446, Training Accuracy: 6.032\n",
            "Validation Loss: 4.232023398081462, Validation Accuracy: 6.21\n",
            "[148/150]: Training Loss: 4.248689578129695, Training Accuracy: 6.046\n",
            "Validation Loss: 4.232014338175456, Validation Accuracy: 6.21\n",
            "[149/150]: Training Loss: 4.2455423428462105, Training Accuracy: 5.968\n",
            "Validation Loss: 4.232010682423909, Validation Accuracy: 6.21\n",
            "[150/150]: Training Loss: 4.251231266902043, Training Accuracy: 5.994\n",
            "Validation Loss: 4.23201052347819, Validation Accuracy: 6.21\n",
            "**********************************************************************\n",
            "Test Loss: 4.23201052347819, Test Accuracy: 6.21\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▁▅</td></tr><tr><td>Test Loss</td><td>▁█▅</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██████████</td></tr><tr><td>Train Loss</td><td>█████████▇▇▇▇▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>6.21</td></tr><tr><td>Test Loss</td><td>4.23201</td></tr><tr><td>Train Accuracy</td><td>5.994</td></tr><tr><td>Train Loss</td><td>4.25123</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=32768 learning_rate=0.768 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/99jlxrtf' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/99jlxrtf</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240908_053723-99jlxrtf/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# with warmup\n",
        "num_epochs = 150\n",
        "wd = 1e-04\n",
        "batch_sizes = [512, 1024, 2048, 4096, 8192, 16384 , 32768]\n",
        "lr = 4.8/(2**5 *1e02) # approximately 15e-04\n",
        "learning_rates = [lr * batch_size / 64.0 for batch_size in batch_sizes] # linear scale-up of learning rate\n",
        "warmup_ratio = [1/320, 1/160, 1/80, 1/40, 1/20, 1/10, 1/5]\n",
        "\n",
        "for i, batch_size in enumerate(batch_sizes):\n",
        "  print('='*50)\n",
        "  print(f'Batch size: {batch_size}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {\n",
        "    'batch_size': batch_size,\n",
        "    'learning_rate': learning_rates[i],\n",
        "    'weight_decay' : wd\n",
        "  }\n",
        "\n",
        "  if batch_size <= 4096:\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= batch_size)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LAMB(model.parameters(), learning_rate=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LAMB_Large_Batches',\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True,\n",
        "        warmup_ratio=warmup_ratio[i]\n",
        "    )\n",
        "  else:\n",
        "    accumulation_steps = batch_size // 4096\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= 4096)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LAMB(model.parameters(), learning_rate=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LAMB_Large_Batches',\n",
        "        accumulation_steps=accumulation_steps,\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True,\n",
        "        warmup_ratio=warmup_ratio[i]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 512\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240909_145647-rylmre02</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/rylmre02' target=\"_blank\">batch_size=512 learning_rate=0.012 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/rylmre02' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/rylmre02</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********************************************************************\n",
            "Test Loss: 2.065195256471634, Test Accuracy: 47.05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▅▃▅▆▂▂▅▅▄▄▄▃▄▅▅▇███</td></tr><tr><td>Test Loss</td><td>▇██▃▁▃▅▁▄▇▅▅▆▅▅▇▅▅▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>47.05</td></tr><tr><td>Test Loss</td><td>2.0652</td></tr><tr><td>Train Accuracy</td><td>49.68</td></tr><tr><td>Train Loss</td><td>1.88869</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=512 learning_rate=0.012 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/rylmre02' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/rylmre02</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240909_145647-rylmre02/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 1024\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240909_145654-j3op57h9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/j3op57h9' target=\"_blank\">batch_size=1024 learning_rate=0.024 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/j3op57h9' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/j3op57h9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********************************************************************\n",
            "Test Loss: 2.2387969970703123, Test Accuracy: 42.42\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▅▁▂█▇▇▆▄▅▄</td></tr><tr><td>Test Loss</td><td>█▄▃▁▃▂▃▄▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>42.42</td></tr><tr><td>Test Loss</td><td>2.2388</td></tr><tr><td>Train Accuracy</td><td>42.978</td></tr><tr><td>Train Loss</td><td>2.20884</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=1024 learning_rate=0.024 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/j3op57h9' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/j3op57h9</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240909_145654-j3op57h9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 2048\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240909_145707-qvp88fuq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/qvp88fuq' target=\"_blank\">batch_size=2048 learning_rate=0.048 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/qvp88fuq' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/qvp88fuq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********************************************************************\n",
            "Test Loss: 2.52529616355896, Test Accuracy: 36.42\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▂█▆▁▄</td></tr><tr><td>Test Loss</td><td>█▁▃▆▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>36.42</td></tr><tr><td>Test Loss</td><td>2.5253</td></tr><tr><td>Train Accuracy</td><td>35.39</td></tr><tr><td>Train Loss</td><td>2.57354</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=2048 learning_rate=0.048 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/qvp88fuq' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/qvp88fuq</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240909_145707-qvp88fuq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 4096\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240909_145714-8snpsfgb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/8snpsfgb' target=\"_blank\">batch_size=4096 learning_rate=0.096 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/8snpsfgb' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/8snpsfgb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********************************************************************\n",
            "Test Loss: 2.9222216606140137, Test Accuracy: 29.16\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▁▄</td></tr><tr><td>Test Loss</td><td>▃█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>29.16</td></tr><tr><td>Test Loss</td><td>2.92222</td></tr><tr><td>Train Accuracy</td><td>26.796</td></tr><tr><td>Train Loss</td><td>3.01636</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=4096 learning_rate=0.096 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/8snpsfgb' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/8snpsfgb</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240909_145714-8snpsfgb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 8192\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240909_145724-08ulbgo6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/08ulbgo6' target=\"_blank\">batch_size=8192 learning_rate=0.192 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/08ulbgo6' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/08ulbgo6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605673789978027, Training Accuracy: 1.004\n",
            "Validation Loss: 4.603172620137532, Validation Accuracy: 1.4\n",
            "[2/150]: Training Loss: 4.6021647453308105, Training Accuracy: 1.504\n",
            "Validation Loss: 4.599251429239909, Validation Accuracy: 2.01\n",
            "[3/150]: Training Loss: 4.598014794863188, Training Accuracy: 2.032\n",
            "Validation Loss: 4.593982378641765, Validation Accuracy: 2.5\n",
            "[4/150]: Training Loss: 4.592094348027156, Training Accuracy: 2.464\n",
            "Validation Loss: 4.58679739634196, Validation Accuracy: 3.04\n",
            "[5/150]: Training Loss: 4.584430401141827, Training Accuracy: 2.894\n",
            "Validation Loss: 4.577277660369873, Validation Accuracy: 3.18\n",
            "[6/150]: Training Loss: 4.574394446152907, Training Accuracy: 3.174\n",
            "Validation Loss: 4.565267880757649, Validation Accuracy: 3.34\n",
            "[7/150]: Training Loss: 4.561383870931772, Training Accuracy: 3.256\n",
            "Validation Loss: 4.550575415293376, Validation Accuracy: 3.38\n",
            "[8/150]: Training Loss: 4.546961234166072, Training Accuracy: 3.272\n",
            "Validation Loss: 4.533235549926758, Validation Accuracy: 3.34\n",
            "[9/150]: Training Loss: 4.5296975282522345, Training Accuracy: 3.388\n",
            "Validation Loss: 4.513258934020996, Validation Accuracy: 3.29\n",
            "[10/150]: Training Loss: 4.509594073662391, Training Accuracy: 3.626\n",
            "Validation Loss: 4.490594863891602, Validation Accuracy: 3.71\n",
            "[11/150]: Training Loss: 4.486775948451116, Training Accuracy: 3.95\n",
            "Validation Loss: 4.465421358744304, Validation Accuracy: 4.02\n",
            "[12/150]: Training Loss: 4.461170710050142, Training Accuracy: 4.168\n",
            "Validation Loss: 4.437962373097737, Validation Accuracy: 4.16\n",
            "[13/150]: Training Loss: 4.436244561122014, Training Accuracy: 4.33\n",
            "Validation Loss: 4.408618609110515, Validation Accuracy: 4.29\n",
            "[14/150]: Training Loss: 4.405451554518479, Training Accuracy: 4.434\n",
            "Validation Loss: 4.377900123596191, Validation Accuracy: 4.4\n",
            "[15/150]: Training Loss: 4.374882477980393, Training Accuracy: 4.622\n",
            "Validation Loss: 4.3459523518880205, Validation Accuracy: 4.74\n",
            "[16/150]: Training Loss: 4.343658080467811, Training Accuracy: 4.856\n",
            "Validation Loss: 4.313935120900472, Validation Accuracy: 5.09\n",
            "[17/150]: Training Loss: 4.313073635101318, Training Accuracy: 4.978\n",
            "Validation Loss: 4.281905968983968, Validation Accuracy: 5.41\n",
            "[18/150]: Training Loss: 4.28425488105187, Training Accuracy: 5.264\n",
            "Validation Loss: 4.250247001647949, Validation Accuracy: 5.9\n",
            "[19/150]: Training Loss: 4.251151268298809, Training Accuracy: 5.934\n",
            "Validation Loss: 4.2192403475443525, Validation Accuracy: 6.35\n",
            "[20/150]: Training Loss: 4.220469107994666, Training Accuracy: 6.28\n",
            "Validation Loss: 4.190240383148193, Validation Accuracy: 6.77\n",
            "[21/150]: Training Loss: 4.196890867673433, Training Accuracy: 6.82\n",
            "Validation Loss: 4.162028471628825, Validation Accuracy: 7.13\n",
            "[22/150]: Training Loss: 4.169805856851431, Training Accuracy: 6.956\n",
            "Validation Loss: 4.1370697021484375, Validation Accuracy: 7.42\n",
            "[23/150]: Training Loss: 4.146580365987925, Training Accuracy: 7.268\n",
            "Validation Loss: 4.111580848693848, Validation Accuracy: 7.77\n",
            "[24/150]: Training Loss: 4.124923449296218, Training Accuracy: 7.388\n",
            "Validation Loss: 4.088384787241618, Validation Accuracy: 8.11\n",
            "[25/150]: Training Loss: 4.106351192180927, Training Accuracy: 7.89\n",
            "Validation Loss: 4.0675350824991865, Validation Accuracy: 8.28\n",
            "[26/150]: Training Loss: 4.090589926793025, Training Accuracy: 7.988\n",
            "Validation Loss: 4.045119603474935, Validation Accuracy: 8.7\n",
            "[27/150]: Training Loss: 4.069770923027625, Training Accuracy: 8.424\n",
            "Validation Loss: 4.027670224507649, Validation Accuracy: 8.9\n",
            "[28/150]: Training Loss: 4.055119331066425, Training Accuracy: 8.574\n",
            "Validation Loss: 4.006628036499023, Validation Accuracy: 9.29\n",
            "[29/150]: Training Loss: 4.036514942462627, Training Accuracy: 8.94\n",
            "Validation Loss: 3.988546291987101, Validation Accuracy: 9.69\n",
            "[30/150]: Training Loss: 4.0226076016059285, Training Accuracy: 9.242\n",
            "Validation Loss: 3.9728242556254068, Validation Accuracy: 9.85\n",
            "[31/150]: Training Loss: 4.005695324677688, Training Accuracy: 9.258\n",
            "Validation Loss: 3.955763578414917, Validation Accuracy: 10.16\n",
            "[32/150]: Training Loss: 3.9925988820882945, Training Accuracy: 9.686\n",
            "Validation Loss: 3.9387665589650473, Validation Accuracy: 10.39\n",
            "[33/150]: Training Loss: 3.9718362368070164, Training Accuracy: 9.794\n",
            "Validation Loss: 3.9237552483876548, Validation Accuracy: 10.81\n",
            "[34/150]: Training Loss: 3.9614541163811317, Training Accuracy: 10.096\n",
            "Validation Loss: 3.9093241691589355, Validation Accuracy: 11.25\n",
            "[35/150]: Training Loss: 3.948722325838529, Training Accuracy: 10.026\n",
            "Validation Loss: 3.893221855163574, Validation Accuracy: 11.26\n",
            "[36/150]: Training Loss: 3.932725319495568, Training Accuracy: 10.598\n",
            "Validation Loss: 3.877638816833496, Validation Accuracy: 12.03\n",
            "[37/150]: Training Loss: 3.926033992033738, Training Accuracy: 10.684\n",
            "Validation Loss: 3.8624867598215737, Validation Accuracy: 12.1\n",
            "[38/150]: Training Loss: 3.9092987133906436, Training Accuracy: 11.328\n",
            "Validation Loss: 3.848502000172933, Validation Accuracy: 12.29\n",
            "[39/150]: Training Loss: 3.893089092694796, Training Accuracy: 11.29\n",
            "Validation Loss: 3.834321896235148, Validation Accuracy: 12.77\n",
            "[40/150]: Training Loss: 3.884645407016461, Training Accuracy: 11.42\n",
            "Validation Loss: 3.820708751678467, Validation Accuracy: 13.1\n",
            "[41/150]: Training Loss: 3.864343679868258, Training Accuracy: 11.712\n",
            "Validation Loss: 3.80800199508667, Validation Accuracy: 12.63\n",
            "[42/150]: Training Loss: 3.860282090994028, Training Accuracy: 11.864\n",
            "Validation Loss: 3.794013818105062, Validation Accuracy: 13.49\n",
            "[43/150]: Training Loss: 3.8438269175015964, Training Accuracy: 12.156\n",
            "Validation Loss: 3.783547878265381, Validation Accuracy: 13.31\n",
            "[44/150]: Training Loss: 3.832957157721886, Training Accuracy: 12.506\n",
            "Validation Loss: 3.767104387283325, Validation Accuracy: 13.71\n",
            "[45/150]: Training Loss: 3.821493754020104, Training Accuracy: 12.534\n",
            "Validation Loss: 3.7548185189565024, Validation Accuracy: 13.75\n",
            "[46/150]: Training Loss: 3.8128317502828746, Training Accuracy: 12.712\n",
            "Validation Loss: 3.7457602818806968, Validation Accuracy: 13.96\n",
            "[47/150]: Training Loss: 3.799284898317777, Training Accuracy: 12.848\n",
            "Validation Loss: 3.732426325480143, Validation Accuracy: 14.23\n",
            "[48/150]: Training Loss: 3.788727100078876, Training Accuracy: 12.876\n",
            "Validation Loss: 3.7206246852874756, Validation Accuracy: 14.36\n",
            "[49/150]: Training Loss: 3.779775784565852, Training Accuracy: 13.046\n",
            "Validation Loss: 3.7122843265533447, Validation Accuracy: 14.68\n",
            "[50/150]: Training Loss: 3.7671666145324707, Training Accuracy: 13.554\n",
            "Validation Loss: 3.6995603243509927, Validation Accuracy: 14.77\n",
            "[51/150]: Training Loss: 3.753618973952073, Training Accuracy: 13.548\n",
            "Validation Loss: 3.6900697549184165, Validation Accuracy: 14.92\n",
            "[52/150]: Training Loss: 3.7465223715855527, Training Accuracy: 13.624\n",
            "Validation Loss: 3.6770246823628745, Validation Accuracy: 15.32\n",
            "[53/150]: Training Loss: 3.7361971781804013, Training Accuracy: 14.07\n",
            "Validation Loss: 3.668203433354696, Validation Accuracy: 15.6\n",
            "[54/150]: Training Loss: 3.732413310271043, Training Accuracy: 13.938\n",
            "Validation Loss: 3.6599122683207193, Validation Accuracy: 15.6\n",
            "[55/150]: Training Loss: 3.717363247504601, Training Accuracy: 14.34\n",
            "Validation Loss: 3.64747683207194, Validation Accuracy: 15.72\n",
            "[56/150]: Training Loss: 3.706014321400569, Training Accuracy: 14.296\n",
            "Validation Loss: 3.6423365275065103, Validation Accuracy: 15.78\n",
            "[57/150]: Training Loss: 3.698903413919302, Training Accuracy: 14.662\n",
            "Validation Loss: 3.629081885019938, Validation Accuracy: 16.13\n",
            "[58/150]: Training Loss: 3.692998170852661, Training Accuracy: 14.62\n",
            "Validation Loss: 3.622988780339559, Validation Accuracy: 16.34\n",
            "[59/150]: Training Loss: 3.6882295241722693, Training Accuracy: 14.818\n",
            "Validation Loss: 3.6133206685384116, Validation Accuracy: 16.57\n",
            "[60/150]: Training Loss: 3.673269877066979, Training Accuracy: 14.974\n",
            "Validation Loss: 3.6024888356526694, Validation Accuracy: 16.52\n",
            "[61/150]: Training Loss: 3.6757072118612437, Training Accuracy: 14.944\n",
            "Validation Loss: 3.5948301951090493, Validation Accuracy: 17.05\n",
            "[62/150]: Training Loss: 3.6556694507598877, Training Accuracy: 15.268\n",
            "Validation Loss: 3.5849552949269614, Validation Accuracy: 17.2\n",
            "[63/150]: Training Loss: 3.6547499803396373, Training Accuracy: 15.37\n",
            "Validation Loss: 3.5779122511545816, Validation Accuracy: 17.29\n",
            "[64/150]: Training Loss: 3.6503575765169582, Training Accuracy: 15.254\n",
            "Validation Loss: 3.5726945400238037, Validation Accuracy: 17.24\n",
            "[65/150]: Training Loss: 3.6367343939267673, Training Accuracy: 15.374\n",
            "Validation Loss: 3.56174365679423, Validation Accuracy: 17.59\n",
            "[66/150]: Training Loss: 3.632755444600032, Training Accuracy: 15.694\n",
            "Validation Loss: 3.553685426712036, Validation Accuracy: 17.57\n",
            "[67/150]: Training Loss: 3.6256109384390025, Training Accuracy: 15.878\n",
            "Validation Loss: 3.547111670176188, Validation Accuracy: 18.09\n",
            "[68/150]: Training Loss: 3.612996193078848, Training Accuracy: 15.946\n",
            "Validation Loss: 3.538224697113037, Validation Accuracy: 17.96\n",
            "[69/150]: Training Loss: 3.606119852799636, Training Accuracy: 15.944\n",
            "Validation Loss: 3.5335005124409995, Validation Accuracy: 18.1\n",
            "[70/150]: Training Loss: 3.596926285670354, Training Accuracy: 16.176\n",
            "Validation Loss: 3.5244624614715576, Validation Accuracy: 18.13\n",
            "[71/150]: Training Loss: 3.5915960715367246, Training Accuracy: 16.362\n",
            "Validation Loss: 3.5186524391174316, Validation Accuracy: 18.09\n",
            "[72/150]: Training Loss: 3.584472582890437, Training Accuracy: 16.292\n",
            "Validation Loss: 3.51072096824646, Validation Accuracy: 18.26\n",
            "[73/150]: Training Loss: 3.581347868992732, Training Accuracy: 16.558\n",
            "Validation Loss: 3.504685878753662, Validation Accuracy: 18.49\n",
            "[74/150]: Training Loss: 3.58165011039147, Training Accuracy: 16.498\n",
            "Validation Loss: 3.4988303979237876, Validation Accuracy: 18.54\n",
            "[75/150]: Training Loss: 3.5699041073138895, Training Accuracy: 16.828\n",
            "Validation Loss: 3.4898081620534263, Validation Accuracy: 18.68\n",
            "[76/150]: Training Loss: 3.5658033994527965, Training Accuracy: 16.662\n",
            "Validation Loss: 3.4856945673624673, Validation Accuracy: 18.5\n",
            "[77/150]: Training Loss: 3.5581005353194017, Training Accuracy: 16.878\n",
            "Validation Loss: 3.4795003732045493, Validation Accuracy: 18.98\n",
            "[78/150]: Training Loss: 3.557800604746892, Training Accuracy: 16.818\n",
            "Validation Loss: 3.470628579457601, Validation Accuracy: 18.88\n",
            "[79/150]: Training Loss: 3.5468271328852725, Training Accuracy: 17.03\n",
            "Validation Loss: 3.4669202168782554, Validation Accuracy: 19.23\n",
            "[80/150]: Training Loss: 3.5375594175778904, Training Accuracy: 17.102\n",
            "Validation Loss: 3.4611424605051675, Validation Accuracy: 18.96\n",
            "[81/150]: Training Loss: 3.530799205486591, Training Accuracy: 17.122\n",
            "Validation Loss: 3.454043467839559, Validation Accuracy: 19.19\n",
            "[82/150]: Training Loss: 3.527560857626108, Training Accuracy: 17.402\n",
            "Validation Loss: 3.4501057465871177, Validation Accuracy: 19.16\n",
            "[83/150]: Training Loss: 3.526811067874615, Training Accuracy: 17.44\n",
            "Validation Loss: 3.4434916178385415, Validation Accuracy: 19.4\n",
            "[84/150]: Training Loss: 3.5197765276982236, Training Accuracy: 17.354\n",
            "Validation Loss: 3.441331148147583, Validation Accuracy: 19.31\n",
            "[85/150]: Training Loss: 3.516063983623798, Training Accuracy: 17.46\n",
            "Validation Loss: 3.4334657986958823, Validation Accuracy: 19.27\n",
            "[86/150]: Training Loss: 3.514261374106774, Training Accuracy: 17.772\n",
            "Validation Loss: 3.4295153617858887, Validation Accuracy: 19.75\n",
            "[87/150]: Training Loss: 3.5015467680417576, Training Accuracy: 17.908\n",
            "Validation Loss: 3.424708684285482, Validation Accuracy: 19.34\n",
            "[88/150]: Training Loss: 3.4970909448770375, Training Accuracy: 18.002\n",
            "Validation Loss: 3.419410467147827, Validation Accuracy: 19.57\n",
            "[89/150]: Training Loss: 3.500006675720215, Training Accuracy: 17.9\n",
            "Validation Loss: 3.4150261878967285, Validation Accuracy: 19.81\n",
            "[90/150]: Training Loss: 3.4914853389446554, Training Accuracy: 18.234\n",
            "Validation Loss: 3.409981886545817, Validation Accuracy: 19.61\n",
            "[91/150]: Training Loss: 3.4877009575183573, Training Accuracy: 17.972\n",
            "Validation Loss: 3.407442649205526, Validation Accuracy: 20.07\n",
            "[92/150]: Training Loss: 3.4813090654519887, Training Accuracy: 18.322\n",
            "Validation Loss: 3.402990738550822, Validation Accuracy: 19.76\n",
            "[93/150]: Training Loss: 3.4836324728452244, Training Accuracy: 18.204\n",
            "Validation Loss: 3.398730436960856, Validation Accuracy: 19.95\n",
            "[94/150]: Training Loss: 3.472793909219595, Training Accuracy: 18.162\n",
            "Validation Loss: 3.3949574629465737, Validation Accuracy: 19.84\n",
            "[95/150]: Training Loss: 3.47738163287823, Training Accuracy: 18.218\n",
            "Validation Loss: 3.3903884887695312, Validation Accuracy: 20.24\n",
            "[96/150]: Training Loss: 3.4620140149043155, Training Accuracy: 18.682\n",
            "Validation Loss: 3.3866339524586997, Validation Accuracy: 20.18\n",
            "[97/150]: Training Loss: 3.4607166143564076, Training Accuracy: 18.462\n",
            "Validation Loss: 3.3829449812571206, Validation Accuracy: 20.14\n",
            "[98/150]: Training Loss: 3.4573050278883715, Training Accuracy: 18.446\n",
            "Validation Loss: 3.379037777582804, Validation Accuracy: 20.13\n",
            "[99/150]: Training Loss: 3.4551599392524133, Training Accuracy: 18.696\n",
            "Validation Loss: 3.376901388168335, Validation Accuracy: 20.24\n",
            "[100/150]: Training Loss: 3.45456339762761, Training Accuracy: 18.952\n",
            "Validation Loss: 3.373955329259237, Validation Accuracy: 20.28\n",
            "[101/150]: Training Loss: 3.454335414446317, Training Accuracy: 18.672\n",
            "Validation Loss: 3.3703362941741943, Validation Accuracy: 20.3\n",
            "[102/150]: Training Loss: 3.4415004069988546, Training Accuracy: 18.678\n",
            "Validation Loss: 3.3669620354970298, Validation Accuracy: 20.41\n",
            "[103/150]: Training Loss: 3.4507093246166525, Training Accuracy: 18.62\n",
            "Validation Loss: 3.365283727645874, Validation Accuracy: 20.42\n",
            "[104/150]: Training Loss: 3.4429184106680064, Training Accuracy: 18.754\n",
            "Validation Loss: 3.361640135447184, Validation Accuracy: 20.54\n",
            "[105/150]: Training Loss: 3.440951310671293, Training Accuracy: 18.95\n",
            "Validation Loss: 3.359872738520304, Validation Accuracy: 20.46\n",
            "[106/150]: Training Loss: 3.4390654747302714, Training Accuracy: 19.022\n",
            "Validation Loss: 3.3565423488616943, Validation Accuracy: 20.69\n",
            "[107/150]: Training Loss: 3.4380143055549035, Training Accuracy: 18.996\n",
            "Validation Loss: 3.354651610056559, Validation Accuracy: 20.7\n",
            "[108/150]: Training Loss: 3.432965718782865, Training Accuracy: 19.034\n",
            "Validation Loss: 3.3536680539449057, Validation Accuracy: 20.7\n",
            "[109/150]: Training Loss: 3.4401520582345815, Training Accuracy: 18.988\n",
            "Validation Loss: 3.3500028451283774, Validation Accuracy: 20.78\n",
            "[110/150]: Training Loss: 3.4267717874967136, Training Accuracy: 19.244\n",
            "Validation Loss: 3.348034461339315, Validation Accuracy: 20.82\n",
            "[111/150]: Training Loss: 3.4189424698169413, Training Accuracy: 19.232\n",
            "Validation Loss: 3.3472952842712402, Validation Accuracy: 20.66\n",
            "[112/150]: Training Loss: 3.417113634256216, Training Accuracy: 19.016\n",
            "Validation Loss: 3.3435869216918945, Validation Accuracy: 20.72\n",
            "[113/150]: Training Loss: 3.4243063559898963, Training Accuracy: 19.286\n",
            "Validation Loss: 3.3425599733988443, Validation Accuracy: 20.79\n",
            "[114/150]: Training Loss: 3.4240196484785814, Training Accuracy: 19.132\n",
            "Validation Loss: 3.3407468795776367, Validation Accuracy: 20.99\n",
            "[115/150]: Training Loss: 3.4174892718975363, Training Accuracy: 19.368\n",
            "Validation Loss: 3.3392948309580484, Validation Accuracy: 20.89\n",
            "[116/150]: Training Loss: 3.4151670199174147, Training Accuracy: 19.36\n",
            "Validation Loss: 3.337822198867798, Validation Accuracy: 20.87\n",
            "[117/150]: Training Loss: 3.4172579508561354, Training Accuracy: 19.448\n",
            "Validation Loss: 3.335693836212158, Validation Accuracy: 21.01\n",
            "[118/150]: Training Loss: 3.4106742235330434, Training Accuracy: 19.484\n",
            "Validation Loss: 3.334500233332316, Validation Accuracy: 21.03\n",
            "[119/150]: Training Loss: 3.41116503568796, Training Accuracy: 19.404\n",
            "Validation Loss: 3.3329128424326577, Validation Accuracy: 20.89\n",
            "[120/150]: Training Loss: 3.4052381332104025, Training Accuracy: 19.508\n",
            "Validation Loss: 3.3317155838012695, Validation Accuracy: 21.1\n",
            "[121/150]: Training Loss: 3.4083389869103065, Training Accuracy: 19.52\n",
            "Validation Loss: 3.3303403854370117, Validation Accuracy: 21.16\n",
            "[122/150]: Training Loss: 3.4139573390667257, Training Accuracy: 19.49\n",
            "Validation Loss: 3.329955577850342, Validation Accuracy: 21.09\n",
            "[123/150]: Training Loss: 3.4044644832611084, Training Accuracy: 19.552\n",
            "Validation Loss: 3.329407533009847, Validation Accuracy: 20.92\n",
            "[124/150]: Training Loss: 3.415868979233962, Training Accuracy: 19.428\n",
            "Validation Loss: 3.327802896499634, Validation Accuracy: 21.09\n",
            "[125/150]: Training Loss: 3.4050546242640567, Training Accuracy: 19.64\n",
            "Validation Loss: 3.327040513356527, Validation Accuracy: 21.26\n",
            "[126/150]: Training Loss: 3.400483791644757, Training Accuracy: 19.612\n",
            "Validation Loss: 3.326249837875366, Validation Accuracy: 21.15\n",
            "[127/150]: Training Loss: 3.3962889451246996, Training Accuracy: 19.436\n",
            "Validation Loss: 3.325516939163208, Validation Accuracy: 21.11\n",
            "[128/150]: Training Loss: 3.406737070817214, Training Accuracy: 19.614\n",
            "Validation Loss: 3.3247179985046387, Validation Accuracy: 21.2\n",
            "[129/150]: Training Loss: 3.4036002159118652, Training Accuracy: 19.56\n",
            "Validation Loss: 3.3241010506947837, Validation Accuracy: 21.09\n",
            "[130/150]: Training Loss: 3.400709555699275, Training Accuracy: 19.652\n",
            "Validation Loss: 3.323312520980835, Validation Accuracy: 21.12\n",
            "[131/150]: Training Loss: 3.396713220156156, Training Accuracy: 19.736\n",
            "Validation Loss: 3.3226539293924966, Validation Accuracy: 21.16\n",
            "[132/150]: Training Loss: 3.4037779294527493, Training Accuracy: 19.556\n",
            "Validation Loss: 3.3221782048543296, Validation Accuracy: 21.26\n",
            "[133/150]: Training Loss: 3.4009239306816688, Training Accuracy: 19.564\n",
            "Validation Loss: 3.3217578728993735, Validation Accuracy: 21.28\n",
            "[134/150]: Training Loss: 3.4030585839198184, Training Accuracy: 19.808\n",
            "Validation Loss: 3.3210951487223306, Validation Accuracy: 21.22\n",
            "[135/150]: Training Loss: 3.3973034711984487, Training Accuracy: 19.492\n",
            "Validation Loss: 3.321049769719442, Validation Accuracy: 21.24\n",
            "[136/150]: Training Loss: 3.4022842737344594, Training Accuracy: 19.576\n",
            "Validation Loss: 3.3207225799560547, Validation Accuracy: 21.33\n",
            "[137/150]: Training Loss: 3.3996014961829553, Training Accuracy: 19.79\n",
            "Validation Loss: 3.320549805959066, Validation Accuracy: 21.31\n",
            "[138/150]: Training Loss: 3.395272530042208, Training Accuracy: 19.664\n",
            "Validation Loss: 3.3203049500783286, Validation Accuracy: 21.2\n",
            "[139/150]: Training Loss: 3.401835533288809, Training Accuracy: 19.862\n",
            "Validation Loss: 3.3200790882110596, Validation Accuracy: 21.28\n",
            "[140/150]: Training Loss: 3.3989504484029918, Training Accuracy: 19.628\n",
            "Validation Loss: 3.3198469479878745, Validation Accuracy: 21.23\n",
            "[141/150]: Training Loss: 3.3995608183053823, Training Accuracy: 19.576\n",
            "Validation Loss: 3.3197208245595298, Validation Accuracy: 21.21\n",
            "[142/150]: Training Loss: 3.3933530770815334, Training Accuracy: 19.402\n",
            "Validation Loss: 3.319636344909668, Validation Accuracy: 21.14\n",
            "[143/150]: Training Loss: 3.3966986399430494, Training Accuracy: 19.656\n",
            "Validation Loss: 3.3195318380991616, Validation Accuracy: 21.16\n",
            "[144/150]: Training Loss: 3.396922680047842, Training Accuracy: 19.57\n",
            "Validation Loss: 3.3194310665130615, Validation Accuracy: 21.21\n",
            "[145/150]: Training Loss: 3.398597680605375, Training Accuracy: 19.748\n",
            "Validation Loss: 3.3193484942118325, Validation Accuracy: 21.21\n",
            "[146/150]: Training Loss: 3.394164103728074, Training Accuracy: 19.594\n",
            "Validation Loss: 3.319284995396932, Validation Accuracy: 21.18\n",
            "[147/150]: Training Loss: 3.3981102246504564, Training Accuracy: 19.648\n",
            "Validation Loss: 3.319268782933553, Validation Accuracy: 21.2\n",
            "[148/150]: Training Loss: 3.39579569376432, Training Accuracy: 19.484\n",
            "Validation Loss: 3.31925098101298, Validation Accuracy: 21.17\n",
            "[149/150]: Training Loss: 3.3968943449167104, Training Accuracy: 19.676\n",
            "Validation Loss: 3.319252093633016, Validation Accuracy: 21.2\n",
            "[150/150]: Training Loss: 3.4045968605921817, Training Accuracy: 19.494\n",
            "Validation Loss: 3.3192431131998696, Validation Accuracy: 21.19\n",
            "**********************************************************************\n",
            "Test Loss: 3.3192431131998696, Test Accuracy: 21.19\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▁█</td></tr><tr><td>Test Loss</td><td>▃█▁</td></tr><tr><td>Train Accuracy</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Train Loss</td><td>███▇▇▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>21.19</td></tr><tr><td>Test Loss</td><td>3.31924</td></tr><tr><td>Train Accuracy</td><td>19.494</td></tr><tr><td>Train Loss</td><td>3.4046</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=8192 learning_rate=0.192 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/08ulbgo6' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/08ulbgo6</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240909_145724-08ulbgo6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 16384\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240909_151914-9o39fzf2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/9o39fzf2' target=\"_blank\">batch_size=16384 learning_rate=0.384 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/9o39fzf2' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/9o39fzf2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605195302229661, Training Accuracy: 1.188\n",
            "Validation Loss: 4.603690306345622, Validation Accuracy: 1.18\n",
            "[2/150]: Training Loss: 4.603228348952073, Training Accuracy: 1.262\n",
            "Validation Loss: 4.6015998522440595, Validation Accuracy: 1.22\n",
            "[3/150]: Training Loss: 4.601418531858004, Training Accuracy: 1.376\n",
            "Validation Loss: 4.599244912465413, Validation Accuracy: 1.35\n",
            "[4/150]: Training Loss: 4.598752498626709, Training Accuracy: 1.48\n",
            "Validation Loss: 4.596530914306641, Validation Accuracy: 1.43\n",
            "[5/150]: Training Loss: 4.596376162308913, Training Accuracy: 1.56\n",
            "Validation Loss: 4.593318621317546, Validation Accuracy: 1.57\n",
            "[6/150]: Training Loss: 4.5930657019981975, Training Accuracy: 1.696\n",
            "Validation Loss: 4.5895466804504395, Validation Accuracy: 1.83\n",
            "[7/150]: Training Loss: 4.58920170710637, Training Accuracy: 1.724\n",
            "Validation Loss: 4.5851461092631025, Validation Accuracy: 1.93\n",
            "[8/150]: Training Loss: 4.585251294649565, Training Accuracy: 1.872\n",
            "Validation Loss: 4.580074310302734, Validation Accuracy: 2.07\n",
            "[9/150]: Training Loss: 4.579356853778545, Training Accuracy: 1.98\n",
            "Validation Loss: 4.574283758799235, Validation Accuracy: 2.47\n",
            "[10/150]: Training Loss: 4.573918599348802, Training Accuracy: 2.314\n",
            "Validation Loss: 4.567745049794515, Validation Accuracy: 2.7\n",
            "[11/150]: Training Loss: 4.5675644140977125, Training Accuracy: 2.604\n",
            "Validation Loss: 4.560464700063069, Validation Accuracy: 3.05\n",
            "[12/150]: Training Loss: 4.560195996211125, Training Accuracy: 2.954\n",
            "Validation Loss: 4.552396933237712, Validation Accuracy: 3.26\n",
            "[13/150]: Training Loss: 4.553067097297082, Training Accuracy: 3.228\n",
            "Validation Loss: 4.543521563212077, Validation Accuracy: 3.6\n",
            "[14/150]: Training Loss: 4.544567474952111, Training Accuracy: 3.38\n",
            "Validation Loss: 4.533738772074382, Validation Accuracy: 3.76\n",
            "[15/150]: Training Loss: 4.5344684307391825, Training Accuracy: 3.602\n",
            "Validation Loss: 4.523073196411133, Validation Accuracy: 3.85\n",
            "[16/150]: Training Loss: 4.523993345407339, Training Accuracy: 3.74\n",
            "Validation Loss: 4.511544386545817, Validation Accuracy: 3.98\n",
            "[17/150]: Training Loss: 4.51254547559298, Training Accuracy: 3.83\n",
            "Validation Loss: 4.499191602071126, Validation Accuracy: 4.02\n",
            "[18/150]: Training Loss: 4.500373950371375, Training Accuracy: 3.988\n",
            "Validation Loss: 4.486039320627849, Validation Accuracy: 4.17\n",
            "[19/150]: Training Loss: 4.487048405867356, Training Accuracy: 4.168\n",
            "Validation Loss: 4.472110112508138, Validation Accuracy: 4.3\n",
            "[20/150]: Training Loss: 4.474564515627348, Training Accuracy: 4.198\n",
            "Validation Loss: 4.457418759663899, Validation Accuracy: 4.32\n",
            "[21/150]: Training Loss: 4.461341784550593, Training Accuracy: 4.378\n",
            "Validation Loss: 4.442025820414226, Validation Accuracy: 4.53\n",
            "[22/150]: Training Loss: 4.445833132817195, Training Accuracy: 4.544\n",
            "Validation Loss: 4.426051139831543, Validation Accuracy: 4.88\n",
            "[23/150]: Training Loss: 4.4286202650803785, Training Accuracy: 4.756\n",
            "Validation Loss: 4.409497896830241, Validation Accuracy: 5.32\n",
            "[24/150]: Training Loss: 4.414448518019456, Training Accuracy: 4.882\n",
            "Validation Loss: 4.392421404520671, Validation Accuracy: 5.49\n",
            "[25/150]: Training Loss: 4.39862086222722, Training Accuracy: 4.958\n",
            "Validation Loss: 4.374902725219727, Validation Accuracy: 5.52\n",
            "[26/150]: Training Loss: 4.3802571296691895, Training Accuracy: 5.066\n",
            "Validation Loss: 4.357069810231526, Validation Accuracy: 5.5\n",
            "[27/150]: Training Loss: 4.361958833841177, Training Accuracy: 5.182\n",
            "Validation Loss: 4.338964939117432, Validation Accuracy: 5.64\n",
            "[28/150]: Training Loss: 4.344714018014761, Training Accuracy: 5.238\n",
            "Validation Loss: 4.32078218460083, Validation Accuracy: 5.7\n",
            "[29/150]: Training Loss: 4.328766125899095, Training Accuracy: 5.512\n",
            "Validation Loss: 4.3025557200113935, Validation Accuracy: 5.73\n",
            "[30/150]: Training Loss: 4.308638902810904, Training Accuracy: 5.556\n",
            "Validation Loss: 4.284190018971761, Validation Accuracy: 5.89\n",
            "[31/150]: Training Loss: 4.295640431917631, Training Accuracy: 5.74\n",
            "Validation Loss: 4.266061305999756, Validation Accuracy: 6.13\n",
            "[32/150]: Training Loss: 4.273596543532151, Training Accuracy: 5.766\n",
            "Validation Loss: 4.248404026031494, Validation Accuracy: 6.1\n",
            "[33/150]: Training Loss: 4.26108455657959, Training Accuracy: 6.078\n",
            "Validation Loss: 4.230844020843506, Validation Accuracy: 6.39\n",
            "[34/150]: Training Loss: 4.245028239030105, Training Accuracy: 6.248\n",
            "Validation Loss: 4.213306903839111, Validation Accuracy: 6.66\n",
            "[35/150]: Training Loss: 4.222837191361648, Training Accuracy: 6.528\n",
            "Validation Loss: 4.1965861320495605, Validation Accuracy: 6.97\n",
            "[36/150]: Training Loss: 4.211817007798415, Training Accuracy: 6.72\n",
            "Validation Loss: 4.180171966552734, Validation Accuracy: 7.2\n",
            "[37/150]: Training Loss: 4.194102434011606, Training Accuracy: 6.968\n",
            "Validation Loss: 4.163711865743001, Validation Accuracy: 7.27\n",
            "[38/150]: Training Loss: 4.1818108925452595, Training Accuracy: 7.164\n",
            "Validation Loss: 4.148358980814616, Validation Accuracy: 7.43\n",
            "[39/150]: Training Loss: 4.167687709514912, Training Accuracy: 7.2\n",
            "Validation Loss: 4.134000460306804, Validation Accuracy: 7.75\n",
            "[40/150]: Training Loss: 4.154652888958271, Training Accuracy: 7.544\n",
            "Validation Loss: 4.119375069936116, Validation Accuracy: 8.02\n",
            "[41/150]: Training Loss: 4.138120797964243, Training Accuracy: 7.726\n",
            "Validation Loss: 4.1061147054036455, Validation Accuracy: 8.18\n",
            "[42/150]: Training Loss: 4.127343397874099, Training Accuracy: 7.752\n",
            "Validation Loss: 4.093040784200032, Validation Accuracy: 8.44\n",
            "[43/150]: Training Loss: 4.116818354679988, Training Accuracy: 7.854\n",
            "Validation Loss: 4.080575784047444, Validation Accuracy: 8.51\n",
            "[44/150]: Training Loss: 4.105071581326998, Training Accuracy: 8.01\n",
            "Validation Loss: 4.068602085113525, Validation Accuracy: 8.63\n",
            "[45/150]: Training Loss: 4.095542100759653, Training Accuracy: 8.106\n",
            "Validation Loss: 4.05771271387736, Validation Accuracy: 8.67\n",
            "[46/150]: Training Loss: 4.086432530329778, Training Accuracy: 8.216\n",
            "Validation Loss: 4.047398726145427, Validation Accuracy: 9.05\n",
            "[47/150]: Training Loss: 4.077257779928354, Training Accuracy: 8.298\n",
            "Validation Loss: 4.037300745646159, Validation Accuracy: 9.25\n",
            "[48/150]: Training Loss: 4.070531661693867, Training Accuracy: 8.444\n",
            "Validation Loss: 4.026993910471599, Validation Accuracy: 9.35\n",
            "[49/150]: Training Loss: 4.056069080646221, Training Accuracy: 8.372\n",
            "Validation Loss: 4.017492135365804, Validation Accuracy: 9.47\n",
            "[50/150]: Training Loss: 4.050787889040434, Training Accuracy: 8.65\n",
            "Validation Loss: 4.008095184961955, Validation Accuracy: 9.42\n",
            "[51/150]: Training Loss: 4.0411876531747675, Training Accuracy: 8.766\n",
            "Validation Loss: 3.999807278315226, Validation Accuracy: 9.67\n",
            "[52/150]: Training Loss: 4.032361874213586, Training Accuracy: 8.88\n",
            "Validation Loss: 3.9911611080169678, Validation Accuracy: 9.98\n",
            "[53/150]: Training Loss: 4.026911222017729, Training Accuracy: 8.928\n",
            "Validation Loss: 3.983031988143921, Validation Accuracy: 10.07\n",
            "[54/150]: Training Loss: 4.010928612488967, Training Accuracy: 9.168\n",
            "Validation Loss: 3.974648952484131, Validation Accuracy: 10.11\n",
            "[55/150]: Training Loss: 4.008889455061692, Training Accuracy: 9.324\n",
            "Validation Loss: 3.966226895650228, Validation Accuracy: 10.06\n",
            "[56/150]: Training Loss: 4.000082969665527, Training Accuracy: 9.412\n",
            "Validation Loss: 3.9585606257120767, Validation Accuracy: 10.37\n",
            "[57/150]: Training Loss: 4.002067309159499, Training Accuracy: 9.524\n",
            "Validation Loss: 3.951122442881266, Validation Accuracy: 10.68\n",
            "[58/150]: Training Loss: 3.98760856114901, Training Accuracy: 9.656\n",
            "Validation Loss: 3.9432335694630942, Validation Accuracy: 10.69\n",
            "[59/150]: Training Loss: 3.9832713420574484, Training Accuracy: 9.78\n",
            "Validation Loss: 3.936310370763143, Validation Accuracy: 10.64\n",
            "[60/150]: Training Loss: 3.9734121652749868, Training Accuracy: 10.03\n",
            "Validation Loss: 3.9290212790171304, Validation Accuracy: 10.89\n",
            "[61/150]: Training Loss: 3.968223810195923, Training Accuracy: 10.12\n",
            "Validation Loss: 3.9214232762654624, Validation Accuracy: 10.94\n",
            "[62/150]: Training Loss: 3.961422094931969, Training Accuracy: 10.152\n",
            "Validation Loss: 3.9146029154459634, Validation Accuracy: 11.19\n",
            "[63/150]: Training Loss: 3.9600368096278262, Training Accuracy: 10.312\n",
            "Validation Loss: 3.9071216583251953, Validation Accuracy: 11.22\n",
            "[64/150]: Training Loss: 3.9439797768226037, Training Accuracy: 10.428\n",
            "Validation Loss: 3.9005125363667807, Validation Accuracy: 11.5\n",
            "[65/150]: Training Loss: 3.9393946757683387, Training Accuracy: 10.648\n",
            "Validation Loss: 3.893540700276693, Validation Accuracy: 11.37\n",
            "[66/150]: Training Loss: 3.9361312022575965, Training Accuracy: 10.492\n",
            "Validation Loss: 3.8872005144755044, Validation Accuracy: 11.66\n",
            "[67/150]: Training Loss: 3.926033148398766, Training Accuracy: 10.67\n",
            "Validation Loss: 3.880112330118815, Validation Accuracy: 11.93\n",
            "[68/150]: Training Loss: 3.923636693220872, Training Accuracy: 10.81\n",
            "Validation Loss: 3.8733906745910645, Validation Accuracy: 12.1\n",
            "[69/150]: Training Loss: 3.9207047865940976, Training Accuracy: 11.098\n",
            "Validation Loss: 3.868489980697632, Validation Accuracy: 12.05\n",
            "[70/150]: Training Loss: 3.913045131243192, Training Accuracy: 10.984\n",
            "Validation Loss: 3.8620645999908447, Validation Accuracy: 12.0\n",
            "[71/150]: Training Loss: 3.9061514230874868, Training Accuracy: 10.892\n",
            "Validation Loss: 3.8555472691853843, Validation Accuracy: 12.19\n",
            "[72/150]: Training Loss: 3.9040152843181906, Training Accuracy: 11.146\n",
            "Validation Loss: 3.8497202396392822, Validation Accuracy: 12.47\n",
            "[73/150]: Training Loss: 3.8962000700143666, Training Accuracy: 11.252\n",
            "Validation Loss: 3.8439696629842124, Validation Accuracy: 12.44\n",
            "[74/150]: Training Loss: 3.890927479817317, Training Accuracy: 11.38\n",
            "Validation Loss: 3.83886710802714, Validation Accuracy: 12.58\n",
            "[75/150]: Training Loss: 3.8821005821228027, Training Accuracy: 11.44\n",
            "Validation Loss: 3.832904895146688, Validation Accuracy: 12.79\n",
            "[76/150]: Training Loss: 3.883660995043241, Training Accuracy: 11.508\n",
            "Validation Loss: 3.8277692000071206, Validation Accuracy: 12.71\n",
            "[77/150]: Training Loss: 3.877098560333252, Training Accuracy: 11.58\n",
            "Validation Loss: 3.822240670522054, Validation Accuracy: 12.81\n",
            "[78/150]: Training Loss: 3.8770018357496996, Training Accuracy: 11.724\n",
            "Validation Loss: 3.8177859783172607, Validation Accuracy: 12.89\n",
            "[79/150]: Training Loss: 3.8679996637197642, Training Accuracy: 11.886\n",
            "Validation Loss: 3.8130906422932944, Validation Accuracy: 13.03\n",
            "[80/150]: Training Loss: 3.8641435366410475, Training Accuracy: 11.774\n",
            "Validation Loss: 3.807639996210734, Validation Accuracy: 12.97\n",
            "[81/150]: Training Loss: 3.8596745270949144, Training Accuracy: 11.856\n",
            "Validation Loss: 3.8031133015950522, Validation Accuracy: 13.19\n",
            "[82/150]: Training Loss: 3.8582583574148326, Training Accuracy: 12.054\n",
            "Validation Loss: 3.798879782358805, Validation Accuracy: 13.07\n",
            "[83/150]: Training Loss: 3.8505105605492225, Training Accuracy: 11.986\n",
            "Validation Loss: 3.7947102387746177, Validation Accuracy: 13.02\n",
            "[84/150]: Training Loss: 3.843576119496272, Training Accuracy: 12.022\n",
            "Validation Loss: 3.790907700856527, Validation Accuracy: 13.24\n",
            "[85/150]: Training Loss: 3.8492014408111572, Training Accuracy: 12.31\n",
            "Validation Loss: 3.78625480333964, Validation Accuracy: 13.31\n",
            "[86/150]: Training Loss: 3.8407978277940016, Training Accuracy: 12.232\n",
            "Validation Loss: 3.7826090653737388, Validation Accuracy: 13.34\n",
            "[87/150]: Training Loss: 3.8324120411506066, Training Accuracy: 12.202\n",
            "Validation Loss: 3.7791737715403237, Validation Accuracy: 13.28\n",
            "[88/150]: Training Loss: 3.8318311067727895, Training Accuracy: 12.368\n",
            "Validation Loss: 3.774513324101766, Validation Accuracy: 13.33\n",
            "[89/150]: Training Loss: 3.8312925008627086, Training Accuracy: 12.35\n",
            "Validation Loss: 3.7711471716562905, Validation Accuracy: 13.41\n",
            "[90/150]: Training Loss: 3.8262144969059873, Training Accuracy: 12.554\n",
            "Validation Loss: 3.768536647160848, Validation Accuracy: 13.56\n",
            "[91/150]: Training Loss: 3.824205288520226, Training Accuracy: 12.404\n",
            "Validation Loss: 3.7648067474365234, Validation Accuracy: 13.57\n",
            "[92/150]: Training Loss: 3.8177336179293118, Training Accuracy: 12.574\n",
            "Validation Loss: 3.7615938981374106, Validation Accuracy: 13.74\n",
            "[93/150]: Training Loss: 3.8196998559511623, Training Accuracy: 12.696\n",
            "Validation Loss: 3.758485794067383, Validation Accuracy: 13.71\n",
            "[94/150]: Training Loss: 3.8129977079538198, Training Accuracy: 12.732\n",
            "Validation Loss: 3.7551841735839844, Validation Accuracy: 13.65\n",
            "[95/150]: Training Loss: 3.8086902361649733, Training Accuracy: 12.71\n",
            "Validation Loss: 3.7523674964904785, Validation Accuracy: 13.81\n",
            "[96/150]: Training Loss: 3.808085661668044, Training Accuracy: 12.696\n",
            "Validation Loss: 3.7495387395222983, Validation Accuracy: 13.85\n",
            "[97/150]: Training Loss: 3.802524841748751, Training Accuracy: 12.962\n",
            "Validation Loss: 3.746455272038778, Validation Accuracy: 13.95\n",
            "[98/150]: Training Loss: 3.802551031112671, Training Accuracy: 12.852\n",
            "Validation Loss: 3.743435780207316, Validation Accuracy: 14.0\n",
            "[99/150]: Training Loss: 3.8002174267402062, Training Accuracy: 12.768\n",
            "Validation Loss: 3.7410970528920493, Validation Accuracy: 14.1\n",
            "[100/150]: Training Loss: 3.7964800137739916, Training Accuracy: 13.008\n",
            "Validation Loss: 3.738691965738932, Validation Accuracy: 14.04\n",
            "[101/150]: Training Loss: 3.8008696482731748, Training Accuracy: 12.928\n",
            "Validation Loss: 3.736536423365275, Validation Accuracy: 14.12\n",
            "[102/150]: Training Loss: 3.7985153381641092, Training Accuracy: 13.046\n",
            "Validation Loss: 3.7342260678609214, Validation Accuracy: 14.13\n",
            "[103/150]: Training Loss: 3.788901237341074, Training Accuracy: 12.956\n",
            "Validation Loss: 3.7322537104288735, Validation Accuracy: 14.16\n",
            "[104/150]: Training Loss: 3.789390307206374, Training Accuracy: 13.152\n",
            "Validation Loss: 3.7304598490397134, Validation Accuracy: 14.21\n",
            "[105/150]: Training Loss: 3.794120220037607, Training Accuracy: 13.074\n",
            "Validation Loss: 3.728029807408651, Validation Accuracy: 14.22\n",
            "[106/150]: Training Loss: 3.7867697935837965, Training Accuracy: 13.112\n",
            "Validation Loss: 3.72580893834432, Validation Accuracy: 14.29\n",
            "[107/150]: Training Loss: 3.783896464567918, Training Accuracy: 13.17\n",
            "Validation Loss: 3.724055051803589, Validation Accuracy: 14.48\n",
            "[108/150]: Training Loss: 3.7876304663144627, Training Accuracy: 13.246\n",
            "Validation Loss: 3.7227447827657065, Validation Accuracy: 14.43\n",
            "[109/150]: Training Loss: 3.7854911914238563, Training Accuracy: 13.37\n",
            "Validation Loss: 3.7210354010264077, Validation Accuracy: 14.37\n",
            "[110/150]: Training Loss: 3.782796804721539, Training Accuracy: 13.138\n",
            "Validation Loss: 3.719072182973226, Validation Accuracy: 14.29\n",
            "[111/150]: Training Loss: 3.774929560147799, Training Accuracy: 13.166\n",
            "Validation Loss: 3.7176389694213867, Validation Accuracy: 14.36\n",
            "[112/150]: Training Loss: 3.7832509921147275, Training Accuracy: 13.264\n",
            "Validation Loss: 3.716113011042277, Validation Accuracy: 14.49\n",
            "[113/150]: Training Loss: 3.7756609366490292, Training Accuracy: 13.498\n",
            "Validation Loss: 3.714516798655192, Validation Accuracy: 14.59\n",
            "[114/150]: Training Loss: 3.7737104342533994, Training Accuracy: 13.49\n",
            "Validation Loss: 3.7133214473724365, Validation Accuracy: 14.64\n",
            "[115/150]: Training Loss: 3.7693820733290453, Training Accuracy: 13.302\n",
            "Validation Loss: 3.712203105290731, Validation Accuracy: 14.65\n",
            "[116/150]: Training Loss: 3.7698808633364163, Training Accuracy: 13.234\n",
            "Validation Loss: 3.7110175291697183, Validation Accuracy: 14.62\n",
            "[117/150]: Training Loss: 3.767371287712684, Training Accuracy: 13.46\n",
            "Validation Loss: 3.7098007996877036, Validation Accuracy: 14.59\n",
            "[118/150]: Training Loss: 3.7649275889763465, Training Accuracy: 13.428\n",
            "Validation Loss: 3.7086034615834556, Validation Accuracy: 14.53\n",
            "[119/150]: Training Loss: 3.774198972261869, Training Accuracy: 13.548\n",
            "Validation Loss: 3.707650105158488, Validation Accuracy: 14.6\n",
            "[120/150]: Training Loss: 3.7659242703364444, Training Accuracy: 13.432\n",
            "Validation Loss: 3.706760803858439, Validation Accuracy: 14.63\n",
            "[121/150]: Training Loss: 3.764529906786405, Training Accuracy: 13.468\n",
            "Validation Loss: 3.7060027917226157, Validation Accuracy: 14.68\n",
            "[122/150]: Training Loss: 3.766430451319768, Training Accuracy: 13.38\n",
            "Validation Loss: 3.705351988474528, Validation Accuracy: 14.77\n",
            "[123/150]: Training Loss: 3.7624277701744666, Training Accuracy: 13.58\n",
            "Validation Loss: 3.704648971557617, Validation Accuracy: 14.82\n",
            "[124/150]: Training Loss: 3.7651446415827823, Training Accuracy: 13.578\n",
            "Validation Loss: 3.7039496103922525, Validation Accuracy: 14.78\n",
            "[125/150]: Training Loss: 3.7655874765836277, Training Accuracy: 13.662\n",
            "Validation Loss: 3.7032397588094077, Validation Accuracy: 14.78\n",
            "[126/150]: Training Loss: 3.7654868822831373, Training Accuracy: 13.572\n",
            "Validation Loss: 3.7027341524759927, Validation Accuracy: 14.77\n",
            "[127/150]: Training Loss: 3.7642886271843543, Training Accuracy: 13.51\n",
            "Validation Loss: 3.702260176340739, Validation Accuracy: 14.74\n",
            "[128/150]: Training Loss: 3.762056093949538, Training Accuracy: 13.506\n",
            "Validation Loss: 3.701777617136637, Validation Accuracy: 14.71\n",
            "[129/150]: Training Loss: 3.761383790236253, Training Accuracy: 13.618\n",
            "Validation Loss: 3.7012685934702554, Validation Accuracy: 14.69\n",
            "[130/150]: Training Loss: 3.7614092276646542, Training Accuracy: 13.486\n",
            "Validation Loss: 3.7008363405863443, Validation Accuracy: 14.74\n",
            "[131/150]: Training Loss: 3.758950710296631, Training Accuracy: 13.588\n",
            "Validation Loss: 3.700547695159912, Validation Accuracy: 14.77\n",
            "[132/150]: Training Loss: 3.7610786878145657, Training Accuracy: 13.596\n",
            "Validation Loss: 3.7002243200937905, Validation Accuracy: 14.77\n",
            "[133/150]: Training Loss: 3.7634519797105055, Training Accuracy: 13.738\n",
            "Validation Loss: 3.6998650232950845, Validation Accuracy: 14.78\n",
            "[134/150]: Training Loss: 3.7617851954240065, Training Accuracy: 13.51\n",
            "Validation Loss: 3.699594577153524, Validation Accuracy: 14.8\n",
            "[135/150]: Training Loss: 3.7595828496492825, Training Accuracy: 13.486\n",
            "Validation Loss: 3.699335257212321, Validation Accuracy: 14.81\n",
            "[136/150]: Training Loss: 3.7621973844674916, Training Accuracy: 13.678\n",
            "Validation Loss: 3.6990674336751304, Validation Accuracy: 14.85\n",
            "[137/150]: Training Loss: 3.764283143557035, Training Accuracy: 13.482\n",
            "Validation Loss: 3.698822816212972, Validation Accuracy: 14.84\n",
            "[138/150]: Training Loss: 3.7604700051821194, Training Accuracy: 13.52\n",
            "Validation Loss: 3.6986215114593506, Validation Accuracy: 14.85\n",
            "[139/150]: Training Loss: 3.7576978573432336, Training Accuracy: 13.658\n",
            "Validation Loss: 3.6984519958496094, Validation Accuracy: 14.8\n",
            "[140/150]: Training Loss: 3.759762158760658, Training Accuracy: 13.742\n",
            "Validation Loss: 3.6983555952707925, Validation Accuracy: 14.79\n",
            "[141/150]: Training Loss: 3.7484704164358287, Training Accuracy: 13.7\n",
            "Validation Loss: 3.6982423464457193, Validation Accuracy: 14.78\n",
            "[142/150]: Training Loss: 3.76192490871136, Training Accuracy: 13.532\n",
            "Validation Loss: 3.698158343633016, Validation Accuracy: 14.8\n",
            "[143/150]: Training Loss: 3.75566422022306, Training Accuracy: 13.656\n",
            "Validation Loss: 3.698089361190796, Validation Accuracy: 14.81\n",
            "[144/150]: Training Loss: 3.7641571301680345, Training Accuracy: 13.488\n",
            "Validation Loss: 3.698044538497925, Validation Accuracy: 14.8\n",
            "[145/150]: Training Loss: 3.7635292823498068, Training Accuracy: 13.548\n",
            "Validation Loss: 3.698029120763143, Validation Accuracy: 14.81\n",
            "[146/150]: Training Loss: 3.760449666243333, Training Accuracy: 13.56\n",
            "Validation Loss: 3.6980087757110596, Validation Accuracy: 14.82\n",
            "[147/150]: Training Loss: 3.757055851129385, Training Accuracy: 13.466\n",
            "Validation Loss: 3.697993437449137, Validation Accuracy: 14.84\n",
            "[148/150]: Training Loss: 3.759396663078895, Training Accuracy: 13.506\n",
            "Validation Loss: 3.6979875564575195, Validation Accuracy: 14.83\n",
            "[149/150]: Training Loss: 3.760320938550509, Training Accuracy: 13.56\n",
            "Validation Loss: 3.697981913884481, Validation Accuracy: 14.82\n",
            "[150/150]: Training Loss: 3.758735124881451, Training Accuracy: 13.586\n",
            "Validation Loss: 3.6979848543802896, Validation Accuracy: 14.82\n",
            "**********************************************************************\n",
            "Test Loss: 3.6979848543802896, Test Accuracy: 14.82\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▁▃</td></tr><tr><td>Test Loss</td><td>▁█▃</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>Train Loss</td><td>████▇▇▆▆▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>14.82</td></tr><tr><td>Test Loss</td><td>3.69798</td></tr><tr><td>Train Accuracy</td><td>13.586</td></tr><tr><td>Train Loss</td><td>3.75874</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=16384 learning_rate=0.384 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/9o39fzf2' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/9o39fzf2</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240909_151914-9o39fzf2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 32768\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.9 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/mldl/wandb/run-20240909_154143-tuoafh0s</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/tuoafh0s' target=\"_blank\">batch_size=32768 learning_rate=0.768 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/tuoafh0s' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/tuoafh0s</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605868999774639, Training Accuracy: 1.022\n",
            "Validation Loss: 4.605166753133138, Validation Accuracy: 1.02\n",
            "[2/150]: Training Loss: 4.604829091292161, Training Accuracy: 1.06\n",
            "Validation Loss: 4.603924592336019, Validation Accuracy: 1.02\n",
            "[3/150]: Training Loss: 4.603444246145395, Training Accuracy: 1.108\n",
            "Validation Loss: 4.60266653696696, Validation Accuracy: 1.07\n",
            "[4/150]: Training Loss: 4.602419009575477, Training Accuracy: 1.162\n",
            "Validation Loss: 4.601371606190999, Validation Accuracy: 1.12\n",
            "[5/150]: Training Loss: 4.601135877462534, Training Accuracy: 1.23\n",
            "Validation Loss: 4.600017070770264, Validation Accuracy: 1.2\n",
            "[6/150]: Training Loss: 4.59978433755728, Training Accuracy: 1.272\n",
            "Validation Loss: 4.598594347635905, Validation Accuracy: 1.24\n",
            "[7/150]: Training Loss: 4.598548449002779, Training Accuracy: 1.28\n",
            "Validation Loss: 4.597080548604329, Validation Accuracy: 1.32\n",
            "[8/150]: Training Loss: 4.596810964437632, Training Accuracy: 1.34\n",
            "Validation Loss: 4.595457553863525, Validation Accuracy: 1.38\n",
            "[9/150]: Training Loss: 4.595354153559758, Training Accuracy: 1.292\n",
            "Validation Loss: 4.593699137369792, Validation Accuracy: 1.31\n",
            "[10/150]: Training Loss: 4.593594991243803, Training Accuracy: 1.232\n",
            "Validation Loss: 4.591802914937337, Validation Accuracy: 1.2\n",
            "[11/150]: Training Loss: 4.59193046276386, Training Accuracy: 1.226\n",
            "Validation Loss: 4.589766502380371, Validation Accuracy: 1.13\n",
            "[12/150]: Training Loss: 4.589760413536658, Training Accuracy: 1.31\n",
            "Validation Loss: 4.587597052256267, Validation Accuracy: 1.14\n",
            "[13/150]: Training Loss: 4.587990944202129, Training Accuracy: 1.258\n",
            "Validation Loss: 4.585292339324951, Validation Accuracy: 1.2\n",
            "[14/150]: Training Loss: 4.585862783285288, Training Accuracy: 1.434\n",
            "Validation Loss: 4.5828351974487305, Validation Accuracy: 1.44\n",
            "[15/150]: Training Loss: 4.583077357365535, Training Accuracy: 1.52\n",
            "Validation Loss: 4.580225149790446, Validation Accuracy: 1.68\n",
            "[16/150]: Training Loss: 4.58097270818857, Training Accuracy: 1.756\n",
            "Validation Loss: 4.577462355295817, Validation Accuracy: 1.99\n",
            "[17/150]: Training Loss: 4.578157204848069, Training Accuracy: 2.026\n",
            "Validation Loss: 4.5745368003845215, Validation Accuracy: 2.18\n",
            "[18/150]: Training Loss: 4.575414290794959, Training Accuracy: 2.35\n",
            "Validation Loss: 4.571436723073323, Validation Accuracy: 2.43\n",
            "[19/150]: Training Loss: 4.572410656855657, Training Accuracy: 2.618\n",
            "Validation Loss: 4.568168481190999, Validation Accuracy: 2.56\n",
            "[20/150]: Training Loss: 4.569349839137151, Training Accuracy: 2.728\n",
            "Validation Loss: 4.564742088317871, Validation Accuracy: 2.7\n",
            "[21/150]: Training Loss: 4.566024303436279, Training Accuracy: 2.896\n",
            "Validation Loss: 4.561145146687825, Validation Accuracy: 2.91\n",
            "[22/150]: Training Loss: 4.561892436100886, Training Accuracy: 3.062\n",
            "Validation Loss: 4.557381788889567, Validation Accuracy: 3.09\n",
            "[23/150]: Training Loss: 4.558697773860051, Training Accuracy: 3.184\n",
            "Validation Loss: 4.5534515380859375, Validation Accuracy: 3.2\n",
            "[24/150]: Training Loss: 4.5543107619652385, Training Accuracy: 3.274\n",
            "Validation Loss: 4.549355665842692, Validation Accuracy: 3.3\n",
            "[25/150]: Training Loss: 4.551203544323261, Training Accuracy: 3.282\n",
            "Validation Loss: 4.545085748036702, Validation Accuracy: 3.44\n",
            "[26/150]: Training Loss: 4.546850461226243, Training Accuracy: 3.448\n",
            "Validation Loss: 4.540655771891276, Validation Accuracy: 3.51\n",
            "[27/150]: Training Loss: 4.542212632986216, Training Accuracy: 3.498\n",
            "Validation Loss: 4.536064624786377, Validation Accuracy: 3.66\n",
            "[28/150]: Training Loss: 4.5382771125206585, Training Accuracy: 3.586\n",
            "Validation Loss: 4.531307220458984, Validation Accuracy: 3.7\n",
            "[29/150]: Training Loss: 4.533272376427283, Training Accuracy: 3.62\n",
            "Validation Loss: 4.526397069295247, Validation Accuracy: 3.73\n",
            "[30/150]: Training Loss: 4.529263056241549, Training Accuracy: 3.818\n",
            "Validation Loss: 4.521340052286784, Validation Accuracy: 3.8\n",
            "[31/150]: Training Loss: 4.524033069610596, Training Accuracy: 3.72\n",
            "Validation Loss: 4.5161333084106445, Validation Accuracy: 3.84\n",
            "[32/150]: Training Loss: 4.519355553847093, Training Accuracy: 3.802\n",
            "Validation Loss: 4.510790665944417, Validation Accuracy: 3.85\n",
            "[33/150]: Training Loss: 4.514233038975642, Training Accuracy: 3.818\n",
            "Validation Loss: 4.505308945973714, Validation Accuracy: 3.89\n",
            "[34/150]: Training Loss: 4.509561355297382, Training Accuracy: 3.878\n",
            "Validation Loss: 4.499698638916016, Validation Accuracy: 3.9\n",
            "[35/150]: Training Loss: 4.503680155827449, Training Accuracy: 3.842\n",
            "Validation Loss: 4.493971347808838, Validation Accuracy: 3.98\n",
            "[36/150]: Training Loss: 4.498465831463154, Training Accuracy: 3.786\n",
            "Validation Loss: 4.488128503163655, Validation Accuracy: 4.02\n",
            "[37/150]: Training Loss: 4.492101926069993, Training Accuracy: 3.898\n",
            "Validation Loss: 4.482178370157878, Validation Accuracy: 4.03\n",
            "[38/150]: Training Loss: 4.486426793611967, Training Accuracy: 3.828\n",
            "Validation Loss: 4.476130485534668, Validation Accuracy: 4.06\n",
            "[39/150]: Training Loss: 4.480412043057955, Training Accuracy: 3.87\n",
            "Validation Loss: 4.469987551371257, Validation Accuracy: 4.15\n",
            "[40/150]: Training Loss: 4.475217855893648, Training Accuracy: 3.852\n",
            "Validation Loss: 4.463754812876384, Validation Accuracy: 4.13\n",
            "[41/150]: Training Loss: 4.469030857086182, Training Accuracy: 3.908\n",
            "Validation Loss: 4.457444985707601, Validation Accuracy: 4.18\n",
            "[42/150]: Training Loss: 4.462915824009822, Training Accuracy: 3.93\n",
            "Validation Loss: 4.451065540313721, Validation Accuracy: 4.19\n",
            "[43/150]: Training Loss: 4.4580793013939495, Training Accuracy: 3.87\n",
            "Validation Loss: 4.444647630055745, Validation Accuracy: 4.16\n",
            "[44/150]: Training Loss: 4.451702007880578, Training Accuracy: 3.93\n",
            "Validation Loss: 4.438174406687419, Validation Accuracy: 4.14\n",
            "[45/150]: Training Loss: 4.445636712587797, Training Accuracy: 3.916\n",
            "Validation Loss: 4.431674480438232, Validation Accuracy: 4.09\n",
            "[46/150]: Training Loss: 4.438098173875075, Training Accuracy: 3.91\n",
            "Validation Loss: 4.425153891245524, Validation Accuracy: 4.05\n",
            "[47/150]: Training Loss: 4.431908534123347, Training Accuracy: 4.012\n",
            "Validation Loss: 4.41862138112386, Validation Accuracy: 4.07\n",
            "[48/150]: Training Loss: 4.426296637608455, Training Accuracy: 4.062\n",
            "Validation Loss: 4.412086168924968, Validation Accuracy: 4.17\n",
            "[49/150]: Training Loss: 4.4212087117708645, Training Accuracy: 3.948\n",
            "Validation Loss: 4.405553340911865, Validation Accuracy: 4.17\n",
            "[50/150]: Training Loss: 4.4148682814378, Training Accuracy: 4.208\n",
            "Validation Loss: 4.399036407470703, Validation Accuracy: 4.19\n",
            "[51/150]: Training Loss: 4.4069629815908575, Training Accuracy: 3.966\n",
            "Validation Loss: 4.3925407727559405, Validation Accuracy: 4.27\n",
            "[52/150]: Training Loss: 4.401810352618877, Training Accuracy: 4.098\n",
            "Validation Loss: 4.386075496673584, Validation Accuracy: 4.34\n",
            "[53/150]: Training Loss: 4.395487968738262, Training Accuracy: 4.162\n",
            "Validation Loss: 4.379663785298665, Validation Accuracy: 4.46\n",
            "[54/150]: Training Loss: 4.390330498035137, Training Accuracy: 4.322\n",
            "Validation Loss: 4.373293558756511, Validation Accuracy: 4.54\n",
            "[55/150]: Training Loss: 4.383749411656306, Training Accuracy: 4.226\n",
            "Validation Loss: 4.366978486378987, Validation Accuracy: 4.58\n",
            "[56/150]: Training Loss: 4.376887284792387, Training Accuracy: 4.382\n",
            "Validation Loss: 4.360726833343506, Validation Accuracy: 4.66\n",
            "[57/150]: Training Loss: 4.369322116558369, Training Accuracy: 4.41\n",
            "Validation Loss: 4.354573408762614, Validation Accuracy: 4.79\n",
            "[58/150]: Training Loss: 4.364068178030161, Training Accuracy: 4.528\n",
            "Validation Loss: 4.348487536112468, Validation Accuracy: 4.8\n",
            "[59/150]: Training Loss: 4.360609458043025, Training Accuracy: 4.58\n",
            "Validation Loss: 4.342475573221843, Validation Accuracy: 4.89\n",
            "[60/150]: Training Loss: 4.354535799760085, Training Accuracy: 4.6\n",
            "Validation Loss: 4.336496194203694, Validation Accuracy: 4.89\n",
            "[61/150]: Training Loss: 4.348764933072603, Training Accuracy: 4.638\n",
            "Validation Loss: 4.3305770556132, Validation Accuracy: 5.02\n",
            "[62/150]: Training Loss: 4.344545584458571, Training Accuracy: 4.744\n",
            "Validation Loss: 4.324695746103923, Validation Accuracy: 5.1\n",
            "[63/150]: Training Loss: 4.339366252605732, Training Accuracy: 4.98\n",
            "Validation Loss: 4.318894068400065, Validation Accuracy: 5.19\n",
            "[64/150]: Training Loss: 4.331437074221098, Training Accuracy: 4.942\n",
            "Validation Loss: 4.313167413075765, Validation Accuracy: 5.29\n",
            "[65/150]: Training Loss: 4.3248784358684835, Training Accuracy: 5.012\n",
            "Validation Loss: 4.307529926300049, Validation Accuracy: 5.33\n",
            "[66/150]: Training Loss: 4.320235289060152, Training Accuracy: 5.072\n",
            "Validation Loss: 4.30199400583903, Validation Accuracy: 5.41\n",
            "[67/150]: Training Loss: 4.314892292022705, Training Accuracy: 5.196\n",
            "Validation Loss: 4.296563466389974, Validation Accuracy: 5.47\n",
            "[68/150]: Training Loss: 4.31286544066209, Training Accuracy: 5.19\n",
            "Validation Loss: 4.291228771209717, Validation Accuracy: 5.53\n",
            "[69/150]: Training Loss: 4.307050521557148, Training Accuracy: 5.36\n",
            "Validation Loss: 4.28596830368042, Validation Accuracy: 5.69\n",
            "[70/150]: Training Loss: 4.300308631016658, Training Accuracy: 5.518\n",
            "Validation Loss: 4.280826250712077, Validation Accuracy: 5.78\n",
            "[71/150]: Training Loss: 4.2977202122028055, Training Accuracy: 5.45\n",
            "Validation Loss: 4.275750954945882, Validation Accuracy: 5.88\n",
            "[72/150]: Training Loss: 4.292005355541523, Training Accuracy: 5.486\n",
            "Validation Loss: 4.270729700724284, Validation Accuracy: 5.95\n",
            "[73/150]: Training Loss: 4.288644974048321, Training Accuracy: 5.562\n",
            "Validation Loss: 4.2658162117004395, Validation Accuracy: 6.03\n",
            "[74/150]: Training Loss: 4.280538925757775, Training Accuracy: 5.654\n",
            "Validation Loss: 4.261026541392009, Validation Accuracy: 6.1\n",
            "[75/150]: Training Loss: 4.276312791384184, Training Accuracy: 5.748\n",
            "Validation Loss: 4.2563527425130205, Validation Accuracy: 6.16\n",
            "[76/150]: Training Loss: 4.2695871866666355, Training Accuracy: 5.91\n",
            "Validation Loss: 4.251786231994629, Validation Accuracy: 6.13\n",
            "[77/150]: Training Loss: 4.2690965212308445, Training Accuracy: 5.724\n",
            "Validation Loss: 4.2473251024882, Validation Accuracy: 6.22\n",
            "[78/150]: Training Loss: 4.262235421400804, Training Accuracy: 5.894\n",
            "Validation Loss: 4.2429154713948565, Validation Accuracy: 6.26\n",
            "[79/150]: Training Loss: 4.261227387648362, Training Accuracy: 5.892\n",
            "Validation Loss: 4.2385679880778, Validation Accuracy: 6.25\n",
            "[80/150]: Training Loss: 4.255170712104211, Training Accuracy: 6.01\n",
            "Validation Loss: 4.234347820281982, Validation Accuracy: 6.45\n",
            "[81/150]: Training Loss: 4.25046627338116, Training Accuracy: 5.976\n",
            "Validation Loss: 4.23021936416626, Validation Accuracy: 6.56\n",
            "[82/150]: Training Loss: 4.245743384728065, Training Accuracy: 6.192\n",
            "Validation Loss: 4.226193745930989, Validation Accuracy: 6.64\n",
            "[83/150]: Training Loss: 4.2433240963862495, Training Accuracy: 6.402\n",
            "Validation Loss: 4.222262541453044, Validation Accuracy: 6.75\n",
            "[84/150]: Training Loss: 4.238566288581262, Training Accuracy: 6.25\n",
            "Validation Loss: 4.218420664469401, Validation Accuracy: 6.87\n",
            "[85/150]: Training Loss: 4.23705295416025, Training Accuracy: 6.298\n",
            "Validation Loss: 4.214700380961101, Validation Accuracy: 6.96\n",
            "[86/150]: Training Loss: 4.233789333930383, Training Accuracy: 6.332\n",
            "Validation Loss: 4.211048285166423, Validation Accuracy: 6.99\n",
            "[87/150]: Training Loss: 4.226792628948505, Training Accuracy: 6.504\n",
            "Validation Loss: 4.207512537638347, Validation Accuracy: 7.11\n",
            "[88/150]: Training Loss: 4.224984792562632, Training Accuracy: 6.692\n",
            "Validation Loss: 4.204057852427165, Validation Accuracy: 7.13\n",
            "[89/150]: Training Loss: 4.222308562352107, Training Accuracy: 6.614\n",
            "Validation Loss: 4.200714747111003, Validation Accuracy: 7.23\n",
            "[90/150]: Training Loss: 4.219607353210449, Training Accuracy: 6.606\n",
            "Validation Loss: 4.197493712107341, Validation Accuracy: 7.23\n",
            "[91/150]: Training Loss: 4.218163453615629, Training Accuracy: 6.696\n",
            "Validation Loss: 4.194405237833659, Validation Accuracy: 7.25\n",
            "[92/150]: Training Loss: 4.212328250591572, Training Accuracy: 6.732\n",
            "Validation Loss: 4.1913994153340655, Validation Accuracy: 7.24\n",
            "[93/150]: Training Loss: 4.211502331953782, Training Accuracy: 6.548\n",
            "Validation Loss: 4.188492298126221, Validation Accuracy: 7.23\n",
            "[94/150]: Training Loss: 4.205521290118877, Training Accuracy: 6.676\n",
            "Validation Loss: 4.18562110265096, Validation Accuracy: 7.29\n",
            "[95/150]: Training Loss: 4.208450720860408, Training Accuracy: 6.746\n",
            "Validation Loss: 4.182815869649251, Validation Accuracy: 7.33\n",
            "[96/150]: Training Loss: 4.203882180727446, Training Accuracy: 6.828\n",
            "Validation Loss: 4.180105050404866, Validation Accuracy: 7.38\n",
            "[97/150]: Training Loss: 4.19746967462393, Training Accuracy: 6.792\n",
            "Validation Loss: 4.177504062652588, Validation Accuracy: 7.4\n",
            "[98/150]: Training Loss: 4.199424340174748, Training Accuracy: 6.89\n",
            "Validation Loss: 4.175002257029216, Validation Accuracy: 7.47\n",
            "[99/150]: Training Loss: 4.197685498457688, Training Accuracy: 6.842\n",
            "Validation Loss: 4.172624429066976, Validation Accuracy: 7.46\n",
            "[100/150]: Training Loss: 4.195333334115835, Training Accuracy: 6.81\n",
            "Validation Loss: 4.170355002085368, Validation Accuracy: 7.46\n",
            "[101/150]: Training Loss: 4.194277323209322, Training Accuracy: 6.998\n",
            "Validation Loss: 4.168156623840332, Validation Accuracy: 7.49\n",
            "[102/150]: Training Loss: 4.188217676602877, Training Accuracy: 6.952\n",
            "Validation Loss: 4.166011015574138, Validation Accuracy: 7.52\n",
            "[103/150]: Training Loss: 4.187051039475661, Training Accuracy: 7.06\n",
            "Validation Loss: 4.163937091827393, Validation Accuracy: 7.55\n",
            "[104/150]: Training Loss: 4.184410645411565, Training Accuracy: 6.926\n",
            "Validation Loss: 4.16193962097168, Validation Accuracy: 7.54\n",
            "[105/150]: Training Loss: 4.1814015828646145, Training Accuracy: 6.988\n",
            "Validation Loss: 4.159995714823405, Validation Accuracy: 7.58\n",
            "[106/150]: Training Loss: 4.18063732293936, Training Accuracy: 6.944\n",
            "Validation Loss: 4.1581400235493975, Validation Accuracy: 7.6\n",
            "[107/150]: Training Loss: 4.178157843076265, Training Accuracy: 7.138\n",
            "Validation Loss: 4.156374295552571, Validation Accuracy: 7.61\n",
            "[108/150]: Training Loss: 4.178112140068641, Training Accuracy: 7.052\n",
            "Validation Loss: 4.154716332753499, Validation Accuracy: 7.59\n",
            "[109/150]: Training Loss: 4.178236667926495, Training Accuracy: 6.976\n",
            "Validation Loss: 4.153132915496826, Validation Accuracy: 7.62\n",
            "[110/150]: Training Loss: 4.17401541196383, Training Accuracy: 7.07\n",
            "Validation Loss: 4.151645024617513, Validation Accuracy: 7.64\n",
            "[111/150]: Training Loss: 4.1733221640953655, Training Accuracy: 7.136\n",
            "Validation Loss: 4.150219917297363, Validation Accuracy: 7.6\n",
            "[112/150]: Training Loss: 4.170546091519869, Training Accuracy: 7.122\n",
            "Validation Loss: 4.148863951365153, Validation Accuracy: 7.62\n",
            "[113/150]: Training Loss: 4.168694716233474, Training Accuracy: 7.04\n",
            "Validation Loss: 4.1475830078125, Validation Accuracy: 7.59\n",
            "[114/150]: Training Loss: 4.169821555797871, Training Accuracy: 7.056\n",
            "Validation Loss: 4.1463901201883955, Validation Accuracy: 7.61\n",
            "[115/150]: Training Loss: 4.170394420623779, Training Accuracy: 7.046\n",
            "Validation Loss: 4.145220915476481, Validation Accuracy: 7.61\n",
            "[116/150]: Training Loss: 4.166883615347055, Training Accuracy: 7.086\n",
            "Validation Loss: 4.144114017486572, Validation Accuracy: 7.63\n",
            "[117/150]: Training Loss: 4.164706340202918, Training Accuracy: 7.138\n",
            "Validation Loss: 4.143089135487874, Validation Accuracy: 7.61\n",
            "[118/150]: Training Loss: 4.164192566504846, Training Accuracy: 7.164\n",
            "Validation Loss: 4.142117023468018, Validation Accuracy: 7.63\n",
            "[119/150]: Training Loss: 4.165675640106201, Training Accuracy: 7.098\n",
            "Validation Loss: 4.1411943435668945, Validation Accuracy: 7.64\n",
            "[120/150]: Training Loss: 4.162878366617056, Training Accuracy: 7.266\n",
            "Validation Loss: 4.140330155690511, Validation Accuracy: 7.64\n",
            "[121/150]: Training Loss: 4.159975051879883, Training Accuracy: 7.138\n",
            "Validation Loss: 4.139519691467285, Validation Accuracy: 7.65\n",
            "[122/150]: Training Loss: 4.162002453437219, Training Accuracy: 7.26\n",
            "Validation Loss: 4.138749440511067, Validation Accuracy: 7.61\n",
            "[123/150]: Training Loss: 4.162216443281907, Training Accuracy: 7.082\n",
            "Validation Loss: 4.138024648030599, Validation Accuracy: 7.61\n",
            "[124/150]: Training Loss: 4.1589250564575195, Training Accuracy: 7.088\n",
            "Validation Loss: 4.137344042460124, Validation Accuracy: 7.64\n",
            "[125/150]: Training Loss: 4.163330261523907, Training Accuracy: 7.282\n",
            "Validation Loss: 4.136723041534424, Validation Accuracy: 7.65\n",
            "[126/150]: Training Loss: 4.160318191234882, Training Accuracy: 7.21\n",
            "Validation Loss: 4.136135896046956, Validation Accuracy: 7.65\n",
            "[127/150]: Training Loss: 4.162310380202073, Training Accuracy: 7.264\n",
            "Validation Loss: 4.135598182678223, Validation Accuracy: 7.65\n",
            "[128/150]: Training Loss: 4.158579642956074, Training Accuracy: 7.172\n",
            "Validation Loss: 4.135109901428223, Validation Accuracy: 7.65\n",
            "[129/150]: Training Loss: 4.158799134767973, Training Accuracy: 7.246\n",
            "Validation Loss: 4.134642918904622, Validation Accuracy: 7.65\n",
            "[130/150]: Training Loss: 4.158485449277437, Training Accuracy: 7.24\n",
            "Validation Loss: 4.134247620900472, Validation Accuracy: 7.65\n",
            "[131/150]: Training Loss: 4.156849054189829, Training Accuracy: 7.222\n",
            "Validation Loss: 4.133876005808513, Validation Accuracy: 7.66\n",
            "[132/150]: Training Loss: 4.157496452331543, Training Accuracy: 7.282\n",
            "Validation Loss: 4.133555889129639, Validation Accuracy: 7.68\n",
            "[133/150]: Training Loss: 4.1564310147212105, Training Accuracy: 7.208\n",
            "Validation Loss: 4.133261839548747, Validation Accuracy: 7.67\n",
            "[134/150]: Training Loss: 4.157222784482515, Training Accuracy: 7.22\n",
            "Validation Loss: 4.133004029591878, Validation Accuracy: 7.67\n",
            "[135/150]: Training Loss: 4.156645994919997, Training Accuracy: 7.352\n",
            "Validation Loss: 4.132774670918782, Validation Accuracy: 7.66\n",
            "[136/150]: Training Loss: 4.155048370361328, Training Accuracy: 7.088\n",
            "Validation Loss: 4.132576147715251, Validation Accuracy: 7.69\n",
            "[137/150]: Training Loss: 4.154701452988845, Training Accuracy: 7.31\n",
            "Validation Loss: 4.132399876912435, Validation Accuracy: 7.69\n",
            "[138/150]: Training Loss: 4.154373278984656, Training Accuracy: 7.17\n",
            "Validation Loss: 4.132242043813069, Validation Accuracy: 7.7\n",
            "[139/150]: Training Loss: 4.155180087456336, Training Accuracy: 7.204\n",
            "Validation Loss: 4.132113933563232, Validation Accuracy: 7.69\n",
            "[140/150]: Training Loss: 4.158755449148325, Training Accuracy: 7.256\n",
            "Validation Loss: 4.1319953600565595, Validation Accuracy: 7.71\n",
            "[141/150]: Training Loss: 4.157180492694561, Training Accuracy: 7.224\n",
            "Validation Loss: 4.13189697265625, Validation Accuracy: 7.72\n",
            "[142/150]: Training Loss: 4.153392058152419, Training Accuracy: 7.224\n",
            "Validation Loss: 4.131821314493815, Validation Accuracy: 7.7\n",
            "[143/150]: Training Loss: 4.155054092407227, Training Accuracy: 7.286\n",
            "Validation Loss: 4.131768226623535, Validation Accuracy: 7.71\n",
            "[144/150]: Training Loss: 4.155116411355825, Training Accuracy: 7.146\n",
            "Validation Loss: 4.1317189534505205, Validation Accuracy: 7.7\n",
            "[145/150]: Training Loss: 4.154843367063082, Training Accuracy: 7.082\n",
            "Validation Loss: 4.131686846415202, Validation Accuracy: 7.7\n",
            "[146/150]: Training Loss: 4.155945851252629, Training Accuracy: 7.196\n",
            "Validation Loss: 4.131664276123047, Validation Accuracy: 7.71\n",
            "[147/150]: Training Loss: 4.15521317261916, Training Accuracy: 7.102\n",
            "Validation Loss: 4.131643454233806, Validation Accuracy: 7.69\n",
            "[148/150]: Training Loss: 4.152954138242281, Training Accuracy: 7.28\n",
            "Validation Loss: 4.131637732187907, Validation Accuracy: 7.72\n",
            "[149/150]: Training Loss: 4.157525135920598, Training Accuracy: 7.206\n",
            "Validation Loss: 4.131630897521973, Validation Accuracy: 7.72\n",
            "[150/150]: Training Loss: 4.153202643761268, Training Accuracy: 7.258\n",
            "Validation Loss: 4.131630261739095, Validation Accuracy: 7.73\n",
            "**********************************************************************\n",
            "Test Loss: 4.131630261739095, Test Accuracy: 7.73\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>█▁▂</td></tr><tr><td>Test Loss</td><td>▁█▁</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▁▂▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇█▇█████████████</td></tr><tr><td>Train Loss</td><td>█████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>7.73</td></tr><tr><td>Test Loss</td><td>4.13163</td></tr><tr><td>Train Accuracy</td><td>7.258</td></tr><tr><td>Train Loss</td><td>4.1532</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=32768 learning_rate=0.768 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/tuoafh0s' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup/runs/tuoafh0s</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches_without_warmup</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240909_154143-tuoafh0s/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# without warmup\n",
        "num_epochs = 150\n",
        "wd = 1e-04\n",
        "batch_sizes = [512, 1024, 2048, 4096, 8192, 16384 , 32768]\n",
        "lr = 4.8/(2**5 *1e02) # approximately 15e-04\n",
        "learning_rates = [lr * batch_size / 64.0 for batch_size in batch_sizes] # linear scale-up of learning rate\n",
        "# warmup_ratio = [1/320, 1/160, 1/80, 1/40, 1/20, 1/10, 1/5]\n",
        "\n",
        "for i, batch_size in enumerate(batch_sizes):\n",
        "  print('='*50)\n",
        "  print(f'Batch size: {batch_size}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {\n",
        "    'batch_size': batch_size,\n",
        "    'learning_rate': learning_rates[i],\n",
        "    'weight_decay' : wd\n",
        "  }\n",
        "\n",
        "  if batch_size <= 4096:\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= batch_size)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LAMB(model.parameters(), learning_rate=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LAMB_Large_Batches_without_warmup',\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True\n",
        "    )\n",
        "  else:\n",
        "    accumulation_steps = batch_size // 4096\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= 4096)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LAMB(model.parameters(), learning_rate=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LAMB_Large_Batches_without_warmup',\n",
        "        accumulation_steps=accumulation_steps,\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Distributed Approaches**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hRd9SL7iRiKl"
      },
      "outputs": [],
      "source": [
        "# Initialize a model and save its initial parameters\n",
        "initial_model = LeNet5()\n",
        "initial_state_dict = initial_model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GXZaFXruZI_"
      },
      "source": [
        "### Local SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "I8hvzRbQW32s"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class LocalSGDOptimizer(Optimizer):\n",
        "    def __init__(self, global_model, lr=0.01):\n",
        "        self.global_model = global_model\n",
        "        self.lr = lr\n",
        "        params = list(global_model.parameters())\n",
        "        super(LocalSGDOptimizer, self).__init__(params, {'lr': lr})\n",
        "\n",
        "    def step(self, local_models):\n",
        "        # Get global model parameters\n",
        "        global_params = list(self.global_model.parameters())\n",
        "        \n",
        "        # Initialize deltas for each parameter, same size as global parameters\n",
        "        deltas = [torch.zeros_like(param) for param in global_params]\n",
        "        \n",
        "        # Sum up differences between global model and local models\n",
        "        for local_model in local_models:\n",
        "            local_params = list(local_model.parameters())\n",
        "            for i, param in enumerate(local_params):\n",
        "                deltas[i] += (global_params[i] - param)\n",
        "        \n",
        "        # Average the delta over the number of local models\n",
        "        num_models = len(local_models)\n",
        "        for i, delta in enumerate(deltas):\n",
        "            deltas[i] /= self.lr\n",
        "            deltas[i] /= num_models\n",
        "        \n",
        "        # Update global model parameters\n",
        "        with torch.no_grad():\n",
        "            for i, param in enumerate(global_params):\n",
        "                param.copy_(param - self.lr * deltas[i])\n",
        "        \n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def local_SGD(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, is_wandb=False):\n",
        "      \n",
        "  total_start_time = time.time()\n",
        "\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "  \n",
        "  global_optimizer = LocalSGDOptimizer(global_model, lr)\n",
        "  \n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "  \n",
        "  checkpoint = load_checkpoint('local_sgd', 64, {'k': k, 'j': j})\n",
        "  if checkpoint is not None:\n",
        "      global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      global_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      \n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn)\n",
        "      print(f'Global Update: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      \n",
        "      return None\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for loca_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    global_optimizer.step(local_models)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    for local_optimizer in local_optimizers:\n",
        "        local_optimizer.param_groups[0]['lr'] = global_optimizer.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "    for local_model in local_models:\n",
        "      local_model.load_state_dict(global_model.state_dict())\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Global Update {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "  \n",
        "  total_end_time = time.time()\n",
        "  \n",
        "  # Save checkpoint\n",
        "  save_checkpoint({\n",
        "              'epoch': num_epochs,\n",
        "              'model_state_dict': global_model.state_dict(),\n",
        "              'optimizer_state_dict': global_optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss': loss_fn\n",
        "              }, 149, 64, 'local_sgd', {'k': k, 'j': j})\n",
        " \n",
        "\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for local_SGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:4\n",
            "==================================================\n",
            "Global Update: Test Loss: 1.891312885, Test Accuracy: 56.100\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:8\n",
            "==================================================\n",
            "Global Update: Test Loss: 1.937131206, Test Accuracy: 55.350\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:16\n",
            "==================================================\n",
            "Global Update: Test Loss: 1.966259042, Test Accuracy: 55.450\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:32\n",
            "==================================================\n",
            "Global Update: Test Loss: 2.047621591, Test Accuracy: 53.230\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:64\n",
            "==================================================\n",
            "Global Update: Test Loss: 2.210712717, Test Accuracy: 42.830\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:4\n",
            "==================================================\n",
            "Worker 1, [01/04]: Training Loss: 4.491476694, Training Accuracy: 2.296\n",
            "Worker 1, [02/04]: Training Loss: 4.125288497, Training Accuracy: 6.296\n",
            "Worker 1, [03/04]: Training Loss: 3.907576191, Training Accuracy: 9.392\n",
            "Worker 1, [04/04]: Training Loss: 3.742026955, Training Accuracy: 11.960\n",
            "Time taken for training worker 1: 0:00:24.501987\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.491540872, Training Accuracy: 2.584\n",
            "Worker 2, [02/04]: Training Loss: 4.110686047, Training Accuracy: 6.664\n",
            "Worker 2, [03/04]: Training Loss: 3.879409549, Training Accuracy: 9.704\n",
            "Worker 2, [04/04]: Training Loss: 3.733214139, Training Accuracy: 11.816\n",
            "Time taken for training worker 2: 0:00:29.458311\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 4.492117276, Training Accuracy: 2.624\n",
            "Worker 3, [02/04]: Training Loss: 4.101069477, Training Accuracy: 6.880\n",
            "Worker 3, [03/04]: Training Loss: 3.863219131, Training Accuracy: 10.352\n",
            "Worker 3, [04/04]: Training Loss: 3.719093076, Training Accuracy: 12.672\n",
            "Time taken for training worker 3: 0:00:25.850792\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 4.493042156, Training Accuracy: 2.392\n",
            "Worker 4, [02/04]: Training Loss: 4.111767839, Training Accuracy: 6.448\n",
            "Worker 4, [03/04]: Training Loss: 3.876119532, Training Accuracy: 9.824\n",
            "Worker 4, [04/04]: Training Loss: 3.729633875, Training Accuracy: 12.544\n",
            "Time taken for training worker 4: 0:00:25.306886\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.010190\n",
            "Global Update 01: Test Loss: 3.656595048, Test Accuracy: 16.010\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.654426393, Training Accuracy: 13.680\n",
            "Worker 1, [02/04]: Training Loss: 3.482564069, Training Accuracy: 16.384\n",
            "Worker 1, [03/04]: Training Loss: 3.372387092, Training Accuracy: 18.368\n",
            "Worker 1, [04/04]: Training Loss: 3.257039411, Training Accuracy: 20.544\n",
            "Time taken for training worker 1: 0:00:25.636189\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.648854978, Training Accuracy: 13.576\n",
            "Worker 2, [02/04]: Training Loss: 3.505996925, Training Accuracy: 15.160\n",
            "Worker 2, [03/04]: Training Loss: 3.383527015, Training Accuracy: 17.640\n",
            "Worker 2, [04/04]: Training Loss: 3.273007244, Training Accuracy: 19.744\n",
            "Time taken for training worker 2: 0:00:25.239183\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.648658929, Training Accuracy: 13.704\n",
            "Worker 3, [02/04]: Training Loss: 3.500264235, Training Accuracy: 15.920\n",
            "Worker 3, [03/04]: Training Loss: 3.372660774, Training Accuracy: 18.024\n",
            "Worker 3, [04/04]: Training Loss: 3.270497595, Training Accuracy: 19.712\n",
            "Time taken for training worker 3: 0:00:25.749987\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.658244073, Training Accuracy: 13.312\n",
            "Worker 4, [02/04]: Training Loss: 3.499930134, Training Accuracy: 16.008\n",
            "Worker 4, [03/04]: Training Loss: 3.378343321, Training Accuracy: 17.984\n",
            "Worker 4, [04/04]: Training Loss: 3.282330329, Training Accuracy: 19.144\n",
            "Time taken for training worker 4: 0:00:25.222200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003056\n",
            "Global Update 02: Test Loss: 3.064773923, Test Accuracy: 24.960\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.228967271, Training Accuracy: 20.840\n",
            "Worker 1, [02/04]: Training Loss: 3.143452885, Training Accuracy: 22.344\n",
            "Worker 1, [03/04]: Training Loss: 3.030343118, Training Accuracy: 24.728\n",
            "Worker 1, [04/04]: Training Loss: 2.963983370, Training Accuracy: 25.136\n",
            "Time taken for training worker 1: 0:00:26.696938\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.247215356, Training Accuracy: 20.128\n",
            "Worker 2, [02/04]: Training Loss: 3.143344747, Training Accuracy: 22.304\n",
            "Worker 2, [03/04]: Training Loss: 3.056005784, Training Accuracy: 23.512\n",
            "Worker 2, [04/04]: Training Loss: 2.976820072, Training Accuracy: 25.232\n",
            "Time taken for training worker 2: 0:00:27.385323\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.235601605, Training Accuracy: 20.456\n",
            "Worker 3, [02/04]: Training Loss: 3.137501427, Training Accuracy: 22.296\n",
            "Worker 3, [03/04]: Training Loss: 3.044976654, Training Accuracy: 24.160\n",
            "Worker 3, [04/04]: Training Loss: 2.945152981, Training Accuracy: 25.400\n",
            "Time taken for training worker 3: 0:00:26.830809\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.228892796, Training Accuracy: 21.096\n",
            "Worker 4, [02/04]: Training Loss: 3.153859919, Training Accuracy: 21.936\n",
            "Worker 4, [03/04]: Training Loss: 3.045799557, Training Accuracy: 24.376\n",
            "Worker 4, [04/04]: Training Loss: 2.955936873, Training Accuracy: 25.152\n",
            "Time taken for training worker 4: 0:00:25.707696\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003036\n",
            "Global Update 03: Test Loss: 2.768369644, Test Accuracy: 30.650\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.961480715, Training Accuracy: 26.432\n",
            "Worker 1, [02/04]: Training Loss: 2.869402074, Training Accuracy: 27.592\n",
            "Worker 1, [03/04]: Training Loss: 2.803671043, Training Accuracy: 28.408\n",
            "Worker 1, [04/04]: Training Loss: 2.696911050, Training Accuracy: 30.752\n",
            "Time taken for training worker 1: 0:00:26.547226\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.979001498, Training Accuracy: 25.672\n",
            "Worker 2, [02/04]: Training Loss: 2.859465407, Training Accuracy: 27.672\n",
            "Worker 2, [03/04]: Training Loss: 2.790336120, Training Accuracy: 28.632\n",
            "Worker 2, [04/04]: Training Loss: 2.721917656, Training Accuracy: 30.232\n",
            "Time taken for training worker 2: 0:00:25.640905\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.977099269, Training Accuracy: 25.392\n",
            "Worker 3, [02/04]: Training Loss: 2.862050822, Training Accuracy: 27.512\n",
            "Worker 3, [03/04]: Training Loss: 2.798217707, Training Accuracy: 28.752\n",
            "Worker 3, [04/04]: Training Loss: 2.709290525, Training Accuracy: 30.560\n",
            "Time taken for training worker 3: 0:00:27.761539\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.957667219, Training Accuracy: 25.976\n",
            "Worker 4, [02/04]: Training Loss: 2.877612412, Training Accuracy: 26.976\n",
            "Worker 4, [03/04]: Training Loss: 2.775044894, Training Accuracy: 28.824\n",
            "Worker 4, [04/04]: Training Loss: 2.712134800, Training Accuracy: 30.224\n",
            "Time taken for training worker 4: 0:00:27.519029\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002804\n",
            "Global Update 04: Test Loss: 2.528462304, Test Accuracy: 35.410\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.757309738, Training Accuracy: 30.528\n",
            "Worker 1, [02/04]: Training Loss: 2.663105148, Training Accuracy: 31.488\n",
            "Worker 1, [03/04]: Training Loss: 2.577983558, Training Accuracy: 33.504\n",
            "Worker 1, [04/04]: Training Loss: 2.538163133, Training Accuracy: 33.960\n",
            "Time taken for training worker 1: 0:00:27.999713\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.739949988, Training Accuracy: 30.168\n",
            "Worker 2, [02/04]: Training Loss: 2.662568741, Training Accuracy: 31.544\n",
            "Worker 2, [03/04]: Training Loss: 2.574878356, Training Accuracy: 33.392\n",
            "Worker 2, [04/04]: Training Loss: 2.511097689, Training Accuracy: 34.328\n",
            "Time taken for training worker 2: 0:00:26.716516\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.724623943, Training Accuracy: 30.744\n",
            "Worker 3, [02/04]: Training Loss: 2.650886134, Training Accuracy: 31.952\n",
            "Worker 3, [03/04]: Training Loss: 2.565375996, Training Accuracy: 33.392\n",
            "Worker 3, [04/04]: Training Loss: 2.516432384, Training Accuracy: 34.608\n",
            "Time taken for training worker 3: 0:00:26.849522\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.743120685, Training Accuracy: 30.272\n",
            "Worker 4, [02/04]: Training Loss: 2.662766877, Training Accuracy: 32.200\n",
            "Worker 4, [03/04]: Training Loss: 2.587024528, Training Accuracy: 32.848\n",
            "Worker 4, [04/04]: Training Loss: 2.537474501, Training Accuracy: 33.912\n",
            "Time taken for training worker 4: 0:00:28.299742\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003298\n",
            "Global Update 05: Test Loss: 2.350132209, Test Accuracy: 39.590\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.569522084, Training Accuracy: 34.040\n",
            "Worker 1, [02/04]: Training Loss: 2.517413969, Training Accuracy: 34.568\n",
            "Worker 1, [03/04]: Training Loss: 2.447932454, Training Accuracy: 35.728\n",
            "Worker 1, [04/04]: Training Loss: 2.374307487, Training Accuracy: 37.584\n",
            "Time taken for training worker 1: 0:00:27.486727\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.597147187, Training Accuracy: 32.528\n",
            "Worker 2, [02/04]: Training Loss: 2.489443136, Training Accuracy: 34.952\n",
            "Worker 2, [03/04]: Training Loss: 2.431623327, Training Accuracy: 36.136\n",
            "Worker 2, [04/04]: Training Loss: 2.342562204, Training Accuracy: 37.480\n",
            "Time taken for training worker 2: 0:00:27.554686\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.566657710, Training Accuracy: 33.592\n",
            "Worker 3, [02/04]: Training Loss: 2.464149875, Training Accuracy: 35.720\n",
            "Worker 3, [03/04]: Training Loss: 2.422861743, Training Accuracy: 36.216\n",
            "Worker 3, [04/04]: Training Loss: 2.366597318, Training Accuracy: 37.640\n",
            "Time taken for training worker 3: 0:00:27.800708\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.589973831, Training Accuracy: 33.304\n",
            "Worker 4, [02/04]: Training Loss: 2.504515055, Training Accuracy: 34.792\n",
            "Worker 4, [03/04]: Training Loss: 2.451220890, Training Accuracy: 35.768\n",
            "Worker 4, [04/04]: Training Loss: 2.386508978, Training Accuracy: 36.784\n",
            "Time taken for training worker 4: 0:00:28.951633\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002965\n",
            "Global Update 06: Test Loss: 2.273132645, Test Accuracy: 40.740\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.446141733, Training Accuracy: 36.552\n",
            "Worker 1, [02/04]: Training Loss: 2.375989444, Training Accuracy: 38.536\n",
            "Worker 1, [03/04]: Training Loss: 2.307359518, Training Accuracy: 38.800\n",
            "Worker 1, [04/04]: Training Loss: 2.247018214, Training Accuracy: 40.392\n",
            "Time taken for training worker 1: 0:00:27.700233\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.445323364, Training Accuracy: 36.176\n",
            "Worker 2, [02/04]: Training Loss: 2.361057591, Training Accuracy: 37.904\n",
            "Worker 2, [03/04]: Training Loss: 2.290261490, Training Accuracy: 38.904\n",
            "Worker 2, [04/04]: Training Loss: 2.252068777, Training Accuracy: 39.744\n",
            "Time taken for training worker 2: 0:00:26.857580\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.450114576, Training Accuracy: 36.072\n",
            "Worker 3, [02/04]: Training Loss: 2.350948157, Training Accuracy: 38.096\n",
            "Worker 3, [03/04]: Training Loss: 2.281668910, Training Accuracy: 39.408\n",
            "Worker 3, [04/04]: Training Loss: 2.242882850, Training Accuracy: 40.288\n",
            "Time taken for training worker 3: 0:00:27.522329\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.474159295, Training Accuracy: 35.216\n",
            "Worker 4, [02/04]: Training Loss: 2.389842083, Training Accuracy: 37.280\n",
            "Worker 4, [03/04]: Training Loss: 2.300049182, Training Accuracy: 38.824\n",
            "Worker 4, [04/04]: Training Loss: 2.270686806, Training Accuracy: 39.136\n",
            "Time taken for training worker 4: 0:00:25.623578\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002785\n",
            "Global Update 07: Test Loss: 2.171375395, Test Accuracy: 43.150\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.354576493, Training Accuracy: 38.712\n",
            "Worker 1, [02/04]: Training Loss: 2.282627779, Training Accuracy: 40.176\n",
            "Worker 1, [03/04]: Training Loss: 2.205986551, Training Accuracy: 40.880\n",
            "Worker 1, [04/04]: Training Loss: 2.144752706, Training Accuracy: 42.736\n",
            "Time taken for training worker 1: 0:00:25.962316\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.334620939, Training Accuracy: 38.432\n",
            "Worker 2, [02/04]: Training Loss: 2.254865294, Training Accuracy: 40.344\n",
            "Worker 2, [03/04]: Training Loss: 2.167349983, Training Accuracy: 42.064\n",
            "Worker 2, [04/04]: Training Loss: 2.115518424, Training Accuracy: 42.760\n",
            "Time taken for training worker 2: 0:00:26.478583\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.330136861, Training Accuracy: 38.672\n",
            "Worker 3, [02/04]: Training Loss: 2.243650362, Training Accuracy: 40.496\n",
            "Worker 3, [03/04]: Training Loss: 2.177347064, Training Accuracy: 41.576\n",
            "Worker 3, [04/04]: Training Loss: 2.126238422, Training Accuracy: 42.656\n",
            "Time taken for training worker 3: 0:00:25.830125\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.337353688, Training Accuracy: 38.672\n",
            "Worker 4, [02/04]: Training Loss: 2.269831736, Training Accuracy: 39.544\n",
            "Worker 4, [03/04]: Training Loss: 2.196145383, Training Accuracy: 41.088\n",
            "Worker 4, [04/04]: Training Loss: 2.153565533, Training Accuracy: 42.288\n",
            "Time taken for training worker 4: 0:00:27.132456\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003228\n",
            "Global Update 08: Test Loss: 2.108470377, Test Accuracy: 44.250\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.261054763, Training Accuracy: 40.360\n",
            "Worker 1, [02/04]: Training Loss: 2.149515221, Training Accuracy: 42.608\n",
            "Worker 1, [03/04]: Training Loss: 2.092739312, Training Accuracy: 43.320\n",
            "Worker 1, [04/04]: Training Loss: 2.025439453, Training Accuracy: 45.664\n",
            "Time taken for training worker 1: 0:00:26.880918\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.257188023, Training Accuracy: 40.848\n",
            "Worker 2, [02/04]: Training Loss: 2.154126094, Training Accuracy: 41.992\n",
            "Worker 2, [03/04]: Training Loss: 2.093184221, Training Accuracy: 43.536\n",
            "Worker 2, [04/04]: Training Loss: 2.028062778, Training Accuracy: 44.736\n",
            "Time taken for training worker 2: 0:00:27.128017\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.231175808, Training Accuracy: 40.616\n",
            "Worker 3, [02/04]: Training Loss: 2.146155775, Training Accuracy: 42.336\n",
            "Worker 3, [03/04]: Training Loss: 2.083326528, Training Accuracy: 43.912\n",
            "Worker 3, [04/04]: Training Loss: 2.023187115, Training Accuracy: 45.256\n",
            "Time taken for training worker 3: 0:00:27.246247\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.275291043, Training Accuracy: 40.000\n",
            "Worker 4, [02/04]: Training Loss: 2.182380280, Training Accuracy: 41.792\n",
            "Worker 4, [03/04]: Training Loss: 2.110348940, Training Accuracy: 43.048\n",
            "Worker 4, [04/04]: Training Loss: 2.040706392, Training Accuracy: 44.408\n",
            "Time taken for training worker 4: 0:00:26.629338\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002883\n",
            "Global Update 09: Test Loss: 2.054195866, Test Accuracy: 45.940\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.154534235, Training Accuracy: 42.888\n",
            "Worker 1, [02/04]: Training Loss: 2.074962792, Training Accuracy: 44.464\n",
            "Worker 1, [03/04]: Training Loss: 2.008184826, Training Accuracy: 45.776\n",
            "Worker 1, [04/04]: Training Loss: 1.937866959, Training Accuracy: 46.744\n",
            "Time taken for training worker 1: 0:00:26.164354\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.156593255, Training Accuracy: 42.280\n",
            "Worker 2, [02/04]: Training Loss: 2.080859419, Training Accuracy: 44.096\n",
            "Worker 2, [03/04]: Training Loss: 1.997519005, Training Accuracy: 45.488\n",
            "Worker 2, [04/04]: Training Loss: 1.960751388, Training Accuracy: 46.568\n",
            "Time taken for training worker 2: 0:00:26.961021\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.155957051, Training Accuracy: 42.760\n",
            "Worker 3, [02/04]: Training Loss: 2.057599139, Training Accuracy: 44.304\n",
            "Worker 3, [03/04]: Training Loss: 1.985908922, Training Accuracy: 46.184\n",
            "Worker 3, [04/04]: Training Loss: 1.934328917, Training Accuracy: 47.264\n",
            "Time taken for training worker 3: 0:00:26.898123\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.163998227, Training Accuracy: 42.928\n",
            "Worker 4, [02/04]: Training Loss: 2.071512092, Training Accuracy: 44.192\n",
            "Worker 4, [03/04]: Training Loss: 2.022786990, Training Accuracy: 44.920\n",
            "Worker 4, [04/04]: Training Loss: 1.952108633, Training Accuracy: 46.528\n",
            "Time taken for training worker 4: 0:00:26.601220\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002796\n",
            "Global Update 10: Test Loss: 1.993157648, Test Accuracy: 47.400\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.070794327, Training Accuracy: 44.496\n",
            "Worker 1, [02/04]: Training Loss: 1.997763547, Training Accuracy: 45.864\n",
            "Worker 1, [03/04]: Training Loss: 1.927076392, Training Accuracy: 47.280\n",
            "Worker 1, [04/04]: Training Loss: 1.868441657, Training Accuracy: 48.792\n",
            "Time taken for training worker 1: 0:00:26.267744\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.084633245, Training Accuracy: 44.416\n",
            "Worker 2, [02/04]: Training Loss: 1.993940947, Training Accuracy: 45.560\n",
            "Worker 2, [03/04]: Training Loss: 1.928284020, Training Accuracy: 46.880\n",
            "Worker 2, [04/04]: Training Loss: 1.886010405, Training Accuracy: 47.880\n",
            "Time taken for training worker 2: 0:00:26.102671\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.070965664, Training Accuracy: 44.824\n",
            "Worker 3, [02/04]: Training Loss: 1.972889509, Training Accuracy: 46.512\n",
            "Worker 3, [03/04]: Training Loss: 1.902283755, Training Accuracy: 47.536\n",
            "Worker 3, [04/04]: Training Loss: 1.866158195, Training Accuracy: 49.200\n",
            "Time taken for training worker 3: 0:00:26.211900\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.100888284, Training Accuracy: 43.904\n",
            "Worker 4, [02/04]: Training Loss: 1.988132637, Training Accuracy: 46.152\n",
            "Worker 4, [03/04]: Training Loss: 1.934973132, Training Accuracy: 46.584\n",
            "Worker 4, [04/04]: Training Loss: 1.870185280, Training Accuracy: 48.536\n",
            "Time taken for training worker 4: 0:00:28.030993\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003407\n",
            "Global Update 11: Test Loss: 1.966244446, Test Accuracy: 47.990\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.991408815, Training Accuracy: 47.032\n",
            "Worker 1, [02/04]: Training Loss: 1.933233895, Training Accuracy: 47.432\n",
            "Worker 1, [03/04]: Training Loss: 1.856256239, Training Accuracy: 49.304\n",
            "Worker 1, [04/04]: Training Loss: 1.784707812, Training Accuracy: 50.784\n",
            "Time taken for training worker 1: 0:00:26.509625\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.992192553, Training Accuracy: 45.968\n",
            "Worker 2, [02/04]: Training Loss: 1.915097012, Training Accuracy: 47.888\n",
            "Worker 2, [03/04]: Training Loss: 1.862712683, Training Accuracy: 48.416\n",
            "Worker 2, [04/04]: Training Loss: 1.768727480, Training Accuracy: 50.728\n",
            "Time taken for training worker 2: 0:00:26.922671\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.995870974, Training Accuracy: 46.624\n",
            "Worker 3, [02/04]: Training Loss: 1.913072151, Training Accuracy: 47.888\n",
            "Worker 3, [03/04]: Training Loss: 1.849726190, Training Accuracy: 49.432\n",
            "Worker 3, [04/04]: Training Loss: 1.765513497, Training Accuracy: 50.624\n",
            "Time taken for training worker 3: 0:00:26.596874\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.001257405, Training Accuracy: 45.960\n",
            "Worker 4, [02/04]: Training Loss: 1.922772486, Training Accuracy: 47.728\n",
            "Worker 4, [03/04]: Training Loss: 1.873698674, Training Accuracy: 48.552\n",
            "Worker 4, [04/04]: Training Loss: 1.790062902, Training Accuracy: 50.320\n",
            "Time taken for training worker 4: 0:00:26.485674\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002859\n",
            "Global Update 12: Test Loss: 1.925141104, Test Accuracy: 48.680\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.925111890, Training Accuracy: 47.488\n",
            "Worker 1, [02/04]: Training Loss: 1.846805269, Training Accuracy: 50.104\n",
            "Worker 1, [03/04]: Training Loss: 1.774105235, Training Accuracy: 50.888\n",
            "Worker 1, [04/04]: Training Loss: 1.736102831, Training Accuracy: 52.280\n",
            "Time taken for training worker 1: 0:00:26.159994\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.930042986, Training Accuracy: 47.248\n",
            "Worker 2, [02/04]: Training Loss: 1.835575423, Training Accuracy: 49.520\n",
            "Worker 2, [03/04]: Training Loss: 1.765081320, Training Accuracy: 50.824\n",
            "Worker 2, [04/04]: Training Loss: 1.723602898, Training Accuracy: 51.032\n",
            "Time taken for training worker 2: 0:00:25.257935\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.936840165, Training Accuracy: 47.280\n",
            "Worker 3, [02/04]: Training Loss: 1.819268075, Training Accuracy: 50.096\n",
            "Worker 3, [03/04]: Training Loss: 1.770088628, Training Accuracy: 50.944\n",
            "Worker 3, [04/04]: Training Loss: 1.697476164, Training Accuracy: 52.504\n",
            "Time taken for training worker 3: 0:00:27.385929\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.965235429, Training Accuracy: 46.888\n",
            "Worker 4, [02/04]: Training Loss: 1.843149416, Training Accuracy: 49.440\n",
            "Worker 4, [03/04]: Training Loss: 1.762505764, Training Accuracy: 51.032\n",
            "Worker 4, [04/04]: Training Loss: 1.733216095, Training Accuracy: 52.192\n",
            "Time taken for training worker 4: 0:00:26.552576\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002999\n",
            "Global Update 13: Test Loss: 1.893112352, Test Accuracy: 49.690\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.880031848, Training Accuracy: 48.696\n",
            "Worker 1, [02/04]: Training Loss: 1.766161486, Training Accuracy: 51.080\n",
            "Worker 1, [03/04]: Training Loss: 1.721510233, Training Accuracy: 52.592\n",
            "Worker 1, [04/04]: Training Loss: 1.649594848, Training Accuracy: 53.904\n",
            "Time taken for training worker 1: 0:00:26.653260\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.851813588, Training Accuracy: 48.712\n",
            "Worker 2, [02/04]: Training Loss: 1.767422565, Training Accuracy: 50.920\n",
            "Worker 2, [03/04]: Training Loss: 1.692296341, Training Accuracy: 52.896\n",
            "Worker 2, [04/04]: Training Loss: 1.640360317, Training Accuracy: 54.088\n",
            "Time taken for training worker 2: 0:00:26.460741\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.871122081, Training Accuracy: 49.104\n",
            "Worker 3, [02/04]: Training Loss: 1.766691611, Training Accuracy: 51.064\n",
            "Worker 3, [03/04]: Training Loss: 1.699636018, Training Accuracy: 52.496\n",
            "Worker 3, [04/04]: Training Loss: 1.623840944, Training Accuracy: 54.792\n",
            "Time taken for training worker 3: 0:00:27.578848\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.867074908, Training Accuracy: 48.640\n",
            "Worker 4, [02/04]: Training Loss: 1.788077477, Training Accuracy: 50.280\n",
            "Worker 4, [03/04]: Training Loss: 1.696230486, Training Accuracy: 52.536\n",
            "Worker 4, [04/04]: Training Loss: 1.662376384, Training Accuracy: 53.984\n",
            "Time taken for training worker 4: 0:00:25.787839\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003091\n",
            "Global Update 14: Test Loss: 1.906971048, Test Accuracy: 49.850\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.800721137, Training Accuracy: 51.032\n",
            "Worker 1, [02/04]: Training Loss: 1.707449365, Training Accuracy: 52.720\n",
            "Worker 1, [03/04]: Training Loss: 1.627407980, Training Accuracy: 54.576\n",
            "Worker 1, [04/04]: Training Loss: 1.584895387, Training Accuracy: 55.528\n",
            "Time taken for training worker 1: 0:00:26.188456\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.796231281, Training Accuracy: 50.552\n",
            "Worker 2, [02/04]: Training Loss: 1.731365442, Training Accuracy: 51.816\n",
            "Worker 2, [03/04]: Training Loss: 1.645537657, Training Accuracy: 53.768\n",
            "Worker 2, [04/04]: Training Loss: 1.587456015, Training Accuracy: 55.264\n",
            "Time taken for training worker 2: 0:00:26.997605\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.794592723, Training Accuracy: 50.872\n",
            "Worker 3, [02/04]: Training Loss: 1.697071690, Training Accuracy: 52.768\n",
            "Worker 3, [03/04]: Training Loss: 1.625705981, Training Accuracy: 54.368\n",
            "Worker 3, [04/04]: Training Loss: 1.579482592, Training Accuracy: 55.504\n",
            "Time taken for training worker 3: 0:00:27.409389\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.815740510, Training Accuracy: 50.168\n",
            "Worker 4, [02/04]: Training Loss: 1.717091673, Training Accuracy: 52.432\n",
            "Worker 4, [03/04]: Training Loss: 1.667770997, Training Accuracy: 53.672\n",
            "Worker 4, [04/04]: Training Loss: 1.568932088, Training Accuracy: 55.704\n",
            "Time taken for training worker 4: 0:00:26.414348\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002880\n",
            "Global Update 15: Test Loss: 1.862969218, Test Accuracy: 50.760\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.744463573, Training Accuracy: 51.728\n",
            "Worker 1, [02/04]: Training Loss: 1.662401021, Training Accuracy: 53.256\n",
            "Worker 1, [03/04]: Training Loss: 1.587304606, Training Accuracy: 55.864\n",
            "Worker 1, [04/04]: Training Loss: 1.520211010, Training Accuracy: 57.032\n",
            "Time taken for training worker 1: 0:00:26.691851\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.744472015, Training Accuracy: 51.992\n",
            "Worker 2, [02/04]: Training Loss: 1.636174356, Training Accuracy: 54.056\n",
            "Worker 2, [03/04]: Training Loss: 1.567844720, Training Accuracy: 56.144\n",
            "Worker 2, [04/04]: Training Loss: 1.491112093, Training Accuracy: 57.264\n",
            "Time taken for training worker 2: 0:00:26.239544\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.742536109, Training Accuracy: 51.392\n",
            "Worker 3, [02/04]: Training Loss: 1.657361780, Training Accuracy: 53.752\n",
            "Worker 3, [03/04]: Training Loss: 1.571276350, Training Accuracy: 55.312\n",
            "Worker 3, [04/04]: Training Loss: 1.509487785, Training Accuracy: 57.344\n",
            "Time taken for training worker 3: 0:00:26.757379\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.761149959, Training Accuracy: 50.920\n",
            "Worker 4, [02/04]: Training Loss: 1.644271949, Training Accuracy: 54.288\n",
            "Worker 4, [03/04]: Training Loss: 1.586931988, Training Accuracy: 54.984\n",
            "Worker 4, [04/04]: Training Loss: 1.540033238, Training Accuracy: 56.464\n",
            "Time taken for training worker 4: 0:00:26.570509\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002912\n",
            "Global Update 16: Test Loss: 1.841965328, Test Accuracy: 52.050\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.694768760, Training Accuracy: 52.800\n",
            "Worker 1, [02/04]: Training Loss: 1.595459765, Training Accuracy: 55.088\n",
            "Worker 1, [03/04]: Training Loss: 1.533472208, Training Accuracy: 56.800\n",
            "Worker 1, [04/04]: Training Loss: 1.453466260, Training Accuracy: 58.760\n",
            "Time taken for training worker 1: 0:00:25.774504\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.659496163, Training Accuracy: 53.128\n",
            "Worker 2, [02/04]: Training Loss: 1.577498667, Training Accuracy: 55.424\n",
            "Worker 2, [03/04]: Training Loss: 1.499718584, Training Accuracy: 57.320\n",
            "Worker 2, [04/04]: Training Loss: 1.439736648, Training Accuracy: 58.784\n",
            "Time taken for training worker 2: 0:00:27.076025\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.694539348, Training Accuracy: 52.512\n",
            "Worker 3, [02/04]: Training Loss: 1.570663615, Training Accuracy: 56.096\n",
            "Worker 3, [03/04]: Training Loss: 1.510449096, Training Accuracy: 57.328\n",
            "Worker 3, [04/04]: Training Loss: 1.450213296, Training Accuracy: 58.952\n",
            "Time taken for training worker 3: 0:00:26.172060\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.713270214, Training Accuracy: 52.584\n",
            "Worker 4, [02/04]: Training Loss: 1.598593101, Training Accuracy: 54.584\n",
            "Worker 4, [03/04]: Training Loss: 1.529849787, Training Accuracy: 56.800\n",
            "Worker 4, [04/04]: Training Loss: 1.469161787, Training Accuracy: 58.328\n",
            "Time taken for training worker 4: 0:00:26.439974\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003124\n",
            "Global Update 17: Test Loss: 1.835032080, Test Accuracy: 51.490\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.621743358, Training Accuracy: 55.256\n",
            "Worker 1, [02/04]: Training Loss: 1.532407149, Training Accuracy: 57.024\n",
            "Worker 1, [03/04]: Training Loss: 1.461450810, Training Accuracy: 58.504\n",
            "Worker 1, [04/04]: Training Loss: 1.384960909, Training Accuracy: 60.512\n",
            "Time taken for training worker 1: 0:00:26.653394\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.642647324, Training Accuracy: 54.080\n",
            "Worker 2, [02/04]: Training Loss: 1.527981687, Training Accuracy: 56.568\n",
            "Worker 2, [03/04]: Training Loss: 1.447970838, Training Accuracy: 58.472\n",
            "Worker 2, [04/04]: Training Loss: 1.383278826, Training Accuracy: 60.464\n",
            "Time taken for training worker 2: 0:00:27.039899\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.620939825, Training Accuracy: 55.152\n",
            "Worker 3, [02/04]: Training Loss: 1.527509500, Training Accuracy: 56.744\n",
            "Worker 3, [03/04]: Training Loss: 1.439974138, Training Accuracy: 58.912\n",
            "Worker 3, [04/04]: Training Loss: 1.407809904, Training Accuracy: 59.280\n",
            "Time taken for training worker 3: 0:00:24.361792\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.647810495, Training Accuracy: 54.448\n",
            "Worker 4, [02/04]: Training Loss: 1.542609178, Training Accuracy: 56.816\n",
            "Worker 4, [03/04]: Training Loss: 1.489572021, Training Accuracy: 57.880\n",
            "Worker 4, [04/04]: Training Loss: 1.421500906, Training Accuracy: 59.352\n",
            "Time taken for training worker 4: 0:00:25.515277\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002721\n",
            "Global Update 18: Test Loss: 1.831855042, Test Accuracy: 52.030\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.579383160, Training Accuracy: 55.744\n",
            "Worker 1, [02/04]: Training Loss: 1.481725867, Training Accuracy: 58.768\n",
            "Worker 1, [03/04]: Training Loss: 1.396903255, Training Accuracy: 60.032\n",
            "Worker 1, [04/04]: Training Loss: 1.356440743, Training Accuracy: 61.400\n",
            "Time taken for training worker 1: 0:00:27.440557\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.588956034, Training Accuracy: 55.240\n",
            "Worker 2, [02/04]: Training Loss: 1.454482664, Training Accuracy: 58.488\n",
            "Worker 2, [03/04]: Training Loss: 1.393198250, Training Accuracy: 60.168\n",
            "Worker 2, [04/04]: Training Loss: 1.332683719, Training Accuracy: 61.480\n",
            "Time taken for training worker 2: 0:00:28.216655\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.583691537, Training Accuracy: 55.656\n",
            "Worker 3, [02/04]: Training Loss: 1.450868910, Training Accuracy: 58.720\n",
            "Worker 3, [03/04]: Training Loss: 1.376213041, Training Accuracy: 60.560\n",
            "Worker 3, [04/04]: Training Loss: 1.342070946, Training Accuracy: 61.200\n",
            "Time taken for training worker 3: 0:00:25.430744\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.593259477, Training Accuracy: 55.416\n",
            "Worker 4, [02/04]: Training Loss: 1.478186974, Training Accuracy: 58.624\n",
            "Worker 4, [03/04]: Training Loss: 1.404407130, Training Accuracy: 59.648\n",
            "Worker 4, [04/04]: Training Loss: 1.357265402, Training Accuracy: 61.520\n",
            "Time taken for training worker 4: 0:00:25.797898\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002972\n",
            "Global Update 19: Test Loss: 1.822498927, Test Accuracy: 52.210\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.528411045, Training Accuracy: 56.864\n",
            "Worker 1, [02/04]: Training Loss: 1.415352421, Training Accuracy: 60.336\n",
            "Worker 1, [03/04]: Training Loss: 1.341298608, Training Accuracy: 61.704\n",
            "Worker 1, [04/04]: Training Loss: 1.271835560, Training Accuracy: 63.800\n",
            "Time taken for training worker 1: 0:00:24.189270\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.513793928, Training Accuracy: 57.248\n",
            "Worker 2, [02/04]: Training Loss: 1.407347496, Training Accuracy: 59.440\n",
            "Worker 2, [03/04]: Training Loss: 1.348529935, Training Accuracy: 60.912\n",
            "Worker 2, [04/04]: Training Loss: 1.264631978, Training Accuracy: 63.544\n",
            "Time taken for training worker 2: 0:00:23.970326\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.508136851, Training Accuracy: 57.616\n",
            "Worker 3, [02/04]: Training Loss: 1.413766816, Training Accuracy: 59.696\n",
            "Worker 3, [03/04]: Training Loss: 1.337337596, Training Accuracy: 61.816\n",
            "Worker 3, [04/04]: Training Loss: 1.261525843, Training Accuracy: 63.832\n",
            "Time taken for training worker 3: 0:00:25.090186\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.534350943, Training Accuracy: 56.744\n",
            "Worker 4, [02/04]: Training Loss: 1.420875899, Training Accuracy: 59.448\n",
            "Worker 4, [03/04]: Training Loss: 1.348553153, Training Accuracy: 61.560\n",
            "Worker 4, [04/04]: Training Loss: 1.310328131, Training Accuracy: 62.104\n",
            "Time taken for training worker 4: 0:00:24.781605\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002863\n",
            "Global Update 20: Test Loss: 1.841215128, Test Accuracy: 52.030\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.481370571, Training Accuracy: 58.416\n",
            "Worker 1, [02/04]: Training Loss: 1.342837679, Training Accuracy: 61.608\n",
            "Worker 1, [03/04]: Training Loss: 1.274970673, Training Accuracy: 63.064\n",
            "Worker 1, [04/04]: Training Loss: 1.232461407, Training Accuracy: 64.736\n",
            "Time taken for training worker 1: 0:00:24.657029\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.458580437, Training Accuracy: 58.376\n",
            "Worker 2, [02/04]: Training Loss: 1.344786346, Training Accuracy: 61.256\n",
            "Worker 2, [03/04]: Training Loss: 1.281745292, Training Accuracy: 62.976\n",
            "Worker 2, [04/04]: Training Loss: 1.237248103, Training Accuracy: 64.072\n",
            "Time taken for training worker 2: 0:00:24.362503\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.449939891, Training Accuracy: 58.824\n",
            "Worker 3, [02/04]: Training Loss: 1.365845312, Training Accuracy: 61.248\n",
            "Worker 3, [03/04]: Training Loss: 1.267924892, Training Accuracy: 63.856\n",
            "Worker 3, [04/04]: Training Loss: 1.215762157, Training Accuracy: 64.736\n",
            "Time taken for training worker 3: 0:00:25.289690\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.488516188, Training Accuracy: 58.008\n",
            "Worker 4, [02/04]: Training Loss: 1.368260087, Training Accuracy: 60.664\n",
            "Worker 4, [03/04]: Training Loss: 1.293585347, Training Accuracy: 63.168\n",
            "Worker 4, [04/04]: Training Loss: 1.237383964, Training Accuracy: 63.960\n",
            "Time taken for training worker 4: 0:00:25.372971\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002804\n",
            "Global Update 21: Test Loss: 1.829193193, Test Accuracy: 53.090\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.417513870, Training Accuracy: 59.792\n",
            "Worker 1, [02/04]: Training Loss: 1.309942222, Training Accuracy: 62.744\n",
            "Worker 1, [03/04]: Training Loss: 1.221321367, Training Accuracy: 64.648\n",
            "Worker 1, [04/04]: Training Loss: 1.175525229, Training Accuracy: 65.760\n",
            "Time taken for training worker 1: 0:00:24.431722\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.411331628, Training Accuracy: 59.480\n",
            "Worker 2, [02/04]: Training Loss: 1.293054864, Training Accuracy: 62.728\n",
            "Worker 2, [03/04]: Training Loss: 1.225191541, Training Accuracy: 64.192\n",
            "Worker 2, [04/04]: Training Loss: 1.154216042, Training Accuracy: 66.432\n",
            "Time taken for training worker 2: 0:00:25.048229\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.431495800, Training Accuracy: 59.104\n",
            "Worker 3, [02/04]: Training Loss: 1.305124170, Training Accuracy: 62.512\n",
            "Worker 3, [03/04]: Training Loss: 1.217073668, Training Accuracy: 64.592\n",
            "Worker 3, [04/04]: Training Loss: 1.171217830, Training Accuracy: 66.192\n",
            "Time taken for training worker 3: 0:00:23.561454\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.437123739, Training Accuracy: 59.272\n",
            "Worker 4, [02/04]: Training Loss: 1.327987713, Training Accuracy: 61.736\n",
            "Worker 4, [03/04]: Training Loss: 1.250730719, Training Accuracy: 63.920\n",
            "Worker 4, [04/04]: Training Loss: 1.171064394, Training Accuracy: 66.008\n",
            "Time taken for training worker 4: 0:00:23.639038\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002807\n",
            "Global Update 22: Test Loss: 1.819768673, Test Accuracy: 52.940\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.375190645, Training Accuracy: 60.440\n",
            "Worker 1, [02/04]: Training Loss: 1.250120011, Training Accuracy: 63.968\n",
            "Worker 1, [03/04]: Training Loss: 1.157995223, Training Accuracy: 66.320\n",
            "Worker 1, [04/04]: Training Loss: 1.120133557, Training Accuracy: 67.808\n",
            "Time taken for training worker 1: 0:00:24.069868\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.384340338, Training Accuracy: 60.304\n",
            "Worker 2, [02/04]: Training Loss: 1.245317737, Training Accuracy: 63.648\n",
            "Worker 2, [03/04]: Training Loss: 1.171238164, Training Accuracy: 65.968\n",
            "Worker 2, [04/04]: Training Loss: 1.103962626, Training Accuracy: 67.544\n",
            "Time taken for training worker 2: 0:00:23.880462\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.360240290, Training Accuracy: 61.328\n",
            "Worker 3, [02/04]: Training Loss: 1.227781301, Training Accuracy: 64.120\n",
            "Worker 3, [03/04]: Training Loss: 1.178393626, Training Accuracy: 65.976\n",
            "Worker 3, [04/04]: Training Loss: 1.107094330, Training Accuracy: 67.512\n",
            "Time taken for training worker 3: 0:00:24.305430\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.383039358, Training Accuracy: 60.760\n",
            "Worker 4, [02/04]: Training Loss: 1.277970265, Training Accuracy: 63.064\n",
            "Worker 4, [03/04]: Training Loss: 1.192492903, Training Accuracy: 66.032\n",
            "Worker 4, [04/04]: Training Loss: 1.127262397, Training Accuracy: 66.696\n",
            "Time taken for training worker 4: 0:00:25.526304\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002895\n",
            "Global Update 23: Test Loss: 1.834310198, Test Accuracy: 53.450\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.333107896, Training Accuracy: 61.968\n",
            "Worker 1, [02/04]: Training Loss: 1.206809002, Training Accuracy: 65.240\n",
            "Worker 1, [03/04]: Training Loss: 1.115019852, Training Accuracy: 67.104\n",
            "Worker 1, [04/04]: Training Loss: 1.064498236, Training Accuracy: 69.104\n",
            "Time taken for training worker 1: 0:00:24.926738\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.312362313, Training Accuracy: 61.920\n",
            "Worker 2, [02/04]: Training Loss: 1.170022873, Training Accuracy: 66.144\n",
            "Worker 2, [03/04]: Training Loss: 1.116596635, Training Accuracy: 67.072\n",
            "Worker 2, [04/04]: Training Loss: 1.068427193, Training Accuracy: 68.536\n",
            "Time taken for training worker 2: 0:00:24.494128\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.301074534, Training Accuracy: 62.456\n",
            "Worker 3, [02/04]: Training Loss: 1.201267149, Training Accuracy: 65.032\n",
            "Worker 3, [03/04]: Training Loss: 1.113525330, Training Accuracy: 67.472\n",
            "Worker 3, [04/04]: Training Loss: 1.072558525, Training Accuracy: 68.600\n",
            "Time taken for training worker 3: 0:00:24.695555\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.314450574, Training Accuracy: 61.960\n",
            "Worker 4, [02/04]: Training Loss: 1.223381735, Training Accuracy: 65.024\n",
            "Worker 4, [03/04]: Training Loss: 1.143763512, Training Accuracy: 66.664\n",
            "Worker 4, [04/04]: Training Loss: 1.085625858, Training Accuracy: 68.648\n",
            "Time taken for training worker 4: 0:00:24.603605\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002693\n",
            "Global Update 24: Test Loss: 1.829087799, Test Accuracy: 53.790\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.260827883, Training Accuracy: 63.408\n",
            "Worker 1, [02/04]: Training Loss: 1.141382682, Training Accuracy: 67.304\n",
            "Worker 1, [03/04]: Training Loss: 1.091551516, Training Accuracy: 68.224\n",
            "Worker 1, [04/04]: Training Loss: 1.009920725, Training Accuracy: 70.448\n",
            "Time taken for training worker 1: 0:00:24.539505\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.264004777, Training Accuracy: 63.136\n",
            "Worker 2, [02/04]: Training Loss: 1.143316595, Training Accuracy: 66.896\n",
            "Worker 2, [03/04]: Training Loss: 1.074482565, Training Accuracy: 68.272\n",
            "Worker 2, [04/04]: Training Loss: 1.029204002, Training Accuracy: 69.568\n",
            "Time taken for training worker 2: 0:00:25.061972\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.262516380, Training Accuracy: 63.672\n",
            "Worker 3, [02/04]: Training Loss: 1.140513269, Training Accuracy: 66.576\n",
            "Worker 3, [03/04]: Training Loss: 1.069322845, Training Accuracy: 68.864\n",
            "Worker 3, [04/04]: Training Loss: 1.014480029, Training Accuracy: 70.208\n",
            "Time taken for training worker 3: 0:00:24.722862\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.286144036, Training Accuracy: 63.456\n",
            "Worker 4, [02/04]: Training Loss: 1.162229858, Training Accuracy: 66.488\n",
            "Worker 4, [03/04]: Training Loss: 1.077664683, Training Accuracy: 68.384\n",
            "Worker 4, [04/04]: Training Loss: 1.037995755, Training Accuracy: 69.736\n",
            "Time taken for training worker 4: 0:00:23.513604\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002701\n",
            "Global Update 25: Test Loss: 1.840472238, Test Accuracy: 53.600\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.225377529, Training Accuracy: 64.552\n",
            "Worker 1, [02/04]: Training Loss: 1.103679094, Training Accuracy: 68.248\n",
            "Worker 1, [03/04]: Training Loss: 1.018290660, Training Accuracy: 70.216\n",
            "Worker 1, [04/04]: Training Loss: 0.977464108, Training Accuracy: 71.568\n",
            "Time taken for training worker 1: 0:00:24.941958\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.224556906, Training Accuracy: 63.736\n",
            "Worker 2, [02/04]: Training Loss: 1.093370421, Training Accuracy: 67.896\n",
            "Worker 2, [03/04]: Training Loss: 1.023565653, Training Accuracy: 69.568\n",
            "Worker 2, [04/04]: Training Loss: 0.970076258, Training Accuracy: 71.112\n",
            "Time taken for training worker 2: 0:00:23.674011\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.228194770, Training Accuracy: 64.728\n",
            "Worker 3, [02/04]: Training Loss: 1.090819837, Training Accuracy: 68.360\n",
            "Worker 3, [03/04]: Training Loss: 1.026838127, Training Accuracy: 70.240\n",
            "Worker 3, [04/04]: Training Loss: 0.983831650, Training Accuracy: 70.816\n",
            "Time taken for training worker 3: 0:00:24.024531\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.238663379, Training Accuracy: 63.760\n",
            "Worker 4, [02/04]: Training Loss: 1.123636440, Training Accuracy: 67.496\n",
            "Worker 4, [03/04]: Training Loss: 1.046261411, Training Accuracy: 69.704\n",
            "Worker 4, [04/04]: Training Loss: 0.989839003, Training Accuracy: 71.312\n",
            "Time taken for training worker 4: 0:00:24.424239\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002676\n",
            "Global Update 26: Test Loss: 1.844632948, Test Accuracy: 53.870\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.177897622, Training Accuracy: 65.744\n",
            "Worker 1, [02/04]: Training Loss: 1.058741579, Training Accuracy: 69.344\n",
            "Worker 1, [03/04]: Training Loss: 0.983683273, Training Accuracy: 71.544\n",
            "Worker 1, [04/04]: Training Loss: 0.936400259, Training Accuracy: 72.504\n",
            "Time taken for training worker 1: 0:00:24.155868\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.156703174, Training Accuracy: 66.008\n",
            "Worker 2, [02/04]: Training Loss: 1.058579864, Training Accuracy: 69.408\n",
            "Worker 2, [03/04]: Training Loss: 0.983710714, Training Accuracy: 71.120\n",
            "Worker 2, [04/04]: Training Loss: 0.914557063, Training Accuracy: 72.888\n",
            "Time taken for training worker 2: 0:00:24.284424\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.168564711, Training Accuracy: 66.560\n",
            "Worker 3, [02/04]: Training Loss: 1.057450743, Training Accuracy: 69.072\n",
            "Worker 3, [03/04]: Training Loss: 0.993631269, Training Accuracy: 70.808\n",
            "Worker 3, [04/04]: Training Loss: 0.930700769, Training Accuracy: 72.480\n",
            "Time taken for training worker 3: 0:00:24.327069\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.188939478, Training Accuracy: 65.064\n",
            "Worker 4, [02/04]: Training Loss: 1.071672243, Training Accuracy: 68.888\n",
            "Worker 4, [03/04]: Training Loss: 1.010482535, Training Accuracy: 70.640\n",
            "Worker 4, [04/04]: Training Loss: 0.959617401, Training Accuracy: 71.968\n",
            "Time taken for training worker 4: 0:00:24.672918\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002655\n",
            "Global Update 27: Test Loss: 1.855012590, Test Accuracy: 53.830\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.122527222, Training Accuracy: 66.840\n",
            "Worker 1, [02/04]: Training Loss: 1.024003128, Training Accuracy: 70.424\n",
            "Worker 1, [03/04]: Training Loss: 0.946542212, Training Accuracy: 72.296\n",
            "Worker 1, [04/04]: Training Loss: 0.908952275, Training Accuracy: 73.472\n",
            "Time taken for training worker 1: 0:00:23.920171\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.124232988, Training Accuracy: 66.824\n",
            "Worker 2, [02/04]: Training Loss: 1.028750395, Training Accuracy: 69.904\n",
            "Worker 2, [03/04]: Training Loss: 0.951048658, Training Accuracy: 72.032\n",
            "Worker 2, [04/04]: Training Loss: 0.911326087, Training Accuracy: 73.240\n",
            "Time taken for training worker 2: 0:00:24.025597\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.135175401, Training Accuracy: 67.032\n",
            "Worker 3, [02/04]: Training Loss: 1.009418652, Training Accuracy: 70.192\n",
            "Worker 3, [03/04]: Training Loss: 0.953778780, Training Accuracy: 71.776\n",
            "Worker 3, [04/04]: Training Loss: 0.902911273, Training Accuracy: 73.328\n",
            "Time taken for training worker 3: 0:00:23.513452\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.147615074, Training Accuracy: 67.168\n",
            "Worker 4, [02/04]: Training Loss: 1.048696178, Training Accuracy: 69.496\n",
            "Worker 4, [03/04]: Training Loss: 0.966133960, Training Accuracy: 72.072\n",
            "Worker 4, [04/04]: Training Loss: 0.916947751, Training Accuracy: 73.328\n",
            "Time taken for training worker 4: 0:00:24.208700\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002848\n",
            "Global Update 28: Test Loss: 1.859507785, Test Accuracy: 54.020\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.089300955, Training Accuracy: 68.000\n",
            "Worker 1, [02/04]: Training Loss: 0.986892483, Training Accuracy: 70.944\n",
            "Worker 1, [03/04]: Training Loss: 0.917629985, Training Accuracy: 73.224\n",
            "Worker 1, [04/04]: Training Loss: 0.879264136, Training Accuracy: 74.368\n",
            "Time taken for training worker 1: 0:00:24.360531\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.074475315, Training Accuracy: 67.952\n",
            "Worker 2, [02/04]: Training Loss: 0.970582209, Training Accuracy: 71.624\n",
            "Worker 2, [03/04]: Training Loss: 0.924319836, Training Accuracy: 73.064\n",
            "Worker 2, [04/04]: Training Loss: 0.882622005, Training Accuracy: 73.928\n",
            "Time taken for training worker 2: 0:00:24.578932\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.074176795, Training Accuracy: 68.336\n",
            "Worker 3, [02/04]: Training Loss: 0.971669053, Training Accuracy: 71.712\n",
            "Worker 3, [03/04]: Training Loss: 0.921495365, Training Accuracy: 73.088\n",
            "Worker 3, [04/04]: Training Loss: 0.880689352, Training Accuracy: 74.112\n",
            "Time taken for training worker 3: 0:00:23.992875\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.117426025, Training Accuracy: 67.432\n",
            "Worker 4, [02/04]: Training Loss: 1.002876285, Training Accuracy: 70.904\n",
            "Worker 4, [03/04]: Training Loss: 0.941554536, Training Accuracy: 72.208\n",
            "Worker 4, [04/04]: Training Loss: 0.890842869, Training Accuracy: 73.840\n",
            "Time taken for training worker 4: 0:00:24.420264\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002627\n",
            "Global Update 29: Test Loss: 1.868398977, Test Accuracy: 53.990\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.051848600, Training Accuracy: 69.672\n",
            "Worker 1, [02/04]: Training Loss: 0.961000753, Training Accuracy: 72.224\n",
            "Worker 1, [03/04]: Training Loss: 0.890701578, Training Accuracy: 74.224\n",
            "Worker 1, [04/04]: Training Loss: 0.860946518, Training Accuracy: 75.496\n",
            "Time taken for training worker 1: 0:00:23.629943\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.034591254, Training Accuracy: 69.680\n",
            "Worker 2, [02/04]: Training Loss: 0.955504239, Training Accuracy: 71.864\n",
            "Worker 2, [03/04]: Training Loss: 0.884261009, Training Accuracy: 74.464\n",
            "Worker 2, [04/04]: Training Loss: 0.854926133, Training Accuracy: 75.040\n",
            "Time taken for training worker 2: 0:00:23.162830\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.054891620, Training Accuracy: 69.000\n",
            "Worker 3, [02/04]: Training Loss: 0.953291661, Training Accuracy: 72.048\n",
            "Worker 3, [03/04]: Training Loss: 0.875226631, Training Accuracy: 74.808\n",
            "Worker 3, [04/04]: Training Loss: 0.841889726, Training Accuracy: 75.288\n",
            "Time taken for training worker 3: 0:00:23.612530\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.067371260, Training Accuracy: 68.608\n",
            "Worker 4, [02/04]: Training Loss: 0.965074163, Training Accuracy: 71.592\n",
            "Worker 4, [03/04]: Training Loss: 0.912147146, Training Accuracy: 73.056\n",
            "Worker 4, [04/04]: Training Loss: 0.879556059, Training Accuracy: 74.280\n",
            "Time taken for training worker 4: 0:00:23.898420\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002738\n",
            "Global Update 30: Test Loss: 1.875695598, Test Accuracy: 54.350\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.026585709, Training Accuracy: 69.784\n",
            "Worker 1, [02/04]: Training Loss: 0.936150841, Training Accuracy: 73.120\n",
            "Worker 1, [03/04]: Training Loss: 0.871643015, Training Accuracy: 75.016\n",
            "Worker 1, [04/04]: Training Loss: 0.844899731, Training Accuracy: 75.560\n",
            "Time taken for training worker 1: 0:00:24.315500\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.023029887, Training Accuracy: 70.200\n",
            "Worker 2, [02/04]: Training Loss: 0.925055667, Training Accuracy: 73.040\n",
            "Worker 2, [03/04]: Training Loss: 0.889047226, Training Accuracy: 73.928\n",
            "Worker 2, [04/04]: Training Loss: 0.836171890, Training Accuracy: 75.536\n",
            "Time taken for training worker 2: 0:00:23.016847\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.022925301, Training Accuracy: 69.744\n",
            "Worker 3, [02/04]: Training Loss: 0.929925328, Training Accuracy: 72.552\n",
            "Worker 3, [03/04]: Training Loss: 0.878946922, Training Accuracy: 73.896\n",
            "Worker 3, [04/04]: Training Loss: 0.830529825, Training Accuracy: 75.904\n",
            "Time taken for training worker 3: 0:00:23.757429\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.026093655, Training Accuracy: 70.640\n",
            "Worker 4, [02/04]: Training Loss: 0.948739368, Training Accuracy: 72.400\n",
            "Worker 4, [03/04]: Training Loss: 0.889385806, Training Accuracy: 73.736\n",
            "Worker 4, [04/04]: Training Loss: 0.849895156, Training Accuracy: 74.880\n",
            "Time taken for training worker 4: 0:00:23.398177\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002614\n",
            "Global Update 31: Test Loss: 1.881992789, Test Accuracy: 54.180\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.997227189, Training Accuracy: 71.520\n",
            "Worker 1, [02/04]: Training Loss: 0.920097524, Training Accuracy: 73.328\n",
            "Worker 1, [03/04]: Training Loss: 0.863225251, Training Accuracy: 75.200\n",
            "Worker 1, [04/04]: Training Loss: 0.830199819, Training Accuracy: 75.600\n",
            "Time taken for training worker 1: 0:00:23.735249\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.979714266, Training Accuracy: 70.672\n",
            "Worker 2, [02/04]: Training Loss: 0.912313160, Training Accuracy: 72.912\n",
            "Worker 2, [03/04]: Training Loss: 0.867725622, Training Accuracy: 74.608\n",
            "Worker 2, [04/04]: Training Loss: 0.829635989, Training Accuracy: 75.568\n",
            "Time taken for training worker 2: 0:00:22.986685\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.981421831, Training Accuracy: 71.392\n",
            "Worker 3, [02/04]: Training Loss: 0.910181556, Training Accuracy: 73.424\n",
            "Worker 3, [03/04]: Training Loss: 0.867247577, Training Accuracy: 75.080\n",
            "Worker 3, [04/04]: Training Loss: 0.828672517, Training Accuracy: 76.016\n",
            "Time taken for training worker 3: 0:00:23.474955\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.003655479, Training Accuracy: 70.280\n",
            "Worker 4, [02/04]: Training Loss: 0.925069209, Training Accuracy: 73.144\n",
            "Worker 4, [03/04]: Training Loss: 0.883168533, Training Accuracy: 74.552\n",
            "Worker 4, [04/04]: Training Loss: 0.840912365, Training Accuracy: 75.792\n",
            "Time taken for training worker 4: 0:00:25.093799\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002717\n",
            "Global Update 32: Test Loss: 1.888131342, Test Accuracy: 54.330\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.963639078, Training Accuracy: 72.112\n",
            "Worker 1, [02/04]: Training Loss: 0.915416676, Training Accuracy: 73.560\n",
            "Worker 1, [03/04]: Training Loss: 0.872774079, Training Accuracy: 74.736\n",
            "Worker 1, [04/04]: Training Loss: 0.829789636, Training Accuracy: 76.184\n",
            "Time taken for training worker 1: 0:00:23.443848\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.957356203, Training Accuracy: 71.896\n",
            "Worker 2, [02/04]: Training Loss: 0.899339532, Training Accuracy: 73.616\n",
            "Worker 2, [03/04]: Training Loss: 0.857125677, Training Accuracy: 74.576\n",
            "Worker 2, [04/04]: Training Loss: 0.823321919, Training Accuracy: 75.968\n",
            "Time taken for training worker 2: 0:00:23.970398\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.952691335, Training Accuracy: 72.496\n",
            "Worker 3, [02/04]: Training Loss: 0.899258277, Training Accuracy: 73.840\n",
            "Worker 3, [03/04]: Training Loss: 0.868927568, Training Accuracy: 74.680\n",
            "Worker 3, [04/04]: Training Loss: 0.828986028, Training Accuracy: 75.952\n",
            "Time taken for training worker 3: 0:00:24.945826\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.971628198, Training Accuracy: 71.696\n",
            "Worker 4, [02/04]: Training Loss: 0.913587704, Training Accuracy: 73.232\n",
            "Worker 4, [03/04]: Training Loss: 0.880105709, Training Accuracy: 74.584\n",
            "Worker 4, [04/04]: Training Loss: 0.847482001, Training Accuracy: 75.456\n",
            "Time taken for training worker 4: 0:00:24.627414\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002831\n",
            "Global Update 33: Test Loss: 1.886641938, Test Accuracy: 54.000\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.955331962, Training Accuracy: 72.208\n",
            "Worker 1, [02/04]: Training Loss: 0.895360745, Training Accuracy: 73.768\n",
            "Worker 1, [03/04]: Training Loss: 0.877809180, Training Accuracy: 74.200\n",
            "Worker 1, [04/04]: Training Loss: 0.851602238, Training Accuracy: 75.648\n",
            "Time taken for training worker 1: 0:00:23.636582\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.944781825, Training Accuracy: 71.640\n",
            "Worker 2, [02/04]: Training Loss: 0.884454596, Training Accuracy: 73.544\n",
            "Worker 2, [03/04]: Training Loss: 0.869202396, Training Accuracy: 74.704\n",
            "Worker 2, [04/04]: Training Loss: 0.832013534, Training Accuracy: 75.632\n",
            "Time taken for training worker 2: 0:00:23.012300\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.941898748, Training Accuracy: 72.576\n",
            "Worker 3, [02/04]: Training Loss: 0.899599533, Training Accuracy: 73.800\n",
            "Worker 3, [03/04]: Training Loss: 0.859639556, Training Accuracy: 74.720\n",
            "Worker 3, [04/04]: Training Loss: 0.843390120, Training Accuracy: 75.312\n",
            "Time taken for training worker 3: 0:00:22.969325\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.969941031, Training Accuracy: 71.784\n",
            "Worker 4, [02/04]: Training Loss: 0.917845869, Training Accuracy: 72.952\n",
            "Worker 4, [03/04]: Training Loss: 0.892346276, Training Accuracy: 74.192\n",
            "Worker 4, [04/04]: Training Loss: 0.849671662, Training Accuracy: 75.648\n",
            "Time taken for training worker 4: 0:00:23.085522\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002497\n",
            "Global Update 34: Test Loss: 1.881349562, Test Accuracy: 54.220\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.941099607, Training Accuracy: 72.296\n",
            "Worker 1, [02/04]: Training Loss: 0.907315239, Training Accuracy: 73.368\n",
            "Worker 1, [03/04]: Training Loss: 0.876840470, Training Accuracy: 74.712\n",
            "Worker 1, [04/04]: Training Loss: 0.853934789, Training Accuracy: 75.400\n",
            "Time taken for training worker 1: 0:00:23.757391\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.936926345, Training Accuracy: 72.224\n",
            "Worker 2, [02/04]: Training Loss: 0.897643370, Training Accuracy: 73.752\n",
            "Worker 2, [03/04]: Training Loss: 0.870629731, Training Accuracy: 74.384\n",
            "Worker 2, [04/04]: Training Loss: 0.871020154, Training Accuracy: 74.672\n",
            "Time taken for training worker 2: 0:00:23.688816\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.925479263, Training Accuracy: 72.560\n",
            "Worker 3, [02/04]: Training Loss: 0.900847125, Training Accuracy: 73.480\n",
            "Worker 3, [03/04]: Training Loss: 0.866185269, Training Accuracy: 74.696\n",
            "Worker 3, [04/04]: Training Loss: 0.857236733, Training Accuracy: 75.216\n",
            "Time taken for training worker 3: 0:00:23.037267\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.958665614, Training Accuracy: 72.336\n",
            "Worker 4, [02/04]: Training Loss: 0.921544006, Training Accuracy: 72.896\n",
            "Worker 4, [03/04]: Training Loss: 0.893745897, Training Accuracy: 74.224\n",
            "Worker 4, [04/04]: Training Loss: 0.881512277, Training Accuracy: 74.328\n",
            "Time taken for training worker 4: 0:00:22.658343\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002468\n",
            "Global Update 35: Test Loss: 1.880235110, Test Accuracy: 54.330\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.941326966, Training Accuracy: 72.624\n",
            "Worker 1, [02/04]: Training Loss: 0.921736385, Training Accuracy: 73.184\n",
            "Worker 1, [03/04]: Training Loss: 0.904585701, Training Accuracy: 73.736\n",
            "Worker 1, [04/04]: Training Loss: 0.882675426, Training Accuracy: 74.688\n",
            "Time taken for training worker 1: 0:00:25.004875\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.940020281, Training Accuracy: 72.336\n",
            "Worker 2, [02/04]: Training Loss: 0.907842920, Training Accuracy: 72.888\n",
            "Worker 2, [03/04]: Training Loss: 0.881657673, Training Accuracy: 73.920\n",
            "Worker 2, [04/04]: Training Loss: 0.869542115, Training Accuracy: 74.472\n",
            "Time taken for training worker 2: 0:00:24.448644\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.935369135, Training Accuracy: 72.536\n",
            "Worker 3, [02/04]: Training Loss: 0.909361950, Training Accuracy: 73.224\n",
            "Worker 3, [03/04]: Training Loss: 0.892424568, Training Accuracy: 73.816\n",
            "Worker 3, [04/04]: Training Loss: 0.891697439, Training Accuracy: 74.000\n",
            "Time taken for training worker 3: 0:00:23.298272\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.951782877, Training Accuracy: 72.304\n",
            "Worker 4, [02/04]: Training Loss: 0.940607235, Training Accuracy: 72.048\n",
            "Worker 4, [03/04]: Training Loss: 0.910663987, Training Accuracy: 73.560\n",
            "Worker 4, [04/04]: Training Loss: 0.901321651, Training Accuracy: 73.848\n",
            "Time taken for training worker 4: 0:00:23.361163\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002620\n",
            "Global Update 36: Test Loss: 1.877620286, Test Accuracy: 54.240\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.934900025, Training Accuracy: 72.600\n",
            "Worker 1, [02/04]: Training Loss: 0.921739290, Training Accuracy: 72.960\n",
            "Worker 1, [03/04]: Training Loss: 0.910959111, Training Accuracy: 73.072\n",
            "Worker 1, [04/04]: Training Loss: 0.916175647, Training Accuracy: 72.880\n",
            "Time taken for training worker 1: 0:00:23.327262\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 0.936087354, Training Accuracy: 72.128\n",
            "Worker 2, [02/04]: Training Loss: 0.917700852, Training Accuracy: 72.864\n",
            "Worker 2, [03/04]: Training Loss: 0.899990376, Training Accuracy: 72.992\n",
            "Worker 2, [04/04]: Training Loss: 0.912657333, Training Accuracy: 72.720\n",
            "Time taken for training worker 2: 0:00:23.709168\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 0.928918072, Training Accuracy: 72.976\n",
            "Worker 3, [02/04]: Training Loss: 0.920879834, Training Accuracy: 73.040\n",
            "Worker 3, [03/04]: Training Loss: 0.919831066, Training Accuracy: 72.936\n",
            "Worker 3, [04/04]: Training Loss: 0.906234947, Training Accuracy: 73.240\n",
            "Time taken for training worker 3: 0:00:23.624929\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 0.942217697, Training Accuracy: 72.488\n",
            "Worker 4, [02/04]: Training Loss: 0.934024430, Training Accuracy: 72.432\n",
            "Worker 4, [03/04]: Training Loss: 0.941104813, Training Accuracy: 72.704\n",
            "Worker 4, [04/04]: Training Loss: 0.924843467, Training Accuracy: 72.952\n",
            "Time taken for training worker 4: 0:00:24.715223\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002821\n",
            "Global Update 37: Test Loss: 1.875574264, Test Accuracy: 54.280\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 1:03:17.692196\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:8\n",
            "==================================================\n",
            "Worker 1, [01/08]: Training Loss: 4.484875402, Training Accuracy: 2.416\n",
            "Worker 1, [02/08]: Training Loss: 4.115391067, Training Accuracy: 6.400\n",
            "Worker 1, [03/08]: Training Loss: 3.897697417, Training Accuracy: 9.424\n",
            "Worker 1, [04/08]: Training Loss: 3.737545608, Training Accuracy: 12.096\n",
            "Worker 1, [05/08]: Training Loss: 3.604588114, Training Accuracy: 14.832\n",
            "Worker 1, [06/08]: Training Loss: 3.469517809, Training Accuracy: 16.216\n",
            "Worker 1, [07/08]: Training Loss: 3.354940256, Training Accuracy: 18.336\n",
            "Worker 1, [08/08]: Training Loss: 3.252390121, Training Accuracy: 19.888\n",
            "Time taken for training worker 1: 0:00:48.810503\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 4.500109755, Training Accuracy: 2.480\n",
            "Worker 2, [02/08]: Training Loss: 4.116367418, Training Accuracy: 6.552\n",
            "Worker 2, [03/08]: Training Loss: 3.907669573, Training Accuracy: 9.440\n",
            "Worker 2, [04/08]: Training Loss: 3.743217339, Training Accuracy: 11.824\n",
            "Worker 2, [05/08]: Training Loss: 3.621930693, Training Accuracy: 13.688\n",
            "Worker 2, [06/08]: Training Loss: 3.486312237, Training Accuracy: 15.840\n",
            "Worker 2, [07/08]: Training Loss: 3.374178118, Training Accuracy: 17.992\n",
            "Worker 2, [08/08]: Training Loss: 3.252875076, Training Accuracy: 19.896\n",
            "Time taken for training worker 2: 0:00:47.943606\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 4.491468758, Training Accuracy: 2.736\n",
            "Worker 3, [02/08]: Training Loss: 4.102110168, Training Accuracy: 6.856\n",
            "Worker 3, [03/08]: Training Loss: 3.879506373, Training Accuracy: 10.008\n",
            "Worker 3, [04/08]: Training Loss: 3.734534810, Training Accuracy: 12.128\n",
            "Worker 3, [05/08]: Training Loss: 3.582874798, Training Accuracy: 14.760\n",
            "Worker 3, [06/08]: Training Loss: 3.465584080, Training Accuracy: 16.344\n",
            "Worker 3, [07/08]: Training Loss: 3.326704335, Training Accuracy: 18.672\n",
            "Worker 3, [08/08]: Training Loss: 3.241763265, Training Accuracy: 20.016\n",
            "Time taken for training worker 3: 0:00:46.952554\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 4.494781095, Training Accuracy: 2.440\n",
            "Worker 4, [02/08]: Training Loss: 4.115270385, Training Accuracy: 6.656\n",
            "Worker 4, [03/08]: Training Loss: 3.886945804, Training Accuracy: 9.416\n",
            "Worker 4, [04/08]: Training Loss: 3.736618043, Training Accuracy: 12.056\n",
            "Worker 4, [05/08]: Training Loss: 3.608919245, Training Accuracy: 14.064\n",
            "Worker 4, [06/08]: Training Loss: 3.499306714, Training Accuracy: 15.968\n",
            "Worker 4, [07/08]: Training Loss: 3.378461275, Training Accuracy: 18.304\n",
            "Worker 4, [08/08]: Training Loss: 3.264573046, Training Accuracy: 19.976\n",
            "Time taken for training worker 4: 0:00:47.478958\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002738\n",
            "Global Update 01: Test Loss: 3.374448198, Test Accuracy: 21.800\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 3.322176611, Training Accuracy: 19.056\n",
            "Worker 1, [02/08]: Training Loss: 3.187812396, Training Accuracy: 21.168\n",
            "Worker 1, [03/08]: Training Loss: 3.109616441, Training Accuracy: 22.760\n",
            "Worker 1, [04/08]: Training Loss: 3.011332951, Training Accuracy: 25.112\n",
            "Worker 1, [05/08]: Training Loss: 2.910016998, Training Accuracy: 26.920\n",
            "Worker 1, [06/08]: Training Loss: 2.837447281, Training Accuracy: 28.024\n",
            "Worker 1, [07/08]: Training Loss: 2.759465538, Training Accuracy: 29.664\n",
            "Worker 1, [08/08]: Training Loss: 2.700246869, Training Accuracy: 30.328\n",
            "Time taken for training worker 1: 0:00:48.172232\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 3.326668991, Training Accuracy: 18.984\n",
            "Worker 2, [02/08]: Training Loss: 3.183971615, Training Accuracy: 20.968\n",
            "Worker 2, [03/08]: Training Loss: 3.114591887, Training Accuracy: 22.744\n",
            "Worker 2, [04/08]: Training Loss: 3.023563643, Training Accuracy: 23.968\n",
            "Worker 2, [05/08]: Training Loss: 2.914728857, Training Accuracy: 25.904\n",
            "Worker 2, [06/08]: Training Loss: 2.839077301, Training Accuracy: 27.568\n",
            "Worker 2, [07/08]: Training Loss: 2.760106946, Training Accuracy: 29.184\n",
            "Worker 2, [08/08]: Training Loss: 2.695921190, Training Accuracy: 30.192\n",
            "Time taken for training worker 2: 0:00:47.153420\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 3.316363327, Training Accuracy: 19.128\n",
            "Worker 3, [02/08]: Training Loss: 3.204196441, Training Accuracy: 21.088\n",
            "Worker 3, [03/08]: Training Loss: 3.080370884, Training Accuracy: 23.080\n",
            "Worker 3, [04/08]: Training Loss: 2.990766289, Training Accuracy: 25.072\n",
            "Worker 3, [05/08]: Training Loss: 2.892413821, Training Accuracy: 26.816\n",
            "Worker 3, [06/08]: Training Loss: 2.823163970, Training Accuracy: 28.088\n",
            "Worker 3, [07/08]: Training Loss: 2.762580807, Training Accuracy: 29.224\n",
            "Worker 3, [08/08]: Training Loss: 2.678269822, Training Accuracy: 31.152\n",
            "Time taken for training worker 3: 0:00:47.018444\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 3.334956457, Training Accuracy: 18.720\n",
            "Worker 4, [02/08]: Training Loss: 3.206755283, Training Accuracy: 20.936\n",
            "Worker 4, [03/08]: Training Loss: 3.106995398, Training Accuracy: 22.960\n",
            "Worker 4, [04/08]: Training Loss: 3.004837925, Training Accuracy: 24.592\n",
            "Worker 4, [05/08]: Training Loss: 2.912631434, Training Accuracy: 26.024\n",
            "Worker 4, [06/08]: Training Loss: 2.831895804, Training Accuracy: 27.760\n",
            "Worker 4, [07/08]: Training Loss: 2.761374989, Training Accuracy: 29.016\n",
            "Worker 4, [08/08]: Training Loss: 2.695977614, Training Accuracy: 30.832\n",
            "Time taken for training worker 4: 0:00:47.655052\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002772\n",
            "Global Update 02: Test Loss: 2.604404489, Test Accuracy: 34.040\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.796282295, Training Accuracy: 29.440\n",
            "Worker 1, [02/08]: Training Loss: 2.708877544, Training Accuracy: 31.304\n",
            "Worker 1, [03/08]: Training Loss: 2.628992120, Training Accuracy: 32.232\n",
            "Worker 1, [04/08]: Training Loss: 2.568500554, Training Accuracy: 34.080\n",
            "Worker 1, [05/08]: Training Loss: 2.481979381, Training Accuracy: 35.400\n",
            "Worker 1, [06/08]: Training Loss: 2.432951487, Training Accuracy: 36.440\n",
            "Worker 1, [07/08]: Training Loss: 2.396996589, Training Accuracy: 37.192\n",
            "Worker 1, [08/08]: Training Loss: 2.342683211, Training Accuracy: 38.120\n",
            "Time taken for training worker 1: 0:00:48.748798\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.781612405, Training Accuracy: 29.280\n",
            "Worker 2, [02/08]: Training Loss: 2.714278675, Training Accuracy: 30.848\n",
            "Worker 2, [03/08]: Training Loss: 2.632833399, Training Accuracy: 31.856\n",
            "Worker 2, [04/08]: Training Loss: 2.537039492, Training Accuracy: 34.064\n",
            "Worker 2, [05/08]: Training Loss: 2.492740585, Training Accuracy: 34.456\n",
            "Worker 2, [06/08]: Training Loss: 2.444578111, Training Accuracy: 36.592\n",
            "Worker 2, [07/08]: Training Loss: 2.380484434, Training Accuracy: 36.776\n",
            "Worker 2, [08/08]: Training Loss: 2.298591288, Training Accuracy: 38.416\n",
            "Time taken for training worker 2: 0:00:48.364469\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.772621759, Training Accuracy: 29.496\n",
            "Worker 3, [02/08]: Training Loss: 2.701811602, Training Accuracy: 30.696\n",
            "Worker 3, [03/08]: Training Loss: 2.600053424, Training Accuracy: 33.472\n",
            "Worker 3, [04/08]: Training Loss: 2.537590846, Training Accuracy: 34.048\n",
            "Worker 3, [05/08]: Training Loss: 2.485235717, Training Accuracy: 35.080\n",
            "Worker 3, [06/08]: Training Loss: 2.441185125, Training Accuracy: 36.136\n",
            "Worker 3, [07/08]: Training Loss: 2.383157919, Training Accuracy: 36.776\n",
            "Worker 3, [08/08]: Training Loss: 2.327667657, Training Accuracy: 38.096\n",
            "Time taken for training worker 3: 0:00:46.889920\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.776725664, Training Accuracy: 29.504\n",
            "Worker 4, [02/08]: Training Loss: 2.715554125, Training Accuracy: 30.320\n",
            "Worker 4, [03/08]: Training Loss: 2.636399611, Training Accuracy: 31.552\n",
            "Worker 4, [04/08]: Training Loss: 2.554565734, Training Accuracy: 33.328\n",
            "Worker 4, [05/08]: Training Loss: 2.478945745, Training Accuracy: 34.984\n",
            "Worker 4, [06/08]: Training Loss: 2.450783811, Training Accuracy: 35.672\n",
            "Worker 4, [07/08]: Training Loss: 2.395390060, Training Accuracy: 36.696\n",
            "Worker 4, [08/08]: Training Loss: 2.318962709, Training Accuracy: 38.472\n",
            "Time taken for training worker 4: 0:00:49.083871\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002730\n",
            "Global Update 03: Test Loss: 2.317131373, Test Accuracy: 39.620\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.495315193, Training Accuracy: 35.280\n",
            "Worker 1, [02/08]: Training Loss: 2.431708032, Training Accuracy: 36.656\n",
            "Worker 1, [03/08]: Training Loss: 2.328725271, Training Accuracy: 38.488\n",
            "Worker 1, [04/08]: Training Loss: 2.286170711, Training Accuracy: 39.528\n",
            "Worker 1, [05/08]: Training Loss: 2.200458485, Training Accuracy: 41.304\n",
            "Worker 1, [06/08]: Training Loss: 2.169149604, Training Accuracy: 41.816\n",
            "Worker 1, [07/08]: Training Loss: 2.121886613, Training Accuracy: 42.616\n",
            "Worker 1, [08/08]: Training Loss: 2.063325580, Training Accuracy: 44.136\n",
            "Time taken for training worker 1: 0:00:47.125389\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.486855887, Training Accuracy: 35.376\n",
            "Worker 2, [02/08]: Training Loss: 2.409281288, Training Accuracy: 36.896\n",
            "Worker 2, [03/08]: Training Loss: 2.314806820, Training Accuracy: 38.840\n",
            "Worker 2, [04/08]: Training Loss: 2.271096079, Training Accuracy: 39.056\n",
            "Worker 2, [05/08]: Training Loss: 2.228504686, Training Accuracy: 39.824\n",
            "Worker 2, [06/08]: Training Loss: 2.152996994, Training Accuracy: 42.168\n",
            "Worker 2, [07/08]: Training Loss: 2.099628872, Training Accuracy: 43.392\n",
            "Worker 2, [08/08]: Training Loss: 2.054846927, Training Accuracy: 44.264\n",
            "Time taken for training worker 2: 0:00:49.170115\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.492641693, Training Accuracy: 35.600\n",
            "Worker 3, [02/08]: Training Loss: 2.413609997, Training Accuracy: 36.992\n",
            "Worker 3, [03/08]: Training Loss: 2.320603407, Training Accuracy: 38.480\n",
            "Worker 3, [04/08]: Training Loss: 2.280763430, Training Accuracy: 39.152\n",
            "Worker 3, [05/08]: Training Loss: 2.211743992, Training Accuracy: 40.632\n",
            "Worker 3, [06/08]: Training Loss: 2.166021552, Training Accuracy: 42.032\n",
            "Worker 3, [07/08]: Training Loss: 2.109343008, Training Accuracy: 42.728\n",
            "Worker 3, [08/08]: Training Loss: 2.057781060, Training Accuracy: 44.624\n",
            "Time taken for training worker 3: 0:00:48.537654\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.494852942, Training Accuracy: 35.456\n",
            "Worker 4, [02/08]: Training Loss: 2.410470844, Training Accuracy: 36.552\n",
            "Worker 4, [03/08]: Training Loss: 2.327958400, Training Accuracy: 37.784\n",
            "Worker 4, [04/08]: Training Loss: 2.266473005, Training Accuracy: 39.712\n",
            "Worker 4, [05/08]: Training Loss: 2.219683242, Training Accuracy: 40.368\n",
            "Worker 4, [06/08]: Training Loss: 2.175793111, Training Accuracy: 41.712\n",
            "Worker 4, [07/08]: Training Loss: 2.131019078, Training Accuracy: 42.648\n",
            "Worker 4, [08/08]: Training Loss: 2.064041777, Training Accuracy: 44.064\n",
            "Time taken for training worker 4: 0:00:49.402648\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002866\n",
            "Global Update 04: Test Loss: 2.167614236, Test Accuracy: 43.180\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.280031569, Training Accuracy: 40.520\n",
            "Worker 1, [02/08]: Training Loss: 2.177786589, Training Accuracy: 42.016\n",
            "Worker 1, [03/08]: Training Loss: 2.126405360, Training Accuracy: 42.928\n",
            "Worker 1, [04/08]: Training Loss: 2.038015502, Training Accuracy: 44.344\n",
            "Worker 1, [05/08]: Training Loss: 1.987976673, Training Accuracy: 46.424\n",
            "Worker 1, [06/08]: Training Loss: 1.928970392, Training Accuracy: 46.824\n",
            "Worker 1, [07/08]: Training Loss: 1.894478145, Training Accuracy: 47.752\n",
            "Worker 1, [08/08]: Training Loss: 1.858442580, Training Accuracy: 48.624\n",
            "Time taken for training worker 1: 0:00:47.874906\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.286186030, Training Accuracy: 39.472\n",
            "Worker 2, [02/08]: Training Loss: 2.182677560, Training Accuracy: 40.968\n",
            "Worker 2, [03/08]: Training Loss: 2.101632156, Training Accuracy: 43.512\n",
            "Worker 2, [04/08]: Training Loss: 2.043338760, Training Accuracy: 44.512\n",
            "Worker 2, [05/08]: Training Loss: 2.001581679, Training Accuracy: 45.312\n",
            "Worker 2, [06/08]: Training Loss: 1.938544054, Training Accuracy: 46.912\n",
            "Worker 2, [07/08]: Training Loss: 1.892600034, Training Accuracy: 47.512\n",
            "Worker 2, [08/08]: Training Loss: 1.835070662, Training Accuracy: 49.520\n",
            "Time taken for training worker 2: 0:00:46.243371\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.287739622, Training Accuracy: 39.352\n",
            "Worker 3, [02/08]: Training Loss: 2.195848126, Training Accuracy: 41.464\n",
            "Worker 3, [03/08]: Training Loss: 2.108840943, Training Accuracy: 43.216\n",
            "Worker 3, [04/08]: Training Loss: 2.050325958, Training Accuracy: 44.488\n",
            "Worker 3, [05/08]: Training Loss: 1.996385847, Training Accuracy: 45.176\n",
            "Worker 3, [06/08]: Training Loss: 1.953195444, Training Accuracy: 46.592\n",
            "Worker 3, [07/08]: Training Loss: 1.896688515, Training Accuracy: 47.928\n",
            "Worker 3, [08/08]: Training Loss: 1.877768160, Training Accuracy: 48.184\n",
            "Time taken for training worker 3: 0:00:48.587926\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.287089009, Training Accuracy: 39.848\n",
            "Worker 4, [02/08]: Training Loss: 2.212690286, Training Accuracy: 41.352\n",
            "Worker 4, [03/08]: Training Loss: 2.115864566, Training Accuracy: 42.760\n",
            "Worker 4, [04/08]: Training Loss: 2.040862985, Training Accuracy: 44.728\n",
            "Worker 4, [05/08]: Training Loss: 2.003112272, Training Accuracy: 45.120\n",
            "Worker 4, [06/08]: Training Loss: 1.944937803, Training Accuracy: 46.608\n",
            "Worker 4, [07/08]: Training Loss: 1.920614873, Training Accuracy: 47.024\n",
            "Worker 4, [08/08]: Training Loss: 1.845145702, Training Accuracy: 48.784\n",
            "Time taken for training worker 4: 0:00:46.763544\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002611\n",
            "Global Update 05: Test Loss: 2.040154165, Test Accuracy: 46.030\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.123705178, Training Accuracy: 43.136\n",
            "Worker 1, [02/08]: Training Loss: 2.019928545, Training Accuracy: 45.560\n",
            "Worker 1, [03/08]: Training Loss: 1.932975466, Training Accuracy: 47.256\n",
            "Worker 1, [04/08]: Training Loss: 1.882809808, Training Accuracy: 48.496\n",
            "Worker 1, [05/08]: Training Loss: 1.805171035, Training Accuracy: 50.048\n",
            "Worker 1, [06/08]: Training Loss: 1.786257720, Training Accuracy: 50.408\n",
            "Worker 1, [07/08]: Training Loss: 1.730388890, Training Accuracy: 51.392\n",
            "Worker 1, [08/08]: Training Loss: 1.680448539, Training Accuracy: 53.016\n",
            "Time taken for training worker 1: 0:00:48.792699\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.095161246, Training Accuracy: 43.672\n",
            "Worker 2, [02/08]: Training Loss: 1.998335402, Training Accuracy: 46.032\n",
            "Worker 2, [03/08]: Training Loss: 1.931223504, Training Accuracy: 47.160\n",
            "Worker 2, [04/08]: Training Loss: 1.848646266, Training Accuracy: 49.048\n",
            "Worker 2, [05/08]: Training Loss: 1.787307618, Training Accuracy: 49.560\n",
            "Worker 2, [06/08]: Training Loss: 1.754990324, Training Accuracy: 50.464\n",
            "Worker 2, [07/08]: Training Loss: 1.728110550, Training Accuracy: 52.064\n",
            "Worker 2, [08/08]: Training Loss: 1.664341886, Training Accuracy: 53.120\n",
            "Time taken for training worker 2: 0:00:47.499041\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.114912275, Training Accuracy: 43.720\n",
            "Worker 3, [02/08]: Training Loss: 2.011351434, Training Accuracy: 45.608\n",
            "Worker 3, [03/08]: Training Loss: 1.930675479, Training Accuracy: 46.880\n",
            "Worker 3, [04/08]: Training Loss: 1.872970000, Training Accuracy: 48.208\n",
            "Worker 3, [05/08]: Training Loss: 1.806099938, Training Accuracy: 49.688\n",
            "Worker 3, [06/08]: Training Loss: 1.749260837, Training Accuracy: 51.224\n",
            "Worker 3, [07/08]: Training Loss: 1.697463781, Training Accuracy: 52.384\n",
            "Worker 3, [08/08]: Training Loss: 1.673165081, Training Accuracy: 53.040\n",
            "Time taken for training worker 3: 0:00:48.864615\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.105862253, Training Accuracy: 43.624\n",
            "Worker 4, [02/08]: Training Loss: 2.010053289, Training Accuracy: 45.000\n",
            "Worker 4, [03/08]: Training Loss: 1.932110924, Training Accuracy: 47.160\n",
            "Worker 4, [04/08]: Training Loss: 1.870602127, Training Accuracy: 48.360\n",
            "Worker 4, [05/08]: Training Loss: 1.829910882, Training Accuracy: 49.264\n",
            "Worker 4, [06/08]: Training Loss: 1.769318527, Training Accuracy: 50.848\n",
            "Worker 4, [07/08]: Training Loss: 1.713908093, Training Accuracy: 52.664\n",
            "Worker 4, [08/08]: Training Loss: 1.675522251, Training Accuracy: 52.384\n",
            "Time taken for training worker 4: 0:00:47.304950\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002660\n",
            "Global Update 06: Test Loss: 1.991095796, Test Accuracy: 47.960\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.981444538, Training Accuracy: 46.872\n",
            "Worker 1, [02/08]: Training Loss: 1.861743308, Training Accuracy: 49.152\n",
            "Worker 1, [03/08]: Training Loss: 1.786875634, Training Accuracy: 50.144\n",
            "Worker 1, [04/08]: Training Loss: 1.704365888, Training Accuracy: 52.944\n",
            "Worker 1, [05/08]: Training Loss: 1.642971749, Training Accuracy: 53.424\n",
            "Worker 1, [06/08]: Training Loss: 1.602262402, Training Accuracy: 54.280\n",
            "Worker 1, [07/08]: Training Loss: 1.609103926, Training Accuracy: 54.472\n",
            "Worker 1, [08/08]: Training Loss: 1.507245805, Training Accuracy: 57.232\n",
            "Time taken for training worker 1: 0:00:48.641386\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.950421599, Training Accuracy: 46.832\n",
            "Worker 2, [02/08]: Training Loss: 1.847968013, Training Accuracy: 49.560\n",
            "Worker 2, [03/08]: Training Loss: 1.766044383, Training Accuracy: 50.792\n",
            "Worker 2, [04/08]: Training Loss: 1.690243857, Training Accuracy: 52.776\n",
            "Worker 2, [05/08]: Training Loss: 1.614474236, Training Accuracy: 55.112\n",
            "Worker 2, [06/08]: Training Loss: 1.588995243, Training Accuracy: 55.448\n",
            "Worker 2, [07/08]: Training Loss: 1.560372005, Training Accuracy: 55.416\n",
            "Worker 2, [08/08]: Training Loss: 1.506177377, Training Accuracy: 56.760\n",
            "Time taken for training worker 2: 0:00:47.667953\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.959670149, Training Accuracy: 47.360\n",
            "Worker 3, [02/08]: Training Loss: 1.842129723, Training Accuracy: 49.784\n",
            "Worker 3, [03/08]: Training Loss: 1.776300967, Training Accuracy: 50.792\n",
            "Worker 3, [04/08]: Training Loss: 1.687116149, Training Accuracy: 52.888\n",
            "Worker 3, [05/08]: Training Loss: 1.654890765, Training Accuracy: 53.624\n",
            "Worker 3, [06/08]: Training Loss: 1.591589227, Training Accuracy: 55.264\n",
            "Worker 3, [07/08]: Training Loss: 1.549230615, Training Accuracy: 56.688\n",
            "Worker 3, [08/08]: Training Loss: 1.509998570, Training Accuracy: 56.616\n",
            "Time taken for training worker 3: 0:00:46.683627\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.964095461, Training Accuracy: 46.936\n",
            "Worker 4, [02/08]: Training Loss: 1.859052810, Training Accuracy: 48.376\n",
            "Worker 4, [03/08]: Training Loss: 1.781035765, Training Accuracy: 50.984\n",
            "Worker 4, [04/08]: Training Loss: 1.722103517, Training Accuracy: 51.944\n",
            "Worker 4, [05/08]: Training Loss: 1.651325697, Training Accuracy: 53.416\n",
            "Worker 4, [06/08]: Training Loss: 1.608567373, Training Accuracy: 54.264\n",
            "Worker 4, [07/08]: Training Loss: 1.566988836, Training Accuracy: 55.480\n",
            "Worker 4, [08/08]: Training Loss: 1.512788360, Training Accuracy: 57.008\n",
            "Time taken for training worker 4: 0:00:47.892406\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002786\n",
            "Global Update 07: Test Loss: 1.958039595, Test Accuracy: 48.890\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.846881637, Training Accuracy: 49.536\n",
            "Worker 1, [02/08]: Training Loss: 1.701549478, Training Accuracy: 52.968\n",
            "Worker 1, [03/08]: Training Loss: 1.655815989, Training Accuracy: 53.896\n",
            "Worker 1, [04/08]: Training Loss: 1.555433037, Training Accuracy: 55.928\n",
            "Worker 1, [05/08]: Training Loss: 1.523731862, Training Accuracy: 56.648\n",
            "Worker 1, [06/08]: Training Loss: 1.459999229, Training Accuracy: 58.416\n",
            "Worker 1, [07/08]: Training Loss: 1.399375396, Training Accuracy: 60.248\n",
            "Worker 1, [08/08]: Training Loss: 1.375957135, Training Accuracy: 60.560\n",
            "Time taken for training worker 1: 0:00:47.071836\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.819630754, Training Accuracy: 50.216\n",
            "Worker 2, [02/08]: Training Loss: 1.699191128, Training Accuracy: 52.328\n",
            "Worker 2, [03/08]: Training Loss: 1.607779020, Training Accuracy: 54.912\n",
            "Worker 2, [04/08]: Training Loss: 1.564447466, Training Accuracy: 55.816\n",
            "Worker 2, [05/08]: Training Loss: 1.506834238, Training Accuracy: 57.000\n",
            "Worker 2, [06/08]: Training Loss: 1.435188071, Training Accuracy: 58.488\n",
            "Worker 2, [07/08]: Training Loss: 1.388984340, Training Accuracy: 59.992\n",
            "Worker 2, [08/08]: Training Loss: 1.330038718, Training Accuracy: 61.176\n",
            "Time taken for training worker 2: 0:00:47.908949\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.818412636, Training Accuracy: 49.776\n",
            "Worker 3, [02/08]: Training Loss: 1.697514752, Training Accuracy: 52.760\n",
            "Worker 3, [03/08]: Training Loss: 1.622744774, Training Accuracy: 54.792\n",
            "Worker 3, [04/08]: Training Loss: 1.571274566, Training Accuracy: 55.552\n",
            "Worker 3, [05/08]: Training Loss: 1.518188108, Training Accuracy: 57.304\n",
            "Worker 3, [06/08]: Training Loss: 1.453321701, Training Accuracy: 58.464\n",
            "Worker 3, [07/08]: Training Loss: 1.409855730, Training Accuracy: 59.528\n",
            "Worker 3, [08/08]: Training Loss: 1.356224187, Training Accuracy: 60.784\n",
            "Time taken for training worker 3: 0:00:46.833358\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.849894542, Training Accuracy: 49.264\n",
            "Worker 4, [02/08]: Training Loss: 1.711242459, Training Accuracy: 52.224\n",
            "Worker 4, [03/08]: Training Loss: 1.636939862, Training Accuracy: 54.352\n",
            "Worker 4, [04/08]: Training Loss: 1.586037955, Training Accuracy: 55.200\n",
            "Worker 4, [05/08]: Training Loss: 1.515206303, Training Accuracy: 56.496\n",
            "Worker 4, [06/08]: Training Loss: 1.454800071, Training Accuracy: 58.088\n",
            "Worker 4, [07/08]: Training Loss: 1.411054354, Training Accuracy: 59.120\n",
            "Worker 4, [08/08]: Training Loss: 1.373008038, Training Accuracy: 60.568\n",
            "Time taken for training worker 4: 0:00:47.516078\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002679\n",
            "Global Update 08: Test Loss: 1.929923540, Test Accuracy: 49.660\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.744564455, Training Accuracy: 51.984\n",
            "Worker 1, [02/08]: Training Loss: 1.608714816, Training Accuracy: 55.064\n",
            "Worker 1, [03/08]: Training Loss: 1.503272045, Training Accuracy: 57.600\n",
            "Worker 1, [04/08]: Training Loss: 1.426577953, Training Accuracy: 59.232\n",
            "Worker 1, [05/08]: Training Loss: 1.376876936, Training Accuracy: 60.832\n",
            "Worker 1, [06/08]: Training Loss: 1.347884781, Training Accuracy: 61.304\n",
            "Worker 1, [07/08]: Training Loss: 1.280522873, Training Accuracy: 63.008\n",
            "Worker 1, [08/08]: Training Loss: 1.253746320, Training Accuracy: 63.824\n",
            "Time taken for training worker 1: 0:00:46.795842\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.708821240, Training Accuracy: 52.496\n",
            "Worker 2, [02/08]: Training Loss: 1.566009965, Training Accuracy: 55.904\n",
            "Worker 2, [03/08]: Training Loss: 1.461234628, Training Accuracy: 58.096\n",
            "Worker 2, [04/08]: Training Loss: 1.419475218, Training Accuracy: 59.496\n",
            "Worker 2, [05/08]: Training Loss: 1.367248237, Training Accuracy: 61.104\n",
            "Worker 2, [06/08]: Training Loss: 1.298137709, Training Accuracy: 62.136\n",
            "Worker 2, [07/08]: Training Loss: 1.262375803, Training Accuracy: 63.160\n",
            "Worker 2, [08/08]: Training Loss: 1.220424181, Training Accuracy: 64.528\n",
            "Time taken for training worker 2: 0:00:48.018297\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.731771315, Training Accuracy: 52.216\n",
            "Worker 3, [02/08]: Training Loss: 1.568369489, Training Accuracy: 55.568\n",
            "Worker 3, [03/08]: Training Loss: 1.479403301, Training Accuracy: 57.952\n",
            "Worker 3, [04/08]: Training Loss: 1.414649893, Training Accuracy: 59.632\n",
            "Worker 3, [05/08]: Training Loss: 1.378682527, Training Accuracy: 60.280\n",
            "Worker 3, [06/08]: Training Loss: 1.317474852, Training Accuracy: 62.168\n",
            "Worker 3, [07/08]: Training Loss: 1.276371187, Training Accuracy: 63.008\n",
            "Worker 3, [08/08]: Training Loss: 1.239588134, Training Accuracy: 63.880\n",
            "Time taken for training worker 3: 0:00:47.328751\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.723472294, Training Accuracy: 51.976\n",
            "Worker 4, [02/08]: Training Loss: 1.572822743, Training Accuracy: 55.624\n",
            "Worker 4, [03/08]: Training Loss: 1.504042528, Training Accuracy: 57.376\n",
            "Worker 4, [04/08]: Training Loss: 1.425889641, Training Accuracy: 58.624\n",
            "Worker 4, [05/08]: Training Loss: 1.394443569, Training Accuracy: 60.088\n",
            "Worker 4, [06/08]: Training Loss: 1.344607526, Training Accuracy: 60.968\n",
            "Worker 4, [07/08]: Training Loss: 1.261084765, Training Accuracy: 63.136\n",
            "Worker 4, [08/08]: Training Loss: 1.246461391, Training Accuracy: 63.896\n",
            "Time taken for training worker 4: 0:00:48.320370\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002538\n",
            "Global Update 09: Test Loss: 1.911306529, Test Accuracy: 50.480\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.607518331, Training Accuracy: 55.184\n",
            "Worker 1, [02/08]: Training Loss: 1.461515265, Training Accuracy: 57.904\n",
            "Worker 1, [03/08]: Training Loss: 1.362920118, Training Accuracy: 61.088\n",
            "Worker 1, [04/08]: Training Loss: 1.323261734, Training Accuracy: 62.232\n",
            "Worker 1, [05/08]: Training Loss: 1.251279016, Training Accuracy: 63.552\n",
            "Worker 1, [06/08]: Training Loss: 1.191547961, Training Accuracy: 65.472\n",
            "Worker 1, [07/08]: Training Loss: 1.150279404, Training Accuracy: 66.920\n",
            "Worker 1, [08/08]: Training Loss: 1.126346954, Training Accuracy: 67.208\n",
            "Time taken for training worker 1: 0:00:47.676145\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.602647582, Training Accuracy: 54.688\n",
            "Worker 2, [02/08]: Training Loss: 1.444444018, Training Accuracy: 58.800\n",
            "Worker 2, [03/08]: Training Loss: 1.353015823, Training Accuracy: 61.128\n",
            "Worker 2, [04/08]: Training Loss: 1.275146658, Training Accuracy: 62.504\n",
            "Worker 2, [05/08]: Training Loss: 1.231589801, Training Accuracy: 64.256\n",
            "Worker 2, [06/08]: Training Loss: 1.191561342, Training Accuracy: 65.272\n",
            "Worker 2, [07/08]: Training Loss: 1.114175409, Training Accuracy: 67.024\n",
            "Worker 2, [08/08]: Training Loss: 1.102048237, Training Accuracy: 66.984\n",
            "Time taken for training worker 2: 0:00:48.190227\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.604234771, Training Accuracy: 55.064\n",
            "Worker 3, [02/08]: Training Loss: 1.461172877, Training Accuracy: 58.584\n",
            "Worker 3, [03/08]: Training Loss: 1.349822933, Training Accuracy: 61.456\n",
            "Worker 3, [04/08]: Training Loss: 1.288215075, Training Accuracy: 62.024\n",
            "Worker 3, [05/08]: Training Loss: 1.249607194, Training Accuracy: 63.728\n",
            "Worker 3, [06/08]: Training Loss: 1.186859282, Training Accuracy: 65.728\n",
            "Worker 3, [07/08]: Training Loss: 1.182080176, Training Accuracy: 65.448\n",
            "Worker 3, [08/08]: Training Loss: 1.111923572, Training Accuracy: 67.168\n",
            "Time taken for training worker 3: 0:00:47.580493\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.615727800, Training Accuracy: 55.176\n",
            "Worker 4, [02/08]: Training Loss: 1.454391445, Training Accuracy: 57.912\n",
            "Worker 4, [03/08]: Training Loss: 1.393256275, Training Accuracy: 60.176\n",
            "Worker 4, [04/08]: Training Loss: 1.319203292, Training Accuracy: 61.680\n",
            "Worker 4, [05/08]: Training Loss: 1.237830128, Training Accuracy: 63.872\n",
            "Worker 4, [06/08]: Training Loss: 1.186248491, Training Accuracy: 64.888\n",
            "Worker 4, [07/08]: Training Loss: 1.169239387, Training Accuracy: 65.976\n",
            "Worker 4, [08/08]: Training Loss: 1.100812912, Training Accuracy: 67.312\n",
            "Time taken for training worker 4: 0:00:46.508824\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002541\n",
            "Global Update 10: Test Loss: 1.932201306, Test Accuracy: 51.050\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.504808211, Training Accuracy: 57.520\n",
            "Worker 1, [02/08]: Training Loss: 1.365279602, Training Accuracy: 60.960\n",
            "Worker 1, [03/08]: Training Loss: 1.247217069, Training Accuracy: 64.136\n",
            "Worker 1, [04/08]: Training Loss: 1.192418350, Training Accuracy: 65.056\n",
            "Worker 1, [05/08]: Training Loss: 1.143986443, Training Accuracy: 66.608\n",
            "Worker 1, [06/08]: Training Loss: 1.070497924, Training Accuracy: 68.672\n",
            "Worker 1, [07/08]: Training Loss: 1.045905152, Training Accuracy: 69.760\n",
            "Worker 1, [08/08]: Training Loss: 0.994347668, Training Accuracy: 70.104\n",
            "Time taken for training worker 1: 0:00:46.862099\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.462755097, Training Accuracy: 58.088\n",
            "Worker 2, [02/08]: Training Loss: 1.335283391, Training Accuracy: 61.608\n",
            "Worker 2, [03/08]: Training Loss: 1.235906257, Training Accuracy: 63.760\n",
            "Worker 2, [04/08]: Training Loss: 1.157562282, Training Accuracy: 65.824\n",
            "Worker 2, [05/08]: Training Loss: 1.116071164, Training Accuracy: 66.728\n",
            "Worker 2, [06/08]: Training Loss: 1.055059685, Training Accuracy: 68.840\n",
            "Worker 2, [07/08]: Training Loss: 1.033328517, Training Accuracy: 69.272\n",
            "Worker 2, [08/08]: Training Loss: 0.977145936, Training Accuracy: 70.824\n",
            "Time taken for training worker 2: 0:00:46.895339\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.513641446, Training Accuracy: 57.184\n",
            "Worker 3, [02/08]: Training Loss: 1.354309722, Training Accuracy: 61.136\n",
            "Worker 3, [03/08]: Training Loss: 1.254665351, Training Accuracy: 64.008\n",
            "Worker 3, [04/08]: Training Loss: 1.173399333, Training Accuracy: 65.640\n",
            "Worker 3, [05/08]: Training Loss: 1.121275056, Training Accuracy: 67.216\n",
            "Worker 3, [06/08]: Training Loss: 1.089070693, Training Accuracy: 68.184\n",
            "Worker 3, [07/08]: Training Loss: 1.058718092, Training Accuracy: 68.840\n",
            "Worker 3, [08/08]: Training Loss: 0.996160603, Training Accuracy: 70.488\n",
            "Time taken for training worker 3: 0:00:46.767958\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.505947579, Training Accuracy: 57.152\n",
            "Worker 4, [02/08]: Training Loss: 1.338304223, Training Accuracy: 61.664\n",
            "Worker 4, [03/08]: Training Loss: 1.241710403, Training Accuracy: 63.912\n",
            "Worker 4, [04/08]: Training Loss: 1.192050485, Training Accuracy: 65.536\n",
            "Worker 4, [05/08]: Training Loss: 1.132160223, Training Accuracy: 67.040\n",
            "Worker 4, [06/08]: Training Loss: 1.075208445, Training Accuracy: 68.624\n",
            "Worker 4, [07/08]: Training Loss: 1.039418220, Training Accuracy: 69.408\n",
            "Worker 4, [08/08]: Training Loss: 0.999873974, Training Accuracy: 70.152\n",
            "Time taken for training worker 4: 0:00:49.119600\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002856\n",
            "Global Update 11: Test Loss: 1.936222262, Test Accuracy: 52.090\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.407221339, Training Accuracy: 59.176\n",
            "Worker 1, [02/08]: Training Loss: 1.239835173, Training Accuracy: 64.096\n",
            "Worker 1, [03/08]: Training Loss: 1.162276071, Training Accuracy: 66.240\n",
            "Worker 1, [04/08]: Training Loss: 1.082161879, Training Accuracy: 68.720\n",
            "Worker 1, [05/08]: Training Loss: 1.037504494, Training Accuracy: 69.640\n",
            "Worker 1, [06/08]: Training Loss: 0.983416687, Training Accuracy: 71.008\n",
            "Worker 1, [07/08]: Training Loss: 0.939513759, Training Accuracy: 72.152\n",
            "Worker 1, [08/08]: Training Loss: 0.907216051, Training Accuracy: 73.328\n",
            "Time taken for training worker 1: 0:00:46.452008\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.359575394, Training Accuracy: 60.456\n",
            "Worker 2, [02/08]: Training Loss: 1.234756352, Training Accuracy: 64.192\n",
            "Worker 2, [03/08]: Training Loss: 1.123695201, Training Accuracy: 67.528\n",
            "Worker 2, [04/08]: Training Loss: 1.056219126, Training Accuracy: 68.656\n",
            "Worker 2, [05/08]: Training Loss: 0.992034162, Training Accuracy: 71.016\n",
            "Worker 2, [06/08]: Training Loss: 0.945533560, Training Accuracy: 72.224\n",
            "Worker 2, [07/08]: Training Loss: 0.889207673, Training Accuracy: 73.432\n",
            "Worker 2, [08/08]: Training Loss: 0.886121777, Training Accuracy: 73.816\n",
            "Time taken for training worker 2: 0:00:47.138992\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.373286519, Training Accuracy: 60.544\n",
            "Worker 3, [02/08]: Training Loss: 1.237087716, Training Accuracy: 64.512\n",
            "Worker 3, [03/08]: Training Loss: 1.131936217, Training Accuracy: 66.840\n",
            "Worker 3, [04/08]: Training Loss: 1.078017780, Training Accuracy: 69.056\n",
            "Worker 3, [05/08]: Training Loss: 1.012776575, Training Accuracy: 70.272\n",
            "Worker 3, [06/08]: Training Loss: 0.978705094, Training Accuracy: 70.768\n",
            "Worker 3, [07/08]: Training Loss: 0.935707933, Training Accuracy: 72.312\n",
            "Worker 3, [08/08]: Training Loss: 0.894062195, Training Accuracy: 73.296\n",
            "Time taken for training worker 3: 0:00:46.017527\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.406682987, Training Accuracy: 59.312\n",
            "Worker 4, [02/08]: Training Loss: 1.231100257, Training Accuracy: 64.344\n",
            "Worker 4, [03/08]: Training Loss: 1.140076563, Training Accuracy: 66.456\n",
            "Worker 4, [04/08]: Training Loss: 1.075069046, Training Accuracy: 68.136\n",
            "Worker 4, [05/08]: Training Loss: 1.012475323, Training Accuracy: 70.336\n",
            "Worker 4, [06/08]: Training Loss: 0.981364685, Training Accuracy: 71.064\n",
            "Worker 4, [07/08]: Training Loss: 0.924815704, Training Accuracy: 72.048\n",
            "Worker 4, [08/08]: Training Loss: 0.910282543, Training Accuracy: 72.768\n",
            "Time taken for training worker 4: 0:00:48.005423\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.009717\n",
            "Global Update 12: Test Loss: 1.953908456, Test Accuracy: 52.030\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.292709447, Training Accuracy: 62.512\n",
            "Worker 1, [02/08]: Training Loss: 1.166881136, Training Accuracy: 66.072\n",
            "Worker 1, [03/08]: Training Loss: 1.049343473, Training Accuracy: 69.624\n",
            "Worker 1, [04/08]: Training Loss: 0.979384994, Training Accuracy: 71.600\n",
            "Worker 1, [05/08]: Training Loss: 0.937614208, Training Accuracy: 72.536\n",
            "Worker 1, [06/08]: Training Loss: 0.888471712, Training Accuracy: 73.720\n",
            "Worker 1, [07/08]: Training Loss: 0.854787304, Training Accuracy: 74.920\n",
            "Worker 1, [08/08]: Training Loss: 0.816001456, Training Accuracy: 75.888\n",
            "Time taken for training worker 1: 0:00:49.365498\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.282879305, Training Accuracy: 62.968\n",
            "Worker 2, [02/08]: Training Loss: 1.118824086, Training Accuracy: 67.200\n",
            "Worker 2, [03/08]: Training Loss: 1.012205745, Training Accuracy: 70.416\n",
            "Worker 2, [04/08]: Training Loss: 0.959819592, Training Accuracy: 71.544\n",
            "Worker 2, [05/08]: Training Loss: 0.922611267, Training Accuracy: 72.448\n",
            "Worker 2, [06/08]: Training Loss: 0.871175804, Training Accuracy: 74.200\n",
            "Worker 2, [07/08]: Training Loss: 0.810026592, Training Accuracy: 75.712\n",
            "Worker 2, [08/08]: Training Loss: 0.791949116, Training Accuracy: 76.272\n",
            "Time taken for training worker 2: 0:00:46.785266\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.282248854, Training Accuracy: 63.104\n",
            "Worker 3, [02/08]: Training Loss: 1.143031077, Training Accuracy: 66.872\n",
            "Worker 3, [03/08]: Training Loss: 1.055749585, Training Accuracy: 69.440\n",
            "Worker 3, [04/08]: Training Loss: 0.980607174, Training Accuracy: 71.464\n",
            "Worker 3, [05/08]: Training Loss: 0.936540559, Training Accuracy: 72.960\n",
            "Worker 3, [06/08]: Training Loss: 0.882837249, Training Accuracy: 74.152\n",
            "Worker 3, [07/08]: Training Loss: 0.858993496, Training Accuracy: 74.600\n",
            "Worker 3, [08/08]: Training Loss: 0.808795847, Training Accuracy: 75.744\n",
            "Time taken for training worker 3: 0:00:47.634132\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.291815039, Training Accuracy: 62.464\n",
            "Worker 4, [02/08]: Training Loss: 1.141816983, Training Accuracy: 66.544\n",
            "Worker 4, [03/08]: Training Loss: 1.055660207, Training Accuracy: 69.192\n",
            "Worker 4, [04/08]: Training Loss: 0.985405600, Training Accuracy: 71.024\n",
            "Worker 4, [05/08]: Training Loss: 0.922309419, Training Accuracy: 72.912\n",
            "Worker 4, [06/08]: Training Loss: 0.878758375, Training Accuracy: 74.008\n",
            "Worker 4, [07/08]: Training Loss: 0.842781675, Training Accuracy: 75.024\n",
            "Worker 4, [08/08]: Training Loss: 0.815100060, Training Accuracy: 75.720\n",
            "Time taken for training worker 4: 0:00:46.938007\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002605\n",
            "Global Update 13: Test Loss: 1.984901122, Test Accuracy: 52.230\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.203539642, Training Accuracy: 65.112\n",
            "Worker 1, [02/08]: Training Loss: 1.069547608, Training Accuracy: 68.728\n",
            "Worker 1, [03/08]: Training Loss: 0.988647077, Training Accuracy: 71.024\n",
            "Worker 1, [04/08]: Training Loss: 0.924356454, Training Accuracy: 73.456\n",
            "Worker 1, [05/08]: Training Loss: 0.864370103, Training Accuracy: 74.264\n",
            "Worker 1, [06/08]: Training Loss: 0.801866275, Training Accuracy: 76.072\n",
            "Worker 1, [07/08]: Training Loss: 0.793050562, Training Accuracy: 76.848\n",
            "Worker 1, [08/08]: Training Loss: 0.749101975, Training Accuracy: 78.024\n",
            "Time taken for training worker 1: 0:00:48.439959\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.180144201, Training Accuracy: 65.088\n",
            "Worker 2, [02/08]: Training Loss: 1.033560481, Training Accuracy: 69.936\n",
            "Worker 2, [03/08]: Training Loss: 0.940142278, Training Accuracy: 71.944\n",
            "Worker 2, [04/08]: Training Loss: 0.888617115, Training Accuracy: 73.568\n",
            "Worker 2, [05/08]: Training Loss: 0.821215409, Training Accuracy: 75.832\n",
            "Worker 2, [06/08]: Training Loss: 0.803008904, Training Accuracy: 76.344\n",
            "Worker 2, [07/08]: Training Loss: 0.757565552, Training Accuracy: 77.720\n",
            "Worker 2, [08/08]: Training Loss: 0.720522108, Training Accuracy: 78.360\n",
            "Time taken for training worker 2: 0:00:48.123632\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.203056753, Training Accuracy: 65.312\n",
            "Worker 3, [02/08]: Training Loss: 1.052852523, Training Accuracy: 69.344\n",
            "Worker 3, [03/08]: Training Loss: 0.971159962, Training Accuracy: 71.320\n",
            "Worker 3, [04/08]: Training Loss: 0.908304633, Training Accuracy: 73.432\n",
            "Worker 3, [05/08]: Training Loss: 0.847907536, Training Accuracy: 75.608\n",
            "Worker 3, [06/08]: Training Loss: 0.826411872, Training Accuracy: 75.424\n",
            "Worker 3, [07/08]: Training Loss: 0.763168546, Training Accuracy: 77.352\n",
            "Worker 3, [08/08]: Training Loss: 0.717492128, Training Accuracy: 78.752\n",
            "Time taken for training worker 3: 0:00:47.840164\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.213402107, Training Accuracy: 64.360\n",
            "Worker 4, [02/08]: Training Loss: 1.059689523, Training Accuracy: 68.864\n",
            "Worker 4, [03/08]: Training Loss: 0.961265291, Training Accuracy: 71.376\n",
            "Worker 4, [04/08]: Training Loss: 0.902780463, Training Accuracy: 73.680\n",
            "Worker 4, [05/08]: Training Loss: 0.845330821, Training Accuracy: 74.784\n",
            "Worker 4, [06/08]: Training Loss: 0.814704891, Training Accuracy: 76.040\n",
            "Worker 4, [07/08]: Training Loss: 0.785865108, Training Accuracy: 76.384\n",
            "Worker 4, [08/08]: Training Loss: 0.757191813, Training Accuracy: 77.360\n",
            "Time taken for training worker 4: 0:00:47.836905\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002476\n",
            "Global Update 14: Test Loss: 2.012378694, Test Accuracy: 52.630\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.114708571, Training Accuracy: 66.896\n",
            "Worker 1, [02/08]: Training Loss: 0.989622967, Training Accuracy: 71.192\n",
            "Worker 1, [03/08]: Training Loss: 0.917539083, Training Accuracy: 73.008\n",
            "Worker 1, [04/08]: Training Loss: 0.867352032, Training Accuracy: 74.904\n",
            "Worker 1, [05/08]: Training Loss: 0.806377048, Training Accuracy: 76.456\n",
            "Worker 1, [06/08]: Training Loss: 0.776288177, Training Accuracy: 77.448\n",
            "Worker 1, [07/08]: Training Loss: 0.743814342, Training Accuracy: 78.392\n",
            "Worker 1, [08/08]: Training Loss: 0.708863259, Training Accuracy: 79.352\n",
            "Time taken for training worker 1: 0:00:47.981051\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.101682233, Training Accuracy: 66.984\n",
            "Worker 2, [02/08]: Training Loss: 0.964937783, Training Accuracy: 71.320\n",
            "Worker 2, [03/08]: Training Loss: 0.891388208, Training Accuracy: 73.800\n",
            "Worker 2, [04/08]: Training Loss: 0.838684041, Training Accuracy: 75.856\n",
            "Worker 2, [05/08]: Training Loss: 0.784472718, Training Accuracy: 76.536\n",
            "Worker 2, [06/08]: Training Loss: 0.751109128, Training Accuracy: 78.192\n",
            "Worker 2, [07/08]: Training Loss: 0.701582856, Training Accuracy: 79.536\n",
            "Worker 2, [08/08]: Training Loss: 0.658047679, Training Accuracy: 80.896\n",
            "Time taken for training worker 2: 0:00:48.780929\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.118333133, Training Accuracy: 66.888\n",
            "Worker 3, [02/08]: Training Loss: 0.988385132, Training Accuracy: 70.824\n",
            "Worker 3, [03/08]: Training Loss: 0.911236913, Training Accuracy: 72.952\n",
            "Worker 3, [04/08]: Training Loss: 0.868720672, Training Accuracy: 74.736\n",
            "Worker 3, [05/08]: Training Loss: 0.804426003, Training Accuracy: 76.488\n",
            "Worker 3, [06/08]: Training Loss: 0.766174469, Training Accuracy: 77.344\n",
            "Worker 3, [07/08]: Training Loss: 0.732556746, Training Accuracy: 78.760\n",
            "Worker 3, [08/08]: Training Loss: 0.702577019, Training Accuracy: 79.560\n",
            "Time taken for training worker 3: 0:00:48.367002\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.121196140, Training Accuracy: 67.144\n",
            "Worker 4, [02/08]: Training Loss: 1.006726650, Training Accuracy: 69.992\n",
            "Worker 4, [03/08]: Training Loss: 0.921677803, Training Accuracy: 72.992\n",
            "Worker 4, [04/08]: Training Loss: 0.853422865, Training Accuracy: 74.608\n",
            "Worker 4, [05/08]: Training Loss: 0.819371008, Training Accuracy: 75.784\n",
            "Worker 4, [06/08]: Training Loss: 0.772347557, Training Accuracy: 77.616\n",
            "Worker 4, [07/08]: Training Loss: 0.737167281, Training Accuracy: 78.488\n",
            "Worker 4, [08/08]: Training Loss: 0.713191859, Training Accuracy: 78.856\n",
            "Time taken for training worker 4: 0:00:48.580287\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002796\n",
            "Global Update 15: Test Loss: 2.026890353, Test Accuracy: 52.470\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.054806339, Training Accuracy: 69.112\n",
            "Worker 1, [02/08]: Training Loss: 0.951235215, Training Accuracy: 72.040\n",
            "Worker 1, [03/08]: Training Loss: 0.903351454, Training Accuracy: 73.472\n",
            "Worker 1, [04/08]: Training Loss: 0.844312558, Training Accuracy: 75.392\n",
            "Worker 1, [05/08]: Training Loss: 0.822149030, Training Accuracy: 75.960\n",
            "Worker 1, [06/08]: Training Loss: 0.770850182, Training Accuracy: 77.840\n",
            "Worker 1, [07/08]: Training Loss: 0.744057008, Training Accuracy: 78.384\n",
            "Worker 1, [08/08]: Training Loss: 0.727713894, Training Accuracy: 78.576\n",
            "Time taken for training worker 1: 0:00:47.579470\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.026714098, Training Accuracy: 69.440\n",
            "Worker 2, [02/08]: Training Loss: 0.919955657, Training Accuracy: 72.952\n",
            "Worker 2, [03/08]: Training Loss: 0.869429106, Training Accuracy: 74.656\n",
            "Worker 2, [04/08]: Training Loss: 0.819060252, Training Accuracy: 75.840\n",
            "Worker 2, [05/08]: Training Loss: 0.773804622, Training Accuracy: 77.360\n",
            "Worker 2, [06/08]: Training Loss: 0.739741366, Training Accuracy: 78.504\n",
            "Worker 2, [07/08]: Training Loss: 0.712088902, Training Accuracy: 79.096\n",
            "Worker 2, [08/08]: Training Loss: 0.690223312, Training Accuracy: 79.904\n",
            "Time taken for training worker 2: 0:00:47.607073\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.050738277, Training Accuracy: 69.248\n",
            "Worker 3, [02/08]: Training Loss: 0.949481469, Training Accuracy: 72.048\n",
            "Worker 3, [03/08]: Training Loss: 0.880336708, Training Accuracy: 74.144\n",
            "Worker 3, [04/08]: Training Loss: 0.850044884, Training Accuracy: 75.488\n",
            "Worker 3, [05/08]: Training Loss: 0.808554517, Training Accuracy: 76.368\n",
            "Worker 3, [06/08]: Training Loss: 0.769512113, Training Accuracy: 77.224\n",
            "Worker 3, [07/08]: Training Loss: 0.739583358, Training Accuracy: 78.464\n",
            "Worker 3, [08/08]: Training Loss: 0.717384665, Training Accuracy: 79.208\n",
            "Time taken for training worker 3: 0:00:47.837084\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.059528525, Training Accuracy: 68.776\n",
            "Worker 4, [02/08]: Training Loss: 0.954705090, Training Accuracy: 71.640\n",
            "Worker 4, [03/08]: Training Loss: 0.893909271, Training Accuracy: 73.712\n",
            "Worker 4, [04/08]: Training Loss: 0.843760159, Training Accuracy: 75.080\n",
            "Worker 4, [05/08]: Training Loss: 0.806312541, Training Accuracy: 76.408\n",
            "Worker 4, [06/08]: Training Loss: 0.778991886, Training Accuracy: 77.008\n",
            "Worker 4, [07/08]: Training Loss: 0.744364777, Training Accuracy: 77.928\n",
            "Worker 4, [08/08]: Training Loss: 0.715579800, Training Accuracy: 79.424\n",
            "Time taken for training worker 4: 0:00:48.865587\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002695\n",
            "Global Update 16: Test Loss: 2.017267652, Test Accuracy: 52.530\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.011652681, Training Accuracy: 70.520\n",
            "Worker 1, [02/08]: Training Loss: 0.943261311, Training Accuracy: 72.448\n",
            "Worker 1, [03/08]: Training Loss: 0.892132813, Training Accuracy: 73.744\n",
            "Worker 1, [04/08]: Training Loss: 0.863536338, Training Accuracy: 74.624\n",
            "Worker 1, [05/08]: Training Loss: 0.847984637, Training Accuracy: 75.392\n",
            "Worker 1, [06/08]: Training Loss: 0.803294563, Training Accuracy: 76.616\n",
            "Worker 1, [07/08]: Training Loss: 0.788832598, Training Accuracy: 76.872\n",
            "Worker 1, [08/08]: Training Loss: 0.775414286, Training Accuracy: 77.512\n",
            "Time taken for training worker 1: 0:00:48.505388\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 0.975234305, Training Accuracy: 70.720\n",
            "Worker 2, [02/08]: Training Loss: 0.914251539, Training Accuracy: 72.840\n",
            "Worker 2, [03/08]: Training Loss: 0.877543271, Training Accuracy: 73.800\n",
            "Worker 2, [04/08]: Training Loss: 0.834568160, Training Accuracy: 75.088\n",
            "Worker 2, [05/08]: Training Loss: 0.808494991, Training Accuracy: 76.496\n",
            "Worker 2, [06/08]: Training Loss: 0.776348232, Training Accuracy: 77.320\n",
            "Worker 2, [07/08]: Training Loss: 0.758399423, Training Accuracy: 77.576\n",
            "Worker 2, [08/08]: Training Loss: 0.746704916, Training Accuracy: 78.048\n",
            "Time taken for training worker 2: 0:00:48.210030\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.016064946, Training Accuracy: 70.320\n",
            "Worker 3, [02/08]: Training Loss: 0.944727701, Training Accuracy: 72.104\n",
            "Worker 3, [03/08]: Training Loss: 0.895153529, Training Accuracy: 73.840\n",
            "Worker 3, [04/08]: Training Loss: 0.859842247, Training Accuracy: 74.528\n",
            "Worker 3, [05/08]: Training Loss: 0.839238484, Training Accuracy: 75.712\n",
            "Worker 3, [06/08]: Training Loss: 0.806857885, Training Accuracy: 76.304\n",
            "Worker 3, [07/08]: Training Loss: 0.800085457, Training Accuracy: 76.560\n",
            "Worker 3, [08/08]: Training Loss: 0.777585174, Training Accuracy: 77.416\n",
            "Time taken for training worker 3: 0:00:47.257946\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.002853515, Training Accuracy: 70.024\n",
            "Worker 4, [02/08]: Training Loss: 0.947243927, Training Accuracy: 71.704\n",
            "Worker 4, [03/08]: Training Loss: 0.898685652, Training Accuracy: 73.608\n",
            "Worker 4, [04/08]: Training Loss: 0.870833000, Training Accuracy: 74.600\n",
            "Worker 4, [05/08]: Training Loss: 0.842790091, Training Accuracy: 75.048\n",
            "Worker 4, [06/08]: Training Loss: 0.813871412, Training Accuracy: 75.968\n",
            "Worker 4, [07/08]: Training Loss: 0.791682051, Training Accuracy: 76.856\n",
            "Worker 4, [08/08]: Training Loss: 0.766163688, Training Accuracy: 77.776\n",
            "Time taken for training worker 4: 0:00:47.472944\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002685\n",
            "Global Update 17: Test Loss: 2.005406363, Test Accuracy: 52.730\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 0.973454230, Training Accuracy: 71.152\n",
            "Worker 1, [02/08]: Training Loss: 0.967275852, Training Accuracy: 71.592\n",
            "Worker 1, [03/08]: Training Loss: 0.937737627, Training Accuracy: 72.136\n",
            "Worker 1, [04/08]: Training Loss: 0.919484195, Training Accuracy: 73.176\n",
            "Worker 1, [05/08]: Training Loss: 0.905154340, Training Accuracy: 73.712\n",
            "Worker 1, [06/08]: Training Loss: 0.895397247, Training Accuracy: 73.456\n",
            "Worker 1, [07/08]: Training Loss: 0.883131610, Training Accuracy: 74.272\n",
            "Worker 1, [08/08]: Training Loss: 0.875291899, Training Accuracy: 74.600\n",
            "Time taken for training worker 1: 0:00:48.764456\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 0.971791010, Training Accuracy: 70.976\n",
            "Worker 2, [02/08]: Training Loss: 0.927475570, Training Accuracy: 72.280\n",
            "Worker 2, [03/08]: Training Loss: 0.914811255, Training Accuracy: 72.800\n",
            "Worker 2, [04/08]: Training Loss: 0.899159555, Training Accuracy: 73.584\n",
            "Worker 2, [05/08]: Training Loss: 0.873747343, Training Accuracy: 74.056\n",
            "Worker 2, [06/08]: Training Loss: 0.863150238, Training Accuracy: 73.920\n",
            "Worker 2, [07/08]: Training Loss: 0.854749393, Training Accuracy: 74.672\n",
            "Worker 2, [08/08]: Training Loss: 0.840331911, Training Accuracy: 75.112\n",
            "Time taken for training worker 2: 0:00:47.501500\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 0.988715249, Training Accuracy: 71.056\n",
            "Worker 3, [02/08]: Training Loss: 0.947432623, Training Accuracy: 71.944\n",
            "Worker 3, [03/08]: Training Loss: 0.922302158, Training Accuracy: 72.816\n",
            "Worker 3, [04/08]: Training Loss: 0.925665989, Training Accuracy: 72.448\n",
            "Worker 3, [05/08]: Training Loss: 0.899938216, Training Accuracy: 73.376\n",
            "Worker 3, [06/08]: Training Loss: 0.903659170, Training Accuracy: 73.728\n",
            "Worker 3, [07/08]: Training Loss: 0.877824264, Training Accuracy: 73.944\n",
            "Worker 3, [08/08]: Training Loss: 0.871547034, Training Accuracy: 74.656\n",
            "Time taken for training worker 3: 0:00:49.008317\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 0.988247367, Training Accuracy: 70.936\n",
            "Worker 4, [02/08]: Training Loss: 0.957660225, Training Accuracy: 71.600\n",
            "Worker 4, [03/08]: Training Loss: 0.950310201, Training Accuracy: 72.376\n",
            "Worker 4, [04/08]: Training Loss: 0.923168894, Training Accuracy: 72.760\n",
            "Worker 4, [05/08]: Training Loss: 0.911425683, Training Accuracy: 73.480\n",
            "Worker 4, [06/08]: Training Loss: 0.886032138, Training Accuracy: 74.016\n",
            "Worker 4, [07/08]: Training Loss: 0.879495857, Training Accuracy: 73.864\n",
            "Worker 4, [08/08]: Training Loss: 0.873488752, Training Accuracy: 74.520\n",
            "Time taken for training worker 4: 0:00:49.023738\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002940\n",
            "Global Update 18: Test Loss: 1.986871252, Test Accuracy: 52.610\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:57:40.159363\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:16\n",
            "==================================================\n",
            "Worker 1, [01/16]: Training Loss: 4.490180191, Training Accuracy: 2.448\n",
            "Worker 1, [02/16]: Training Loss: 4.117218886, Training Accuracy: 6.344\n",
            "Worker 1, [03/16]: Training Loss: 3.886120242, Training Accuracy: 9.656\n",
            "Worker 1, [04/16]: Training Loss: 3.729690328, Training Accuracy: 12.256\n",
            "Worker 1, [05/16]: Training Loss: 3.586262868, Training Accuracy: 14.808\n",
            "Worker 1, [06/16]: Training Loss: 3.440018158, Training Accuracy: 17.144\n",
            "Worker 1, [07/16]: Training Loss: 3.340647693, Training Accuracy: 18.320\n",
            "Worker 1, [08/16]: Training Loss: 3.224996789, Training Accuracy: 20.728\n",
            "Worker 1, [09/16]: Training Loss: 3.133162951, Training Accuracy: 22.376\n",
            "Worker 1, [10/16]: Training Loss: 3.076428743, Training Accuracy: 23.440\n",
            "Worker 1, [11/16]: Training Loss: 3.002579183, Training Accuracy: 24.984\n",
            "Worker 1, [12/16]: Training Loss: 2.918431435, Training Accuracy: 26.576\n",
            "Worker 1, [13/16]: Training Loss: 2.845787777, Training Accuracy: 27.672\n",
            "Worker 1, [14/16]: Training Loss: 2.760461768, Training Accuracy: 29.856\n",
            "Worker 1, [15/16]: Training Loss: 2.712332555, Training Accuracy: 29.840\n",
            "Worker 1, [16/16]: Training Loss: 2.647637635, Training Accuracy: 31.880\n",
            "Time taken for training worker 1: 0:01:33.960613\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 4.485987690, Training Accuracy: 2.704\n",
            "Worker 2, [02/16]: Training Loss: 4.110479750, Training Accuracy: 6.576\n",
            "Worker 2, [03/16]: Training Loss: 3.897538371, Training Accuracy: 9.376\n",
            "Worker 2, [04/16]: Training Loss: 3.735850512, Training Accuracy: 12.040\n",
            "Worker 2, [05/16]: Training Loss: 3.611357006, Training Accuracy: 13.864\n",
            "Worker 2, [06/16]: Training Loss: 3.474385617, Training Accuracy: 16.136\n",
            "Worker 2, [07/16]: Training Loss: 3.354416488, Training Accuracy: 18.048\n",
            "Worker 2, [08/16]: Training Loss: 3.261191534, Training Accuracy: 19.728\n",
            "Worker 2, [09/16]: Training Loss: 3.162337651, Training Accuracy: 21.400\n",
            "Worker 2, [10/16]: Training Loss: 3.093574922, Training Accuracy: 22.824\n",
            "Worker 2, [11/16]: Training Loss: 2.999667239, Training Accuracy: 24.336\n",
            "Worker 2, [12/16]: Training Loss: 2.925174045, Training Accuracy: 26.400\n",
            "Worker 2, [13/16]: Training Loss: 2.850592133, Training Accuracy: 26.936\n",
            "Worker 2, [14/16]: Training Loss: 2.782102770, Training Accuracy: 29.376\n",
            "Worker 2, [15/16]: Training Loss: 2.694450039, Training Accuracy: 30.376\n",
            "Worker 2, [16/16]: Training Loss: 2.628402434, Training Accuracy: 32.048\n",
            "Time taken for training worker 2: 0:01:36.456531\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 4.492087128, Training Accuracy: 2.720\n",
            "Worker 3, [02/16]: Training Loss: 4.103903208, Training Accuracy: 6.744\n",
            "Worker 3, [03/16]: Training Loss: 3.878398909, Training Accuracy: 10.072\n",
            "Worker 3, [04/16]: Training Loss: 3.725910992, Training Accuracy: 12.464\n",
            "Worker 3, [05/16]: Training Loss: 3.594735133, Training Accuracy: 14.184\n",
            "Worker 3, [06/16]: Training Loss: 3.457506070, Training Accuracy: 16.792\n",
            "Worker 3, [07/16]: Training Loss: 3.343334756, Training Accuracy: 18.880\n",
            "Worker 3, [08/16]: Training Loss: 3.239372789, Training Accuracy: 19.984\n",
            "Worker 3, [09/16]: Training Loss: 3.143362836, Training Accuracy: 22.232\n",
            "Worker 3, [10/16]: Training Loss: 3.078212237, Training Accuracy: 22.784\n",
            "Worker 3, [11/16]: Training Loss: 2.992523785, Training Accuracy: 24.560\n",
            "Worker 3, [12/16]: Training Loss: 2.894693629, Training Accuracy: 26.616\n",
            "Worker 3, [13/16]: Training Loss: 2.844679770, Training Accuracy: 27.256\n",
            "Worker 3, [14/16]: Training Loss: 2.773396389, Training Accuracy: 29.056\n",
            "Worker 3, [15/16]: Training Loss: 2.702528281, Training Accuracy: 30.448\n",
            "Worker 3, [16/16]: Training Loss: 2.639461859, Training Accuracy: 31.520\n",
            "Time taken for training worker 3: 0:01:36.079762\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 4.495323240, Training Accuracy: 2.288\n",
            "Worker 4, [02/16]: Training Loss: 4.122156496, Training Accuracy: 6.320\n",
            "Worker 4, [03/16]: Training Loss: 3.881319040, Training Accuracy: 9.448\n",
            "Worker 4, [04/16]: Training Loss: 3.736889461, Training Accuracy: 11.784\n",
            "Worker 4, [05/16]: Training Loss: 3.601832549, Training Accuracy: 14.416\n",
            "Worker 4, [06/16]: Training Loss: 3.475850710, Training Accuracy: 16.144\n",
            "Worker 4, [07/16]: Training Loss: 3.378050135, Training Accuracy: 18.152\n",
            "Worker 4, [08/16]: Training Loss: 3.273479604, Training Accuracy: 19.792\n",
            "Worker 4, [09/16]: Training Loss: 3.173203112, Training Accuracy: 21.600\n",
            "Worker 4, [10/16]: Training Loss: 3.088226925, Training Accuracy: 22.992\n",
            "Worker 4, [11/16]: Training Loss: 3.021522621, Training Accuracy: 24.264\n",
            "Worker 4, [12/16]: Training Loss: 2.927834034, Training Accuracy: 25.632\n",
            "Worker 4, [13/16]: Training Loss: 2.863688133, Training Accuracy: 27.168\n",
            "Worker 4, [14/16]: Training Loss: 2.812040414, Training Accuracy: 28.320\n",
            "Worker 4, [15/16]: Training Loss: 2.718440990, Training Accuracy: 29.896\n",
            "Worker 4, [16/16]: Training Loss: 2.662980178, Training Accuracy: 31.328\n",
            "Time taken for training worker 4: 0:01:35.800187\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002668\n",
            "Global Update 01: Test Loss: 3.336415669, Test Accuracy: 27.920\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.975831466, Training Accuracy: 25.760\n",
            "Worker 1, [02/16]: Training Loss: 2.824174168, Training Accuracy: 28.952\n",
            "Worker 1, [03/16]: Training Loss: 2.721997345, Training Accuracy: 30.536\n",
            "Worker 1, [04/16]: Training Loss: 2.648343047, Training Accuracy: 31.856\n",
            "Worker 1, [05/16]: Training Loss: 2.564012088, Training Accuracy: 33.520\n",
            "Worker 1, [06/16]: Training Loss: 2.515203047, Training Accuracy: 34.488\n",
            "Worker 1, [07/16]: Training Loss: 2.449709738, Training Accuracy: 35.664\n",
            "Worker 1, [08/16]: Training Loss: 2.391746415, Training Accuracy: 37.144\n",
            "Worker 1, [09/16]: Training Loss: 2.344127333, Training Accuracy: 37.440\n",
            "Worker 1, [10/16]: Training Loss: 2.265192827, Training Accuracy: 39.688\n",
            "Worker 1, [11/16]: Training Loss: 2.227008868, Training Accuracy: 40.712\n",
            "Worker 1, [12/16]: Training Loss: 2.200998155, Training Accuracy: 41.216\n",
            "Worker 1, [13/16]: Training Loss: 2.143788481, Training Accuracy: 42.472\n",
            "Worker 1, [14/16]: Training Loss: 2.123712844, Training Accuracy: 42.256\n",
            "Worker 1, [15/16]: Training Loss: 2.052637851, Training Accuracy: 44.064\n",
            "Worker 1, [16/16]: Training Loss: 1.997392647, Training Accuracy: 45.616\n",
            "Time taken for training worker 1: 0:01:33.077136\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 3.012589672, Training Accuracy: 24.624\n",
            "Worker 2, [02/16]: Training Loss: 2.837126094, Training Accuracy: 28.056\n",
            "Worker 2, [03/16]: Training Loss: 2.740649743, Training Accuracy: 30.000\n",
            "Worker 2, [04/16]: Training Loss: 2.651959743, Training Accuracy: 31.512\n",
            "Worker 2, [05/16]: Training Loss: 2.572685584, Training Accuracy: 32.568\n",
            "Worker 2, [06/16]: Training Loss: 2.536515122, Training Accuracy: 33.816\n",
            "Worker 2, [07/16]: Training Loss: 2.470020384, Training Accuracy: 34.712\n",
            "Worker 2, [08/16]: Training Loss: 2.403839224, Training Accuracy: 36.352\n",
            "Worker 2, [09/16]: Training Loss: 2.336005907, Training Accuracy: 37.856\n",
            "Worker 2, [10/16]: Training Loss: 2.303975059, Training Accuracy: 38.456\n",
            "Worker 2, [11/16]: Training Loss: 2.218287970, Training Accuracy: 39.808\n",
            "Worker 2, [12/16]: Training Loss: 2.170326771, Training Accuracy: 41.112\n",
            "Worker 2, [13/16]: Training Loss: 2.146742899, Training Accuracy: 42.272\n",
            "Worker 2, [14/16]: Training Loss: 2.095961668, Training Accuracy: 42.992\n",
            "Worker 2, [15/16]: Training Loss: 2.046978536, Training Accuracy: 43.680\n",
            "Worker 2, [16/16]: Training Loss: 1.989004492, Training Accuracy: 45.240\n",
            "Time taken for training worker 2: 0:01:37.092427\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.974189191, Training Accuracy: 24.816\n",
            "Worker 3, [02/16]: Training Loss: 2.807344095, Training Accuracy: 28.216\n",
            "Worker 3, [03/16]: Training Loss: 2.722238806, Training Accuracy: 30.088\n",
            "Worker 3, [04/16]: Training Loss: 2.643002493, Training Accuracy: 31.744\n",
            "Worker 3, [05/16]: Training Loss: 2.581981692, Training Accuracy: 32.680\n",
            "Worker 3, [06/16]: Training Loss: 2.503483779, Training Accuracy: 35.016\n",
            "Worker 3, [07/16]: Training Loss: 2.447617210, Training Accuracy: 36.024\n",
            "Worker 3, [08/16]: Training Loss: 2.380623742, Training Accuracy: 37.200\n",
            "Worker 3, [09/16]: Training Loss: 2.331704858, Training Accuracy: 38.200\n",
            "Worker 3, [10/16]: Training Loss: 2.264868997, Training Accuracy: 39.696\n",
            "Worker 3, [11/16]: Training Loss: 2.224269835, Training Accuracy: 40.488\n",
            "Worker 3, [12/16]: Training Loss: 2.185389891, Training Accuracy: 41.248\n",
            "Worker 3, [13/16]: Training Loss: 2.143510222, Training Accuracy: 42.024\n",
            "Worker 3, [14/16]: Training Loss: 2.099025799, Training Accuracy: 43.120\n",
            "Worker 3, [15/16]: Training Loss: 2.045235762, Training Accuracy: 43.808\n",
            "Worker 3, [16/16]: Training Loss: 2.005219481, Training Accuracy: 44.608\n",
            "Time taken for training worker 3: 0:01:34.090311\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.990492433, Training Accuracy: 25.104\n",
            "Worker 4, [02/16]: Training Loss: 2.820222085, Training Accuracy: 28.232\n",
            "Worker 4, [03/16]: Training Loss: 2.742216738, Training Accuracy: 29.544\n",
            "Worker 4, [04/16]: Training Loss: 2.677853936, Training Accuracy: 31.176\n",
            "Worker 4, [05/16]: Training Loss: 2.586983785, Training Accuracy: 32.856\n",
            "Worker 4, [06/16]: Training Loss: 2.528576494, Training Accuracy: 33.640\n",
            "Worker 4, [07/16]: Training Loss: 2.457678728, Training Accuracy: 35.208\n",
            "Worker 4, [08/16]: Training Loss: 2.388090255, Training Accuracy: 36.232\n",
            "Worker 4, [09/16]: Training Loss: 2.363054538, Training Accuracy: 37.352\n",
            "Worker 4, [10/16]: Training Loss: 2.303246403, Training Accuracy: 38.728\n",
            "Worker 4, [11/16]: Training Loss: 2.246715132, Training Accuracy: 39.528\n",
            "Worker 4, [12/16]: Training Loss: 2.185335255, Training Accuracy: 40.872\n",
            "Worker 4, [13/16]: Training Loss: 2.146531485, Training Accuracy: 42.136\n",
            "Worker 4, [14/16]: Training Loss: 2.113058478, Training Accuracy: 42.456\n",
            "Worker 4, [15/16]: Training Loss: 2.092537151, Training Accuracy: 43.008\n",
            "Worker 4, [16/16]: Training Loss: 2.017732445, Training Accuracy: 44.280\n",
            "Time taken for training worker 4: 0:01:32.517103\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.009732\n",
            "Global Update 02: Test Loss: 2.287868928, Test Accuracy: 40.190\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.398624717, Training Accuracy: 37.832\n",
            "Worker 1, [02/16]: Training Loss: 2.254887942, Training Accuracy: 40.104\n",
            "Worker 1, [03/16]: Training Loss: 2.175807646, Training Accuracy: 41.888\n",
            "Worker 1, [04/16]: Training Loss: 2.122529473, Training Accuracy: 42.760\n",
            "Worker 1, [05/16]: Training Loss: 2.048054016, Training Accuracy: 44.840\n",
            "Worker 1, [06/16]: Training Loss: 2.004051926, Training Accuracy: 45.752\n",
            "Worker 1, [07/16]: Training Loss: 1.965223930, Training Accuracy: 46.104\n",
            "Worker 1, [08/16]: Training Loss: 1.897734257, Training Accuracy: 48.048\n",
            "Worker 1, [09/16]: Training Loss: 1.853506563, Training Accuracy: 48.688\n",
            "Worker 1, [10/16]: Training Loss: 1.810681956, Training Accuracy: 49.608\n",
            "Worker 1, [11/16]: Training Loss: 1.774447787, Training Accuracy: 50.472\n",
            "Worker 1, [12/16]: Training Loss: 1.728761609, Training Accuracy: 51.784\n",
            "Worker 1, [13/16]: Training Loss: 1.713511432, Training Accuracy: 52.464\n",
            "Worker 1, [14/16]: Training Loss: 1.664450976, Training Accuracy: 53.256\n",
            "Worker 1, [15/16]: Training Loss: 1.644253671, Training Accuracy: 53.880\n",
            "Worker 1, [16/16]: Training Loss: 1.614542296, Training Accuracy: 54.456\n",
            "Time taken for training worker 1: 0:01:35.324576\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.367993383, Training Accuracy: 37.760\n",
            "Worker 2, [02/16]: Training Loss: 2.268568571, Training Accuracy: 39.312\n",
            "Worker 2, [03/16]: Training Loss: 2.183205764, Training Accuracy: 41.296\n",
            "Worker 2, [04/16]: Training Loss: 2.108094802, Training Accuracy: 43.336\n",
            "Worker 2, [05/16]: Training Loss: 2.051496885, Training Accuracy: 44.488\n",
            "Worker 2, [06/16]: Training Loss: 2.009393809, Training Accuracy: 45.520\n",
            "Worker 2, [07/16]: Training Loss: 1.961543856, Training Accuracy: 46.320\n",
            "Worker 2, [08/16]: Training Loss: 1.879282051, Training Accuracy: 48.168\n",
            "Worker 2, [09/16]: Training Loss: 1.859449915, Training Accuracy: 48.296\n",
            "Worker 2, [10/16]: Training Loss: 1.840082862, Training Accuracy: 48.416\n",
            "Worker 2, [11/16]: Training Loss: 1.770960788, Training Accuracy: 50.152\n",
            "Worker 2, [12/16]: Training Loss: 1.723181709, Training Accuracy: 51.904\n",
            "Worker 2, [13/16]: Training Loss: 1.680313081, Training Accuracy: 52.064\n",
            "Worker 2, [14/16]: Training Loss: 1.656785003, Training Accuracy: 53.504\n",
            "Worker 2, [15/16]: Training Loss: 1.634399711, Training Accuracy: 53.952\n",
            "Worker 2, [16/16]: Training Loss: 1.582096685, Training Accuracy: 55.400\n",
            "Time taken for training worker 2: 0:01:34.527904\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.367107676, Training Accuracy: 37.856\n",
            "Worker 3, [02/16]: Training Loss: 2.272072583, Training Accuracy: 39.576\n",
            "Worker 3, [03/16]: Training Loss: 2.178488140, Training Accuracy: 41.792\n",
            "Worker 3, [04/16]: Training Loss: 2.100412706, Training Accuracy: 43.264\n",
            "Worker 3, [05/16]: Training Loss: 2.040611846, Training Accuracy: 44.376\n",
            "Worker 3, [06/16]: Training Loss: 1.991572496, Training Accuracy: 45.016\n",
            "Worker 3, [07/16]: Training Loss: 1.939644946, Training Accuracy: 46.568\n",
            "Worker 3, [08/16]: Training Loss: 1.911302795, Training Accuracy: 47.256\n",
            "Worker 3, [09/16]: Training Loss: 1.837458101, Training Accuracy: 48.704\n",
            "Worker 3, [10/16]: Training Loss: 1.803003863, Training Accuracy: 50.080\n",
            "Worker 3, [11/16]: Training Loss: 1.788809507, Training Accuracy: 49.960\n",
            "Worker 3, [12/16]: Training Loss: 1.731163122, Training Accuracy: 51.816\n",
            "Worker 3, [13/16]: Training Loss: 1.710231861, Training Accuracy: 51.728\n",
            "Worker 3, [14/16]: Training Loss: 1.666626789, Training Accuracy: 52.808\n",
            "Worker 3, [15/16]: Training Loss: 1.620181138, Training Accuracy: 54.328\n",
            "Worker 3, [16/16]: Training Loss: 1.637153455, Training Accuracy: 53.480\n",
            "Time taken for training worker 3: 0:01:36.674902\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.382603828, Training Accuracy: 37.256\n",
            "Worker 4, [02/16]: Training Loss: 2.265558898, Training Accuracy: 40.024\n",
            "Worker 4, [03/16]: Training Loss: 2.179862505, Training Accuracy: 41.656\n",
            "Worker 4, [04/16]: Training Loss: 2.121145306, Training Accuracy: 42.736\n",
            "Worker 4, [05/16]: Training Loss: 2.057396066, Training Accuracy: 44.200\n",
            "Worker 4, [06/16]: Training Loss: 2.024465836, Training Accuracy: 44.808\n",
            "Worker 4, [07/16]: Training Loss: 1.955254807, Training Accuracy: 46.008\n",
            "Worker 4, [08/16]: Training Loss: 1.894791352, Training Accuracy: 47.528\n",
            "Worker 4, [09/16]: Training Loss: 1.870967327, Training Accuracy: 48.120\n",
            "Worker 4, [10/16]: Training Loss: 1.834869991, Training Accuracy: 48.496\n",
            "Worker 4, [11/16]: Training Loss: 1.776780339, Training Accuracy: 50.184\n",
            "Worker 4, [12/16]: Training Loss: 1.761264672, Training Accuracy: 50.056\n",
            "Worker 4, [13/16]: Training Loss: 1.694286826, Training Accuracy: 52.160\n",
            "Worker 4, [14/16]: Training Loss: 1.675738168, Training Accuracy: 52.552\n",
            "Worker 4, [15/16]: Training Loss: 1.645361706, Training Accuracy: 53.584\n",
            "Worker 4, [16/16]: Training Loss: 1.611658736, Training Accuracy: 54.128\n",
            "Time taken for training worker 4: 0:01:34.996768\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002589\n",
            "Global Update 03: Test Loss: 2.111165752, Test Accuracy: 44.650\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.076083619, Training Accuracy: 44.304\n",
            "Worker 1, [02/16]: Training Loss: 1.910532787, Training Accuracy: 47.600\n",
            "Worker 1, [03/16]: Training Loss: 1.813471822, Training Accuracy: 49.984\n",
            "Worker 1, [04/16]: Training Loss: 1.745877925, Training Accuracy: 51.192\n",
            "Worker 1, [05/16]: Training Loss: 1.703379918, Training Accuracy: 52.512\n",
            "Worker 1, [06/16]: Training Loss: 1.630625032, Training Accuracy: 53.888\n",
            "Worker 1, [07/16]: Training Loss: 1.585785066, Training Accuracy: 55.256\n",
            "Worker 1, [08/16]: Training Loss: 1.561595645, Training Accuracy: 55.544\n",
            "Worker 1, [09/16]: Training Loss: 1.518742198, Training Accuracy: 57.016\n",
            "Worker 1, [10/16]: Training Loss: 1.459591712, Training Accuracy: 58.296\n",
            "Worker 1, [11/16]: Training Loss: 1.417184598, Training Accuracy: 59.432\n",
            "Worker 1, [12/16]: Training Loss: 1.385234619, Training Accuracy: 59.568\n",
            "Worker 1, [13/16]: Training Loss: 1.353630265, Training Accuracy: 61.240\n",
            "Worker 1, [14/16]: Training Loss: 1.342028153, Training Accuracy: 61.120\n",
            "Worker 1, [15/16]: Training Loss: 1.302076689, Training Accuracy: 62.040\n",
            "Worker 1, [16/16]: Training Loss: 1.287048036, Training Accuracy: 62.456\n",
            "Time taken for training worker 1: 0:01:36.242533\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.068211607, Training Accuracy: 44.096\n",
            "Worker 2, [02/16]: Training Loss: 1.909543759, Training Accuracy: 48.240\n",
            "Worker 2, [03/16]: Training Loss: 1.828393649, Training Accuracy: 49.344\n",
            "Worker 2, [04/16]: Training Loss: 1.749240417, Training Accuracy: 50.792\n",
            "Worker 2, [05/16]: Training Loss: 1.679674467, Training Accuracy: 52.904\n",
            "Worker 2, [06/16]: Training Loss: 1.629304308, Training Accuracy: 54.072\n",
            "Worker 2, [07/16]: Training Loss: 1.579975524, Training Accuracy: 55.088\n",
            "Worker 2, [08/16]: Training Loss: 1.538835917, Training Accuracy: 56.312\n",
            "Worker 2, [09/16]: Training Loss: 1.494499729, Training Accuracy: 57.256\n",
            "Worker 2, [10/16]: Training Loss: 1.453764870, Training Accuracy: 58.728\n",
            "Worker 2, [11/16]: Training Loss: 1.415486034, Training Accuracy: 59.160\n",
            "Worker 2, [12/16]: Training Loss: 1.389004047, Training Accuracy: 60.312\n",
            "Worker 2, [13/16]: Training Loss: 1.351598792, Training Accuracy: 60.632\n",
            "Worker 2, [14/16]: Training Loss: 1.345493936, Training Accuracy: 60.552\n",
            "Worker 2, [15/16]: Training Loss: 1.327296189, Training Accuracy: 60.864\n",
            "Worker 2, [16/16]: Training Loss: 1.250146720, Training Accuracy: 63.488\n",
            "Time taken for training worker 2: 0:01:38.948389\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.061171873, Training Accuracy: 44.136\n",
            "Worker 3, [02/16]: Training Loss: 1.936709232, Training Accuracy: 47.304\n",
            "Worker 3, [03/16]: Training Loss: 1.824430897, Training Accuracy: 49.984\n",
            "Worker 3, [04/16]: Training Loss: 1.771777362, Training Accuracy: 50.376\n",
            "Worker 3, [05/16]: Training Loss: 1.696147412, Training Accuracy: 52.448\n",
            "Worker 3, [06/16]: Training Loss: 1.612311360, Training Accuracy: 54.288\n",
            "Worker 3, [07/16]: Training Loss: 1.602657928, Training Accuracy: 54.208\n",
            "Worker 3, [08/16]: Training Loss: 1.530791831, Training Accuracy: 56.152\n",
            "Worker 3, [09/16]: Training Loss: 1.505934411, Training Accuracy: 56.872\n",
            "Worker 3, [10/16]: Training Loss: 1.465085045, Training Accuracy: 58.104\n",
            "Worker 3, [11/16]: Training Loss: 1.447175374, Training Accuracy: 57.776\n",
            "Worker 3, [12/16]: Training Loss: 1.391001862, Training Accuracy: 59.904\n",
            "Worker 3, [13/16]: Training Loss: 1.368715980, Training Accuracy: 60.872\n",
            "Worker 3, [14/16]: Training Loss: 1.330862343, Training Accuracy: 61.216\n",
            "Worker 3, [15/16]: Training Loss: 1.307469985, Training Accuracy: 62.248\n",
            "Worker 3, [16/16]: Training Loss: 1.299893614, Training Accuracy: 62.112\n",
            "Time taken for training worker 3: 0:01:35.051992\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.077916078, Training Accuracy: 43.664\n",
            "Worker 4, [02/16]: Training Loss: 1.927364252, Training Accuracy: 47.104\n",
            "Worker 4, [03/16]: Training Loss: 1.837839091, Training Accuracy: 49.168\n",
            "Worker 4, [04/16]: Training Loss: 1.740219703, Training Accuracy: 51.192\n",
            "Worker 4, [05/16]: Training Loss: 1.696272050, Training Accuracy: 52.448\n",
            "Worker 4, [06/16]: Training Loss: 1.653232192, Training Accuracy: 53.504\n",
            "Worker 4, [07/16]: Training Loss: 1.598457973, Training Accuracy: 54.904\n",
            "Worker 4, [08/16]: Training Loss: 1.557037144, Training Accuracy: 55.576\n",
            "Worker 4, [09/16]: Training Loss: 1.515283327, Training Accuracy: 57.000\n",
            "Worker 4, [10/16]: Training Loss: 1.481737779, Training Accuracy: 57.320\n",
            "Worker 4, [11/16]: Training Loss: 1.446301865, Training Accuracy: 58.192\n",
            "Worker 4, [12/16]: Training Loss: 1.417170466, Training Accuracy: 59.328\n",
            "Worker 4, [13/16]: Training Loss: 1.367617215, Training Accuracy: 60.392\n",
            "Worker 4, [14/16]: Training Loss: 1.326696351, Training Accuracy: 61.376\n",
            "Worker 4, [15/16]: Training Loss: 1.319576861, Training Accuracy: 61.496\n",
            "Worker 4, [16/16]: Training Loss: 1.289176093, Training Accuracy: 62.696\n",
            "Time taken for training worker 4: 0:01:41.861708\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002868\n",
            "Global Update 04: Test Loss: 2.040649007, Test Accuracy: 47.250\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.860387198, Training Accuracy: 49.504\n",
            "Worker 1, [02/16]: Training Loss: 1.661866728, Training Accuracy: 53.360\n",
            "Worker 1, [03/16]: Training Loss: 1.529366951, Training Accuracy: 57.000\n",
            "Worker 1, [04/16]: Training Loss: 1.450568836, Training Accuracy: 59.104\n",
            "Worker 1, [05/16]: Training Loss: 1.407538274, Training Accuracy: 59.752\n",
            "Worker 1, [06/16]: Training Loss: 1.340213666, Training Accuracy: 61.712\n",
            "Worker 1, [07/16]: Training Loss: 1.299530742, Training Accuracy: 62.272\n",
            "Worker 1, [08/16]: Training Loss: 1.251236584, Training Accuracy: 63.696\n",
            "Worker 1, [09/16]: Training Loss: 1.187309846, Training Accuracy: 65.296\n",
            "Worker 1, [10/16]: Training Loss: 1.178842797, Training Accuracy: 65.512\n",
            "Worker 1, [11/16]: Training Loss: 1.128009088, Training Accuracy: 67.016\n",
            "Worker 1, [12/16]: Training Loss: 1.134039629, Training Accuracy: 66.272\n",
            "Worker 1, [13/16]: Training Loss: 1.090939214, Training Accuracy: 68.240\n",
            "Worker 1, [14/16]: Training Loss: 1.048192014, Training Accuracy: 68.872\n",
            "Worker 1, [15/16]: Training Loss: 1.011184820, Training Accuracy: 69.912\n",
            "Worker 1, [16/16]: Training Loss: 1.021209023, Training Accuracy: 69.264\n",
            "Time taken for training worker 1: 0:01:35.644908\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.820470213, Training Accuracy: 50.080\n",
            "Worker 2, [02/16]: Training Loss: 1.644763135, Training Accuracy: 53.648\n",
            "Worker 2, [03/16]: Training Loss: 1.531213620, Training Accuracy: 56.624\n",
            "Worker 2, [04/16]: Training Loss: 1.460772316, Training Accuracy: 58.048\n",
            "Worker 2, [05/16]: Training Loss: 1.394907370, Training Accuracy: 59.864\n",
            "Worker 2, [06/16]: Training Loss: 1.325244606, Training Accuracy: 61.280\n",
            "Worker 2, [07/16]: Training Loss: 1.285114230, Training Accuracy: 62.544\n",
            "Worker 2, [08/16]: Training Loss: 1.218889168, Training Accuracy: 64.456\n",
            "Worker 2, [09/16]: Training Loss: 1.192706842, Training Accuracy: 64.944\n",
            "Worker 2, [10/16]: Training Loss: 1.184289293, Training Accuracy: 65.472\n",
            "Worker 2, [11/16]: Training Loss: 1.147122619, Training Accuracy: 66.024\n",
            "Worker 2, [12/16]: Training Loss: 1.080442834, Training Accuracy: 68.200\n",
            "Worker 2, [13/16]: Training Loss: 1.095603604, Training Accuracy: 67.512\n",
            "Worker 2, [14/16]: Training Loss: 1.034015705, Training Accuracy: 69.256\n",
            "Worker 2, [15/16]: Training Loss: 1.029726058, Training Accuracy: 69.160\n",
            "Worker 2, [16/16]: Training Loss: 0.996704063, Training Accuracy: 70.384\n",
            "Time taken for training worker 2: 0:01:34.052708\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.821913230, Training Accuracy: 50.288\n",
            "Worker 3, [02/16]: Training Loss: 1.637932767, Training Accuracy: 53.904\n",
            "Worker 3, [03/16]: Training Loss: 1.550837595, Training Accuracy: 56.512\n",
            "Worker 3, [04/16]: Training Loss: 1.450871917, Training Accuracy: 58.640\n",
            "Worker 3, [05/16]: Training Loss: 1.422074980, Training Accuracy: 58.888\n",
            "Worker 3, [06/16]: Training Loss: 1.316929382, Training Accuracy: 61.592\n",
            "Worker 3, [07/16]: Training Loss: 1.280133581, Training Accuracy: 62.704\n",
            "Worker 3, [08/16]: Training Loss: 1.237967368, Training Accuracy: 63.912\n",
            "Worker 3, [09/16]: Training Loss: 1.205878235, Training Accuracy: 64.712\n",
            "Worker 3, [10/16]: Training Loss: 1.188302891, Training Accuracy: 65.200\n",
            "Worker 3, [11/16]: Training Loss: 1.139126751, Training Accuracy: 66.032\n",
            "Worker 3, [12/16]: Training Loss: 1.120412282, Training Accuracy: 66.896\n",
            "Worker 3, [13/16]: Training Loss: 1.089970876, Training Accuracy: 67.672\n",
            "Worker 3, [14/16]: Training Loss: 1.049053487, Training Accuracy: 68.888\n",
            "Worker 3, [15/16]: Training Loss: 1.029810405, Training Accuracy: 69.368\n",
            "Worker 3, [16/16]: Training Loss: 1.021029292, Training Accuracy: 69.560\n",
            "Time taken for training worker 3: 0:01:35.987981\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.847132888, Training Accuracy: 49.360\n",
            "Worker 4, [02/16]: Training Loss: 1.670811249, Training Accuracy: 53.312\n",
            "Worker 4, [03/16]: Training Loss: 1.557937750, Training Accuracy: 55.872\n",
            "Worker 4, [04/16]: Training Loss: 1.479708756, Training Accuracy: 57.776\n",
            "Worker 4, [05/16]: Training Loss: 1.424183679, Training Accuracy: 58.816\n",
            "Worker 4, [06/16]: Training Loss: 1.354281064, Training Accuracy: 60.776\n",
            "Worker 4, [07/16]: Training Loss: 1.291727924, Training Accuracy: 62.016\n",
            "Worker 4, [08/16]: Training Loss: 1.280053074, Training Accuracy: 62.704\n",
            "Worker 4, [09/16]: Training Loss: 1.218569484, Training Accuracy: 64.184\n",
            "Worker 4, [10/16]: Training Loss: 1.188391250, Training Accuracy: 65.040\n",
            "Worker 4, [11/16]: Training Loss: 1.156078766, Training Accuracy: 65.752\n",
            "Worker 4, [12/16]: Training Loss: 1.126089477, Training Accuracy: 66.776\n",
            "Worker 4, [13/16]: Training Loss: 1.082592030, Training Accuracy: 68.312\n",
            "Worker 4, [14/16]: Training Loss: 1.058849810, Training Accuracy: 68.496\n",
            "Worker 4, [15/16]: Training Loss: 1.030670615, Training Accuracy: 69.168\n",
            "Worker 4, [16/16]: Training Loss: 1.013748726, Training Accuracy: 69.664\n",
            "Time taken for training worker 4: 0:01:36.214400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002775\n",
            "Global Update 05: Test Loss: 2.064402039, Test Accuracy: 48.730\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.615489477, Training Accuracy: 55.008\n",
            "Worker 1, [02/16]: Training Loss: 1.425039683, Training Accuracy: 59.248\n",
            "Worker 1, [03/16]: Training Loss: 1.285720868, Training Accuracy: 62.744\n",
            "Worker 1, [04/16]: Training Loss: 1.213825287, Training Accuracy: 64.784\n",
            "Worker 1, [05/16]: Training Loss: 1.148268363, Training Accuracy: 66.376\n",
            "Worker 1, [06/16]: Training Loss: 1.075746083, Training Accuracy: 68.496\n",
            "Worker 1, [07/16]: Training Loss: 1.027064816, Training Accuracy: 70.424\n",
            "Worker 1, [08/16]: Training Loss: 1.020631786, Training Accuracy: 70.048\n",
            "Worker 1, [09/16]: Training Loss: 0.944435693, Training Accuracy: 71.928\n",
            "Worker 1, [10/16]: Training Loss: 0.904667084, Training Accuracy: 73.192\n",
            "Worker 1, [11/16]: Training Loss: 0.884140296, Training Accuracy: 73.872\n",
            "Worker 1, [12/16]: Training Loss: 0.864148735, Training Accuracy: 73.960\n",
            "Worker 1, [13/16]: Training Loss: 0.828185478, Training Accuracy: 75.088\n",
            "Worker 1, [14/16]: Training Loss: 0.811743518, Training Accuracy: 75.640\n",
            "Worker 1, [15/16]: Training Loss: 0.790899396, Training Accuracy: 76.184\n",
            "Worker 1, [16/16]: Training Loss: 0.773523343, Training Accuracy: 76.928\n",
            "Time taken for training worker 1: 0:01:35.770611\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.610232579, Training Accuracy: 54.504\n",
            "Worker 2, [02/16]: Training Loss: 1.412831158, Training Accuracy: 59.272\n",
            "Worker 2, [03/16]: Training Loss: 1.294106058, Training Accuracy: 62.464\n",
            "Worker 2, [04/16]: Training Loss: 1.216245288, Training Accuracy: 64.600\n",
            "Worker 2, [05/16]: Training Loss: 1.142859431, Training Accuracy: 66.112\n",
            "Worker 2, [06/16]: Training Loss: 1.074000714, Training Accuracy: 68.224\n",
            "Worker 2, [07/16]: Training Loss: 1.020677482, Training Accuracy: 70.016\n",
            "Worker 2, [08/16]: Training Loss: 0.983605308, Training Accuracy: 71.160\n",
            "Worker 2, [09/16]: Training Loss: 0.951929098, Training Accuracy: 71.720\n",
            "Worker 2, [10/16]: Training Loss: 0.911091098, Training Accuracy: 72.808\n",
            "Worker 2, [11/16]: Training Loss: 0.861989881, Training Accuracy: 74.256\n",
            "Worker 2, [12/16]: Training Loss: 0.857874931, Training Accuracy: 74.752\n",
            "Worker 2, [13/16]: Training Loss: 0.825151186, Training Accuracy: 75.144\n",
            "Worker 2, [14/16]: Training Loss: 0.806963943, Training Accuracy: 75.744\n",
            "Worker 2, [15/16]: Training Loss: 0.802086788, Training Accuracy: 75.512\n",
            "Worker 2, [16/16]: Training Loss: 0.758390788, Training Accuracy: 77.288\n",
            "Time taken for training worker 2: 0:01:35.868389\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.633318408, Training Accuracy: 54.368\n",
            "Worker 3, [02/16]: Training Loss: 1.431831354, Training Accuracy: 59.176\n",
            "Worker 3, [03/16]: Training Loss: 1.301313041, Training Accuracy: 62.016\n",
            "Worker 3, [04/16]: Training Loss: 1.205999140, Training Accuracy: 64.488\n",
            "Worker 3, [05/16]: Training Loss: 1.144939417, Training Accuracy: 66.432\n",
            "Worker 3, [06/16]: Training Loss: 1.096822978, Training Accuracy: 67.600\n",
            "Worker 3, [07/16]: Training Loss: 1.055556988, Training Accuracy: 68.584\n",
            "Worker 3, [08/16]: Training Loss: 1.005287678, Training Accuracy: 70.432\n",
            "Worker 3, [09/16]: Training Loss: 0.957202373, Training Accuracy: 71.352\n",
            "Worker 3, [10/16]: Training Loss: 0.925836923, Training Accuracy: 72.544\n",
            "Worker 3, [11/16]: Training Loss: 0.885863423, Training Accuracy: 73.528\n",
            "Worker 3, [12/16]: Training Loss: 0.877277980, Training Accuracy: 74.056\n",
            "Worker 3, [13/16]: Training Loss: 0.836329211, Training Accuracy: 75.264\n",
            "Worker 3, [14/16]: Training Loss: 0.810848686, Training Accuracy: 75.432\n",
            "Worker 3, [15/16]: Training Loss: 0.800074819, Training Accuracy: 76.184\n",
            "Worker 3, [16/16]: Training Loss: 0.775383463, Training Accuracy: 76.488\n",
            "Time taken for training worker 3: 0:01:35.370255\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.652512595, Training Accuracy: 54.200\n",
            "Worker 4, [02/16]: Training Loss: 1.446797389, Training Accuracy: 58.432\n",
            "Worker 4, [03/16]: Training Loss: 1.319199248, Training Accuracy: 61.944\n",
            "Worker 4, [04/16]: Training Loss: 1.233087256, Training Accuracy: 64.248\n",
            "Worker 4, [05/16]: Training Loss: 1.154249792, Training Accuracy: 66.424\n",
            "Worker 4, [06/16]: Training Loss: 1.105820628, Training Accuracy: 67.240\n",
            "Worker 4, [07/16]: Training Loss: 1.061251916, Training Accuracy: 68.280\n",
            "Worker 4, [08/16]: Training Loss: 1.016182999, Training Accuracy: 69.824\n",
            "Worker 4, [09/16]: Training Loss: 0.973557232, Training Accuracy: 70.752\n",
            "Worker 4, [10/16]: Training Loss: 0.947247225, Training Accuracy: 71.752\n",
            "Worker 4, [11/16]: Training Loss: 0.905628580, Training Accuracy: 72.936\n",
            "Worker 4, [12/16]: Training Loss: 0.864120028, Training Accuracy: 74.160\n",
            "Worker 4, [13/16]: Training Loss: 0.855021968, Training Accuracy: 73.872\n",
            "Worker 4, [14/16]: Training Loss: 0.834772743, Training Accuracy: 74.736\n",
            "Worker 4, [15/16]: Training Loss: 0.817364882, Training Accuracy: 75.024\n",
            "Worker 4, [16/16]: Training Loss: 0.768842773, Training Accuracy: 76.456\n",
            "Time taken for training worker 4: 0:01:35.785927\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002610\n",
            "Global Update 06: Test Loss: 2.117858115, Test Accuracy: 49.790\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.437944953, Training Accuracy: 58.904\n",
            "Worker 1, [02/16]: Training Loss: 1.231124738, Training Accuracy: 64.808\n",
            "Worker 1, [03/16]: Training Loss: 1.091120194, Training Accuracy: 67.696\n",
            "Worker 1, [04/16]: Training Loss: 1.016471529, Training Accuracy: 70.264\n",
            "Worker 1, [05/16]: Training Loss: 0.956276757, Training Accuracy: 71.800\n",
            "Worker 1, [06/16]: Training Loss: 0.895334662, Training Accuracy: 73.792\n",
            "Worker 1, [07/16]: Training Loss: 0.850546718, Training Accuracy: 74.728\n",
            "Worker 1, [08/16]: Training Loss: 0.810696584, Training Accuracy: 76.048\n",
            "Worker 1, [09/16]: Training Loss: 0.766666786, Training Accuracy: 77.216\n",
            "Worker 1, [10/16]: Training Loss: 0.730071685, Training Accuracy: 78.744\n",
            "Worker 1, [11/16]: Training Loss: 0.711483316, Training Accuracy: 78.600\n",
            "Worker 1, [12/16]: Training Loss: 0.681580833, Training Accuracy: 79.640\n",
            "Worker 1, [13/16]: Training Loss: 0.650261580, Training Accuracy: 80.496\n",
            "Worker 1, [14/16]: Training Loss: 0.609340242, Training Accuracy: 81.864\n",
            "Worker 1, [15/16]: Training Loss: 0.618521317, Training Accuracy: 81.240\n",
            "Worker 1, [16/16]: Training Loss: 0.597251119, Training Accuracy: 82.280\n",
            "Time taken for training worker 1: 0:01:34.744436\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.427098670, Training Accuracy: 59.152\n",
            "Worker 2, [02/16]: Training Loss: 1.218170905, Training Accuracy: 64.384\n",
            "Worker 2, [03/16]: Training Loss: 1.120675379, Training Accuracy: 66.704\n",
            "Worker 2, [04/16]: Training Loss: 1.021209509, Training Accuracy: 70.024\n",
            "Worker 2, [05/16]: Training Loss: 0.960790695, Training Accuracy: 71.384\n",
            "Worker 2, [06/16]: Training Loss: 0.898574517, Training Accuracy: 73.352\n",
            "Worker 2, [07/16]: Training Loss: 0.823366447, Training Accuracy: 75.464\n",
            "Worker 2, [08/16]: Training Loss: 0.802520029, Training Accuracy: 75.760\n",
            "Worker 2, [09/16]: Training Loss: 0.743727306, Training Accuracy: 78.032\n",
            "Worker 2, [10/16]: Training Loss: 0.725527128, Training Accuracy: 78.080\n",
            "Worker 2, [11/16]: Training Loss: 0.705332236, Training Accuracy: 78.936\n",
            "Worker 2, [12/16]: Training Loss: 0.663130031, Training Accuracy: 79.808\n",
            "Worker 2, [13/16]: Training Loss: 0.656291884, Training Accuracy: 80.144\n",
            "Worker 2, [14/16]: Training Loss: 0.625045870, Training Accuracy: 81.200\n",
            "Worker 2, [15/16]: Training Loss: 0.587038518, Training Accuracy: 82.560\n",
            "Worker 2, [16/16]: Training Loss: 0.576790402, Training Accuracy: 82.504\n",
            "Time taken for training worker 2: 0:01:37.485968\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.430610668, Training Accuracy: 59.288\n",
            "Worker 3, [02/16]: Training Loss: 1.230064671, Training Accuracy: 64.072\n",
            "Worker 3, [03/16]: Training Loss: 1.107223020, Training Accuracy: 67.160\n",
            "Worker 3, [04/16]: Training Loss: 1.033129638, Training Accuracy: 69.824\n",
            "Worker 3, [05/16]: Training Loss: 0.958110192, Training Accuracy: 71.792\n",
            "Worker 3, [06/16]: Training Loss: 0.893491876, Training Accuracy: 73.848\n",
            "Worker 3, [07/16]: Training Loss: 0.849663035, Training Accuracy: 75.168\n",
            "Worker 3, [08/16]: Training Loss: 0.807561625, Training Accuracy: 76.000\n",
            "Worker 3, [09/16]: Training Loss: 0.751001182, Training Accuracy: 77.896\n",
            "Worker 3, [10/16]: Training Loss: 0.734915006, Training Accuracy: 77.720\n",
            "Worker 3, [11/16]: Training Loss: 0.695990358, Training Accuracy: 79.080\n",
            "Worker 3, [12/16]: Training Loss: 0.687237834, Training Accuracy: 79.248\n",
            "Worker 3, [13/16]: Training Loss: 0.663539714, Training Accuracy: 79.896\n",
            "Worker 3, [14/16]: Training Loss: 0.625626361, Training Accuracy: 81.464\n",
            "Worker 3, [15/16]: Training Loss: 0.605922270, Training Accuracy: 81.744\n",
            "Worker 3, [16/16]: Training Loss: 0.590637888, Training Accuracy: 82.456\n",
            "Time taken for training worker 3: 0:01:36.302540\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.440304398, Training Accuracy: 59.088\n",
            "Worker 4, [02/16]: Training Loss: 1.241766349, Training Accuracy: 64.080\n",
            "Worker 4, [03/16]: Training Loss: 1.134912468, Training Accuracy: 67.080\n",
            "Worker 4, [04/16]: Training Loss: 1.047429636, Training Accuracy: 69.000\n",
            "Worker 4, [05/16]: Training Loss: 0.971379023, Training Accuracy: 71.048\n",
            "Worker 4, [06/16]: Training Loss: 0.921647364, Training Accuracy: 73.088\n",
            "Worker 4, [07/16]: Training Loss: 0.863414089, Training Accuracy: 74.648\n",
            "Worker 4, [08/16]: Training Loss: 0.845737167, Training Accuracy: 74.640\n",
            "Worker 4, [09/16]: Training Loss: 0.773780807, Training Accuracy: 76.592\n",
            "Worker 4, [10/16]: Training Loss: 0.733798582, Training Accuracy: 78.152\n",
            "Worker 4, [11/16]: Training Loss: 0.709143827, Training Accuracy: 79.000\n",
            "Worker 4, [12/16]: Training Loss: 0.686359259, Training Accuracy: 79.352\n",
            "Worker 4, [13/16]: Training Loss: 0.657735791, Training Accuracy: 80.536\n",
            "Worker 4, [14/16]: Training Loss: 0.651767839, Training Accuracy: 80.336\n",
            "Worker 4, [15/16]: Training Loss: 0.627757039, Training Accuracy: 81.080\n",
            "Worker 4, [16/16]: Training Loss: 0.601192954, Training Accuracy: 81.600\n",
            "Time taken for training worker 4: 0:01:35.080107\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002864\n",
            "Global Update 07: Test Loss: 2.195450829, Test Accuracy: 50.890\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.243732435, Training Accuracy: 64.160\n",
            "Worker 1, [02/16]: Training Loss: 1.088201405, Training Accuracy: 68.256\n",
            "Worker 1, [03/16]: Training Loss: 0.977057414, Training Accuracy: 70.912\n",
            "Worker 1, [04/16]: Training Loss: 0.903815261, Training Accuracy: 73.040\n",
            "Worker 1, [05/16]: Training Loss: 0.839850392, Training Accuracy: 75.336\n",
            "Worker 1, [06/16]: Training Loss: 0.802578842, Training Accuracy: 76.168\n",
            "Worker 1, [07/16]: Training Loss: 0.763880317, Training Accuracy: 77.384\n",
            "Worker 1, [08/16]: Training Loss: 0.731206881, Training Accuracy: 78.664\n",
            "Worker 1, [09/16]: Training Loss: 0.691293795, Training Accuracy: 79.952\n",
            "Worker 1, [10/16]: Training Loss: 0.642978425, Training Accuracy: 80.912\n",
            "Worker 1, [11/16]: Training Loss: 0.615629301, Training Accuracy: 82.016\n",
            "Worker 1, [12/16]: Training Loss: 0.594408480, Training Accuracy: 82.768\n",
            "Worker 1, [13/16]: Training Loss: 0.583197250, Training Accuracy: 82.800\n",
            "Worker 1, [14/16]: Training Loss: 0.563968461, Training Accuracy: 83.832\n",
            "Worker 1, [15/16]: Training Loss: 0.543342320, Training Accuracy: 84.320\n",
            "Worker 1, [16/16]: Training Loss: 0.520518530, Training Accuracy: 84.968\n",
            "Time taken for training worker 1: 0:01:38.637203\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.248282831, Training Accuracy: 63.456\n",
            "Worker 2, [02/16]: Training Loss: 1.076363221, Training Accuracy: 68.248\n",
            "Worker 2, [03/16]: Training Loss: 0.973721812, Training Accuracy: 70.864\n",
            "Worker 2, [04/16]: Training Loss: 0.911577792, Training Accuracy: 72.808\n",
            "Worker 2, [05/16]: Training Loss: 0.834938717, Training Accuracy: 75.456\n",
            "Worker 2, [06/16]: Training Loss: 0.791126547, Training Accuracy: 76.560\n",
            "Worker 2, [07/16]: Training Loss: 0.748687644, Training Accuracy: 77.360\n",
            "Worker 2, [08/16]: Training Loss: 0.712381801, Training Accuracy: 79.224\n",
            "Worker 2, [09/16]: Training Loss: 0.677926366, Training Accuracy: 79.816\n",
            "Worker 2, [10/16]: Training Loss: 0.640666367, Training Accuracy: 80.976\n",
            "Worker 2, [11/16]: Training Loss: 0.620788597, Training Accuracy: 81.872\n",
            "Worker 2, [12/16]: Training Loss: 0.597018683, Training Accuracy: 82.328\n",
            "Worker 2, [13/16]: Training Loss: 0.582748190, Training Accuracy: 83.080\n",
            "Worker 2, [14/16]: Training Loss: 0.548471692, Training Accuracy: 83.656\n",
            "Worker 2, [15/16]: Training Loss: 0.533884372, Training Accuracy: 84.080\n",
            "Worker 2, [16/16]: Training Loss: 0.506560094, Training Accuracy: 85.424\n",
            "Time taken for training worker 2: 0:01:37.245810\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.250934605, Training Accuracy: 63.440\n",
            "Worker 3, [02/16]: Training Loss: 1.091786969, Training Accuracy: 68.272\n",
            "Worker 3, [03/16]: Training Loss: 0.996341901, Training Accuracy: 70.832\n",
            "Worker 3, [04/16]: Training Loss: 0.922996936, Training Accuracy: 72.816\n",
            "Worker 3, [05/16]: Training Loss: 0.852199986, Training Accuracy: 74.480\n",
            "Worker 3, [06/16]: Training Loss: 0.793289887, Training Accuracy: 76.376\n",
            "Worker 3, [07/16]: Training Loss: 0.762964330, Training Accuracy: 77.360\n",
            "Worker 3, [08/16]: Training Loss: 0.714689282, Training Accuracy: 78.856\n",
            "Worker 3, [09/16]: Training Loss: 0.689632774, Training Accuracy: 79.728\n",
            "Worker 3, [10/16]: Training Loss: 0.659449191, Training Accuracy: 80.720\n",
            "Worker 3, [11/16]: Training Loss: 0.637219563, Training Accuracy: 81.416\n",
            "Worker 3, [12/16]: Training Loss: 0.611256351, Training Accuracy: 81.984\n",
            "Worker 3, [13/16]: Training Loss: 0.577770283, Training Accuracy: 83.080\n",
            "Worker 3, [14/16]: Training Loss: 0.572347387, Training Accuracy: 83.240\n",
            "Worker 3, [15/16]: Training Loss: 0.542331872, Training Accuracy: 84.216\n",
            "Worker 3, [16/16]: Training Loss: 0.516693129, Training Accuracy: 85.136\n",
            "Time taken for training worker 3: 0:01:35.939669\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.278129267, Training Accuracy: 63.248\n",
            "Worker 4, [02/16]: Training Loss: 1.116291563, Training Accuracy: 66.904\n",
            "Worker 4, [03/16]: Training Loss: 1.009631483, Training Accuracy: 70.176\n",
            "Worker 4, [04/16]: Training Loss: 0.933449502, Training Accuracy: 72.440\n",
            "Worker 4, [05/16]: Training Loss: 0.870144409, Training Accuracy: 74.104\n",
            "Worker 4, [06/16]: Training Loss: 0.820762581, Training Accuracy: 75.840\n",
            "Worker 4, [07/16]: Training Loss: 0.771848528, Training Accuracy: 77.024\n",
            "Worker 4, [08/16]: Training Loss: 0.741970359, Training Accuracy: 78.120\n",
            "Worker 4, [09/16]: Training Loss: 0.726527105, Training Accuracy: 78.616\n",
            "Worker 4, [10/16]: Training Loss: 0.676054719, Training Accuracy: 80.320\n",
            "Worker 4, [11/16]: Training Loss: 0.649917993, Training Accuracy: 81.024\n",
            "Worker 4, [12/16]: Training Loss: 0.626280294, Training Accuracy: 81.544\n",
            "Worker 4, [13/16]: Training Loss: 0.605771160, Training Accuracy: 82.328\n",
            "Worker 4, [14/16]: Training Loss: 0.572012269, Training Accuracy: 83.208\n",
            "Worker 4, [15/16]: Training Loss: 0.551953177, Training Accuracy: 83.888\n",
            "Worker 4, [16/16]: Training Loss: 0.528271382, Training Accuracy: 84.776\n",
            "Time taken for training worker 4: 0:01:36.913668\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002619\n",
            "Global Update 08: Test Loss: 2.222064523, Test Accuracy: 51.590\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.142784057, Training Accuracy: 66.640\n",
            "Worker 1, [02/16]: Training Loss: 1.048220493, Training Accuracy: 69.144\n",
            "Worker 1, [03/16]: Training Loss: 0.986123381, Training Accuracy: 70.696\n",
            "Worker 1, [04/16]: Training Loss: 0.934337887, Training Accuracy: 71.904\n",
            "Worker 1, [05/16]: Training Loss: 0.907082625, Training Accuracy: 72.736\n",
            "Worker 1, [06/16]: Training Loss: 0.857558785, Training Accuracy: 74.584\n",
            "Worker 1, [07/16]: Training Loss: 0.836593387, Training Accuracy: 75.288\n",
            "Worker 1, [08/16]: Training Loss: 0.800589084, Training Accuracy: 76.776\n",
            "Worker 1, [09/16]: Training Loss: 0.785131676, Training Accuracy: 76.648\n",
            "Worker 1, [10/16]: Training Loss: 0.759028219, Training Accuracy: 77.808\n",
            "Worker 1, [11/16]: Training Loss: 0.732448971, Training Accuracy: 78.240\n",
            "Worker 1, [12/16]: Training Loss: 0.721206256, Training Accuracy: 78.512\n",
            "Worker 1, [13/16]: Training Loss: 0.707673995, Training Accuracy: 79.440\n",
            "Worker 1, [14/16]: Training Loss: 0.694322478, Training Accuracy: 79.440\n",
            "Worker 1, [15/16]: Training Loss: 0.672034863, Training Accuracy: 80.488\n",
            "Worker 1, [16/16]: Training Loss: 0.655632827, Training Accuracy: 80.896\n",
            "Time taken for training worker 1: 0:01:37.193384\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.137666742, Training Accuracy: 66.864\n",
            "Worker 2, [02/16]: Training Loss: 1.034437496, Training Accuracy: 68.720\n",
            "Worker 2, [03/16]: Training Loss: 0.987476621, Training Accuracy: 70.392\n",
            "Worker 2, [04/16]: Training Loss: 0.934301473, Training Accuracy: 71.736\n",
            "Worker 2, [05/16]: Training Loss: 0.884278198, Training Accuracy: 73.584\n",
            "Worker 2, [06/16]: Training Loss: 0.865059963, Training Accuracy: 74.176\n",
            "Worker 2, [07/16]: Training Loss: 0.803743504, Training Accuracy: 76.096\n",
            "Worker 2, [08/16]: Training Loss: 0.798712369, Training Accuracy: 76.304\n",
            "Worker 2, [09/16]: Training Loss: 0.770713567, Training Accuracy: 77.536\n",
            "Worker 2, [10/16]: Training Loss: 0.767753726, Training Accuracy: 77.368\n",
            "Worker 2, [11/16]: Training Loss: 0.736431056, Training Accuracy: 78.464\n",
            "Worker 2, [12/16]: Training Loss: 0.709986538, Training Accuracy: 79.232\n",
            "Worker 2, [13/16]: Training Loss: 0.700895691, Training Accuracy: 79.592\n",
            "Worker 2, [14/16]: Training Loss: 0.671484819, Training Accuracy: 80.152\n",
            "Worker 2, [15/16]: Training Loss: 0.676437847, Training Accuracy: 80.328\n",
            "Worker 2, [16/16]: Training Loss: 0.653949963, Training Accuracy: 81.024\n",
            "Time taken for training worker 2: 0:01:36.472118\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.134076373, Training Accuracy: 66.944\n",
            "Worker 3, [02/16]: Training Loss: 1.036341435, Training Accuracy: 69.016\n",
            "Worker 3, [03/16]: Training Loss: 0.980108867, Training Accuracy: 70.504\n",
            "Worker 3, [04/16]: Training Loss: 0.944219749, Training Accuracy: 71.952\n",
            "Worker 3, [05/16]: Training Loss: 0.900947458, Training Accuracy: 73.512\n",
            "Worker 3, [06/16]: Training Loss: 0.885009526, Training Accuracy: 74.296\n",
            "Worker 3, [07/16]: Training Loss: 0.841928564, Training Accuracy: 75.224\n",
            "Worker 3, [08/16]: Training Loss: 0.824356518, Training Accuracy: 75.496\n",
            "Worker 3, [09/16]: Training Loss: 0.796990610, Training Accuracy: 76.744\n",
            "Worker 3, [10/16]: Training Loss: 0.775074301, Training Accuracy: 77.120\n",
            "Worker 3, [11/16]: Training Loss: 0.746343510, Training Accuracy: 78.264\n",
            "Worker 3, [12/16]: Training Loss: 0.732812200, Training Accuracy: 78.600\n",
            "Worker 3, [13/16]: Training Loss: 0.715750680, Training Accuracy: 78.792\n",
            "Worker 3, [14/16]: Training Loss: 0.684759888, Training Accuracy: 80.024\n",
            "Worker 3, [15/16]: Training Loss: 0.678139658, Training Accuracy: 80.432\n",
            "Worker 3, [16/16]: Training Loss: 0.646848262, Training Accuracy: 81.448\n",
            "Time taken for training worker 3: 0:01:36.392288\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.167146306, Training Accuracy: 65.848\n",
            "Worker 4, [02/16]: Training Loss: 1.063759359, Training Accuracy: 68.432\n",
            "Worker 4, [03/16]: Training Loss: 1.010818817, Training Accuracy: 69.960\n",
            "Worker 4, [04/16]: Training Loss: 0.957416982, Training Accuracy: 71.416\n",
            "Worker 4, [05/16]: Training Loss: 0.916808351, Training Accuracy: 72.792\n",
            "Worker 4, [06/16]: Training Loss: 0.902285482, Training Accuracy: 73.560\n",
            "Worker 4, [07/16]: Training Loss: 0.867663159, Training Accuracy: 74.072\n",
            "Worker 4, [08/16]: Training Loss: 0.833256801, Training Accuracy: 75.368\n",
            "Worker 4, [09/16]: Training Loss: 0.799080262, Training Accuracy: 75.856\n",
            "Worker 4, [10/16]: Training Loss: 0.771979170, Training Accuracy: 77.376\n",
            "Worker 4, [11/16]: Training Loss: 0.761805676, Training Accuracy: 77.504\n",
            "Worker 4, [12/16]: Training Loss: 0.736968939, Training Accuracy: 78.464\n",
            "Worker 4, [13/16]: Training Loss: 0.726500852, Training Accuracy: 78.624\n",
            "Worker 4, [14/16]: Training Loss: 0.710222073, Training Accuracy: 79.520\n",
            "Worker 4, [15/16]: Training Loss: 0.691868143, Training Accuracy: 79.840\n",
            "Worker 4, [16/16]: Training Loss: 0.692573357, Training Accuracy: 80.000\n",
            "Time taken for training worker 4: 0:01:35.868283\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002679\n",
            "Global Update 09: Test Loss: 2.156295303, Test Accuracy: 51.480\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:57:44.432497\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:32\n",
            "==================================================\n",
            "Worker 1, [01/32]: Training Loss: 4.488577333, Training Accuracy: 2.832\n",
            "Worker 1, [02/32]: Training Loss: 4.102320097, Training Accuracy: 6.736\n",
            "Worker 1, [03/32]: Training Loss: 3.890591818, Training Accuracy: 9.680\n",
            "Worker 1, [04/32]: Training Loss: 3.724995288, Training Accuracy: 12.648\n",
            "Worker 1, [05/32]: Training Loss: 3.577774188, Training Accuracy: 14.928\n",
            "Worker 1, [06/32]: Training Loss: 3.470188162, Training Accuracy: 16.824\n",
            "Worker 1, [07/32]: Training Loss: 3.339989420, Training Accuracy: 18.776\n",
            "Worker 1, [08/32]: Training Loss: 3.239822644, Training Accuracy: 20.088\n",
            "Worker 1, [09/32]: Training Loss: 3.143321009, Training Accuracy: 21.960\n",
            "Worker 1, [10/32]: Training Loss: 3.078907952, Training Accuracy: 23.544\n",
            "Worker 1, [11/32]: Training Loss: 3.002774679, Training Accuracy: 24.968\n",
            "Worker 1, [12/32]: Training Loss: 2.929644273, Training Accuracy: 26.624\n",
            "Worker 1, [13/32]: Training Loss: 2.835126068, Training Accuracy: 28.064\n",
            "Worker 1, [14/32]: Training Loss: 2.773482236, Training Accuracy: 28.976\n",
            "Worker 1, [15/32]: Training Loss: 2.706976645, Training Accuracy: 30.696\n",
            "Worker 1, [16/32]: Training Loss: 2.650870740, Training Accuracy: 32.032\n",
            "Worker 1, [17/32]: Training Loss: 2.585240680, Training Accuracy: 32.744\n",
            "Worker 1, [18/32]: Training Loss: 2.536917668, Training Accuracy: 34.184\n",
            "Worker 1, [19/32]: Training Loss: 2.455505003, Training Accuracy: 35.720\n",
            "Worker 1, [20/32]: Training Loss: 2.429434469, Training Accuracy: 35.816\n",
            "Worker 1, [21/32]: Training Loss: 2.397528530, Training Accuracy: 36.392\n",
            "Worker 1, [22/32]: Training Loss: 2.327581825, Training Accuracy: 37.648\n",
            "Worker 1, [23/32]: Training Loss: 2.288917323, Training Accuracy: 39.232\n",
            "Worker 1, [24/32]: Training Loss: 2.210637777, Training Accuracy: 40.504\n",
            "Worker 1, [25/32]: Training Loss: 2.181287334, Training Accuracy: 41.704\n",
            "Worker 1, [26/32]: Training Loss: 2.173804916, Training Accuracy: 41.960\n",
            "Worker 1, [27/32]: Training Loss: 2.120570324, Training Accuracy: 42.568\n",
            "Worker 1, [28/32]: Training Loss: 2.073084597, Training Accuracy: 43.544\n",
            "Worker 1, [29/32]: Training Loss: 2.057007667, Training Accuracy: 44.072\n",
            "Worker 1, [30/32]: Training Loss: 1.997973227, Training Accuracy: 45.232\n",
            "Worker 1, [31/32]: Training Loss: 1.963405387, Training Accuracy: 45.920\n",
            "Worker 1, [32/32]: Training Loss: 1.949562820, Training Accuracy: 46.552\n",
            "Time taken for training worker 1: 0:03:12.797834\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 4.497439321, Training Accuracy: 2.416\n",
            "Worker 2, [02/32]: Training Loss: 4.111608347, Training Accuracy: 6.368\n",
            "Worker 2, [03/32]: Training Loss: 3.901964789, Training Accuracy: 9.704\n",
            "Worker 2, [04/32]: Training Loss: 3.734044152, Training Accuracy: 12.288\n",
            "Worker 2, [05/32]: Training Loss: 3.602218321, Training Accuracy: 14.104\n",
            "Worker 2, [06/32]: Training Loss: 3.484060750, Training Accuracy: 16.328\n",
            "Worker 2, [07/32]: Training Loss: 3.344174777, Training Accuracy: 18.616\n",
            "Worker 2, [08/32]: Training Loss: 3.266126116, Training Accuracy: 19.672\n",
            "Worker 2, [09/32]: Training Loss: 3.185883959, Training Accuracy: 21.096\n",
            "Worker 2, [10/32]: Training Loss: 3.090424461, Training Accuracy: 23.192\n",
            "Worker 2, [11/32]: Training Loss: 2.996074156, Training Accuracy: 24.288\n",
            "Worker 2, [12/32]: Training Loss: 2.931834883, Training Accuracy: 25.656\n",
            "Worker 2, [13/32]: Training Loss: 2.847647295, Training Accuracy: 28.104\n",
            "Worker 2, [14/32]: Training Loss: 2.777027671, Training Accuracy: 28.520\n",
            "Worker 2, [15/32]: Training Loss: 2.707629249, Training Accuracy: 30.384\n",
            "Worker 2, [16/32]: Training Loss: 2.640167825, Training Accuracy: 31.584\n",
            "Worker 2, [17/32]: Training Loss: 2.586810049, Training Accuracy: 32.456\n",
            "Worker 2, [18/32]: Training Loss: 2.535903491, Training Accuracy: 33.688\n",
            "Worker 2, [19/32]: Training Loss: 2.469212250, Training Accuracy: 35.072\n",
            "Worker 2, [20/32]: Training Loss: 2.412583865, Training Accuracy: 35.864\n",
            "Worker 2, [21/32]: Training Loss: 2.365681885, Training Accuracy: 37.200\n",
            "Worker 2, [22/32]: Training Loss: 2.318921178, Training Accuracy: 38.136\n",
            "Worker 2, [23/32]: Training Loss: 2.265228032, Training Accuracy: 39.400\n",
            "Worker 2, [24/32]: Training Loss: 2.220204827, Training Accuracy: 40.608\n",
            "Worker 2, [25/32]: Training Loss: 2.196384628, Training Accuracy: 41.320\n",
            "Worker 2, [26/32]: Training Loss: 2.132377208, Training Accuracy: 42.664\n",
            "Worker 2, [27/32]: Training Loss: 2.107455653, Training Accuracy: 42.824\n",
            "Worker 2, [28/32]: Training Loss: 2.061384956, Training Accuracy: 43.664\n",
            "Worker 2, [29/32]: Training Loss: 2.028729140, Training Accuracy: 44.344\n",
            "Worker 2, [30/32]: Training Loss: 2.022114454, Training Accuracy: 44.656\n",
            "Worker 2, [31/32]: Training Loss: 1.954277740, Training Accuracy: 46.432\n",
            "Worker 2, [32/32]: Training Loss: 1.930109093, Training Accuracy: 46.672\n",
            "Time taken for training worker 2: 0:03:12.500640\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 4.483724791, Training Accuracy: 2.792\n",
            "Worker 3, [02/32]: Training Loss: 4.113130467, Training Accuracy: 6.688\n",
            "Worker 3, [03/32]: Training Loss: 3.892860481, Training Accuracy: 9.800\n",
            "Worker 3, [04/32]: Training Loss: 3.713044693, Training Accuracy: 12.568\n",
            "Worker 3, [05/32]: Training Loss: 3.576140726, Training Accuracy: 14.808\n",
            "Worker 3, [06/32]: Training Loss: 3.460439984, Training Accuracy: 16.680\n",
            "Worker 3, [07/32]: Training Loss: 3.338184952, Training Accuracy: 18.560\n",
            "Worker 3, [08/32]: Training Loss: 3.240773694, Training Accuracy: 20.176\n",
            "Worker 3, [09/32]: Training Loss: 3.161152883, Training Accuracy: 21.304\n",
            "Worker 3, [10/32]: Training Loss: 3.072026514, Training Accuracy: 22.992\n",
            "Worker 3, [11/32]: Training Loss: 2.998223994, Training Accuracy: 24.248\n",
            "Worker 3, [12/32]: Training Loss: 2.920204279, Training Accuracy: 26.480\n",
            "Worker 3, [13/32]: Training Loss: 2.817819606, Training Accuracy: 28.288\n",
            "Worker 3, [14/32]: Training Loss: 2.759844072, Training Accuracy: 29.208\n",
            "Worker 3, [15/32]: Training Loss: 2.719515793, Training Accuracy: 29.672\n",
            "Worker 3, [16/32]: Training Loss: 2.635722680, Training Accuracy: 32.048\n",
            "Worker 3, [17/32]: Training Loss: 2.574668847, Training Accuracy: 33.288\n",
            "Worker 3, [18/32]: Training Loss: 2.538494580, Training Accuracy: 33.896\n",
            "Worker 3, [19/32]: Training Loss: 2.450423112, Training Accuracy: 35.520\n",
            "Worker 3, [20/32]: Training Loss: 2.424933822, Training Accuracy: 36.512\n",
            "Worker 3, [21/32]: Training Loss: 2.363494416, Training Accuracy: 36.784\n",
            "Worker 3, [22/32]: Training Loss: 2.329425206, Training Accuracy: 38.024\n",
            "Worker 3, [23/32]: Training Loss: 2.278185377, Training Accuracy: 39.264\n",
            "Worker 3, [24/32]: Training Loss: 2.245615839, Training Accuracy: 40.000\n",
            "Worker 3, [25/32]: Training Loss: 2.210076171, Training Accuracy: 40.760\n",
            "Worker 3, [26/32]: Training Loss: 2.147459695, Training Accuracy: 41.736\n",
            "Worker 3, [27/32]: Training Loss: 2.137292849, Training Accuracy: 42.264\n",
            "Worker 3, [28/32]: Training Loss: 2.086708041, Training Accuracy: 42.648\n",
            "Worker 3, [29/32]: Training Loss: 2.047978011, Training Accuracy: 43.936\n",
            "Worker 3, [30/32]: Training Loss: 2.009433734, Training Accuracy: 44.968\n",
            "Worker 3, [31/32]: Training Loss: 1.953224267, Training Accuracy: 46.160\n",
            "Worker 3, [32/32]: Training Loss: 1.931471896, Training Accuracy: 46.544\n",
            "Time taken for training worker 3: 0:03:13.358701\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 4.488616583, Training Accuracy: 2.632\n",
            "Worker 4, [02/32]: Training Loss: 4.117580158, Training Accuracy: 6.496\n",
            "Worker 4, [03/32]: Training Loss: 3.891690605, Training Accuracy: 9.768\n",
            "Worker 4, [04/32]: Training Loss: 3.729187728, Training Accuracy: 12.008\n",
            "Worker 4, [05/32]: Training Loss: 3.598752600, Training Accuracy: 14.280\n",
            "Worker 4, [06/32]: Training Loss: 3.461616429, Training Accuracy: 16.560\n",
            "Worker 4, [07/32]: Training Loss: 3.343067864, Training Accuracy: 18.488\n",
            "Worker 4, [08/32]: Training Loss: 3.235433840, Training Accuracy: 20.984\n",
            "Worker 4, [09/32]: Training Loss: 3.159930358, Training Accuracy: 21.944\n",
            "Worker 4, [10/32]: Training Loss: 3.062882516, Training Accuracy: 23.296\n",
            "Worker 4, [11/32]: Training Loss: 2.969898102, Training Accuracy: 25.344\n",
            "Worker 4, [12/32]: Training Loss: 2.908425191, Training Accuracy: 26.536\n",
            "Worker 4, [13/32]: Training Loss: 2.822930596, Training Accuracy: 28.224\n",
            "Worker 4, [14/32]: Training Loss: 2.748232467, Training Accuracy: 30.120\n",
            "Worker 4, [15/32]: Training Loss: 2.692468120, Training Accuracy: 30.440\n",
            "Worker 4, [16/32]: Training Loss: 2.629377710, Training Accuracy: 32.296\n",
            "Worker 4, [17/32]: Training Loss: 2.543932594, Training Accuracy: 33.368\n",
            "Worker 4, [18/32]: Training Loss: 2.491869562, Training Accuracy: 34.440\n",
            "Worker 4, [19/32]: Training Loss: 2.456988829, Training Accuracy: 35.000\n",
            "Worker 4, [20/32]: Training Loss: 2.401738076, Training Accuracy: 36.008\n",
            "Worker 4, [21/32]: Training Loss: 2.356869997, Training Accuracy: 37.264\n",
            "Worker 4, [22/32]: Training Loss: 2.317591865, Training Accuracy: 38.688\n",
            "Worker 4, [23/32]: Training Loss: 2.287732453, Training Accuracy: 38.856\n",
            "Worker 4, [24/32]: Training Loss: 2.224685250, Training Accuracy: 39.672\n",
            "Worker 4, [25/32]: Training Loss: 2.168878344, Training Accuracy: 41.304\n",
            "Worker 4, [26/32]: Training Loss: 2.149182331, Training Accuracy: 41.752\n",
            "Worker 4, [27/32]: Training Loss: 2.096358302, Training Accuracy: 43.008\n",
            "Worker 4, [28/32]: Training Loss: 2.083170289, Training Accuracy: 43.304\n",
            "Worker 4, [29/32]: Training Loss: 2.050613170, Training Accuracy: 43.784\n",
            "Worker 4, [30/32]: Training Loss: 2.001208507, Training Accuracy: 44.880\n",
            "Worker 4, [31/32]: Training Loss: 1.948078835, Training Accuracy: 46.872\n",
            "Worker 4, [32/32]: Training Loss: 1.930539421, Training Accuracy: 46.816\n",
            "Time taken for training worker 4: 0:03:10.197744\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002655\n",
            "Global Update 01: Test Loss: 3.508392086, Test Accuracy: 26.330\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.840042005, Training Accuracy: 28.296\n",
            "Worker 1, [02/32]: Training Loss: 2.572436301, Training Accuracy: 33.592\n",
            "Worker 1, [03/32]: Training Loss: 2.453991283, Training Accuracy: 35.736\n",
            "Worker 1, [04/32]: Training Loss: 2.353129133, Training Accuracy: 37.800\n",
            "Worker 1, [05/32]: Training Loss: 2.252905462, Training Accuracy: 39.744\n",
            "Worker 1, [06/32]: Training Loss: 2.211107988, Training Accuracy: 40.752\n",
            "Worker 1, [07/32]: Training Loss: 2.127531647, Training Accuracy: 43.000\n",
            "Worker 1, [08/32]: Training Loss: 2.086868189, Training Accuracy: 43.400\n",
            "Worker 1, [09/32]: Training Loss: 2.040166252, Training Accuracy: 44.608\n",
            "Worker 1, [10/32]: Training Loss: 1.987992927, Training Accuracy: 45.560\n",
            "Worker 1, [11/32]: Training Loss: 1.941109519, Training Accuracy: 46.560\n",
            "Worker 1, [12/32]: Training Loss: 1.897549295, Training Accuracy: 47.328\n",
            "Worker 1, [13/32]: Training Loss: 1.865006645, Training Accuracy: 48.368\n",
            "Worker 1, [14/32]: Training Loss: 1.800225381, Training Accuracy: 49.984\n",
            "Worker 1, [15/32]: Training Loss: 1.782674414, Training Accuracy: 50.072\n",
            "Worker 1, [16/32]: Training Loss: 1.724819900, Training Accuracy: 52.048\n",
            "Worker 1, [17/32]: Training Loss: 1.688196355, Training Accuracy: 52.256\n",
            "Worker 1, [18/32]: Training Loss: 1.656441652, Training Accuracy: 53.352\n",
            "Worker 1, [19/32]: Training Loss: 1.603830820, Training Accuracy: 54.496\n",
            "Worker 1, [20/32]: Training Loss: 1.577257656, Training Accuracy: 54.952\n",
            "Worker 1, [21/32]: Training Loss: 1.545339802, Training Accuracy: 56.320\n",
            "Worker 1, [22/32]: Training Loss: 1.511444084, Training Accuracy: 56.760\n",
            "Worker 1, [23/32]: Training Loss: 1.495691290, Training Accuracy: 57.440\n",
            "Worker 1, [24/32]: Training Loss: 1.459389291, Training Accuracy: 58.168\n",
            "Worker 1, [25/32]: Training Loss: 1.421948694, Training Accuracy: 59.424\n",
            "Worker 1, [26/32]: Training Loss: 1.397269651, Training Accuracy: 59.888\n",
            "Worker 1, [27/32]: Training Loss: 1.358466524, Training Accuracy: 60.952\n",
            "Worker 1, [28/32]: Training Loss: 1.380515231, Training Accuracy: 60.552\n",
            "Worker 1, [29/32]: Training Loss: 1.356593866, Training Accuracy: 60.976\n",
            "Worker 1, [30/32]: Training Loss: 1.327579763, Training Accuracy: 61.784\n",
            "Worker 1, [31/32]: Training Loss: 1.315422335, Training Accuracy: 61.952\n",
            "Worker 1, [32/32]: Training Loss: 1.277954251, Training Accuracy: 62.856\n",
            "Time taken for training worker 1: 0:03:11.893862\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.809752750, Training Accuracy: 28.360\n",
            "Worker 2, [02/32]: Training Loss: 2.549328643, Training Accuracy: 33.696\n",
            "Worker 2, [03/32]: Training Loss: 2.432729577, Training Accuracy: 36.184\n",
            "Worker 2, [04/32]: Training Loss: 2.338271119, Training Accuracy: 38.032\n",
            "Worker 2, [05/32]: Training Loss: 2.275531392, Training Accuracy: 39.296\n",
            "Worker 2, [06/32]: Training Loss: 2.204902985, Training Accuracy: 40.360\n",
            "Worker 2, [07/32]: Training Loss: 2.135871713, Training Accuracy: 41.984\n",
            "Worker 2, [08/32]: Training Loss: 2.076124463, Training Accuracy: 43.216\n",
            "Worker 2, [09/32]: Training Loss: 2.020979046, Training Accuracy: 44.760\n",
            "Worker 2, [10/32]: Training Loss: 1.962068941, Training Accuracy: 45.992\n",
            "Worker 2, [11/32]: Training Loss: 1.936242912, Training Accuracy: 46.328\n",
            "Worker 2, [12/32]: Training Loss: 1.871154523, Training Accuracy: 47.936\n",
            "Worker 2, [13/32]: Training Loss: 1.836099702, Training Accuracy: 48.480\n",
            "Worker 2, [14/32]: Training Loss: 1.795184778, Training Accuracy: 50.248\n",
            "Worker 2, [15/32]: Training Loss: 1.732316249, Training Accuracy: 51.040\n",
            "Worker 2, [16/32]: Training Loss: 1.728922022, Training Accuracy: 51.344\n",
            "Worker 2, [17/32]: Training Loss: 1.685052304, Training Accuracy: 53.064\n",
            "Worker 2, [18/32]: Training Loss: 1.665020864, Training Accuracy: 53.024\n",
            "Worker 2, [19/32]: Training Loss: 1.606931310, Training Accuracy: 54.224\n",
            "Worker 2, [20/32]: Training Loss: 1.553795150, Training Accuracy: 55.760\n",
            "Worker 2, [21/32]: Training Loss: 1.533681115, Training Accuracy: 56.296\n",
            "Worker 2, [22/32]: Training Loss: 1.514419595, Training Accuracy: 56.272\n",
            "Worker 2, [23/32]: Training Loss: 1.513241580, Training Accuracy: 56.856\n",
            "Worker 2, [24/32]: Training Loss: 1.430145804, Training Accuracy: 58.744\n",
            "Worker 2, [25/32]: Training Loss: 1.431553156, Training Accuracy: 58.576\n",
            "Worker 2, [26/32]: Training Loss: 1.386011141, Training Accuracy: 59.824\n",
            "Worker 2, [27/32]: Training Loss: 1.368284001, Training Accuracy: 60.184\n",
            "Worker 2, [28/32]: Training Loss: 1.333836110, Training Accuracy: 61.216\n",
            "Worker 2, [29/32]: Training Loss: 1.317005614, Training Accuracy: 61.592\n",
            "Worker 2, [30/32]: Training Loss: 1.288883879, Training Accuracy: 62.664\n",
            "Worker 2, [31/32]: Training Loss: 1.265837666, Training Accuracy: 63.128\n",
            "Worker 2, [32/32]: Training Loss: 1.274023723, Training Accuracy: 62.672\n",
            "Time taken for training worker 2: 0:03:10.767973\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 2.816678230, Training Accuracy: 28.928\n",
            "Worker 3, [02/32]: Training Loss: 2.548953093, Training Accuracy: 33.712\n",
            "Worker 3, [03/32]: Training Loss: 2.427181141, Training Accuracy: 36.504\n",
            "Worker 3, [04/32]: Training Loss: 2.346636944, Training Accuracy: 38.008\n",
            "Worker 3, [05/32]: Training Loss: 2.276880868, Training Accuracy: 38.888\n",
            "Worker 3, [06/32]: Training Loss: 2.203897263, Training Accuracy: 40.448\n",
            "Worker 3, [07/32]: Training Loss: 2.145111169, Training Accuracy: 42.336\n",
            "Worker 3, [08/32]: Training Loss: 2.085433729, Training Accuracy: 43.352\n",
            "Worker 3, [09/32]: Training Loss: 2.027159174, Training Accuracy: 44.392\n",
            "Worker 3, [10/32]: Training Loss: 1.988740646, Training Accuracy: 45.624\n",
            "Worker 3, [11/32]: Training Loss: 1.933083692, Training Accuracy: 46.720\n",
            "Worker 3, [12/32]: Training Loss: 1.881683428, Training Accuracy: 47.256\n",
            "Worker 3, [13/32]: Training Loss: 1.849115420, Training Accuracy: 48.992\n",
            "Worker 3, [14/32]: Training Loss: 1.793230544, Training Accuracy: 50.480\n",
            "Worker 3, [15/32]: Training Loss: 1.789773147, Training Accuracy: 49.856\n",
            "Worker 3, [16/32]: Training Loss: 1.738647913, Training Accuracy: 51.312\n",
            "Worker 3, [17/32]: Training Loss: 1.697230092, Training Accuracy: 52.272\n",
            "Worker 3, [18/32]: Training Loss: 1.651262167, Training Accuracy: 53.160\n",
            "Worker 3, [19/32]: Training Loss: 1.616624289, Training Accuracy: 54.368\n",
            "Worker 3, [20/32]: Training Loss: 1.580304728, Training Accuracy: 55.464\n",
            "Worker 3, [21/32]: Training Loss: 1.581576869, Training Accuracy: 54.824\n",
            "Worker 3, [22/32]: Training Loss: 1.521611670, Training Accuracy: 56.528\n",
            "Worker 3, [23/32]: Training Loss: 1.490983058, Training Accuracy: 57.832\n",
            "Worker 3, [24/32]: Training Loss: 1.475506708, Training Accuracy: 57.824\n",
            "Worker 3, [25/32]: Training Loss: 1.436828529, Training Accuracy: 58.416\n",
            "Worker 3, [26/32]: Training Loss: 1.440197387, Training Accuracy: 58.816\n",
            "Worker 3, [27/32]: Training Loss: 1.394493431, Training Accuracy: 60.408\n",
            "Worker 3, [28/32]: Training Loss: 1.373539797, Training Accuracy: 60.160\n",
            "Worker 3, [29/32]: Training Loss: 1.359154303, Training Accuracy: 60.560\n",
            "Worker 3, [30/32]: Training Loss: 1.351510427, Training Accuracy: 61.000\n",
            "Worker 3, [31/32]: Training Loss: 1.327207071, Training Accuracy: 61.408\n",
            "Worker 3, [32/32]: Training Loss: 1.284574880, Training Accuracy: 62.024\n",
            "Time taken for training worker 3: 0:03:10.447915\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 2.819447724, Training Accuracy: 28.680\n",
            "Worker 4, [02/32]: Training Loss: 2.578949833, Training Accuracy: 33.304\n",
            "Worker 4, [03/32]: Training Loss: 2.456292741, Training Accuracy: 35.416\n",
            "Worker 4, [04/32]: Training Loss: 2.363428759, Training Accuracy: 37.192\n",
            "Worker 4, [05/32]: Training Loss: 2.290515195, Training Accuracy: 38.792\n",
            "Worker 4, [06/32]: Training Loss: 2.210016988, Training Accuracy: 40.104\n",
            "Worker 4, [07/32]: Training Loss: 2.179668162, Training Accuracy: 41.112\n",
            "Worker 4, [08/32]: Training Loss: 2.098478916, Training Accuracy: 43.032\n",
            "Worker 4, [09/32]: Training Loss: 2.048941837, Training Accuracy: 43.800\n",
            "Worker 4, [10/32]: Training Loss: 1.979057948, Training Accuracy: 45.272\n",
            "Worker 4, [11/32]: Training Loss: 1.931690179, Training Accuracy: 47.096\n",
            "Worker 4, [12/32]: Training Loss: 1.891671453, Training Accuracy: 47.768\n",
            "Worker 4, [13/32]: Training Loss: 1.831309361, Training Accuracy: 49.160\n",
            "Worker 4, [14/32]: Training Loss: 1.843943090, Training Accuracy: 49.184\n",
            "Worker 4, [15/32]: Training Loss: 1.766411438, Training Accuracy: 50.624\n",
            "Worker 4, [16/32]: Training Loss: 1.746139095, Training Accuracy: 51.312\n",
            "Worker 4, [17/32]: Training Loss: 1.707302518, Training Accuracy: 52.048\n",
            "Worker 4, [18/32]: Training Loss: 1.657159060, Training Accuracy: 52.912\n",
            "Worker 4, [19/32]: Training Loss: 1.621562517, Training Accuracy: 54.208\n",
            "Worker 4, [20/32]: Training Loss: 1.598277019, Training Accuracy: 54.848\n",
            "Worker 4, [21/32]: Training Loss: 1.576294793, Training Accuracy: 55.152\n",
            "Worker 4, [22/32]: Training Loss: 1.536410540, Training Accuracy: 56.536\n",
            "Worker 4, [23/32]: Training Loss: 1.486508742, Training Accuracy: 57.608\n",
            "Worker 4, [24/32]: Training Loss: 1.476918250, Training Accuracy: 57.296\n",
            "Worker 4, [25/32]: Training Loss: 1.452675699, Training Accuracy: 58.760\n",
            "Worker 4, [26/32]: Training Loss: 1.438650458, Training Accuracy: 58.544\n",
            "Worker 4, [27/32]: Training Loss: 1.380231577, Training Accuracy: 60.448\n",
            "Worker 4, [28/32]: Training Loss: 1.407975120, Training Accuracy: 59.432\n",
            "Worker 4, [29/32]: Training Loss: 1.341673340, Training Accuracy: 61.120\n",
            "Worker 4, [30/32]: Training Loss: 1.344400145, Training Accuracy: 60.736\n",
            "Worker 4, [31/32]: Training Loss: 1.301738561, Training Accuracy: 62.264\n",
            "Worker 4, [32/32]: Training Loss: 1.301224263, Training Accuracy: 61.952\n",
            "Time taken for training worker 4: 0:03:18.117360\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003235\n",
            "Global Update 02: Test Loss: 2.168924171, Test Accuracy: 43.720\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.042962739, Training Accuracy: 45.304\n",
            "Worker 1, [02/32]: Training Loss: 1.848519044, Training Accuracy: 49.280\n",
            "Worker 1, [03/32]: Training Loss: 1.722797861, Training Accuracy: 52.776\n",
            "Worker 1, [04/32]: Training Loss: 1.595202775, Training Accuracy: 55.064\n",
            "Worker 1, [05/32]: Training Loss: 1.563579405, Training Accuracy: 55.800\n",
            "Worker 1, [06/32]: Training Loss: 1.467084613, Training Accuracy: 58.184\n",
            "Worker 1, [07/32]: Training Loss: 1.394524406, Training Accuracy: 60.032\n",
            "Worker 1, [08/32]: Training Loss: 1.332256466, Training Accuracy: 61.648\n",
            "Worker 1, [09/32]: Training Loss: 1.282874050, Training Accuracy: 62.712\n",
            "Worker 1, [10/32]: Training Loss: 1.242165486, Training Accuracy: 64.168\n",
            "Worker 1, [11/32]: Training Loss: 1.204494605, Training Accuracy: 65.000\n",
            "Worker 1, [12/32]: Training Loss: 1.157233393, Training Accuracy: 65.808\n",
            "Worker 1, [13/32]: Training Loss: 1.110564943, Training Accuracy: 67.160\n",
            "Worker 1, [14/32]: Training Loss: 1.102889422, Training Accuracy: 67.760\n",
            "Worker 1, [15/32]: Training Loss: 1.059177885, Training Accuracy: 68.816\n",
            "Worker 1, [16/32]: Training Loss: 1.037101040, Training Accuracy: 69.144\n",
            "Worker 1, [17/32]: Training Loss: 0.993634539, Training Accuracy: 70.176\n",
            "Worker 1, [18/32]: Training Loss: 0.972267640, Training Accuracy: 70.864\n",
            "Worker 1, [19/32]: Training Loss: 0.989405079, Training Accuracy: 70.400\n",
            "Worker 1, [20/32]: Training Loss: 0.939216260, Training Accuracy: 71.600\n",
            "Worker 1, [21/32]: Training Loss: 0.928436305, Training Accuracy: 72.280\n",
            "Worker 1, [22/32]: Training Loss: 0.883257085, Training Accuracy: 73.688\n",
            "Worker 1, [23/32]: Training Loss: 0.858567411, Training Accuracy: 74.256\n",
            "Worker 1, [24/32]: Training Loss: 0.835458565, Training Accuracy: 74.536\n",
            "Worker 1, [25/32]: Training Loss: 0.829190014, Training Accuracy: 74.616\n",
            "Worker 1, [26/32]: Training Loss: 0.814075494, Training Accuracy: 75.328\n",
            "Worker 1, [27/32]: Training Loss: 0.804198494, Training Accuracy: 75.840\n",
            "Worker 1, [28/32]: Training Loss: 0.789587654, Training Accuracy: 75.768\n",
            "Worker 1, [29/32]: Training Loss: 0.752852977, Training Accuracy: 77.272\n",
            "Worker 1, [30/32]: Training Loss: 0.797936537, Training Accuracy: 75.952\n",
            "Worker 1, [31/32]: Training Loss: 0.757208903, Training Accuracy: 76.712\n",
            "Worker 1, [32/32]: Training Loss: 0.732130678, Training Accuracy: 77.712\n",
            "Time taken for training worker 1: 0:03:22.457609\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.023371346, Training Accuracy: 45.200\n",
            "Worker 2, [02/32]: Training Loss: 1.826455193, Training Accuracy: 49.720\n",
            "Worker 2, [03/32]: Training Loss: 1.686930226, Training Accuracy: 53.112\n",
            "Worker 2, [04/32]: Training Loss: 1.597743354, Training Accuracy: 55.136\n",
            "Worker 2, [05/32]: Training Loss: 1.524409670, Training Accuracy: 56.664\n",
            "Worker 2, [06/32]: Training Loss: 1.433969651, Training Accuracy: 58.648\n",
            "Worker 2, [07/32]: Training Loss: 1.397732143, Training Accuracy: 59.784\n",
            "Worker 2, [08/32]: Training Loss: 1.310526566, Training Accuracy: 62.048\n",
            "Worker 2, [09/32]: Training Loss: 1.282567880, Training Accuracy: 62.904\n",
            "Worker 2, [10/32]: Training Loss: 1.240938191, Training Accuracy: 63.616\n",
            "Worker 2, [11/32]: Training Loss: 1.205783080, Training Accuracy: 64.848\n",
            "Worker 2, [12/32]: Training Loss: 1.140977566, Training Accuracy: 66.200\n",
            "Worker 2, [13/32]: Training Loss: 1.099887468, Training Accuracy: 67.488\n",
            "Worker 2, [14/32]: Training Loss: 1.069711642, Training Accuracy: 68.320\n",
            "Worker 2, [15/32]: Training Loss: 1.048716668, Training Accuracy: 68.824\n",
            "Worker 2, [16/32]: Training Loss: 1.026260187, Training Accuracy: 69.200\n",
            "Worker 2, [17/32]: Training Loss: 0.980220833, Training Accuracy: 70.864\n",
            "Worker 2, [18/32]: Training Loss: 0.971188927, Training Accuracy: 71.304\n",
            "Worker 2, [19/32]: Training Loss: 0.945696535, Training Accuracy: 71.688\n",
            "Worker 2, [20/32]: Training Loss: 0.914687271, Training Accuracy: 72.672\n",
            "Worker 2, [21/32]: Training Loss: 0.902583739, Training Accuracy: 72.824\n",
            "Worker 2, [22/32]: Training Loss: 0.886057983, Training Accuracy: 73.352\n",
            "Worker 2, [23/32]: Training Loss: 0.854728974, Training Accuracy: 73.880\n",
            "Worker 2, [24/32]: Training Loss: 0.832483670, Training Accuracy: 74.856\n",
            "Worker 2, [25/32]: Training Loss: 0.831498816, Training Accuracy: 75.176\n",
            "Worker 2, [26/32]: Training Loss: 0.809489999, Training Accuracy: 75.312\n",
            "Worker 2, [27/32]: Training Loss: 0.788910283, Training Accuracy: 75.896\n",
            "Worker 2, [28/32]: Training Loss: 0.753549047, Training Accuracy: 77.240\n",
            "Worker 2, [29/32]: Training Loss: 0.776566254, Training Accuracy: 76.448\n",
            "Worker 2, [30/32]: Training Loss: 0.729412542, Training Accuracy: 77.904\n",
            "Worker 2, [31/32]: Training Loss: 0.710857141, Training Accuracy: 78.544\n",
            "Worker 2, [32/32]: Training Loss: 0.728372440, Training Accuracy: 77.880\n",
            "Time taken for training worker 2: 0:03:20.682456\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 2.025608364, Training Accuracy: 45.056\n",
            "Worker 3, [02/32]: Training Loss: 1.829121083, Training Accuracy: 49.976\n",
            "Worker 3, [03/32]: Training Loss: 1.718340208, Training Accuracy: 52.096\n",
            "Worker 3, [04/32]: Training Loss: 1.601689995, Training Accuracy: 54.928\n",
            "Worker 3, [05/32]: Training Loss: 1.543087466, Training Accuracy: 56.512\n",
            "Worker 3, [06/32]: Training Loss: 1.456227408, Training Accuracy: 58.016\n",
            "Worker 3, [07/32]: Training Loss: 1.401885882, Training Accuracy: 59.968\n",
            "Worker 3, [08/32]: Training Loss: 1.353811613, Training Accuracy: 60.512\n",
            "Worker 3, [09/32]: Training Loss: 1.290246790, Training Accuracy: 62.704\n",
            "Worker 3, [10/32]: Training Loss: 1.252512211, Training Accuracy: 63.328\n",
            "Worker 3, [11/32]: Training Loss: 1.211384466, Training Accuracy: 64.456\n",
            "Worker 3, [12/32]: Training Loss: 1.173932006, Training Accuracy: 65.416\n",
            "Worker 3, [13/32]: Training Loss: 1.154405676, Training Accuracy: 66.024\n",
            "Worker 3, [14/32]: Training Loss: 1.088791686, Training Accuracy: 68.152\n",
            "Worker 3, [15/32]: Training Loss: 1.076492776, Training Accuracy: 68.208\n",
            "Worker 3, [16/32]: Training Loss: 1.037822556, Training Accuracy: 69.256\n",
            "Worker 3, [17/32]: Training Loss: 1.028148800, Training Accuracy: 68.816\n",
            "Worker 3, [18/32]: Training Loss: 0.974362013, Training Accuracy: 70.216\n",
            "Worker 3, [19/32]: Training Loss: 0.970178944, Training Accuracy: 71.032\n",
            "Worker 3, [20/32]: Training Loss: 0.910843212, Training Accuracy: 72.456\n",
            "Worker 3, [21/32]: Training Loss: 0.925388846, Training Accuracy: 71.840\n",
            "Worker 3, [22/32]: Training Loss: 0.906396284, Training Accuracy: 72.912\n",
            "Worker 3, [23/32]: Training Loss: 0.875669538, Training Accuracy: 73.200\n",
            "Worker 3, [24/32]: Training Loss: 0.862990428, Training Accuracy: 73.504\n",
            "Worker 3, [25/32]: Training Loss: 0.825772677, Training Accuracy: 74.704\n",
            "Worker 3, [26/32]: Training Loss: 0.798069558, Training Accuracy: 75.744\n",
            "Worker 3, [27/32]: Training Loss: 0.796170592, Training Accuracy: 76.064\n",
            "Worker 3, [28/32]: Training Loss: 0.795402802, Training Accuracy: 75.584\n",
            "Worker 3, [29/32]: Training Loss: 0.779330592, Training Accuracy: 76.432\n",
            "Worker 3, [30/32]: Training Loss: 0.760563842, Training Accuracy: 76.360\n",
            "Worker 3, [31/32]: Training Loss: 0.723188152, Training Accuracy: 77.656\n",
            "Worker 3, [32/32]: Training Loss: 0.754712769, Training Accuracy: 76.800\n",
            "Time taken for training worker 3: 0:03:21.290533\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 2.045689666, Training Accuracy: 45.096\n",
            "Worker 4, [02/32]: Training Loss: 1.872883241, Training Accuracy: 48.376\n",
            "Worker 4, [03/32]: Training Loss: 1.733044711, Training Accuracy: 51.720\n",
            "Worker 4, [04/32]: Training Loss: 1.626073832, Training Accuracy: 54.200\n",
            "Worker 4, [05/32]: Training Loss: 1.560128019, Training Accuracy: 55.688\n",
            "Worker 4, [06/32]: Training Loss: 1.465804459, Training Accuracy: 57.824\n",
            "Worker 4, [07/32]: Training Loss: 1.396070376, Training Accuracy: 59.752\n",
            "Worker 4, [08/32]: Training Loss: 1.355370123, Training Accuracy: 60.880\n",
            "Worker 4, [09/32]: Training Loss: 1.329241572, Training Accuracy: 61.824\n",
            "Worker 4, [10/32]: Training Loss: 1.253457254, Training Accuracy: 63.504\n",
            "Worker 4, [11/32]: Training Loss: 1.207039894, Training Accuracy: 63.960\n",
            "Worker 4, [12/32]: Training Loss: 1.177623594, Training Accuracy: 65.200\n",
            "Worker 4, [13/32]: Training Loss: 1.125761131, Training Accuracy: 67.096\n",
            "Worker 4, [14/32]: Training Loss: 1.109023636, Training Accuracy: 66.984\n",
            "Worker 4, [15/32]: Training Loss: 1.060845495, Training Accuracy: 68.688\n",
            "Worker 4, [16/32]: Training Loss: 1.046993316, Training Accuracy: 68.976\n",
            "Worker 4, [17/32]: Training Loss: 1.008920474, Training Accuracy: 69.488\n",
            "Worker 4, [18/32]: Training Loss: 1.007043276, Training Accuracy: 69.776\n",
            "Worker 4, [19/32]: Training Loss: 0.978821857, Training Accuracy: 71.040\n",
            "Worker 4, [20/32]: Training Loss: 0.921994302, Training Accuracy: 72.320\n",
            "Worker 4, [21/32]: Training Loss: 0.919911690, Training Accuracy: 72.488\n",
            "Worker 4, [22/32]: Training Loss: 0.891809375, Training Accuracy: 72.728\n",
            "Worker 4, [23/32]: Training Loss: 0.874351672, Training Accuracy: 73.904\n",
            "Worker 4, [24/32]: Training Loss: 0.871926546, Training Accuracy: 73.736\n",
            "Worker 4, [25/32]: Training Loss: 0.845079485, Training Accuracy: 74.760\n",
            "Worker 4, [26/32]: Training Loss: 0.806834353, Training Accuracy: 75.320\n",
            "Worker 4, [27/32]: Training Loss: 0.798495881, Training Accuracy: 75.656\n",
            "Worker 4, [28/32]: Training Loss: 0.793199945, Training Accuracy: 75.776\n",
            "Worker 4, [29/32]: Training Loss: 0.774487881, Training Accuracy: 76.144\n",
            "Worker 4, [30/32]: Training Loss: 0.777481151, Training Accuracy: 76.432\n",
            "Worker 4, [31/32]: Training Loss: 0.758113863, Training Accuracy: 76.872\n",
            "Worker 4, [32/32]: Training Loss: 0.749446111, Training Accuracy: 77.064\n",
            "Time taken for training worker 4: 0:03:19.293709\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.009579\n",
            "Global Update 03: Test Loss: 2.174707738, Test Accuracy: 47.930\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 1.647201662, Training Accuracy: 54.584\n",
            "Worker 1, [02/32]: Training Loss: 1.475448678, Training Accuracy: 58.208\n",
            "Worker 1, [03/32]: Training Loss: 1.364917736, Training Accuracy: 60.816\n",
            "Worker 1, [04/32]: Training Loss: 1.259023055, Training Accuracy: 63.512\n",
            "Worker 1, [05/32]: Training Loss: 1.183807338, Training Accuracy: 65.632\n",
            "Worker 1, [06/32]: Training Loss: 1.117421991, Training Accuracy: 67.864\n",
            "Worker 1, [07/32]: Training Loss: 1.066123908, Training Accuracy: 69.152\n",
            "Worker 1, [08/32]: Training Loss: 1.017206472, Training Accuracy: 70.200\n",
            "Worker 1, [09/32]: Training Loss: 0.958878410, Training Accuracy: 71.696\n",
            "Worker 1, [10/32]: Training Loss: 0.919036660, Training Accuracy: 72.808\n",
            "Worker 1, [11/32]: Training Loss: 0.877107342, Training Accuracy: 73.904\n",
            "Worker 1, [12/32]: Training Loss: 0.826561592, Training Accuracy: 75.664\n",
            "Worker 1, [13/32]: Training Loss: 0.790218804, Training Accuracy: 77.104\n",
            "Worker 1, [14/32]: Training Loss: 0.778363145, Training Accuracy: 77.432\n",
            "Worker 1, [15/32]: Training Loss: 0.745055006, Training Accuracy: 77.880\n",
            "Worker 1, [16/32]: Training Loss: 0.715232719, Training Accuracy: 79.040\n",
            "Worker 1, [17/32]: Training Loss: 0.681275563, Training Accuracy: 79.992\n",
            "Worker 1, [18/32]: Training Loss: 0.659877825, Training Accuracy: 80.640\n",
            "Worker 1, [19/32]: Training Loss: 0.626575095, Training Accuracy: 81.648\n",
            "Worker 1, [20/32]: Training Loss: 0.612526804, Training Accuracy: 81.936\n",
            "Worker 1, [21/32]: Training Loss: 0.584112656, Training Accuracy: 82.792\n",
            "Worker 1, [22/32]: Training Loss: 0.579433856, Training Accuracy: 82.720\n",
            "Worker 1, [23/32]: Training Loss: 0.558810568, Training Accuracy: 84.152\n",
            "Worker 1, [24/32]: Training Loss: 0.538659568, Training Accuracy: 84.208\n",
            "Worker 1, [25/32]: Training Loss: 0.519734834, Training Accuracy: 84.808\n",
            "Worker 1, [26/32]: Training Loss: 0.514352414, Training Accuracy: 84.976\n",
            "Worker 1, [27/32]: Training Loss: 0.505726679, Training Accuracy: 85.008\n",
            "Worker 1, [28/32]: Training Loss: 0.473346000, Training Accuracy: 86.168\n",
            "Worker 1, [29/32]: Training Loss: 0.452100696, Training Accuracy: 87.000\n",
            "Worker 1, [30/32]: Training Loss: 0.443861385, Training Accuracy: 87.056\n",
            "Worker 1, [31/32]: Training Loss: 0.449056612, Training Accuracy: 86.888\n",
            "Worker 1, [32/32]: Training Loss: 0.435430306, Training Accuracy: 87.320\n",
            "Time taken for training worker 1: 0:03:12.742039\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 1.633922511, Training Accuracy: 54.000\n",
            "Worker 2, [02/32]: Training Loss: 1.468362651, Training Accuracy: 57.960\n",
            "Worker 2, [03/32]: Training Loss: 1.351947584, Training Accuracy: 60.552\n",
            "Worker 2, [04/32]: Training Loss: 1.252448613, Training Accuracy: 64.088\n",
            "Worker 2, [05/32]: Training Loss: 1.186856409, Training Accuracy: 66.232\n",
            "Worker 2, [06/32]: Training Loss: 1.104691099, Training Accuracy: 67.616\n",
            "Worker 2, [07/32]: Training Loss: 1.035242057, Training Accuracy: 69.976\n",
            "Worker 2, [08/32]: Training Loss: 0.976260859, Training Accuracy: 71.480\n",
            "Worker 2, [09/32]: Training Loss: 0.937979067, Training Accuracy: 72.112\n",
            "Worker 2, [10/32]: Training Loss: 0.890146242, Training Accuracy: 73.648\n",
            "Worker 2, [11/32]: Training Loss: 0.855717480, Training Accuracy: 74.640\n",
            "Worker 2, [12/32]: Training Loss: 0.826391181, Training Accuracy: 75.536\n",
            "Worker 2, [13/32]: Training Loss: 0.767242032, Training Accuracy: 77.440\n",
            "Worker 2, [14/32]: Training Loss: 0.738633598, Training Accuracy: 78.456\n",
            "Worker 2, [15/32]: Training Loss: 0.732914742, Training Accuracy: 78.520\n",
            "Worker 2, [16/32]: Training Loss: 0.702730387, Training Accuracy: 79.160\n",
            "Worker 2, [17/32]: Training Loss: 0.653779438, Training Accuracy: 80.704\n",
            "Worker 2, [18/32]: Training Loss: 0.646157750, Training Accuracy: 80.704\n",
            "Worker 2, [19/32]: Training Loss: 0.617593662, Training Accuracy: 82.016\n",
            "Worker 2, [20/32]: Training Loss: 0.589099789, Training Accuracy: 82.696\n",
            "Worker 2, [21/32]: Training Loss: 0.583922658, Training Accuracy: 82.968\n",
            "Worker 2, [22/32]: Training Loss: 0.544238719, Training Accuracy: 84.080\n",
            "Worker 2, [23/32]: Training Loss: 0.535935252, Training Accuracy: 84.040\n",
            "Worker 2, [24/32]: Training Loss: 0.529045402, Training Accuracy: 84.392\n",
            "Worker 2, [25/32]: Training Loss: 0.507137203, Training Accuracy: 85.152\n",
            "Worker 2, [26/32]: Training Loss: 0.493863338, Training Accuracy: 85.312\n",
            "Worker 2, [27/32]: Training Loss: 0.468482217, Training Accuracy: 86.296\n",
            "Worker 2, [28/32]: Training Loss: 0.470958831, Training Accuracy: 86.336\n",
            "Worker 2, [29/32]: Training Loss: 0.449251023, Training Accuracy: 87.056\n",
            "Worker 2, [30/32]: Training Loss: 0.446337757, Training Accuracy: 86.904\n",
            "Worker 2, [31/32]: Training Loss: 0.416355163, Training Accuracy: 87.888\n",
            "Worker 2, [32/32]: Training Loss: 0.403650775, Training Accuracy: 88.336\n",
            "Time taken for training worker 2: 0:03:13.488350\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 1.647222607, Training Accuracy: 54.168\n",
            "Worker 3, [02/32]: Training Loss: 1.470312922, Training Accuracy: 58.224\n",
            "Worker 3, [03/32]: Training Loss: 1.349771381, Training Accuracy: 61.480\n",
            "Worker 3, [04/32]: Training Loss: 1.266599907, Training Accuracy: 63.304\n",
            "Worker 3, [05/32]: Training Loss: 1.179659313, Training Accuracy: 65.616\n",
            "Worker 3, [06/32]: Training Loss: 1.116218552, Training Accuracy: 66.912\n",
            "Worker 3, [07/32]: Training Loss: 1.052575277, Training Accuracy: 68.976\n",
            "Worker 3, [08/32]: Training Loss: 0.998265975, Training Accuracy: 71.224\n",
            "Worker 3, [09/32]: Training Loss: 0.954718553, Training Accuracy: 71.968\n",
            "Worker 3, [10/32]: Training Loss: 0.907691745, Training Accuracy: 73.336\n",
            "Worker 3, [11/32]: Training Loss: 0.882231509, Training Accuracy: 74.088\n",
            "Worker 3, [12/32]: Training Loss: 0.831407680, Training Accuracy: 75.928\n",
            "Worker 3, [13/32]: Training Loss: 0.796751427, Training Accuracy: 76.936\n",
            "Worker 3, [14/32]: Training Loss: 0.771566195, Training Accuracy: 77.680\n",
            "Worker 3, [15/32]: Training Loss: 0.727909983, Training Accuracy: 78.264\n",
            "Worker 3, [16/32]: Training Loss: 0.722451086, Training Accuracy: 78.568\n",
            "Worker 3, [17/32]: Training Loss: 0.669536191, Training Accuracy: 80.152\n",
            "Worker 3, [18/32]: Training Loss: 0.656930495, Training Accuracy: 80.848\n",
            "Worker 3, [19/32]: Training Loss: 0.645025403, Training Accuracy: 80.968\n",
            "Worker 3, [20/32]: Training Loss: 0.615873273, Training Accuracy: 81.656\n",
            "Worker 3, [21/32]: Training Loss: 0.608994767, Training Accuracy: 82.264\n",
            "Worker 3, [22/32]: Training Loss: 0.564945040, Training Accuracy: 83.056\n",
            "Worker 3, [23/32]: Training Loss: 0.562763069, Training Accuracy: 83.912\n",
            "Worker 3, [24/32]: Training Loss: 0.532557747, Training Accuracy: 84.904\n",
            "Worker 3, [25/32]: Training Loss: 0.516454641, Training Accuracy: 84.736\n",
            "Worker 3, [26/32]: Training Loss: 0.516492053, Training Accuracy: 84.640\n",
            "Worker 3, [27/32]: Training Loss: 0.494929456, Training Accuracy: 85.488\n",
            "Worker 3, [28/32]: Training Loss: 0.470662601, Training Accuracy: 86.400\n",
            "Worker 3, [29/32]: Training Loss: 0.453683413, Training Accuracy: 86.808\n",
            "Worker 3, [30/32]: Training Loss: 0.449790205, Training Accuracy: 86.744\n",
            "Worker 3, [31/32]: Training Loss: 0.436689004, Training Accuracy: 87.104\n",
            "Worker 3, [32/32]: Training Loss: 0.420192017, Training Accuracy: 87.416\n",
            "Time taken for training worker 3: 0:03:12.992578\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 1.661615020, Training Accuracy: 53.864\n",
            "Worker 4, [02/32]: Training Loss: 1.489264480, Training Accuracy: 57.920\n",
            "Worker 4, [03/32]: Training Loss: 1.383591478, Training Accuracy: 60.624\n",
            "Worker 4, [04/32]: Training Loss: 1.276503472, Training Accuracy: 62.600\n",
            "Worker 4, [05/32]: Training Loss: 1.213680979, Training Accuracy: 65.112\n",
            "Worker 4, [06/32]: Training Loss: 1.147735595, Training Accuracy: 66.440\n",
            "Worker 4, [07/32]: Training Loss: 1.080949107, Training Accuracy: 68.688\n",
            "Worker 4, [08/32]: Training Loss: 1.002535098, Training Accuracy: 70.456\n",
            "Worker 4, [09/32]: Training Loss: 0.966314916, Training Accuracy: 72.040\n",
            "Worker 4, [10/32]: Training Loss: 0.931032461, Training Accuracy: 72.752\n",
            "Worker 4, [11/32]: Training Loss: 0.885696692, Training Accuracy: 73.880\n",
            "Worker 4, [12/32]: Training Loss: 0.842010512, Training Accuracy: 75.296\n",
            "Worker 4, [13/32]: Training Loss: 0.800194605, Training Accuracy: 76.512\n",
            "Worker 4, [14/32]: Training Loss: 0.766872288, Training Accuracy: 77.640\n",
            "Worker 4, [15/32]: Training Loss: 0.759289778, Training Accuracy: 77.896\n",
            "Worker 4, [16/32]: Training Loss: 0.719352554, Training Accuracy: 78.640\n",
            "Worker 4, [17/32]: Training Loss: 0.696082372, Training Accuracy: 79.616\n",
            "Worker 4, [18/32]: Training Loss: 0.663412351, Training Accuracy: 80.488\n",
            "Worker 4, [19/32]: Training Loss: 0.641697739, Training Accuracy: 80.928\n",
            "Worker 4, [20/32]: Training Loss: 0.621193708, Training Accuracy: 81.672\n",
            "Worker 4, [21/32]: Training Loss: 0.595629519, Training Accuracy: 82.560\n",
            "Worker 4, [22/32]: Training Loss: 0.576449038, Training Accuracy: 83.016\n",
            "Worker 4, [23/32]: Training Loss: 0.555847210, Training Accuracy: 83.520\n",
            "Worker 4, [24/32]: Training Loss: 0.534652582, Training Accuracy: 84.264\n",
            "Worker 4, [25/32]: Training Loss: 0.521962863, Training Accuracy: 84.760\n",
            "Worker 4, [26/32]: Training Loss: 0.509418170, Training Accuracy: 85.000\n",
            "Worker 4, [27/32]: Training Loss: 0.503806055, Training Accuracy: 85.136\n",
            "Worker 4, [28/32]: Training Loss: 0.482785192, Training Accuracy: 86.024\n",
            "Worker 4, [29/32]: Training Loss: 0.463670568, Training Accuracy: 86.368\n",
            "Worker 4, [30/32]: Training Loss: 0.470145716, Training Accuracy: 86.040\n",
            "Worker 4, [31/32]: Training Loss: 0.461315364, Training Accuracy: 86.600\n",
            "Worker 4, [32/32]: Training Loss: 0.439118968, Training Accuracy: 87.152\n",
            "Time taken for training worker 4: 0:03:10.888473\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002697\n",
            "Global Update 04: Test Loss: 2.377478916, Test Accuracy: 50.100\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:51:57.894859\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:64\n",
            "==================================================\n",
            "Worker 1, [01/64]: Training Loss: 4.489053335, Training Accuracy: 2.528\n",
            "Worker 1, [02/64]: Training Loss: 4.103809173, Training Accuracy: 6.656\n",
            "Worker 1, [03/64]: Training Loss: 3.894937620, Training Accuracy: 9.784\n",
            "Worker 1, [04/64]: Training Loss: 3.722447252, Training Accuracy: 12.264\n",
            "Worker 1, [05/64]: Training Loss: 3.587752296, Training Accuracy: 14.544\n",
            "Worker 1, [06/64]: Training Loss: 3.439534452, Training Accuracy: 17.120\n",
            "Worker 1, [07/64]: Training Loss: 3.336848674, Training Accuracy: 19.088\n",
            "Worker 1, [08/64]: Training Loss: 3.233700998, Training Accuracy: 20.336\n",
            "Worker 1, [09/64]: Training Loss: 3.150658039, Training Accuracy: 21.888\n",
            "Worker 1, [10/64]: Training Loss: 3.070111006, Training Accuracy: 23.200\n",
            "Worker 1, [11/64]: Training Loss: 2.988968764, Training Accuracy: 25.200\n",
            "Worker 1, [12/64]: Training Loss: 2.906825474, Training Accuracy: 26.656\n",
            "Worker 1, [13/64]: Training Loss: 2.855081372, Training Accuracy: 27.680\n",
            "Worker 1, [14/64]: Training Loss: 2.771519351, Training Accuracy: 29.736\n",
            "Worker 1, [15/64]: Training Loss: 2.703968614, Training Accuracy: 30.472\n",
            "Worker 1, [16/64]: Training Loss: 2.645562101, Training Accuracy: 31.536\n",
            "Worker 1, [17/64]: Training Loss: 2.574420492, Training Accuracy: 33.528\n",
            "Worker 1, [18/64]: Training Loss: 2.523036639, Training Accuracy: 33.616\n",
            "Worker 1, [19/64]: Training Loss: 2.483383036, Training Accuracy: 34.840\n",
            "Worker 1, [20/64]: Training Loss: 2.413852600, Training Accuracy: 36.200\n",
            "Worker 1, [21/64]: Training Loss: 2.391779091, Training Accuracy: 36.840\n",
            "Worker 1, [22/64]: Training Loss: 2.306880311, Training Accuracy: 38.568\n",
            "Worker 1, [23/64]: Training Loss: 2.280193846, Training Accuracy: 39.208\n",
            "Worker 1, [24/64]: Training Loss: 2.234063374, Training Accuracy: 39.632\n",
            "Worker 1, [25/64]: Training Loss: 2.203987099, Training Accuracy: 40.792\n",
            "Worker 1, [26/64]: Training Loss: 2.149929356, Training Accuracy: 42.112\n",
            "Worker 1, [27/64]: Training Loss: 2.100851209, Training Accuracy: 43.248\n",
            "Worker 1, [28/64]: Training Loss: 2.080540862, Training Accuracy: 43.336\n",
            "Worker 1, [29/64]: Training Loss: 2.058493007, Training Accuracy: 43.760\n",
            "Worker 1, [30/64]: Training Loss: 2.014298596, Training Accuracy: 44.720\n",
            "Worker 1, [31/64]: Training Loss: 1.966143008, Training Accuracy: 45.784\n",
            "Worker 1, [32/64]: Training Loss: 1.909469803, Training Accuracy: 47.640\n",
            "Worker 1, [33/64]: Training Loss: 1.893738140, Training Accuracy: 47.224\n",
            "Worker 1, [34/64]: Training Loss: 1.843722075, Training Accuracy: 48.576\n",
            "Worker 1, [35/64]: Training Loss: 1.855186063, Training Accuracy: 48.288\n",
            "Worker 1, [36/64]: Training Loss: 1.797844834, Training Accuracy: 50.016\n",
            "Worker 1, [37/64]: Training Loss: 1.795629330, Training Accuracy: 49.840\n",
            "Worker 1, [38/64]: Training Loss: 1.762199364, Training Accuracy: 51.152\n",
            "Worker 1, [39/64]: Training Loss: 1.751340986, Training Accuracy: 51.312\n",
            "Worker 1, [40/64]: Training Loss: 1.704646728, Training Accuracy: 52.464\n",
            "Worker 1, [41/64]: Training Loss: 1.702057576, Training Accuracy: 52.016\n",
            "Worker 1, [42/64]: Training Loss: 1.657109287, Training Accuracy: 52.896\n",
            "Worker 1, [43/64]: Training Loss: 1.650073433, Training Accuracy: 54.008\n",
            "Worker 1, [44/64]: Training Loss: 1.584655285, Training Accuracy: 54.896\n",
            "Worker 1, [45/64]: Training Loss: 1.598683268, Training Accuracy: 55.120\n",
            "Worker 1, [46/64]: Training Loss: 1.557731686, Training Accuracy: 55.208\n",
            "Worker 1, [47/64]: Training Loss: 1.549591522, Training Accuracy: 55.600\n",
            "Worker 1, [48/64]: Training Loss: 1.521590488, Training Accuracy: 56.960\n",
            "Worker 1, [49/64]: Training Loss: 1.510108496, Training Accuracy: 56.568\n",
            "Worker 1, [50/64]: Training Loss: 1.459768019, Training Accuracy: 58.432\n",
            "Worker 1, [51/64]: Training Loss: 1.513370260, Training Accuracy: 56.664\n",
            "Worker 1, [52/64]: Training Loss: 1.429855268, Training Accuracy: 58.816\n",
            "Worker 1, [53/64]: Training Loss: 1.419540728, Training Accuracy: 59.200\n",
            "Worker 1, [54/64]: Training Loss: 1.392588080, Training Accuracy: 59.720\n",
            "Worker 1, [55/64]: Training Loss: 1.430393852, Training Accuracy: 59.160\n",
            "Worker 1, [56/64]: Training Loss: 1.375664462, Training Accuracy: 60.544\n",
            "Worker 1, [57/64]: Training Loss: 1.396198931, Training Accuracy: 59.880\n",
            "Worker 1, [58/64]: Training Loss: 1.353616960, Training Accuracy: 60.992\n",
            "Worker 1, [59/64]: Training Loss: 1.349695050, Training Accuracy: 60.568\n",
            "Worker 1, [60/64]: Training Loss: 1.336480366, Training Accuracy: 61.328\n",
            "Worker 1, [61/64]: Training Loss: 1.301334043, Training Accuracy: 62.648\n",
            "Worker 1, [62/64]: Training Loss: 1.293107384, Training Accuracy: 62.480\n",
            "Worker 1, [63/64]: Training Loss: 1.268500346, Training Accuracy: 63.304\n",
            "Worker 1, [64/64]: Training Loss: 1.295289986, Training Accuracy: 62.688\n",
            "Time taken for training worker 1: 0:06:27.052733\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/64]: Training Loss: 4.498113238, Training Accuracy: 2.680\n",
            "Worker 2, [02/64]: Training Loss: 4.103479934, Training Accuracy: 6.656\n",
            "Worker 2, [03/64]: Training Loss: 3.884846825, Training Accuracy: 9.792\n",
            "Worker 2, [04/64]: Training Loss: 3.724416521, Training Accuracy: 12.288\n",
            "Worker 2, [05/64]: Training Loss: 3.608595990, Training Accuracy: 13.800\n",
            "Worker 2, [06/64]: Training Loss: 3.479301381, Training Accuracy: 15.976\n",
            "Worker 2, [07/64]: Training Loss: 3.372209337, Training Accuracy: 18.008\n",
            "Worker 2, [08/64]: Training Loss: 3.253549628, Training Accuracy: 19.704\n",
            "Worker 2, [09/64]: Training Loss: 3.186024382, Training Accuracy: 21.096\n",
            "Worker 2, [10/64]: Training Loss: 3.060713059, Training Accuracy: 23.336\n",
            "Worker 2, [11/64]: Training Loss: 3.002137721, Training Accuracy: 24.632\n",
            "Worker 2, [12/64]: Training Loss: 2.900509958, Training Accuracy: 26.720\n",
            "Worker 2, [13/64]: Training Loss: 2.841007238, Training Accuracy: 27.432\n",
            "Worker 2, [14/64]: Training Loss: 2.749498331, Training Accuracy: 29.416\n",
            "Worker 2, [15/64]: Training Loss: 2.690726809, Training Accuracy: 30.152\n",
            "Worker 2, [16/64]: Training Loss: 2.640924546, Training Accuracy: 31.328\n",
            "Worker 2, [17/64]: Training Loss: 2.564186460, Training Accuracy: 32.672\n",
            "Worker 2, [18/64]: Training Loss: 2.532688003, Training Accuracy: 33.832\n",
            "Worker 2, [19/64]: Training Loss: 2.455962404, Training Accuracy: 35.336\n",
            "Worker 2, [20/64]: Training Loss: 2.397850844, Training Accuracy: 36.496\n",
            "Worker 2, [21/64]: Training Loss: 2.355481331, Training Accuracy: 37.144\n",
            "Worker 2, [22/64]: Training Loss: 2.306538007, Training Accuracy: 39.208\n",
            "Worker 2, [23/64]: Training Loss: 2.254800101, Training Accuracy: 39.552\n",
            "Worker 2, [24/64]: Training Loss: 2.189784652, Training Accuracy: 40.824\n",
            "Worker 2, [25/64]: Training Loss: 2.172032278, Training Accuracy: 41.664\n",
            "Worker 2, [26/64]: Training Loss: 2.133457857, Training Accuracy: 42.144\n",
            "Worker 2, [27/64]: Training Loss: 2.103160007, Training Accuracy: 43.016\n",
            "Worker 2, [28/64]: Training Loss: 2.049927258, Training Accuracy: 43.968\n",
            "Worker 2, [29/64]: Training Loss: 2.022302323, Training Accuracy: 44.968\n",
            "Worker 2, [30/64]: Training Loss: 1.961898326, Training Accuracy: 45.928\n",
            "Worker 2, [31/64]: Training Loss: 1.943215289, Training Accuracy: 46.056\n",
            "Worker 2, [32/64]: Training Loss: 1.912997139, Training Accuracy: 47.104\n",
            "Worker 2, [33/64]: Training Loss: 1.876013138, Training Accuracy: 47.976\n",
            "Worker 2, [34/64]: Training Loss: 1.840215011, Training Accuracy: 48.928\n",
            "Worker 2, [35/64]: Training Loss: 1.813848432, Training Accuracy: 49.816\n",
            "Worker 2, [36/64]: Training Loss: 1.797046956, Training Accuracy: 49.832\n",
            "Worker 2, [37/64]: Training Loss: 1.754328551, Training Accuracy: 51.024\n",
            "Worker 2, [38/64]: Training Loss: 1.712940939, Training Accuracy: 51.976\n",
            "Worker 2, [39/64]: Training Loss: 1.723882738, Training Accuracy: 51.744\n",
            "Worker 2, [40/64]: Training Loss: 1.674480391, Training Accuracy: 52.376\n",
            "Worker 2, [41/64]: Training Loss: 1.648894794, Training Accuracy: 53.304\n",
            "Worker 2, [42/64]: Training Loss: 1.614020172, Training Accuracy: 53.712\n",
            "Worker 2, [43/64]: Training Loss: 1.594850970, Training Accuracy: 55.024\n",
            "Worker 2, [44/64]: Training Loss: 1.566879486, Training Accuracy: 55.216\n",
            "Worker 2, [45/64]: Training Loss: 1.565612027, Training Accuracy: 54.640\n",
            "Worker 2, [46/64]: Training Loss: 1.536221524, Training Accuracy: 56.216\n",
            "Worker 2, [47/64]: Training Loss: 1.526051345, Training Accuracy: 56.680\n",
            "Worker 2, [48/64]: Training Loss: 1.507654030, Training Accuracy: 56.760\n",
            "Worker 2, [49/64]: Training Loss: 1.493687513, Training Accuracy: 57.616\n",
            "Worker 2, [50/64]: Training Loss: 1.458102153, Training Accuracy: 58.424\n",
            "Worker 2, [51/64]: Training Loss: 1.427557801, Training Accuracy: 58.664\n",
            "Worker 2, [52/64]: Training Loss: 1.451005088, Training Accuracy: 58.784\n",
            "Worker 2, [53/64]: Training Loss: 1.411273940, Training Accuracy: 59.784\n",
            "Worker 2, [54/64]: Training Loss: 1.380491060, Training Accuracy: 59.856\n",
            "Worker 2, [55/64]: Training Loss: 1.378030247, Training Accuracy: 60.512\n",
            "Worker 2, [56/64]: Training Loss: 1.379981226, Training Accuracy: 60.144\n",
            "Worker 2, [57/64]: Training Loss: 1.377771812, Training Accuracy: 60.520\n",
            "Worker 2, [58/64]: Training Loss: 1.296881390, Training Accuracy: 62.304\n",
            "Worker 2, [59/64]: Training Loss: 1.320860011, Training Accuracy: 61.912\n",
            "Worker 2, [60/64]: Training Loss: 1.328836481, Training Accuracy: 61.192\n",
            "Worker 2, [61/64]: Training Loss: 1.286505405, Training Accuracy: 62.728\n",
            "Worker 2, [62/64]: Training Loss: 1.291643107, Training Accuracy: 62.768\n",
            "Worker 2, [63/64]: Training Loss: 1.299142599, Training Accuracy: 62.680\n",
            "Worker 2, [64/64]: Training Loss: 1.245424378, Training Accuracy: 63.584\n",
            "Time taken for training worker 2: 0:06:26.474244\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/64]: Training Loss: 4.485452564, Training Accuracy: 2.696\n",
            "Worker 3, [02/64]: Training Loss: 4.104870614, Training Accuracy: 7.016\n",
            "Worker 3, [03/64]: Training Loss: 3.880172521, Training Accuracy: 9.944\n",
            "Worker 3, [04/64]: Training Loss: 3.711595764, Training Accuracy: 12.800\n",
            "Worker 3, [05/64]: Training Loss: 3.606546848, Training Accuracy: 14.744\n",
            "Worker 3, [06/64]: Training Loss: 3.457027170, Training Accuracy: 16.792\n",
            "Worker 3, [07/64]: Training Loss: 3.343552515, Training Accuracy: 18.224\n",
            "Worker 3, [08/64]: Training Loss: 3.229431370, Training Accuracy: 20.120\n",
            "Worker 3, [09/64]: Training Loss: 3.138464301, Training Accuracy: 22.112\n",
            "Worker 3, [10/64]: Training Loss: 3.061128854, Training Accuracy: 23.032\n",
            "Worker 3, [11/64]: Training Loss: 2.998370606, Training Accuracy: 24.520\n",
            "Worker 3, [12/64]: Training Loss: 2.907404418, Training Accuracy: 26.560\n",
            "Worker 3, [13/64]: Training Loss: 2.822650920, Training Accuracy: 27.688\n",
            "Worker 3, [14/64]: Training Loss: 2.733372132, Training Accuracy: 29.848\n",
            "Worker 3, [15/64]: Training Loss: 2.675508901, Training Accuracy: 31.192\n",
            "Worker 3, [16/64]: Training Loss: 2.629341021, Training Accuracy: 31.768\n",
            "Worker 3, [17/64]: Training Loss: 2.569623689, Training Accuracy: 32.608\n",
            "Worker 3, [18/64]: Training Loss: 2.511811910, Training Accuracy: 34.160\n",
            "Worker 3, [19/64]: Training Loss: 2.464873598, Training Accuracy: 35.280\n",
            "Worker 3, [20/64]: Training Loss: 2.387214112, Training Accuracy: 36.256\n",
            "Worker 3, [21/64]: Training Loss: 2.356221995, Training Accuracy: 37.872\n",
            "Worker 3, [22/64]: Training Loss: 2.300866289, Training Accuracy: 38.352\n",
            "Worker 3, [23/64]: Training Loss: 2.261573068, Training Accuracy: 39.272\n",
            "Worker 3, [24/64]: Training Loss: 2.230320480, Training Accuracy: 39.976\n",
            "Worker 3, [25/64]: Training Loss: 2.172493527, Training Accuracy: 41.640\n",
            "Worker 3, [26/64]: Training Loss: 2.122361287, Training Accuracy: 42.776\n",
            "Worker 3, [27/64]: Training Loss: 2.115173125, Training Accuracy: 42.616\n",
            "Worker 3, [28/64]: Training Loss: 2.052505619, Training Accuracy: 43.784\n",
            "Worker 3, [29/64]: Training Loss: 2.020838705, Training Accuracy: 44.744\n",
            "Worker 3, [30/64]: Training Loss: 1.979789210, Training Accuracy: 45.768\n",
            "Worker 3, [31/64]: Training Loss: 1.959204339, Training Accuracy: 46.464\n",
            "Worker 3, [32/64]: Training Loss: 1.913841274, Training Accuracy: 47.528\n",
            "Worker 3, [33/64]: Training Loss: 1.874645964, Training Accuracy: 47.824\n",
            "Worker 3, [34/64]: Training Loss: 1.854674522, Training Accuracy: 48.704\n",
            "Worker 3, [35/64]: Training Loss: 1.815966472, Training Accuracy: 49.560\n",
            "Worker 3, [36/64]: Training Loss: 1.800053401, Training Accuracy: 49.752\n",
            "Worker 3, [37/64]: Training Loss: 1.782812211, Training Accuracy: 50.104\n",
            "Worker 3, [38/64]: Training Loss: 1.725855633, Training Accuracy: 51.896\n",
            "Worker 3, [39/64]: Training Loss: 1.713227743, Training Accuracy: 51.864\n",
            "Worker 3, [40/64]: Training Loss: 1.683600435, Training Accuracy: 53.008\n",
            "Worker 3, [41/64]: Training Loss: 1.658069092, Training Accuracy: 53.288\n",
            "Worker 3, [42/64]: Training Loss: 1.625948767, Training Accuracy: 53.752\n",
            "Worker 3, [43/64]: Training Loss: 1.617220290, Training Accuracy: 54.104\n",
            "Worker 3, [44/64]: Training Loss: 1.596183701, Training Accuracy: 54.720\n",
            "Worker 3, [45/64]: Training Loss: 1.577599799, Training Accuracy: 55.200\n",
            "Worker 3, [46/64]: Training Loss: 1.563294997, Training Accuracy: 55.344\n",
            "Worker 3, [47/64]: Training Loss: 1.539179421, Training Accuracy: 56.416\n",
            "Worker 3, [48/64]: Training Loss: 1.520652686, Training Accuracy: 56.952\n",
            "Worker 3, [49/64]: Training Loss: 1.510368496, Training Accuracy: 56.704\n",
            "Worker 3, [50/64]: Training Loss: 1.468072683, Training Accuracy: 57.720\n",
            "Worker 3, [51/64]: Training Loss: 1.470907979, Training Accuracy: 57.688\n",
            "Worker 3, [52/64]: Training Loss: 1.433098184, Training Accuracy: 58.336\n",
            "Worker 3, [53/64]: Training Loss: 1.440691024, Training Accuracy: 58.680\n",
            "Worker 3, [54/64]: Training Loss: 1.415080962, Training Accuracy: 59.304\n",
            "Worker 3, [55/64]: Training Loss: 1.425370565, Training Accuracy: 58.952\n",
            "Worker 3, [56/64]: Training Loss: 1.380417057, Training Accuracy: 60.144\n",
            "Worker 3, [57/64]: Training Loss: 1.335792282, Training Accuracy: 61.400\n",
            "Worker 3, [58/64]: Training Loss: 1.333719718, Training Accuracy: 61.616\n",
            "Worker 3, [59/64]: Training Loss: 1.353546962, Training Accuracy: 61.128\n",
            "Worker 3, [60/64]: Training Loss: 1.325642717, Training Accuracy: 62.032\n",
            "Worker 3, [61/64]: Training Loss: 1.296866224, Training Accuracy: 61.984\n",
            "Worker 3, [62/64]: Training Loss: 1.289024821, Training Accuracy: 62.416\n",
            "Worker 3, [63/64]: Training Loss: 1.252503191, Training Accuracy: 63.816\n",
            "Worker 3, [64/64]: Training Loss: 1.284621085, Training Accuracy: 62.520\n",
            "Time taken for training worker 3: 0:06:39.936227\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/64]: Training Loss: 4.493775786, Training Accuracy: 2.464\n",
            "Worker 4, [02/64]: Training Loss: 4.101559002, Training Accuracy: 6.792\n",
            "Worker 4, [03/64]: Training Loss: 3.870114888, Training Accuracy: 9.976\n",
            "Worker 4, [04/64]: Training Loss: 3.738758000, Training Accuracy: 11.760\n",
            "Worker 4, [05/64]: Training Loss: 3.587190370, Training Accuracy: 14.632\n",
            "Worker 4, [06/64]: Training Loss: 3.479830129, Training Accuracy: 16.000\n",
            "Worker 4, [07/64]: Training Loss: 3.348124434, Training Accuracy: 18.312\n",
            "Worker 4, [08/64]: Training Loss: 3.254698381, Training Accuracy: 20.328\n",
            "Worker 4, [09/64]: Training Loss: 3.168916072, Training Accuracy: 21.584\n",
            "Worker 4, [10/64]: Training Loss: 3.063933928, Training Accuracy: 23.448\n",
            "Worker 4, [11/64]: Training Loss: 3.001055964, Training Accuracy: 24.704\n",
            "Worker 4, [12/64]: Training Loss: 2.917930456, Training Accuracy: 26.416\n",
            "Worker 4, [13/64]: Training Loss: 2.847432387, Training Accuracy: 27.136\n",
            "Worker 4, [14/64]: Training Loss: 2.780429212, Training Accuracy: 28.896\n",
            "Worker 4, [15/64]: Training Loss: 2.698116813, Training Accuracy: 30.360\n",
            "Worker 4, [16/64]: Training Loss: 2.636839797, Training Accuracy: 31.248\n",
            "Worker 4, [17/64]: Training Loss: 2.584435540, Training Accuracy: 32.784\n",
            "Worker 4, [18/64]: Training Loss: 2.518570345, Training Accuracy: 33.928\n",
            "Worker 4, [19/64]: Training Loss: 2.466528256, Training Accuracy: 34.840\n",
            "Worker 4, [20/64]: Training Loss: 2.419446799, Training Accuracy: 35.824\n",
            "Worker 4, [21/64]: Training Loss: 2.363096605, Training Accuracy: 37.016\n",
            "Worker 4, [22/64]: Training Loss: 2.334050231, Training Accuracy: 38.024\n",
            "Worker 4, [23/64]: Training Loss: 2.278184485, Training Accuracy: 39.088\n",
            "Worker 4, [24/64]: Training Loss: 2.228744708, Training Accuracy: 39.608\n",
            "Worker 4, [25/64]: Training Loss: 2.197995936, Training Accuracy: 40.752\n",
            "Worker 4, [26/64]: Training Loss: 2.152810886, Training Accuracy: 41.440\n",
            "Worker 4, [27/64]: Training Loss: 2.080415794, Training Accuracy: 42.976\n",
            "Worker 4, [28/64]: Training Loss: 2.080480318, Training Accuracy: 43.368\n",
            "Worker 4, [29/64]: Training Loss: 2.013891372, Training Accuracy: 44.416\n",
            "Worker 4, [30/64]: Training Loss: 2.017415376, Training Accuracy: 44.304\n",
            "Worker 4, [31/64]: Training Loss: 1.940636688, Training Accuracy: 46.440\n",
            "Worker 4, [32/64]: Training Loss: 1.923740107, Training Accuracy: 46.784\n",
            "Worker 4, [33/64]: Training Loss: 1.919870456, Training Accuracy: 47.056\n",
            "Worker 4, [34/64]: Training Loss: 1.861494224, Training Accuracy: 47.752\n",
            "Worker 4, [35/64]: Training Loss: 1.835770562, Training Accuracy: 48.888\n",
            "Worker 4, [36/64]: Training Loss: 1.807412348, Training Accuracy: 49.152\n",
            "Worker 4, [37/64]: Training Loss: 1.776597376, Training Accuracy: 50.232\n",
            "Worker 4, [38/64]: Training Loss: 1.744577263, Training Accuracy: 51.232\n",
            "Worker 4, [39/64]: Training Loss: 1.713558019, Training Accuracy: 51.872\n",
            "Worker 4, [40/64]: Training Loss: 1.681626634, Training Accuracy: 52.624\n",
            "Worker 4, [41/64]: Training Loss: 1.678990318, Training Accuracy: 52.408\n",
            "Worker 4, [42/64]: Training Loss: 1.656882246, Training Accuracy: 53.168\n",
            "Worker 4, [43/64]: Training Loss: 1.633261738, Training Accuracy: 53.784\n",
            "Worker 4, [44/64]: Training Loss: 1.590387263, Training Accuracy: 54.464\n",
            "Worker 4, [45/64]: Training Loss: 1.569317288, Training Accuracy: 54.944\n",
            "Worker 4, [46/64]: Training Loss: 1.547344717, Training Accuracy: 56.040\n",
            "Worker 4, [47/64]: Training Loss: 1.544068445, Training Accuracy: 56.504\n",
            "Worker 4, [48/64]: Training Loss: 1.546025354, Training Accuracy: 55.992\n",
            "Worker 4, [49/64]: Training Loss: 1.480347177, Training Accuracy: 57.248\n",
            "Worker 4, [50/64]: Training Loss: 1.452217550, Training Accuracy: 58.576\n",
            "Worker 4, [51/64]: Training Loss: 1.472158049, Training Accuracy: 57.584\n",
            "Worker 4, [52/64]: Training Loss: 1.445683692, Training Accuracy: 58.296\n",
            "Worker 4, [53/64]: Training Loss: 1.480465028, Training Accuracy: 57.728\n",
            "Worker 4, [54/64]: Training Loss: 1.417246920, Training Accuracy: 59.232\n",
            "Worker 4, [55/64]: Training Loss: 1.437679227, Training Accuracy: 58.840\n",
            "Worker 4, [56/64]: Training Loss: 1.416715549, Training Accuracy: 59.440\n",
            "Worker 4, [57/64]: Training Loss: 1.362128791, Training Accuracy: 60.584\n",
            "Worker 4, [58/64]: Training Loss: 1.359830512, Training Accuracy: 60.936\n",
            "Worker 4, [59/64]: Training Loss: 1.342913806, Training Accuracy: 61.288\n",
            "Worker 4, [60/64]: Training Loss: 1.360012851, Training Accuracy: 60.896\n",
            "Worker 4, [61/64]: Training Loss: 1.354664455, Training Accuracy: 61.032\n",
            "Worker 4, [62/64]: Training Loss: 1.285404745, Training Accuracy: 62.496\n",
            "Worker 4, [63/64]: Training Loss: 1.276969216, Training Accuracy: 62.928\n",
            "Worker 4, [64/64]: Training Loss: 1.269317444, Training Accuracy: 63.128\n",
            "Time taken for training worker 4: 0:06:23.918439\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002639\n",
            "Global Update 01: Test Loss: 3.765766010, Test Accuracy: 21.600\n",
            "**************************************************\n",
            "Worker 1, [01/64]: Training Loss: 2.871543306, Training Accuracy: 29.032\n",
            "Worker 1, [02/64]: Training Loss: 2.450060900, Training Accuracy: 35.664\n",
            "Worker 1, [03/64]: Training Loss: 2.294386254, Training Accuracy: 39.216\n",
            "Worker 1, [04/64]: Training Loss: 2.173861943, Training Accuracy: 42.344\n",
            "Worker 1, [05/64]: Training Loss: 2.073478025, Training Accuracy: 43.808\n",
            "Worker 1, [06/64]: Training Loss: 1.985424879, Training Accuracy: 46.344\n",
            "Worker 1, [07/64]: Training Loss: 1.905855061, Training Accuracy: 47.472\n",
            "Worker 1, [08/64]: Training Loss: 1.852959862, Training Accuracy: 49.416\n",
            "Worker 1, [09/64]: Training Loss: 1.760586026, Training Accuracy: 51.304\n",
            "Worker 1, [10/64]: Training Loss: 1.707759023, Training Accuracy: 52.512\n",
            "Worker 1, [11/64]: Training Loss: 1.654628239, Training Accuracy: 53.712\n",
            "Worker 1, [12/64]: Training Loss: 1.608615918, Training Accuracy: 54.840\n",
            "Worker 1, [13/64]: Training Loss: 1.551209421, Training Accuracy: 56.296\n",
            "Worker 1, [14/64]: Training Loss: 1.521445639, Training Accuracy: 56.928\n",
            "Worker 1, [15/64]: Training Loss: 1.451077506, Training Accuracy: 58.304\n",
            "Worker 1, [16/64]: Training Loss: 1.409654645, Training Accuracy: 59.696\n",
            "Worker 1, [17/64]: Training Loss: 1.375551251, Training Accuracy: 60.280\n",
            "Worker 1, [18/64]: Training Loss: 1.331209532, Training Accuracy: 61.624\n",
            "Worker 1, [19/64]: Training Loss: 1.297659710, Training Accuracy: 62.640\n",
            "Worker 1, [20/64]: Training Loss: 1.255341446, Training Accuracy: 63.760\n",
            "Worker 1, [21/64]: Training Loss: 1.241456199, Training Accuracy: 63.448\n",
            "Worker 1, [22/64]: Training Loss: 1.207825582, Training Accuracy: 64.600\n",
            "Worker 1, [23/64]: Training Loss: 1.159544590, Training Accuracy: 66.472\n",
            "Worker 1, [24/64]: Training Loss: 1.143691901, Training Accuracy: 66.600\n",
            "Worker 1, [25/64]: Training Loss: 1.121221255, Training Accuracy: 66.928\n",
            "Worker 1, [26/64]: Training Loss: 1.089127643, Training Accuracy: 67.368\n",
            "Worker 1, [27/64]: Training Loss: 1.074995358, Training Accuracy: 68.200\n",
            "Worker 1, [28/64]: Training Loss: 1.041427220, Training Accuracy: 68.576\n",
            "Worker 1, [29/64]: Training Loss: 1.029203496, Training Accuracy: 69.824\n",
            "Worker 1, [30/64]: Training Loss: 0.991055416, Training Accuracy: 70.528\n",
            "Worker 1, [31/64]: Training Loss: 0.965786294, Training Accuracy: 70.816\n",
            "Worker 1, [32/64]: Training Loss: 0.963127712, Training Accuracy: 71.192\n",
            "Worker 1, [33/64]: Training Loss: 0.910406745, Training Accuracy: 72.688\n",
            "Worker 1, [34/64]: Training Loss: 0.898666123, Training Accuracy: 72.968\n",
            "Worker 1, [35/64]: Training Loss: 0.886341912, Training Accuracy: 73.032\n",
            "Worker 1, [36/64]: Training Loss: 0.859683563, Training Accuracy: 74.112\n",
            "Worker 1, [37/64]: Training Loss: 0.848650257, Training Accuracy: 73.976\n",
            "Worker 1, [38/64]: Training Loss: 0.847554671, Training Accuracy: 74.160\n",
            "Worker 1, [39/64]: Training Loss: 0.837109674, Training Accuracy: 74.440\n",
            "Worker 1, [40/64]: Training Loss: 0.813680752, Training Accuracy: 75.488\n",
            "Worker 1, [41/64]: Training Loss: 0.788640293, Training Accuracy: 75.944\n",
            "Worker 1, [42/64]: Training Loss: 0.789881845, Training Accuracy: 75.848\n",
            "Worker 1, [43/64]: Training Loss: 0.735885640, Training Accuracy: 78.184\n",
            "Worker 1, [44/64]: Training Loss: 0.775231257, Training Accuracy: 76.344\n",
            "Worker 1, [45/64]: Training Loss: 0.750415660, Training Accuracy: 77.088\n",
            "Worker 1, [46/64]: Training Loss: 0.720600046, Training Accuracy: 78.552\n",
            "Worker 1, [47/64]: Training Loss: 0.714144853, Training Accuracy: 77.904\n",
            "Worker 1, [48/64]: Training Loss: 0.711146465, Training Accuracy: 78.184\n",
            "Worker 1, [49/64]: Training Loss: 0.699915467, Training Accuracy: 78.728\n",
            "Worker 1, [50/64]: Training Loss: 0.692254651, Training Accuracy: 79.256\n",
            "Worker 1, [51/64]: Training Loss: 0.686745186, Training Accuracy: 79.248\n",
            "Worker 1, [52/64]: Training Loss: 0.643090656, Training Accuracy: 80.544\n",
            "Worker 1, [53/64]: Training Loss: 0.656017638, Training Accuracy: 79.520\n",
            "Worker 1, [54/64]: Training Loss: 0.646248788, Training Accuracy: 80.152\n",
            "Worker 1, [55/64]: Training Loss: 0.606549574, Training Accuracy: 81.624\n",
            "Worker 1, [56/64]: Training Loss: 0.618646387, Training Accuracy: 81.160\n",
            "Worker 1, [57/64]: Training Loss: 0.612894226, Training Accuracy: 81.128\n",
            "Worker 1, [58/64]: Training Loss: 0.624439114, Training Accuracy: 80.592\n",
            "Worker 1, [59/64]: Training Loss: 0.592419953, Training Accuracy: 81.600\n",
            "Worker 1, [60/64]: Training Loss: 0.575746812, Training Accuracy: 82.480\n",
            "Worker 1, [61/64]: Training Loss: 0.561941219, Training Accuracy: 82.872\n",
            "Worker 1, [62/64]: Training Loss: 0.590236026, Training Accuracy: 81.808\n",
            "Worker 1, [63/64]: Training Loss: 0.591195860, Training Accuracy: 81.888\n",
            "Worker 1, [64/64]: Training Loss: 0.556730203, Training Accuracy: 83.176\n",
            "Time taken for training worker 1: 0:06:23.570472\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/64]: Training Loss: 2.868930920, Training Accuracy: 28.160\n",
            "Worker 2, [02/64]: Training Loss: 2.467347257, Training Accuracy: 35.328\n",
            "Worker 2, [03/64]: Training Loss: 2.292046324, Training Accuracy: 38.896\n",
            "Worker 2, [04/64]: Training Loss: 2.167226153, Training Accuracy: 41.400\n",
            "Worker 2, [05/64]: Training Loss: 2.064890174, Training Accuracy: 44.248\n",
            "Worker 2, [06/64]: Training Loss: 1.975333027, Training Accuracy: 45.408\n",
            "Worker 2, [07/64]: Training Loss: 1.906636025, Training Accuracy: 46.984\n",
            "Worker 2, [08/64]: Training Loss: 1.828756619, Training Accuracy: 48.888\n",
            "Worker 2, [09/64]: Training Loss: 1.769350546, Training Accuracy: 49.960\n",
            "Worker 2, [10/64]: Training Loss: 1.713044331, Training Accuracy: 51.952\n",
            "Worker 2, [11/64]: Training Loss: 1.650592384, Training Accuracy: 53.528\n",
            "Worker 2, [12/64]: Training Loss: 1.585171588, Training Accuracy: 55.376\n",
            "Worker 2, [13/64]: Training Loss: 1.533069895, Training Accuracy: 56.160\n",
            "Worker 2, [14/64]: Training Loss: 1.505038264, Training Accuracy: 56.576\n",
            "Worker 2, [15/64]: Training Loss: 1.451832236, Training Accuracy: 58.768\n",
            "Worker 2, [16/64]: Training Loss: 1.422571355, Training Accuracy: 59.440\n",
            "Worker 2, [17/64]: Training Loss: 1.392737835, Training Accuracy: 59.944\n",
            "Worker 2, [18/64]: Training Loss: 1.342412703, Training Accuracy: 60.904\n",
            "Worker 2, [19/64]: Training Loss: 1.295240519, Training Accuracy: 62.544\n",
            "Worker 2, [20/64]: Training Loss: 1.249545639, Training Accuracy: 63.744\n",
            "Worker 2, [21/64]: Training Loss: 1.214298900, Training Accuracy: 64.584\n",
            "Worker 2, [22/64]: Training Loss: 1.189165655, Training Accuracy: 64.584\n",
            "Worker 2, [23/64]: Training Loss: 1.148324309, Training Accuracy: 66.008\n",
            "Worker 2, [24/64]: Training Loss: 1.145287849, Training Accuracy: 65.776\n",
            "Worker 2, [25/64]: Training Loss: 1.089207883, Training Accuracy: 67.720\n",
            "Worker 2, [26/64]: Training Loss: 1.086200377, Training Accuracy: 67.720\n",
            "Worker 2, [27/64]: Training Loss: 1.060550312, Training Accuracy: 68.744\n",
            "Worker 2, [28/64]: Training Loss: 1.027600592, Training Accuracy: 69.384\n",
            "Worker 2, [29/64]: Training Loss: 0.999837145, Training Accuracy: 70.328\n",
            "Worker 2, [30/64]: Training Loss: 0.984990047, Training Accuracy: 70.296\n",
            "Worker 2, [31/64]: Training Loss: 0.954555197, Training Accuracy: 71.248\n",
            "Worker 2, [32/64]: Training Loss: 0.941653437, Training Accuracy: 71.456\n",
            "Worker 2, [33/64]: Training Loss: 0.929010057, Training Accuracy: 71.848\n",
            "Worker 2, [34/64]: Training Loss: 0.927740713, Training Accuracy: 71.864\n",
            "Worker 2, [35/64]: Training Loss: 0.878688393, Training Accuracy: 73.448\n",
            "Worker 2, [36/64]: Training Loss: 0.880548176, Training Accuracy: 73.304\n",
            "Worker 2, [37/64]: Training Loss: 0.847845949, Training Accuracy: 74.624\n",
            "Worker 2, [38/64]: Training Loss: 0.824217283, Training Accuracy: 74.992\n",
            "Worker 2, [39/64]: Training Loss: 0.851593669, Training Accuracy: 74.328\n",
            "Worker 2, [40/64]: Training Loss: 0.807932529, Training Accuracy: 75.264\n",
            "Worker 2, [41/64]: Training Loss: 0.792517449, Training Accuracy: 76.008\n",
            "Worker 2, [42/64]: Training Loss: 0.741476933, Training Accuracy: 77.432\n",
            "Worker 2, [43/64]: Training Loss: 0.773107434, Training Accuracy: 76.384\n",
            "Worker 2, [44/64]: Training Loss: 0.758840331, Training Accuracy: 76.808\n",
            "Worker 2, [45/64]: Training Loss: 0.727353004, Training Accuracy: 77.304\n",
            "Worker 2, [46/64]: Training Loss: 0.709350165, Training Accuracy: 78.200\n",
            "Worker 2, [47/64]: Training Loss: 0.692294913, Training Accuracy: 78.760\n",
            "Worker 2, [48/64]: Training Loss: 0.711263982, Training Accuracy: 78.168\n",
            "Worker 2, [49/64]: Training Loss: 0.702553762, Training Accuracy: 78.696\n",
            "Worker 2, [50/64]: Training Loss: 0.695600112, Training Accuracy: 78.920\n",
            "Worker 2, [51/64]: Training Loss: 0.674791536, Training Accuracy: 79.384\n",
            "Worker 2, [52/64]: Training Loss: 0.650778097, Training Accuracy: 80.176\n",
            "Worker 2, [53/64]: Training Loss: 0.656744772, Training Accuracy: 79.800\n",
            "Worker 2, [54/64]: Training Loss: 0.639623723, Training Accuracy: 80.416\n",
            "Worker 2, [55/64]: Training Loss: 0.629301751, Training Accuracy: 80.392\n",
            "Worker 2, [56/64]: Training Loss: 0.626934140, Training Accuracy: 80.936\n",
            "Worker 2, [57/64]: Training Loss: 0.605339105, Training Accuracy: 81.384\n",
            "Worker 2, [58/64]: Training Loss: 0.607844740, Training Accuracy: 81.512\n",
            "Worker 2, [59/64]: Training Loss: 0.602047140, Training Accuracy: 81.432\n",
            "Worker 2, [60/64]: Training Loss: 0.619896680, Training Accuracy: 80.816\n",
            "Worker 2, [61/64]: Training Loss: 0.561115022, Training Accuracy: 82.744\n",
            "Worker 2, [62/64]: Training Loss: 0.578392352, Training Accuracy: 82.024\n",
            "Worker 2, [63/64]: Training Loss: 0.587639354, Training Accuracy: 81.960\n",
            "Worker 2, [64/64]: Training Loss: 0.564799133, Training Accuracy: 82.128\n",
            "Time taken for training worker 2: 0:06:25.254647\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/64]: Training Loss: 2.859145428, Training Accuracy: 28.504\n",
            "Worker 3, [02/64]: Training Loss: 2.467692396, Training Accuracy: 35.840\n",
            "Worker 3, [03/64]: Training Loss: 2.308079755, Training Accuracy: 38.672\n",
            "Worker 3, [04/64]: Training Loss: 2.187044057, Training Accuracy: 41.440\n",
            "Worker 3, [05/64]: Training Loss: 2.085867871, Training Accuracy: 43.336\n",
            "Worker 3, [06/64]: Training Loss: 1.989825488, Training Accuracy: 45.448\n",
            "Worker 3, [07/64]: Training Loss: 1.938745097, Training Accuracy: 46.552\n",
            "Worker 3, [08/64]: Training Loss: 1.855248023, Training Accuracy: 48.736\n",
            "Worker 3, [09/64]: Training Loss: 1.796819572, Training Accuracy: 49.960\n",
            "Worker 3, [10/64]: Training Loss: 1.753812685, Training Accuracy: 50.880\n",
            "Worker 3, [11/64]: Training Loss: 1.668179779, Training Accuracy: 52.632\n",
            "Worker 3, [12/64]: Training Loss: 1.641252138, Training Accuracy: 53.600\n",
            "Worker 3, [13/64]: Training Loss: 1.586744347, Training Accuracy: 54.888\n",
            "Worker 3, [14/64]: Training Loss: 1.541335825, Training Accuracy: 55.896\n",
            "Worker 3, [15/64]: Training Loss: 1.493102460, Training Accuracy: 57.552\n",
            "Worker 3, [16/64]: Training Loss: 1.453115388, Training Accuracy: 57.824\n",
            "Worker 3, [17/64]: Training Loss: 1.393797286, Training Accuracy: 59.848\n",
            "Worker 3, [18/64]: Training Loss: 1.382034951, Training Accuracy: 60.080\n",
            "Worker 3, [19/64]: Training Loss: 1.337328990, Training Accuracy: 61.416\n",
            "Worker 3, [20/64]: Training Loss: 1.306258394, Training Accuracy: 61.656\n",
            "Worker 3, [21/64]: Training Loss: 1.282794092, Training Accuracy: 62.344\n",
            "Worker 3, [22/64]: Training Loss: 1.229062814, Training Accuracy: 63.768\n",
            "Worker 3, [23/64]: Training Loss: 1.223957035, Training Accuracy: 64.248\n",
            "Worker 3, [24/64]: Training Loss: 1.206263769, Training Accuracy: 64.248\n",
            "Worker 3, [25/64]: Training Loss: 1.134494368, Training Accuracy: 66.136\n",
            "Worker 3, [26/64]: Training Loss: 1.137129092, Training Accuracy: 66.328\n",
            "Worker 3, [27/64]: Training Loss: 1.096760066, Training Accuracy: 67.240\n",
            "Worker 3, [28/64]: Training Loss: 1.075427634, Training Accuracy: 67.440\n",
            "Worker 3, [29/64]: Training Loss: 1.040318224, Training Accuracy: 69.112\n",
            "Worker 3, [30/64]: Training Loss: 1.025946435, Training Accuracy: 68.928\n",
            "Worker 3, [31/64]: Training Loss: 1.008165793, Training Accuracy: 70.032\n",
            "Worker 3, [32/64]: Training Loss: 0.970784570, Training Accuracy: 71.008\n",
            "Worker 3, [33/64]: Training Loss: 0.952267326, Training Accuracy: 71.472\n",
            "Worker 3, [34/64]: Training Loss: 0.922574275, Training Accuracy: 71.896\n",
            "Worker 3, [35/64]: Training Loss: 0.922914990, Training Accuracy: 71.736\n",
            "Worker 3, [36/64]: Training Loss: 0.874848396, Training Accuracy: 73.800\n",
            "Worker 3, [37/64]: Training Loss: 0.900776840, Training Accuracy: 72.904\n",
            "Worker 3, [38/64]: Training Loss: 0.868294011, Training Accuracy: 74.128\n",
            "Worker 3, [39/64]: Training Loss: 0.834935611, Training Accuracy: 74.616\n",
            "Worker 3, [40/64]: Training Loss: 0.824118400, Training Accuracy: 75.424\n",
            "Worker 3, [41/64]: Training Loss: 0.820506251, Training Accuracy: 75.336\n",
            "Worker 3, [42/64]: Training Loss: 0.831197253, Training Accuracy: 74.584\n",
            "Worker 3, [43/64]: Training Loss: 0.771256465, Training Accuracy: 76.600\n",
            "Worker 3, [44/64]: Training Loss: 0.758079734, Training Accuracy: 76.240\n",
            "Worker 3, [45/64]: Training Loss: 0.747858451, Training Accuracy: 77.424\n",
            "Worker 3, [46/64]: Training Loss: 0.738196806, Training Accuracy: 77.328\n",
            "Worker 3, [47/64]: Training Loss: 0.745627043, Training Accuracy: 77.504\n",
            "Worker 3, [48/64]: Training Loss: 0.722137509, Training Accuracy: 78.152\n",
            "Worker 3, [49/64]: Training Loss: 0.739916467, Training Accuracy: 77.504\n",
            "Worker 3, [50/64]: Training Loss: 0.710507997, Training Accuracy: 78.192\n",
            "Worker 3, [51/64]: Training Loss: 0.682196903, Training Accuracy: 79.704\n",
            "Worker 3, [52/64]: Training Loss: 0.702940442, Training Accuracy: 78.680\n",
            "Worker 3, [53/64]: Training Loss: 0.670405039, Training Accuracy: 79.360\n",
            "Worker 3, [54/64]: Training Loss: 0.660575984, Training Accuracy: 79.984\n",
            "Worker 3, [55/64]: Training Loss: 0.632604752, Training Accuracy: 80.736\n",
            "Worker 3, [56/64]: Training Loss: 0.642677093, Training Accuracy: 80.416\n",
            "Worker 3, [57/64]: Training Loss: 0.618081476, Training Accuracy: 81.232\n",
            "Worker 3, [58/64]: Training Loss: 0.603572449, Training Accuracy: 81.152\n",
            "Worker 3, [59/64]: Training Loss: 0.617547082, Training Accuracy: 81.080\n",
            "Worker 3, [60/64]: Training Loss: 0.631070962, Training Accuracy: 81.040\n",
            "Worker 3, [61/64]: Training Loss: 0.632848607, Training Accuracy: 80.656\n",
            "Worker 3, [62/64]: Training Loss: 0.583942827, Training Accuracy: 81.848\n",
            "Worker 3, [63/64]: Training Loss: 0.584433146, Training Accuracy: 81.904\n",
            "Worker 3, [64/64]: Training Loss: 0.592865894, Training Accuracy: 81.600\n",
            "Time taken for training worker 3: 0:06:21.817943\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/64]: Training Loss: 2.900608096, Training Accuracy: 27.648\n",
            "Worker 4, [02/64]: Training Loss: 2.477443471, Training Accuracy: 35.232\n",
            "Worker 4, [03/64]: Training Loss: 2.327352482, Training Accuracy: 38.264\n",
            "Worker 4, [04/64]: Training Loss: 2.200743671, Training Accuracy: 40.672\n",
            "Worker 4, [05/64]: Training Loss: 2.087649042, Training Accuracy: 43.096\n",
            "Worker 4, [06/64]: Training Loss: 2.009650764, Training Accuracy: 44.304\n",
            "Worker 4, [07/64]: Training Loss: 1.919613821, Training Accuracy: 47.296\n",
            "Worker 4, [08/64]: Training Loss: 1.855159617, Training Accuracy: 48.688\n",
            "Worker 4, [09/64]: Training Loss: 1.810485735, Training Accuracy: 49.616\n",
            "Worker 4, [10/64]: Training Loss: 1.740090255, Training Accuracy: 50.888\n",
            "Worker 4, [11/64]: Training Loss: 1.672731922, Training Accuracy: 52.984\n",
            "Worker 4, [12/64]: Training Loss: 1.628848349, Training Accuracy: 54.024\n",
            "Worker 4, [13/64]: Training Loss: 1.590696022, Training Accuracy: 55.288\n",
            "Worker 4, [14/64]: Training Loss: 1.533768214, Training Accuracy: 56.408\n",
            "Worker 4, [15/64]: Training Loss: 1.502504775, Training Accuracy: 56.328\n",
            "Worker 4, [16/64]: Training Loss: 1.454214562, Training Accuracy: 58.136\n",
            "Worker 4, [17/64]: Training Loss: 1.392004833, Training Accuracy: 59.136\n",
            "Worker 4, [18/64]: Training Loss: 1.383016502, Training Accuracy: 59.720\n",
            "Worker 4, [19/64]: Training Loss: 1.352564975, Training Accuracy: 59.880\n",
            "Worker 4, [20/64]: Training Loss: 1.311950391, Training Accuracy: 61.576\n",
            "Worker 4, [21/64]: Training Loss: 1.268734464, Training Accuracy: 63.000\n",
            "Worker 4, [22/64]: Training Loss: 1.223991649, Training Accuracy: 64.400\n",
            "Worker 4, [23/64]: Training Loss: 1.182642963, Training Accuracy: 65.192\n",
            "Worker 4, [24/64]: Training Loss: 1.161286065, Training Accuracy: 65.552\n",
            "Worker 4, [25/64]: Training Loss: 1.129058455, Training Accuracy: 66.016\n",
            "Worker 4, [26/64]: Training Loss: 1.122442133, Training Accuracy: 66.744\n",
            "Worker 4, [27/64]: Training Loss: 1.092100810, Training Accuracy: 67.544\n",
            "Worker 4, [28/64]: Training Loss: 1.059244579, Training Accuracy: 68.256\n",
            "Worker 4, [29/64]: Training Loss: 1.024049071, Training Accuracy: 69.376\n",
            "Worker 4, [30/64]: Training Loss: 1.010046833, Training Accuracy: 69.848\n",
            "Worker 4, [31/64]: Training Loss: 1.008241394, Training Accuracy: 69.800\n",
            "Worker 4, [32/64]: Training Loss: 0.968437771, Training Accuracy: 70.824\n",
            "Worker 4, [33/64]: Training Loss: 0.945011344, Training Accuracy: 71.536\n",
            "Worker 4, [34/64]: Training Loss: 0.954237710, Training Accuracy: 71.464\n",
            "Worker 4, [35/64]: Training Loss: 0.911924907, Training Accuracy: 72.456\n",
            "Worker 4, [36/64]: Training Loss: 0.920264283, Training Accuracy: 72.200\n",
            "Worker 4, [37/64]: Training Loss: 0.908826705, Training Accuracy: 72.496\n",
            "Worker 4, [38/64]: Training Loss: 0.830027595, Training Accuracy: 74.920\n",
            "Worker 4, [39/64]: Training Loss: 0.848072817, Training Accuracy: 74.616\n",
            "Worker 4, [40/64]: Training Loss: 0.815583819, Training Accuracy: 75.240\n",
            "Worker 4, [41/64]: Training Loss: 0.814515867, Training Accuracy: 74.928\n",
            "Worker 4, [42/64]: Training Loss: 0.815735298, Training Accuracy: 74.696\n",
            "Worker 4, [43/64]: Training Loss: 0.784967639, Training Accuracy: 76.176\n",
            "Worker 4, [44/64]: Training Loss: 0.746237996, Training Accuracy: 77.344\n",
            "Worker 4, [45/64]: Training Loss: 0.757746637, Training Accuracy: 76.776\n",
            "Worker 4, [46/64]: Training Loss: 0.759686346, Training Accuracy: 76.592\n",
            "Worker 4, [47/64]: Training Loss: 0.740064806, Training Accuracy: 77.728\n",
            "Worker 4, [48/64]: Training Loss: 0.725976488, Training Accuracy: 77.528\n",
            "Worker 4, [49/64]: Training Loss: 0.722358664, Training Accuracy: 77.648\n",
            "Worker 4, [50/64]: Training Loss: 0.714300268, Training Accuracy: 78.648\n",
            "Worker 4, [51/64]: Training Loss: 0.690966476, Training Accuracy: 78.928\n",
            "Worker 4, [52/64]: Training Loss: 0.672195195, Training Accuracy: 79.496\n",
            "Worker 4, [53/64]: Training Loss: 0.696083044, Training Accuracy: 78.424\n",
            "Worker 4, [54/64]: Training Loss: 0.651585399, Training Accuracy: 80.408\n",
            "Worker 4, [55/64]: Training Loss: 0.668083139, Training Accuracy: 79.328\n",
            "Worker 4, [56/64]: Training Loss: 0.664584502, Training Accuracy: 79.256\n",
            "Worker 4, [57/64]: Training Loss: 0.617271902, Training Accuracy: 80.576\n",
            "Worker 4, [58/64]: Training Loss: 0.626848056, Training Accuracy: 80.512\n",
            "Worker 4, [59/64]: Training Loss: 0.611562758, Training Accuracy: 81.472\n",
            "Worker 4, [60/64]: Training Loss: 0.599219471, Training Accuracy: 81.480\n",
            "Worker 4, [61/64]: Training Loss: 0.610410321, Training Accuracy: 81.360\n",
            "Worker 4, [62/64]: Training Loss: 0.590196421, Training Accuracy: 81.792\n",
            "Worker 4, [63/64]: Training Loss: 0.616773382, Training Accuracy: 80.896\n",
            "Worker 4, [64/64]: Training Loss: 0.583156034, Training Accuracy: 82.464\n",
            "Time taken for training worker 4: 0:06:23.316566\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.010562\n",
            "Global Update 02: Test Loss: 2.314803950, Test Accuracy: 43.930\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:51:33.364968\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:4\n",
            "==================================================\n",
            "Worker 1, [01/04]: Training Loss: 4.591690321, Training Accuracy: 1.472\n",
            "Worker 1, [02/04]: Training Loss: 4.410703046, Training Accuracy: 3.328\n",
            "Worker 1, [03/04]: Training Loss: 4.191376482, Training Accuracy: 5.040\n",
            "Worker 1, [04/04]: Training Loss: 4.051675872, Training Accuracy: 7.072\n",
            "Time taken for training worker 1: 0:00:12.085822\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.592197876, Training Accuracy: 1.664\n",
            "Worker 2, [02/04]: Training Loss: 4.379548749, Training Accuracy: 4.032\n",
            "Worker 2, [03/04]: Training Loss: 4.157487259, Training Accuracy: 5.952\n",
            "Worker 2, [04/04]: Training Loss: 4.004684147, Training Accuracy: 7.344\n",
            "Time taken for training worker 2: 0:00:12.649565\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 4.590750242, Training Accuracy: 1.552\n",
            "Worker 3, [02/04]: Training Loss: 4.363276920, Training Accuracy: 3.808\n",
            "Worker 3, [03/04]: Training Loss: 4.135503798, Training Accuracy: 5.856\n",
            "Worker 3, [04/04]: Training Loss: 3.991979168, Training Accuracy: 7.872\n",
            "Time taken for training worker 3: 0:00:12.358436\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 4.592198693, Training Accuracy: 1.424\n",
            "Worker 4, [02/04]: Training Loss: 4.377958809, Training Accuracy: 3.776\n",
            "Worker 4, [03/04]: Training Loss: 4.162673422, Training Accuracy: 5.904\n",
            "Worker 4, [04/04]: Training Loss: 4.024832358, Training Accuracy: 7.856\n",
            "Time taken for training worker 4: 0:00:11.954478\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 4.588312884, Training Accuracy: 1.424\n",
            "Worker 5, [02/04]: Training Loss: 4.374200461, Training Accuracy: 3.776\n",
            "Worker 5, [03/04]: Training Loss: 4.158614743, Training Accuracy: 6.208\n",
            "Worker 5, [04/04]: Training Loss: 4.011417265, Training Accuracy: 7.760\n",
            "Time taken for training worker 5: 0:00:11.717735\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 4.590493728, Training Accuracy: 2.128\n",
            "Worker 6, [02/04]: Training Loss: 4.364494757, Training Accuracy: 4.176\n",
            "Worker 6, [03/04]: Training Loss: 4.137870419, Training Accuracy: 6.528\n",
            "Worker 6, [04/04]: Training Loss: 3.978266013, Training Accuracy: 9.104\n",
            "Time taken for training worker 6: 0:00:11.713043\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 4.590553109, Training Accuracy: 1.360\n",
            "Worker 7, [02/04]: Training Loss: 4.385667115, Training Accuracy: 3.248\n",
            "Worker 7, [03/04]: Training Loss: 4.167125198, Training Accuracy: 6.032\n",
            "Worker 7, [04/04]: Training Loss: 4.012202195, Training Accuracy: 7.696\n",
            "Time taken for training worker 7: 0:00:12.071596\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 4.589831600, Training Accuracy: 1.216\n",
            "Worker 8, [02/04]: Training Loss: 4.367230016, Training Accuracy: 3.984\n",
            "Worker 8, [03/04]: Training Loss: 4.150318231, Training Accuracy: 5.648\n",
            "Worker 8, [04/04]: Training Loss: 3.995374281, Training Accuracy: 8.080\n",
            "Time taken for training worker 8: 0:00:12.058914\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.005338\n",
            "Global Update 01: Test Loss: 3.946111914, Test Accuracy: 11.080\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.981524584, Training Accuracy: 8.656\n",
            "Worker 1, [02/04]: Training Loss: 3.878156290, Training Accuracy: 9.776\n",
            "Worker 1, [03/04]: Training Loss: 3.762353240, Training Accuracy: 11.216\n",
            "Worker 1, [04/04]: Training Loss: 3.676733803, Training Accuracy: 13.312\n",
            "Time taken for training worker 1: 0:00:12.252354\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.951198468, Training Accuracy: 9.088\n",
            "Worker 2, [02/04]: Training Loss: 3.805005575, Training Accuracy: 10.960\n",
            "Worker 2, [03/04]: Training Loss: 3.713967574, Training Accuracy: 12.720\n",
            "Worker 2, [04/04]: Training Loss: 3.633438127, Training Accuracy: 13.888\n",
            "Time taken for training worker 2: 0:00:11.873633\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.938125783, Training Accuracy: 9.216\n",
            "Worker 3, [02/04]: Training Loss: 3.818817737, Training Accuracy: 10.800\n",
            "Worker 3, [03/04]: Training Loss: 3.740861652, Training Accuracy: 12.000\n",
            "Worker 3, [04/04]: Training Loss: 3.637662554, Training Accuracy: 13.200\n",
            "Time taken for training worker 3: 0:00:11.826855\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.952965243, Training Accuracy: 9.072\n",
            "Worker 4, [02/04]: Training Loss: 3.840263406, Training Accuracy: 10.096\n",
            "Worker 4, [03/04]: Training Loss: 3.731533557, Training Accuracy: 11.792\n",
            "Worker 4, [04/04]: Training Loss: 3.659794180, Training Accuracy: 13.056\n",
            "Time taken for training worker 4: 0:00:12.668660\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 3.940492197, Training Accuracy: 9.664\n",
            "Worker 5, [02/04]: Training Loss: 3.829525099, Training Accuracy: 11.072\n",
            "Worker 5, [03/04]: Training Loss: 3.726577323, Training Accuracy: 12.304\n",
            "Worker 5, [04/04]: Training Loss: 3.636175983, Training Accuracy: 13.792\n",
            "Time taken for training worker 5: 0:00:12.667297\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 3.930183454, Training Accuracy: 9.440\n",
            "Worker 6, [02/04]: Training Loss: 3.808987936, Training Accuracy: 10.720\n",
            "Worker 6, [03/04]: Training Loss: 3.727935411, Training Accuracy: 12.416\n",
            "Worker 6, [04/04]: Training Loss: 3.635845250, Training Accuracy: 13.392\n",
            "Time taken for training worker 6: 0:00:11.808813\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 3.948724311, Training Accuracy: 9.152\n",
            "Worker 7, [02/04]: Training Loss: 3.818177046, Training Accuracy: 10.912\n",
            "Worker 7, [03/04]: Training Loss: 3.736376342, Training Accuracy: 11.808\n",
            "Worker 7, [04/04]: Training Loss: 3.628909362, Training Accuracy: 13.984\n",
            "Time taken for training worker 7: 0:00:11.693489\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 3.937608176, Training Accuracy: 8.560\n",
            "Worker 8, [02/04]: Training Loss: 3.831027104, Training Accuracy: 10.528\n",
            "Worker 8, [03/04]: Training Loss: 3.715047150, Training Accuracy: 12.192\n",
            "Worker 8, [04/04]: Training Loss: 3.634040088, Training Accuracy: 13.664\n",
            "Time taken for training worker 8: 0:00:11.777871\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.011095\n",
            "Global Update 02: Test Loss: 3.505119573, Test Accuracy: 17.240\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.668068051, Training Accuracy: 14.256\n",
            "Worker 1, [02/04]: Training Loss: 3.565242882, Training Accuracy: 15.008\n",
            "Worker 1, [03/04]: Training Loss: 3.474222434, Training Accuracy: 15.984\n",
            "Worker 1, [04/04]: Training Loss: 3.416730007, Training Accuracy: 16.816\n",
            "Time taken for training worker 1: 0:00:12.985008\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.612128216, Training Accuracy: 14.576\n",
            "Worker 2, [02/04]: Training Loss: 3.496183196, Training Accuracy: 15.744\n",
            "Worker 2, [03/04]: Training Loss: 3.424324072, Training Accuracy: 16.672\n",
            "Worker 2, [04/04]: Training Loss: 3.320806017, Training Accuracy: 18.800\n",
            "Time taken for training worker 2: 0:00:12.346911\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.635675189, Training Accuracy: 13.824\n",
            "Worker 3, [02/04]: Training Loss: 3.524743260, Training Accuracy: 15.920\n",
            "Worker 3, [03/04]: Training Loss: 3.422475333, Training Accuracy: 16.720\n",
            "Worker 3, [04/04]: Training Loss: 3.363871098, Training Accuracy: 17.584\n",
            "Time taken for training worker 3: 0:00:12.400847\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.653617674, Training Accuracy: 13.088\n",
            "Worker 4, [02/04]: Training Loss: 3.545926972, Training Accuracy: 14.576\n",
            "Worker 4, [03/04]: Training Loss: 3.461839160, Training Accuracy: 16.288\n",
            "Worker 4, [04/04]: Training Loss: 3.376461104, Training Accuracy: 17.600\n",
            "Time taken for training worker 4: 0:00:11.699727\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 3.632128244, Training Accuracy: 13.824\n",
            "Worker 5, [02/04]: Training Loss: 3.553890895, Training Accuracy: 15.600\n",
            "Worker 5, [03/04]: Training Loss: 3.442831219, Training Accuracy: 17.168\n",
            "Worker 5, [04/04]: Training Loss: 3.354726096, Training Accuracy: 18.208\n",
            "Time taken for training worker 5: 0:00:12.547129\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 3.628398292, Training Accuracy: 14.256\n",
            "Worker 6, [02/04]: Training Loss: 3.507803177, Training Accuracy: 15.728\n",
            "Worker 6, [03/04]: Training Loss: 3.433049370, Training Accuracy: 17.216\n",
            "Worker 6, [04/04]: Training Loss: 3.339555952, Training Accuracy: 18.448\n",
            "Time taken for training worker 6: 0:00:11.825756\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 3.623601736, Training Accuracy: 14.400\n",
            "Worker 7, [02/04]: Training Loss: 3.521822934, Training Accuracy: 16.016\n",
            "Worker 7, [03/04]: Training Loss: 3.412591027, Training Accuracy: 17.248\n",
            "Worker 7, [04/04]: Training Loss: 3.323976899, Training Accuracy: 19.264\n",
            "Time taken for training worker 7: 0:00:11.845928\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 3.609034363, Training Accuracy: 14.656\n",
            "Worker 8, [02/04]: Training Loss: 3.517037321, Training Accuracy: 15.792\n",
            "Worker 8, [03/04]: Training Loss: 3.421768164, Training Accuracy: 17.360\n",
            "Worker 8, [04/04]: Training Loss: 3.311242751, Training Accuracy: 18.608\n",
            "Time taken for training worker 8: 0:00:11.705038\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004240\n",
            "Global Update 03: Test Loss: 3.180366417, Test Accuracy: 22.410\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.404881952, Training Accuracy: 17.952\n",
            "Worker 1, [02/04]: Training Loss: 3.307038658, Training Accuracy: 19.120\n",
            "Worker 1, [03/04]: Training Loss: 3.266158099, Training Accuracy: 19.888\n",
            "Worker 1, [04/04]: Training Loss: 3.179013569, Training Accuracy: 21.152\n",
            "Time taken for training worker 1: 0:00:13.143494\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.357199314, Training Accuracy: 18.096\n",
            "Worker 2, [02/04]: Training Loss: 3.267647933, Training Accuracy: 19.904\n",
            "Worker 2, [03/04]: Training Loss: 3.175105041, Training Accuracy: 21.248\n",
            "Worker 2, [04/04]: Training Loss: 3.099369117, Training Accuracy: 22.368\n",
            "Time taken for training worker 2: 0:00:12.394295\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.370552262, Training Accuracy: 18.224\n",
            "Worker 3, [02/04]: Training Loss: 3.283283582, Training Accuracy: 19.728\n",
            "Worker 3, [03/04]: Training Loss: 3.234574561, Training Accuracy: 20.272\n",
            "Worker 3, [04/04]: Training Loss: 3.141869939, Training Accuracy: 22.128\n",
            "Time taken for training worker 3: 0:00:12.686349\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.397363181, Training Accuracy: 16.912\n",
            "Worker 4, [02/04]: Training Loss: 3.304645047, Training Accuracy: 19.312\n",
            "Worker 4, [03/04]: Training Loss: 3.237503280, Training Accuracy: 20.128\n",
            "Worker 4, [04/04]: Training Loss: 3.156128443, Training Accuracy: 21.504\n",
            "Time taken for training worker 4: 0:00:11.799027\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 3.385090166, Training Accuracy: 18.608\n",
            "Worker 5, [02/04]: Training Loss: 3.279919892, Training Accuracy: 20.112\n",
            "Worker 5, [03/04]: Training Loss: 3.219660642, Training Accuracy: 20.048\n",
            "Worker 5, [04/04]: Training Loss: 3.141919304, Training Accuracy: 22.000\n",
            "Time taken for training worker 5: 0:00:12.093626\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 3.372922593, Training Accuracy: 18.240\n",
            "Worker 6, [02/04]: Training Loss: 3.284895182, Training Accuracy: 19.456\n",
            "Worker 6, [03/04]: Training Loss: 3.194945968, Training Accuracy: 20.528\n",
            "Worker 6, [04/04]: Training Loss: 3.124884829, Training Accuracy: 22.000\n",
            "Time taken for training worker 6: 0:00:11.744205\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 3.399009001, Training Accuracy: 17.856\n",
            "Worker 7, [02/04]: Training Loss: 3.296471771, Training Accuracy: 19.360\n",
            "Worker 7, [03/04]: Training Loss: 3.200047841, Training Accuracy: 21.216\n",
            "Worker 7, [04/04]: Training Loss: 3.103180691, Training Accuracy: 21.968\n",
            "Time taken for training worker 7: 0:00:11.872404\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 3.361305770, Training Accuracy: 18.464\n",
            "Worker 8, [02/04]: Training Loss: 3.275780585, Training Accuracy: 19.952\n",
            "Worker 8, [03/04]: Training Loss: 3.171947161, Training Accuracy: 21.632\n",
            "Worker 8, [04/04]: Training Loss: 3.135914878, Training Accuracy: 21.520\n",
            "Time taken for training worker 8: 0:00:12.046695\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004410\n",
            "Global Update 04: Test Loss: 2.989448664, Test Accuracy: 26.180\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.229717544, Training Accuracy: 21.472\n",
            "Worker 1, [02/04]: Training Loss: 3.125237531, Training Accuracy: 22.192\n",
            "Worker 1, [03/04]: Training Loss: 3.072968982, Training Accuracy: 22.800\n",
            "Worker 1, [04/04]: Training Loss: 2.980372074, Training Accuracy: 24.576\n",
            "Time taken for training worker 1: 0:00:11.726467\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.171697619, Training Accuracy: 21.472\n",
            "Worker 2, [02/04]: Training Loss: 3.104913826, Training Accuracy: 22.720\n",
            "Worker 2, [03/04]: Training Loss: 3.001722669, Training Accuracy: 24.080\n",
            "Worker 2, [04/04]: Training Loss: 2.952094037, Training Accuracy: 24.944\n",
            "Time taken for training worker 2: 0:00:11.618376\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.182825629, Training Accuracy: 21.376\n",
            "Worker 3, [02/04]: Training Loss: 3.117821737, Training Accuracy: 23.104\n",
            "Worker 3, [03/04]: Training Loss: 3.026054796, Training Accuracy: 23.840\n",
            "Worker 3, [04/04]: Training Loss: 2.960769549, Training Accuracy: 25.040\n",
            "Time taken for training worker 3: 0:00:11.710189\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.208824659, Training Accuracy: 21.040\n",
            "Worker 4, [02/04]: Training Loss: 3.131114714, Training Accuracy: 22.160\n",
            "Worker 4, [03/04]: Training Loss: 3.071370249, Training Accuracy: 22.752\n",
            "Worker 4, [04/04]: Training Loss: 2.953141692, Training Accuracy: 25.472\n",
            "Time taken for training worker 4: 0:00:12.776918\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 3.189695808, Training Accuracy: 22.128\n",
            "Worker 5, [02/04]: Training Loss: 3.110598576, Training Accuracy: 22.928\n",
            "Worker 5, [03/04]: Training Loss: 3.035858685, Training Accuracy: 24.080\n",
            "Worker 5, [04/04]: Training Loss: 2.982130056, Training Accuracy: 25.056\n",
            "Time taken for training worker 5: 0:00:12.565651\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 3.191538295, Training Accuracy: 21.696\n",
            "Worker 6, [02/04]: Training Loss: 3.081572837, Training Accuracy: 22.208\n",
            "Worker 6, [03/04]: Training Loss: 3.017890402, Training Accuracy: 24.448\n",
            "Worker 6, [04/04]: Training Loss: 2.951135008, Training Accuracy: 25.648\n",
            "Time taken for training worker 6: 0:00:12.703321\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 3.184826547, Training Accuracy: 21.712\n",
            "Worker 7, [02/04]: Training Loss: 3.108916886, Training Accuracy: 22.848\n",
            "Worker 7, [03/04]: Training Loss: 3.035578309, Training Accuracy: 23.520\n",
            "Worker 7, [04/04]: Training Loss: 2.953965978, Training Accuracy: 25.600\n",
            "Time taken for training worker 7: 0:00:12.383759\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 3.183036629, Training Accuracy: 22.192\n",
            "Worker 8, [02/04]: Training Loss: 3.100598593, Training Accuracy: 23.584\n",
            "Worker 8, [03/04]: Training Loss: 3.037666839, Training Accuracy: 24.096\n",
            "Worker 8, [04/04]: Training Loss: 2.966296639, Training Accuracy: 25.632\n",
            "Time taken for training worker 8: 0:00:12.880731\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004814\n",
            "Global Update 05: Test Loss: 2.840367284, Test Accuracy: 29.110\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.088024534, Training Accuracy: 23.248\n",
            "Worker 1, [02/04]: Training Loss: 3.010902962, Training Accuracy: 24.544\n",
            "Worker 1, [03/04]: Training Loss: 2.922057495, Training Accuracy: 26.304\n",
            "Worker 1, [04/04]: Training Loss: 2.838983611, Training Accuracy: 27.024\n",
            "Time taken for training worker 1: 0:00:12.403269\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.031312928, Training Accuracy: 25.072\n",
            "Worker 2, [02/04]: Training Loss: 2.926480072, Training Accuracy: 26.400\n",
            "Worker 2, [03/04]: Training Loss: 2.862367255, Training Accuracy: 26.144\n",
            "Worker 2, [04/04]: Training Loss: 2.766675791, Training Accuracy: 28.400\n",
            "Time taken for training worker 2: 0:00:12.199147\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.090470290, Training Accuracy: 23.216\n",
            "Worker 3, [02/04]: Training Loss: 2.979840352, Training Accuracy: 24.640\n",
            "Worker 3, [03/04]: Training Loss: 2.900926755, Training Accuracy: 26.800\n",
            "Worker 3, [04/04]: Training Loss: 2.810104735, Training Accuracy: 28.704\n",
            "Time taken for training worker 3: 0:00:11.950721\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.059728710, Training Accuracy: 23.968\n",
            "Worker 4, [02/04]: Training Loss: 2.984354297, Training Accuracy: 25.072\n",
            "Worker 4, [03/04]: Training Loss: 2.903072501, Training Accuracy: 26.352\n",
            "Worker 4, [04/04]: Training Loss: 2.825255190, Training Accuracy: 27.984\n",
            "Time taken for training worker 4: 0:00:12.481488\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 3.067440617, Training Accuracy: 23.952\n",
            "Worker 5, [02/04]: Training Loss: 2.953938929, Training Accuracy: 26.224\n",
            "Worker 5, [03/04]: Training Loss: 2.868996552, Training Accuracy: 27.584\n",
            "Worker 5, [04/04]: Training Loss: 2.787744043, Training Accuracy: 28.528\n",
            "Time taken for training worker 5: 0:00:11.939616\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 3.028440149, Training Accuracy: 24.464\n",
            "Worker 6, [02/04]: Training Loss: 2.970877879, Training Accuracy: 25.152\n",
            "Worker 6, [03/04]: Training Loss: 2.882545121, Training Accuracy: 26.640\n",
            "Worker 6, [04/04]: Training Loss: 2.810486487, Training Accuracy: 27.408\n",
            "Time taken for training worker 6: 0:00:12.165401\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 3.047300891, Training Accuracy: 24.368\n",
            "Worker 7, [02/04]: Training Loss: 2.978645938, Training Accuracy: 24.576\n",
            "Worker 7, [03/04]: Training Loss: 2.859049026, Training Accuracy: 27.360\n",
            "Worker 7, [04/04]: Training Loss: 2.770842343, Training Accuracy: 29.504\n",
            "Time taken for training worker 7: 0:00:12.066512\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 3.032853798, Training Accuracy: 23.904\n",
            "Worker 8, [02/04]: Training Loss: 2.938578477, Training Accuracy: 25.568\n",
            "Worker 8, [03/04]: Training Loss: 2.863477483, Training Accuracy: 27.872\n",
            "Worker 8, [04/04]: Training Loss: 2.755466177, Training Accuracy: 28.896\n",
            "Time taken for training worker 8: 0:00:11.725806\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004080\n",
            "Global Update 06: Test Loss: 2.698567307, Test Accuracy: 32.130\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.974278029, Training Accuracy: 25.552\n",
            "Worker 1, [02/04]: Training Loss: 2.852087967, Training Accuracy: 27.600\n",
            "Worker 1, [03/04]: Training Loss: 2.773097566, Training Accuracy: 28.960\n",
            "Worker 1, [04/04]: Training Loss: 2.671591472, Training Accuracy: 31.104\n",
            "Time taken for training worker 1: 0:00:12.132028\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.869306202, Training Accuracy: 26.672\n",
            "Worker 2, [02/04]: Training Loss: 2.785778435, Training Accuracy: 29.296\n",
            "Worker 2, [03/04]: Training Loss: 2.685688211, Training Accuracy: 29.616\n",
            "Worker 2, [04/04]: Training Loss: 2.616500477, Training Accuracy: 31.952\n",
            "Time taken for training worker 2: 0:00:11.809675\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.915877070, Training Accuracy: 27.536\n",
            "Worker 3, [02/04]: Training Loss: 2.831954525, Training Accuracy: 28.480\n",
            "Worker 3, [03/04]: Training Loss: 2.733956120, Training Accuracy: 29.344\n",
            "Worker 3, [04/04]: Training Loss: 2.649571151, Training Accuracy: 31.440\n",
            "Time taken for training worker 3: 0:00:11.558202\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.952717039, Training Accuracy: 25.872\n",
            "Worker 4, [02/04]: Training Loss: 2.833997814, Training Accuracy: 28.240\n",
            "Worker 4, [03/04]: Training Loss: 2.775327135, Training Accuracy: 28.128\n",
            "Worker 4, [04/04]: Training Loss: 2.676090613, Training Accuracy: 30.608\n",
            "Time taken for training worker 4: 0:00:11.635919\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.913728865, Training Accuracy: 27.744\n",
            "Worker 5, [02/04]: Training Loss: 2.829854571, Training Accuracy: 28.656\n",
            "Worker 5, [03/04]: Training Loss: 2.722917180, Training Accuracy: 29.792\n",
            "Worker 5, [04/04]: Training Loss: 2.632539628, Training Accuracy: 32.064\n",
            "Time taken for training worker 5: 0:00:12.931206\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.896289808, Training Accuracy: 27.472\n",
            "Worker 6, [02/04]: Training Loss: 2.827694063, Training Accuracy: 28.080\n",
            "Worker 6, [03/04]: Training Loss: 2.737700358, Training Accuracy: 29.968\n",
            "Worker 6, [04/04]: Training Loss: 2.637313945, Training Accuracy: 31.712\n",
            "Time taken for training worker 6: 0:00:12.901691\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.912176149, Training Accuracy: 27.792\n",
            "Worker 7, [02/04]: Training Loss: 2.832862394, Training Accuracy: 28.336\n",
            "Worker 7, [03/04]: Training Loss: 2.728151397, Training Accuracy: 29.808\n",
            "Worker 7, [04/04]: Training Loss: 2.679385253, Training Accuracy: 30.576\n",
            "Time taken for training worker 7: 0:00:13.159957\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.882584385, Training Accuracy: 27.360\n",
            "Worker 8, [02/04]: Training Loss: 2.794294296, Training Accuracy: 28.976\n",
            "Worker 8, [03/04]: Training Loss: 2.711364089, Training Accuracy: 30.976\n",
            "Worker 8, [04/04]: Training Loss: 2.650781252, Training Accuracy: 31.248\n",
            "Time taken for training worker 8: 0:00:12.685933\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004247\n",
            "Global Update 07: Test Loss: 2.566984055, Test Accuracy: 34.510\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.831155388, Training Accuracy: 27.952\n",
            "Worker 1, [02/04]: Training Loss: 2.752959244, Training Accuracy: 29.232\n",
            "Worker 1, [03/04]: Training Loss: 2.634409878, Training Accuracy: 31.376\n",
            "Worker 1, [04/04]: Training Loss: 2.562600340, Training Accuracy: 33.040\n",
            "Time taken for training worker 1: 0:00:11.829731\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.759272142, Training Accuracy: 29.760\n",
            "Worker 2, [02/04]: Training Loss: 2.687073720, Training Accuracy: 30.672\n",
            "Worker 2, [03/04]: Training Loss: 2.578074680, Training Accuracy: 33.088\n",
            "Worker 2, [04/04]: Training Loss: 2.471244149, Training Accuracy: 34.368\n",
            "Time taken for training worker 2: 0:00:12.354227\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.813078659, Training Accuracy: 28.928\n",
            "Worker 3, [02/04]: Training Loss: 2.722355565, Training Accuracy: 29.728\n",
            "Worker 3, [03/04]: Training Loss: 2.610441643, Training Accuracy: 31.952\n",
            "Worker 3, [04/04]: Training Loss: 2.534108743, Training Accuracy: 33.008\n",
            "Time taken for training worker 3: 0:00:11.817383\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.804388888, Training Accuracy: 29.280\n",
            "Worker 4, [02/04]: Training Loss: 2.697649121, Training Accuracy: 30.976\n",
            "Worker 4, [03/04]: Training Loss: 2.623338115, Training Accuracy: 32.512\n",
            "Worker 4, [04/04]: Training Loss: 2.518784472, Training Accuracy: 33.712\n",
            "Time taken for training worker 4: 0:00:12.198462\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.815788004, Training Accuracy: 28.016\n",
            "Worker 5, [02/04]: Training Loss: 2.690926335, Training Accuracy: 31.184\n",
            "Worker 5, [03/04]: Training Loss: 2.600322587, Training Accuracy: 32.864\n",
            "Worker 5, [04/04]: Training Loss: 2.518997825, Training Accuracy: 34.464\n",
            "Time taken for training worker 5: 0:00:11.743523\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.743561937, Training Accuracy: 29.904\n",
            "Worker 6, [02/04]: Training Loss: 2.674070100, Training Accuracy: 30.768\n",
            "Worker 6, [03/04]: Training Loss: 2.602358052, Training Accuracy: 31.904\n",
            "Worker 6, [04/04]: Training Loss: 2.510568086, Training Accuracy: 33.984\n",
            "Time taken for training worker 6: 0:00:12.464718\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.815444394, Training Accuracy: 29.184\n",
            "Worker 7, [02/04]: Training Loss: 2.672131572, Training Accuracy: 31.536\n",
            "Worker 7, [03/04]: Training Loss: 2.604435505, Training Accuracy: 32.896\n",
            "Worker 7, [04/04]: Training Loss: 2.527557602, Training Accuracy: 33.792\n",
            "Time taken for training worker 7: 0:00:12.214698\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.771207802, Training Accuracy: 29.472\n",
            "Worker 8, [02/04]: Training Loss: 2.690960210, Training Accuracy: 30.944\n",
            "Worker 8, [03/04]: Training Loss: 2.571578216, Training Accuracy: 33.040\n",
            "Worker 8, [04/04]: Training Loss: 2.481638045, Training Accuracy: 34.704\n",
            "Time taken for training worker 8: 0:00:11.656947\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004066\n",
            "Global Update 08: Test Loss: 2.466554681, Test Accuracy: 36.140\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.750395006, Training Accuracy: 30.528\n",
            "Worker 1, [02/04]: Training Loss: 2.595055660, Training Accuracy: 33.120\n",
            "Worker 1, [03/04]: Training Loss: 2.500564162, Training Accuracy: 34.912\n",
            "Worker 1, [04/04]: Training Loss: 2.453597055, Training Accuracy: 34.560\n",
            "Time taken for training worker 1: 0:00:11.617867\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.656200112, Training Accuracy: 31.536\n",
            "Worker 2, [02/04]: Training Loss: 2.558239370, Training Accuracy: 33.936\n",
            "Worker 2, [03/04]: Training Loss: 2.436316236, Training Accuracy: 36.224\n",
            "Worker 2, [04/04]: Training Loss: 2.387002539, Training Accuracy: 36.240\n",
            "Time taken for training worker 2: 0:00:12.165606\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.695436894, Training Accuracy: 31.296\n",
            "Worker 3, [02/04]: Training Loss: 2.578300890, Training Accuracy: 31.968\n",
            "Worker 3, [03/04]: Training Loss: 2.508163107, Training Accuracy: 34.128\n",
            "Worker 3, [04/04]: Training Loss: 2.456879239, Training Accuracy: 34.352\n",
            "Time taken for training worker 3: 0:00:12.817692\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.717185471, Training Accuracy: 31.216\n",
            "Worker 4, [02/04]: Training Loss: 2.612088420, Training Accuracy: 32.272\n",
            "Worker 4, [03/04]: Training Loss: 2.526050312, Training Accuracy: 34.080\n",
            "Worker 4, [04/04]: Training Loss: 2.460341078, Training Accuracy: 35.056\n",
            "Time taken for training worker 4: 0:00:12.199346\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.686882700, Training Accuracy: 31.680\n",
            "Worker 5, [02/04]: Training Loss: 2.584007139, Training Accuracy: 33.488\n",
            "Worker 5, [03/04]: Training Loss: 2.486600640, Training Accuracy: 34.736\n",
            "Worker 5, [04/04]: Training Loss: 2.409054245, Training Accuracy: 36.080\n",
            "Time taken for training worker 5: 0:00:12.606868\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.681597011, Training Accuracy: 30.800\n",
            "Worker 6, [02/04]: Training Loss: 2.552521961, Training Accuracy: 33.808\n",
            "Worker 6, [03/04]: Training Loss: 2.473727443, Training Accuracy: 35.104\n",
            "Worker 6, [04/04]: Training Loss: 2.407031839, Training Accuracy: 36.160\n",
            "Time taken for training worker 6: 0:00:12.078801\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.673361581, Training Accuracy: 31.968\n",
            "Worker 7, [02/04]: Training Loss: 2.579162748, Training Accuracy: 33.696\n",
            "Worker 7, [03/04]: Training Loss: 2.468050378, Training Accuracy: 35.568\n",
            "Worker 7, [04/04]: Training Loss: 2.415461262, Training Accuracy: 35.984\n",
            "Time taken for training worker 7: 0:00:12.487396\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.661379454, Training Accuracy: 31.552\n",
            "Worker 8, [02/04]: Training Loss: 2.557661348, Training Accuracy: 34.208\n",
            "Worker 8, [03/04]: Training Loss: 2.466196918, Training Accuracy: 35.168\n",
            "Worker 8, [04/04]: Training Loss: 2.371766100, Training Accuracy: 35.984\n",
            "Time taken for training worker 8: 0:00:12.944573\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004573\n",
            "Global Update 09: Test Loss: 2.391692504, Test Accuracy: 38.410\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.642950462, Training Accuracy: 32.224\n",
            "Worker 1, [02/04]: Training Loss: 2.513772760, Training Accuracy: 34.400\n",
            "Worker 1, [03/04]: Training Loss: 2.413446830, Training Accuracy: 36.640\n",
            "Worker 1, [04/04]: Training Loss: 2.366478629, Training Accuracy: 37.616\n",
            "Time taken for training worker 1: 0:00:12.034120\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.579097764, Training Accuracy: 33.120\n",
            "Worker 2, [02/04]: Training Loss: 2.443275924, Training Accuracy: 35.232\n",
            "Worker 2, [03/04]: Training Loss: 2.373274351, Training Accuracy: 36.512\n",
            "Worker 2, [04/04]: Training Loss: 2.253060163, Training Accuracy: 40.080\n",
            "Time taken for training worker 2: 0:00:11.662100\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.636661099, Training Accuracy: 31.968\n",
            "Worker 3, [02/04]: Training Loss: 2.479629340, Training Accuracy: 34.880\n",
            "Worker 3, [03/04]: Training Loss: 2.403215129, Training Accuracy: 36.176\n",
            "Worker 3, [04/04]: Training Loss: 2.303245133, Training Accuracy: 39.168\n",
            "Time taken for training worker 3: 0:00:11.765312\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.615387508, Training Accuracy: 32.816\n",
            "Worker 4, [02/04]: Training Loss: 2.473811544, Training Accuracy: 35.664\n",
            "Worker 4, [03/04]: Training Loss: 2.423806313, Training Accuracy: 35.792\n",
            "Worker 4, [04/04]: Training Loss: 2.324218991, Training Accuracy: 38.032\n",
            "Time taken for training worker 4: 0:00:11.660955\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.626963299, Training Accuracy: 32.896\n",
            "Worker 5, [02/04]: Training Loss: 2.485162265, Training Accuracy: 34.848\n",
            "Worker 5, [03/04]: Training Loss: 2.368390863, Training Accuracy: 37.328\n",
            "Worker 5, [04/04]: Training Loss: 2.289968551, Training Accuracy: 39.040\n",
            "Time taken for training worker 5: 0:00:11.730823\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.571456670, Training Accuracy: 33.392\n",
            "Worker 6, [02/04]: Training Loss: 2.463585642, Training Accuracy: 35.488\n",
            "Worker 6, [03/04]: Training Loss: 2.375487076, Training Accuracy: 37.424\n",
            "Worker 6, [04/04]: Training Loss: 2.311275103, Training Accuracy: 38.512\n",
            "Time taken for training worker 6: 0:00:11.810209\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.573657685, Training Accuracy: 33.520\n",
            "Worker 7, [02/04]: Training Loss: 2.480245913, Training Accuracy: 34.720\n",
            "Worker 7, [03/04]: Training Loss: 2.376790865, Training Accuracy: 36.864\n",
            "Worker 7, [04/04]: Training Loss: 2.328312609, Training Accuracy: 38.384\n",
            "Time taken for training worker 7: 0:00:12.468274\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.577637047, Training Accuracy: 33.568\n",
            "Worker 8, [02/04]: Training Loss: 2.471201082, Training Accuracy: 35.472\n",
            "Worker 8, [03/04]: Training Loss: 2.371160723, Training Accuracy: 37.008\n",
            "Worker 8, [04/04]: Training Loss: 2.281586405, Training Accuracy: 39.168\n",
            "Time taken for training worker 8: 0:00:12.098958\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004541\n",
            "Global Update 10: Test Loss: 2.321560775, Test Accuracy: 39.810\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.554077182, Training Accuracy: 33.648\n",
            "Worker 1, [02/04]: Training Loss: 2.408729964, Training Accuracy: 36.880\n",
            "Worker 1, [03/04]: Training Loss: 2.310666977, Training Accuracy: 38.704\n",
            "Worker 1, [04/04]: Training Loss: 2.257623452, Training Accuracy: 39.056\n",
            "Time taken for training worker 1: 0:00:12.853431\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.491091416, Training Accuracy: 34.864\n",
            "Worker 2, [02/04]: Training Loss: 2.355551295, Training Accuracy: 37.664\n",
            "Worker 2, [03/04]: Training Loss: 2.289780847, Training Accuracy: 38.576\n",
            "Worker 2, [04/04]: Training Loss: 2.192881162, Training Accuracy: 41.328\n",
            "Time taken for training worker 2: 0:00:12.297827\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.517055972, Training Accuracy: 34.496\n",
            "Worker 3, [02/04]: Training Loss: 2.407165563, Training Accuracy: 37.056\n",
            "Worker 3, [03/04]: Training Loss: 2.275559150, Training Accuracy: 39.568\n",
            "Worker 3, [04/04]: Training Loss: 2.250978455, Training Accuracy: 39.936\n",
            "Time taken for training worker 3: 0:00:12.431989\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.538258847, Training Accuracy: 34.416\n",
            "Worker 4, [02/04]: Training Loss: 2.400633617, Training Accuracy: 37.280\n",
            "Worker 4, [03/04]: Training Loss: 2.333820803, Training Accuracy: 38.240\n",
            "Worker 4, [04/04]: Training Loss: 2.233147895, Training Accuracy: 40.960\n",
            "Time taken for training worker 4: 0:00:12.294224\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.491687322, Training Accuracy: 35.936\n",
            "Worker 5, [02/04]: Training Loss: 2.387380449, Training Accuracy: 37.088\n",
            "Worker 5, [03/04]: Training Loss: 2.298736516, Training Accuracy: 39.104\n",
            "Worker 5, [04/04]: Training Loss: 2.205812459, Training Accuracy: 40.960\n",
            "Time taken for training worker 5: 0:00:12.053532\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.494765433, Training Accuracy: 34.400\n",
            "Worker 6, [02/04]: Training Loss: 2.375691471, Training Accuracy: 36.672\n",
            "Worker 6, [03/04]: Training Loss: 2.246085470, Training Accuracy: 39.520\n",
            "Worker 6, [04/04]: Training Loss: 2.216246713, Training Accuracy: 39.984\n",
            "Time taken for training worker 6: 0:00:12.068068\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.503715398, Training Accuracy: 34.592\n",
            "Worker 7, [02/04]: Training Loss: 2.369766091, Training Accuracy: 37.856\n",
            "Worker 7, [03/04]: Training Loss: 2.285059834, Training Accuracy: 39.632\n",
            "Worker 7, [04/04]: Training Loss: 2.196779563, Training Accuracy: 41.200\n",
            "Time taken for training worker 7: 0:00:11.729868\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.478820497, Training Accuracy: 35.808\n",
            "Worker 8, [02/04]: Training Loss: 2.340954331, Training Accuracy: 37.312\n",
            "Worker 8, [03/04]: Training Loss: 2.265479767, Training Accuracy: 39.296\n",
            "Worker 8, [04/04]: Training Loss: 2.187326136, Training Accuracy: 41.536\n",
            "Time taken for training worker 8: 0:00:11.678428\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004318\n",
            "Global Update 11: Test Loss: 2.274624806, Test Accuracy: 41.050\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.442747862, Training Accuracy: 36.608\n",
            "Worker 1, [02/04]: Training Loss: 2.353377643, Training Accuracy: 37.968\n",
            "Worker 1, [03/04]: Training Loss: 2.216084822, Training Accuracy: 40.208\n",
            "Worker 1, [04/04]: Training Loss: 2.195522505, Training Accuracy: 40.880\n",
            "Time taken for training worker 1: 0:00:11.985065\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.422374652, Training Accuracy: 36.704\n",
            "Worker 2, [02/04]: Training Loss: 2.266371163, Training Accuracy: 39.488\n",
            "Worker 2, [03/04]: Training Loss: 2.197669125, Training Accuracy: 41.072\n",
            "Worker 2, [04/04]: Training Loss: 2.099977493, Training Accuracy: 42.304\n",
            "Time taken for training worker 2: 0:00:12.146054\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.428741071, Training Accuracy: 36.928\n",
            "Worker 3, [02/04]: Training Loss: 2.298536178, Training Accuracy: 39.248\n",
            "Worker 3, [03/04]: Training Loss: 2.212231696, Training Accuracy: 40.960\n",
            "Worker 3, [04/04]: Training Loss: 2.147305626, Training Accuracy: 41.696\n",
            "Time taken for training worker 3: 0:00:12.775234\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.455844186, Training Accuracy: 36.208\n",
            "Worker 4, [02/04]: Training Loss: 2.324076269, Training Accuracy: 38.704\n",
            "Worker 4, [03/04]: Training Loss: 2.249846328, Training Accuracy: 39.760\n",
            "Worker 4, [04/04]: Training Loss: 2.156915458, Training Accuracy: 42.016\n",
            "Time taken for training worker 4: 0:00:12.655766\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.437261446, Training Accuracy: 36.992\n",
            "Worker 5, [02/04]: Training Loss: 2.305022106, Training Accuracy: 39.312\n",
            "Worker 5, [03/04]: Training Loss: 2.207923583, Training Accuracy: 41.088\n",
            "Worker 5, [04/04]: Training Loss: 2.132633181, Training Accuracy: 42.160\n",
            "Time taken for training worker 5: 0:00:11.915031\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.412440216, Training Accuracy: 36.384\n",
            "Worker 6, [02/04]: Training Loss: 2.295682816, Training Accuracy: 38.816\n",
            "Worker 6, [03/04]: Training Loss: 2.200824559, Training Accuracy: 41.248\n",
            "Worker 6, [04/04]: Training Loss: 2.103776707, Training Accuracy: 43.968\n",
            "Time taken for training worker 6: 0:00:12.129375\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.439598412, Training Accuracy: 36.496\n",
            "Worker 7, [02/04]: Training Loss: 2.279347335, Training Accuracy: 39.232\n",
            "Worker 7, [03/04]: Training Loss: 2.200141853, Training Accuracy: 41.200\n",
            "Worker 7, [04/04]: Training Loss: 2.154151750, Training Accuracy: 42.560\n",
            "Time taken for training worker 7: 0:00:12.315097\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.400174989, Training Accuracy: 37.456\n",
            "Worker 8, [02/04]: Training Loss: 2.288264758, Training Accuracy: 38.880\n",
            "Worker 8, [03/04]: Training Loss: 2.203871124, Training Accuracy: 40.608\n",
            "Worker 8, [04/04]: Training Loss: 2.093070995, Training Accuracy: 43.248\n",
            "Time taken for training worker 8: 0:00:11.942299\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004377\n",
            "Global Update 12: Test Loss: 2.213652368, Test Accuracy: 42.130\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.377484322, Training Accuracy: 37.696\n",
            "Worker 1, [02/04]: Training Loss: 2.255495560, Training Accuracy: 40.016\n",
            "Worker 1, [03/04]: Training Loss: 2.179881360, Training Accuracy: 41.872\n",
            "Worker 1, [04/04]: Training Loss: 2.087540005, Training Accuracy: 43.504\n",
            "Time taken for training worker 1: 0:00:11.980445\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.325215173, Training Accuracy: 38.704\n",
            "Worker 2, [02/04]: Training Loss: 2.197497816, Training Accuracy: 41.456\n",
            "Worker 2, [03/04]: Training Loss: 2.068583373, Training Accuracy: 43.696\n",
            "Worker 2, [04/04]: Training Loss: 1.999657273, Training Accuracy: 44.624\n",
            "Time taken for training worker 2: 0:00:12.049614\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.370970870, Training Accuracy: 37.296\n",
            "Worker 3, [02/04]: Training Loss: 2.242878545, Training Accuracy: 40.176\n",
            "Worker 3, [03/04]: Training Loss: 2.139368168, Training Accuracy: 42.672\n",
            "Worker 3, [04/04]: Training Loss: 2.046288447, Training Accuracy: 44.512\n",
            "Time taken for training worker 3: 0:00:12.234194\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.365399805, Training Accuracy: 37.952\n",
            "Worker 4, [02/04]: Training Loss: 2.264975386, Training Accuracy: 40.656\n",
            "Worker 4, [03/04]: Training Loss: 2.177874446, Training Accuracy: 41.600\n",
            "Worker 4, [04/04]: Training Loss: 2.060257042, Training Accuracy: 44.800\n",
            "Time taken for training worker 4: 0:00:11.690779\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.356401491, Training Accuracy: 38.304\n",
            "Worker 5, [02/04]: Training Loss: 2.240798347, Training Accuracy: 40.400\n",
            "Worker 5, [03/04]: Training Loss: 2.110105341, Training Accuracy: 42.848\n",
            "Worker 5, [04/04]: Training Loss: 2.008636417, Training Accuracy: 45.504\n",
            "Time taken for training worker 5: 0:00:12.150456\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.332037661, Training Accuracy: 38.336\n",
            "Worker 6, [02/04]: Training Loss: 2.189485905, Training Accuracy: 41.440\n",
            "Worker 6, [03/04]: Training Loss: 2.129125152, Training Accuracy: 42.304\n",
            "Worker 6, [04/04]: Training Loss: 2.056971222, Training Accuracy: 44.080\n",
            "Time taken for training worker 6: 0:00:12.355335\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.341936913, Training Accuracy: 39.040\n",
            "Worker 7, [02/04]: Training Loss: 2.202735756, Training Accuracy: 41.152\n",
            "Worker 7, [03/04]: Training Loss: 2.102238363, Training Accuracy: 43.568\n",
            "Worker 7, [04/04]: Training Loss: 2.049241049, Training Accuracy: 44.688\n",
            "Time taken for training worker 7: 0:00:12.148500\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.346957872, Training Accuracy: 38.384\n",
            "Worker 8, [02/04]: Training Loss: 2.200614609, Training Accuracy: 41.408\n",
            "Worker 8, [03/04]: Training Loss: 2.119635522, Training Accuracy: 42.880\n",
            "Worker 8, [04/04]: Training Loss: 2.010508513, Training Accuracy: 45.488\n",
            "Time taken for training worker 8: 0:00:12.956477\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004702\n",
            "Global Update 13: Test Loss: 2.163390894, Test Accuracy: 43.370\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.316573301, Training Accuracy: 39.824\n",
            "Worker 1, [02/04]: Training Loss: 2.202705180, Training Accuracy: 41.744\n",
            "Worker 1, [03/04]: Training Loss: 2.060379545, Training Accuracy: 43.728\n",
            "Worker 1, [04/04]: Training Loss: 1.996760534, Training Accuracy: 45.264\n",
            "Time taken for training worker 1: 0:00:12.449038\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.243956567, Training Accuracy: 39.984\n",
            "Worker 2, [02/04]: Training Loss: 2.142121861, Training Accuracy: 42.176\n",
            "Worker 2, [03/04]: Training Loss: 2.015077771, Training Accuracy: 44.608\n",
            "Worker 2, [04/04]: Training Loss: 1.941571170, Training Accuracy: 46.576\n",
            "Time taken for training worker 2: 0:00:12.261470\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.315226501, Training Accuracy: 39.120\n",
            "Worker 3, [02/04]: Training Loss: 2.159848959, Training Accuracy: 42.384\n",
            "Worker 3, [03/04]: Training Loss: 2.063635393, Training Accuracy: 44.544\n",
            "Worker 3, [04/04]: Training Loss: 1.966456832, Training Accuracy: 45.808\n",
            "Time taken for training worker 3: 0:00:12.778025\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.315101396, Training Accuracy: 38.640\n",
            "Worker 4, [02/04]: Training Loss: 2.166467708, Training Accuracy: 42.896\n",
            "Worker 4, [03/04]: Training Loss: 2.073593020, Training Accuracy: 42.816\n",
            "Worker 4, [04/04]: Training Loss: 1.985992398, Training Accuracy: 45.600\n",
            "Time taken for training worker 4: 0:00:12.584129\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.291582640, Training Accuracy: 40.112\n",
            "Worker 5, [02/04]: Training Loss: 2.158513300, Training Accuracy: 41.952\n",
            "Worker 5, [03/04]: Training Loss: 2.034806271, Training Accuracy: 44.640\n",
            "Worker 5, [04/04]: Training Loss: 1.952671828, Training Accuracy: 46.224\n",
            "Time taken for training worker 5: 0:00:11.775957\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.272720685, Training Accuracy: 39.088\n",
            "Worker 6, [02/04]: Training Loss: 2.129316435, Training Accuracy: 42.432\n",
            "Worker 6, [03/04]: Training Loss: 2.017171450, Training Accuracy: 45.136\n",
            "Worker 6, [04/04]: Training Loss: 1.932899692, Training Accuracy: 47.248\n",
            "Time taken for training worker 6: 0:00:12.437050\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.254526395, Training Accuracy: 40.416\n",
            "Worker 7, [02/04]: Training Loss: 2.163868587, Training Accuracy: 42.016\n",
            "Worker 7, [03/04]: Training Loss: 2.054814218, Training Accuracy: 44.128\n",
            "Worker 7, [04/04]: Training Loss: 1.958837530, Training Accuracy: 46.144\n",
            "Time taken for training worker 7: 0:00:12.470935\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.269787500, Training Accuracy: 39.520\n",
            "Worker 8, [02/04]: Training Loss: 2.131415011, Training Accuracy: 42.352\n",
            "Worker 8, [03/04]: Training Loss: 2.022103377, Training Accuracy: 44.720\n",
            "Worker 8, [04/04]: Training Loss: 1.925509598, Training Accuracy: 47.472\n",
            "Time taken for training worker 8: 0:00:12.836459\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004588\n",
            "Global Update 14: Test Loss: 2.136021059, Test Accuracy: 43.870\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.248335567, Training Accuracy: 40.416\n",
            "Worker 1, [02/04]: Training Loss: 2.095482701, Training Accuracy: 43.584\n",
            "Worker 1, [03/04]: Training Loss: 2.004722387, Training Accuracy: 45.856\n",
            "Worker 1, [04/04]: Training Loss: 1.939851091, Training Accuracy: 46.224\n",
            "Time taken for training worker 1: 0:00:11.907175\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.207235881, Training Accuracy: 41.248\n",
            "Worker 2, [02/04]: Training Loss: 2.044485840, Training Accuracy: 44.176\n",
            "Worker 2, [03/04]: Training Loss: 1.938131039, Training Accuracy: 46.320\n",
            "Worker 2, [04/04]: Training Loss: 1.890031970, Training Accuracy: 47.376\n",
            "Time taken for training worker 2: 0:00:12.202110\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.238724290, Training Accuracy: 41.072\n",
            "Worker 3, [02/04]: Training Loss: 2.108443396, Training Accuracy: 43.264\n",
            "Worker 3, [03/04]: Training Loss: 1.981127643, Training Accuracy: 45.872\n",
            "Worker 3, [04/04]: Training Loss: 1.907385068, Training Accuracy: 47.840\n",
            "Time taken for training worker 3: 0:00:12.355246\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.242267414, Training Accuracy: 41.440\n",
            "Worker 4, [02/04]: Training Loss: 2.102972677, Training Accuracy: 43.984\n",
            "Worker 4, [03/04]: Training Loss: 1.998982624, Training Accuracy: 46.480\n",
            "Worker 4, [04/04]: Training Loss: 1.932691968, Training Accuracy: 47.360\n",
            "Time taken for training worker 4: 0:00:12.968050\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.210556311, Training Accuracy: 41.696\n",
            "Worker 5, [02/04]: Training Loss: 2.053557865, Training Accuracy: 45.040\n",
            "Worker 5, [03/04]: Training Loss: 1.984555331, Training Accuracy: 46.320\n",
            "Worker 5, [04/04]: Training Loss: 1.869105733, Training Accuracy: 48.816\n",
            "Time taken for training worker 5: 0:00:11.716449\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.182402748, Training Accuracy: 41.312\n",
            "Worker 6, [02/04]: Training Loss: 2.057958858, Training Accuracy: 44.336\n",
            "Worker 6, [03/04]: Training Loss: 1.944590660, Training Accuracy: 46.800\n",
            "Worker 6, [04/04]: Training Loss: 1.870057351, Training Accuracy: 49.056\n",
            "Time taken for training worker 6: 0:00:12.363753\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.230058033, Training Accuracy: 40.960\n",
            "Worker 7, [02/04]: Training Loss: 2.087494621, Training Accuracy: 44.352\n",
            "Worker 7, [03/04]: Training Loss: 1.982922104, Training Accuracy: 46.848\n",
            "Worker 7, [04/04]: Training Loss: 1.845282147, Training Accuracy: 50.224\n",
            "Time taken for training worker 7: 0:00:12.884908\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.200889911, Training Accuracy: 41.264\n",
            "Worker 8, [02/04]: Training Loss: 2.044343678, Training Accuracy: 44.624\n",
            "Worker 8, [03/04]: Training Loss: 1.968967010, Training Accuracy: 45.920\n",
            "Worker 8, [04/04]: Training Loss: 1.867463081, Training Accuracy: 48.016\n",
            "Time taken for training worker 8: 0:00:12.066109\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.011516\n",
            "Global Update 15: Test Loss: 2.104322992, Test Accuracy: 44.250\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.200024319, Training Accuracy: 41.856\n",
            "Worker 1, [02/04]: Training Loss: 2.038775224, Training Accuracy: 45.344\n",
            "Worker 1, [03/04]: Training Loss: 1.977006956, Training Accuracy: 45.264\n",
            "Worker 1, [04/04]: Training Loss: 1.830372066, Training Accuracy: 49.008\n",
            "Time taken for training worker 1: 0:00:11.887751\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.107129063, Training Accuracy: 43.712\n",
            "Worker 2, [02/04]: Training Loss: 1.979752923, Training Accuracy: 45.712\n",
            "Worker 2, [03/04]: Training Loss: 1.891934884, Training Accuracy: 47.696\n",
            "Worker 2, [04/04]: Training Loss: 1.794146899, Training Accuracy: 50.000\n",
            "Time taken for training worker 2: 0:00:12.434811\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.173075547, Training Accuracy: 42.096\n",
            "Worker 3, [02/04]: Training Loss: 2.021574217, Training Accuracy: 45.040\n",
            "Worker 3, [03/04]: Training Loss: 1.899734878, Training Accuracy: 47.248\n",
            "Worker 3, [04/04]: Training Loss: 1.843639863, Training Accuracy: 49.504\n",
            "Time taken for training worker 3: 0:00:12.201567\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.184895649, Training Accuracy: 41.936\n",
            "Worker 4, [02/04]: Training Loss: 2.023080360, Training Accuracy: 45.376\n",
            "Worker 4, [03/04]: Training Loss: 1.932214349, Training Accuracy: 47.904\n",
            "Worker 4, [04/04]: Training Loss: 1.848986935, Training Accuracy: 48.992\n",
            "Time taken for training worker 4: 0:00:11.851939\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.156518806, Training Accuracy: 42.464\n",
            "Worker 5, [02/04]: Training Loss: 1.993159863, Training Accuracy: 46.480\n",
            "Worker 5, [03/04]: Training Loss: 1.914819479, Training Accuracy: 47.568\n",
            "Worker 5, [04/04]: Training Loss: 1.824989140, Training Accuracy: 49.488\n",
            "Time taken for training worker 5: 0:00:11.742600\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.139937560, Training Accuracy: 42.736\n",
            "Worker 6, [02/04]: Training Loss: 1.997210650, Training Accuracy: 45.312\n",
            "Worker 6, [03/04]: Training Loss: 1.884024270, Training Accuracy: 48.016\n",
            "Worker 6, [04/04]: Training Loss: 1.807867369, Training Accuracy: 49.504\n",
            "Time taken for training worker 6: 0:00:11.896352\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.155073817, Training Accuracy: 42.624\n",
            "Worker 7, [02/04]: Training Loss: 1.994460545, Training Accuracy: 45.952\n",
            "Worker 7, [03/04]: Training Loss: 1.914660569, Training Accuracy: 48.176\n",
            "Worker 7, [04/04]: Training Loss: 1.806015780, Training Accuracy: 49.008\n",
            "Time taken for training worker 7: 0:00:12.209614\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.131438698, Training Accuracy: 42.528\n",
            "Worker 8, [02/04]: Training Loss: 1.988606657, Training Accuracy: 46.240\n",
            "Worker 8, [03/04]: Training Loss: 1.860128933, Training Accuracy: 48.592\n",
            "Worker 8, [04/04]: Training Loss: 1.778362045, Training Accuracy: 51.376\n",
            "Time taken for training worker 8: 0:00:11.484921\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004078\n",
            "Global Update 16: Test Loss: 2.078934067, Test Accuracy: 45.320\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.135262570, Training Accuracy: 43.264\n",
            "Worker 1, [02/04]: Training Loss: 1.983139834, Training Accuracy: 46.336\n",
            "Worker 1, [03/04]: Training Loss: 1.884313790, Training Accuracy: 47.776\n",
            "Worker 1, [04/04]: Training Loss: 1.784757501, Training Accuracy: 50.832\n",
            "Time taken for training worker 1: 0:00:12.369281\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.075170386, Training Accuracy: 43.888\n",
            "Worker 2, [02/04]: Training Loss: 1.906910522, Training Accuracy: 47.744\n",
            "Worker 2, [03/04]: Training Loss: 1.821034386, Training Accuracy: 49.184\n",
            "Worker 2, [04/04]: Training Loss: 1.753605287, Training Accuracy: 50.736\n",
            "Time taken for training worker 2: 0:00:12.815693\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.095751731, Training Accuracy: 44.512\n",
            "Worker 3, [02/04]: Training Loss: 1.929883439, Training Accuracy: 48.192\n",
            "Worker 3, [03/04]: Training Loss: 1.840599076, Training Accuracy: 49.552\n",
            "Worker 3, [04/04]: Training Loss: 1.774842782, Training Accuracy: 50.592\n",
            "Time taken for training worker 3: 0:00:12.160557\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.108076401, Training Accuracy: 43.888\n",
            "Worker 4, [02/04]: Training Loss: 1.958065170, Training Accuracy: 46.448\n",
            "Worker 4, [03/04]: Training Loss: 1.870261512, Training Accuracy: 48.208\n",
            "Worker 4, [04/04]: Training Loss: 1.747825304, Training Accuracy: 51.296\n",
            "Time taken for training worker 4: 0:00:12.003241\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.095472280, Training Accuracy: 43.440\n",
            "Worker 5, [02/04]: Training Loss: 1.948331771, Training Accuracy: 46.608\n",
            "Worker 5, [03/04]: Training Loss: 1.834932547, Training Accuracy: 50.432\n",
            "Worker 5, [04/04]: Training Loss: 1.765750462, Training Accuracy: 49.920\n",
            "Time taken for training worker 5: 0:00:12.327148\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.076901139, Training Accuracy: 44.096\n",
            "Worker 6, [02/04]: Training Loss: 1.941968984, Training Accuracy: 46.480\n",
            "Worker 6, [03/04]: Training Loss: 1.821246532, Training Accuracy: 49.312\n",
            "Worker 6, [04/04]: Training Loss: 1.726065909, Training Accuracy: 51.232\n",
            "Time taken for training worker 6: 0:00:11.891525\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.075599312, Training Accuracy: 44.464\n",
            "Worker 7, [02/04]: Training Loss: 1.927342967, Training Accuracy: 47.696\n",
            "Worker 7, [03/04]: Training Loss: 1.829230063, Training Accuracy: 50.416\n",
            "Worker 7, [04/04]: Training Loss: 1.744602573, Training Accuracy: 51.968\n",
            "Time taken for training worker 7: 0:00:11.974509\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.060352260, Training Accuracy: 44.016\n",
            "Worker 8, [02/04]: Training Loss: 1.935857607, Training Accuracy: 47.424\n",
            "Worker 8, [03/04]: Training Loss: 1.791996794, Training Accuracy: 50.416\n",
            "Worker 8, [04/04]: Training Loss: 1.708021840, Training Accuracy: 52.096\n",
            "Time taken for training worker 8: 0:00:11.799757\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004267\n",
            "Global Update 17: Test Loss: 2.058595610, Test Accuracy: 46.030\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.072990252, Training Accuracy: 43.728\n",
            "Worker 1, [02/04]: Training Loss: 1.918580998, Training Accuracy: 47.200\n",
            "Worker 1, [03/04]: Training Loss: 1.795842419, Training Accuracy: 50.112\n",
            "Worker 1, [04/04]: Training Loss: 1.707856339, Training Accuracy: 52.176\n",
            "Time taken for training worker 1: 0:00:11.564008\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.052793486, Training Accuracy: 45.216\n",
            "Worker 2, [02/04]: Training Loss: 1.840200704, Training Accuracy: 48.352\n",
            "Worker 2, [03/04]: Training Loss: 1.746756458, Training Accuracy: 51.232\n",
            "Worker 2, [04/04]: Training Loss: 1.681257708, Training Accuracy: 52.432\n",
            "Time taken for training worker 2: 0:00:11.499515\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.047465614, Training Accuracy: 45.392\n",
            "Worker 3, [02/04]: Training Loss: 1.920589481, Training Accuracy: 48.000\n",
            "Worker 3, [03/04]: Training Loss: 1.795182099, Training Accuracy: 50.352\n",
            "Worker 3, [04/04]: Training Loss: 1.710290752, Training Accuracy: 52.832\n",
            "Time taken for training worker 3: 0:00:12.014556\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.053124122, Training Accuracy: 44.448\n",
            "Worker 4, [02/04]: Training Loss: 1.900779427, Training Accuracy: 48.176\n",
            "Worker 4, [03/04]: Training Loss: 1.807267935, Training Accuracy: 50.560\n",
            "Worker 4, [04/04]: Training Loss: 1.714178052, Training Accuracy: 52.080\n",
            "Time taken for training worker 4: 0:00:12.306098\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 2.045132844, Training Accuracy: 45.328\n",
            "Worker 5, [02/04]: Training Loss: 1.864383339, Training Accuracy: 48.768\n",
            "Worker 5, [03/04]: Training Loss: 1.782025803, Training Accuracy: 50.240\n",
            "Worker 5, [04/04]: Training Loss: 1.641406033, Training Accuracy: 53.776\n",
            "Time taken for training worker 5: 0:00:11.787366\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 2.014793526, Training Accuracy: 45.840\n",
            "Worker 6, [02/04]: Training Loss: 1.882593261, Training Accuracy: 48.560\n",
            "Worker 6, [03/04]: Training Loss: 1.761398731, Training Accuracy: 50.976\n",
            "Worker 6, [04/04]: Training Loss: 1.697950793, Training Accuracy: 51.984\n",
            "Time taken for training worker 6: 0:00:11.712687\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 2.018453995, Training Accuracy: 45.696\n",
            "Worker 7, [02/04]: Training Loss: 1.879396913, Training Accuracy: 48.240\n",
            "Worker 7, [03/04]: Training Loss: 1.777945050, Training Accuracy: 51.696\n",
            "Worker 7, [04/04]: Training Loss: 1.682459785, Training Accuracy: 54.320\n",
            "Time taken for training worker 7: 0:00:12.319462\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 2.009202580, Training Accuracy: 46.256\n",
            "Worker 8, [02/04]: Training Loss: 1.862254288, Training Accuracy: 49.120\n",
            "Worker 8, [03/04]: Training Loss: 1.770099140, Training Accuracy: 50.864\n",
            "Worker 8, [04/04]: Training Loss: 1.657205297, Training Accuracy: 53.568\n",
            "Time taken for training worker 8: 0:00:11.692770\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004401\n",
            "Global Update 18: Test Loss: 2.035798746, Test Accuracy: 46.560\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.003167081, Training Accuracy: 46.816\n",
            "Worker 1, [02/04]: Training Loss: 1.872736016, Training Accuracy: 48.848\n",
            "Worker 1, [03/04]: Training Loss: 1.733504837, Training Accuracy: 51.984\n",
            "Worker 1, [04/04]: Training Loss: 1.678342861, Training Accuracy: 53.024\n",
            "Time taken for training worker 1: 0:00:12.732463\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.977158382, Training Accuracy: 46.192\n",
            "Worker 2, [02/04]: Training Loss: 1.816645158, Training Accuracy: 49.776\n",
            "Worker 2, [03/04]: Training Loss: 1.682735040, Training Accuracy: 52.512\n",
            "Worker 2, [04/04]: Training Loss: 1.594375344, Training Accuracy: 55.120\n",
            "Time taken for training worker 2: 0:00:12.103289\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.995509455, Training Accuracy: 46.640\n",
            "Worker 3, [02/04]: Training Loss: 1.848991685, Training Accuracy: 49.104\n",
            "Worker 3, [03/04]: Training Loss: 1.724376031, Training Accuracy: 51.904\n",
            "Worker 3, [04/04]: Training Loss: 1.634116715, Training Accuracy: 54.784\n",
            "Time taken for training worker 3: 0:00:12.257699\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.988798656, Training Accuracy: 46.416\n",
            "Worker 4, [02/04]: Training Loss: 1.842403301, Training Accuracy: 50.224\n",
            "Worker 4, [03/04]: Training Loss: 1.757331144, Training Accuracy: 51.520\n",
            "Worker 4, [04/04]: Training Loss: 1.632324553, Training Accuracy: 53.920\n",
            "Time taken for training worker 4: 0:00:12.716529\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.990665126, Training Accuracy: 46.000\n",
            "Worker 5, [02/04]: Training Loss: 1.828772594, Training Accuracy: 49.600\n",
            "Worker 5, [03/04]: Training Loss: 1.677400324, Training Accuracy: 53.504\n",
            "Worker 5, [04/04]: Training Loss: 1.593208568, Training Accuracy: 54.960\n",
            "Time taken for training worker 5: 0:00:12.843143\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.969104299, Training Accuracy: 46.624\n",
            "Worker 6, [02/04]: Training Loss: 1.802743594, Training Accuracy: 50.320\n",
            "Worker 6, [03/04]: Training Loss: 1.712081881, Training Accuracy: 52.080\n",
            "Worker 6, [04/04]: Training Loss: 1.607746279, Training Accuracy: 55.168\n",
            "Time taken for training worker 6: 0:00:12.906549\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.984528139, Training Accuracy: 46.416\n",
            "Worker 7, [02/04]: Training Loss: 1.837736972, Training Accuracy: 49.536\n",
            "Worker 7, [03/04]: Training Loss: 1.710129212, Training Accuracy: 52.576\n",
            "Worker 7, [04/04]: Training Loss: 1.630070005, Training Accuracy: 54.496\n",
            "Time taken for training worker 7: 0:00:12.488030\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.970507782, Training Accuracy: 46.368\n",
            "Worker 8, [02/04]: Training Loss: 1.814814892, Training Accuracy: 50.176\n",
            "Worker 8, [03/04]: Training Loss: 1.699329876, Training Accuracy: 52.624\n",
            "Worker 8, [04/04]: Training Loss: 1.607654770, Training Accuracy: 54.320\n",
            "Time taken for training worker 8: 0:00:11.911547\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004500\n",
            "Global Update 19: Test Loss: 2.028806653, Test Accuracy: 46.960\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.958272197, Training Accuracy: 47.856\n",
            "Worker 1, [02/04]: Training Loss: 1.792184942, Training Accuracy: 50.864\n",
            "Worker 1, [03/04]: Training Loss: 1.673608944, Training Accuracy: 53.536\n",
            "Worker 1, [04/04]: Training Loss: 1.598600336, Training Accuracy: 54.832\n",
            "Time taken for training worker 1: 0:00:12.296412\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.912472258, Training Accuracy: 47.888\n",
            "Worker 2, [02/04]: Training Loss: 1.759804980, Training Accuracy: 51.360\n",
            "Worker 2, [03/04]: Training Loss: 1.633093143, Training Accuracy: 53.648\n",
            "Worker 2, [04/04]: Training Loss: 1.551074040, Training Accuracy: 55.984\n",
            "Time taken for training worker 2: 0:00:12.210382\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.940555111, Training Accuracy: 48.400\n",
            "Worker 3, [02/04]: Training Loss: 1.777083666, Training Accuracy: 51.696\n",
            "Worker 3, [03/04]: Training Loss: 1.658749676, Training Accuracy: 54.496\n",
            "Worker 3, [04/04]: Training Loss: 1.572866411, Training Accuracy: 56.016\n",
            "Time taken for training worker 3: 0:00:12.206833\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.946682999, Training Accuracy: 47.904\n",
            "Worker 4, [02/04]: Training Loss: 1.801616939, Training Accuracy: 50.128\n",
            "Worker 4, [03/04]: Training Loss: 1.693382379, Training Accuracy: 52.848\n",
            "Worker 4, [04/04]: Training Loss: 1.589514771, Training Accuracy: 55.616\n",
            "Time taken for training worker 4: 0:00:12.176282\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.935330590, Training Accuracy: 46.912\n",
            "Worker 5, [02/04]: Training Loss: 1.741773356, Training Accuracy: 52.144\n",
            "Worker 5, [03/04]: Training Loss: 1.668836533, Training Accuracy: 53.648\n",
            "Worker 5, [04/04]: Training Loss: 1.563771102, Training Accuracy: 56.256\n",
            "Time taken for training worker 5: 0:00:11.830296\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.907211680, Training Accuracy: 47.600\n",
            "Worker 6, [02/04]: Training Loss: 1.755183321, Training Accuracy: 51.184\n",
            "Worker 6, [03/04]: Training Loss: 1.646128085, Training Accuracy: 53.728\n",
            "Worker 6, [04/04]: Training Loss: 1.543797903, Training Accuracy: 56.432\n",
            "Time taken for training worker 6: 0:00:12.325995\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.947118856, Training Accuracy: 47.568\n",
            "Worker 7, [02/04]: Training Loss: 1.747332803, Training Accuracy: 51.728\n",
            "Worker 7, [03/04]: Training Loss: 1.660571078, Training Accuracy: 53.392\n",
            "Worker 7, [04/04]: Training Loss: 1.548294088, Training Accuracy: 55.952\n",
            "Time taken for training worker 7: 0:00:12.326569\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.911004433, Training Accuracy: 47.600\n",
            "Worker 8, [02/04]: Training Loss: 1.742421920, Training Accuracy: 51.536\n",
            "Worker 8, [03/04]: Training Loss: 1.626253964, Training Accuracy: 54.032\n",
            "Worker 8, [04/04]: Training Loss: 1.551967852, Training Accuracy: 56.192\n",
            "Time taken for training worker 8: 0:00:11.751809\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004667\n",
            "Global Update 20: Test Loss: 2.001934666, Test Accuracy: 47.930\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.912647548, Training Accuracy: 47.408\n",
            "Worker 1, [02/04]: Training Loss: 1.730562135, Training Accuracy: 52.672\n",
            "Worker 1, [03/04]: Training Loss: 1.614203578, Training Accuracy: 55.072\n",
            "Worker 1, [04/04]: Training Loss: 1.545790096, Training Accuracy: 56.816\n",
            "Time taken for training worker 1: 0:00:11.694531\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.866032709, Training Accuracy: 49.440\n",
            "Worker 2, [02/04]: Training Loss: 1.680586294, Training Accuracy: 53.344\n",
            "Worker 2, [03/04]: Training Loss: 1.587509844, Training Accuracy: 55.584\n",
            "Worker 2, [04/04]: Training Loss: 1.468244258, Training Accuracy: 57.744\n",
            "Time taken for training worker 2: 0:00:12.308293\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.894476021, Training Accuracy: 48.576\n",
            "Worker 3, [02/04]: Training Loss: 1.734119053, Training Accuracy: 52.304\n",
            "Worker 3, [03/04]: Training Loss: 1.614843691, Training Accuracy: 54.816\n",
            "Worker 3, [04/04]: Training Loss: 1.545035287, Training Accuracy: 56.656\n",
            "Time taken for training worker 3: 0:00:11.915092\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.896133633, Training Accuracy: 48.480\n",
            "Worker 4, [02/04]: Training Loss: 1.726372766, Training Accuracy: 52.768\n",
            "Worker 4, [03/04]: Training Loss: 1.635301642, Training Accuracy: 54.688\n",
            "Worker 4, [04/04]: Training Loss: 1.505622845, Training Accuracy: 58.272\n",
            "Time taken for training worker 4: 0:00:11.680862\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.886642423, Training Accuracy: 48.176\n",
            "Worker 5, [02/04]: Training Loss: 1.730030335, Training Accuracy: 52.496\n",
            "Worker 5, [03/04]: Training Loss: 1.613864533, Training Accuracy: 54.608\n",
            "Worker 5, [04/04]: Training Loss: 1.496892366, Training Accuracy: 58.080\n",
            "Time taken for training worker 5: 0:00:11.628195\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.876513527, Training Accuracy: 48.576\n",
            "Worker 6, [02/04]: Training Loss: 1.677806100, Training Accuracy: 53.792\n",
            "Worker 6, [03/04]: Training Loss: 1.590532466, Training Accuracy: 55.536\n",
            "Worker 6, [04/04]: Training Loss: 1.479379903, Training Accuracy: 57.712\n",
            "Time taken for training worker 6: 0:00:11.758694\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.885831878, Training Accuracy: 48.064\n",
            "Worker 7, [02/04]: Training Loss: 1.688110920, Training Accuracy: 52.784\n",
            "Worker 7, [03/04]: Training Loss: 1.588729158, Training Accuracy: 56.144\n",
            "Worker 7, [04/04]: Training Loss: 1.520267822, Training Accuracy: 58.080\n",
            "Time taken for training worker 7: 0:00:12.309197\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.852569108, Training Accuracy: 49.520\n",
            "Worker 8, [02/04]: Training Loss: 1.684628522, Training Accuracy: 53.136\n",
            "Worker 8, [03/04]: Training Loss: 1.576210890, Training Accuracy: 55.632\n",
            "Worker 8, [04/04]: Training Loss: 1.502376226, Training Accuracy: 57.360\n",
            "Time taken for training worker 8: 0:00:12.804198\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004197\n",
            "Global Update 21: Test Loss: 1.992196069, Test Accuracy: 48.100\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.869615813, Training Accuracy: 49.120\n",
            "Worker 1, [02/04]: Training Loss: 1.690177835, Training Accuracy: 53.728\n",
            "Worker 1, [03/04]: Training Loss: 1.583274401, Training Accuracy: 56.000\n",
            "Worker 1, [04/04]: Training Loss: 1.498414907, Training Accuracy: 58.224\n",
            "Time taken for training worker 1: 0:00:12.557958\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.812411896, Training Accuracy: 50.032\n",
            "Worker 2, [02/04]: Training Loss: 1.634059211, Training Accuracy: 54.656\n",
            "Worker 2, [03/04]: Training Loss: 1.548240359, Training Accuracy: 56.432\n",
            "Worker 2, [04/04]: Training Loss: 1.452019178, Training Accuracy: 58.160\n",
            "Time taken for training worker 2: 0:00:12.439117\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.840233358, Training Accuracy: 49.936\n",
            "Worker 3, [02/04]: Training Loss: 1.691312502, Training Accuracy: 53.296\n",
            "Worker 3, [03/04]: Training Loss: 1.569047584, Training Accuracy: 55.456\n",
            "Worker 3, [04/04]: Training Loss: 1.482575971, Training Accuracy: 57.888\n",
            "Time taken for training worker 3: 0:00:11.698437\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.852123260, Training Accuracy: 49.776\n",
            "Worker 4, [02/04]: Training Loss: 1.689326194, Training Accuracy: 53.520\n",
            "Worker 4, [03/04]: Training Loss: 1.574102349, Training Accuracy: 55.536\n",
            "Worker 4, [04/04]: Training Loss: 1.483675352, Training Accuracy: 58.592\n",
            "Time taken for training worker 4: 0:00:11.807285\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.838058422, Training Accuracy: 49.456\n",
            "Worker 5, [02/04]: Training Loss: 1.644163702, Training Accuracy: 55.232\n",
            "Worker 5, [03/04]: Training Loss: 1.540482017, Training Accuracy: 56.864\n",
            "Worker 5, [04/04]: Training Loss: 1.460658652, Training Accuracy: 58.832\n",
            "Time taken for training worker 5: 0:00:12.011197\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.815432631, Training Accuracy: 49.520\n",
            "Worker 6, [02/04]: Training Loss: 1.648081671, Training Accuracy: 54.688\n",
            "Worker 6, [03/04]: Training Loss: 1.538814469, Training Accuracy: 56.320\n",
            "Worker 6, [04/04]: Training Loss: 1.446165076, Training Accuracy: 58.736\n",
            "Time taken for training worker 6: 0:00:12.133582\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.826510949, Training Accuracy: 50.624\n",
            "Worker 7, [02/04]: Training Loss: 1.682339594, Training Accuracy: 53.328\n",
            "Worker 7, [03/04]: Training Loss: 1.552706626, Training Accuracy: 56.896\n",
            "Worker 7, [04/04]: Training Loss: 1.460337503, Training Accuracy: 58.720\n",
            "Time taken for training worker 7: 0:00:12.764510\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.832959011, Training Accuracy: 48.656\n",
            "Worker 8, [02/04]: Training Loss: 1.641976035, Training Accuracy: 54.016\n",
            "Worker 8, [03/04]: Training Loss: 1.533648041, Training Accuracy: 56.944\n",
            "Worker 8, [04/04]: Training Loss: 1.469495132, Training Accuracy: 58.336\n",
            "Time taken for training worker 8: 0:00:11.995505\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004544\n",
            "Global Update 22: Test Loss: 1.980519904, Test Accuracy: 48.050\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.818619903, Training Accuracy: 49.584\n",
            "Worker 1, [02/04]: Training Loss: 1.652058626, Training Accuracy: 53.792\n",
            "Worker 1, [03/04]: Training Loss: 1.522608780, Training Accuracy: 57.312\n",
            "Worker 1, [04/04]: Training Loss: 1.443396634, Training Accuracy: 59.008\n",
            "Time taken for training worker 1: 0:00:12.099591\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.744511432, Training Accuracy: 52.032\n",
            "Worker 2, [02/04]: Training Loss: 1.594723964, Training Accuracy: 55.568\n",
            "Worker 2, [03/04]: Training Loss: 1.475353787, Training Accuracy: 58.160\n",
            "Worker 2, [04/04]: Training Loss: 1.384488726, Training Accuracy: 60.368\n",
            "Time taken for training worker 2: 0:00:12.460325\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.787670266, Training Accuracy: 51.840\n",
            "Worker 3, [02/04]: Training Loss: 1.626203927, Training Accuracy: 54.528\n",
            "Worker 3, [03/04]: Training Loss: 1.502171772, Training Accuracy: 58.048\n",
            "Worker 3, [04/04]: Training Loss: 1.433701193, Training Accuracy: 59.680\n",
            "Time taken for training worker 3: 0:00:12.820202\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.786411709, Training Accuracy: 51.200\n",
            "Worker 4, [02/04]: Training Loss: 1.646958157, Training Accuracy: 54.224\n",
            "Worker 4, [03/04]: Training Loss: 1.509143134, Training Accuracy: 58.112\n",
            "Worker 4, [04/04]: Training Loss: 1.430487793, Training Accuracy: 59.360\n",
            "Time taken for training worker 4: 0:00:12.265256\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.789967901, Training Accuracy: 50.480\n",
            "Worker 5, [02/04]: Training Loss: 1.624532623, Training Accuracy: 54.432\n",
            "Worker 5, [03/04]: Training Loss: 1.508270609, Training Accuracy: 57.456\n",
            "Worker 5, [04/04]: Training Loss: 1.438047255, Training Accuracy: 59.952\n",
            "Time taken for training worker 5: 0:00:12.018819\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.745001668, Training Accuracy: 51.056\n",
            "Worker 6, [02/04]: Training Loss: 1.585121486, Training Accuracy: 56.032\n",
            "Worker 6, [03/04]: Training Loss: 1.478903401, Training Accuracy: 57.200\n",
            "Worker 6, [04/04]: Training Loss: 1.390407884, Training Accuracy: 60.128\n",
            "Time taken for training worker 6: 0:00:12.271753\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.781937375, Training Accuracy: 50.896\n",
            "Worker 7, [02/04]: Training Loss: 1.620694064, Training Accuracy: 55.264\n",
            "Worker 7, [03/04]: Training Loss: 1.499396727, Training Accuracy: 58.512\n",
            "Worker 7, [04/04]: Training Loss: 1.433334359, Training Accuracy: 59.680\n",
            "Time taken for training worker 7: 0:00:11.840659\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.756204355, Training Accuracy: 51.120\n",
            "Worker 8, [02/04]: Training Loss: 1.599579430, Training Accuracy: 55.408\n",
            "Worker 8, [03/04]: Training Loss: 1.496396010, Training Accuracy: 56.976\n",
            "Worker 8, [04/04]: Training Loss: 1.414734709, Training Accuracy: 59.568\n",
            "Time taken for training worker 8: 0:00:13.123310\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004656\n",
            "Global Update 23: Test Loss: 1.971892863, Test Accuracy: 48.670\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.762886600, Training Accuracy: 50.960\n",
            "Worker 1, [02/04]: Training Loss: 1.592560837, Training Accuracy: 55.776\n",
            "Worker 1, [03/04]: Training Loss: 1.484109836, Training Accuracy: 58.336\n",
            "Worker 1, [04/04]: Training Loss: 1.388117254, Training Accuracy: 60.672\n",
            "Time taken for training worker 1: 0:00:12.002076\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.724750278, Training Accuracy: 52.048\n",
            "Worker 2, [02/04]: Training Loss: 1.519336705, Training Accuracy: 57.344\n",
            "Worker 2, [03/04]: Training Loss: 1.434166953, Training Accuracy: 58.704\n",
            "Worker 2, [04/04]: Training Loss: 1.347145633, Training Accuracy: 61.680\n",
            "Time taken for training worker 2: 0:00:11.986229\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.724418774, Training Accuracy: 52.352\n",
            "Worker 3, [02/04]: Training Loss: 1.573024778, Training Accuracy: 55.792\n",
            "Worker 3, [03/04]: Training Loss: 1.479622574, Training Accuracy: 58.576\n",
            "Worker 3, [04/04]: Training Loss: 1.390624551, Training Accuracy: 60.544\n",
            "Time taken for training worker 3: 0:00:11.949256\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.755590449, Training Accuracy: 52.528\n",
            "Worker 4, [02/04]: Training Loss: 1.595966546, Training Accuracy: 55.504\n",
            "Worker 4, [03/04]: Training Loss: 1.490359232, Training Accuracy: 57.984\n",
            "Worker 4, [04/04]: Training Loss: 1.395238078, Training Accuracy: 60.976\n",
            "Time taken for training worker 4: 0:00:12.116554\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.747691305, Training Accuracy: 51.840\n",
            "Worker 5, [02/04]: Training Loss: 1.589328839, Training Accuracy: 54.512\n",
            "Worker 5, [03/04]: Training Loss: 1.463480820, Training Accuracy: 59.136\n",
            "Worker 5, [04/04]: Training Loss: 1.387419438, Training Accuracy: 60.592\n",
            "Time taken for training worker 5: 0:00:12.593276\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.699851766, Training Accuracy: 52.288\n",
            "Worker 6, [02/04]: Training Loss: 1.550991721, Training Accuracy: 56.448\n",
            "Worker 6, [03/04]: Training Loss: 1.427619682, Training Accuracy: 58.784\n",
            "Worker 6, [04/04]: Training Loss: 1.364579309, Training Accuracy: 61.520\n",
            "Time taken for training worker 6: 0:00:12.718806\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.734468713, Training Accuracy: 52.320\n",
            "Worker 7, [02/04]: Training Loss: 1.548806097, Training Accuracy: 57.024\n",
            "Worker 7, [03/04]: Training Loss: 1.471228879, Training Accuracy: 58.480\n",
            "Worker 7, [04/04]: Training Loss: 1.350928468, Training Accuracy: 61.600\n",
            "Time taken for training worker 7: 0:00:12.641649\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.709525669, Training Accuracy: 52.624\n",
            "Worker 8, [02/04]: Training Loss: 1.553935752, Training Accuracy: 56.192\n",
            "Worker 8, [03/04]: Training Loss: 1.424913250, Training Accuracy: 59.168\n",
            "Worker 8, [04/04]: Training Loss: 1.341904210, Training Accuracy: 61.344\n",
            "Time taken for training worker 8: 0:00:12.293737\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004592\n",
            "Global Update 24: Test Loss: 1.972783587, Test Accuracy: 48.810\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.719014124, Training Accuracy: 52.192\n",
            "Worker 1, [02/04]: Training Loss: 1.543026748, Training Accuracy: 56.784\n",
            "Worker 1, [03/04]: Training Loss: 1.461231628, Training Accuracy: 58.352\n",
            "Worker 1, [04/04]: Training Loss: 1.376497284, Training Accuracy: 60.960\n",
            "Time taken for training worker 1: 0:00:12.191041\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.667706311, Training Accuracy: 53.328\n",
            "Worker 2, [02/04]: Training Loss: 1.492742103, Training Accuracy: 58.336\n",
            "Worker 2, [03/04]: Training Loss: 1.407618618, Training Accuracy: 59.568\n",
            "Worker 2, [04/04]: Training Loss: 1.312208233, Training Accuracy: 62.864\n",
            "Time taken for training worker 2: 0:00:11.656802\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.698237380, Training Accuracy: 53.808\n",
            "Worker 3, [02/04]: Training Loss: 1.531185727, Training Accuracy: 57.376\n",
            "Worker 3, [03/04]: Training Loss: 1.431201205, Training Accuracy: 59.680\n",
            "Worker 3, [04/04]: Training Loss: 1.348906132, Training Accuracy: 61.248\n",
            "Time taken for training worker 3: 0:00:11.600058\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.701102743, Training Accuracy: 52.880\n",
            "Worker 4, [02/04]: Training Loss: 1.535966715, Training Accuracy: 57.776\n",
            "Worker 4, [03/04]: Training Loss: 1.440583773, Training Accuracy: 59.008\n",
            "Worker 4, [04/04]: Training Loss: 1.345312515, Training Accuracy: 61.920\n",
            "Time taken for training worker 4: 0:00:12.436026\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.701724400, Training Accuracy: 52.832\n",
            "Worker 5, [02/04]: Training Loss: 1.536373164, Training Accuracy: 57.312\n",
            "Worker 5, [03/04]: Training Loss: 1.416456209, Training Accuracy: 59.792\n",
            "Worker 5, [04/04]: Training Loss: 1.341571859, Training Accuracy: 62.240\n",
            "Time taken for training worker 5: 0:00:12.073379\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.666222068, Training Accuracy: 53.328\n",
            "Worker 6, [02/04]: Training Loss: 1.495017858, Training Accuracy: 58.176\n",
            "Worker 6, [03/04]: Training Loss: 1.387922763, Training Accuracy: 60.528\n",
            "Worker 6, [04/04]: Training Loss: 1.328145105, Training Accuracy: 62.224\n",
            "Time taken for training worker 6: 0:00:11.747466\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.714321477, Training Accuracy: 53.200\n",
            "Worker 7, [02/04]: Training Loss: 1.544013400, Training Accuracy: 57.184\n",
            "Worker 7, [03/04]: Training Loss: 1.426269907, Training Accuracy: 59.968\n",
            "Worker 7, [04/04]: Training Loss: 1.341962939, Training Accuracy: 61.744\n",
            "Time taken for training worker 7: 0:00:12.579205\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.681646370, Training Accuracy: 54.368\n",
            "Worker 8, [02/04]: Training Loss: 1.500819361, Training Accuracy: 58.192\n",
            "Worker 8, [03/04]: Training Loss: 1.399714913, Training Accuracy: 60.096\n",
            "Worker 8, [04/04]: Training Loss: 1.312466947, Training Accuracy: 62.016\n",
            "Time taken for training worker 8: 0:00:12.183428\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004309\n",
            "Global Update 25: Test Loss: 1.971139829, Test Accuracy: 49.310\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.707474050, Training Accuracy: 52.608\n",
            "Worker 1, [02/04]: Training Loss: 1.514475804, Training Accuracy: 56.944\n",
            "Worker 1, [03/04]: Training Loss: 1.416619306, Training Accuracy: 59.616\n",
            "Worker 1, [04/04]: Training Loss: 1.335885637, Training Accuracy: 62.384\n",
            "Time taken for training worker 1: 0:00:12.257451\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.628480715, Training Accuracy: 53.840\n",
            "Worker 2, [02/04]: Training Loss: 1.477649870, Training Accuracy: 57.296\n",
            "Worker 2, [03/04]: Training Loss: 1.375724162, Training Accuracy: 60.960\n",
            "Worker 2, [04/04]: Training Loss: 1.263290826, Training Accuracy: 63.824\n",
            "Time taken for training worker 2: 0:00:12.217877\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.663344431, Training Accuracy: 54.016\n",
            "Worker 3, [02/04]: Training Loss: 1.467455671, Training Accuracy: 59.056\n",
            "Worker 3, [03/04]: Training Loss: 1.385110715, Training Accuracy: 61.104\n",
            "Worker 3, [04/04]: Training Loss: 1.296806607, Training Accuracy: 63.328\n",
            "Time taken for training worker 3: 0:00:11.827680\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.677459800, Training Accuracy: 53.984\n",
            "Worker 4, [02/04]: Training Loss: 1.526377953, Training Accuracy: 57.424\n",
            "Worker 4, [03/04]: Training Loss: 1.404652077, Training Accuracy: 60.896\n",
            "Worker 4, [04/04]: Training Loss: 1.308466814, Training Accuracy: 63.392\n",
            "Time taken for training worker 4: 0:00:12.156196\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.668058111, Training Accuracy: 53.568\n",
            "Worker 5, [02/04]: Training Loss: 1.484954024, Training Accuracy: 57.744\n",
            "Worker 5, [03/04]: Training Loss: 1.371207483, Training Accuracy: 61.344\n",
            "Worker 5, [04/04]: Training Loss: 1.288660567, Training Accuracy: 63.920\n",
            "Time taken for training worker 5: 0:00:11.936981\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.625072766, Training Accuracy: 54.048\n",
            "Worker 6, [02/04]: Training Loss: 1.478426571, Training Accuracy: 58.464\n",
            "Worker 6, [03/04]: Training Loss: 1.379365235, Training Accuracy: 60.944\n",
            "Worker 6, [04/04]: Training Loss: 1.284726442, Training Accuracy: 63.264\n",
            "Time taken for training worker 6: 0:00:12.029174\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.648132990, Training Accuracy: 53.280\n",
            "Worker 7, [02/04]: Training Loss: 1.485996072, Training Accuracy: 58.672\n",
            "Worker 7, [03/04]: Training Loss: 1.393373087, Training Accuracy: 60.128\n",
            "Worker 7, [04/04]: Training Loss: 1.313224306, Training Accuracy: 62.480\n",
            "Time taken for training worker 7: 0:00:12.540416\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.646382803, Training Accuracy: 54.160\n",
            "Worker 8, [02/04]: Training Loss: 1.484467477, Training Accuracy: 58.432\n",
            "Worker 8, [03/04]: Training Loss: 1.375918702, Training Accuracy: 60.784\n",
            "Worker 8, [04/04]: Training Loss: 1.279984795, Training Accuracy: 63.888\n",
            "Time taken for training worker 8: 0:00:12.785475\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004593\n",
            "Global Update 26: Test Loss: 1.967986274, Test Accuracy: 49.480\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.662002601, Training Accuracy: 53.424\n",
            "Worker 1, [02/04]: Training Loss: 1.471923919, Training Accuracy: 58.992\n",
            "Worker 1, [03/04]: Training Loss: 1.366350985, Training Accuracy: 61.360\n",
            "Worker 1, [04/04]: Training Loss: 1.295264543, Training Accuracy: 63.216\n",
            "Time taken for training worker 1: 0:00:12.548158\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.592892397, Training Accuracy: 56.048\n",
            "Worker 2, [02/04]: Training Loss: 1.424783791, Training Accuracy: 59.552\n",
            "Worker 2, [03/04]: Training Loss: 1.337902615, Training Accuracy: 61.664\n",
            "Worker 2, [04/04]: Training Loss: 1.264164667, Training Accuracy: 63.712\n",
            "Time taken for training worker 2: 0:00:11.842350\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.598164038, Training Accuracy: 55.392\n",
            "Worker 3, [02/04]: Training Loss: 1.466967897, Training Accuracy: 58.928\n",
            "Worker 3, [03/04]: Training Loss: 1.358200986, Training Accuracy: 61.968\n",
            "Worker 3, [04/04]: Training Loss: 1.254377172, Training Accuracy: 63.824\n",
            "Time taken for training worker 3: 0:00:12.726246\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.620111057, Training Accuracy: 54.576\n",
            "Worker 4, [02/04]: Training Loss: 1.470920811, Training Accuracy: 58.192\n",
            "Worker 4, [03/04]: Training Loss: 1.378010075, Training Accuracy: 61.552\n",
            "Worker 4, [04/04]: Training Loss: 1.281272233, Training Accuracy: 64.176\n",
            "Time taken for training worker 4: 0:00:11.976001\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.621972369, Training Accuracy: 55.376\n",
            "Worker 5, [02/04]: Training Loss: 1.435600744, Training Accuracy: 59.856\n",
            "Worker 5, [03/04]: Training Loss: 1.352578961, Training Accuracy: 62.256\n",
            "Worker 5, [04/04]: Training Loss: 1.270935861, Training Accuracy: 63.312\n",
            "Time taken for training worker 5: 0:00:12.269473\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.600789180, Training Accuracy: 55.584\n",
            "Worker 6, [02/04]: Training Loss: 1.446713847, Training Accuracy: 58.656\n",
            "Worker 6, [03/04]: Training Loss: 1.347791489, Training Accuracy: 61.600\n",
            "Worker 6, [04/04]: Training Loss: 1.254402855, Training Accuracy: 64.320\n",
            "Time taken for training worker 6: 0:00:12.472844\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.609461095, Training Accuracy: 55.488\n",
            "Worker 7, [02/04]: Training Loss: 1.445186917, Training Accuracy: 59.328\n",
            "Worker 7, [03/04]: Training Loss: 1.347985771, Training Accuracy: 62.176\n",
            "Worker 7, [04/04]: Training Loss: 1.281767898, Training Accuracy: 63.248\n",
            "Time taken for training worker 7: 0:00:12.038052\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.614187203, Training Accuracy: 54.768\n",
            "Worker 8, [02/04]: Training Loss: 1.426891425, Training Accuracy: 60.256\n",
            "Worker 8, [03/04]: Training Loss: 1.334102765, Training Accuracy: 61.760\n",
            "Worker 8, [04/04]: Training Loss: 1.270499792, Training Accuracy: 64.000\n",
            "Time taken for training worker 8: 0:00:12.256198\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004177\n",
            "Global Update 27: Test Loss: 1.964266362, Test Accuracy: 49.970\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.593119477, Training Accuracy: 55.632\n",
            "Worker 1, [02/04]: Training Loss: 1.456769915, Training Accuracy: 59.184\n",
            "Worker 1, [03/04]: Training Loss: 1.346130930, Training Accuracy: 62.176\n",
            "Worker 1, [04/04]: Training Loss: 1.285972050, Training Accuracy: 63.568\n",
            "Time taken for training worker 1: 0:00:12.223498\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.560558905, Training Accuracy: 55.760\n",
            "Worker 2, [02/04]: Training Loss: 1.434559092, Training Accuracy: 59.376\n",
            "Worker 2, [03/04]: Training Loss: 1.304377875, Training Accuracy: 63.008\n",
            "Worker 2, [04/04]: Training Loss: 1.217815453, Training Accuracy: 64.768\n",
            "Time taken for training worker 2: 0:00:12.643088\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.560240680, Training Accuracy: 56.448\n",
            "Worker 3, [02/04]: Training Loss: 1.421750885, Training Accuracy: 60.752\n",
            "Worker 3, [03/04]: Training Loss: 1.349228883, Training Accuracy: 62.576\n",
            "Worker 3, [04/04]: Training Loss: 1.258590816, Training Accuracy: 64.528\n",
            "Time taken for training worker 3: 0:00:12.125489\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.593856183, Training Accuracy: 55.936\n",
            "Worker 4, [02/04]: Training Loss: 1.436829166, Training Accuracy: 59.888\n",
            "Worker 4, [03/04]: Training Loss: 1.362255733, Training Accuracy: 62.496\n",
            "Worker 4, [04/04]: Training Loss: 1.280176551, Training Accuracy: 64.032\n",
            "Time taken for training worker 4: 0:00:12.187194\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.565460199, Training Accuracy: 56.368\n",
            "Worker 5, [02/04]: Training Loss: 1.439939500, Training Accuracy: 59.520\n",
            "Worker 5, [03/04]: Training Loss: 1.333477570, Training Accuracy: 62.320\n",
            "Worker 5, [04/04]: Training Loss: 1.258868991, Training Accuracy: 64.480\n",
            "Time taken for training worker 5: 0:00:12.132763\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.552337255, Training Accuracy: 56.528\n",
            "Worker 6, [02/04]: Training Loss: 1.399094398, Training Accuracy: 60.240\n",
            "Worker 6, [03/04]: Training Loss: 1.295374847, Training Accuracy: 63.968\n",
            "Worker 6, [04/04]: Training Loss: 1.240588163, Training Accuracy: 64.864\n",
            "Time taken for training worker 6: 0:00:12.282619\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.578782827, Training Accuracy: 55.376\n",
            "Worker 7, [02/04]: Training Loss: 1.435821600, Training Accuracy: 59.904\n",
            "Worker 7, [03/04]: Training Loss: 1.335868774, Training Accuracy: 62.304\n",
            "Worker 7, [04/04]: Training Loss: 1.258322080, Training Accuracy: 64.576\n",
            "Time taken for training worker 7: 0:00:11.962746\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.532977603, Training Accuracy: 56.704\n",
            "Worker 8, [02/04]: Training Loss: 1.406981106, Training Accuracy: 60.000\n",
            "Worker 8, [03/04]: Training Loss: 1.295100676, Training Accuracy: 62.480\n",
            "Worker 8, [04/04]: Training Loss: 1.234005314, Training Accuracy: 64.368\n",
            "Time taken for training worker 8: 0:00:11.708451\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004154\n",
            "Global Update 28: Test Loss: 1.967371711, Test Accuracy: 49.990\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.571570709, Training Accuracy: 55.808\n",
            "Worker 1, [02/04]: Training Loss: 1.420235003, Training Accuracy: 59.312\n",
            "Worker 1, [03/04]: Training Loss: 1.331565096, Training Accuracy: 62.256\n",
            "Worker 1, [04/04]: Training Loss: 1.265604363, Training Accuracy: 63.280\n",
            "Time taken for training worker 1: 0:00:13.005803\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.498783480, Training Accuracy: 58.048\n",
            "Worker 2, [02/04]: Training Loss: 1.393316590, Training Accuracy: 60.192\n",
            "Worker 2, [03/04]: Training Loss: 1.267531882, Training Accuracy: 63.520\n",
            "Worker 2, [04/04]: Training Loss: 1.231066274, Training Accuracy: 64.400\n",
            "Time taken for training worker 2: 0:00:12.186945\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.546960070, Training Accuracy: 57.280\n",
            "Worker 3, [02/04]: Training Loss: 1.388146312, Training Accuracy: 60.672\n",
            "Worker 3, [03/04]: Training Loss: 1.297998847, Training Accuracy: 63.296\n",
            "Worker 3, [04/04]: Training Loss: 1.233931182, Training Accuracy: 65.040\n",
            "Time taken for training worker 3: 0:00:11.901212\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.564019134, Training Accuracy: 55.920\n",
            "Worker 4, [02/04]: Training Loss: 1.397945263, Training Accuracy: 60.304\n",
            "Worker 4, [03/04]: Training Loss: 1.317891111, Training Accuracy: 63.696\n",
            "Worker 4, [04/04]: Training Loss: 1.256683874, Training Accuracy: 64.784\n",
            "Time taken for training worker 4: 0:00:12.523048\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.527026000, Training Accuracy: 57.408\n",
            "Worker 5, [02/04]: Training Loss: 1.390963048, Training Accuracy: 60.320\n",
            "Worker 5, [03/04]: Training Loss: 1.305583893, Training Accuracy: 63.408\n",
            "Worker 5, [04/04]: Training Loss: 1.244398893, Training Accuracy: 65.104\n",
            "Time taken for training worker 5: 0:00:12.484234\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.510190354, Training Accuracy: 57.856\n",
            "Worker 6, [02/04]: Training Loss: 1.365495905, Training Accuracy: 61.040\n",
            "Worker 6, [03/04]: Training Loss: 1.289322145, Training Accuracy: 63.232\n",
            "Worker 6, [04/04]: Training Loss: 1.201317573, Training Accuracy: 65.376\n",
            "Time taken for training worker 6: 0:00:12.762900\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.524651875, Training Accuracy: 57.312\n",
            "Worker 7, [02/04]: Training Loss: 1.388921711, Training Accuracy: 60.992\n",
            "Worker 7, [03/04]: Training Loss: 1.315854220, Training Accuracy: 62.880\n",
            "Worker 7, [04/04]: Training Loss: 1.235157206, Training Accuracy: 65.184\n",
            "Time taken for training worker 7: 0:00:12.331211\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.513902841, Training Accuracy: 57.344\n",
            "Worker 8, [02/04]: Training Loss: 1.361097162, Training Accuracy: 60.864\n",
            "Worker 8, [03/04]: Training Loss: 1.295610919, Training Accuracy: 62.896\n",
            "Worker 8, [04/04]: Training Loss: 1.212691555, Training Accuracy: 65.008\n",
            "Time taken for training worker 8: 0:00:12.790099\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004011\n",
            "Global Update 29: Test Loss: 1.967178011, Test Accuracy: 50.350\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.529275117, Training Accuracy: 56.976\n",
            "Worker 1, [02/04]: Training Loss: 1.402830999, Training Accuracy: 60.848\n",
            "Worker 1, [03/04]: Training Loss: 1.330671354, Training Accuracy: 62.048\n",
            "Worker 1, [04/04]: Training Loss: 1.266972701, Training Accuracy: 63.856\n",
            "Time taken for training worker 1: 0:00:11.750647\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.475700156, Training Accuracy: 58.016\n",
            "Worker 2, [02/04]: Training Loss: 1.351321608, Training Accuracy: 61.296\n",
            "Worker 2, [03/04]: Training Loss: 1.295022352, Training Accuracy: 63.232\n",
            "Worker 2, [04/04]: Training Loss: 1.221894304, Training Accuracy: 65.088\n",
            "Time taken for training worker 2: 0:00:11.656757\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.492068723, Training Accuracy: 58.416\n",
            "Worker 3, [02/04]: Training Loss: 1.375266911, Training Accuracy: 61.104\n",
            "Worker 3, [03/04]: Training Loss: 1.290970207, Training Accuracy: 63.744\n",
            "Worker 3, [04/04]: Training Loss: 1.238667854, Training Accuracy: 65.440\n",
            "Time taken for training worker 3: 0:00:12.459510\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.529666772, Training Accuracy: 57.312\n",
            "Worker 4, [02/04]: Training Loss: 1.393026810, Training Accuracy: 61.168\n",
            "Worker 4, [03/04]: Training Loss: 1.316629710, Training Accuracy: 63.808\n",
            "Worker 4, [04/04]: Training Loss: 1.251517844, Training Accuracy: 65.552\n",
            "Time taken for training worker 4: 0:00:12.676507\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.506061423, Training Accuracy: 57.120\n",
            "Worker 5, [02/04]: Training Loss: 1.386700526, Training Accuracy: 61.136\n",
            "Worker 5, [03/04]: Training Loss: 1.289479371, Training Accuracy: 63.872\n",
            "Worker 5, [04/04]: Training Loss: 1.232923988, Training Accuracy: 64.912\n",
            "Time taken for training worker 5: 0:00:12.489193\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.482675228, Training Accuracy: 57.664\n",
            "Worker 6, [02/04]: Training Loss: 1.366709406, Training Accuracy: 61.440\n",
            "Worker 6, [03/04]: Training Loss: 1.286966711, Training Accuracy: 63.440\n",
            "Worker 6, [04/04]: Training Loss: 1.203105838, Training Accuracy: 65.312\n",
            "Time taken for training worker 6: 0:00:12.710520\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.506412286, Training Accuracy: 58.032\n",
            "Worker 7, [02/04]: Training Loss: 1.388718910, Training Accuracy: 61.184\n",
            "Worker 7, [03/04]: Training Loss: 1.291515837, Training Accuracy: 63.584\n",
            "Worker 7, [04/04]: Training Loss: 1.234143785, Training Accuracy: 65.040\n",
            "Time taken for training worker 7: 0:00:12.765632\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.497525055, Training Accuracy: 58.288\n",
            "Worker 8, [02/04]: Training Loss: 1.349277706, Training Accuracy: 62.032\n",
            "Worker 8, [03/04]: Training Loss: 1.290136566, Training Accuracy: 63.264\n",
            "Worker 8, [04/04]: Training Loss: 1.212585686, Training Accuracy: 65.344\n",
            "Time taken for training worker 8: 0:00:13.067799\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004418\n",
            "Global Update 30: Test Loss: 1.965155289, Test Accuracy: 50.060\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.498921566, Training Accuracy: 57.504\n",
            "Worker 1, [02/04]: Training Loss: 1.390185798, Training Accuracy: 60.864\n",
            "Worker 1, [03/04]: Training Loss: 1.302710203, Training Accuracy: 63.104\n",
            "Worker 1, [04/04]: Training Loss: 1.256014065, Training Accuracy: 63.968\n",
            "Time taken for training worker 1: 0:00:12.235876\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.458145642, Training Accuracy: 58.672\n",
            "Worker 2, [02/04]: Training Loss: 1.326655321, Training Accuracy: 61.760\n",
            "Worker 2, [03/04]: Training Loss: 1.268175228, Training Accuracy: 64.320\n",
            "Worker 2, [04/04]: Training Loss: 1.197934635, Training Accuracy: 65.936\n",
            "Time taken for training worker 2: 0:00:12.172741\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.483957675, Training Accuracy: 58.800\n",
            "Worker 3, [02/04]: Training Loss: 1.373725454, Training Accuracy: 61.280\n",
            "Worker 3, [03/04]: Training Loss: 1.285877191, Training Accuracy: 63.792\n",
            "Worker 3, [04/04]: Training Loss: 1.237269581, Training Accuracy: 65.280\n",
            "Time taken for training worker 3: 0:00:11.731352\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.484073424, Training Accuracy: 59.296\n",
            "Worker 4, [02/04]: Training Loss: 1.396245409, Training Accuracy: 60.976\n",
            "Worker 4, [03/04]: Training Loss: 1.309270324, Training Accuracy: 63.376\n",
            "Worker 4, [04/04]: Training Loss: 1.257837738, Training Accuracy: 65.248\n",
            "Time taken for training worker 4: 0:00:12.224968\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.468230026, Training Accuracy: 58.624\n",
            "Worker 5, [02/04]: Training Loss: 1.363269900, Training Accuracy: 60.976\n",
            "Worker 5, [03/04]: Training Loss: 1.291110319, Training Accuracy: 63.504\n",
            "Worker 5, [04/04]: Training Loss: 1.226659719, Training Accuracy: 65.696\n",
            "Time taken for training worker 5: 0:00:12.302342\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.453843313, Training Accuracy: 58.752\n",
            "Worker 6, [02/04]: Training Loss: 1.343155418, Training Accuracy: 61.712\n",
            "Worker 6, [03/04]: Training Loss: 1.281711308, Training Accuracy: 63.040\n",
            "Worker 6, [04/04]: Training Loss: 1.206472206, Training Accuracy: 65.584\n",
            "Time taken for training worker 6: 0:00:12.092653\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.472296647, Training Accuracy: 58.704\n",
            "Worker 7, [02/04]: Training Loss: 1.365010325, Training Accuracy: 61.808\n",
            "Worker 7, [03/04]: Training Loss: 1.289741065, Training Accuracy: 63.600\n",
            "Worker 7, [04/04]: Training Loss: 1.231825660, Training Accuracy: 64.896\n",
            "Time taken for training worker 7: 0:00:11.724875\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.458826020, Training Accuracy: 59.056\n",
            "Worker 8, [02/04]: Training Loss: 1.324712136, Training Accuracy: 62.096\n",
            "Worker 8, [03/04]: Training Loss: 1.263865027, Training Accuracy: 63.456\n",
            "Worker 8, [04/04]: Training Loss: 1.215346378, Training Accuracy: 65.520\n",
            "Time taken for training worker 8: 0:00:12.141679\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004495\n",
            "Global Update 31: Test Loss: 1.964070073, Test Accuracy: 50.420\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.486611062, Training Accuracy: 58.416\n",
            "Worker 1, [02/04]: Training Loss: 1.392360788, Training Accuracy: 60.144\n",
            "Worker 1, [03/04]: Training Loss: 1.321322686, Training Accuracy: 62.128\n",
            "Worker 1, [04/04]: Training Loss: 1.283594790, Training Accuracy: 63.696\n",
            "Time taken for training worker 1: 0:00:13.053174\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.420098231, Training Accuracy: 59.552\n",
            "Worker 2, [02/04]: Training Loss: 1.335656587, Training Accuracy: 62.656\n",
            "Worker 2, [03/04]: Training Loss: 1.247250511, Training Accuracy: 64.224\n",
            "Worker 2, [04/04]: Training Loss: 1.213490018, Training Accuracy: 65.184\n",
            "Time taken for training worker 2: 0:00:13.249017\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.433115098, Training Accuracy: 60.224\n",
            "Worker 3, [02/04]: Training Loss: 1.349258327, Training Accuracy: 62.304\n",
            "Worker 3, [03/04]: Training Loss: 1.285211656, Training Accuracy: 64.176\n",
            "Worker 3, [04/04]: Training Loss: 1.238848580, Training Accuracy: 64.976\n",
            "Time taken for training worker 3: 0:00:12.566645\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.480088818, Training Accuracy: 58.912\n",
            "Worker 4, [02/04]: Training Loss: 1.376795853, Training Accuracy: 61.936\n",
            "Worker 4, [03/04]: Training Loss: 1.302159918, Training Accuracy: 63.552\n",
            "Worker 4, [04/04]: Training Loss: 1.267790526, Training Accuracy: 65.024\n",
            "Time taken for training worker 4: 0:00:12.448470\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.456714264, Training Accuracy: 59.072\n",
            "Worker 5, [02/04]: Training Loss: 1.345010390, Training Accuracy: 61.632\n",
            "Worker 5, [03/04]: Training Loss: 1.274540077, Training Accuracy: 63.344\n",
            "Worker 5, [04/04]: Training Loss: 1.234501448, Training Accuracy: 65.952\n",
            "Time taken for training worker 5: 0:00:11.942549\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.435035844, Training Accuracy: 58.384\n",
            "Worker 6, [02/04]: Training Loss: 1.348470485, Training Accuracy: 62.416\n",
            "Worker 6, [03/04]: Training Loss: 1.273917730, Training Accuracy: 63.872\n",
            "Worker 6, [04/04]: Training Loss: 1.229198701, Training Accuracy: 65.120\n",
            "Time taken for training worker 6: 0:00:11.806668\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.458254629, Training Accuracy: 59.200\n",
            "Worker 7, [02/04]: Training Loss: 1.351037289, Training Accuracy: 62.528\n",
            "Worker 7, [03/04]: Training Loss: 1.284274924, Training Accuracy: 64.096\n",
            "Worker 7, [04/04]: Training Loss: 1.240406446, Training Accuracy: 65.392\n",
            "Time taken for training worker 7: 0:00:12.080867\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.409941294, Training Accuracy: 59.728\n",
            "Worker 8, [02/04]: Training Loss: 1.323876645, Training Accuracy: 62.224\n",
            "Worker 8, [03/04]: Training Loss: 1.275319285, Training Accuracy: 63.744\n",
            "Worker 8, [04/04]: Training Loss: 1.219020038, Training Accuracy: 65.200\n",
            "Time taken for training worker 8: 0:00:12.149648\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004272\n",
            "Global Update 32: Test Loss: 1.959011016, Test Accuracy: 50.410\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.470790480, Training Accuracy: 58.864\n",
            "Worker 1, [02/04]: Training Loss: 1.391028159, Training Accuracy: 60.992\n",
            "Worker 1, [03/04]: Training Loss: 1.316637218, Training Accuracy: 63.232\n",
            "Worker 1, [04/04]: Training Loss: 1.277110163, Training Accuracy: 63.568\n",
            "Time taken for training worker 1: 0:00:11.972475\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.425984594, Training Accuracy: 59.472\n",
            "Worker 2, [02/04]: Training Loss: 1.324100178, Training Accuracy: 61.904\n",
            "Worker 2, [03/04]: Training Loss: 1.255669788, Training Accuracy: 64.480\n",
            "Worker 2, [04/04]: Training Loss: 1.237476836, Training Accuracy: 64.624\n",
            "Time taken for training worker 2: 0:00:11.741658\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.438360248, Training Accuracy: 60.192\n",
            "Worker 3, [02/04]: Training Loss: 1.344599954, Training Accuracy: 62.368\n",
            "Worker 3, [03/04]: Training Loss: 1.293603282, Training Accuracy: 63.648\n",
            "Worker 3, [04/04]: Training Loss: 1.257105342, Training Accuracy: 65.216\n",
            "Time taken for training worker 3: 0:00:12.139979\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.452760713, Training Accuracy: 59.136\n",
            "Worker 4, [02/04]: Training Loss: 1.361855441, Training Accuracy: 62.384\n",
            "Worker 4, [03/04]: Training Loss: 1.315985147, Training Accuracy: 63.168\n",
            "Worker 4, [04/04]: Training Loss: 1.273224194, Training Accuracy: 64.528\n",
            "Time taken for training worker 4: 0:00:12.175346\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.425943158, Training Accuracy: 60.352\n",
            "Worker 5, [02/04]: Training Loss: 1.335660855, Training Accuracy: 61.920\n",
            "Worker 5, [03/04]: Training Loss: 1.305551753, Training Accuracy: 63.664\n",
            "Worker 5, [04/04]: Training Loss: 1.252976557, Training Accuracy: 64.688\n",
            "Time taken for training worker 5: 0:00:12.155885\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.416958786, Training Accuracy: 59.776\n",
            "Worker 6, [02/04]: Training Loss: 1.323558897, Training Accuracy: 62.496\n",
            "Worker 6, [03/04]: Training Loss: 1.283649269, Training Accuracy: 64.448\n",
            "Worker 6, [04/04]: Training Loss: 1.222767865, Training Accuracy: 65.776\n",
            "Time taken for training worker 6: 0:00:12.142983\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.422228785, Training Accuracy: 59.744\n",
            "Worker 7, [02/04]: Training Loss: 1.357510363, Training Accuracy: 62.208\n",
            "Worker 7, [03/04]: Training Loss: 1.301449652, Training Accuracy: 63.040\n",
            "Worker 7, [04/04]: Training Loss: 1.257097323, Training Accuracy: 64.656\n",
            "Time taken for training worker 7: 0:00:12.688215\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.439476273, Training Accuracy: 59.904\n",
            "Worker 8, [02/04]: Training Loss: 1.335244668, Training Accuracy: 62.384\n",
            "Worker 8, [03/04]: Training Loss: 1.280159526, Training Accuracy: 63.920\n",
            "Worker 8, [04/04]: Training Loss: 1.228024110, Training Accuracy: 65.008\n",
            "Time taken for training worker 8: 0:00:12.270970\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004594\n",
            "Global Update 33: Test Loss: 1.955886691, Test Accuracy: 50.870\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.442593486, Training Accuracy: 59.232\n",
            "Worker 1, [02/04]: Training Loss: 1.362281787, Training Accuracy: 61.168\n",
            "Worker 1, [03/04]: Training Loss: 1.328690866, Training Accuracy: 63.136\n",
            "Worker 1, [04/04]: Training Loss: 1.316790995, Training Accuracy: 63.024\n",
            "Time taken for training worker 1: 0:00:11.899446\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.393831057, Training Accuracy: 61.072\n",
            "Worker 2, [02/04]: Training Loss: 1.350032188, Training Accuracy: 61.568\n",
            "Worker 2, [03/04]: Training Loss: 1.296937751, Training Accuracy: 62.848\n",
            "Worker 2, [04/04]: Training Loss: 1.251225388, Training Accuracy: 64.592\n",
            "Time taken for training worker 2: 0:00:13.235598\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.425276784, Training Accuracy: 59.968\n",
            "Worker 3, [02/04]: Training Loss: 1.363790128, Training Accuracy: 62.016\n",
            "Worker 3, [03/04]: Training Loss: 1.308100189, Training Accuracy: 63.040\n",
            "Worker 3, [04/04]: Training Loss: 1.285057220, Training Accuracy: 64.000\n",
            "Time taken for training worker 3: 0:00:12.885788\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.444622239, Training Accuracy: 59.984\n",
            "Worker 4, [02/04]: Training Loss: 1.381395685, Training Accuracy: 61.744\n",
            "Worker 4, [03/04]: Training Loss: 1.337186712, Training Accuracy: 62.768\n",
            "Worker 4, [04/04]: Training Loss: 1.290384594, Training Accuracy: 63.296\n",
            "Time taken for training worker 4: 0:00:12.441332\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.417716144, Training Accuracy: 59.744\n",
            "Worker 5, [02/04]: Training Loss: 1.346635819, Training Accuracy: 62.000\n",
            "Worker 5, [03/04]: Training Loss: 1.318985192, Training Accuracy: 62.784\n",
            "Worker 5, [04/04]: Training Loss: 1.267508825, Training Accuracy: 64.544\n",
            "Time taken for training worker 5: 0:00:12.385502\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.384144369, Training Accuracy: 60.864\n",
            "Worker 6, [02/04]: Training Loss: 1.339986863, Training Accuracy: 62.480\n",
            "Worker 6, [03/04]: Training Loss: 1.285563960, Training Accuracy: 63.216\n",
            "Worker 6, [04/04]: Training Loss: 1.252982720, Training Accuracy: 64.432\n",
            "Time taken for training worker 6: 0:00:11.808083\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.437008973, Training Accuracy: 59.728\n",
            "Worker 7, [02/04]: Training Loss: 1.382454010, Training Accuracy: 60.912\n",
            "Worker 7, [03/04]: Training Loss: 1.294684548, Training Accuracy: 63.536\n",
            "Worker 7, [04/04]: Training Loss: 1.281637118, Training Accuracy: 64.224\n",
            "Time taken for training worker 7: 0:00:12.110466\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.418986462, Training Accuracy: 60.080\n",
            "Worker 8, [02/04]: Training Loss: 1.344447236, Training Accuracy: 62.048\n",
            "Worker 8, [03/04]: Training Loss: 1.298846007, Training Accuracy: 63.584\n",
            "Worker 8, [04/04]: Training Loss: 1.263202886, Training Accuracy: 64.464\n",
            "Time taken for training worker 8: 0:00:12.214751\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004941\n",
            "Global Update 34: Test Loss: 1.950872451, Test Accuracy: 50.710\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.451464534, Training Accuracy: 59.040\n",
            "Worker 1, [02/04]: Training Loss: 1.401769957, Training Accuracy: 60.176\n",
            "Worker 1, [03/04]: Training Loss: 1.370683493, Training Accuracy: 61.904\n",
            "Worker 1, [04/04]: Training Loss: 1.340632319, Training Accuracy: 61.808\n",
            "Time taken for training worker 1: 0:00:12.189847\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.390521374, Training Accuracy: 60.816\n",
            "Worker 2, [02/04]: Training Loss: 1.345317176, Training Accuracy: 61.552\n",
            "Worker 2, [03/04]: Training Loss: 1.306360282, Training Accuracy: 63.296\n",
            "Worker 2, [04/04]: Training Loss: 1.285767942, Training Accuracy: 63.504\n",
            "Time taken for training worker 2: 0:00:12.365613\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.415193349, Training Accuracy: 59.984\n",
            "Worker 3, [02/04]: Training Loss: 1.374050673, Training Accuracy: 62.016\n",
            "Worker 3, [03/04]: Training Loss: 1.343161331, Training Accuracy: 62.496\n",
            "Worker 3, [04/04]: Training Loss: 1.306419139, Training Accuracy: 63.488\n",
            "Time taken for training worker 3: 0:00:12.316419\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.443763836, Training Accuracy: 60.048\n",
            "Worker 4, [02/04]: Training Loss: 1.387728406, Training Accuracy: 61.120\n",
            "Worker 4, [03/04]: Training Loss: 1.358074922, Training Accuracy: 61.600\n",
            "Worker 4, [04/04]: Training Loss: 1.334625851, Training Accuracy: 63.216\n",
            "Time taken for training worker 4: 0:00:12.726126\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.410309490, Training Accuracy: 60.624\n",
            "Worker 5, [02/04]: Training Loss: 1.378164116, Training Accuracy: 60.528\n",
            "Worker 5, [03/04]: Training Loss: 1.326536722, Training Accuracy: 62.304\n",
            "Worker 5, [04/04]: Training Loss: 1.305834405, Training Accuracy: 62.896\n",
            "Time taken for training worker 5: 0:00:12.154345\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.388130268, Training Accuracy: 60.768\n",
            "Worker 6, [02/04]: Training Loss: 1.343908408, Training Accuracy: 61.840\n",
            "Worker 6, [03/04]: Training Loss: 1.322486402, Training Accuracy: 62.368\n",
            "Worker 6, [04/04]: Training Loss: 1.303524961, Training Accuracy: 63.280\n",
            "Time taken for training worker 6: 0:00:12.027918\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.422001165, Training Accuracy: 59.296\n",
            "Worker 7, [02/04]: Training Loss: 1.373667325, Training Accuracy: 62.032\n",
            "Worker 7, [03/04]: Training Loss: 1.325227821, Training Accuracy: 62.720\n",
            "Worker 7, [04/04]: Training Loss: 1.311145262, Training Accuracy: 63.120\n",
            "Time taken for training worker 7: 0:00:12.022871\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.391957565, Training Accuracy: 60.832\n",
            "Worker 8, [02/04]: Training Loss: 1.337311709, Training Accuracy: 62.192\n",
            "Worker 8, [03/04]: Training Loss: 1.324440907, Training Accuracy: 62.688\n",
            "Worker 8, [04/04]: Training Loss: 1.296826492, Training Accuracy: 63.600\n",
            "Time taken for training worker 8: 0:00:12.591352\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004525\n",
            "Global Update 35: Test Loss: 1.946677588, Test Accuracy: 50.840\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.440714572, Training Accuracy: 59.264\n",
            "Worker 1, [02/04]: Training Loss: 1.399960504, Training Accuracy: 59.888\n",
            "Worker 1, [03/04]: Training Loss: 1.394306535, Training Accuracy: 60.832\n",
            "Worker 1, [04/04]: Training Loss: 1.389854388, Training Accuracy: 60.784\n",
            "Time taken for training worker 1: 0:00:12.176315\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.377323612, Training Accuracy: 60.448\n",
            "Worker 2, [02/04]: Training Loss: 1.364934333, Training Accuracy: 60.576\n",
            "Worker 2, [03/04]: Training Loss: 1.341129630, Training Accuracy: 62.288\n",
            "Worker 2, [04/04]: Training Loss: 1.336010626, Training Accuracy: 61.872\n",
            "Time taken for training worker 2: 0:00:11.472589\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.394998589, Training Accuracy: 60.560\n",
            "Worker 3, [02/04]: Training Loss: 1.402045443, Training Accuracy: 60.624\n",
            "Worker 3, [03/04]: Training Loss: 1.369168271, Training Accuracy: 61.568\n",
            "Worker 3, [04/04]: Training Loss: 1.336746777, Training Accuracy: 62.624\n",
            "Time taken for training worker 3: 0:00:11.698241\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.424278787, Training Accuracy: 59.776\n",
            "Worker 4, [02/04]: Training Loss: 1.414957576, Training Accuracy: 60.912\n",
            "Worker 4, [03/04]: Training Loss: 1.369654474, Training Accuracy: 61.664\n",
            "Worker 4, [04/04]: Training Loss: 1.369716054, Training Accuracy: 61.712\n",
            "Time taken for training worker 4: 0:00:11.858127\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.402058657, Training Accuracy: 60.688\n",
            "Worker 5, [02/04]: Training Loss: 1.364727999, Training Accuracy: 60.896\n",
            "Worker 5, [03/04]: Training Loss: 1.357190393, Training Accuracy: 61.824\n",
            "Worker 5, [04/04]: Training Loss: 1.350941241, Training Accuracy: 61.632\n",
            "Time taken for training worker 5: 0:00:13.158093\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.390031686, Training Accuracy: 60.320\n",
            "Worker 6, [02/04]: Training Loss: 1.351172277, Training Accuracy: 62.128\n",
            "Worker 6, [03/04]: Training Loss: 1.348020682, Training Accuracy: 62.512\n",
            "Worker 6, [04/04]: Training Loss: 1.311183783, Training Accuracy: 63.056\n",
            "Time taken for training worker 6: 0:00:11.672284\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.408274218, Training Accuracy: 60.240\n",
            "Worker 7, [02/04]: Training Loss: 1.386283170, Training Accuracy: 61.216\n",
            "Worker 7, [03/04]: Training Loss: 1.371446779, Training Accuracy: 60.912\n",
            "Worker 7, [04/04]: Training Loss: 1.359832593, Training Accuracy: 61.888\n",
            "Time taken for training worker 7: 0:00:11.747682\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.375738387, Training Accuracy: 61.440\n",
            "Worker 8, [02/04]: Training Loss: 1.396371897, Training Accuracy: 60.848\n",
            "Worker 8, [03/04]: Training Loss: 1.340565390, Training Accuracy: 62.384\n",
            "Worker 8, [04/04]: Training Loss: 1.347410734, Training Accuracy: 61.632\n",
            "Time taken for training worker 8: 0:00:12.134732\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004363\n",
            "Global Update 36: Test Loss: 1.944857278, Test Accuracy: 50.840\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.433085882, Training Accuracy: 59.664\n",
            "Worker 1, [02/04]: Training Loss: 1.441926073, Training Accuracy: 59.472\n",
            "Worker 1, [03/04]: Training Loss: 1.417802339, Training Accuracy: 59.552\n",
            "Worker 1, [04/04]: Training Loss: 1.414318861, Training Accuracy: 60.816\n",
            "Time taken for training worker 1: 0:00:11.960094\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.381528052, Training Accuracy: 60.704\n",
            "Worker 2, [02/04]: Training Loss: 1.388608253, Training Accuracy: 61.168\n",
            "Worker 2, [03/04]: Training Loss: 1.371540560, Training Accuracy: 60.544\n",
            "Worker 2, [04/04]: Training Loss: 1.362657092, Training Accuracy: 61.584\n",
            "Time taken for training worker 2: 0:00:12.250744\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 1.389196855, Training Accuracy: 60.624\n",
            "Worker 3, [02/04]: Training Loss: 1.396158805, Training Accuracy: 60.912\n",
            "Worker 3, [03/04]: Training Loss: 1.400707947, Training Accuracy: 60.848\n",
            "Worker 3, [04/04]: Training Loss: 1.380651562, Training Accuracy: 61.728\n",
            "Time taken for training worker 3: 0:00:12.132774\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 1.429097603, Training Accuracy: 60.128\n",
            "Worker 4, [02/04]: Training Loss: 1.408881869, Training Accuracy: 59.936\n",
            "Worker 4, [03/04]: Training Loss: 1.400197289, Training Accuracy: 60.048\n",
            "Worker 4, [04/04]: Training Loss: 1.419292697, Training Accuracy: 60.480\n",
            "Time taken for training worker 4: 0:00:12.958379\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/04]: Training Loss: 1.408880256, Training Accuracy: 59.600\n",
            "Worker 5, [02/04]: Training Loss: 1.400052092, Training Accuracy: 59.904\n",
            "Worker 5, [03/04]: Training Loss: 1.398406222, Training Accuracy: 60.864\n",
            "Worker 5, [04/04]: Training Loss: 1.388508539, Training Accuracy: 60.704\n",
            "Time taken for training worker 5: 0:00:12.509428\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/04]: Training Loss: 1.387349731, Training Accuracy: 61.552\n",
            "Worker 6, [02/04]: Training Loss: 1.373407398, Training Accuracy: 60.272\n",
            "Worker 6, [03/04]: Training Loss: 1.374206212, Training Accuracy: 60.592\n",
            "Worker 6, [04/04]: Training Loss: 1.353678039, Training Accuracy: 61.328\n",
            "Time taken for training worker 6: 0:00:11.953288\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/04]: Training Loss: 1.418833366, Training Accuracy: 60.192\n",
            "Worker 7, [02/04]: Training Loss: 1.402491871, Training Accuracy: 60.256\n",
            "Worker 7, [03/04]: Training Loss: 1.403457380, Training Accuracy: 61.184\n",
            "Worker 7, [04/04]: Training Loss: 1.388459511, Training Accuracy: 60.992\n",
            "Time taken for training worker 7: 0:00:12.012886\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/04]: Training Loss: 1.383331908, Training Accuracy: 60.496\n",
            "Worker 8, [02/04]: Training Loss: 1.389526680, Training Accuracy: 60.816\n",
            "Worker 8, [03/04]: Training Loss: 1.373190268, Training Accuracy: 60.688\n",
            "Worker 8, [04/04]: Training Loss: 1.368294016, Training Accuracy: 61.088\n",
            "Time taken for training worker 8: 0:00:12.749230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004608\n",
            "Global Update 37: Test Loss: 1.944273071, Test Accuracy: 50.760\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 1:00:50.078861\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:8\n",
            "==================================================\n",
            "Worker 1, [01/08]: Training Loss: 4.592445948, Training Accuracy: 1.120\n",
            "Worker 1, [02/08]: Training Loss: 4.405845365, Training Accuracy: 3.248\n",
            "Worker 1, [03/08]: Training Loss: 4.190371620, Training Accuracy: 5.888\n",
            "Worker 1, [04/08]: Training Loss: 4.051818702, Training Accuracy: 7.136\n",
            "Worker 1, [05/08]: Training Loss: 3.925714293, Training Accuracy: 8.672\n",
            "Worker 1, [06/08]: Training Loss: 3.844272679, Training Accuracy: 9.680\n",
            "Worker 1, [07/08]: Training Loss: 3.739282226, Training Accuracy: 11.632\n",
            "Worker 1, [08/08]: Training Loss: 3.664396220, Training Accuracy: 13.040\n",
            "Time taken for training worker 1: 0:00:23.904805\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 4.593680990, Training Accuracy: 1.232\n",
            "Worker 2, [02/08]: Training Loss: 4.395912448, Training Accuracy: 3.584\n",
            "Worker 2, [03/08]: Training Loss: 4.164397636, Training Accuracy: 5.952\n",
            "Worker 2, [04/08]: Training Loss: 4.025903376, Training Accuracy: 8.048\n",
            "Worker 2, [05/08]: Training Loss: 3.891099643, Training Accuracy: 10.176\n",
            "Worker 2, [06/08]: Training Loss: 3.780096806, Training Accuracy: 11.312\n",
            "Worker 2, [07/08]: Training Loss: 3.684532187, Training Accuracy: 12.752\n",
            "Worker 2, [08/08]: Training Loss: 3.609626921, Training Accuracy: 14.320\n",
            "Time taken for training worker 2: 0:00:24.958073\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 4.586555247, Training Accuracy: 1.808\n",
            "Worker 3, [02/08]: Training Loss: 4.363323951, Training Accuracy: 3.552\n",
            "Worker 3, [03/08]: Training Loss: 4.136500393, Training Accuracy: 5.952\n",
            "Worker 3, [04/08]: Training Loss: 3.993322754, Training Accuracy: 8.368\n",
            "Worker 3, [05/08]: Training Loss: 3.882248416, Training Accuracy: 9.712\n",
            "Worker 3, [06/08]: Training Loss: 3.770319834, Training Accuracy: 11.744\n",
            "Worker 3, [07/08]: Training Loss: 3.687244172, Training Accuracy: 12.768\n",
            "Worker 3, [08/08]: Training Loss: 3.599955885, Training Accuracy: 14.784\n",
            "Time taken for training worker 3: 0:00:23.931377\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 4.592522290, Training Accuracy: 1.472\n",
            "Worker 4, [02/08]: Training Loss: 4.379847400, Training Accuracy: 3.728\n",
            "Worker 4, [03/08]: Training Loss: 4.151496493, Training Accuracy: 6.112\n",
            "Worker 4, [04/08]: Training Loss: 4.015664040, Training Accuracy: 7.552\n",
            "Worker 4, [05/08]: Training Loss: 3.898808852, Training Accuracy: 9.360\n",
            "Worker 4, [06/08]: Training Loss: 3.800598332, Training Accuracy: 10.528\n",
            "Worker 4, [07/08]: Training Loss: 3.707996551, Training Accuracy: 12.352\n",
            "Worker 4, [08/08]: Training Loss: 3.607920002, Training Accuracy: 13.280\n",
            "Time taken for training worker 4: 0:00:23.714930\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 4.590316096, Training Accuracy: 1.232\n",
            "Worker 5, [02/08]: Training Loss: 4.384341722, Training Accuracy: 3.728\n",
            "Worker 5, [03/08]: Training Loss: 4.167890600, Training Accuracy: 5.520\n",
            "Worker 5, [04/08]: Training Loss: 4.038031396, Training Accuracy: 7.488\n",
            "Worker 5, [05/08]: Training Loss: 3.901030485, Training Accuracy: 10.016\n",
            "Worker 5, [06/08]: Training Loss: 3.774159512, Training Accuracy: 12.016\n",
            "Worker 5, [07/08]: Training Loss: 3.686446131, Training Accuracy: 13.168\n",
            "Worker 5, [08/08]: Training Loss: 3.600060139, Training Accuracy: 14.320\n",
            "Time taken for training worker 5: 0:00:23.606423\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 4.592872595, Training Accuracy: 2.000\n",
            "Worker 6, [02/08]: Training Loss: 4.361082330, Training Accuracy: 4.160\n",
            "Worker 6, [03/08]: Training Loss: 4.130390213, Training Accuracy: 6.672\n",
            "Worker 6, [04/08]: Training Loss: 3.985783358, Training Accuracy: 8.864\n",
            "Worker 6, [05/08]: Training Loss: 3.868253769, Training Accuracy: 9.904\n",
            "Worker 6, [06/08]: Training Loss: 3.775728537, Training Accuracy: 11.328\n",
            "Worker 6, [07/08]: Training Loss: 3.666022831, Training Accuracy: 13.312\n",
            "Worker 6, [08/08]: Training Loss: 3.591974597, Training Accuracy: 14.800\n",
            "Time taken for training worker 6: 0:00:24.690125\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 4.592570796, Training Accuracy: 1.680\n",
            "Worker 7, [02/08]: Training Loss: 4.385392063, Training Accuracy: 3.344\n",
            "Worker 7, [03/08]: Training Loss: 4.157205599, Training Accuracy: 5.568\n",
            "Worker 7, [04/08]: Training Loss: 3.990401725, Training Accuracy: 8.208\n",
            "Worker 7, [05/08]: Training Loss: 3.897042652, Training Accuracy: 9.712\n",
            "Worker 7, [06/08]: Training Loss: 3.794657649, Training Accuracy: 10.928\n",
            "Worker 7, [07/08]: Training Loss: 3.696208737, Training Accuracy: 12.192\n",
            "Worker 7, [08/08]: Training Loss: 3.617651645, Training Accuracy: 13.808\n",
            "Time taken for training worker 7: 0:00:24.696930\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 4.588207318, Training Accuracy: 1.648\n",
            "Worker 8, [02/08]: Training Loss: 4.354869220, Training Accuracy: 4.128\n",
            "Worker 8, [03/08]: Training Loss: 4.138172101, Training Accuracy: 6.192\n",
            "Worker 8, [04/08]: Training Loss: 3.990942245, Training Accuracy: 8.528\n",
            "Worker 8, [05/08]: Training Loss: 3.877170765, Training Accuracy: 10.480\n",
            "Worker 8, [06/08]: Training Loss: 3.772279683, Training Accuracy: 11.584\n",
            "Worker 8, [07/08]: Training Loss: 3.679700362, Training Accuracy: 13.072\n",
            "Worker 8, [08/08]: Training Loss: 3.575998518, Training Accuracy: 14.864\n",
            "Time taken for training worker 8: 0:00:24.348386\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004163\n",
            "Global Update 01: Test Loss: 3.674379577, Test Accuracy: 16.580\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 3.715131803, Training Accuracy: 12.848\n",
            "Worker 1, [02/08]: Training Loss: 3.625855908, Training Accuracy: 13.280\n",
            "Worker 1, [03/08]: Training Loss: 3.489482687, Training Accuracy: 15.712\n",
            "Worker 1, [04/08]: Training Loss: 3.397870195, Training Accuracy: 17.376\n",
            "Worker 1, [05/08]: Training Loss: 3.339416229, Training Accuracy: 18.272\n",
            "Worker 1, [06/08]: Training Loss: 3.264049007, Training Accuracy: 19.104\n",
            "Worker 1, [07/08]: Training Loss: 3.143837605, Training Accuracy: 21.936\n",
            "Worker 1, [08/08]: Training Loss: 3.093597483, Training Accuracy: 22.128\n",
            "Time taken for training worker 1: 0:00:24.850631\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 3.659102535, Training Accuracy: 13.200\n",
            "Worker 2, [02/08]: Training Loss: 3.540432986, Training Accuracy: 14.912\n",
            "Worker 2, [03/08]: Training Loss: 3.425832700, Training Accuracy: 16.752\n",
            "Worker 2, [04/08]: Training Loss: 3.342910168, Training Accuracy: 18.592\n",
            "Worker 2, [05/08]: Training Loss: 3.270669913, Training Accuracy: 19.728\n",
            "Worker 2, [06/08]: Training Loss: 3.193204916, Training Accuracy: 20.656\n",
            "Worker 2, [07/08]: Training Loss: 3.141126730, Training Accuracy: 21.968\n",
            "Worker 2, [08/08]: Training Loss: 3.046842342, Training Accuracy: 22.736\n",
            "Time taken for training worker 2: 0:00:23.828176\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 3.672593723, Training Accuracy: 12.832\n",
            "Worker 3, [02/08]: Training Loss: 3.560429393, Training Accuracy: 14.480\n",
            "Worker 3, [03/08]: Training Loss: 3.443627936, Training Accuracy: 16.464\n",
            "Worker 3, [04/08]: Training Loss: 3.358900494, Training Accuracy: 17.920\n",
            "Worker 3, [05/08]: Training Loss: 3.307666518, Training Accuracy: 18.896\n",
            "Worker 3, [06/08]: Training Loss: 3.229054706, Training Accuracy: 20.864\n",
            "Worker 3, [07/08]: Training Loss: 3.158835608, Training Accuracy: 20.928\n",
            "Worker 3, [08/08]: Training Loss: 3.095337060, Training Accuracy: 22.192\n",
            "Time taken for training worker 3: 0:00:24.422089\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 3.688371880, Training Accuracy: 12.688\n",
            "Worker 4, [02/08]: Training Loss: 3.571939860, Training Accuracy: 14.272\n",
            "Worker 4, [03/08]: Training Loss: 3.485989439, Training Accuracy: 15.376\n",
            "Worker 4, [04/08]: Training Loss: 3.396841716, Training Accuracy: 17.472\n",
            "Worker 4, [05/08]: Training Loss: 3.336285336, Training Accuracy: 18.384\n",
            "Worker 4, [06/08]: Training Loss: 3.227800048, Training Accuracy: 19.776\n",
            "Worker 4, [07/08]: Training Loss: 3.182761535, Training Accuracy: 20.832\n",
            "Worker 4, [08/08]: Training Loss: 3.103891528, Training Accuracy: 22.368\n",
            "Time taken for training worker 4: 0:00:23.738358\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 3.711735631, Training Accuracy: 13.648\n",
            "Worker 5, [02/08]: Training Loss: 3.565278399, Training Accuracy: 14.832\n",
            "Worker 5, [03/08]: Training Loss: 3.484466251, Training Accuracy: 16.288\n",
            "Worker 5, [04/08]: Training Loss: 3.367065894, Training Accuracy: 18.240\n",
            "Worker 5, [05/08]: Training Loss: 3.303789479, Training Accuracy: 19.840\n",
            "Worker 5, [06/08]: Training Loss: 3.225941797, Training Accuracy: 20.864\n",
            "Worker 5, [07/08]: Training Loss: 3.151417791, Training Accuracy: 22.624\n",
            "Worker 5, [08/08]: Training Loss: 3.077803437, Training Accuracy: 22.528\n",
            "Time taken for training worker 5: 0:00:23.881276\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 3.665318404, Training Accuracy: 13.712\n",
            "Worker 6, [02/08]: Training Loss: 3.556017309, Training Accuracy: 14.608\n",
            "Worker 6, [03/08]: Training Loss: 3.439562613, Training Accuracy: 17.232\n",
            "Worker 6, [04/08]: Training Loss: 3.366548835, Training Accuracy: 18.672\n",
            "Worker 6, [05/08]: Training Loss: 3.290772830, Training Accuracy: 19.872\n",
            "Worker 6, [06/08]: Training Loss: 3.208945318, Training Accuracy: 20.880\n",
            "Worker 6, [07/08]: Training Loss: 3.124633028, Training Accuracy: 21.776\n",
            "Worker 6, [08/08]: Training Loss: 3.076259131, Training Accuracy: 22.400\n",
            "Time taken for training worker 6: 0:00:24.520633\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 3.671972474, Training Accuracy: 13.712\n",
            "Worker 7, [02/08]: Training Loss: 3.569275386, Training Accuracy: 14.672\n",
            "Worker 7, [03/08]: Training Loss: 3.450533485, Training Accuracy: 17.200\n",
            "Worker 7, [04/08]: Training Loss: 3.388440743, Training Accuracy: 17.696\n",
            "Worker 7, [05/08]: Training Loss: 3.252733009, Training Accuracy: 19.808\n",
            "Worker 7, [06/08]: Training Loss: 3.240675892, Training Accuracy: 19.696\n",
            "Worker 7, [07/08]: Training Loss: 3.154071776, Training Accuracy: 21.760\n",
            "Worker 7, [08/08]: Training Loss: 3.088999634, Training Accuracy: 21.408\n",
            "Time taken for training worker 7: 0:00:24.020665\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 3.662429209, Training Accuracy: 13.584\n",
            "Worker 8, [02/08]: Training Loss: 3.567302098, Training Accuracy: 15.424\n",
            "Worker 8, [03/08]: Training Loss: 3.444226983, Training Accuracy: 16.736\n",
            "Worker 8, [04/08]: Training Loss: 3.360790328, Training Accuracy: 18.400\n",
            "Worker 8, [05/08]: Training Loss: 3.263815838, Training Accuracy: 19.584\n",
            "Worker 8, [06/08]: Training Loss: 3.193085753, Training Accuracy: 21.344\n",
            "Worker 8, [07/08]: Training Loss: 3.120429545, Training Accuracy: 22.624\n",
            "Worker 8, [08/08]: Training Loss: 3.051861853, Training Accuracy: 23.168\n",
            "Time taken for training worker 8: 0:00:25.990283\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004356\n",
            "Global Update 02: Test Loss: 3.030932836, Test Accuracy: 25.180\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 3.255385345, Training Accuracy: 20.752\n",
            "Worker 1, [02/08]: Training Loss: 3.156741179, Training Accuracy: 21.872\n",
            "Worker 1, [03/08]: Training Loss: 3.088496271, Training Accuracy: 22.400\n",
            "Worker 1, [04/08]: Training Loss: 3.003762508, Training Accuracy: 24.544\n",
            "Worker 1, [05/08]: Training Loss: 2.931672619, Training Accuracy: 25.184\n",
            "Worker 1, [06/08]: Training Loss: 2.871415143, Training Accuracy: 27.536\n",
            "Worker 1, [07/08]: Training Loss: 2.812103775, Training Accuracy: 27.856\n",
            "Worker 1, [08/08]: Training Loss: 2.761333580, Training Accuracy: 28.240\n",
            "Time taken for training worker 1: 0:00:24.522831\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 3.201858749, Training Accuracy: 20.560\n",
            "Worker 2, [02/08]: Training Loss: 3.104936089, Training Accuracy: 22.432\n",
            "Worker 2, [03/08]: Training Loss: 3.020501300, Training Accuracy: 24.032\n",
            "Worker 2, [04/08]: Training Loss: 2.956912498, Training Accuracy: 25.056\n",
            "Worker 2, [05/08]: Training Loss: 2.854484821, Training Accuracy: 26.992\n",
            "Worker 2, [06/08]: Training Loss: 2.791243609, Training Accuracy: 28.288\n",
            "Worker 2, [07/08]: Training Loss: 2.749978153, Training Accuracy: 28.912\n",
            "Worker 2, [08/08]: Training Loss: 2.690618525, Training Accuracy: 29.808\n",
            "Time taken for training worker 2: 0:00:24.563667\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 3.229118257, Training Accuracy: 20.912\n",
            "Worker 3, [02/08]: Training Loss: 3.129746206, Training Accuracy: 23.088\n",
            "Worker 3, [03/08]: Training Loss: 3.060331512, Training Accuracy: 23.408\n",
            "Worker 3, [04/08]: Training Loss: 2.994018494, Training Accuracy: 24.528\n",
            "Worker 3, [05/08]: Training Loss: 2.927912257, Training Accuracy: 25.808\n",
            "Worker 3, [06/08]: Training Loss: 2.851959898, Training Accuracy: 26.848\n",
            "Worker 3, [07/08]: Training Loss: 2.788602223, Training Accuracy: 28.208\n",
            "Worker 3, [08/08]: Training Loss: 2.714243115, Training Accuracy: 29.408\n",
            "Time taken for training worker 3: 0:00:24.343083\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 3.240210874, Training Accuracy: 20.496\n",
            "Worker 4, [02/08]: Training Loss: 3.136758167, Training Accuracy: 22.480\n",
            "Worker 4, [03/08]: Training Loss: 3.067453345, Training Accuracy: 23.280\n",
            "Worker 4, [04/08]: Training Loss: 2.984126704, Training Accuracy: 24.704\n",
            "Worker 4, [05/08]: Training Loss: 2.931614204, Training Accuracy: 25.248\n",
            "Worker 4, [06/08]: Training Loss: 2.835137073, Training Accuracy: 27.616\n",
            "Worker 4, [07/08]: Training Loss: 2.779496458, Training Accuracy: 28.016\n",
            "Worker 4, [08/08]: Training Loss: 2.724088876, Training Accuracy: 29.824\n",
            "Time taken for training worker 4: 0:00:25.536916\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 3.233893414, Training Accuracy: 21.040\n",
            "Worker 5, [02/08]: Training Loss: 3.120836221, Training Accuracy: 22.496\n",
            "Worker 5, [03/08]: Training Loss: 3.049840428, Training Accuracy: 24.048\n",
            "Worker 5, [04/08]: Training Loss: 2.987134211, Training Accuracy: 24.864\n",
            "Worker 5, [05/08]: Training Loss: 2.898057675, Training Accuracy: 26.480\n",
            "Worker 5, [06/08]: Training Loss: 2.841905049, Training Accuracy: 27.424\n",
            "Worker 5, [07/08]: Training Loss: 2.779973857, Training Accuracy: 28.176\n",
            "Worker 5, [08/08]: Training Loss: 2.694151988, Training Accuracy: 30.128\n",
            "Time taken for training worker 5: 0:00:23.827418\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 3.187505141, Training Accuracy: 21.088\n",
            "Worker 6, [02/08]: Training Loss: 3.096963420, Training Accuracy: 23.136\n",
            "Worker 6, [03/08]: Training Loss: 3.021888862, Training Accuracy: 23.936\n",
            "Worker 6, [04/08]: Training Loss: 2.970391923, Training Accuracy: 25.200\n",
            "Worker 6, [05/08]: Training Loss: 2.925709498, Training Accuracy: 25.584\n",
            "Worker 6, [06/08]: Training Loss: 2.816993059, Training Accuracy: 27.312\n",
            "Worker 6, [07/08]: Training Loss: 2.739439091, Training Accuracy: 29.360\n",
            "Worker 6, [08/08]: Training Loss: 2.699782140, Training Accuracy: 29.392\n",
            "Time taken for training worker 6: 0:00:24.324378\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 3.199677701, Training Accuracy: 21.248\n",
            "Worker 7, [02/08]: Training Loss: 3.139135614, Training Accuracy: 21.584\n",
            "Worker 7, [03/08]: Training Loss: 3.027397888, Training Accuracy: 24.256\n",
            "Worker 7, [04/08]: Training Loss: 2.969920951, Training Accuracy: 24.656\n",
            "Worker 7, [05/08]: Training Loss: 2.905497001, Training Accuracy: 25.776\n",
            "Worker 7, [06/08]: Training Loss: 2.832735573, Training Accuracy: 26.784\n",
            "Worker 7, [07/08]: Training Loss: 2.777672408, Training Accuracy: 27.872\n",
            "Worker 7, [08/08]: Training Loss: 2.703912241, Training Accuracy: 30.400\n",
            "Time taken for training worker 7: 0:00:24.031784\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 3.198680282, Training Accuracy: 21.872\n",
            "Worker 8, [02/08]: Training Loss: 3.090836420, Training Accuracy: 22.656\n",
            "Worker 8, [03/08]: Training Loss: 3.055611199, Training Accuracy: 23.824\n",
            "Worker 8, [04/08]: Training Loss: 2.954443640, Training Accuracy: 25.424\n",
            "Worker 8, [05/08]: Training Loss: 2.890741414, Training Accuracy: 25.904\n",
            "Worker 8, [06/08]: Training Loss: 2.829224535, Training Accuracy: 27.680\n",
            "Worker 8, [07/08]: Training Loss: 2.735720240, Training Accuracy: 29.712\n",
            "Worker 8, [08/08]: Training Loss: 2.711481654, Training Accuracy: 29.296\n",
            "Time taken for training worker 8: 0:00:24.914536\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004585\n",
            "Global Update 03: Test Loss: 2.744227993, Test Accuracy: 31.060\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.978629005, Training Accuracy: 26.144\n",
            "Worker 1, [02/08]: Training Loss: 2.891648928, Training Accuracy: 26.464\n",
            "Worker 1, [03/08]: Training Loss: 2.797644647, Training Accuracy: 27.904\n",
            "Worker 1, [04/08]: Training Loss: 2.686718028, Training Accuracy: 30.704\n",
            "Worker 1, [05/08]: Training Loss: 2.630946349, Training Accuracy: 32.080\n",
            "Worker 1, [06/08]: Training Loss: 2.565322567, Training Accuracy: 32.448\n",
            "Worker 1, [07/08]: Training Loss: 2.467238375, Training Accuracy: 35.152\n",
            "Worker 1, [08/08]: Training Loss: 2.438095076, Training Accuracy: 35.872\n",
            "Time taken for training worker 1: 0:00:24.146482\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.929816326, Training Accuracy: 26.656\n",
            "Worker 2, [02/08]: Training Loss: 2.822289104, Training Accuracy: 28.256\n",
            "Worker 2, [03/08]: Training Loss: 2.723226440, Training Accuracy: 29.584\n",
            "Worker 2, [04/08]: Training Loss: 2.652451398, Training Accuracy: 31.376\n",
            "Worker 2, [05/08]: Training Loss: 2.563951675, Training Accuracy: 33.392\n",
            "Worker 2, [06/08]: Training Loss: 2.497430592, Training Accuracy: 33.904\n",
            "Worker 2, [07/08]: Training Loss: 2.423254230, Training Accuracy: 36.208\n",
            "Worker 2, [08/08]: Training Loss: 2.381209789, Training Accuracy: 36.016\n",
            "Time taken for training worker 2: 0:00:25.309919\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.969460079, Training Accuracy: 25.376\n",
            "Worker 3, [02/08]: Training Loss: 2.864268045, Training Accuracy: 27.920\n",
            "Worker 3, [03/08]: Training Loss: 2.771305493, Training Accuracy: 29.808\n",
            "Worker 3, [04/08]: Training Loss: 2.702934966, Training Accuracy: 30.416\n",
            "Worker 3, [05/08]: Training Loss: 2.604263224, Training Accuracy: 32.512\n",
            "Worker 3, [06/08]: Training Loss: 2.542449759, Training Accuracy: 33.280\n",
            "Worker 3, [07/08]: Training Loss: 2.445727131, Training Accuracy: 34.864\n",
            "Worker 3, [08/08]: Training Loss: 2.410722922, Training Accuracy: 36.416\n",
            "Time taken for training worker 3: 0:00:23.409840\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.983938803, Training Accuracy: 26.224\n",
            "Worker 4, [02/08]: Training Loss: 2.838740972, Training Accuracy: 27.520\n",
            "Worker 4, [03/08]: Training Loss: 2.757988859, Training Accuracy: 29.136\n",
            "Worker 4, [04/08]: Training Loss: 2.705894735, Training Accuracy: 29.616\n",
            "Worker 4, [05/08]: Training Loss: 2.621711767, Training Accuracy: 31.872\n",
            "Worker 4, [06/08]: Training Loss: 2.534672457, Training Accuracy: 33.808\n",
            "Worker 4, [07/08]: Training Loss: 2.482068935, Training Accuracy: 34.912\n",
            "Worker 4, [08/08]: Training Loss: 2.399269810, Training Accuracy: 36.208\n",
            "Time taken for training worker 4: 0:00:23.829700\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 2.939804092, Training Accuracy: 26.256\n",
            "Worker 5, [02/08]: Training Loss: 2.842888752, Training Accuracy: 27.216\n",
            "Worker 5, [03/08]: Training Loss: 2.742222579, Training Accuracy: 29.824\n",
            "Worker 5, [04/08]: Training Loss: 2.659406521, Training Accuracy: 31.376\n",
            "Worker 5, [05/08]: Training Loss: 2.595676640, Training Accuracy: 32.816\n",
            "Worker 5, [06/08]: Training Loss: 2.508603528, Training Accuracy: 33.808\n",
            "Worker 5, [07/08]: Training Loss: 2.425671451, Training Accuracy: 34.736\n",
            "Worker 5, [08/08]: Training Loss: 2.390671494, Training Accuracy: 36.096\n",
            "Time taken for training worker 5: 0:00:24.577127\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 2.938574158, Training Accuracy: 25.584\n",
            "Worker 6, [02/08]: Training Loss: 2.846973334, Training Accuracy: 28.144\n",
            "Worker 6, [03/08]: Training Loss: 2.759403540, Training Accuracy: 29.056\n",
            "Worker 6, [04/08]: Training Loss: 2.702894388, Training Accuracy: 29.648\n",
            "Worker 6, [05/08]: Training Loss: 2.601962469, Training Accuracy: 32.400\n",
            "Worker 6, [06/08]: Training Loss: 2.531569067, Training Accuracy: 33.856\n",
            "Worker 6, [07/08]: Training Loss: 2.460528820, Training Accuracy: 35.232\n",
            "Worker 6, [08/08]: Training Loss: 2.363773600, Training Accuracy: 36.128\n",
            "Time taken for training worker 6: 0:00:25.704124\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 2.939532852, Training Accuracy: 26.528\n",
            "Worker 7, [02/08]: Training Loss: 2.850532427, Training Accuracy: 27.488\n",
            "Worker 7, [03/08]: Training Loss: 2.743359602, Training Accuracy: 29.744\n",
            "Worker 7, [04/08]: Training Loss: 2.664231909, Training Accuracy: 30.784\n",
            "Worker 7, [05/08]: Training Loss: 2.591917383, Training Accuracy: 32.752\n",
            "Worker 7, [06/08]: Training Loss: 2.514410070, Training Accuracy: 33.600\n",
            "Worker 7, [07/08]: Training Loss: 2.461312995, Training Accuracy: 34.816\n",
            "Worker 7, [08/08]: Training Loss: 2.359024926, Training Accuracy: 37.264\n",
            "Time taken for training worker 7: 0:00:24.875676\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.910228700, Training Accuracy: 26.384\n",
            "Worker 8, [02/08]: Training Loss: 2.841120581, Training Accuracy: 28.016\n",
            "Worker 8, [03/08]: Training Loss: 2.728200219, Training Accuracy: 29.776\n",
            "Worker 8, [04/08]: Training Loss: 2.644294449, Training Accuracy: 31.936\n",
            "Worker 8, [05/08]: Training Loss: 2.570509721, Training Accuracy: 33.232\n",
            "Worker 8, [06/08]: Training Loss: 2.519744177, Training Accuracy: 33.504\n",
            "Worker 8, [07/08]: Training Loss: 2.429111922, Training Accuracy: 35.664\n",
            "Worker 8, [08/08]: Training Loss: 2.370264061, Training Accuracy: 36.208\n",
            "Time taken for training worker 8: 0:00:24.741432\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004288\n",
            "Global Update 04: Test Loss: 2.530182343, Test Accuracy: 35.220\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.766684955, Training Accuracy: 29.984\n",
            "Worker 1, [02/08]: Training Loss: 2.608012435, Training Accuracy: 32.720\n",
            "Worker 1, [03/08]: Training Loss: 2.505354889, Training Accuracy: 34.000\n",
            "Worker 1, [04/08]: Training Loss: 2.424995590, Training Accuracy: 35.664\n",
            "Worker 1, [05/08]: Training Loss: 2.343759060, Training Accuracy: 37.600\n",
            "Worker 1, [06/08]: Training Loss: 2.296448014, Training Accuracy: 38.272\n",
            "Worker 1, [07/08]: Training Loss: 2.218100354, Training Accuracy: 39.584\n",
            "Worker 1, [08/08]: Training Loss: 2.149940133, Training Accuracy: 41.040\n",
            "Time taken for training worker 1: 0:00:25.023687\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.689907597, Training Accuracy: 31.472\n",
            "Worker 2, [02/08]: Training Loss: 2.536103358, Training Accuracy: 33.200\n",
            "Worker 2, [03/08]: Training Loss: 2.457772999, Training Accuracy: 35.472\n",
            "Worker 2, [04/08]: Training Loss: 2.381342708, Training Accuracy: 36.768\n",
            "Worker 2, [05/08]: Training Loss: 2.309918541, Training Accuracy: 38.464\n",
            "Worker 2, [06/08]: Training Loss: 2.245396786, Training Accuracy: 39.552\n",
            "Worker 2, [07/08]: Training Loss: 2.117199259, Training Accuracy: 42.352\n",
            "Worker 2, [08/08]: Training Loss: 2.113264741, Training Accuracy: 41.696\n",
            "Time taken for training worker 2: 0:00:23.999800\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.741071151, Training Accuracy: 30.240\n",
            "Worker 3, [02/08]: Training Loss: 2.610242474, Training Accuracy: 33.216\n",
            "Worker 3, [03/08]: Training Loss: 2.496352439, Training Accuracy: 35.104\n",
            "Worker 3, [04/08]: Training Loss: 2.425568862, Training Accuracy: 35.952\n",
            "Worker 3, [05/08]: Training Loss: 2.338863873, Training Accuracy: 37.216\n",
            "Worker 3, [06/08]: Training Loss: 2.277901651, Training Accuracy: 39.040\n",
            "Worker 3, [07/08]: Training Loss: 2.192149672, Training Accuracy: 40.656\n",
            "Worker 3, [08/08]: Training Loss: 2.106534937, Training Accuracy: 42.512\n",
            "Time taken for training worker 3: 0:00:23.912647\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.728709931, Training Accuracy: 29.904\n",
            "Worker 4, [02/08]: Training Loss: 2.604349917, Training Accuracy: 31.904\n",
            "Worker 4, [03/08]: Training Loss: 2.503562757, Training Accuracy: 34.416\n",
            "Worker 4, [04/08]: Training Loss: 2.431959194, Training Accuracy: 35.200\n",
            "Worker 4, [05/08]: Training Loss: 2.338065292, Training Accuracy: 37.616\n",
            "Worker 4, [06/08]: Training Loss: 2.267266996, Training Accuracy: 38.784\n",
            "Worker 4, [07/08]: Training Loss: 2.210357811, Training Accuracy: 40.768\n",
            "Worker 4, [08/08]: Training Loss: 2.168550316, Training Accuracy: 41.648\n",
            "Time taken for training worker 4: 0:00:25.009856\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 2.702115219, Training Accuracy: 31.728\n",
            "Worker 5, [02/08]: Training Loss: 2.599934753, Training Accuracy: 32.992\n",
            "Worker 5, [03/08]: Training Loss: 2.487980338, Training Accuracy: 35.200\n",
            "Worker 5, [04/08]: Training Loss: 2.389317609, Training Accuracy: 37.136\n",
            "Worker 5, [05/08]: Training Loss: 2.355993641, Training Accuracy: 37.488\n",
            "Worker 5, [06/08]: Training Loss: 2.264443797, Training Accuracy: 39.936\n",
            "Worker 5, [07/08]: Training Loss: 2.189534218, Training Accuracy: 40.832\n",
            "Worker 5, [08/08]: Training Loss: 2.143925622, Training Accuracy: 42.000\n",
            "Time taken for training worker 5: 0:00:24.435583\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 2.692564877, Training Accuracy: 30.560\n",
            "Worker 6, [02/08]: Training Loss: 2.588275863, Training Accuracy: 32.752\n",
            "Worker 6, [03/08]: Training Loss: 2.475835927, Training Accuracy: 34.944\n",
            "Worker 6, [04/08]: Training Loss: 2.413842644, Training Accuracy: 36.096\n",
            "Worker 6, [05/08]: Training Loss: 2.347863962, Training Accuracy: 37.248\n",
            "Worker 6, [06/08]: Training Loss: 2.224070083, Training Accuracy: 39.792\n",
            "Worker 6, [07/08]: Training Loss: 2.194212212, Training Accuracy: 39.984\n",
            "Worker 6, [08/08]: Training Loss: 2.114600460, Training Accuracy: 41.968\n",
            "Time taken for training worker 6: 0:00:23.827041\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 2.710499129, Training Accuracy: 30.736\n",
            "Worker 7, [02/08]: Training Loss: 2.568197625, Training Accuracy: 33.808\n",
            "Worker 7, [03/08]: Training Loss: 2.483288424, Training Accuracy: 34.848\n",
            "Worker 7, [04/08]: Training Loss: 2.423544869, Training Accuracy: 35.680\n",
            "Worker 7, [05/08]: Training Loss: 2.334451952, Training Accuracy: 37.488\n",
            "Worker 7, [06/08]: Training Loss: 2.248920109, Training Accuracy: 38.912\n",
            "Worker 7, [07/08]: Training Loss: 2.204957563, Training Accuracy: 40.064\n",
            "Worker 7, [08/08]: Training Loss: 2.144185985, Training Accuracy: 41.296\n",
            "Time taken for training worker 7: 0:00:24.616476\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.697282132, Training Accuracy: 30.368\n",
            "Worker 8, [02/08]: Training Loss: 2.582720353, Training Accuracy: 32.848\n",
            "Worker 8, [03/08]: Training Loss: 2.496468548, Training Accuracy: 34.960\n",
            "Worker 8, [04/08]: Training Loss: 2.410167683, Training Accuracy: 35.840\n",
            "Worker 8, [05/08]: Training Loss: 2.284728796, Training Accuracy: 38.880\n",
            "Worker 8, [06/08]: Training Loss: 2.255042853, Training Accuracy: 39.520\n",
            "Worker 8, [07/08]: Training Loss: 2.164342496, Training Accuracy: 41.344\n",
            "Worker 8, [08/08]: Training Loss: 2.128110181, Training Accuracy: 42.304\n",
            "Time taken for training worker 8: 0:00:25.420310\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004535\n",
            "Global Update 05: Test Loss: 2.379337876, Test Accuracy: 38.690\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.570131226, Training Accuracy: 33.472\n",
            "Worker 1, [02/08]: Training Loss: 2.414862144, Training Accuracy: 35.824\n",
            "Worker 1, [03/08]: Training Loss: 2.307587813, Training Accuracy: 38.304\n",
            "Worker 1, [04/08]: Training Loss: 2.214660085, Training Accuracy: 40.656\n",
            "Worker 1, [05/08]: Training Loss: 2.151701574, Training Accuracy: 41.888\n",
            "Worker 1, [06/08]: Training Loss: 2.062800652, Training Accuracy: 42.816\n",
            "Worker 1, [07/08]: Training Loss: 2.014703134, Training Accuracy: 44.176\n",
            "Worker 1, [08/08]: Training Loss: 1.920066881, Training Accuracy: 46.816\n",
            "Time taken for training worker 1: 0:00:23.416853\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.507352076, Training Accuracy: 34.512\n",
            "Worker 2, [02/08]: Training Loss: 2.375794174, Training Accuracy: 38.064\n",
            "Worker 2, [03/08]: Training Loss: 2.266029649, Training Accuracy: 39.008\n",
            "Worker 2, [04/08]: Training Loss: 2.185364261, Training Accuracy: 40.800\n",
            "Worker 2, [05/08]: Training Loss: 2.079913131, Training Accuracy: 42.864\n",
            "Worker 2, [06/08]: Training Loss: 2.014894701, Training Accuracy: 44.880\n",
            "Worker 2, [07/08]: Training Loss: 1.971651861, Training Accuracy: 45.280\n",
            "Worker 2, [08/08]: Training Loss: 1.878333913, Training Accuracy: 47.072\n",
            "Time taken for training worker 2: 0:00:24.733017\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.541776534, Training Accuracy: 34.256\n",
            "Worker 3, [02/08]: Training Loss: 2.421270180, Training Accuracy: 36.432\n",
            "Worker 3, [03/08]: Training Loss: 2.318378346, Training Accuracy: 38.720\n",
            "Worker 3, [04/08]: Training Loss: 2.210126641, Training Accuracy: 40.848\n",
            "Worker 3, [05/08]: Training Loss: 2.130411576, Training Accuracy: 42.576\n",
            "Worker 3, [06/08]: Training Loss: 2.044813624, Training Accuracy: 44.048\n",
            "Worker 3, [07/08]: Training Loss: 2.014763181, Training Accuracy: 45.552\n",
            "Worker 3, [08/08]: Training Loss: 1.930436953, Training Accuracy: 46.896\n",
            "Time taken for training worker 3: 0:00:24.428773\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.556118897, Training Accuracy: 34.336\n",
            "Worker 4, [02/08]: Training Loss: 2.411552585, Training Accuracy: 37.328\n",
            "Worker 4, [03/08]: Training Loss: 2.321767952, Training Accuracy: 39.136\n",
            "Worker 4, [04/08]: Training Loss: 2.223157363, Training Accuracy: 40.096\n",
            "Worker 4, [05/08]: Training Loss: 2.174700670, Training Accuracy: 41.648\n",
            "Worker 4, [06/08]: Training Loss: 2.042279127, Training Accuracy: 44.064\n",
            "Worker 4, [07/08]: Training Loss: 2.017267745, Training Accuracy: 44.784\n",
            "Worker 4, [08/08]: Training Loss: 1.927478365, Training Accuracy: 46.656\n",
            "Time taken for training worker 4: 0:00:25.371251\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 2.510687515, Training Accuracy: 34.384\n",
            "Worker 5, [02/08]: Training Loss: 2.395581834, Training Accuracy: 37.232\n",
            "Worker 5, [03/08]: Training Loss: 2.272268687, Training Accuracy: 38.880\n",
            "Worker 5, [04/08]: Training Loss: 2.212153455, Training Accuracy: 41.248\n",
            "Worker 5, [05/08]: Training Loss: 2.110252976, Training Accuracy: 41.536\n",
            "Worker 5, [06/08]: Training Loss: 2.019795338, Training Accuracy: 44.672\n",
            "Worker 5, [07/08]: Training Loss: 1.953701141, Training Accuracy: 46.032\n",
            "Worker 5, [08/08]: Training Loss: 1.940902232, Training Accuracy: 46.000\n",
            "Time taken for training worker 5: 0:00:24.630147\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 2.521860320, Training Accuracy: 34.240\n",
            "Worker 6, [02/08]: Training Loss: 2.357506688, Training Accuracy: 37.728\n",
            "Worker 6, [03/08]: Training Loss: 2.277662103, Training Accuracy: 38.752\n",
            "Worker 6, [04/08]: Training Loss: 2.203342188, Training Accuracy: 40.800\n",
            "Worker 6, [05/08]: Training Loss: 2.117455161, Training Accuracy: 42.560\n",
            "Worker 6, [06/08]: Training Loss: 2.028561435, Training Accuracy: 44.784\n",
            "Worker 6, [07/08]: Training Loss: 1.943798985, Training Accuracy: 46.816\n",
            "Worker 6, [08/08]: Training Loss: 1.920985195, Training Accuracy: 46.304\n",
            "Time taken for training worker 6: 0:00:23.940484\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 2.535329820, Training Accuracy: 34.800\n",
            "Worker 7, [02/08]: Training Loss: 2.385210163, Training Accuracy: 37.728\n",
            "Worker 7, [03/08]: Training Loss: 2.269136731, Training Accuracy: 39.824\n",
            "Worker 7, [04/08]: Training Loss: 2.202552321, Training Accuracy: 41.008\n",
            "Worker 7, [05/08]: Training Loss: 2.126781091, Training Accuracy: 42.528\n",
            "Worker 7, [06/08]: Training Loss: 2.045460553, Training Accuracy: 44.112\n",
            "Worker 7, [07/08]: Training Loss: 1.955141741, Training Accuracy: 46.336\n",
            "Worker 7, [08/08]: Training Loss: 1.896893726, Training Accuracy: 47.840\n",
            "Time taken for training worker 7: 0:00:24.276157\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.521653657, Training Accuracy: 33.936\n",
            "Worker 8, [02/08]: Training Loss: 2.362276064, Training Accuracy: 37.312\n",
            "Worker 8, [03/08]: Training Loss: 2.272825231, Training Accuracy: 39.152\n",
            "Worker 8, [04/08]: Training Loss: 2.203965880, Training Accuracy: 40.384\n",
            "Worker 8, [05/08]: Training Loss: 2.086140690, Training Accuracy: 42.320\n",
            "Worker 8, [06/08]: Training Loss: 2.018315900, Training Accuracy: 44.784\n",
            "Worker 8, [07/08]: Training Loss: 1.979625768, Training Accuracy: 45.568\n",
            "Worker 8, [08/08]: Training Loss: 1.899173745, Training Accuracy: 47.088\n",
            "Time taken for training worker 8: 0:00:24.610646\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004278\n",
            "Global Update 06: Test Loss: 2.286562654, Test Accuracy: 40.780\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.422432470, Training Accuracy: 36.992\n",
            "Worker 1, [02/08]: Training Loss: 2.227320593, Training Accuracy: 40.320\n",
            "Worker 1, [03/08]: Training Loss: 2.117012418, Training Accuracy: 42.816\n",
            "Worker 1, [04/08]: Training Loss: 2.053442908, Training Accuracy: 43.408\n",
            "Worker 1, [05/08]: Training Loss: 1.963123845, Training Accuracy: 45.936\n",
            "Worker 1, [06/08]: Training Loss: 1.895633659, Training Accuracy: 46.864\n",
            "Worker 1, [07/08]: Training Loss: 1.835891844, Training Accuracy: 48.736\n",
            "Worker 1, [08/08]: Training Loss: 1.750772529, Training Accuracy: 50.992\n",
            "Time taken for training worker 1: 0:00:24.141567\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.376534722, Training Accuracy: 37.520\n",
            "Worker 2, [02/08]: Training Loss: 2.165598496, Training Accuracy: 42.240\n",
            "Worker 2, [03/08]: Training Loss: 2.103144708, Training Accuracy: 42.672\n",
            "Worker 2, [04/08]: Training Loss: 1.976183748, Training Accuracy: 45.520\n",
            "Worker 2, [05/08]: Training Loss: 1.907440953, Training Accuracy: 47.856\n",
            "Worker 2, [06/08]: Training Loss: 1.842386764, Training Accuracy: 48.656\n",
            "Worker 2, [07/08]: Training Loss: 1.774708329, Training Accuracy: 51.136\n",
            "Worker 2, [08/08]: Training Loss: 1.737659533, Training Accuracy: 51.152\n",
            "Time taken for training worker 2: 0:00:24.065222\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.393450121, Training Accuracy: 38.240\n",
            "Worker 3, [02/08]: Training Loss: 2.242515088, Training Accuracy: 40.928\n",
            "Worker 3, [03/08]: Training Loss: 2.123867119, Training Accuracy: 42.304\n",
            "Worker 3, [04/08]: Training Loss: 2.017392650, Training Accuracy: 45.200\n",
            "Worker 3, [05/08]: Training Loss: 1.935024538, Training Accuracy: 47.280\n",
            "Worker 3, [06/08]: Training Loss: 1.892004742, Training Accuracy: 47.616\n",
            "Worker 3, [07/08]: Training Loss: 1.794513677, Training Accuracy: 49.792\n",
            "Worker 3, [08/08]: Training Loss: 1.750563731, Training Accuracy: 50.880\n",
            "Time taken for training worker 3: 0:00:23.594720\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.406812985, Training Accuracy: 37.248\n",
            "Worker 4, [02/08]: Training Loss: 2.256651661, Training Accuracy: 40.736\n",
            "Worker 4, [03/08]: Training Loss: 2.110976857, Training Accuracy: 42.880\n",
            "Worker 4, [04/08]: Training Loss: 2.034652787, Training Accuracy: 44.512\n",
            "Worker 4, [05/08]: Training Loss: 1.925106145, Training Accuracy: 46.160\n",
            "Worker 4, [06/08]: Training Loss: 1.898320081, Training Accuracy: 47.184\n",
            "Worker 4, [07/08]: Training Loss: 1.800497201, Training Accuracy: 49.984\n",
            "Worker 4, [08/08]: Training Loss: 1.734023280, Training Accuracy: 51.296\n",
            "Time taken for training worker 4: 0:00:24.079967\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 2.397262396, Training Accuracy: 37.408\n",
            "Worker 5, [02/08]: Training Loss: 2.203476867, Training Accuracy: 41.440\n",
            "Worker 5, [03/08]: Training Loss: 2.099496380, Training Accuracy: 44.016\n",
            "Worker 5, [04/08]: Training Loss: 2.007747132, Training Accuracy: 45.312\n",
            "Worker 5, [05/08]: Training Loss: 1.930288141, Training Accuracy: 47.104\n",
            "Worker 5, [06/08]: Training Loss: 1.859973616, Training Accuracy: 48.720\n",
            "Worker 5, [07/08]: Training Loss: 1.773862555, Training Accuracy: 50.336\n",
            "Worker 5, [08/08]: Training Loss: 1.731158136, Training Accuracy: 51.072\n",
            "Time taken for training worker 5: 0:00:24.478590\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 2.380445568, Training Accuracy: 37.312\n",
            "Worker 6, [02/08]: Training Loss: 2.221276603, Training Accuracy: 41.264\n",
            "Worker 6, [03/08]: Training Loss: 2.104869931, Training Accuracy: 42.384\n",
            "Worker 6, [04/08]: Training Loss: 2.011875665, Training Accuracy: 44.592\n",
            "Worker 6, [05/08]: Training Loss: 1.935039135, Training Accuracy: 47.456\n",
            "Worker 6, [06/08]: Training Loss: 1.842459069, Training Accuracy: 48.784\n",
            "Worker 6, [07/08]: Training Loss: 1.758753213, Training Accuracy: 49.984\n",
            "Worker 6, [08/08]: Training Loss: 1.718216119, Training Accuracy: 51.776\n",
            "Time taken for training worker 6: 0:00:24.890230\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 2.389817164, Training Accuracy: 37.744\n",
            "Worker 7, [02/08]: Training Loss: 2.239332936, Training Accuracy: 40.640\n",
            "Worker 7, [03/08]: Training Loss: 2.133768461, Training Accuracy: 43.424\n",
            "Worker 7, [04/08]: Training Loss: 1.996563882, Training Accuracy: 45.744\n",
            "Worker 7, [05/08]: Training Loss: 1.934838148, Training Accuracy: 46.304\n",
            "Worker 7, [06/08]: Training Loss: 1.869073941, Training Accuracy: 48.672\n",
            "Worker 7, [07/08]: Training Loss: 1.796697103, Training Accuracy: 50.272\n",
            "Worker 7, [08/08]: Training Loss: 1.719521509, Training Accuracy: 51.376\n",
            "Time taken for training worker 7: 0:00:24.679154\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.374978591, Training Accuracy: 37.744\n",
            "Worker 8, [02/08]: Training Loss: 2.227709788, Training Accuracy: 41.376\n",
            "Worker 8, [03/08]: Training Loss: 2.083929141, Training Accuracy: 44.176\n",
            "Worker 8, [04/08]: Training Loss: 1.996783317, Training Accuracy: 45.472\n",
            "Worker 8, [05/08]: Training Loss: 1.902466444, Training Accuracy: 46.640\n",
            "Worker 8, [06/08]: Training Loss: 1.828427241, Training Accuracy: 48.960\n",
            "Worker 8, [07/08]: Training Loss: 1.790084478, Training Accuracy: 49.808\n",
            "Worker 8, [08/08]: Training Loss: 1.729872274, Training Accuracy: 50.496\n",
            "Time taken for training worker 8: 0:00:24.909487\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004369\n",
            "Global Update 07: Test Loss: 2.227791734, Test Accuracy: 42.450\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.278276254, Training Accuracy: 40.144\n",
            "Worker 1, [02/08]: Training Loss: 2.113930949, Training Accuracy: 42.320\n",
            "Worker 1, [03/08]: Training Loss: 1.966858507, Training Accuracy: 46.384\n",
            "Worker 1, [04/08]: Training Loss: 1.869796813, Training Accuracy: 49.008\n",
            "Worker 1, [05/08]: Training Loss: 1.801942457, Training Accuracy: 49.472\n",
            "Worker 1, [06/08]: Training Loss: 1.686256618, Training Accuracy: 52.576\n",
            "Worker 1, [07/08]: Training Loss: 1.656822295, Training Accuracy: 53.184\n",
            "Worker 1, [08/08]: Training Loss: 1.599696122, Training Accuracy: 54.656\n",
            "Time taken for training worker 1: 0:00:24.103830\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.231041630, Training Accuracy: 40.800\n",
            "Worker 2, [02/08]: Training Loss: 2.042436754, Training Accuracy: 44.384\n",
            "Worker 2, [03/08]: Training Loss: 1.942992378, Training Accuracy: 46.096\n",
            "Worker 2, [04/08]: Training Loss: 1.825958503, Training Accuracy: 49.792\n",
            "Worker 2, [05/08]: Training Loss: 1.733461741, Training Accuracy: 51.136\n",
            "Worker 2, [06/08]: Training Loss: 1.657534244, Training Accuracy: 52.992\n",
            "Worker 2, [07/08]: Training Loss: 1.611965546, Training Accuracy: 53.408\n",
            "Worker 2, [08/08]: Training Loss: 1.540063000, Training Accuracy: 56.096\n",
            "Time taken for training worker 2: 0:00:24.103613\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.283595737, Training Accuracy: 39.488\n",
            "Worker 3, [02/08]: Training Loss: 2.094719681, Training Accuracy: 43.264\n",
            "Worker 3, [03/08]: Training Loss: 1.948007089, Training Accuracy: 46.448\n",
            "Worker 3, [04/08]: Training Loss: 1.864294181, Training Accuracy: 49.728\n",
            "Worker 3, [05/08]: Training Loss: 1.781195734, Training Accuracy: 50.704\n",
            "Worker 3, [06/08]: Training Loss: 1.701443558, Training Accuracy: 52.832\n",
            "Worker 3, [07/08]: Training Loss: 1.650201520, Training Accuracy: 53.504\n",
            "Worker 3, [08/08]: Training Loss: 1.576176586, Training Accuracy: 55.008\n",
            "Time taken for training worker 3: 0:00:24.955191\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.271710548, Training Accuracy: 40.656\n",
            "Worker 4, [02/08]: Training Loss: 2.093371776, Training Accuracy: 44.016\n",
            "Worker 4, [03/08]: Training Loss: 1.966824867, Training Accuracy: 46.160\n",
            "Worker 4, [04/08]: Training Loss: 1.886880922, Training Accuracy: 48.624\n",
            "Worker 4, [05/08]: Training Loss: 1.789379883, Training Accuracy: 50.160\n",
            "Worker 4, [06/08]: Training Loss: 1.693121171, Training Accuracy: 51.952\n",
            "Worker 4, [07/08]: Training Loss: 1.632298381, Training Accuracy: 53.856\n",
            "Worker 4, [08/08]: Training Loss: 1.574743396, Training Accuracy: 55.232\n",
            "Time taken for training worker 4: 0:00:24.870384\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 2.268214486, Training Accuracy: 39.808\n",
            "Worker 5, [02/08]: Training Loss: 2.080928939, Training Accuracy: 43.744\n",
            "Worker 5, [03/08]: Training Loss: 1.955447306, Training Accuracy: 46.256\n",
            "Worker 5, [04/08]: Training Loss: 1.858945733, Training Accuracy: 48.688\n",
            "Worker 5, [05/08]: Training Loss: 1.755586118, Training Accuracy: 51.680\n",
            "Worker 5, [06/08]: Training Loss: 1.700373150, Training Accuracy: 52.288\n",
            "Worker 5, [07/08]: Training Loss: 1.617832928, Training Accuracy: 54.096\n",
            "Worker 5, [08/08]: Training Loss: 1.567936493, Training Accuracy: 55.504\n",
            "Time taken for training worker 5: 0:00:23.376920\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 2.240140396, Training Accuracy: 40.032\n",
            "Worker 6, [02/08]: Training Loss: 2.031170496, Training Accuracy: 45.296\n",
            "Worker 6, [03/08]: Training Loss: 1.925612839, Training Accuracy: 46.896\n",
            "Worker 6, [04/08]: Training Loss: 1.855739530, Training Accuracy: 49.136\n",
            "Worker 6, [05/08]: Training Loss: 1.767308401, Training Accuracy: 50.496\n",
            "Worker 6, [06/08]: Training Loss: 1.694151489, Training Accuracy: 52.192\n",
            "Worker 6, [07/08]: Training Loss: 1.620397986, Training Accuracy: 53.232\n",
            "Worker 6, [08/08]: Training Loss: 1.561371395, Training Accuracy: 55.536\n",
            "Time taken for training worker 6: 0:00:25.785247\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 2.273094907, Training Accuracy: 40.064\n",
            "Worker 7, [02/08]: Training Loss: 2.067425081, Training Accuracy: 44.208\n",
            "Worker 7, [03/08]: Training Loss: 1.951904325, Training Accuracy: 46.448\n",
            "Worker 7, [04/08]: Training Loss: 1.874832320, Training Accuracy: 49.008\n",
            "Worker 7, [05/08]: Training Loss: 1.759927456, Training Accuracy: 51.088\n",
            "Worker 7, [06/08]: Training Loss: 1.693924249, Training Accuracy: 52.272\n",
            "Worker 7, [07/08]: Training Loss: 1.646908541, Training Accuracy: 54.096\n",
            "Worker 7, [08/08]: Training Loss: 1.582951473, Training Accuracy: 55.600\n",
            "Time taken for training worker 7: 0:00:24.456505\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.224208854, Training Accuracy: 40.960\n",
            "Worker 8, [02/08]: Training Loss: 2.063450465, Training Accuracy: 44.960\n",
            "Worker 8, [03/08]: Training Loss: 1.925610286, Training Accuracy: 47.216\n",
            "Worker 8, [04/08]: Training Loss: 1.840329969, Training Accuracy: 48.208\n",
            "Worker 8, [05/08]: Training Loss: 1.740065960, Training Accuracy: 50.864\n",
            "Worker 8, [06/08]: Training Loss: 1.685756782, Training Accuracy: 52.640\n",
            "Worker 8, [07/08]: Training Loss: 1.638411770, Training Accuracy: 53.984\n",
            "Worker 8, [08/08]: Training Loss: 1.543752083, Training Accuracy: 56.032\n",
            "Time taken for training worker 8: 0:00:24.010811\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.005006\n",
            "Global Update 08: Test Loss: 2.186967330, Test Accuracy: 43.800\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.180880714, Training Accuracy: 42.224\n",
            "Worker 1, [02/08]: Training Loss: 1.977698427, Training Accuracy: 46.544\n",
            "Worker 1, [03/08]: Training Loss: 1.842923961, Training Accuracy: 49.152\n",
            "Worker 1, [04/08]: Training Loss: 1.732111435, Training Accuracy: 51.360\n",
            "Worker 1, [05/08]: Training Loss: 1.636959140, Training Accuracy: 53.744\n",
            "Worker 1, [06/08]: Training Loss: 1.575157554, Training Accuracy: 54.448\n",
            "Worker 1, [07/08]: Training Loss: 1.468181490, Training Accuracy: 58.000\n",
            "Worker 1, [08/08]: Training Loss: 1.449637825, Training Accuracy: 58.832\n",
            "Time taken for training worker 1: 0:00:25.233625\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.111645999, Training Accuracy: 43.536\n",
            "Worker 2, [02/08]: Training Loss: 1.888317913, Training Accuracy: 47.872\n",
            "Worker 2, [03/08]: Training Loss: 1.831176246, Training Accuracy: 49.104\n",
            "Worker 2, [04/08]: Training Loss: 1.688723137, Training Accuracy: 52.944\n",
            "Worker 2, [05/08]: Training Loss: 1.605805742, Training Accuracy: 53.888\n",
            "Worker 2, [06/08]: Training Loss: 1.524639223, Training Accuracy: 56.256\n",
            "Worker 2, [07/08]: Training Loss: 1.479644631, Training Accuracy: 57.632\n",
            "Worker 2, [08/08]: Training Loss: 1.391412185, Training Accuracy: 59.504\n",
            "Time taken for training worker 2: 0:00:24.530476\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.159476065, Training Accuracy: 42.928\n",
            "Worker 3, [02/08]: Training Loss: 1.948445484, Training Accuracy: 46.416\n",
            "Worker 3, [03/08]: Training Loss: 1.809718683, Training Accuracy: 49.744\n",
            "Worker 3, [04/08]: Training Loss: 1.723575168, Training Accuracy: 51.920\n",
            "Worker 3, [05/08]: Training Loss: 1.624059323, Training Accuracy: 54.912\n",
            "Worker 3, [06/08]: Training Loss: 1.546442061, Training Accuracy: 56.496\n",
            "Worker 3, [07/08]: Training Loss: 1.476415056, Training Accuracy: 57.616\n",
            "Worker 3, [08/08]: Training Loss: 1.434653937, Training Accuracy: 59.584\n",
            "Time taken for training worker 3: 0:00:24.988978\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.181733734, Training Accuracy: 42.384\n",
            "Worker 4, [02/08]: Training Loss: 1.951809015, Training Accuracy: 47.104\n",
            "Worker 4, [03/08]: Training Loss: 1.823164164, Training Accuracy: 49.904\n",
            "Worker 4, [04/08]: Training Loss: 1.737047812, Training Accuracy: 52.112\n",
            "Worker 4, [05/08]: Training Loss: 1.635754371, Training Accuracy: 54.320\n",
            "Worker 4, [06/08]: Training Loss: 1.568931391, Training Accuracy: 54.928\n",
            "Worker 4, [07/08]: Training Loss: 1.479187880, Training Accuracy: 57.408\n",
            "Worker 4, [08/08]: Training Loss: 1.416675909, Training Accuracy: 60.080\n",
            "Time taken for training worker 4: 0:00:24.861149\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 2.129415832, Training Accuracy: 43.568\n",
            "Worker 5, [02/08]: Training Loss: 1.923489119, Training Accuracy: 47.712\n",
            "Worker 5, [03/08]: Training Loss: 1.833330899, Training Accuracy: 49.856\n",
            "Worker 5, [04/08]: Training Loss: 1.694374698, Training Accuracy: 52.944\n",
            "Worker 5, [05/08]: Training Loss: 1.620107735, Training Accuracy: 53.824\n",
            "Worker 5, [06/08]: Training Loss: 1.514525106, Training Accuracy: 57.664\n",
            "Worker 5, [07/08]: Training Loss: 1.497994315, Training Accuracy: 56.816\n",
            "Worker 5, [08/08]: Training Loss: 1.413247003, Training Accuracy: 59.536\n",
            "Time taken for training worker 5: 0:00:25.321955\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 2.118627942, Training Accuracy: 42.880\n",
            "Worker 6, [02/08]: Training Loss: 1.910404207, Training Accuracy: 47.264\n",
            "Worker 6, [03/08]: Training Loss: 1.802812759, Training Accuracy: 50.480\n",
            "Worker 6, [04/08]: Training Loss: 1.706120991, Training Accuracy: 52.080\n",
            "Worker 6, [05/08]: Training Loss: 1.612909971, Training Accuracy: 54.352\n",
            "Worker 6, [06/08]: Training Loss: 1.504541425, Training Accuracy: 56.448\n",
            "Worker 6, [07/08]: Training Loss: 1.458385435, Training Accuracy: 57.888\n",
            "Worker 6, [08/08]: Training Loss: 1.380118562, Training Accuracy: 59.888\n",
            "Time taken for training worker 6: 0:00:24.856432\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 2.150388849, Training Accuracy: 42.720\n",
            "Worker 7, [02/08]: Training Loss: 1.951960500, Training Accuracy: 46.544\n",
            "Worker 7, [03/08]: Training Loss: 1.811419840, Training Accuracy: 50.112\n",
            "Worker 7, [04/08]: Training Loss: 1.692442204, Training Accuracy: 52.064\n",
            "Worker 7, [05/08]: Training Loss: 1.619807674, Training Accuracy: 54.272\n",
            "Worker 7, [06/08]: Training Loss: 1.535904238, Training Accuracy: 56.512\n",
            "Worker 7, [07/08]: Training Loss: 1.465162423, Training Accuracy: 59.088\n",
            "Worker 7, [08/08]: Training Loss: 1.390213615, Training Accuracy: 59.904\n",
            "Time taken for training worker 7: 0:00:23.388893\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.095749115, Training Accuracy: 43.920\n",
            "Worker 8, [02/08]: Training Loss: 1.913391864, Training Accuracy: 47.392\n",
            "Worker 8, [03/08]: Training Loss: 1.796867088, Training Accuracy: 49.904\n",
            "Worker 8, [04/08]: Training Loss: 1.700325105, Training Accuracy: 52.576\n",
            "Worker 8, [05/08]: Training Loss: 1.603819122, Training Accuracy: 54.880\n",
            "Worker 8, [06/08]: Training Loss: 1.547479917, Training Accuracy: 56.288\n",
            "Worker 8, [07/08]: Training Loss: 1.460000755, Training Accuracy: 58.432\n",
            "Worker 8, [08/08]: Training Loss: 1.389953286, Training Accuracy: 59.680\n",
            "Time taken for training worker 8: 0:00:23.956596\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004572\n",
            "Global Update 09: Test Loss: 2.171508822, Test Accuracy: 44.690\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.050810201, Training Accuracy: 44.560\n",
            "Worker 1, [02/08]: Training Loss: 1.837660834, Training Accuracy: 49.120\n",
            "Worker 1, [03/08]: Training Loss: 1.711573012, Training Accuracy: 52.256\n",
            "Worker 1, [04/08]: Training Loss: 1.596193181, Training Accuracy: 54.480\n",
            "Worker 1, [05/08]: Training Loss: 1.530216110, Training Accuracy: 56.560\n",
            "Worker 1, [06/08]: Training Loss: 1.425519831, Training Accuracy: 59.264\n",
            "Worker 1, [07/08]: Training Loss: 1.347951010, Training Accuracy: 61.728\n",
            "Worker 1, [08/08]: Training Loss: 1.297046159, Training Accuracy: 61.984\n",
            "Time taken for training worker 1: 0:00:24.315739\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.994126564, Training Accuracy: 45.472\n",
            "Worker 2, [02/08]: Training Loss: 1.797083324, Training Accuracy: 50.656\n",
            "Worker 2, [03/08]: Training Loss: 1.651131728, Training Accuracy: 53.392\n",
            "Worker 2, [04/08]: Training Loss: 1.558713935, Training Accuracy: 56.112\n",
            "Worker 2, [05/08]: Training Loss: 1.453234395, Training Accuracy: 58.768\n",
            "Worker 2, [06/08]: Training Loss: 1.373088206, Training Accuracy: 60.544\n",
            "Worker 2, [07/08]: Training Loss: 1.323588642, Training Accuracy: 61.824\n",
            "Worker 2, [08/08]: Training Loss: 1.247685866, Training Accuracy: 63.312\n",
            "Time taken for training worker 2: 0:00:24.459678\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 2.044260906, Training Accuracy: 44.176\n",
            "Worker 3, [02/08]: Training Loss: 1.806317058, Training Accuracy: 50.384\n",
            "Worker 3, [03/08]: Training Loss: 1.693078496, Training Accuracy: 52.288\n",
            "Worker 3, [04/08]: Training Loss: 1.608182996, Training Accuracy: 54.832\n",
            "Worker 3, [05/08]: Training Loss: 1.495655212, Training Accuracy: 57.536\n",
            "Worker 3, [06/08]: Training Loss: 1.420281202, Training Accuracy: 59.616\n",
            "Worker 3, [07/08]: Training Loss: 1.370721926, Training Accuracy: 60.512\n",
            "Worker 3, [08/08]: Training Loss: 1.296121214, Training Accuracy: 62.800\n",
            "Time taken for training worker 3: 0:00:23.727215\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 2.055440512, Training Accuracy: 44.192\n",
            "Worker 4, [02/08]: Training Loss: 1.839037732, Training Accuracy: 49.024\n",
            "Worker 4, [03/08]: Training Loss: 1.678136928, Training Accuracy: 53.296\n",
            "Worker 4, [04/08]: Training Loss: 1.604311000, Training Accuracy: 55.232\n",
            "Worker 4, [05/08]: Training Loss: 1.534503732, Training Accuracy: 57.104\n",
            "Worker 4, [06/08]: Training Loss: 1.418199694, Training Accuracy: 59.888\n",
            "Worker 4, [07/08]: Training Loss: 1.332278747, Training Accuracy: 61.856\n",
            "Worker 4, [08/08]: Training Loss: 1.276950784, Training Accuracy: 63.616\n",
            "Time taken for training worker 4: 0:00:24.896154\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 2.022488185, Training Accuracy: 45.984\n",
            "Worker 5, [02/08]: Training Loss: 1.794672380, Training Accuracy: 50.480\n",
            "Worker 5, [03/08]: Training Loss: 1.687633382, Training Accuracy: 52.768\n",
            "Worker 5, [04/08]: Training Loss: 1.581029731, Training Accuracy: 55.792\n",
            "Worker 5, [05/08]: Training Loss: 1.488748995, Training Accuracy: 58.160\n",
            "Worker 5, [06/08]: Training Loss: 1.391590339, Training Accuracy: 60.272\n",
            "Worker 5, [07/08]: Training Loss: 1.328494373, Training Accuracy: 61.168\n",
            "Worker 5, [08/08]: Training Loss: 1.245650940, Training Accuracy: 64.432\n",
            "Time taken for training worker 5: 0:00:24.713090\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.982737906, Training Accuracy: 45.888\n",
            "Worker 6, [02/08]: Training Loss: 1.805590577, Training Accuracy: 50.304\n",
            "Worker 6, [03/08]: Training Loss: 1.652704786, Training Accuracy: 54.048\n",
            "Worker 6, [04/08]: Training Loss: 1.570225232, Training Accuracy: 55.648\n",
            "Worker 6, [05/08]: Training Loss: 1.445058421, Training Accuracy: 59.088\n",
            "Worker 6, [06/08]: Training Loss: 1.401040035, Training Accuracy: 59.616\n",
            "Worker 6, [07/08]: Training Loss: 1.329826948, Training Accuracy: 61.520\n",
            "Worker 6, [08/08]: Training Loss: 1.240095981, Training Accuracy: 63.872\n",
            "Time taken for training worker 6: 0:00:23.647224\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 2.046190936, Training Accuracy: 45.856\n",
            "Worker 7, [02/08]: Training Loss: 1.827100597, Training Accuracy: 49.632\n",
            "Worker 7, [03/08]: Training Loss: 1.686399208, Training Accuracy: 53.184\n",
            "Worker 7, [04/08]: Training Loss: 1.585651757, Training Accuracy: 55.376\n",
            "Worker 7, [05/08]: Training Loss: 1.498304092, Training Accuracy: 57.232\n",
            "Worker 7, [06/08]: Training Loss: 1.420860362, Training Accuracy: 59.968\n",
            "Worker 7, [07/08]: Training Loss: 1.335323600, Training Accuracy: 61.664\n",
            "Worker 7, [08/08]: Training Loss: 1.280598608, Training Accuracy: 63.040\n",
            "Time taken for training worker 7: 0:00:24.962521\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 2.015933840, Training Accuracy: 45.536\n",
            "Worker 8, [02/08]: Training Loss: 1.791981184, Training Accuracy: 50.608\n",
            "Worker 8, [03/08]: Training Loss: 1.670255163, Training Accuracy: 53.280\n",
            "Worker 8, [04/08]: Training Loss: 1.559125859, Training Accuracy: 55.568\n",
            "Worker 8, [05/08]: Training Loss: 1.465610985, Training Accuracy: 57.760\n",
            "Worker 8, [06/08]: Training Loss: 1.387315057, Training Accuracy: 60.048\n",
            "Worker 8, [07/08]: Training Loss: 1.318857772, Training Accuracy: 61.712\n",
            "Worker 8, [08/08]: Training Loss: 1.270617662, Training Accuracy: 63.072\n",
            "Time taken for training worker 8: 0:00:24.968497\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004385\n",
            "Global Update 10: Test Loss: 2.151475582, Test Accuracy: 45.710\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.980837354, Training Accuracy: 46.400\n",
            "Worker 1, [02/08]: Training Loss: 1.732410123, Training Accuracy: 51.952\n",
            "Worker 1, [03/08]: Training Loss: 1.583982583, Training Accuracy: 54.896\n",
            "Worker 1, [04/08]: Training Loss: 1.505647898, Training Accuracy: 57.808\n",
            "Worker 1, [05/08]: Training Loss: 1.393994743, Training Accuracy: 60.000\n",
            "Worker 1, [06/08]: Training Loss: 1.305275941, Training Accuracy: 61.712\n",
            "Worker 1, [07/08]: Training Loss: 1.220618146, Training Accuracy: 64.560\n",
            "Worker 1, [08/08]: Training Loss: 1.151711447, Training Accuracy: 66.368\n",
            "Time taken for training worker 1: 0:00:25.360347\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.885172546, Training Accuracy: 48.448\n",
            "Worker 2, [02/08]: Training Loss: 1.667109451, Training Accuracy: 53.408\n",
            "Worker 2, [03/08]: Training Loss: 1.535534473, Training Accuracy: 56.368\n",
            "Worker 2, [04/08]: Training Loss: 1.462203705, Training Accuracy: 58.352\n",
            "Worker 2, [05/08]: Training Loss: 1.334589721, Training Accuracy: 60.912\n",
            "Worker 2, [06/08]: Training Loss: 1.246950947, Training Accuracy: 63.632\n",
            "Worker 2, [07/08]: Training Loss: 1.192254034, Training Accuracy: 65.104\n",
            "Worker 2, [08/08]: Training Loss: 1.135562697, Training Accuracy: 67.712\n",
            "Time taken for training worker 2: 0:00:24.483330\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.939740997, Training Accuracy: 48.080\n",
            "Worker 3, [02/08]: Training Loss: 1.699160406, Training Accuracy: 52.880\n",
            "Worker 3, [03/08]: Training Loss: 1.548923110, Training Accuracy: 56.416\n",
            "Worker 3, [04/08]: Training Loss: 1.469573534, Training Accuracy: 58.192\n",
            "Worker 3, [05/08]: Training Loss: 1.355599225, Training Accuracy: 61.040\n",
            "Worker 3, [06/08]: Training Loss: 1.282038143, Training Accuracy: 63.344\n",
            "Worker 3, [07/08]: Training Loss: 1.220334894, Training Accuracy: 64.256\n",
            "Worker 3, [08/08]: Training Loss: 1.164831872, Training Accuracy: 66.240\n",
            "Time taken for training worker 3: 0:00:24.064311\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.961097826, Training Accuracy: 47.104\n",
            "Worker 4, [02/08]: Training Loss: 1.746327454, Training Accuracy: 51.872\n",
            "Worker 4, [03/08]: Training Loss: 1.592562585, Training Accuracy: 55.760\n",
            "Worker 4, [04/08]: Training Loss: 1.478167493, Training Accuracy: 57.968\n",
            "Worker 4, [05/08]: Training Loss: 1.394464771, Training Accuracy: 60.208\n",
            "Worker 4, [06/08]: Training Loss: 1.275213315, Training Accuracy: 63.408\n",
            "Worker 4, [07/08]: Training Loss: 1.237115726, Training Accuracy: 64.016\n",
            "Worker 4, [08/08]: Training Loss: 1.157992576, Training Accuracy: 66.608\n",
            "Time taken for training worker 4: 0:00:24.298758\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.929358912, Training Accuracy: 48.464\n",
            "Worker 5, [02/08]: Training Loss: 1.701871704, Training Accuracy: 52.720\n",
            "Worker 5, [03/08]: Training Loss: 1.578436487, Training Accuracy: 55.792\n",
            "Worker 5, [04/08]: Training Loss: 1.477496649, Training Accuracy: 57.376\n",
            "Worker 5, [05/08]: Training Loss: 1.387651183, Training Accuracy: 59.648\n",
            "Worker 5, [06/08]: Training Loss: 1.288315059, Training Accuracy: 63.168\n",
            "Worker 5, [07/08]: Training Loss: 1.202150024, Training Accuracy: 66.000\n",
            "Worker 5, [08/08]: Training Loss: 1.150694836, Training Accuracy: 66.416\n",
            "Time taken for training worker 5: 0:00:25.079091\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.913795517, Training Accuracy: 47.008\n",
            "Worker 6, [02/08]: Training Loss: 1.704373835, Training Accuracy: 52.864\n",
            "Worker 6, [03/08]: Training Loss: 1.528639093, Training Accuracy: 57.296\n",
            "Worker 6, [04/08]: Training Loss: 1.446511316, Training Accuracy: 58.496\n",
            "Worker 6, [05/08]: Training Loss: 1.358144711, Training Accuracy: 60.688\n",
            "Worker 6, [06/08]: Training Loss: 1.253848642, Training Accuracy: 63.712\n",
            "Worker 6, [07/08]: Training Loss: 1.181392065, Training Accuracy: 65.920\n",
            "Worker 6, [08/08]: Training Loss: 1.141010139, Training Accuracy: 66.560\n",
            "Time taken for training worker 6: 0:00:23.549396\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.923764228, Training Accuracy: 48.064\n",
            "Worker 7, [02/08]: Training Loss: 1.724545498, Training Accuracy: 52.144\n",
            "Worker 7, [03/08]: Training Loss: 1.560411824, Training Accuracy: 55.936\n",
            "Worker 7, [04/08]: Training Loss: 1.455271340, Training Accuracy: 59.136\n",
            "Worker 7, [05/08]: Training Loss: 1.362094605, Training Accuracy: 60.704\n",
            "Worker 7, [06/08]: Training Loss: 1.299577987, Training Accuracy: 61.872\n",
            "Worker 7, [07/08]: Training Loss: 1.213918650, Training Accuracy: 64.720\n",
            "Worker 7, [08/08]: Training Loss: 1.176077523, Training Accuracy: 65.696\n",
            "Time taken for training worker 7: 0:00:24.370315\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.894797268, Training Accuracy: 47.856\n",
            "Worker 8, [02/08]: Training Loss: 1.679691702, Training Accuracy: 53.024\n",
            "Worker 8, [03/08]: Training Loss: 1.540851453, Training Accuracy: 55.904\n",
            "Worker 8, [04/08]: Training Loss: 1.457005039, Training Accuracy: 58.000\n",
            "Worker 8, [05/08]: Training Loss: 1.354952695, Training Accuracy: 60.128\n",
            "Worker 8, [06/08]: Training Loss: 1.287447550, Training Accuracy: 62.672\n",
            "Worker 8, [07/08]: Training Loss: 1.188269374, Training Accuracy: 65.856\n",
            "Worker 8, [08/08]: Training Loss: 1.154042759, Training Accuracy: 66.992\n",
            "Time taken for training worker 8: 0:00:24.928077\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004057\n",
            "Global Update 11: Test Loss: 2.156329425, Test Accuracy: 46.560\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.858910923, Training Accuracy: 49.056\n",
            "Worker 1, [02/08]: Training Loss: 1.627575211, Training Accuracy: 54.336\n",
            "Worker 1, [03/08]: Training Loss: 1.473076436, Training Accuracy: 58.224\n",
            "Worker 1, [04/08]: Training Loss: 1.367910016, Training Accuracy: 60.656\n",
            "Worker 1, [05/08]: Training Loss: 1.278290794, Training Accuracy: 62.640\n",
            "Worker 1, [06/08]: Training Loss: 1.188804801, Training Accuracy: 65.984\n",
            "Worker 1, [07/08]: Training Loss: 1.150126694, Training Accuracy: 66.272\n",
            "Worker 1, [08/08]: Training Loss: 1.059834582, Training Accuracy: 69.120\n",
            "Time taken for training worker 1: 0:00:23.885780\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.804762998, Training Accuracy: 49.408\n",
            "Worker 2, [02/08]: Training Loss: 1.577514308, Training Accuracy: 55.488\n",
            "Worker 2, [03/08]: Training Loss: 1.444722556, Training Accuracy: 58.096\n",
            "Worker 2, [04/08]: Training Loss: 1.337843734, Training Accuracy: 61.376\n",
            "Worker 2, [05/08]: Training Loss: 1.231557942, Training Accuracy: 64.304\n",
            "Worker 2, [06/08]: Training Loss: 1.162237491, Training Accuracy: 65.856\n",
            "Worker 2, [07/08]: Training Loss: 1.082073858, Training Accuracy: 68.496\n",
            "Worker 2, [08/08]: Training Loss: 1.012617805, Training Accuracy: 70.448\n",
            "Time taken for training worker 2: 0:00:25.476578\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.870448917, Training Accuracy: 49.392\n",
            "Worker 3, [02/08]: Training Loss: 1.582151514, Training Accuracy: 56.144\n",
            "Worker 3, [03/08]: Training Loss: 1.507612675, Training Accuracy: 57.040\n",
            "Worker 3, [04/08]: Training Loss: 1.373097790, Training Accuracy: 60.816\n",
            "Worker 3, [05/08]: Training Loss: 1.293020396, Training Accuracy: 63.840\n",
            "Worker 3, [06/08]: Training Loss: 1.193845237, Training Accuracy: 66.064\n",
            "Worker 3, [07/08]: Training Loss: 1.128141775, Training Accuracy: 67.600\n",
            "Worker 3, [08/08]: Training Loss: 1.040835849, Training Accuracy: 69.824\n",
            "Time taken for training worker 3: 0:00:23.809422\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.860869336, Training Accuracy: 49.072\n",
            "Worker 4, [02/08]: Training Loss: 1.614037674, Training Accuracy: 54.768\n",
            "Worker 4, [03/08]: Training Loss: 1.483322127, Training Accuracy: 57.776\n",
            "Worker 4, [04/08]: Training Loss: 1.385422224, Training Accuracy: 61.024\n",
            "Worker 4, [05/08]: Training Loss: 1.285619682, Training Accuracy: 63.344\n",
            "Worker 4, [06/08]: Training Loss: 1.194601403, Training Accuracy: 66.048\n",
            "Worker 4, [07/08]: Training Loss: 1.127458710, Training Accuracy: 67.312\n",
            "Worker 4, [08/08]: Training Loss: 1.061023447, Training Accuracy: 69.648\n",
            "Time taken for training worker 4: 0:00:23.895383\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.846243413, Training Accuracy: 49.264\n",
            "Worker 5, [02/08]: Training Loss: 1.602183104, Training Accuracy: 55.120\n",
            "Worker 5, [03/08]: Training Loss: 1.445483172, Training Accuracy: 59.744\n",
            "Worker 5, [04/08]: Training Loss: 1.377188792, Training Accuracy: 60.864\n",
            "Worker 5, [05/08]: Training Loss: 1.289278258, Training Accuracy: 62.832\n",
            "Worker 5, [06/08]: Training Loss: 1.169777691, Training Accuracy: 66.032\n",
            "Worker 5, [07/08]: Training Loss: 1.129337600, Training Accuracy: 67.248\n",
            "Worker 5, [08/08]: Training Loss: 1.083893347, Training Accuracy: 68.160\n",
            "Time taken for training worker 5: 0:00:24.883613\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.818533272, Training Accuracy: 49.008\n",
            "Worker 6, [02/08]: Training Loss: 1.560458159, Training Accuracy: 55.808\n",
            "Worker 6, [03/08]: Training Loss: 1.436240638, Training Accuracy: 58.688\n",
            "Worker 6, [04/08]: Training Loss: 1.333682785, Training Accuracy: 62.000\n",
            "Worker 6, [05/08]: Training Loss: 1.222870793, Training Accuracy: 64.064\n",
            "Worker 6, [06/08]: Training Loss: 1.151074496, Training Accuracy: 65.920\n",
            "Worker 6, [07/08]: Training Loss: 1.099868855, Training Accuracy: 67.504\n",
            "Worker 6, [08/08]: Training Loss: 1.037086234, Training Accuracy: 69.440\n",
            "Time taken for training worker 6: 0:00:26.153842\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.860792816, Training Accuracy: 49.760\n",
            "Worker 7, [02/08]: Training Loss: 1.592264751, Training Accuracy: 55.344\n",
            "Worker 7, [03/08]: Training Loss: 1.478920777, Training Accuracy: 58.096\n",
            "Worker 7, [04/08]: Training Loss: 1.382173749, Training Accuracy: 60.656\n",
            "Worker 7, [05/08]: Training Loss: 1.256441802, Training Accuracy: 63.952\n",
            "Worker 7, [06/08]: Training Loss: 1.212608348, Training Accuracy: 64.848\n",
            "Worker 7, [07/08]: Training Loss: 1.121940878, Training Accuracy: 67.584\n",
            "Worker 7, [08/08]: Training Loss: 1.079446216, Training Accuracy: 69.184\n",
            "Time taken for training worker 7: 0:00:24.797866\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.812648939, Training Accuracy: 50.256\n",
            "Worker 8, [02/08]: Training Loss: 1.595608820, Training Accuracy: 54.992\n",
            "Worker 8, [03/08]: Training Loss: 1.441093871, Training Accuracy: 59.184\n",
            "Worker 8, [04/08]: Training Loss: 1.319213000, Training Accuracy: 62.016\n",
            "Worker 8, [05/08]: Training Loss: 1.220013032, Training Accuracy: 64.640\n",
            "Worker 8, [06/08]: Training Loss: 1.189487444, Training Accuracy: 65.344\n",
            "Worker 8, [07/08]: Training Loss: 1.106773616, Training Accuracy: 67.856\n",
            "Worker 8, [08/08]: Training Loss: 1.040237725, Training Accuracy: 69.440\n",
            "Time taken for training worker 8: 0:00:24.007895\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004275\n",
            "Global Update 12: Test Loss: 2.166769798, Test Accuracy: 46.920\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.735338795, Training Accuracy: 51.456\n",
            "Worker 1, [02/08]: Training Loss: 1.539670381, Training Accuracy: 56.368\n",
            "Worker 1, [03/08]: Training Loss: 1.385042922, Training Accuracy: 60.736\n",
            "Worker 1, [04/08]: Training Loss: 1.284552980, Training Accuracy: 63.328\n",
            "Worker 1, [05/08]: Training Loss: 1.205114635, Training Accuracy: 65.600\n",
            "Worker 1, [06/08]: Training Loss: 1.133872266, Training Accuracy: 66.800\n",
            "Worker 1, [07/08]: Training Loss: 1.051576215, Training Accuracy: 69.232\n",
            "Worker 1, [08/08]: Training Loss: 0.981923361, Training Accuracy: 71.472\n",
            "Time taken for training worker 1: 0:00:24.795125\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.717457016, Training Accuracy: 52.208\n",
            "Worker 2, [02/08]: Training Loss: 1.505105417, Training Accuracy: 57.520\n",
            "Worker 2, [03/08]: Training Loss: 1.370225194, Training Accuracy: 61.056\n",
            "Worker 2, [04/08]: Training Loss: 1.247255184, Training Accuracy: 64.624\n",
            "Worker 2, [05/08]: Training Loss: 1.152339880, Training Accuracy: 66.224\n",
            "Worker 2, [06/08]: Training Loss: 1.098759077, Training Accuracy: 67.472\n",
            "Worker 2, [07/08]: Training Loss: 1.059786052, Training Accuracy: 68.832\n",
            "Worker 2, [08/08]: Training Loss: 0.990407393, Training Accuracy: 70.688\n",
            "Time taken for training worker 2: 0:00:24.246899\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.780189719, Training Accuracy: 51.008\n",
            "Worker 3, [02/08]: Training Loss: 1.511626604, Training Accuracy: 57.280\n",
            "Worker 3, [03/08]: Training Loss: 1.382116915, Training Accuracy: 61.168\n",
            "Worker 3, [04/08]: Training Loss: 1.274075085, Training Accuracy: 63.520\n",
            "Worker 3, [05/08]: Training Loss: 1.202022325, Training Accuracy: 65.808\n",
            "Worker 3, [06/08]: Training Loss: 1.103894436, Training Accuracy: 68.624\n",
            "Worker 3, [07/08]: Training Loss: 1.047439244, Training Accuracy: 69.856\n",
            "Worker 3, [08/08]: Training Loss: 0.992067105, Training Accuracy: 71.216\n",
            "Time taken for training worker 3: 0:00:24.933566\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.763283439, Training Accuracy: 51.296\n",
            "Worker 4, [02/08]: Training Loss: 1.521136603, Training Accuracy: 57.984\n",
            "Worker 4, [03/08]: Training Loss: 1.382029415, Training Accuracy: 60.432\n",
            "Worker 4, [04/08]: Training Loss: 1.295691872, Training Accuracy: 63.120\n",
            "Worker 4, [05/08]: Training Loss: 1.185927124, Training Accuracy: 66.096\n",
            "Worker 4, [06/08]: Training Loss: 1.120451512, Training Accuracy: 67.904\n",
            "Worker 4, [07/08]: Training Loss: 1.045127152, Training Accuracy: 69.280\n",
            "Worker 4, [08/08]: Training Loss: 1.021219213, Training Accuracy: 70.464\n",
            "Time taken for training worker 4: 0:00:25.237582\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.745847316, Training Accuracy: 52.000\n",
            "Worker 5, [02/08]: Training Loss: 1.517508461, Training Accuracy: 57.360\n",
            "Worker 5, [03/08]: Training Loss: 1.379272715, Training Accuracy: 60.976\n",
            "Worker 5, [04/08]: Training Loss: 1.274265722, Training Accuracy: 63.376\n",
            "Worker 5, [05/08]: Training Loss: 1.189671733, Training Accuracy: 65.920\n",
            "Worker 5, [06/08]: Training Loss: 1.106181032, Training Accuracy: 67.584\n",
            "Worker 5, [07/08]: Training Loss: 1.052526366, Training Accuracy: 69.744\n",
            "Worker 5, [08/08]: Training Loss: 1.007554267, Training Accuracy: 71.072\n",
            "Time taken for training worker 5: 0:00:24.993726\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.729958324, Training Accuracy: 51.104\n",
            "Worker 6, [02/08]: Training Loss: 1.494853062, Training Accuracy: 58.144\n",
            "Worker 6, [03/08]: Training Loss: 1.370367137, Training Accuracy: 60.864\n",
            "Worker 6, [04/08]: Training Loss: 1.267662884, Training Accuracy: 62.928\n",
            "Worker 6, [05/08]: Training Loss: 1.163503952, Training Accuracy: 65.936\n",
            "Worker 6, [06/08]: Training Loss: 1.061152855, Training Accuracy: 69.024\n",
            "Worker 6, [07/08]: Training Loss: 1.017539879, Training Accuracy: 70.576\n",
            "Worker 6, [08/08]: Training Loss: 0.969707001, Training Accuracy: 71.904\n",
            "Time taken for training worker 6: 0:00:23.634778\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.734606216, Training Accuracy: 52.336\n",
            "Worker 7, [02/08]: Training Loss: 1.520558403, Training Accuracy: 57.376\n",
            "Worker 7, [03/08]: Training Loss: 1.387965773, Training Accuracy: 60.432\n",
            "Worker 7, [04/08]: Training Loss: 1.282885804, Training Accuracy: 63.984\n",
            "Worker 7, [05/08]: Training Loss: 1.216859604, Training Accuracy: 64.720\n",
            "Worker 7, [06/08]: Training Loss: 1.118913532, Training Accuracy: 67.856\n",
            "Worker 7, [07/08]: Training Loss: 1.053866042, Training Accuracy: 69.232\n",
            "Worker 7, [08/08]: Training Loss: 0.999289989, Training Accuracy: 71.456\n",
            "Time taken for training worker 7: 0:00:23.691429\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.713536117, Training Accuracy: 52.752\n",
            "Worker 8, [02/08]: Training Loss: 1.502757875, Training Accuracy: 57.728\n",
            "Worker 8, [03/08]: Training Loss: 1.363330029, Training Accuracy: 61.136\n",
            "Worker 8, [04/08]: Training Loss: 1.254539659, Training Accuracy: 63.984\n",
            "Worker 8, [05/08]: Training Loss: 1.173332276, Training Accuracy: 66.032\n",
            "Worker 8, [06/08]: Training Loss: 1.119034866, Training Accuracy: 67.312\n",
            "Worker 8, [07/08]: Training Loss: 1.037067262, Training Accuracy: 69.312\n",
            "Worker 8, [08/08]: Training Loss: 0.982862116, Training Accuracy: 70.896\n",
            "Time taken for training worker 8: 0:00:24.840516\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004042\n",
            "Global Update 13: Test Loss: 2.183688310, Test Accuracy: 47.260\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.684928701, Training Accuracy: 52.944\n",
            "Worker 1, [02/08]: Training Loss: 1.476023721, Training Accuracy: 57.936\n",
            "Worker 1, [03/08]: Training Loss: 1.334724798, Training Accuracy: 62.288\n",
            "Worker 1, [04/08]: Training Loss: 1.241715424, Training Accuracy: 64.048\n",
            "Worker 1, [05/08]: Training Loss: 1.168496524, Training Accuracy: 66.608\n",
            "Worker 1, [06/08]: Training Loss: 1.089777046, Training Accuracy: 68.720\n",
            "Worker 1, [07/08]: Training Loss: 1.031573049, Training Accuracy: 70.032\n",
            "Worker 1, [08/08]: Training Loss: 0.959747522, Training Accuracy: 72.480\n",
            "Time taken for training worker 1: 0:00:25.292190\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.623310255, Training Accuracy: 54.512\n",
            "Worker 2, [02/08]: Training Loss: 1.419447537, Training Accuracy: 59.744\n",
            "Worker 2, [03/08]: Training Loss: 1.302820336, Training Accuracy: 62.512\n",
            "Worker 2, [04/08]: Training Loss: 1.183021248, Training Accuracy: 65.968\n",
            "Worker 2, [05/08]: Training Loss: 1.104535754, Training Accuracy: 68.400\n",
            "Worker 2, [06/08]: Training Loss: 1.058137343, Training Accuracy: 69.088\n",
            "Worker 2, [07/08]: Training Loss: 0.983417862, Training Accuracy: 71.536\n",
            "Worker 2, [08/08]: Training Loss: 0.954339564, Training Accuracy: 72.336\n",
            "Time taken for training worker 2: 0:00:24.523140\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.658717286, Training Accuracy: 53.664\n",
            "Worker 3, [02/08]: Training Loss: 1.450890236, Training Accuracy: 59.184\n",
            "Worker 3, [03/08]: Training Loss: 1.310290340, Training Accuracy: 63.232\n",
            "Worker 3, [04/08]: Training Loss: 1.216640732, Training Accuracy: 65.680\n",
            "Worker 3, [05/08]: Training Loss: 1.150371322, Training Accuracy: 66.992\n",
            "Worker 3, [06/08]: Training Loss: 1.066368441, Training Accuracy: 69.488\n",
            "Worker 3, [07/08]: Training Loss: 1.018596832, Training Accuracy: 70.832\n",
            "Worker 3, [08/08]: Training Loss: 0.966852614, Training Accuracy: 72.480\n",
            "Time taken for training worker 3: 0:00:24.516417\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.669916674, Training Accuracy: 53.216\n",
            "Worker 4, [02/08]: Training Loss: 1.451104252, Training Accuracy: 59.232\n",
            "Worker 4, [03/08]: Training Loss: 1.337755622, Training Accuracy: 61.536\n",
            "Worker 4, [04/08]: Training Loss: 1.227959870, Training Accuracy: 65.024\n",
            "Worker 4, [05/08]: Training Loss: 1.152010972, Training Accuracy: 67.280\n",
            "Worker 4, [06/08]: Training Loss: 1.100938258, Training Accuracy: 68.784\n",
            "Worker 4, [07/08]: Training Loss: 1.044849623, Training Accuracy: 69.536\n",
            "Worker 4, [08/08]: Training Loss: 0.953824289, Training Accuracy: 72.544\n",
            "Time taken for training worker 4: 0:00:25.051118\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.673064712, Training Accuracy: 52.960\n",
            "Worker 5, [02/08]: Training Loss: 1.440435721, Training Accuracy: 59.840\n",
            "Worker 5, [03/08]: Training Loss: 1.315723476, Training Accuracy: 61.808\n",
            "Worker 5, [04/08]: Training Loss: 1.225760406, Training Accuracy: 64.800\n",
            "Worker 5, [05/08]: Training Loss: 1.146643141, Training Accuracy: 67.312\n",
            "Worker 5, [06/08]: Training Loss: 1.076718222, Training Accuracy: 68.800\n",
            "Worker 5, [07/08]: Training Loss: 1.008262510, Training Accuracy: 71.200\n",
            "Worker 5, [08/08]: Training Loss: 0.975117604, Training Accuracy: 71.568\n",
            "Time taken for training worker 5: 0:00:23.892721\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.627619974, Training Accuracy: 54.384\n",
            "Worker 6, [02/08]: Training Loss: 1.426425986, Training Accuracy: 59.792\n",
            "Worker 6, [03/08]: Training Loss: 1.296058697, Training Accuracy: 63.136\n",
            "Worker 6, [04/08]: Training Loss: 1.205392832, Training Accuracy: 65.600\n",
            "Worker 6, [05/08]: Training Loss: 1.110342478, Training Accuracy: 68.176\n",
            "Worker 6, [06/08]: Training Loss: 1.068368887, Training Accuracy: 69.152\n",
            "Worker 6, [07/08]: Training Loss: 0.982009485, Training Accuracy: 71.744\n",
            "Worker 6, [08/08]: Training Loss: 0.946665165, Training Accuracy: 72.288\n",
            "Time taken for training worker 6: 0:00:24.963640\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.679152524, Training Accuracy: 53.536\n",
            "Worker 7, [02/08]: Training Loss: 1.439595332, Training Accuracy: 59.536\n",
            "Worker 7, [03/08]: Training Loss: 1.318292508, Training Accuracy: 62.384\n",
            "Worker 7, [04/08]: Training Loss: 1.227078971, Training Accuracy: 65.136\n",
            "Worker 7, [05/08]: Training Loss: 1.132873558, Training Accuracy: 67.376\n",
            "Worker 7, [06/08]: Training Loss: 1.083641396, Training Accuracy: 68.976\n",
            "Worker 7, [07/08]: Training Loss: 1.016085816, Training Accuracy: 70.592\n",
            "Worker 7, [08/08]: Training Loss: 0.942674949, Training Accuracy: 72.432\n",
            "Time taken for training worker 7: 0:00:23.814277\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.628144882, Training Accuracy: 54.608\n",
            "Worker 8, [02/08]: Training Loss: 1.416891481, Training Accuracy: 60.080\n",
            "Worker 8, [03/08]: Training Loss: 1.292475109, Training Accuracy: 62.832\n",
            "Worker 8, [04/08]: Training Loss: 1.220260884, Training Accuracy: 65.328\n",
            "Worker 8, [05/08]: Training Loss: 1.112435887, Training Accuracy: 67.904\n",
            "Worker 8, [06/08]: Training Loss: 1.058356782, Training Accuracy: 69.216\n",
            "Worker 8, [07/08]: Training Loss: 0.985399157, Training Accuracy: 70.688\n",
            "Worker 8, [08/08]: Training Loss: 0.917836920, Training Accuracy: 73.680\n",
            "Time taken for training worker 8: 0:00:25.350753\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.006082\n",
            "Global Update 14: Test Loss: 2.177643786, Test Accuracy: 47.420\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.596759744, Training Accuracy: 55.248\n",
            "Worker 1, [02/08]: Training Loss: 1.427043016, Training Accuracy: 59.952\n",
            "Worker 1, [03/08]: Training Loss: 1.291220810, Training Accuracy: 62.720\n",
            "Worker 1, [04/08]: Training Loss: 1.213326162, Training Accuracy: 64.864\n",
            "Worker 1, [05/08]: Training Loss: 1.148647794, Training Accuracy: 66.544\n",
            "Worker 1, [06/08]: Training Loss: 1.096713361, Training Accuracy: 69.040\n",
            "Worker 1, [07/08]: Training Loss: 1.027692458, Training Accuracy: 70.336\n",
            "Worker 1, [08/08]: Training Loss: 0.992531529, Training Accuracy: 71.664\n",
            "Time taken for training worker 1: 0:00:24.227904\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.542074793, Training Accuracy: 56.160\n",
            "Worker 2, [02/08]: Training Loss: 1.343138190, Training Accuracy: 60.944\n",
            "Worker 2, [03/08]: Training Loss: 1.256366870, Training Accuracy: 63.680\n",
            "Worker 2, [04/08]: Training Loss: 1.175875110, Training Accuracy: 66.048\n",
            "Worker 2, [05/08]: Training Loss: 1.099661139, Training Accuracy: 68.800\n",
            "Worker 2, [06/08]: Training Loss: 1.042595877, Training Accuracy: 70.416\n",
            "Worker 2, [07/08]: Training Loss: 1.008809767, Training Accuracy: 71.264\n",
            "Worker 2, [08/08]: Training Loss: 0.930190977, Training Accuracy: 72.784\n",
            "Time taken for training worker 2: 0:00:24.147978\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.595502422, Training Accuracy: 55.488\n",
            "Worker 3, [02/08]: Training Loss: 1.390460585, Training Accuracy: 60.272\n",
            "Worker 3, [03/08]: Training Loss: 1.284323022, Training Accuracy: 63.904\n",
            "Worker 3, [04/08]: Training Loss: 1.192762889, Training Accuracy: 66.064\n",
            "Worker 3, [05/08]: Training Loss: 1.123363383, Training Accuracy: 68.688\n",
            "Worker 3, [06/08]: Training Loss: 1.059501248, Training Accuracy: 70.208\n",
            "Worker 3, [07/08]: Training Loss: 1.015435500, Training Accuracy: 70.576\n",
            "Worker 3, [08/08]: Training Loss: 0.962595950, Training Accuracy: 72.336\n",
            "Time taken for training worker 3: 0:00:25.373222\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.602983014, Training Accuracy: 55.584\n",
            "Worker 4, [02/08]: Training Loss: 1.406985613, Training Accuracy: 60.144\n",
            "Worker 4, [03/08]: Training Loss: 1.297897619, Training Accuracy: 63.456\n",
            "Worker 4, [04/08]: Training Loss: 1.193927323, Training Accuracy: 66.256\n",
            "Worker 4, [05/08]: Training Loss: 1.135694405, Training Accuracy: 66.944\n",
            "Worker 4, [06/08]: Training Loss: 1.061965422, Training Accuracy: 69.584\n",
            "Worker 4, [07/08]: Training Loss: 1.013475684, Training Accuracy: 71.040\n",
            "Worker 4, [08/08]: Training Loss: 0.973956078, Training Accuracy: 72.400\n",
            "Time taken for training worker 4: 0:00:24.681489\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.587645548, Training Accuracy: 55.552\n",
            "Worker 5, [02/08]: Training Loss: 1.405076083, Training Accuracy: 60.368\n",
            "Worker 5, [03/08]: Training Loss: 1.297637073, Training Accuracy: 63.072\n",
            "Worker 5, [04/08]: Training Loss: 1.191546518, Training Accuracy: 65.616\n",
            "Worker 5, [05/08]: Training Loss: 1.121912748, Training Accuracy: 68.144\n",
            "Worker 5, [06/08]: Training Loss: 1.073863993, Training Accuracy: 69.632\n",
            "Worker 5, [07/08]: Training Loss: 1.019420150, Training Accuracy: 70.960\n",
            "Worker 5, [08/08]: Training Loss: 0.960134338, Training Accuracy: 72.496\n",
            "Time taken for training worker 5: 0:00:24.108336\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.564670378, Training Accuracy: 55.456\n",
            "Worker 6, [02/08]: Training Loss: 1.359039577, Training Accuracy: 61.120\n",
            "Worker 6, [03/08]: Training Loss: 1.253155123, Training Accuracy: 63.984\n",
            "Worker 6, [04/08]: Training Loss: 1.149336594, Training Accuracy: 66.784\n",
            "Worker 6, [05/08]: Training Loss: 1.121295018, Training Accuracy: 68.192\n",
            "Worker 6, [06/08]: Training Loss: 1.048229286, Training Accuracy: 69.696\n",
            "Worker 6, [07/08]: Training Loss: 0.974339725, Training Accuracy: 72.288\n",
            "Worker 6, [08/08]: Training Loss: 0.936627021, Training Accuracy: 73.200\n",
            "Time taken for training worker 6: 0:00:24.493157\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.606305323, Training Accuracy: 55.248\n",
            "Worker 7, [02/08]: Training Loss: 1.407873346, Training Accuracy: 60.320\n",
            "Worker 7, [03/08]: Training Loss: 1.275115289, Training Accuracy: 63.696\n",
            "Worker 7, [04/08]: Training Loss: 1.209714160, Training Accuracy: 65.408\n",
            "Worker 7, [05/08]: Training Loss: 1.151477336, Training Accuracy: 67.264\n",
            "Worker 7, [06/08]: Training Loss: 1.063590451, Training Accuracy: 69.792\n",
            "Worker 7, [07/08]: Training Loss: 1.021176986, Training Accuracy: 71.984\n",
            "Worker 7, [08/08]: Training Loss: 0.954705463, Training Accuracy: 72.080\n",
            "Time taken for training worker 7: 0:00:24.415516\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.553856251, Training Accuracy: 55.984\n",
            "Worker 8, [02/08]: Training Loss: 1.363850663, Training Accuracy: 61.568\n",
            "Worker 8, [03/08]: Training Loss: 1.275428459, Training Accuracy: 63.744\n",
            "Worker 8, [04/08]: Training Loss: 1.199091751, Training Accuracy: 65.648\n",
            "Worker 8, [05/08]: Training Loss: 1.125303134, Training Accuracy: 67.664\n",
            "Worker 8, [06/08]: Training Loss: 1.043089110, Training Accuracy: 70.464\n",
            "Worker 8, [07/08]: Training Loss: 1.019371354, Training Accuracy: 70.288\n",
            "Worker 8, [08/08]: Training Loss: 0.948849173, Training Accuracy: 73.200\n",
            "Time taken for training worker 8: 0:00:25.775798\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004175\n",
            "Global Update 15: Test Loss: 2.169644745, Test Accuracy: 47.950\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.569221483, Training Accuracy: 55.904\n",
            "Worker 1, [02/08]: Training Loss: 1.386169512, Training Accuracy: 60.576\n",
            "Worker 1, [03/08]: Training Loss: 1.317684949, Training Accuracy: 62.368\n",
            "Worker 1, [04/08]: Training Loss: 1.221265646, Training Accuracy: 65.184\n",
            "Worker 1, [05/08]: Training Loss: 1.183515342, Training Accuracy: 65.984\n",
            "Worker 1, [06/08]: Training Loss: 1.124103564, Training Accuracy: 67.984\n",
            "Worker 1, [07/08]: Training Loss: 1.070907159, Training Accuracy: 69.072\n",
            "Worker 1, [08/08]: Training Loss: 1.032466280, Training Accuracy: 70.544\n",
            "Time taken for training worker 1: 0:00:25.397446\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.480087295, Training Accuracy: 58.176\n",
            "Worker 2, [02/08]: Training Loss: 1.357061021, Training Accuracy: 61.312\n",
            "Worker 2, [03/08]: Training Loss: 1.241635354, Training Accuracy: 64.576\n",
            "Worker 2, [04/08]: Training Loss: 1.201834305, Training Accuracy: 65.200\n",
            "Worker 2, [05/08]: Training Loss: 1.131100101, Training Accuracy: 67.248\n",
            "Worker 2, [06/08]: Training Loss: 1.064438368, Training Accuracy: 69.280\n",
            "Worker 2, [07/08]: Training Loss: 1.027890463, Training Accuracy: 70.608\n",
            "Worker 2, [08/08]: Training Loss: 0.988931231, Training Accuracy: 72.448\n",
            "Time taken for training worker 2: 0:00:25.174630\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.531825115, Training Accuracy: 57.424\n",
            "Worker 3, [02/08]: Training Loss: 1.380022765, Training Accuracy: 61.232\n",
            "Worker 3, [03/08]: Training Loss: 1.283911131, Training Accuracy: 63.728\n",
            "Worker 3, [04/08]: Training Loss: 1.214336043, Training Accuracy: 65.216\n",
            "Worker 3, [05/08]: Training Loss: 1.141776877, Training Accuracy: 67.376\n",
            "Worker 3, [06/08]: Training Loss: 1.109196640, Training Accuracy: 68.560\n",
            "Worker 3, [07/08]: Training Loss: 1.057437010, Training Accuracy: 70.368\n",
            "Worker 3, [08/08]: Training Loss: 0.992514062, Training Accuracy: 71.568\n",
            "Time taken for training worker 3: 0:00:24.710590\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.506947858, Training Accuracy: 57.808\n",
            "Worker 4, [02/08]: Training Loss: 1.378007714, Training Accuracy: 61.312\n",
            "Worker 4, [03/08]: Training Loss: 1.306303954, Training Accuracy: 62.656\n",
            "Worker 4, [04/08]: Training Loss: 1.218601431, Training Accuracy: 66.000\n",
            "Worker 4, [05/08]: Training Loss: 1.177181900, Training Accuracy: 66.560\n",
            "Worker 4, [06/08]: Training Loss: 1.105828142, Training Accuracy: 68.368\n",
            "Worker 4, [07/08]: Training Loss: 1.062215536, Training Accuracy: 69.424\n",
            "Worker 4, [08/08]: Training Loss: 1.017004465, Training Accuracy: 71.120\n",
            "Time taken for training worker 4: 0:00:24.560446\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.539717296, Training Accuracy: 56.944\n",
            "Worker 5, [02/08]: Training Loss: 1.382841535, Training Accuracy: 61.296\n",
            "Worker 5, [03/08]: Training Loss: 1.282626374, Training Accuracy: 62.848\n",
            "Worker 5, [04/08]: Training Loss: 1.218524623, Training Accuracy: 65.360\n",
            "Worker 5, [05/08]: Training Loss: 1.157117022, Training Accuracy: 66.880\n",
            "Worker 5, [06/08]: Training Loss: 1.101000394, Training Accuracy: 68.720\n",
            "Worker 5, [07/08]: Training Loss: 1.062004487, Training Accuracy: 69.792\n",
            "Worker 5, [08/08]: Training Loss: 1.003154715, Training Accuracy: 71.968\n",
            "Time taken for training worker 5: 0:00:24.311212\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.484828835, Training Accuracy: 57.744\n",
            "Worker 6, [02/08]: Training Loss: 1.351680485, Training Accuracy: 60.880\n",
            "Worker 6, [03/08]: Training Loss: 1.265163959, Training Accuracy: 63.296\n",
            "Worker 6, [04/08]: Training Loss: 1.194828103, Training Accuracy: 65.968\n",
            "Worker 6, [05/08]: Training Loss: 1.138358029, Training Accuracy: 67.168\n",
            "Worker 6, [06/08]: Training Loss: 1.081431117, Training Accuracy: 68.496\n",
            "Worker 6, [07/08]: Training Loss: 1.031371347, Training Accuracy: 70.800\n",
            "Worker 6, [08/08]: Training Loss: 0.997330722, Training Accuracy: 71.856\n",
            "Time taken for training worker 6: 0:00:25.197764\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.519209278, Training Accuracy: 57.664\n",
            "Worker 7, [02/08]: Training Loss: 1.388338284, Training Accuracy: 61.376\n",
            "Worker 7, [03/08]: Training Loss: 1.301288408, Training Accuracy: 62.512\n",
            "Worker 7, [04/08]: Training Loss: 1.234601504, Training Accuracy: 65.232\n",
            "Worker 7, [05/08]: Training Loss: 1.161207737, Training Accuracy: 67.120\n",
            "Worker 7, [06/08]: Training Loss: 1.108383879, Training Accuracy: 68.960\n",
            "Worker 7, [07/08]: Training Loss: 1.056802703, Training Accuracy: 70.000\n",
            "Worker 7, [08/08]: Training Loss: 1.030844460, Training Accuracy: 71.040\n",
            "Time taken for training worker 7: 0:00:24.767051\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.486900004, Training Accuracy: 57.840\n",
            "Worker 8, [02/08]: Training Loss: 1.358972536, Training Accuracy: 61.232\n",
            "Worker 8, [03/08]: Training Loss: 1.272821377, Training Accuracy: 63.616\n",
            "Worker 8, [04/08]: Training Loss: 1.190092017, Training Accuracy: 66.112\n",
            "Worker 8, [05/08]: Training Loss: 1.152026181, Training Accuracy: 66.464\n",
            "Worker 8, [06/08]: Training Loss: 1.086170029, Training Accuracy: 69.376\n",
            "Worker 8, [07/08]: Training Loss: 1.027864100, Training Accuracy: 70.704\n",
            "Worker 8, [08/08]: Training Loss: 1.019438833, Training Accuracy: 70.784\n",
            "Time taken for training worker 8: 0:00:25.613527\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004610\n",
            "Global Update 16: Test Loss: 2.154418984, Test Accuracy: 47.880\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.514399403, Training Accuracy: 57.104\n",
            "Worker 1, [02/08]: Training Loss: 1.415402050, Training Accuracy: 59.648\n",
            "Worker 1, [03/08]: Training Loss: 1.354421310, Training Accuracy: 61.536\n",
            "Worker 1, [04/08]: Training Loss: 1.310615173, Training Accuracy: 62.320\n",
            "Worker 1, [05/08]: Training Loss: 1.255525242, Training Accuracy: 64.240\n",
            "Worker 1, [06/08]: Training Loss: 1.222167090, Training Accuracy: 65.088\n",
            "Worker 1, [07/08]: Training Loss: 1.178846160, Training Accuracy: 66.832\n",
            "Worker 1, [08/08]: Training Loss: 1.149666759, Training Accuracy: 67.104\n",
            "Time taken for training worker 1: 0:00:24.932055\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.445912098, Training Accuracy: 58.896\n",
            "Worker 2, [02/08]: Training Loss: 1.350538624, Training Accuracy: 61.664\n",
            "Worker 2, [03/08]: Training Loss: 1.295852204, Training Accuracy: 62.912\n",
            "Worker 2, [04/08]: Training Loss: 1.243878749, Training Accuracy: 64.416\n",
            "Worker 2, [05/08]: Training Loss: 1.207362899, Training Accuracy: 65.648\n",
            "Worker 2, [06/08]: Training Loss: 1.167191660, Training Accuracy: 66.784\n",
            "Worker 2, [07/08]: Training Loss: 1.143756452, Training Accuracy: 67.648\n",
            "Worker 2, [08/08]: Training Loss: 1.098089669, Training Accuracy: 67.936\n",
            "Time taken for training worker 2: 0:00:25.431803\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.491612274, Training Accuracy: 58.800\n",
            "Worker 3, [02/08]: Training Loss: 1.389403822, Training Accuracy: 60.960\n",
            "Worker 3, [03/08]: Training Loss: 1.320663500, Training Accuracy: 62.112\n",
            "Worker 3, [04/08]: Training Loss: 1.271560092, Training Accuracy: 64.560\n",
            "Worker 3, [05/08]: Training Loss: 1.231701709, Training Accuracy: 65.472\n",
            "Worker 3, [06/08]: Training Loss: 1.196162636, Training Accuracy: 66.272\n",
            "Worker 3, [07/08]: Training Loss: 1.168769810, Training Accuracy: 67.536\n",
            "Worker 3, [08/08]: Training Loss: 1.123661267, Training Accuracy: 68.640\n",
            "Time taken for training worker 3: 0:00:24.027612\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.489930137, Training Accuracy: 58.112\n",
            "Worker 4, [02/08]: Training Loss: 1.401219986, Training Accuracy: 60.176\n",
            "Worker 4, [03/08]: Training Loss: 1.341214146, Training Accuracy: 61.712\n",
            "Worker 4, [04/08]: Training Loss: 1.289161912, Training Accuracy: 63.376\n",
            "Worker 4, [05/08]: Training Loss: 1.244853386, Training Accuracy: 64.624\n",
            "Worker 4, [06/08]: Training Loss: 1.196262315, Training Accuracy: 65.904\n",
            "Worker 4, [07/08]: Training Loss: 1.173228155, Training Accuracy: 66.304\n",
            "Worker 4, [08/08]: Training Loss: 1.132934433, Training Accuracy: 67.584\n",
            "Time taken for training worker 4: 0:00:24.780950\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.484256356, Training Accuracy: 57.744\n",
            "Worker 5, [02/08]: Training Loss: 1.387011058, Training Accuracy: 60.016\n",
            "Worker 5, [03/08]: Training Loss: 1.322357423, Training Accuracy: 62.496\n",
            "Worker 5, [04/08]: Training Loss: 1.277793927, Training Accuracy: 63.792\n",
            "Worker 5, [05/08]: Training Loss: 1.235190467, Training Accuracy: 64.800\n",
            "Worker 5, [06/08]: Training Loss: 1.195610063, Training Accuracy: 66.432\n",
            "Worker 5, [07/08]: Training Loss: 1.151038788, Training Accuracy: 67.376\n",
            "Worker 5, [08/08]: Training Loss: 1.132654389, Training Accuracy: 67.696\n",
            "Time taken for training worker 5: 0:00:25.126974\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.444437466, Training Accuracy: 58.944\n",
            "Worker 6, [02/08]: Training Loss: 1.359714665, Training Accuracy: 60.928\n",
            "Worker 6, [03/08]: Training Loss: 1.312675780, Training Accuracy: 62.384\n",
            "Worker 6, [04/08]: Training Loss: 1.249390321, Training Accuracy: 63.936\n",
            "Worker 6, [05/08]: Training Loss: 1.205711105, Training Accuracy: 65.616\n",
            "Worker 6, [06/08]: Training Loss: 1.182046255, Training Accuracy: 66.288\n",
            "Worker 6, [07/08]: Training Loss: 1.154204290, Training Accuracy: 66.944\n",
            "Worker 6, [08/08]: Training Loss: 1.108516585, Training Accuracy: 67.920\n",
            "Time taken for training worker 6: 0:00:24.502860\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.473161521, Training Accuracy: 58.160\n",
            "Worker 7, [02/08]: Training Loss: 1.397464056, Training Accuracy: 60.400\n",
            "Worker 7, [03/08]: Training Loss: 1.318277281, Training Accuracy: 62.560\n",
            "Worker 7, [04/08]: Training Loss: 1.294304087, Training Accuracy: 63.232\n",
            "Worker 7, [05/08]: Training Loss: 1.242115640, Training Accuracy: 64.208\n",
            "Worker 7, [06/08]: Training Loss: 1.176438423, Training Accuracy: 66.448\n",
            "Worker 7, [07/08]: Training Loss: 1.182162485, Training Accuracy: 66.256\n",
            "Worker 7, [08/08]: Training Loss: 1.132993345, Training Accuracy: 68.000\n",
            "Time taken for training worker 7: 0:00:24.739572\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.460437986, Training Accuracy: 57.872\n",
            "Worker 8, [02/08]: Training Loss: 1.367133876, Training Accuracy: 60.384\n",
            "Worker 8, [03/08]: Training Loss: 1.310184537, Training Accuracy: 62.832\n",
            "Worker 8, [04/08]: Training Loss: 1.255901140, Training Accuracy: 64.176\n",
            "Worker 8, [05/08]: Training Loss: 1.210461649, Training Accuracy: 65.728\n",
            "Worker 8, [06/08]: Training Loss: 1.172186210, Training Accuracy: 66.960\n",
            "Worker 8, [07/08]: Training Loss: 1.148278055, Training Accuracy: 67.312\n",
            "Worker 8, [08/08]: Training Loss: 1.109057153, Training Accuracy: 68.464\n",
            "Time taken for training worker 8: 0:00:24.897994\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004287\n",
            "Global Update 17: Test Loss: 2.128888662, Test Accuracy: 48.080\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.485467946, Training Accuracy: 57.712\n",
            "Worker 1, [02/08]: Training Loss: 1.460249868, Training Accuracy: 58.256\n",
            "Worker 1, [03/08]: Training Loss: 1.417447140, Training Accuracy: 59.744\n",
            "Worker 1, [04/08]: Training Loss: 1.397218019, Training Accuracy: 60.448\n",
            "Worker 1, [05/08]: Training Loss: 1.389274070, Training Accuracy: 60.192\n",
            "Worker 1, [06/08]: Training Loss: 1.368756359, Training Accuracy: 60.336\n",
            "Worker 1, [07/08]: Training Loss: 1.349783032, Training Accuracy: 62.080\n",
            "Worker 1, [08/08]: Training Loss: 1.339778177, Training Accuracy: 62.208\n",
            "Time taken for training worker 1: 0:00:24.654414\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.431802947, Training Accuracy: 59.520\n",
            "Worker 2, [02/08]: Training Loss: 1.396173798, Training Accuracy: 60.768\n",
            "Worker 2, [03/08]: Training Loss: 1.358117166, Training Accuracy: 61.168\n",
            "Worker 2, [04/08]: Training Loss: 1.363522929, Training Accuracy: 60.656\n",
            "Worker 2, [05/08]: Training Loss: 1.338907837, Training Accuracy: 61.520\n",
            "Worker 2, [06/08]: Training Loss: 1.309785999, Training Accuracy: 62.336\n",
            "Worker 2, [07/08]: Training Loss: 1.304861592, Training Accuracy: 62.432\n",
            "Worker 2, [08/08]: Training Loss: 1.272076334, Training Accuracy: 62.800\n",
            "Time taken for training worker 2: 0:00:23.711322\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/08]: Training Loss: 1.448244845, Training Accuracy: 59.712\n",
            "Worker 3, [02/08]: Training Loss: 1.426699725, Training Accuracy: 59.984\n",
            "Worker 3, [03/08]: Training Loss: 1.399469542, Training Accuracy: 60.208\n",
            "Worker 3, [04/08]: Training Loss: 1.365178716, Training Accuracy: 61.712\n",
            "Worker 3, [05/08]: Training Loss: 1.336340311, Training Accuracy: 62.032\n",
            "Worker 3, [06/08]: Training Loss: 1.324926112, Training Accuracy: 62.784\n",
            "Worker 3, [07/08]: Training Loss: 1.337887620, Training Accuracy: 62.752\n",
            "Worker 3, [08/08]: Training Loss: 1.300915416, Training Accuracy: 63.408\n",
            "Time taken for training worker 3: 0:00:24.317648\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/08]: Training Loss: 1.451610851, Training Accuracy: 58.896\n",
            "Worker 4, [02/08]: Training Loss: 1.444256257, Training Accuracy: 59.680\n",
            "Worker 4, [03/08]: Training Loss: 1.412516385, Training Accuracy: 60.128\n",
            "Worker 4, [04/08]: Training Loss: 1.400487043, Training Accuracy: 60.240\n",
            "Worker 4, [05/08]: Training Loss: 1.388411744, Training Accuracy: 60.736\n",
            "Worker 4, [06/08]: Training Loss: 1.356117848, Training Accuracy: 61.584\n",
            "Worker 4, [07/08]: Training Loss: 1.348881713, Training Accuracy: 61.488\n",
            "Worker 4, [08/08]: Training Loss: 1.322698599, Training Accuracy: 62.960\n",
            "Time taken for training worker 4: 0:00:24.733988\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/08]: Training Loss: 1.458720711, Training Accuracy: 58.752\n",
            "Worker 5, [02/08]: Training Loss: 1.433128392, Training Accuracy: 58.960\n",
            "Worker 5, [03/08]: Training Loss: 1.412950934, Training Accuracy: 60.032\n",
            "Worker 5, [04/08]: Training Loss: 1.372783185, Training Accuracy: 60.832\n",
            "Worker 5, [05/08]: Training Loss: 1.359387632, Training Accuracy: 61.584\n",
            "Worker 5, [06/08]: Training Loss: 1.334681098, Training Accuracy: 62.432\n",
            "Worker 5, [07/08]: Training Loss: 1.321405220, Training Accuracy: 62.992\n",
            "Worker 5, [08/08]: Training Loss: 1.305999395, Training Accuracy: 62.688\n",
            "Time taken for training worker 5: 0:00:25.073964\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/08]: Training Loss: 1.433122546, Training Accuracy: 60.128\n",
            "Worker 6, [02/08]: Training Loss: 1.389888127, Training Accuracy: 60.416\n",
            "Worker 6, [03/08]: Training Loss: 1.368600355, Training Accuracy: 60.720\n",
            "Worker 6, [04/08]: Training Loss: 1.354749657, Training Accuracy: 61.088\n",
            "Worker 6, [05/08]: Training Loss: 1.325121724, Training Accuracy: 62.000\n",
            "Worker 6, [06/08]: Training Loss: 1.339120133, Training Accuracy: 60.896\n",
            "Worker 6, [07/08]: Training Loss: 1.300102301, Training Accuracy: 62.240\n",
            "Worker 6, [08/08]: Training Loss: 1.295982773, Training Accuracy: 62.800\n",
            "Time taken for training worker 6: 0:00:25.440935\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/08]: Training Loss: 1.474515340, Training Accuracy: 58.864\n",
            "Worker 7, [02/08]: Training Loss: 1.431833666, Training Accuracy: 60.176\n",
            "Worker 7, [03/08]: Training Loss: 1.396661080, Training Accuracy: 59.440\n",
            "Worker 7, [04/08]: Training Loss: 1.369317512, Training Accuracy: 61.728\n",
            "Worker 7, [05/08]: Training Loss: 1.364218688, Training Accuracy: 61.344\n",
            "Worker 7, [06/08]: Training Loss: 1.347152669, Training Accuracy: 61.248\n",
            "Worker 7, [07/08]: Training Loss: 1.320816332, Training Accuracy: 62.656\n",
            "Worker 7, [08/08]: Training Loss: 1.320048665, Training Accuracy: 62.928\n",
            "Time taken for training worker 7: 0:00:25.184321\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/08]: Training Loss: 1.443123061, Training Accuracy: 58.592\n",
            "Worker 8, [02/08]: Training Loss: 1.415478270, Training Accuracy: 59.808\n",
            "Worker 8, [03/08]: Training Loss: 1.385655794, Training Accuracy: 60.320\n",
            "Worker 8, [04/08]: Training Loss: 1.356701678, Training Accuracy: 60.672\n",
            "Worker 8, [05/08]: Training Loss: 1.337664770, Training Accuracy: 61.760\n",
            "Worker 8, [06/08]: Training Loss: 1.306862568, Training Accuracy: 62.720\n",
            "Worker 8, [07/08]: Training Loss: 1.307186192, Training Accuracy: 62.608\n",
            "Worker 8, [08/08]: Training Loss: 1.282722199, Training Accuracy: 63.168\n",
            "Time taken for training worker 8: 0:00:24.414894\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004671\n",
            "Global Update 18: Test Loss: 2.104928839, Test Accuracy: 48.010\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:59:14.333636\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:16\n",
            "==================================================\n",
            "Worker 1, [01/16]: Training Loss: 4.592774445, Training Accuracy: 1.040\n",
            "Worker 1, [02/16]: Training Loss: 4.415323875, Training Accuracy: 3.536\n",
            "Worker 1, [03/16]: Training Loss: 4.197967894, Training Accuracy: 5.776\n",
            "Worker 1, [04/16]: Training Loss: 4.055941740, Training Accuracy: 7.040\n",
            "Worker 1, [05/16]: Training Loss: 3.930704798, Training Accuracy: 8.672\n",
            "Worker 1, [06/16]: Training Loss: 3.841624090, Training Accuracy: 10.336\n",
            "Worker 1, [07/16]: Training Loss: 3.739759229, Training Accuracy: 11.696\n",
            "Worker 1, [08/16]: Training Loss: 3.664336869, Training Accuracy: 12.640\n",
            "Worker 1, [09/16]: Training Loss: 3.589514066, Training Accuracy: 14.016\n",
            "Worker 1, [10/16]: Training Loss: 3.488438511, Training Accuracy: 15.488\n",
            "Worker 1, [11/16]: Training Loss: 3.389733249, Training Accuracy: 17.552\n",
            "Worker 1, [12/16]: Training Loss: 3.294157756, Training Accuracy: 19.024\n",
            "Worker 1, [13/16]: Training Loss: 3.251600937, Training Accuracy: 19.440\n",
            "Worker 1, [14/16]: Training Loss: 3.168972765, Training Accuracy: 21.232\n",
            "Worker 1, [15/16]: Training Loss: 3.105917133, Training Accuracy: 22.400\n",
            "Worker 1, [16/16]: Training Loss: 3.039607963, Training Accuracy: 23.824\n",
            "Time taken for training worker 1: 0:00:48.949315\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 4.591132908, Training Accuracy: 1.440\n",
            "Worker 2, [02/16]: Training Loss: 4.375708055, Training Accuracy: 4.112\n",
            "Worker 2, [03/16]: Training Loss: 4.148142934, Training Accuracy: 5.872\n",
            "Worker 2, [04/16]: Training Loss: 4.008830757, Training Accuracy: 8.000\n",
            "Worker 2, [05/16]: Training Loss: 3.893110110, Training Accuracy: 9.616\n",
            "Worker 2, [06/16]: Training Loss: 3.781956378, Training Accuracy: 11.216\n",
            "Worker 2, [07/16]: Training Loss: 3.717293669, Training Accuracy: 12.016\n",
            "Worker 2, [08/16]: Training Loss: 3.594315273, Training Accuracy: 13.648\n",
            "Worker 2, [09/16]: Training Loss: 3.529598129, Training Accuracy: 14.720\n",
            "Worker 2, [10/16]: Training Loss: 3.437012312, Training Accuracy: 17.424\n",
            "Worker 2, [11/16]: Training Loss: 3.372275270, Training Accuracy: 18.128\n",
            "Worker 2, [12/16]: Training Loss: 3.297903474, Training Accuracy: 18.688\n",
            "Worker 2, [13/16]: Training Loss: 3.183320150, Training Accuracy: 20.480\n",
            "Worker 2, [14/16]: Training Loss: 3.154294080, Training Accuracy: 21.424\n",
            "Worker 2, [15/16]: Training Loss: 3.075705876, Training Accuracy: 22.448\n",
            "Worker 2, [16/16]: Training Loss: 3.017875752, Training Accuracy: 23.600\n",
            "Time taken for training worker 2: 0:00:50.760800\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 4.592278987, Training Accuracy: 1.664\n",
            "Worker 3, [02/16]: Training Loss: 4.380516899, Training Accuracy: 3.408\n",
            "Worker 3, [03/16]: Training Loss: 4.141402473, Training Accuracy: 5.968\n",
            "Worker 3, [04/16]: Training Loss: 4.013920346, Training Accuracy: 7.920\n",
            "Worker 3, [05/16]: Training Loss: 3.903521830, Training Accuracy: 9.488\n",
            "Worker 3, [06/16]: Training Loss: 3.785624144, Training Accuracy: 11.312\n",
            "Worker 3, [07/16]: Training Loss: 3.700398830, Training Accuracy: 12.416\n",
            "Worker 3, [08/16]: Training Loss: 3.610047688, Training Accuracy: 14.384\n",
            "Worker 3, [09/16]: Training Loss: 3.528759372, Training Accuracy: 15.104\n",
            "Worker 3, [10/16]: Training Loss: 3.439615868, Training Accuracy: 16.240\n",
            "Worker 3, [11/16]: Training Loss: 3.364906119, Training Accuracy: 17.520\n",
            "Worker 3, [12/16]: Training Loss: 3.298179558, Training Accuracy: 18.256\n",
            "Worker 3, [13/16]: Training Loss: 3.231758042, Training Accuracy: 20.464\n",
            "Worker 3, [14/16]: Training Loss: 3.147851618, Training Accuracy: 21.488\n",
            "Worker 3, [15/16]: Training Loss: 3.082213720, Training Accuracy: 22.256\n",
            "Worker 3, [16/16]: Training Loss: 3.054371717, Training Accuracy: 22.592\n",
            "Time taken for training worker 3: 0:00:49.207563\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 4.595550265, Training Accuracy: 1.632\n",
            "Worker 4, [02/16]: Training Loss: 4.401398557, Training Accuracy: 3.664\n",
            "Worker 4, [03/16]: Training Loss: 4.167527493, Training Accuracy: 5.424\n",
            "Worker 4, [04/16]: Training Loss: 4.028082072, Training Accuracy: 7.392\n",
            "Worker 4, [05/16]: Training Loss: 3.906572298, Training Accuracy: 9.360\n",
            "Worker 4, [06/16]: Training Loss: 3.802657101, Training Accuracy: 10.896\n",
            "Worker 4, [07/16]: Training Loss: 3.717342394, Training Accuracy: 12.000\n",
            "Worker 4, [08/16]: Training Loss: 3.637517798, Training Accuracy: 12.912\n",
            "Worker 4, [09/16]: Training Loss: 3.571865731, Training Accuracy: 13.744\n",
            "Worker 4, [10/16]: Training Loss: 3.490602097, Training Accuracy: 15.616\n",
            "Worker 4, [11/16]: Training Loss: 3.424054284, Training Accuracy: 17.072\n",
            "Worker 4, [12/16]: Training Loss: 3.363986607, Training Accuracy: 17.024\n",
            "Worker 4, [13/16]: Training Loss: 3.269263589, Training Accuracy: 18.496\n",
            "Worker 4, [14/16]: Training Loss: 3.232352736, Training Accuracy: 19.488\n",
            "Worker 4, [15/16]: Training Loss: 3.160991053, Training Accuracy: 21.312\n",
            "Worker 4, [16/16]: Training Loss: 3.114238824, Training Accuracy: 21.856\n",
            "Time taken for training worker 4: 0:00:50.111903\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 4.590046440, Training Accuracy: 1.392\n",
            "Worker 5, [02/16]: Training Loss: 4.376860477, Training Accuracy: 3.984\n",
            "Worker 5, [03/16]: Training Loss: 4.155488167, Training Accuracy: 6.096\n",
            "Worker 5, [04/16]: Training Loss: 4.016815268, Training Accuracy: 7.840\n",
            "Worker 5, [05/16]: Training Loss: 3.898829832, Training Accuracy: 9.696\n",
            "Worker 5, [06/16]: Training Loss: 3.773105575, Training Accuracy: 11.648\n",
            "Worker 5, [07/16]: Training Loss: 3.690985886, Training Accuracy: 13.488\n",
            "Worker 5, [08/16]: Training Loss: 3.611354249, Training Accuracy: 14.288\n",
            "Worker 5, [09/16]: Training Loss: 3.543042740, Training Accuracy: 15.600\n",
            "Worker 5, [10/16]: Training Loss: 3.458281125, Training Accuracy: 16.880\n",
            "Worker 5, [11/16]: Training Loss: 3.375513274, Training Accuracy: 17.776\n",
            "Worker 5, [12/16]: Training Loss: 3.308230775, Training Accuracy: 18.944\n",
            "Worker 5, [13/16]: Training Loss: 3.237045422, Training Accuracy: 19.792\n",
            "Worker 5, [14/16]: Training Loss: 3.162225952, Training Accuracy: 21.760\n",
            "Worker 5, [15/16]: Training Loss: 3.091171250, Training Accuracy: 22.016\n",
            "Worker 5, [16/16]: Training Loss: 3.033062351, Training Accuracy: 23.280\n",
            "Time taken for training worker 5: 0:00:49.946423\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 4.588259030, Training Accuracy: 1.664\n",
            "Worker 6, [02/16]: Training Loss: 4.352950656, Training Accuracy: 4.400\n",
            "Worker 6, [03/16]: Training Loss: 4.129532948, Training Accuracy: 6.224\n",
            "Worker 6, [04/16]: Training Loss: 3.985986238, Training Accuracy: 8.368\n",
            "Worker 6, [05/16]: Training Loss: 3.870848505, Training Accuracy: 10.560\n",
            "Worker 6, [06/16]: Training Loss: 3.773719968, Training Accuracy: 11.568\n",
            "Worker 6, [07/16]: Training Loss: 3.684624652, Training Accuracy: 12.736\n",
            "Worker 6, [08/16]: Training Loss: 3.599640172, Training Accuracy: 14.208\n",
            "Worker 6, [09/16]: Training Loss: 3.498851691, Training Accuracy: 16.304\n",
            "Worker 6, [10/16]: Training Loss: 3.452125797, Training Accuracy: 16.608\n",
            "Worker 6, [11/16]: Training Loss: 3.356057858, Training Accuracy: 18.432\n",
            "Worker 6, [12/16]: Training Loss: 3.269031897, Training Accuracy: 19.920\n",
            "Worker 6, [13/16]: Training Loss: 3.232407154, Training Accuracy: 20.528\n",
            "Worker 6, [14/16]: Training Loss: 3.137193726, Training Accuracy: 21.888\n",
            "Worker 6, [15/16]: Training Loss: 3.094437589, Training Accuracy: 22.032\n",
            "Worker 6, [16/16]: Training Loss: 3.005628109, Training Accuracy: 24.048\n",
            "Time taken for training worker 6: 0:00:49.384434\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 4.591029260, Training Accuracy: 1.536\n",
            "Worker 7, [02/16]: Training Loss: 4.379212764, Training Accuracy: 3.584\n",
            "Worker 7, [03/16]: Training Loss: 4.151709287, Training Accuracy: 5.488\n",
            "Worker 7, [04/16]: Training Loss: 4.007771441, Training Accuracy: 7.968\n",
            "Worker 7, [05/16]: Training Loss: 3.878479709, Training Accuracy: 10.016\n",
            "Worker 7, [06/16]: Training Loss: 3.778431863, Training Accuracy: 11.200\n",
            "Worker 7, [07/16]: Training Loss: 3.718779929, Training Accuracy: 12.448\n",
            "Worker 7, [08/16]: Training Loss: 3.619478398, Training Accuracy: 13.568\n",
            "Worker 7, [09/16]: Training Loss: 3.520135991, Training Accuracy: 15.120\n",
            "Worker 7, [10/16]: Training Loss: 3.454278073, Training Accuracy: 16.544\n",
            "Worker 7, [11/16]: Training Loss: 3.354019233, Training Accuracy: 18.304\n",
            "Worker 7, [12/16]: Training Loss: 3.286301348, Training Accuracy: 19.232\n",
            "Worker 7, [13/16]: Training Loss: 3.201082368, Training Accuracy: 20.304\n",
            "Worker 7, [14/16]: Training Loss: 3.171350579, Training Accuracy: 21.040\n",
            "Worker 7, [15/16]: Training Loss: 3.087579097, Training Accuracy: 22.464\n",
            "Worker 7, [16/16]: Training Loss: 3.036859464, Training Accuracy: 23.328\n",
            "Time taken for training worker 7: 0:00:49.372175\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 4.587332400, Training Accuracy: 1.712\n",
            "Worker 8, [02/16]: Training Loss: 4.356256412, Training Accuracy: 3.792\n",
            "Worker 8, [03/16]: Training Loss: 4.137269801, Training Accuracy: 6.400\n",
            "Worker 8, [04/16]: Training Loss: 4.012055090, Training Accuracy: 7.792\n",
            "Worker 8, [05/16]: Training Loss: 3.913922969, Training Accuracy: 9.248\n",
            "Worker 8, [06/16]: Training Loss: 3.795640559, Training Accuracy: 10.208\n",
            "Worker 8, [07/16]: Training Loss: 3.704543561, Training Accuracy: 12.720\n",
            "Worker 8, [08/16]: Training Loss: 3.616387289, Training Accuracy: 14.064\n",
            "Worker 8, [09/16]: Training Loss: 3.521564094, Training Accuracy: 15.472\n",
            "Worker 8, [10/16]: Training Loss: 3.459683630, Training Accuracy: 16.000\n",
            "Worker 8, [11/16]: Training Loss: 3.377608353, Training Accuracy: 17.920\n",
            "Worker 8, [12/16]: Training Loss: 3.300212651, Training Accuracy: 19.168\n",
            "Worker 8, [13/16]: Training Loss: 3.217662911, Training Accuracy: 20.752\n",
            "Worker 8, [14/16]: Training Loss: 3.146807040, Training Accuracy: 21.152\n",
            "Worker 8, [15/16]: Training Loss: 3.099246339, Training Accuracy: 22.080\n",
            "Worker 8, [16/16]: Training Loss: 3.041297144, Training Accuracy: 23.472\n",
            "Time taken for training worker 8: 0:00:49.109724\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004153\n",
            "Global Update 01: Test Loss: 3.499294633, Test Accuracy: 21.420\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 3.410135447, Training Accuracy: 17.184\n",
            "Worker 1, [02/16]: Training Loss: 3.281456349, Training Accuracy: 19.488\n",
            "Worker 1, [03/16]: Training Loss: 3.186614572, Training Accuracy: 20.672\n",
            "Worker 1, [04/16]: Training Loss: 3.106194703, Training Accuracy: 22.592\n",
            "Worker 1, [05/16]: Training Loss: 3.006119040, Training Accuracy: 23.632\n",
            "Worker 1, [06/16]: Training Loss: 2.954118544, Training Accuracy: 25.472\n",
            "Worker 1, [07/16]: Training Loss: 2.868628444, Training Accuracy: 26.704\n",
            "Worker 1, [08/16]: Training Loss: 2.824626550, Training Accuracy: 27.440\n",
            "Worker 1, [09/16]: Training Loss: 2.727173350, Training Accuracy: 29.040\n",
            "Worker 1, [10/16]: Training Loss: 2.679162517, Training Accuracy: 30.512\n",
            "Worker 1, [11/16]: Training Loss: 2.607859947, Training Accuracy: 31.952\n",
            "Worker 1, [12/16]: Training Loss: 2.560121758, Training Accuracy: 32.672\n",
            "Worker 1, [13/16]: Training Loss: 2.524596402, Training Accuracy: 33.152\n",
            "Worker 1, [14/16]: Training Loss: 2.432315672, Training Accuracy: 35.184\n",
            "Worker 1, [15/16]: Training Loss: 2.380715041, Training Accuracy: 35.840\n",
            "Worker 1, [16/16]: Training Loss: 2.338054656, Training Accuracy: 37.360\n",
            "Time taken for training worker 1: 0:00:49.985105\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 3.343772234, Training Accuracy: 18.640\n",
            "Worker 2, [02/16]: Training Loss: 3.246804690, Training Accuracy: 20.592\n",
            "Worker 2, [03/16]: Training Loss: 3.126404942, Training Accuracy: 21.376\n",
            "Worker 2, [04/16]: Training Loss: 3.043440953, Training Accuracy: 23.408\n",
            "Worker 2, [05/16]: Training Loss: 2.972244063, Training Accuracy: 25.248\n",
            "Worker 2, [06/16]: Training Loss: 2.907656570, Training Accuracy: 26.304\n",
            "Worker 2, [07/16]: Training Loss: 2.839794529, Training Accuracy: 27.264\n",
            "Worker 2, [08/16]: Training Loss: 2.761056457, Training Accuracy: 28.736\n",
            "Worker 2, [09/16]: Training Loss: 2.703714595, Training Accuracy: 29.584\n",
            "Worker 2, [10/16]: Training Loss: 2.659801768, Training Accuracy: 30.880\n",
            "Worker 2, [11/16]: Training Loss: 2.556903243, Training Accuracy: 31.728\n",
            "Worker 2, [12/16]: Training Loss: 2.484216043, Training Accuracy: 33.280\n",
            "Worker 2, [13/16]: Training Loss: 2.436952552, Training Accuracy: 34.448\n",
            "Worker 2, [14/16]: Training Loss: 2.365298224, Training Accuracy: 36.016\n",
            "Worker 2, [15/16]: Training Loss: 2.341541652, Training Accuracy: 37.792\n",
            "Worker 2, [16/16]: Training Loss: 2.273685302, Training Accuracy: 38.672\n",
            "Time taken for training worker 2: 0:00:49.284717\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 3.391117441, Training Accuracy: 18.160\n",
            "Worker 3, [02/16]: Training Loss: 3.237095784, Training Accuracy: 19.936\n",
            "Worker 3, [03/16]: Training Loss: 3.156980310, Training Accuracy: 21.824\n",
            "Worker 3, [04/16]: Training Loss: 3.068020755, Training Accuracy: 23.088\n",
            "Worker 3, [05/16]: Training Loss: 2.992359451, Training Accuracy: 24.368\n",
            "Worker 3, [06/16]: Training Loss: 2.923572253, Training Accuracy: 26.528\n",
            "Worker 3, [07/16]: Training Loss: 2.867017797, Training Accuracy: 26.208\n",
            "Worker 3, [08/16]: Training Loss: 2.781559698, Training Accuracy: 28.064\n",
            "Worker 3, [09/16]: Training Loss: 2.725410839, Training Accuracy: 30.880\n",
            "Worker 3, [10/16]: Training Loss: 2.690538358, Training Accuracy: 30.368\n",
            "Worker 3, [11/16]: Training Loss: 2.626953217, Training Accuracy: 31.184\n",
            "Worker 3, [12/16]: Training Loss: 2.530336288, Training Accuracy: 33.104\n",
            "Worker 3, [13/16]: Training Loss: 2.490686054, Training Accuracy: 34.288\n",
            "Worker 3, [14/16]: Training Loss: 2.424125787, Training Accuracy: 35.616\n",
            "Worker 3, [15/16]: Training Loss: 2.341089252, Training Accuracy: 37.440\n",
            "Worker 3, [16/16]: Training Loss: 2.297284753, Training Accuracy: 38.416\n",
            "Time taken for training worker 3: 0:00:49.717637\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 3.396988105, Training Accuracy: 18.224\n",
            "Worker 4, [02/16]: Training Loss: 3.250793306, Training Accuracy: 20.192\n",
            "Worker 4, [03/16]: Training Loss: 3.185219030, Training Accuracy: 21.440\n",
            "Worker 4, [04/16]: Training Loss: 3.098370871, Training Accuracy: 22.688\n",
            "Worker 4, [05/16]: Training Loss: 3.015300760, Training Accuracy: 23.616\n",
            "Worker 4, [06/16]: Training Loss: 2.942584471, Training Accuracy: 25.408\n",
            "Worker 4, [07/16]: Training Loss: 2.870436335, Training Accuracy: 26.992\n",
            "Worker 4, [08/16]: Training Loss: 2.805001254, Training Accuracy: 28.624\n",
            "Worker 4, [09/16]: Training Loss: 2.731548971, Training Accuracy: 29.216\n",
            "Worker 4, [10/16]: Training Loss: 2.675684523, Training Accuracy: 29.664\n",
            "Worker 4, [11/16]: Training Loss: 2.582316844, Training Accuracy: 32.480\n",
            "Worker 4, [12/16]: Training Loss: 2.557253828, Training Accuracy: 32.784\n",
            "Worker 4, [13/16]: Training Loss: 2.480170479, Training Accuracy: 33.648\n",
            "Worker 4, [14/16]: Training Loss: 2.429867335, Training Accuracy: 35.024\n",
            "Worker 4, [15/16]: Training Loss: 2.366902471, Training Accuracy: 36.720\n",
            "Worker 4, [16/16]: Training Loss: 2.316640785, Training Accuracy: 36.688\n",
            "Time taken for training worker 4: 0:00:49.266923\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 3.390852578, Training Accuracy: 18.144\n",
            "Worker 5, [02/16]: Training Loss: 3.250130770, Training Accuracy: 20.320\n",
            "Worker 5, [03/16]: Training Loss: 3.145980711, Training Accuracy: 21.456\n",
            "Worker 5, [04/16]: Training Loss: 3.097450777, Training Accuracy: 23.424\n",
            "Worker 5, [05/16]: Training Loss: 2.990025126, Training Accuracy: 25.248\n",
            "Worker 5, [06/16]: Training Loss: 2.916726545, Training Accuracy: 26.000\n",
            "Worker 5, [07/16]: Training Loss: 2.856623097, Training Accuracy: 27.552\n",
            "Worker 5, [08/16]: Training Loss: 2.782049357, Training Accuracy: 28.784\n",
            "Worker 5, [09/16]: Training Loss: 2.702159273, Training Accuracy: 29.952\n",
            "Worker 5, [10/16]: Training Loss: 2.657971942, Training Accuracy: 30.848\n",
            "Worker 5, [11/16]: Training Loss: 2.603687756, Training Accuracy: 32.560\n",
            "Worker 5, [12/16]: Training Loss: 2.528822317, Training Accuracy: 34.000\n",
            "Worker 5, [13/16]: Training Loss: 2.452714893, Training Accuracy: 34.752\n",
            "Worker 5, [14/16]: Training Loss: 2.395406390, Training Accuracy: 35.712\n",
            "Worker 5, [15/16]: Training Loss: 2.354394638, Training Accuracy: 37.888\n",
            "Worker 5, [16/16]: Training Loss: 2.272432209, Training Accuracy: 39.184\n",
            "Time taken for training worker 5: 0:00:49.981433\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 3.358514236, Training Accuracy: 18.304\n",
            "Worker 6, [02/16]: Training Loss: 3.199329520, Training Accuracy: 20.880\n",
            "Worker 6, [03/16]: Training Loss: 3.132588980, Training Accuracy: 21.904\n",
            "Worker 6, [04/16]: Training Loss: 3.058981618, Training Accuracy: 23.232\n",
            "Worker 6, [05/16]: Training Loss: 2.979798913, Training Accuracy: 24.464\n",
            "Worker 6, [06/16]: Training Loss: 2.920230977, Training Accuracy: 26.896\n",
            "Worker 6, [07/16]: Training Loss: 2.827032566, Training Accuracy: 28.256\n",
            "Worker 6, [08/16]: Training Loss: 2.742905245, Training Accuracy: 29.184\n",
            "Worker 6, [09/16]: Training Loss: 2.724317913, Training Accuracy: 29.072\n",
            "Worker 6, [10/16]: Training Loss: 2.650315555, Training Accuracy: 31.232\n",
            "Worker 6, [11/16]: Training Loss: 2.583989907, Training Accuracy: 32.272\n",
            "Worker 6, [12/16]: Training Loss: 2.556988344, Training Accuracy: 32.816\n",
            "Worker 6, [13/16]: Training Loss: 2.451664957, Training Accuracy: 35.088\n",
            "Worker 6, [14/16]: Training Loss: 2.414758829, Training Accuracy: 35.088\n",
            "Worker 6, [15/16]: Training Loss: 2.368280109, Training Accuracy: 36.608\n",
            "Worker 6, [16/16]: Training Loss: 2.319566427, Training Accuracy: 37.904\n",
            "Time taken for training worker 6: 0:00:50.069371\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 3.379565670, Training Accuracy: 18.576\n",
            "Worker 7, [02/16]: Training Loss: 3.233122490, Training Accuracy: 20.224\n",
            "Worker 7, [03/16]: Training Loss: 3.156010752, Training Accuracy: 21.904\n",
            "Worker 7, [04/16]: Training Loss: 3.057782969, Training Accuracy: 23.168\n",
            "Worker 7, [05/16]: Training Loss: 2.978647264, Training Accuracy: 25.296\n",
            "Worker 7, [06/16]: Training Loss: 2.910587788, Training Accuracy: 26.048\n",
            "Worker 7, [07/16]: Training Loss: 2.832412109, Training Accuracy: 27.168\n",
            "Worker 7, [08/16]: Training Loss: 2.778290483, Training Accuracy: 27.984\n",
            "Worker 7, [09/16]: Training Loss: 2.721994376, Training Accuracy: 30.000\n",
            "Worker 7, [10/16]: Training Loss: 2.623231214, Training Accuracy: 31.488\n",
            "Worker 7, [11/16]: Training Loss: 2.586509432, Training Accuracy: 32.368\n",
            "Worker 7, [12/16]: Training Loss: 2.519878018, Training Accuracy: 33.120\n",
            "Worker 7, [13/16]: Training Loss: 2.475256152, Training Accuracy: 34.112\n",
            "Worker 7, [14/16]: Training Loss: 2.377066970, Training Accuracy: 36.928\n",
            "Worker 7, [15/16]: Training Loss: 2.333267577, Training Accuracy: 37.600\n",
            "Worker 7, [16/16]: Training Loss: 2.293203561, Training Accuracy: 38.352\n",
            "Time taken for training worker 7: 0:00:49.193352\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 3.342168326, Training Accuracy: 18.672\n",
            "Worker 8, [02/16]: Training Loss: 3.202576781, Training Accuracy: 20.192\n",
            "Worker 8, [03/16]: Training Loss: 3.117487822, Training Accuracy: 22.512\n",
            "Worker 8, [04/16]: Training Loss: 3.048395330, Training Accuracy: 22.816\n",
            "Worker 8, [05/16]: Training Loss: 2.983821256, Training Accuracy: 24.512\n",
            "Worker 8, [06/16]: Training Loss: 2.894211793, Training Accuracy: 25.744\n",
            "Worker 8, [07/16]: Training Loss: 2.820306099, Training Accuracy: 27.968\n",
            "Worker 8, [08/16]: Training Loss: 2.777348596, Training Accuracy: 28.704\n",
            "Worker 8, [09/16]: Training Loss: 2.654665003, Training Accuracy: 30.560\n",
            "Worker 8, [10/16]: Training Loss: 2.632558679, Training Accuracy: 31.680\n",
            "Worker 8, [11/16]: Training Loss: 2.534224228, Training Accuracy: 33.136\n",
            "Worker 8, [12/16]: Training Loss: 2.500837020, Training Accuracy: 34.096\n",
            "Worker 8, [13/16]: Training Loss: 2.444842683, Training Accuracy: 34.960\n",
            "Worker 8, [14/16]: Training Loss: 2.387729780, Training Accuracy: 36.624\n",
            "Worker 8, [15/16]: Training Loss: 2.277178553, Training Accuracy: 38.608\n",
            "Worker 8, [16/16]: Training Loss: 2.272181541, Training Accuracy: 38.432\n",
            "Time taken for training worker 8: 0:00:50.367079\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004333\n",
            "Global Update 02: Test Loss: 2.657549629, Test Accuracy: 32.740\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.834262790, Training Accuracy: 27.776\n",
            "Worker 1, [02/16]: Training Loss: 2.713792614, Training Accuracy: 30.256\n",
            "Worker 1, [03/16]: Training Loss: 2.589441392, Training Accuracy: 32.976\n",
            "Worker 1, [04/16]: Training Loss: 2.503832968, Training Accuracy: 34.032\n",
            "Worker 1, [05/16]: Training Loss: 2.422223860, Training Accuracy: 35.472\n",
            "Worker 1, [06/16]: Training Loss: 2.367252838, Training Accuracy: 37.008\n",
            "Worker 1, [07/16]: Training Loss: 2.264858422, Training Accuracy: 38.432\n",
            "Worker 1, [08/16]: Training Loss: 2.226771405, Training Accuracy: 39.168\n",
            "Worker 1, [09/16]: Training Loss: 2.158701527, Training Accuracy: 40.928\n",
            "Worker 1, [10/16]: Training Loss: 2.075454330, Training Accuracy: 42.992\n",
            "Worker 1, [11/16]: Training Loss: 1.997307236, Training Accuracy: 44.496\n",
            "Worker 1, [12/16]: Training Loss: 1.929411141, Training Accuracy: 46.112\n",
            "Worker 1, [13/16]: Training Loss: 1.910356136, Training Accuracy: 46.336\n",
            "Worker 1, [14/16]: Training Loss: 1.836749645, Training Accuracy: 48.656\n",
            "Worker 1, [15/16]: Training Loss: 1.776925676, Training Accuracy: 49.728\n",
            "Worker 1, [16/16]: Training Loss: 1.761185184, Training Accuracy: 50.400\n",
            "Time taken for training worker 1: 0:00:50.137940\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.760850279, Training Accuracy: 29.680\n",
            "Worker 2, [02/16]: Training Loss: 2.602760669, Training Accuracy: 32.240\n",
            "Worker 2, [03/16]: Training Loss: 2.532663699, Training Accuracy: 34.192\n",
            "Worker 2, [04/16]: Training Loss: 2.437748452, Training Accuracy: 34.688\n",
            "Worker 2, [05/16]: Training Loss: 2.361192185, Training Accuracy: 37.472\n",
            "Worker 2, [06/16]: Training Loss: 2.329588362, Training Accuracy: 37.104\n",
            "Worker 2, [07/16]: Training Loss: 2.233538645, Training Accuracy: 39.728\n",
            "Worker 2, [08/16]: Training Loss: 2.133617951, Training Accuracy: 42.048\n",
            "Worker 2, [09/16]: Training Loss: 2.120355285, Training Accuracy: 41.744\n",
            "Worker 2, [10/16]: Training Loss: 2.034632033, Training Accuracy: 44.112\n",
            "Worker 2, [11/16]: Training Loss: 1.942754584, Training Accuracy: 45.728\n",
            "Worker 2, [12/16]: Training Loss: 1.937178530, Training Accuracy: 46.256\n",
            "Worker 2, [13/16]: Training Loss: 1.839450274, Training Accuracy: 49.328\n",
            "Worker 2, [14/16]: Training Loss: 1.786721313, Training Accuracy: 48.736\n",
            "Worker 2, [15/16]: Training Loss: 1.744305748, Training Accuracy: 50.864\n",
            "Worker 2, [16/16]: Training Loss: 1.677477624, Training Accuracy: 52.912\n",
            "Time taken for training worker 2: 0:00:49.839682\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.816471100, Training Accuracy: 28.256\n",
            "Worker 3, [02/16]: Training Loss: 2.655848895, Training Accuracy: 32.016\n",
            "Worker 3, [03/16]: Training Loss: 2.578128829, Training Accuracy: 33.088\n",
            "Worker 3, [04/16]: Training Loss: 2.480057040, Training Accuracy: 35.408\n",
            "Worker 3, [05/16]: Training Loss: 2.399436577, Training Accuracy: 36.512\n",
            "Worker 3, [06/16]: Training Loss: 2.350053724, Training Accuracy: 37.760\n",
            "Worker 3, [07/16]: Training Loss: 2.240324557, Training Accuracy: 39.584\n",
            "Worker 3, [08/16]: Training Loss: 2.195088782, Training Accuracy: 39.904\n",
            "Worker 3, [09/16]: Training Loss: 2.128066385, Training Accuracy: 42.448\n",
            "Worker 3, [10/16]: Training Loss: 2.073597538, Training Accuracy: 44.032\n",
            "Worker 3, [11/16]: Training Loss: 2.005035632, Training Accuracy: 44.112\n",
            "Worker 3, [12/16]: Training Loss: 1.937012891, Training Accuracy: 46.672\n",
            "Worker 3, [13/16]: Training Loss: 1.852763289, Training Accuracy: 48.320\n",
            "Worker 3, [14/16]: Training Loss: 1.808416744, Training Accuracy: 48.656\n",
            "Worker 3, [15/16]: Training Loss: 1.788744791, Training Accuracy: 50.032\n",
            "Worker 3, [16/16]: Training Loss: 1.736303483, Training Accuracy: 51.072\n",
            "Time taken for training worker 3: 0:00:50.163138\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.806096240, Training Accuracy: 28.976\n",
            "Worker 4, [02/16]: Training Loss: 2.695727494, Training Accuracy: 31.360\n",
            "Worker 4, [03/16]: Training Loss: 2.557044423, Training Accuracy: 33.664\n",
            "Worker 4, [04/16]: Training Loss: 2.496105502, Training Accuracy: 34.784\n",
            "Worker 4, [05/16]: Training Loss: 2.402766344, Training Accuracy: 36.800\n",
            "Worker 4, [06/16]: Training Loss: 2.348678107, Training Accuracy: 37.168\n",
            "Worker 4, [07/16]: Training Loss: 2.285205382, Training Accuracy: 38.128\n",
            "Worker 4, [08/16]: Training Loss: 2.196685382, Training Accuracy: 40.608\n",
            "Worker 4, [09/16]: Training Loss: 2.134031353, Training Accuracy: 41.776\n",
            "Worker 4, [10/16]: Training Loss: 2.099131608, Training Accuracy: 43.408\n",
            "Worker 4, [11/16]: Training Loss: 2.024798144, Training Accuracy: 43.840\n",
            "Worker 4, [12/16]: Training Loss: 1.923700246, Training Accuracy: 46.208\n",
            "Worker 4, [13/16]: Training Loss: 1.887239224, Training Accuracy: 47.216\n",
            "Worker 4, [14/16]: Training Loss: 1.863698948, Training Accuracy: 48.192\n",
            "Worker 4, [15/16]: Training Loss: 1.777390328, Training Accuracy: 50.128\n",
            "Worker 4, [16/16]: Training Loss: 1.717048559, Training Accuracy: 51.712\n",
            "Time taken for training worker 4: 0:00:50.573876\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 2.788236489, Training Accuracy: 29.584\n",
            "Worker 5, [02/16]: Training Loss: 2.639377703, Training Accuracy: 31.728\n",
            "Worker 5, [03/16]: Training Loss: 2.583077854, Training Accuracy: 33.408\n",
            "Worker 5, [04/16]: Training Loss: 2.468327290, Training Accuracy: 35.504\n",
            "Worker 5, [05/16]: Training Loss: 2.364305777, Training Accuracy: 37.104\n",
            "Worker 5, [06/16]: Training Loss: 2.316961820, Training Accuracy: 38.672\n",
            "Worker 5, [07/16]: Training Loss: 2.235721700, Training Accuracy: 40.832\n",
            "Worker 5, [08/16]: Training Loss: 2.169842171, Training Accuracy: 40.672\n",
            "Worker 5, [09/16]: Training Loss: 2.119064720, Training Accuracy: 43.152\n",
            "Worker 5, [10/16]: Training Loss: 2.052390315, Training Accuracy: 43.568\n",
            "Worker 5, [11/16]: Training Loss: 2.017964481, Training Accuracy: 44.912\n",
            "Worker 5, [12/16]: Training Loss: 1.913684193, Training Accuracy: 47.568\n",
            "Worker 5, [13/16]: Training Loss: 1.910251027, Training Accuracy: 47.456\n",
            "Worker 5, [14/16]: Training Loss: 1.816147383, Training Accuracy: 49.568\n",
            "Worker 5, [15/16]: Training Loss: 1.793368101, Training Accuracy: 48.768\n",
            "Worker 5, [16/16]: Training Loss: 1.720121444, Training Accuracy: 51.488\n",
            "Time taken for training worker 5: 0:00:50.123518\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 2.787965310, Training Accuracy: 29.232\n",
            "Worker 6, [02/16]: Training Loss: 2.659483511, Training Accuracy: 31.456\n",
            "Worker 6, [03/16]: Training Loss: 2.532576009, Training Accuracy: 34.256\n",
            "Worker 6, [04/16]: Training Loss: 2.452525015, Training Accuracy: 35.232\n",
            "Worker 6, [05/16]: Training Loss: 2.408670461, Training Accuracy: 35.584\n",
            "Worker 6, [06/16]: Training Loss: 2.317859600, Training Accuracy: 37.648\n",
            "Worker 6, [07/16]: Training Loss: 2.238944854, Training Accuracy: 39.776\n",
            "Worker 6, [08/16]: Training Loss: 2.186219769, Training Accuracy: 40.624\n",
            "Worker 6, [09/16]: Training Loss: 2.084267625, Training Accuracy: 42.272\n",
            "Worker 6, [10/16]: Training Loss: 2.050325958, Training Accuracy: 43.248\n",
            "Worker 6, [11/16]: Training Loss: 1.984543901, Training Accuracy: 44.368\n",
            "Worker 6, [12/16]: Training Loss: 1.926340118, Training Accuracy: 46.448\n",
            "Worker 6, [13/16]: Training Loss: 1.871966553, Training Accuracy: 47.872\n",
            "Worker 6, [14/16]: Training Loss: 1.847351400, Training Accuracy: 48.464\n",
            "Worker 6, [15/16]: Training Loss: 1.739655391, Training Accuracy: 50.752\n",
            "Worker 6, [16/16]: Training Loss: 1.710210252, Training Accuracy: 50.544\n",
            "Time taken for training worker 6: 0:00:49.586886\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 2.818444084, Training Accuracy: 28.480\n",
            "Worker 7, [02/16]: Training Loss: 2.668236110, Training Accuracy: 31.648\n",
            "Worker 7, [03/16]: Training Loss: 2.570710664, Training Accuracy: 33.168\n",
            "Worker 7, [04/16]: Training Loss: 2.453360714, Training Accuracy: 35.344\n",
            "Worker 7, [05/16]: Training Loss: 2.385897459, Training Accuracy: 37.264\n",
            "Worker 7, [06/16]: Training Loss: 2.335561095, Training Accuracy: 38.080\n",
            "Worker 7, [07/16]: Training Loss: 2.254712962, Training Accuracy: 39.424\n",
            "Worker 7, [08/16]: Training Loss: 2.180600917, Training Accuracy: 40.848\n",
            "Worker 7, [09/16]: Training Loss: 2.131725978, Training Accuracy: 42.048\n",
            "Worker 7, [10/16]: Training Loss: 2.038698393, Training Accuracy: 44.144\n",
            "Worker 7, [11/16]: Training Loss: 1.973670249, Training Accuracy: 46.256\n",
            "Worker 7, [12/16]: Training Loss: 1.942752172, Training Accuracy: 46.000\n",
            "Worker 7, [13/16]: Training Loss: 1.870130437, Training Accuracy: 47.776\n",
            "Worker 7, [14/16]: Training Loss: 1.824897270, Training Accuracy: 48.368\n",
            "Worker 7, [15/16]: Training Loss: 1.731486632, Training Accuracy: 51.328\n",
            "Worker 7, [16/16]: Training Loss: 1.719066042, Training Accuracy: 52.064\n",
            "Time taken for training worker 7: 0:00:49.812146\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 2.784732602, Training Accuracy: 29.232\n",
            "Worker 8, [02/16]: Training Loss: 2.641501777, Training Accuracy: 31.520\n",
            "Worker 8, [03/16]: Training Loss: 2.540678445, Training Accuracy: 33.920\n",
            "Worker 8, [04/16]: Training Loss: 2.466686422, Training Accuracy: 35.968\n",
            "Worker 8, [05/16]: Training Loss: 2.369888570, Training Accuracy: 37.248\n",
            "Worker 8, [06/16]: Training Loss: 2.337896146, Training Accuracy: 37.776\n",
            "Worker 8, [07/16]: Training Loss: 2.224475746, Training Accuracy: 39.840\n",
            "Worker 8, [08/16]: Training Loss: 2.179738090, Training Accuracy: 41.168\n",
            "Worker 8, [09/16]: Training Loss: 2.093013965, Training Accuracy: 42.864\n",
            "Worker 8, [10/16]: Training Loss: 2.030438781, Training Accuracy: 43.952\n",
            "Worker 8, [11/16]: Training Loss: 1.974414704, Training Accuracy: 45.440\n",
            "Worker 8, [12/16]: Training Loss: 1.931685988, Training Accuracy: 46.528\n",
            "Worker 8, [13/16]: Training Loss: 1.859984267, Training Accuracy: 48.080\n",
            "Worker 8, [14/16]: Training Loss: 1.811866225, Training Accuracy: 48.944\n",
            "Worker 8, [15/16]: Training Loss: 1.746802747, Training Accuracy: 51.072\n",
            "Worker 8, [16/16]: Training Loss: 1.695596234, Training Accuracy: 52.272\n",
            "Time taken for training worker 8: 0:00:49.947088\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004029\n",
            "Global Update 03: Test Loss: 2.415923725, Test Accuracy: 38.130\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.537649583, Training Accuracy: 35.344\n",
            "Worker 1, [02/16]: Training Loss: 2.329408788, Training Accuracy: 38.192\n",
            "Worker 1, [03/16]: Training Loss: 2.213580802, Training Accuracy: 40.368\n",
            "Worker 1, [04/16]: Training Loss: 2.098373839, Training Accuracy: 42.896\n",
            "Worker 1, [05/16]: Training Loss: 1.989019945, Training Accuracy: 45.824\n",
            "Worker 1, [06/16]: Training Loss: 1.929668665, Training Accuracy: 46.688\n",
            "Worker 1, [07/16]: Training Loss: 1.846148780, Training Accuracy: 48.672\n",
            "Worker 1, [08/16]: Training Loss: 1.744137179, Training Accuracy: 51.424\n",
            "Worker 1, [09/16]: Training Loss: 1.701177235, Training Accuracy: 51.904\n",
            "Worker 1, [10/16]: Training Loss: 1.644652117, Training Accuracy: 52.608\n",
            "Worker 1, [11/16]: Training Loss: 1.576609715, Training Accuracy: 55.408\n",
            "Worker 1, [12/16]: Training Loss: 1.544556760, Training Accuracy: 55.488\n",
            "Worker 1, [13/16]: Training Loss: 1.451659368, Training Accuracy: 58.096\n",
            "Worker 1, [14/16]: Training Loss: 1.414866334, Training Accuracy: 58.752\n",
            "Worker 1, [15/16]: Training Loss: 1.359370123, Training Accuracy: 60.176\n",
            "Worker 1, [16/16]: Training Loss: 1.360649314, Training Accuracy: 59.984\n",
            "Time taken for training worker 1: 0:00:49.290903\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.457006638, Training Accuracy: 35.488\n",
            "Worker 2, [02/16]: Training Loss: 2.253801076, Training Accuracy: 39.856\n",
            "Worker 2, [03/16]: Training Loss: 2.127205306, Training Accuracy: 42.048\n",
            "Worker 2, [04/16]: Training Loss: 2.030554282, Training Accuracy: 44.336\n",
            "Worker 2, [05/16]: Training Loss: 1.946668129, Training Accuracy: 46.208\n",
            "Worker 2, [06/16]: Training Loss: 1.871534327, Training Accuracy: 47.920\n",
            "Worker 2, [07/16]: Training Loss: 1.792113539, Training Accuracy: 49.968\n",
            "Worker 2, [08/16]: Training Loss: 1.714757846, Training Accuracy: 51.520\n",
            "Worker 2, [09/16]: Training Loss: 1.662666468, Training Accuracy: 52.624\n",
            "Worker 2, [10/16]: Training Loss: 1.577583300, Training Accuracy: 54.576\n",
            "Worker 2, [11/16]: Training Loss: 1.547216963, Training Accuracy: 55.376\n",
            "Worker 2, [12/16]: Training Loss: 1.496828621, Training Accuracy: 56.720\n",
            "Worker 2, [13/16]: Training Loss: 1.429928426, Training Accuracy: 58.656\n",
            "Worker 2, [14/16]: Training Loss: 1.366117724, Training Accuracy: 59.712\n",
            "Worker 2, [15/16]: Training Loss: 1.343072315, Training Accuracy: 60.496\n",
            "Worker 2, [16/16]: Training Loss: 1.316417054, Training Accuracy: 61.648\n",
            "Time taken for training worker 2: 0:00:49.117172\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.496538345, Training Accuracy: 35.856\n",
            "Worker 3, [02/16]: Training Loss: 2.312327789, Training Accuracy: 38.992\n",
            "Worker 3, [03/16]: Training Loss: 2.187736213, Training Accuracy: 41.568\n",
            "Worker 3, [04/16]: Training Loss: 2.050203732, Training Accuracy: 44.128\n",
            "Worker 3, [05/16]: Training Loss: 2.011427220, Training Accuracy: 44.896\n",
            "Worker 3, [06/16]: Training Loss: 1.876056158, Training Accuracy: 48.272\n",
            "Worker 3, [07/16]: Training Loss: 1.834102477, Training Accuracy: 49.184\n",
            "Worker 3, [08/16]: Training Loss: 1.708686063, Training Accuracy: 51.600\n",
            "Worker 3, [09/16]: Training Loss: 1.700450251, Training Accuracy: 52.496\n",
            "Worker 3, [10/16]: Training Loss: 1.645037059, Training Accuracy: 53.168\n",
            "Worker 3, [11/16]: Training Loss: 1.549875280, Training Accuracy: 55.024\n",
            "Worker 3, [12/16]: Training Loss: 1.506825071, Training Accuracy: 57.024\n",
            "Worker 3, [13/16]: Training Loss: 1.439407374, Training Accuracy: 58.816\n",
            "Worker 3, [14/16]: Training Loss: 1.356533464, Training Accuracy: 60.960\n",
            "Worker 3, [15/16]: Training Loss: 1.371845618, Training Accuracy: 59.776\n",
            "Worker 3, [16/16]: Training Loss: 1.345069521, Training Accuracy: 60.512\n",
            "Time taken for training worker 3: 0:00:47.993328\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.505580621, Training Accuracy: 35.120\n",
            "Worker 4, [02/16]: Training Loss: 2.299506468, Training Accuracy: 38.944\n",
            "Worker 4, [03/16]: Training Loss: 2.175315320, Training Accuracy: 41.776\n",
            "Worker 4, [04/16]: Training Loss: 2.089959829, Training Accuracy: 43.488\n",
            "Worker 4, [05/16]: Training Loss: 1.994893592, Training Accuracy: 45.312\n",
            "Worker 4, [06/16]: Training Loss: 1.907056733, Training Accuracy: 47.040\n",
            "Worker 4, [07/16]: Training Loss: 1.834670594, Training Accuracy: 48.800\n",
            "Worker 4, [08/16]: Training Loss: 1.756667767, Training Accuracy: 50.880\n",
            "Worker 4, [09/16]: Training Loss: 1.715191428, Training Accuracy: 50.864\n",
            "Worker 4, [10/16]: Training Loss: 1.635602423, Training Accuracy: 53.936\n",
            "Worker 4, [11/16]: Training Loss: 1.566442282, Training Accuracy: 55.488\n",
            "Worker 4, [12/16]: Training Loss: 1.533925046, Training Accuracy: 55.936\n",
            "Worker 4, [13/16]: Training Loss: 1.474159013, Training Accuracy: 57.424\n",
            "Worker 4, [14/16]: Training Loss: 1.441917403, Training Accuracy: 59.104\n",
            "Worker 4, [15/16]: Training Loss: 1.352342171, Training Accuracy: 60.624\n",
            "Worker 4, [16/16]: Training Loss: 1.267497008, Training Accuracy: 63.152\n",
            "Time taken for training worker 4: 0:00:49.218490\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 2.485158931, Training Accuracy: 35.936\n",
            "Worker 5, [02/16]: Training Loss: 2.277075915, Training Accuracy: 40.016\n",
            "Worker 5, [03/16]: Training Loss: 2.172284331, Training Accuracy: 41.888\n",
            "Worker 5, [04/16]: Training Loss: 2.025459371, Training Accuracy: 44.288\n",
            "Worker 5, [05/16]: Training Loss: 1.961352828, Training Accuracy: 45.776\n",
            "Worker 5, [06/16]: Training Loss: 1.848086190, Training Accuracy: 48.624\n",
            "Worker 5, [07/16]: Training Loss: 1.807901448, Training Accuracy: 49.680\n",
            "Worker 5, [08/16]: Training Loss: 1.724446370, Training Accuracy: 51.584\n",
            "Worker 5, [09/16]: Training Loss: 1.675288801, Training Accuracy: 52.352\n",
            "Worker 5, [10/16]: Training Loss: 1.598945902, Training Accuracy: 54.208\n",
            "Worker 5, [11/16]: Training Loss: 1.559679466, Training Accuracy: 55.312\n",
            "Worker 5, [12/16]: Training Loss: 1.492851160, Training Accuracy: 57.136\n",
            "Worker 5, [13/16]: Training Loss: 1.449722241, Training Accuracy: 57.584\n",
            "Worker 5, [14/16]: Training Loss: 1.403020231, Training Accuracy: 59.376\n",
            "Worker 5, [15/16]: Training Loss: 1.357105327, Training Accuracy: 60.112\n",
            "Worker 5, [16/16]: Training Loss: 1.282446270, Training Accuracy: 62.176\n",
            "Time taken for training worker 5: 0:00:50.996222\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 2.469040278, Training Accuracy: 35.488\n",
            "Worker 6, [02/16]: Training Loss: 2.286606432, Training Accuracy: 40.000\n",
            "Worker 6, [03/16]: Training Loss: 2.158149993, Training Accuracy: 41.456\n",
            "Worker 6, [04/16]: Training Loss: 2.051295321, Training Accuracy: 44.000\n",
            "Worker 6, [05/16]: Training Loss: 1.926732923, Training Accuracy: 46.896\n",
            "Worker 6, [06/16]: Training Loss: 1.875606901, Training Accuracy: 47.440\n",
            "Worker 6, [07/16]: Training Loss: 1.813975118, Training Accuracy: 48.928\n",
            "Worker 6, [08/16]: Training Loss: 1.740577660, Training Accuracy: 50.880\n",
            "Worker 6, [09/16]: Training Loss: 1.668192936, Training Accuracy: 52.720\n",
            "Worker 6, [10/16]: Training Loss: 1.593724805, Training Accuracy: 53.568\n",
            "Worker 6, [11/16]: Training Loss: 1.545854200, Training Accuracy: 55.648\n",
            "Worker 6, [12/16]: Training Loss: 1.486476183, Training Accuracy: 57.232\n",
            "Worker 6, [13/16]: Training Loss: 1.445821585, Training Accuracy: 58.576\n",
            "Worker 6, [14/16]: Training Loss: 1.424575217, Training Accuracy: 58.208\n",
            "Worker 6, [15/16]: Training Loss: 1.332340039, Training Accuracy: 61.616\n",
            "Worker 6, [16/16]: Training Loss: 1.288226776, Training Accuracy: 62.192\n",
            "Time taken for training worker 6: 0:00:49.797269\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 2.480008753, Training Accuracy: 36.336\n",
            "Worker 7, [02/16]: Training Loss: 2.268930277, Training Accuracy: 40.240\n",
            "Worker 7, [03/16]: Training Loss: 2.133000449, Training Accuracy: 42.848\n",
            "Worker 7, [04/16]: Training Loss: 2.069794975, Training Accuracy: 43.824\n",
            "Worker 7, [05/16]: Training Loss: 1.981907266, Training Accuracy: 45.888\n",
            "Worker 7, [06/16]: Training Loss: 1.866933891, Training Accuracy: 48.224\n",
            "Worker 7, [07/16]: Training Loss: 1.811287626, Training Accuracy: 48.848\n",
            "Worker 7, [08/16]: Training Loss: 1.757919939, Training Accuracy: 50.416\n",
            "Worker 7, [09/16]: Training Loss: 1.676133184, Training Accuracy: 51.824\n",
            "Worker 7, [10/16]: Training Loss: 1.619759051, Training Accuracy: 53.808\n",
            "Worker 7, [11/16]: Training Loss: 1.572679753, Training Accuracy: 54.608\n",
            "Worker 7, [12/16]: Training Loss: 1.477797681, Training Accuracy: 57.248\n",
            "Worker 7, [13/16]: Training Loss: 1.480568365, Training Accuracy: 56.992\n",
            "Worker 7, [14/16]: Training Loss: 1.390188693, Training Accuracy: 59.600\n",
            "Worker 7, [15/16]: Training Loss: 1.354129561, Training Accuracy: 60.944\n",
            "Worker 7, [16/16]: Training Loss: 1.316662174, Training Accuracy: 61.520\n",
            "Time taken for training worker 7: 0:00:48.854218\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 2.463535304, Training Accuracy: 35.888\n",
            "Worker 8, [02/16]: Training Loss: 2.277278832, Training Accuracy: 39.488\n",
            "Worker 8, [03/16]: Training Loss: 2.127960896, Training Accuracy: 42.624\n",
            "Worker 8, [04/16]: Training Loss: 2.040449304, Training Accuracy: 44.080\n",
            "Worker 8, [05/16]: Training Loss: 1.947024085, Training Accuracy: 46.160\n",
            "Worker 8, [06/16]: Training Loss: 1.870367169, Training Accuracy: 47.440\n",
            "Worker 8, [07/16]: Training Loss: 1.775666553, Training Accuracy: 48.816\n",
            "Worker 8, [08/16]: Training Loss: 1.718358042, Training Accuracy: 51.936\n",
            "Worker 8, [09/16]: Training Loss: 1.618028949, Training Accuracy: 54.144\n",
            "Worker 8, [10/16]: Training Loss: 1.606322352, Training Accuracy: 53.968\n",
            "Worker 8, [11/16]: Training Loss: 1.550365058, Training Accuracy: 55.408\n",
            "Worker 8, [12/16]: Training Loss: 1.505234685, Training Accuracy: 56.512\n",
            "Worker 8, [13/16]: Training Loss: 1.400400990, Training Accuracy: 59.600\n",
            "Worker 8, [14/16]: Training Loss: 1.374320970, Training Accuracy: 59.792\n",
            "Worker 8, [15/16]: Training Loss: 1.346552231, Training Accuracy: 60.432\n",
            "Worker 8, [16/16]: Training Loss: 1.298774216, Training Accuracy: 62.016\n",
            "Time taken for training worker 8: 0:00:50.127578\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.008494\n",
            "Global Update 04: Test Loss: 2.337622497, Test Accuracy: 41.130\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.295798213, Training Accuracy: 39.952\n",
            "Worker 1, [02/16]: Training Loss: 2.048671730, Training Accuracy: 45.168\n",
            "Worker 1, [03/16]: Training Loss: 1.887131297, Training Accuracy: 48.032\n",
            "Worker 1, [04/16]: Training Loss: 1.770770192, Training Accuracy: 50.960\n",
            "Worker 1, [05/16]: Training Loss: 1.692167196, Training Accuracy: 52.352\n",
            "Worker 1, [06/16]: Training Loss: 1.576897718, Training Accuracy: 54.512\n",
            "Worker 1, [07/16]: Training Loss: 1.485336761, Training Accuracy: 57.264\n",
            "Worker 1, [08/16]: Training Loss: 1.432526449, Training Accuracy: 58.496\n",
            "Worker 1, [09/16]: Training Loss: 1.396483874, Training Accuracy: 58.864\n",
            "Worker 1, [10/16]: Training Loss: 1.334589619, Training Accuracy: 61.136\n",
            "Worker 1, [11/16]: Training Loss: 1.237917989, Training Accuracy: 63.552\n",
            "Worker 1, [12/16]: Training Loss: 1.185575297, Training Accuracy: 65.632\n",
            "Worker 1, [13/16]: Training Loss: 1.150644916, Training Accuracy: 65.808\n",
            "Worker 1, [14/16]: Training Loss: 1.091003383, Training Accuracy: 67.728\n",
            "Worker 1, [15/16]: Training Loss: 1.084171330, Training Accuracy: 68.096\n",
            "Worker 1, [16/16]: Training Loss: 1.035585444, Training Accuracy: 69.184\n",
            "Time taken for training worker 1: 0:00:48.837485\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.237704012, Training Accuracy: 40.992\n",
            "Worker 2, [02/16]: Training Loss: 1.979577041, Training Accuracy: 45.744\n",
            "Worker 2, [03/16]: Training Loss: 1.831401916, Training Accuracy: 49.040\n",
            "Worker 2, [04/16]: Training Loss: 1.713103775, Training Accuracy: 51.616\n",
            "Worker 2, [05/16]: Training Loss: 1.631094920, Training Accuracy: 53.264\n",
            "Worker 2, [06/16]: Training Loss: 1.524781908, Training Accuracy: 55.792\n",
            "Worker 2, [07/16]: Training Loss: 1.457230029, Training Accuracy: 57.904\n",
            "Worker 2, [08/16]: Training Loss: 1.391675020, Training Accuracy: 60.144\n",
            "Worker 2, [09/16]: Training Loss: 1.335222261, Training Accuracy: 60.896\n",
            "Worker 2, [10/16]: Training Loss: 1.275556033, Training Accuracy: 62.688\n",
            "Worker 2, [11/16]: Training Loss: 1.181303091, Training Accuracy: 65.616\n",
            "Worker 2, [12/16]: Training Loss: 1.160712447, Training Accuracy: 65.952\n",
            "Worker 2, [13/16]: Training Loss: 1.124462878, Training Accuracy: 66.320\n",
            "Worker 2, [14/16]: Training Loss: 1.096475307, Training Accuracy: 67.360\n",
            "Worker 2, [15/16]: Training Loss: 1.037516719, Training Accuracy: 69.184\n",
            "Worker 2, [16/16]: Training Loss: 0.954130145, Training Accuracy: 71.392\n",
            "Time taken for training worker 2: 0:00:50.964740\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.290909414, Training Accuracy: 39.904\n",
            "Worker 3, [02/16]: Training Loss: 2.015597449, Training Accuracy: 45.344\n",
            "Worker 3, [03/16]: Training Loss: 1.862919769, Training Accuracy: 48.992\n",
            "Worker 3, [04/16]: Training Loss: 1.755249103, Training Accuracy: 51.088\n",
            "Worker 3, [05/16]: Training Loss: 1.644854196, Training Accuracy: 54.048\n",
            "Worker 3, [06/16]: Training Loss: 1.564630081, Training Accuracy: 55.728\n",
            "Worker 3, [07/16]: Training Loss: 1.455964403, Training Accuracy: 58.304\n",
            "Worker 3, [08/16]: Training Loss: 1.404766174, Training Accuracy: 58.880\n",
            "Worker 3, [09/16]: Training Loss: 1.341286093, Training Accuracy: 60.848\n",
            "Worker 3, [10/16]: Training Loss: 1.280277969, Training Accuracy: 62.896\n",
            "Worker 3, [11/16]: Training Loss: 1.229356322, Training Accuracy: 64.176\n",
            "Worker 3, [12/16]: Training Loss: 1.166868973, Training Accuracy: 65.520\n",
            "Worker 3, [13/16]: Training Loss: 1.113176171, Training Accuracy: 67.376\n",
            "Worker 3, [14/16]: Training Loss: 1.080000643, Training Accuracy: 67.856\n",
            "Worker 3, [15/16]: Training Loss: 1.032085216, Training Accuracy: 68.912\n",
            "Worker 3, [16/16]: Training Loss: 0.981372522, Training Accuracy: 70.416\n",
            "Time taken for training worker 3: 0:00:48.979493\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.281670040, Training Accuracy: 40.976\n",
            "Worker 4, [02/16]: Training Loss: 2.049280496, Training Accuracy: 45.040\n",
            "Worker 4, [03/16]: Training Loss: 1.872453252, Training Accuracy: 48.608\n",
            "Worker 4, [04/16]: Training Loss: 1.752203786, Training Accuracy: 51.792\n",
            "Worker 4, [05/16]: Training Loss: 1.624636480, Training Accuracy: 54.048\n",
            "Worker 4, [06/16]: Training Loss: 1.564208864, Training Accuracy: 55.712\n",
            "Worker 4, [07/16]: Training Loss: 1.466429120, Training Accuracy: 58.720\n",
            "Worker 4, [08/16]: Training Loss: 1.389089340, Training Accuracy: 60.352\n",
            "Worker 4, [09/16]: Training Loss: 1.337323565, Training Accuracy: 61.392\n",
            "Worker 4, [10/16]: Training Loss: 1.260732442, Training Accuracy: 63.280\n",
            "Worker 4, [11/16]: Training Loss: 1.218535951, Training Accuracy: 64.016\n",
            "Worker 4, [12/16]: Training Loss: 1.185277230, Training Accuracy: 65.072\n",
            "Worker 4, [13/16]: Training Loss: 1.129389174, Training Accuracy: 66.784\n",
            "Worker 4, [14/16]: Training Loss: 1.061332782, Training Accuracy: 68.720\n",
            "Worker 4, [15/16]: Training Loss: 1.014812851, Training Accuracy: 69.696\n",
            "Worker 4, [16/16]: Training Loss: 0.978693875, Training Accuracy: 71.168\n",
            "Time taken for training worker 4: 0:00:49.563093\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 2.237261112, Training Accuracy: 40.880\n",
            "Worker 5, [02/16]: Training Loss: 2.000507523, Training Accuracy: 45.856\n",
            "Worker 5, [03/16]: Training Loss: 1.861595912, Training Accuracy: 48.816\n",
            "Worker 5, [04/16]: Training Loss: 1.736760378, Training Accuracy: 51.600\n",
            "Worker 5, [05/16]: Training Loss: 1.647648547, Training Accuracy: 53.600\n",
            "Worker 5, [06/16]: Training Loss: 1.532629946, Training Accuracy: 56.544\n",
            "Worker 5, [07/16]: Training Loss: 1.448099814, Training Accuracy: 58.656\n",
            "Worker 5, [08/16]: Training Loss: 1.396676759, Training Accuracy: 60.384\n",
            "Worker 5, [09/16]: Training Loss: 1.312627478, Training Accuracy: 61.840\n",
            "Worker 5, [10/16]: Training Loss: 1.285212124, Training Accuracy: 61.952\n",
            "Worker 5, [11/16]: Training Loss: 1.208879877, Training Accuracy: 64.944\n",
            "Worker 5, [12/16]: Training Loss: 1.185709411, Training Accuracy: 65.136\n",
            "Worker 5, [13/16]: Training Loss: 1.092705955, Training Accuracy: 68.112\n",
            "Worker 5, [14/16]: Training Loss: 1.044911212, Training Accuracy: 68.912\n",
            "Worker 5, [15/16]: Training Loss: 1.030699822, Training Accuracy: 69.600\n",
            "Worker 5, [16/16]: Training Loss: 0.983828712, Training Accuracy: 70.224\n",
            "Time taken for training worker 5: 0:00:49.432766\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 2.233704249, Training Accuracy: 40.432\n",
            "Worker 6, [02/16]: Training Loss: 2.000160566, Training Accuracy: 45.984\n",
            "Worker 6, [03/16]: Training Loss: 1.846973843, Training Accuracy: 48.816\n",
            "Worker 6, [04/16]: Training Loss: 1.726911304, Training Accuracy: 51.552\n",
            "Worker 6, [05/16]: Training Loss: 1.623114973, Training Accuracy: 54.592\n",
            "Worker 6, [06/16]: Training Loss: 1.538325034, Training Accuracy: 55.664\n",
            "Worker 6, [07/16]: Training Loss: 1.444999240, Training Accuracy: 58.848\n",
            "Worker 6, [08/16]: Training Loss: 1.374007491, Training Accuracy: 60.080\n",
            "Worker 6, [09/16]: Training Loss: 1.342046545, Training Accuracy: 60.960\n",
            "Worker 6, [10/16]: Training Loss: 1.323527690, Training Accuracy: 62.112\n",
            "Worker 6, [11/16]: Training Loss: 1.224843910, Training Accuracy: 63.472\n",
            "Worker 6, [12/16]: Training Loss: 1.179532915, Training Accuracy: 65.248\n",
            "Worker 6, [13/16]: Training Loss: 1.106853866, Training Accuracy: 67.488\n",
            "Worker 6, [14/16]: Training Loss: 1.074970177, Training Accuracy: 67.440\n",
            "Worker 6, [15/16]: Training Loss: 1.029448513, Training Accuracy: 69.824\n",
            "Worker 6, [16/16]: Training Loss: 1.000090872, Training Accuracy: 70.160\n",
            "Time taken for training worker 6: 0:00:49.733094\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 2.248394072, Training Accuracy: 40.576\n",
            "Worker 7, [02/16]: Training Loss: 2.007232926, Training Accuracy: 46.672\n",
            "Worker 7, [03/16]: Training Loss: 1.860232987, Training Accuracy: 48.624\n",
            "Worker 7, [04/16]: Training Loss: 1.753826431, Training Accuracy: 51.408\n",
            "Worker 7, [05/16]: Training Loss: 1.611534020, Training Accuracy: 55.328\n",
            "Worker 7, [06/16]: Training Loss: 1.554770988, Training Accuracy: 56.176\n",
            "Worker 7, [07/16]: Training Loss: 1.444498757, Training Accuracy: 58.880\n",
            "Worker 7, [08/16]: Training Loss: 1.412015413, Training Accuracy: 59.328\n",
            "Worker 7, [09/16]: Training Loss: 1.316449012, Training Accuracy: 62.320\n",
            "Worker 7, [10/16]: Training Loss: 1.269774070, Training Accuracy: 62.512\n",
            "Worker 7, [11/16]: Training Loss: 1.227223499, Training Accuracy: 64.064\n",
            "Worker 7, [12/16]: Training Loss: 1.165435486, Training Accuracy: 65.888\n",
            "Worker 7, [13/16]: Training Loss: 1.130413910, Training Accuracy: 66.464\n",
            "Worker 7, [14/16]: Training Loss: 1.081525044, Training Accuracy: 67.552\n",
            "Worker 7, [15/16]: Training Loss: 1.038445507, Training Accuracy: 69.808\n",
            "Worker 7, [16/16]: Training Loss: 0.986813685, Training Accuracy: 70.528\n",
            "Time taken for training worker 7: 0:00:49.228790\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 2.218851132, Training Accuracy: 40.912\n",
            "Worker 8, [02/16]: Training Loss: 1.984569688, Training Accuracy: 45.328\n",
            "Worker 8, [03/16]: Training Loss: 1.837709173, Training Accuracy: 49.120\n",
            "Worker 8, [04/16]: Training Loss: 1.733711058, Training Accuracy: 51.744\n",
            "Worker 8, [05/16]: Training Loss: 1.618232861, Training Accuracy: 53.856\n",
            "Worker 8, [06/16]: Training Loss: 1.545940046, Training Accuracy: 55.968\n",
            "Worker 8, [07/16]: Training Loss: 1.465616158, Training Accuracy: 58.320\n",
            "Worker 8, [08/16]: Training Loss: 1.415021219, Training Accuracy: 58.784\n",
            "Worker 8, [09/16]: Training Loss: 1.349370534, Training Accuracy: 61.056\n",
            "Worker 8, [10/16]: Training Loss: 1.247666125, Training Accuracy: 63.536\n",
            "Worker 8, [11/16]: Training Loss: 1.194504142, Training Accuracy: 64.992\n",
            "Worker 8, [12/16]: Training Loss: 1.161994416, Training Accuracy: 65.328\n",
            "Worker 8, [13/16]: Training Loss: 1.106958111, Training Accuracy: 66.864\n",
            "Worker 8, [14/16]: Training Loss: 1.076005285, Training Accuracy: 68.080\n",
            "Worker 8, [15/16]: Training Loss: 1.076346963, Training Accuracy: 68.304\n",
            "Worker 8, [16/16]: Training Loss: 0.959704516, Training Accuracy: 71.472\n",
            "Time taken for training worker 8: 0:00:49.957273\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004623\n",
            "Global Update 05: Test Loss: 2.334186571, Test Accuracy: 43.840\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.107094661, Training Accuracy: 43.504\n",
            "Worker 1, [02/16]: Training Loss: 1.832575411, Training Accuracy: 49.456\n",
            "Worker 1, [03/16]: Training Loss: 1.653547765, Training Accuracy: 54.912\n",
            "Worker 1, [04/16]: Training Loss: 1.508655471, Training Accuracy: 57.840\n",
            "Worker 1, [05/16]: Training Loss: 1.400727798, Training Accuracy: 59.056\n",
            "Worker 1, [06/16]: Training Loss: 1.341007007, Training Accuracy: 61.216\n",
            "Worker 1, [07/16]: Training Loss: 1.239156888, Training Accuracy: 63.584\n",
            "Worker 1, [08/16]: Training Loss: 1.139836948, Training Accuracy: 66.544\n",
            "Worker 1, [09/16]: Training Loss: 1.095613803, Training Accuracy: 67.376\n",
            "Worker 1, [10/16]: Training Loss: 1.038573709, Training Accuracy: 69.296\n",
            "Worker 1, [11/16]: Training Loss: 0.970825318, Training Accuracy: 71.232\n",
            "Worker 1, [12/16]: Training Loss: 0.924821703, Training Accuracy: 72.336\n",
            "Worker 1, [13/16]: Training Loss: 0.869025276, Training Accuracy: 74.448\n",
            "Worker 1, [14/16]: Training Loss: 0.856132064, Training Accuracy: 74.672\n",
            "Worker 1, [15/16]: Training Loss: 0.802917973, Training Accuracy: 75.776\n",
            "Worker 1, [16/16]: Training Loss: 0.734870436, Training Accuracy: 77.984\n",
            "Time taken for training worker 1: 0:00:49.550036\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.017940115, Training Accuracy: 45.248\n",
            "Worker 2, [02/16]: Training Loss: 1.766277529, Training Accuracy: 50.752\n",
            "Worker 2, [03/16]: Training Loss: 1.585730371, Training Accuracy: 55.344\n",
            "Worker 2, [04/16]: Training Loss: 1.447185076, Training Accuracy: 59.360\n",
            "Worker 2, [05/16]: Training Loss: 1.360205208, Training Accuracy: 61.232\n",
            "Worker 2, [06/16]: Training Loss: 1.271241647, Training Accuracy: 63.456\n",
            "Worker 2, [07/16]: Training Loss: 1.204715548, Training Accuracy: 64.320\n",
            "Worker 2, [08/16]: Training Loss: 1.120193321, Training Accuracy: 66.784\n",
            "Worker 2, [09/16]: Training Loss: 1.040233776, Training Accuracy: 69.648\n",
            "Worker 2, [10/16]: Training Loss: 1.009391600, Training Accuracy: 69.984\n",
            "Worker 2, [11/16]: Training Loss: 0.942284749, Training Accuracy: 71.760\n",
            "Worker 2, [12/16]: Training Loss: 0.898572073, Training Accuracy: 73.296\n",
            "Worker 2, [13/16]: Training Loss: 0.812287952, Training Accuracy: 75.600\n",
            "Worker 2, [14/16]: Training Loss: 0.814964695, Training Accuracy: 75.344\n",
            "Worker 2, [15/16]: Training Loss: 0.802160383, Training Accuracy: 76.080\n",
            "Worker 2, [16/16]: Training Loss: 0.748643322, Training Accuracy: 77.648\n",
            "Time taken for training worker 2: 0:00:50.134167\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 2.052622374, Training Accuracy: 45.616\n",
            "Worker 3, [02/16]: Training Loss: 1.768607001, Training Accuracy: 51.616\n",
            "Worker 3, [03/16]: Training Loss: 1.647777419, Training Accuracy: 54.256\n",
            "Worker 3, [04/16]: Training Loss: 1.505874977, Training Accuracy: 57.248\n",
            "Worker 3, [05/16]: Training Loss: 1.376444312, Training Accuracy: 60.112\n",
            "Worker 3, [06/16]: Training Loss: 1.289989772, Training Accuracy: 63.040\n",
            "Worker 3, [07/16]: Training Loss: 1.214083273, Training Accuracy: 65.072\n",
            "Worker 3, [08/16]: Training Loss: 1.111801411, Training Accuracy: 67.328\n",
            "Worker 3, [09/16]: Training Loss: 1.089025654, Training Accuracy: 67.920\n",
            "Worker 3, [10/16]: Training Loss: 1.001491297, Training Accuracy: 70.256\n",
            "Worker 3, [11/16]: Training Loss: 0.965656346, Training Accuracy: 71.456\n",
            "Worker 3, [12/16]: Training Loss: 0.901106408, Training Accuracy: 73.568\n",
            "Worker 3, [13/16]: Training Loss: 0.851536129, Training Accuracy: 74.512\n",
            "Worker 3, [14/16]: Training Loss: 0.820797296, Training Accuracy: 75.392\n",
            "Worker 3, [15/16]: Training Loss: 0.802015034, Training Accuracy: 76.512\n",
            "Worker 3, [16/16]: Training Loss: 0.725826596, Training Accuracy: 78.032\n",
            "Time taken for training worker 3: 0:00:50.115134\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 2.085752696, Training Accuracy: 44.080\n",
            "Worker 4, [02/16]: Training Loss: 1.804225478, Training Accuracy: 50.384\n",
            "Worker 4, [03/16]: Training Loss: 1.650530822, Training Accuracy: 54.032\n",
            "Worker 4, [04/16]: Training Loss: 1.499125998, Training Accuracy: 57.936\n",
            "Worker 4, [05/16]: Training Loss: 1.383949196, Training Accuracy: 60.880\n",
            "Worker 4, [06/16]: Training Loss: 1.301801714, Training Accuracy: 62.912\n",
            "Worker 4, [07/16]: Training Loss: 1.217653827, Training Accuracy: 65.872\n",
            "Worker 4, [08/16]: Training Loss: 1.113956360, Training Accuracy: 67.296\n",
            "Worker 4, [09/16]: Training Loss: 1.068073968, Training Accuracy: 68.480\n",
            "Worker 4, [10/16]: Training Loss: 1.017600597, Training Accuracy: 70.208\n",
            "Worker 4, [11/16]: Training Loss: 0.968296453, Training Accuracy: 71.424\n",
            "Worker 4, [12/16]: Training Loss: 0.919253970, Training Accuracy: 72.688\n",
            "Worker 4, [13/16]: Training Loss: 0.878596049, Training Accuracy: 73.952\n",
            "Worker 4, [14/16]: Training Loss: 0.837995052, Training Accuracy: 75.088\n",
            "Worker 4, [15/16]: Training Loss: 0.787186180, Training Accuracy: 76.592\n",
            "Worker 4, [16/16]: Training Loss: 0.730797563, Training Accuracy: 77.728\n",
            "Time taken for training worker 4: 0:00:49.942902\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 2.066800554, Training Accuracy: 45.088\n",
            "Worker 5, [02/16]: Training Loss: 1.793922283, Training Accuracy: 50.928\n",
            "Worker 5, [03/16]: Training Loss: 1.635539360, Training Accuracy: 54.096\n",
            "Worker 5, [04/16]: Training Loss: 1.486000693, Training Accuracy: 57.856\n",
            "Worker 5, [05/16]: Training Loss: 1.377991919, Training Accuracy: 61.232\n",
            "Worker 5, [06/16]: Training Loss: 1.281937780, Training Accuracy: 63.136\n",
            "Worker 5, [07/16]: Training Loss: 1.206896162, Training Accuracy: 64.624\n",
            "Worker 5, [08/16]: Training Loss: 1.129811587, Training Accuracy: 67.184\n",
            "Worker 5, [09/16]: Training Loss: 1.074236439, Training Accuracy: 68.848\n",
            "Worker 5, [10/16]: Training Loss: 0.993526071, Training Accuracy: 70.752\n",
            "Worker 5, [11/16]: Training Loss: 0.929259509, Training Accuracy: 72.672\n",
            "Worker 5, [12/16]: Training Loss: 0.865389114, Training Accuracy: 74.720\n",
            "Worker 5, [13/16]: Training Loss: 0.862896981, Training Accuracy: 74.416\n",
            "Worker 5, [14/16]: Training Loss: 0.801295693, Training Accuracy: 76.016\n",
            "Worker 5, [15/16]: Training Loss: 0.749745061, Training Accuracy: 77.472\n",
            "Worker 5, [16/16]: Training Loss: 0.739831682, Training Accuracy: 77.696\n",
            "Time taken for training worker 5: 0:00:49.074597\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 2.048491749, Training Accuracy: 44.928\n",
            "Worker 6, [02/16]: Training Loss: 1.794754655, Training Accuracy: 50.528\n",
            "Worker 6, [03/16]: Training Loss: 1.585243775, Training Accuracy: 54.912\n",
            "Worker 6, [04/16]: Training Loss: 1.472232579, Training Accuracy: 57.632\n",
            "Worker 6, [05/16]: Training Loss: 1.389649226, Training Accuracy: 60.016\n",
            "Worker 6, [06/16]: Training Loss: 1.280372595, Training Accuracy: 62.480\n",
            "Worker 6, [07/16]: Training Loss: 1.203965093, Training Accuracy: 65.120\n",
            "Worker 6, [08/16]: Training Loss: 1.124834329, Training Accuracy: 66.800\n",
            "Worker 6, [09/16]: Training Loss: 1.053061214, Training Accuracy: 68.400\n",
            "Worker 6, [10/16]: Training Loss: 1.002537614, Training Accuracy: 70.144\n",
            "Worker 6, [11/16]: Training Loss: 0.955916574, Training Accuracy: 71.872\n",
            "Worker 6, [12/16]: Training Loss: 0.922083676, Training Accuracy: 72.864\n",
            "Worker 6, [13/16]: Training Loss: 0.862757586, Training Accuracy: 73.920\n",
            "Worker 6, [14/16]: Training Loss: 0.802220693, Training Accuracy: 76.048\n",
            "Worker 6, [15/16]: Training Loss: 0.778968887, Training Accuracy: 76.832\n",
            "Worker 6, [16/16]: Training Loss: 0.748969835, Training Accuracy: 77.696\n",
            "Time taken for training worker 6: 0:00:51.472794\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 2.050373813, Training Accuracy: 45.200\n",
            "Worker 7, [02/16]: Training Loss: 1.801268623, Training Accuracy: 50.848\n",
            "Worker 7, [03/16]: Training Loss: 1.614590360, Training Accuracy: 54.704\n",
            "Worker 7, [04/16]: Training Loss: 1.492289739, Training Accuracy: 57.776\n",
            "Worker 7, [05/16]: Training Loss: 1.396239942, Training Accuracy: 59.984\n",
            "Worker 7, [06/16]: Training Loss: 1.291813850, Training Accuracy: 63.248\n",
            "Worker 7, [07/16]: Training Loss: 1.190218025, Training Accuracy: 65.904\n",
            "Worker 7, [08/16]: Training Loss: 1.144421102, Training Accuracy: 66.736\n",
            "Worker 7, [09/16]: Training Loss: 1.088397915, Training Accuracy: 68.144\n",
            "Worker 7, [10/16]: Training Loss: 1.050158220, Training Accuracy: 68.496\n",
            "Worker 7, [11/16]: Training Loss: 0.968827882, Training Accuracy: 71.520\n",
            "Worker 7, [12/16]: Training Loss: 0.931245163, Training Accuracy: 71.648\n",
            "Worker 7, [13/16]: Training Loss: 0.854069610, Training Accuracy: 73.776\n",
            "Worker 7, [14/16]: Training Loss: 0.832187921, Training Accuracy: 75.088\n",
            "Worker 7, [15/16]: Training Loss: 0.791597700, Training Accuracy: 76.656\n",
            "Worker 7, [16/16]: Training Loss: 0.770363487, Training Accuracy: 76.544\n",
            "Time taken for training worker 7: 0:00:49.126457\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 2.034887069, Training Accuracy: 44.800\n",
            "Worker 8, [02/16]: Training Loss: 1.767525135, Training Accuracy: 51.488\n",
            "Worker 8, [03/16]: Training Loss: 1.620478473, Training Accuracy: 54.496\n",
            "Worker 8, [04/16]: Training Loss: 1.452994310, Training Accuracy: 58.032\n",
            "Worker 8, [05/16]: Training Loss: 1.365120792, Training Accuracy: 61.024\n",
            "Worker 8, [06/16]: Training Loss: 1.249136773, Training Accuracy: 63.488\n",
            "Worker 8, [07/16]: Training Loss: 1.196894480, Training Accuracy: 65.040\n",
            "Worker 8, [08/16]: Training Loss: 1.132607406, Training Accuracy: 66.560\n",
            "Worker 8, [09/16]: Training Loss: 1.051272781, Training Accuracy: 69.168\n",
            "Worker 8, [10/16]: Training Loss: 1.000357803, Training Accuracy: 70.224\n",
            "Worker 8, [11/16]: Training Loss: 0.950687441, Training Accuracy: 72.128\n",
            "Worker 8, [12/16]: Training Loss: 0.874646685, Training Accuracy: 74.480\n",
            "Worker 8, [13/16]: Training Loss: 0.851894310, Training Accuracy: 74.752\n",
            "Worker 8, [14/16]: Training Loss: 0.818892489, Training Accuracy: 75.712\n",
            "Worker 8, [15/16]: Training Loss: 0.818040813, Training Accuracy: 75.888\n",
            "Worker 8, [16/16]: Training Loss: 0.756969441, Training Accuracy: 77.408\n",
            "Time taken for training worker 8: 0:00:50.348252\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004698\n",
            "Global Update 06: Test Loss: 2.421614197, Test Accuracy: 44.400\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.927990512, Training Accuracy: 48.336\n",
            "Worker 1, [02/16]: Training Loss: 1.638357407, Training Accuracy: 53.648\n",
            "Worker 1, [03/16]: Training Loss: 1.456338715, Training Accuracy: 58.784\n",
            "Worker 1, [04/16]: Training Loss: 1.339133495, Training Accuracy: 62.080\n",
            "Worker 1, [05/16]: Training Loss: 1.230501211, Training Accuracy: 64.032\n",
            "Worker 1, [06/16]: Training Loss: 1.134298634, Training Accuracy: 68.048\n",
            "Worker 1, [07/16]: Training Loss: 1.041602481, Training Accuracy: 69.648\n",
            "Worker 1, [08/16]: Training Loss: 0.976198404, Training Accuracy: 71.664\n",
            "Worker 1, [09/16]: Training Loss: 0.938551946, Training Accuracy: 72.720\n",
            "Worker 1, [10/16]: Training Loss: 0.865954742, Training Accuracy: 74.560\n",
            "Worker 1, [11/16]: Training Loss: 0.850542430, Training Accuracy: 75.536\n",
            "Worker 1, [12/16]: Training Loss: 0.763328322, Training Accuracy: 78.400\n",
            "Worker 1, [13/16]: Training Loss: 0.736132502, Training Accuracy: 78.576\n",
            "Worker 1, [14/16]: Training Loss: 0.697372320, Training Accuracy: 79.728\n",
            "Worker 1, [15/16]: Training Loss: 0.641296489, Training Accuracy: 80.576\n",
            "Worker 1, [16/16]: Training Loss: 0.621643028, Training Accuracy: 81.904\n",
            "Time taken for training worker 1: 0:00:49.937600\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.875505855, Training Accuracy: 48.992\n",
            "Worker 2, [02/16]: Training Loss: 1.586699100, Training Accuracy: 55.776\n",
            "Worker 2, [03/16]: Training Loss: 1.409572547, Training Accuracy: 59.328\n",
            "Worker 2, [04/16]: Training Loss: 1.283099343, Training Accuracy: 62.880\n",
            "Worker 2, [05/16]: Training Loss: 1.162004125, Training Accuracy: 66.112\n",
            "Worker 2, [06/16]: Training Loss: 1.083335930, Training Accuracy: 68.160\n",
            "Worker 2, [07/16]: Training Loss: 1.034836266, Training Accuracy: 70.528\n",
            "Worker 2, [08/16]: Training Loss: 0.953107509, Training Accuracy: 72.576\n",
            "Worker 2, [09/16]: Training Loss: 0.876513534, Training Accuracy: 74.064\n",
            "Worker 2, [10/16]: Training Loss: 0.822127510, Training Accuracy: 75.424\n",
            "Worker 2, [11/16]: Training Loss: 0.784571199, Training Accuracy: 76.784\n",
            "Worker 2, [12/16]: Training Loss: 0.736166463, Training Accuracy: 78.176\n",
            "Worker 2, [13/16]: Training Loss: 0.696877896, Training Accuracy: 79.280\n",
            "Worker 2, [14/16]: Training Loss: 0.665301810, Training Accuracy: 80.480\n",
            "Worker 2, [15/16]: Training Loss: 0.628940723, Training Accuracy: 81.312\n",
            "Worker 2, [16/16]: Training Loss: 0.583395290, Training Accuracy: 83.136\n",
            "Time taken for training worker 2: 0:00:49.543615\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.908912200, Training Accuracy: 49.072\n",
            "Worker 3, [02/16]: Training Loss: 1.600283101, Training Accuracy: 55.184\n",
            "Worker 3, [03/16]: Training Loss: 1.444067706, Training Accuracy: 58.448\n",
            "Worker 3, [04/16]: Training Loss: 1.280146537, Training Accuracy: 63.328\n",
            "Worker 3, [05/16]: Training Loss: 1.205136738, Training Accuracy: 65.696\n",
            "Worker 3, [06/16]: Training Loss: 1.130182357, Training Accuracy: 67.424\n",
            "Worker 3, [07/16]: Training Loss: 1.004564343, Training Accuracy: 71.088\n",
            "Worker 3, [08/16]: Training Loss: 0.951881079, Training Accuracy: 72.736\n",
            "Worker 3, [09/16]: Training Loss: 0.885077098, Training Accuracy: 73.904\n",
            "Worker 3, [10/16]: Training Loss: 0.856323253, Training Accuracy: 74.976\n",
            "Worker 3, [11/16]: Training Loss: 0.797972766, Training Accuracy: 76.672\n",
            "Worker 3, [12/16]: Training Loss: 0.742537803, Training Accuracy: 78.496\n",
            "Worker 3, [13/16]: Training Loss: 0.694294425, Training Accuracy: 79.360\n",
            "Worker 3, [14/16]: Training Loss: 0.658042562, Training Accuracy: 80.736\n",
            "Worker 3, [15/16]: Training Loss: 0.641056404, Training Accuracy: 80.800\n",
            "Worker 3, [16/16]: Training Loss: 0.575253517, Training Accuracy: 83.376\n",
            "Time taken for training worker 3: 0:00:50.545460\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.914736315, Training Accuracy: 48.496\n",
            "Worker 4, [02/16]: Training Loss: 1.619179733, Training Accuracy: 54.928\n",
            "Worker 4, [03/16]: Training Loss: 1.460937156, Training Accuracy: 59.376\n",
            "Worker 4, [04/16]: Training Loss: 1.324621831, Training Accuracy: 62.512\n",
            "Worker 4, [05/16]: Training Loss: 1.198155760, Training Accuracy: 66.048\n",
            "Worker 4, [06/16]: Training Loss: 1.119503594, Training Accuracy: 67.872\n",
            "Worker 4, [07/16]: Training Loss: 1.063167565, Training Accuracy: 69.552\n",
            "Worker 4, [08/16]: Training Loss: 0.987483255, Training Accuracy: 71.104\n",
            "Worker 4, [09/16]: Training Loss: 0.919471307, Training Accuracy: 73.152\n",
            "Worker 4, [10/16]: Training Loss: 0.868759043, Training Accuracy: 74.432\n",
            "Worker 4, [11/16]: Training Loss: 0.814038004, Training Accuracy: 76.064\n",
            "Worker 4, [12/16]: Training Loss: 0.737595939, Training Accuracy: 78.800\n",
            "Worker 4, [13/16]: Training Loss: 0.712973777, Training Accuracy: 79.632\n",
            "Worker 4, [14/16]: Training Loss: 0.660335075, Training Accuracy: 80.640\n",
            "Worker 4, [15/16]: Training Loss: 0.658827554, Training Accuracy: 80.896\n",
            "Worker 4, [16/16]: Training Loss: 0.637054649, Training Accuracy: 81.232\n",
            "Time taken for training worker 4: 0:00:49.402672\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 1.892988638, Training Accuracy: 48.720\n",
            "Worker 5, [02/16]: Training Loss: 1.601192877, Training Accuracy: 55.792\n",
            "Worker 5, [03/16]: Training Loss: 1.441259681, Training Accuracy: 59.424\n",
            "Worker 5, [04/16]: Training Loss: 1.314835626, Training Accuracy: 62.528\n",
            "Worker 5, [05/16]: Training Loss: 1.215946383, Training Accuracy: 64.704\n",
            "Worker 5, [06/16]: Training Loss: 1.092597251, Training Accuracy: 68.992\n",
            "Worker 5, [07/16]: Training Loss: 1.032154418, Training Accuracy: 70.448\n",
            "Worker 5, [08/16]: Training Loss: 0.952811622, Training Accuracy: 71.824\n",
            "Worker 5, [09/16]: Training Loss: 0.875339658, Training Accuracy: 74.560\n",
            "Worker 5, [10/16]: Training Loss: 0.854218776, Training Accuracy: 74.512\n",
            "Worker 5, [11/16]: Training Loss: 0.791162489, Training Accuracy: 76.656\n",
            "Worker 5, [12/16]: Training Loss: 0.763232899, Training Accuracy: 77.360\n",
            "Worker 5, [13/16]: Training Loss: 0.709265767, Training Accuracy: 78.976\n",
            "Worker 5, [14/16]: Training Loss: 0.678264742, Training Accuracy: 80.256\n",
            "Worker 5, [15/16]: Training Loss: 0.632213603, Training Accuracy: 81.536\n",
            "Worker 5, [16/16]: Training Loss: 0.611920064, Training Accuracy: 81.808\n",
            "Time taken for training worker 5: 0:00:49.735224\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 1.909287217, Training Accuracy: 48.256\n",
            "Worker 6, [02/16]: Training Loss: 1.617889856, Training Accuracy: 54.640\n",
            "Worker 6, [03/16]: Training Loss: 1.428273800, Training Accuracy: 59.488\n",
            "Worker 6, [04/16]: Training Loss: 1.300803761, Training Accuracy: 62.720\n",
            "Worker 6, [05/16]: Training Loss: 1.199091527, Training Accuracy: 65.456\n",
            "Worker 6, [06/16]: Training Loss: 1.096381398, Training Accuracy: 67.536\n",
            "Worker 6, [07/16]: Training Loss: 1.022687988, Training Accuracy: 70.368\n",
            "Worker 6, [08/16]: Training Loss: 0.958153574, Training Accuracy: 71.984\n",
            "Worker 6, [09/16]: Training Loss: 0.895872460, Training Accuracy: 73.216\n",
            "Worker 6, [10/16]: Training Loss: 0.829265754, Training Accuracy: 75.536\n",
            "Worker 6, [11/16]: Training Loss: 0.790610874, Training Accuracy: 77.312\n",
            "Worker 6, [12/16]: Training Loss: 0.753195978, Training Accuracy: 77.968\n",
            "Worker 6, [13/16]: Training Loss: 0.702921988, Training Accuracy: 79.312\n",
            "Worker 6, [14/16]: Training Loss: 0.651385113, Training Accuracy: 81.104\n",
            "Worker 6, [15/16]: Training Loss: 0.621345266, Training Accuracy: 81.664\n",
            "Worker 6, [16/16]: Training Loss: 0.608447556, Training Accuracy: 82.208\n",
            "Time taken for training worker 6: 0:00:50.302887\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 1.909843135, Training Accuracy: 49.056\n",
            "Worker 7, [02/16]: Training Loss: 1.619334597, Training Accuracy: 55.168\n",
            "Worker 7, [03/16]: Training Loss: 1.450077539, Training Accuracy: 59.136\n",
            "Worker 7, [04/16]: Training Loss: 1.314661228, Training Accuracy: 62.560\n",
            "Worker 7, [05/16]: Training Loss: 1.211594035, Training Accuracy: 65.120\n",
            "Worker 7, [06/16]: Training Loss: 1.112482149, Training Accuracy: 67.936\n",
            "Worker 7, [07/16]: Training Loss: 1.057671324, Training Accuracy: 68.960\n",
            "Worker 7, [08/16]: Training Loss: 0.967504695, Training Accuracy: 71.856\n",
            "Worker 7, [09/16]: Training Loss: 0.909953855, Training Accuracy: 73.008\n",
            "Worker 7, [10/16]: Training Loss: 0.864480753, Training Accuracy: 74.288\n",
            "Worker 7, [11/16]: Training Loss: 0.797847431, Training Accuracy: 76.640\n",
            "Worker 7, [12/16]: Training Loss: 0.780893251, Training Accuracy: 77.440\n",
            "Worker 7, [13/16]: Training Loss: 0.719828656, Training Accuracy: 78.448\n",
            "Worker 7, [14/16]: Training Loss: 0.683066583, Training Accuracy: 80.304\n",
            "Worker 7, [15/16]: Training Loss: 0.666417449, Training Accuracy: 80.928\n",
            "Worker 7, [16/16]: Training Loss: 0.613519593, Training Accuracy: 81.536\n",
            "Time taken for training worker 7: 0:00:48.930534\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 1.909423020, Training Accuracy: 47.888\n",
            "Worker 8, [02/16]: Training Loss: 1.593645076, Training Accuracy: 55.552\n",
            "Worker 8, [03/16]: Training Loss: 1.427510548, Training Accuracy: 59.248\n",
            "Worker 8, [04/16]: Training Loss: 1.283571056, Training Accuracy: 63.024\n",
            "Worker 8, [05/16]: Training Loss: 1.198028959, Training Accuracy: 65.120\n",
            "Worker 8, [06/16]: Training Loss: 1.109069360, Training Accuracy: 67.360\n",
            "Worker 8, [07/16]: Training Loss: 1.028065601, Training Accuracy: 70.096\n",
            "Worker 8, [08/16]: Training Loss: 0.960751766, Training Accuracy: 71.712\n",
            "Worker 8, [09/16]: Training Loss: 0.883096207, Training Accuracy: 74.096\n",
            "Worker 8, [10/16]: Training Loss: 0.855502553, Training Accuracy: 75.104\n",
            "Worker 8, [11/16]: Training Loss: 0.821912890, Training Accuracy: 75.584\n",
            "Worker 8, [12/16]: Training Loss: 0.739390353, Training Accuracy: 78.016\n",
            "Worker 8, [13/16]: Training Loss: 0.709453888, Training Accuracy: 79.184\n",
            "Worker 8, [14/16]: Training Loss: 0.668918074, Training Accuracy: 79.696\n",
            "Worker 8, [15/16]: Training Loss: 0.628972354, Training Accuracy: 81.488\n",
            "Worker 8, [16/16]: Training Loss: 0.601074869, Training Accuracy: 83.008\n",
            "Time taken for training worker 8: 0:00:48.597917\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004416\n",
            "Global Update 07: Test Loss: 2.494624335, Test Accuracy: 45.710\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.792491064, Training Accuracy: 51.792\n",
            "Worker 1, [02/16]: Training Loss: 1.544640249, Training Accuracy: 55.952\n",
            "Worker 1, [03/16]: Training Loss: 1.374566098, Training Accuracy: 61.584\n",
            "Worker 1, [04/16]: Training Loss: 1.247163476, Training Accuracy: 63.968\n",
            "Worker 1, [05/16]: Training Loss: 1.184119749, Training Accuracy: 66.464\n",
            "Worker 1, [06/16]: Training Loss: 1.103678411, Training Accuracy: 67.680\n",
            "Worker 1, [07/16]: Training Loss: 1.032591272, Training Accuracy: 70.416\n",
            "Worker 1, [08/16]: Training Loss: 0.963584325, Training Accuracy: 72.176\n",
            "Worker 1, [09/16]: Training Loss: 0.889748261, Training Accuracy: 74.112\n",
            "Worker 1, [10/16]: Training Loss: 0.891160356, Training Accuracy: 74.464\n",
            "Worker 1, [11/16]: Training Loss: 0.805914183, Training Accuracy: 77.408\n",
            "Worker 1, [12/16]: Training Loss: 0.735299017, Training Accuracy: 78.896\n",
            "Worker 1, [13/16]: Training Loss: 0.727876971, Training Accuracy: 79.248\n",
            "Worker 1, [14/16]: Training Loss: 0.701681733, Training Accuracy: 79.936\n",
            "Worker 1, [15/16]: Training Loss: 0.668080639, Training Accuracy: 81.568\n",
            "Worker 1, [16/16]: Training Loss: 0.636985874, Training Accuracy: 82.000\n",
            "Time taken for training worker 1: 0:00:49.804470\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.718072131, Training Accuracy: 51.584\n",
            "Worker 2, [02/16]: Training Loss: 1.477660142, Training Accuracy: 58.064\n",
            "Worker 2, [03/16]: Training Loss: 1.316423779, Training Accuracy: 62.320\n",
            "Worker 2, [04/16]: Training Loss: 1.228326360, Training Accuracy: 64.704\n",
            "Worker 2, [05/16]: Training Loss: 1.124111861, Training Accuracy: 67.072\n",
            "Worker 2, [06/16]: Training Loss: 1.053294732, Training Accuracy: 69.664\n",
            "Worker 2, [07/16]: Training Loss: 0.988039049, Training Accuracy: 70.448\n",
            "Worker 2, [08/16]: Training Loss: 0.954455827, Training Accuracy: 72.016\n",
            "Worker 2, [09/16]: Training Loss: 0.863700627, Training Accuracy: 74.784\n",
            "Worker 2, [10/16]: Training Loss: 0.816020222, Training Accuracy: 76.992\n",
            "Worker 2, [11/16]: Training Loss: 0.774255419, Training Accuracy: 78.096\n",
            "Worker 2, [12/16]: Training Loss: 0.730255014, Training Accuracy: 78.608\n",
            "Worker 2, [13/16]: Training Loss: 0.703237849, Training Accuracy: 79.856\n",
            "Worker 2, [14/16]: Training Loss: 0.669676181, Training Accuracy: 81.136\n",
            "Worker 2, [15/16]: Training Loss: 0.627028383, Training Accuracy: 81.856\n",
            "Worker 2, [16/16]: Training Loss: 0.615984879, Training Accuracy: 82.160\n",
            "Time taken for training worker 2: 0:00:49.787145\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.756417563, Training Accuracy: 52.496\n",
            "Worker 3, [02/16]: Training Loss: 1.499488940, Training Accuracy: 57.840\n",
            "Worker 3, [03/16]: Training Loss: 1.346759506, Training Accuracy: 61.616\n",
            "Worker 3, [04/16]: Training Loss: 1.239657609, Training Accuracy: 64.880\n",
            "Worker 3, [05/16]: Training Loss: 1.130626008, Training Accuracy: 67.792\n",
            "Worker 3, [06/16]: Training Loss: 1.057478811, Training Accuracy: 68.912\n",
            "Worker 3, [07/16]: Training Loss: 0.997423873, Training Accuracy: 71.696\n",
            "Worker 3, [08/16]: Training Loss: 0.917895344, Training Accuracy: 72.960\n",
            "Worker 3, [09/16]: Training Loss: 0.882155666, Training Accuracy: 75.056\n",
            "Worker 3, [10/16]: Training Loss: 0.841427245, Training Accuracy: 75.824\n",
            "Worker 3, [11/16]: Training Loss: 0.795216326, Training Accuracy: 77.120\n",
            "Worker 3, [12/16]: Training Loss: 0.747543793, Training Accuracy: 78.192\n",
            "Worker 3, [13/16]: Training Loss: 0.692484055, Training Accuracy: 79.488\n",
            "Worker 3, [14/16]: Training Loss: 0.666942526, Training Accuracy: 80.576\n",
            "Worker 3, [15/16]: Training Loss: 0.648462663, Training Accuracy: 81.424\n",
            "Worker 3, [16/16]: Training Loss: 0.591742497, Training Accuracy: 83.504\n",
            "Time taken for training worker 3: 0:00:49.569404\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.784312523, Training Accuracy: 51.328\n",
            "Worker 4, [02/16]: Training Loss: 1.520510393, Training Accuracy: 57.104\n",
            "Worker 4, [03/16]: Training Loss: 1.350846235, Training Accuracy: 61.680\n",
            "Worker 4, [04/16]: Training Loss: 1.248252055, Training Accuracy: 64.576\n",
            "Worker 4, [05/16]: Training Loss: 1.136083490, Training Accuracy: 67.680\n",
            "Worker 4, [06/16]: Training Loss: 1.071572688, Training Accuracy: 69.792\n",
            "Worker 4, [07/16]: Training Loss: 1.023758131, Training Accuracy: 70.432\n",
            "Worker 4, [08/16]: Training Loss: 0.963337575, Training Accuracy: 72.608\n",
            "Worker 4, [09/16]: Training Loss: 0.915234847, Training Accuracy: 74.624\n",
            "Worker 4, [10/16]: Training Loss: 0.864166758, Training Accuracy: 75.408\n",
            "Worker 4, [11/16]: Training Loss: 0.788696392, Training Accuracy: 77.456\n",
            "Worker 4, [12/16]: Training Loss: 0.759173451, Training Accuracy: 78.736\n",
            "Worker 4, [13/16]: Training Loss: 0.733621743, Training Accuracy: 78.432\n",
            "Worker 4, [14/16]: Training Loss: 0.671741274, Training Accuracy: 80.976\n",
            "Worker 4, [15/16]: Training Loss: 0.654439115, Training Accuracy: 81.808\n",
            "Worker 4, [16/16]: Training Loss: 0.621231567, Training Accuracy: 82.384\n",
            "Time taken for training worker 4: 0:00:50.165411\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 1.756316852, Training Accuracy: 52.128\n",
            "Worker 5, [02/16]: Training Loss: 1.517502155, Training Accuracy: 56.832\n",
            "Worker 5, [03/16]: Training Loss: 1.350455886, Training Accuracy: 61.776\n",
            "Worker 5, [04/16]: Training Loss: 1.248442937, Training Accuracy: 64.368\n",
            "Worker 5, [05/16]: Training Loss: 1.149382142, Training Accuracy: 67.056\n",
            "Worker 5, [06/16]: Training Loss: 1.079602237, Training Accuracy: 68.960\n",
            "Worker 5, [07/16]: Training Loss: 1.023345242, Training Accuracy: 70.256\n",
            "Worker 5, [08/16]: Training Loss: 0.956821442, Training Accuracy: 72.368\n",
            "Worker 5, [09/16]: Training Loss: 0.869867578, Training Accuracy: 75.616\n",
            "Worker 5, [10/16]: Training Loss: 0.834578788, Training Accuracy: 75.824\n",
            "Worker 5, [11/16]: Training Loss: 0.804280581, Training Accuracy: 77.056\n",
            "Worker 5, [12/16]: Training Loss: 0.745228784, Training Accuracy: 78.608\n",
            "Worker 5, [13/16]: Training Loss: 0.732281851, Training Accuracy: 78.528\n",
            "Worker 5, [14/16]: Training Loss: 0.672016547, Training Accuracy: 80.272\n",
            "Worker 5, [15/16]: Training Loss: 0.647030737, Training Accuracy: 81.280\n",
            "Worker 5, [16/16]: Training Loss: 0.634449916, Training Accuracy: 81.376\n",
            "Time taken for training worker 5: 0:00:50.317283\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 1.746454710, Training Accuracy: 51.936\n",
            "Worker 6, [02/16]: Training Loss: 1.504873076, Training Accuracy: 57.920\n",
            "Worker 6, [03/16]: Training Loss: 1.343705719, Training Accuracy: 61.536\n",
            "Worker 6, [04/16]: Training Loss: 1.240163142, Training Accuracy: 64.672\n",
            "Worker 6, [05/16]: Training Loss: 1.147423408, Training Accuracy: 66.944\n",
            "Worker 6, [06/16]: Training Loss: 1.075651214, Training Accuracy: 68.640\n",
            "Worker 6, [07/16]: Training Loss: 1.006676633, Training Accuracy: 70.576\n",
            "Worker 6, [08/16]: Training Loss: 0.945064980, Training Accuracy: 73.088\n",
            "Worker 6, [09/16]: Training Loss: 0.882951989, Training Accuracy: 73.808\n",
            "Worker 6, [10/16]: Training Loss: 0.841432911, Training Accuracy: 75.760\n",
            "Worker 6, [11/16]: Training Loss: 0.796720952, Training Accuracy: 77.520\n",
            "Worker 6, [12/16]: Training Loss: 0.756484728, Training Accuracy: 78.352\n",
            "Worker 6, [13/16]: Training Loss: 0.717970596, Training Accuracy: 79.584\n",
            "Worker 6, [14/16]: Training Loss: 0.681871255, Training Accuracy: 80.448\n",
            "Worker 6, [15/16]: Training Loss: 0.644623237, Training Accuracy: 81.904\n",
            "Worker 6, [16/16]: Training Loss: 0.599556527, Training Accuracy: 83.056\n",
            "Time taken for training worker 6: 0:00:52.259999\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 1.785189309, Training Accuracy: 51.232\n",
            "Worker 7, [02/16]: Training Loss: 1.518868663, Training Accuracy: 57.168\n",
            "Worker 7, [03/16]: Training Loss: 1.363891029, Training Accuracy: 60.864\n",
            "Worker 7, [04/16]: Training Loss: 1.257087869, Training Accuracy: 63.904\n",
            "Worker 7, [05/16]: Training Loss: 1.174872068, Training Accuracy: 66.688\n",
            "Worker 7, [06/16]: Training Loss: 1.084846114, Training Accuracy: 68.992\n",
            "Worker 7, [07/16]: Training Loss: 1.004469504, Training Accuracy: 70.736\n",
            "Worker 7, [08/16]: Training Loss: 0.977405145, Training Accuracy: 72.224\n",
            "Worker 7, [09/16]: Training Loss: 0.907603401, Training Accuracy: 73.808\n",
            "Worker 7, [10/16]: Training Loss: 0.862211842, Training Accuracy: 75.136\n",
            "Worker 7, [11/16]: Training Loss: 0.816817736, Training Accuracy: 76.112\n",
            "Worker 7, [12/16]: Training Loss: 0.754538003, Training Accuracy: 78.736\n",
            "Worker 7, [13/16]: Training Loss: 0.722343518, Training Accuracy: 79.472\n",
            "Worker 7, [14/16]: Training Loss: 0.694407036, Training Accuracy: 80.432\n",
            "Worker 7, [15/16]: Training Loss: 0.658120282, Training Accuracy: 81.728\n",
            "Worker 7, [16/16]: Training Loss: 0.631579451, Training Accuracy: 82.096\n",
            "Time taken for training worker 7: 0:00:49.871654\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 1.726325963, Training Accuracy: 53.216\n",
            "Worker 8, [02/16]: Training Loss: 1.492729671, Training Accuracy: 57.664\n",
            "Worker 8, [03/16]: Training Loss: 1.339451130, Training Accuracy: 61.840\n",
            "Worker 8, [04/16]: Training Loss: 1.235683463, Training Accuracy: 64.224\n",
            "Worker 8, [05/16]: Training Loss: 1.154538582, Training Accuracy: 66.896\n",
            "Worker 8, [06/16]: Training Loss: 1.064364421, Training Accuracy: 69.248\n",
            "Worker 8, [07/16]: Training Loss: 0.998244877, Training Accuracy: 71.696\n",
            "Worker 8, [08/16]: Training Loss: 0.942803348, Training Accuracy: 72.736\n",
            "Worker 8, [09/16]: Training Loss: 0.878861818, Training Accuracy: 74.752\n",
            "Worker 8, [10/16]: Training Loss: 0.826654873, Training Accuracy: 75.776\n",
            "Worker 8, [11/16]: Training Loss: 0.797420790, Training Accuracy: 76.944\n",
            "Worker 8, [12/16]: Training Loss: 0.747163784, Training Accuracy: 78.944\n",
            "Worker 8, [13/16]: Training Loss: 0.718329618, Training Accuracy: 79.376\n",
            "Worker 8, [14/16]: Training Loss: 0.682238684, Training Accuracy: 80.912\n",
            "Worker 8, [15/16]: Training Loss: 0.645185337, Training Accuracy: 81.968\n",
            "Worker 8, [16/16]: Training Loss: 0.637875876, Training Accuracy: 82.048\n",
            "Time taken for training worker 8: 0:00:50.370690\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004580\n",
            "Global Update 08: Test Loss: 2.469740245, Test Accuracy: 46.060\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.710238577, Training Accuracy: 53.536\n",
            "Worker 1, [02/16]: Training Loss: 1.545617152, Training Accuracy: 56.928\n",
            "Worker 1, [03/16]: Training Loss: 1.444091042, Training Accuracy: 59.008\n",
            "Worker 1, [04/16]: Training Loss: 1.380007418, Training Accuracy: 60.352\n",
            "Worker 1, [05/16]: Training Loss: 1.330526651, Training Accuracy: 61.520\n",
            "Worker 1, [06/16]: Training Loss: 1.267911238, Training Accuracy: 63.104\n",
            "Worker 1, [07/16]: Training Loss: 1.223887238, Training Accuracy: 64.256\n",
            "Worker 1, [08/16]: Training Loss: 1.197503857, Training Accuracy: 65.616\n",
            "Worker 1, [09/16]: Training Loss: 1.147989955, Training Accuracy: 67.744\n",
            "Worker 1, [10/16]: Training Loss: 1.140603115, Training Accuracy: 67.248\n",
            "Worker 1, [11/16]: Training Loss: 1.078706845, Training Accuracy: 69.024\n",
            "Worker 1, [12/16]: Training Loss: 1.062082195, Training Accuracy: 68.944\n",
            "Worker 1, [13/16]: Training Loss: 1.016028555, Training Accuracy: 70.784\n",
            "Worker 1, [14/16]: Training Loss: 0.999717563, Training Accuracy: 71.296\n",
            "Worker 1, [15/16]: Training Loss: 0.963587321, Training Accuracy: 72.976\n",
            "Worker 1, [16/16]: Training Loss: 0.952047976, Training Accuracy: 73.168\n",
            "Time taken for training worker 1: 0:00:49.112499\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.627447814, Training Accuracy: 54.912\n",
            "Worker 2, [02/16]: Training Loss: 1.462642952, Training Accuracy: 58.096\n",
            "Worker 2, [03/16]: Training Loss: 1.404785316, Training Accuracy: 59.488\n",
            "Worker 2, [04/16]: Training Loss: 1.338346776, Training Accuracy: 61.456\n",
            "Worker 2, [05/16]: Training Loss: 1.262257999, Training Accuracy: 63.040\n",
            "Worker 2, [06/16]: Training Loss: 1.219036739, Training Accuracy: 64.400\n",
            "Worker 2, [07/16]: Training Loss: 1.188829308, Training Accuracy: 65.584\n",
            "Worker 2, [08/16]: Training Loss: 1.149323450, Training Accuracy: 66.240\n",
            "Worker 2, [09/16]: Training Loss: 1.093699958, Training Accuracy: 67.952\n",
            "Worker 2, [10/16]: Training Loss: 1.056823311, Training Accuracy: 68.928\n",
            "Worker 2, [11/16]: Training Loss: 1.035188766, Training Accuracy: 69.888\n",
            "Worker 2, [12/16]: Training Loss: 1.009639351, Training Accuracy: 70.352\n",
            "Worker 2, [13/16]: Training Loss: 0.979269651, Training Accuracy: 71.296\n",
            "Worker 2, [14/16]: Training Loss: 0.954212639, Training Accuracy: 73.152\n",
            "Worker 2, [15/16]: Training Loss: 0.922644291, Training Accuracy: 73.680\n",
            "Worker 2, [16/16]: Training Loss: 0.901165725, Training Accuracy: 74.608\n",
            "Time taken for training worker 2: 0:00:50.007319\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/16]: Training Loss: 1.660840082, Training Accuracy: 55.328\n",
            "Worker 3, [02/16]: Training Loss: 1.511011271, Training Accuracy: 57.456\n",
            "Worker 3, [03/16]: Training Loss: 1.413623006, Training Accuracy: 60.416\n",
            "Worker 3, [04/16]: Training Loss: 1.345167774, Training Accuracy: 61.904\n",
            "Worker 3, [05/16]: Training Loss: 1.278414250, Training Accuracy: 63.840\n",
            "Worker 3, [06/16]: Training Loss: 1.237131122, Training Accuracy: 64.640\n",
            "Worker 3, [07/16]: Training Loss: 1.193018355, Training Accuracy: 65.104\n",
            "Worker 3, [08/16]: Training Loss: 1.157867977, Training Accuracy: 66.688\n",
            "Worker 3, [09/16]: Training Loss: 1.115793638, Training Accuracy: 68.032\n",
            "Worker 3, [10/16]: Training Loss: 1.063452294, Training Accuracy: 69.168\n",
            "Worker 3, [11/16]: Training Loss: 1.061703772, Training Accuracy: 69.648\n",
            "Worker 3, [12/16]: Training Loss: 1.005167301, Training Accuracy: 71.168\n",
            "Worker 3, [13/16]: Training Loss: 0.997359520, Training Accuracy: 71.376\n",
            "Worker 3, [14/16]: Training Loss: 0.973493069, Training Accuracy: 72.768\n",
            "Worker 3, [15/16]: Training Loss: 0.936083384, Training Accuracy: 73.408\n",
            "Worker 3, [16/16]: Training Loss: 0.902162788, Training Accuracy: 73.920\n",
            "Time taken for training worker 3: 0:00:48.689325\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/16]: Training Loss: 1.668988599, Training Accuracy: 54.496\n",
            "Worker 4, [02/16]: Training Loss: 1.508657970, Training Accuracy: 58.224\n",
            "Worker 4, [03/16]: Training Loss: 1.440149363, Training Accuracy: 58.816\n",
            "Worker 4, [04/16]: Training Loss: 1.374007516, Training Accuracy: 60.880\n",
            "Worker 4, [05/16]: Training Loss: 1.322784637, Training Accuracy: 61.936\n",
            "Worker 4, [06/16]: Training Loss: 1.260833746, Training Accuracy: 63.904\n",
            "Worker 4, [07/16]: Training Loss: 1.211949272, Training Accuracy: 65.408\n",
            "Worker 4, [08/16]: Training Loss: 1.171260198, Training Accuracy: 66.112\n",
            "Worker 4, [09/16]: Training Loss: 1.141738534, Training Accuracy: 67.648\n",
            "Worker 4, [10/16]: Training Loss: 1.106072671, Training Accuracy: 68.160\n",
            "Worker 4, [11/16]: Training Loss: 1.072075660, Training Accuracy: 69.008\n",
            "Worker 4, [12/16]: Training Loss: 1.034030591, Training Accuracy: 69.936\n",
            "Worker 4, [13/16]: Training Loss: 1.016921367, Training Accuracy: 70.608\n",
            "Worker 4, [14/16]: Training Loss: 0.986399600, Training Accuracy: 72.368\n",
            "Worker 4, [15/16]: Training Loss: 0.979519964, Training Accuracy: 72.048\n",
            "Worker 4, [16/16]: Training Loss: 0.933307508, Training Accuracy: 73.824\n",
            "Time taken for training worker 4: 0:00:48.698791\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/16]: Training Loss: 1.664170592, Training Accuracy: 54.464\n",
            "Worker 5, [02/16]: Training Loss: 1.520786360, Training Accuracy: 57.040\n",
            "Worker 5, [03/16]: Training Loss: 1.429389199, Training Accuracy: 59.280\n",
            "Worker 5, [04/16]: Training Loss: 1.352266981, Training Accuracy: 61.104\n",
            "Worker 5, [05/16]: Training Loss: 1.303106951, Training Accuracy: 63.008\n",
            "Worker 5, [06/16]: Training Loss: 1.251447765, Training Accuracy: 64.448\n",
            "Worker 5, [07/16]: Training Loss: 1.206234026, Training Accuracy: 65.184\n",
            "Worker 5, [08/16]: Training Loss: 1.171766723, Training Accuracy: 65.696\n",
            "Worker 5, [09/16]: Training Loss: 1.117142236, Training Accuracy: 67.152\n",
            "Worker 5, [10/16]: Training Loss: 1.080561079, Training Accuracy: 68.704\n",
            "Worker 5, [11/16]: Training Loss: 1.051123589, Training Accuracy: 69.040\n",
            "Worker 5, [12/16]: Training Loss: 1.038442107, Training Accuracy: 69.952\n",
            "Worker 5, [13/16]: Training Loss: 1.002676069, Training Accuracy: 70.944\n",
            "Worker 5, [14/16]: Training Loss: 0.978943319, Training Accuracy: 72.176\n",
            "Worker 5, [15/16]: Training Loss: 0.956264179, Training Accuracy: 73.088\n",
            "Worker 5, [16/16]: Training Loss: 0.935137813, Training Accuracy: 73.392\n",
            "Time taken for training worker 5: 0:00:49.483876\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/16]: Training Loss: 1.640880265, Training Accuracy: 54.672\n",
            "Worker 6, [02/16]: Training Loss: 1.502707356, Training Accuracy: 57.616\n",
            "Worker 6, [03/16]: Training Loss: 1.409418211, Training Accuracy: 59.584\n",
            "Worker 6, [04/16]: Training Loss: 1.353111117, Training Accuracy: 60.992\n",
            "Worker 6, [05/16]: Training Loss: 1.268645104, Training Accuracy: 63.904\n",
            "Worker 6, [06/16]: Training Loss: 1.254182115, Training Accuracy: 63.856\n",
            "Worker 6, [07/16]: Training Loss: 1.210550412, Training Accuracy: 64.560\n",
            "Worker 6, [08/16]: Training Loss: 1.156978602, Training Accuracy: 66.128\n",
            "Worker 6, [09/16]: Training Loss: 1.128748631, Training Accuracy: 66.864\n",
            "Worker 6, [10/16]: Training Loss: 1.103269216, Training Accuracy: 68.576\n",
            "Worker 6, [11/16]: Training Loss: 1.059832858, Training Accuracy: 69.840\n",
            "Worker 6, [12/16]: Training Loss: 1.028887942, Training Accuracy: 70.128\n",
            "Worker 6, [13/16]: Training Loss: 1.001606261, Training Accuracy: 70.464\n",
            "Worker 6, [14/16]: Training Loss: 0.967565004, Training Accuracy: 72.336\n",
            "Worker 6, [15/16]: Training Loss: 0.951162867, Training Accuracy: 72.896\n",
            "Worker 6, [16/16]: Training Loss: 0.927828249, Training Accuracy: 73.360\n",
            "Time taken for training worker 6: 0:00:49.882637\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/16]: Training Loss: 1.710088088, Training Accuracy: 53.744\n",
            "Worker 7, [02/16]: Training Loss: 1.518251290, Training Accuracy: 57.152\n",
            "Worker 7, [03/16]: Training Loss: 1.443840695, Training Accuracy: 58.720\n",
            "Worker 7, [04/16]: Training Loss: 1.386936078, Training Accuracy: 60.400\n",
            "Worker 7, [05/16]: Training Loss: 1.306493925, Training Accuracy: 61.776\n",
            "Worker 7, [06/16]: Training Loss: 1.274421621, Training Accuracy: 63.744\n",
            "Worker 7, [07/16]: Training Loss: 1.228439297, Training Accuracy: 64.752\n",
            "Worker 7, [08/16]: Training Loss: 1.175758133, Training Accuracy: 66.304\n",
            "Worker 7, [09/16]: Training Loss: 1.140961821, Training Accuracy: 66.400\n",
            "Worker 7, [10/16]: Training Loss: 1.104900826, Training Accuracy: 68.352\n",
            "Worker 7, [11/16]: Training Loss: 1.096600056, Training Accuracy: 68.320\n",
            "Worker 7, [12/16]: Training Loss: 1.035544191, Training Accuracy: 69.840\n",
            "Worker 7, [13/16]: Training Loss: 0.985740130, Training Accuracy: 70.992\n",
            "Worker 7, [14/16]: Training Loss: 0.978427852, Training Accuracy: 71.296\n",
            "Worker 7, [15/16]: Training Loss: 0.960263799, Training Accuracy: 71.936\n",
            "Worker 7, [16/16]: Training Loss: 0.929736092, Training Accuracy: 73.296\n",
            "Time taken for training worker 7: 0:00:50.471951\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/16]: Training Loss: 1.652982201, Training Accuracy: 55.008\n",
            "Worker 8, [02/16]: Training Loss: 1.500741417, Training Accuracy: 57.632\n",
            "Worker 8, [03/16]: Training Loss: 1.393065212, Training Accuracy: 60.752\n",
            "Worker 8, [04/16]: Training Loss: 1.347731428, Training Accuracy: 61.552\n",
            "Worker 8, [05/16]: Training Loss: 1.288402089, Training Accuracy: 62.800\n",
            "Worker 8, [06/16]: Training Loss: 1.239420166, Training Accuracy: 64.304\n",
            "Worker 8, [07/16]: Training Loss: 1.222543667, Training Accuracy: 64.752\n",
            "Worker 8, [08/16]: Training Loss: 1.169184103, Training Accuracy: 65.968\n",
            "Worker 8, [09/16]: Training Loss: 1.129275475, Training Accuracy: 67.504\n",
            "Worker 8, [10/16]: Training Loss: 1.077694429, Training Accuracy: 68.640\n",
            "Worker 8, [11/16]: Training Loss: 1.058189643, Training Accuracy: 69.760\n",
            "Worker 8, [12/16]: Training Loss: 1.022810815, Training Accuracy: 69.776\n",
            "Worker 8, [13/16]: Training Loss: 1.015751297, Training Accuracy: 70.240\n",
            "Worker 8, [14/16]: Training Loss: 0.961477807, Training Accuracy: 72.352\n",
            "Worker 8, [15/16]: Training Loss: 0.953758332, Training Accuracy: 72.048\n",
            "Worker 8, [16/16]: Training Loss: 0.916696573, Training Accuracy: 73.088\n",
            "Time taken for training worker 8: 0:00:50.354522\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.006196\n",
            "Global Update 09: Test Loss: 2.337245413, Test Accuracy: 46.160\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:59:51.734604\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:32\n",
            "==================================================\n",
            "Worker 1, [01/32]: Training Loss: 4.594131898, Training Accuracy: 1.264\n",
            "Worker 1, [02/32]: Training Loss: 4.422197108, Training Accuracy: 3.360\n",
            "Worker 1, [03/32]: Training Loss: 4.211429102, Training Accuracy: 5.232\n",
            "Worker 1, [04/32]: Training Loss: 4.069520493, Training Accuracy: 6.976\n",
            "Worker 1, [05/32]: Training Loss: 3.944030407, Training Accuracy: 8.496\n",
            "Worker 1, [06/32]: Training Loss: 3.851028832, Training Accuracy: 9.856\n",
            "Worker 1, [07/32]: Training Loss: 3.767156314, Training Accuracy: 11.344\n",
            "Worker 1, [08/32]: Training Loss: 3.672454824, Training Accuracy: 13.104\n",
            "Worker 1, [09/32]: Training Loss: 3.591914257, Training Accuracy: 14.288\n",
            "Worker 1, [10/32]: Training Loss: 3.512445657, Training Accuracy: 14.544\n",
            "Worker 1, [11/32]: Training Loss: 3.434599266, Training Accuracy: 16.752\n",
            "Worker 1, [12/32]: Training Loss: 3.356104296, Training Accuracy: 18.128\n",
            "Worker 1, [13/32]: Training Loss: 3.286195059, Training Accuracy: 18.400\n",
            "Worker 1, [14/32]: Training Loss: 3.184623964, Training Accuracy: 20.976\n",
            "Worker 1, [15/32]: Training Loss: 3.133703237, Training Accuracy: 21.728\n",
            "Worker 1, [16/32]: Training Loss: 3.079843504, Training Accuracy: 22.208\n",
            "Worker 1, [17/32]: Training Loss: 3.010214202, Training Accuracy: 23.984\n",
            "Worker 1, [18/32]: Training Loss: 2.967235701, Training Accuracy: 24.080\n",
            "Worker 1, [19/32]: Training Loss: 2.901578040, Training Accuracy: 25.792\n",
            "Worker 1, [20/32]: Training Loss: 2.836248828, Training Accuracy: 28.016\n",
            "Worker 1, [21/32]: Training Loss: 2.780088673, Training Accuracy: 28.528\n",
            "Worker 1, [22/32]: Training Loss: 2.731854543, Training Accuracy: 29.200\n",
            "Worker 1, [23/32]: Training Loss: 2.666093230, Training Accuracy: 30.064\n",
            "Worker 1, [24/32]: Training Loss: 2.604822699, Training Accuracy: 31.648\n",
            "Worker 1, [25/32]: Training Loss: 2.543995843, Training Accuracy: 31.904\n",
            "Worker 1, [26/32]: Training Loss: 2.480562974, Training Accuracy: 33.440\n",
            "Worker 1, [27/32]: Training Loss: 2.431975811, Training Accuracy: 35.584\n",
            "Worker 1, [28/32]: Training Loss: 2.382591526, Training Accuracy: 36.032\n",
            "Worker 1, [29/32]: Training Loss: 2.319908103, Training Accuracy: 37.296\n",
            "Worker 1, [30/32]: Training Loss: 2.290094619, Training Accuracy: 37.440\n",
            "Worker 1, [31/32]: Training Loss: 2.240141139, Training Accuracy: 39.200\n",
            "Worker 1, [32/32]: Training Loss: 2.186235681, Training Accuracy: 40.160\n",
            "Time taken for training worker 1: 0:01:41.294357\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 4.595188910, Training Accuracy: 1.552\n",
            "Worker 2, [02/32]: Training Loss: 4.390064823, Training Accuracy: 3.936\n",
            "Worker 2, [03/32]: Training Loss: 4.170081883, Training Accuracy: 5.664\n",
            "Worker 2, [04/32]: Training Loss: 4.010772617, Training Accuracy: 7.920\n",
            "Worker 2, [05/32]: Training Loss: 3.891299287, Training Accuracy: 9.584\n",
            "Worker 2, [06/32]: Training Loss: 3.786593145, Training Accuracy: 11.104\n",
            "Worker 2, [07/32]: Training Loss: 3.684802695, Training Accuracy: 13.104\n",
            "Worker 2, [08/32]: Training Loss: 3.610951759, Training Accuracy: 13.904\n",
            "Worker 2, [09/32]: Training Loss: 3.527924217, Training Accuracy: 15.152\n",
            "Worker 2, [10/32]: Training Loss: 3.434970440, Training Accuracy: 16.208\n",
            "Worker 2, [11/32]: Training Loss: 3.368397056, Training Accuracy: 16.976\n",
            "Worker 2, [12/32]: Training Loss: 3.281600349, Training Accuracy: 19.312\n",
            "Worker 2, [13/32]: Training Loss: 3.199730005, Training Accuracy: 20.656\n",
            "Worker 2, [14/32]: Training Loss: 3.144127829, Training Accuracy: 21.280\n",
            "Worker 2, [15/32]: Training Loss: 3.070193831, Training Accuracy: 22.336\n",
            "Worker 2, [16/32]: Training Loss: 3.002938438, Training Accuracy: 23.920\n",
            "Worker 2, [17/32]: Training Loss: 2.966115300, Training Accuracy: 24.080\n",
            "Worker 2, [18/32]: Training Loss: 2.893758543, Training Accuracy: 26.352\n",
            "Worker 2, [19/32]: Training Loss: 2.852894177, Training Accuracy: 26.912\n",
            "Worker 2, [20/32]: Training Loss: 2.794947940, Training Accuracy: 27.232\n",
            "Worker 2, [21/32]: Training Loss: 2.742618901, Training Accuracy: 29.232\n",
            "Worker 2, [22/32]: Training Loss: 2.677408720, Training Accuracy: 29.616\n",
            "Worker 2, [23/32]: Training Loss: 2.617419790, Training Accuracy: 31.296\n",
            "Worker 2, [24/32]: Training Loss: 2.591075279, Training Accuracy: 30.880\n",
            "Worker 2, [25/32]: Training Loss: 2.498363351, Training Accuracy: 33.216\n",
            "Worker 2, [26/32]: Training Loss: 2.470429678, Training Accuracy: 34.416\n",
            "Worker 2, [27/32]: Training Loss: 2.387872370, Training Accuracy: 36.368\n",
            "Worker 2, [28/32]: Training Loss: 2.336798685, Training Accuracy: 36.512\n",
            "Worker 2, [29/32]: Training Loss: 2.304350841, Training Accuracy: 36.976\n",
            "Worker 2, [30/32]: Training Loss: 2.244633709, Training Accuracy: 38.384\n",
            "Worker 2, [31/32]: Training Loss: 2.202306555, Training Accuracy: 39.328\n",
            "Worker 2, [32/32]: Training Loss: 2.142415637, Training Accuracy: 40.800\n",
            "Time taken for training worker 2: 0:01:39.407510\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 4.590472966, Training Accuracy: 1.488\n",
            "Worker 3, [02/32]: Training Loss: 4.366768219, Training Accuracy: 3.856\n",
            "Worker 3, [03/32]: Training Loss: 4.143428888, Training Accuracy: 5.808\n",
            "Worker 3, [04/32]: Training Loss: 3.994118715, Training Accuracy: 8.208\n",
            "Worker 3, [05/32]: Training Loss: 3.873422044, Training Accuracy: 10.752\n",
            "Worker 3, [06/32]: Training Loss: 3.775635267, Training Accuracy: 11.344\n",
            "Worker 3, [07/32]: Training Loss: 3.674302318, Training Accuracy: 12.656\n",
            "Worker 3, [08/32]: Training Loss: 3.597588337, Training Accuracy: 14.208\n",
            "Worker 3, [09/32]: Training Loss: 3.511782726, Training Accuracy: 15.456\n",
            "Worker 3, [10/32]: Training Loss: 3.426028836, Training Accuracy: 16.816\n",
            "Worker 3, [11/32]: Training Loss: 3.373530823, Training Accuracy: 17.488\n",
            "Worker 3, [12/32]: Training Loss: 3.305080431, Training Accuracy: 18.912\n",
            "Worker 3, [13/32]: Training Loss: 3.220953810, Training Accuracy: 20.352\n",
            "Worker 3, [14/32]: Training Loss: 3.172855801, Training Accuracy: 21.072\n",
            "Worker 3, [15/32]: Training Loss: 3.126229335, Training Accuracy: 21.760\n",
            "Worker 3, [16/32]: Training Loss: 3.026838227, Training Accuracy: 23.600\n",
            "Worker 3, [17/32]: Training Loss: 2.975842712, Training Accuracy: 24.720\n",
            "Worker 3, [18/32]: Training Loss: 2.912860491, Training Accuracy: 25.584\n",
            "Worker 3, [19/32]: Training Loss: 2.851860443, Training Accuracy: 26.640\n",
            "Worker 3, [20/32]: Training Loss: 2.781311921, Training Accuracy: 27.856\n",
            "Worker 3, [21/32]: Training Loss: 2.733610216, Training Accuracy: 29.184\n",
            "Worker 3, [22/32]: Training Loss: 2.671182142, Training Accuracy: 30.288\n",
            "Worker 3, [23/32]: Training Loss: 2.631017082, Training Accuracy: 30.912\n",
            "Worker 3, [24/32]: Training Loss: 2.586149257, Training Accuracy: 32.960\n",
            "Worker 3, [25/32]: Training Loss: 2.516176868, Training Accuracy: 33.328\n",
            "Worker 3, [26/32]: Training Loss: 2.440301304, Training Accuracy: 34.688\n",
            "Worker 3, [27/32]: Training Loss: 2.434820808, Training Accuracy: 34.880\n",
            "Worker 3, [28/32]: Training Loss: 2.366786261, Training Accuracy: 36.288\n",
            "Worker 3, [29/32]: Training Loss: 2.288021572, Training Accuracy: 38.032\n",
            "Worker 3, [30/32]: Training Loss: 2.248048990, Training Accuracy: 38.848\n",
            "Worker 3, [31/32]: Training Loss: 2.199747869, Training Accuracy: 39.216\n",
            "Worker 3, [32/32]: Training Loss: 2.134329230, Training Accuracy: 41.808\n",
            "Time taken for training worker 3: 0:01:40.404823\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 4.592794744, Training Accuracy: 2.240\n",
            "Worker 4, [02/32]: Training Loss: 4.382031618, Training Accuracy: 3.776\n",
            "Worker 4, [03/32]: Training Loss: 4.160641206, Training Accuracy: 5.968\n",
            "Worker 4, [04/32]: Training Loss: 4.023420142, Training Accuracy: 7.536\n",
            "Worker 4, [05/32]: Training Loss: 3.900575334, Training Accuracy: 9.360\n",
            "Worker 4, [06/32]: Training Loss: 3.804216010, Training Accuracy: 11.232\n",
            "Worker 4, [07/32]: Training Loss: 3.710908955, Training Accuracy: 12.208\n",
            "Worker 4, [08/32]: Training Loss: 3.629889097, Training Accuracy: 13.360\n",
            "Worker 4, [09/32]: Training Loss: 3.544558017, Training Accuracy: 14.784\n",
            "Worker 4, [10/32]: Training Loss: 3.483348688, Training Accuracy: 15.232\n",
            "Worker 4, [11/32]: Training Loss: 3.427012998, Training Accuracy: 17.216\n",
            "Worker 4, [12/32]: Training Loss: 3.344023432, Training Accuracy: 17.584\n",
            "Worker 4, [13/32]: Training Loss: 3.261167451, Training Accuracy: 19.072\n",
            "Worker 4, [14/32]: Training Loss: 3.218516175, Training Accuracy: 20.384\n",
            "Worker 4, [15/32]: Training Loss: 3.144969332, Training Accuracy: 21.584\n",
            "Worker 4, [16/32]: Training Loss: 3.081703826, Training Accuracy: 22.496\n",
            "Worker 4, [17/32]: Training Loss: 3.028241544, Training Accuracy: 23.200\n",
            "Worker 4, [18/32]: Training Loss: 2.964654137, Training Accuracy: 24.544\n",
            "Worker 4, [19/32]: Training Loss: 2.907345071, Training Accuracy: 26.368\n",
            "Worker 4, [20/32]: Training Loss: 2.820718578, Training Accuracy: 27.968\n",
            "Worker 4, [21/32]: Training Loss: 2.763895988, Training Accuracy: 28.208\n",
            "Worker 4, [22/32]: Training Loss: 2.717649258, Training Accuracy: 29.216\n",
            "Worker 4, [23/32]: Training Loss: 2.654900609, Training Accuracy: 30.240\n",
            "Worker 4, [24/32]: Training Loss: 2.607434927, Training Accuracy: 31.200\n",
            "Worker 4, [25/32]: Training Loss: 2.575725818, Training Accuracy: 32.416\n",
            "Worker 4, [26/32]: Training Loss: 2.496739537, Training Accuracy: 34.080\n",
            "Worker 4, [27/32]: Training Loss: 2.439247758, Training Accuracy: 35.248\n",
            "Worker 4, [28/32]: Training Loss: 2.379320777, Training Accuracy: 35.824\n",
            "Worker 4, [29/32]: Training Loss: 2.341792392, Training Accuracy: 37.440\n",
            "Worker 4, [30/32]: Training Loss: 2.276294057, Training Accuracy: 38.192\n",
            "Worker 4, [31/32]: Training Loss: 2.252352763, Training Accuracy: 38.880\n",
            "Worker 4, [32/32]: Training Loss: 2.196244107, Training Accuracy: 40.352\n",
            "Time taken for training worker 4: 0:01:40.031460\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/32]: Training Loss: 4.592158060, Training Accuracy: 1.440\n",
            "Worker 5, [02/32]: Training Loss: 4.390817720, Training Accuracy: 3.792\n",
            "Worker 5, [03/32]: Training Loss: 4.171869147, Training Accuracy: 5.984\n",
            "Worker 5, [04/32]: Training Loss: 4.026995369, Training Accuracy: 8.224\n",
            "Worker 5, [05/32]: Training Loss: 3.902197495, Training Accuracy: 9.440\n",
            "Worker 5, [06/32]: Training Loss: 3.795421009, Training Accuracy: 10.880\n",
            "Worker 5, [07/32]: Training Loss: 3.713508102, Training Accuracy: 12.640\n",
            "Worker 5, [08/32]: Training Loss: 3.615182548, Training Accuracy: 13.792\n",
            "Worker 5, [09/32]: Training Loss: 3.540670013, Training Accuracy: 15.312\n",
            "Worker 5, [10/32]: Training Loss: 3.482155603, Training Accuracy: 15.824\n",
            "Worker 5, [11/32]: Training Loss: 3.390293540, Training Accuracy: 17.712\n",
            "Worker 5, [12/32]: Training Loss: 3.327213798, Training Accuracy: 19.008\n",
            "Worker 5, [13/32]: Training Loss: 3.248199536, Training Accuracy: 20.048\n",
            "Worker 5, [14/32]: Training Loss: 3.170854432, Training Accuracy: 21.248\n",
            "Worker 5, [15/32]: Training Loss: 3.088862709, Training Accuracy: 22.560\n",
            "Worker 5, [16/32]: Training Loss: 3.051360622, Training Accuracy: 22.592\n",
            "Worker 5, [17/32]: Training Loss: 2.971597878, Training Accuracy: 24.176\n",
            "Worker 5, [18/32]: Training Loss: 2.908515444, Training Accuracy: 25.600\n",
            "Worker 5, [19/32]: Training Loss: 2.850618615, Training Accuracy: 27.136\n",
            "Worker 5, [20/32]: Training Loss: 2.766363672, Training Accuracy: 28.816\n",
            "Worker 5, [21/32]: Training Loss: 2.714652847, Training Accuracy: 29.776\n",
            "Worker 5, [22/32]: Training Loss: 2.673396660, Training Accuracy: 30.368\n",
            "Worker 5, [23/32]: Training Loss: 2.616343564, Training Accuracy: 31.008\n",
            "Worker 5, [24/32]: Training Loss: 2.562951078, Training Accuracy: 32.384\n",
            "Worker 5, [25/32]: Training Loss: 2.488821297, Training Accuracy: 34.688\n",
            "Worker 5, [26/32]: Training Loss: 2.430540613, Training Accuracy: 34.944\n",
            "Worker 5, [27/32]: Training Loss: 2.372889195, Training Accuracy: 36.784\n",
            "Worker 5, [28/32]: Training Loss: 2.338681120, Training Accuracy: 36.288\n",
            "Worker 5, [29/32]: Training Loss: 2.276216680, Training Accuracy: 38.384\n",
            "Worker 5, [30/32]: Training Loss: 2.255819716, Training Accuracy: 38.960\n",
            "Worker 5, [31/32]: Training Loss: 2.189379651, Training Accuracy: 40.512\n",
            "Worker 5, [32/32]: Training Loss: 2.118275892, Training Accuracy: 42.256\n",
            "Time taken for training worker 5: 0:01:39.058250\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/32]: Training Loss: 4.590883751, Training Accuracy: 1.696\n",
            "Worker 6, [02/32]: Training Loss: 4.362472184, Training Accuracy: 4.192\n",
            "Worker 6, [03/32]: Training Loss: 4.130157069, Training Accuracy: 6.704\n",
            "Worker 6, [04/32]: Training Loss: 3.985551467, Training Accuracy: 8.976\n",
            "Worker 6, [05/32]: Training Loss: 3.877292412, Training Accuracy: 9.776\n",
            "Worker 6, [06/32]: Training Loss: 3.775408674, Training Accuracy: 11.536\n",
            "Worker 6, [07/32]: Training Loss: 3.662208460, Training Accuracy: 13.440\n",
            "Worker 6, [08/32]: Training Loss: 3.594508242, Training Accuracy: 14.368\n",
            "Worker 6, [09/32]: Training Loss: 3.531994022, Training Accuracy: 15.488\n",
            "Worker 6, [10/32]: Training Loss: 3.443244932, Training Accuracy: 16.960\n",
            "Worker 6, [11/32]: Training Loss: 3.334127570, Training Accuracy: 18.736\n",
            "Worker 6, [12/32]: Training Loss: 3.248589988, Training Accuracy: 20.576\n",
            "Worker 6, [13/32]: Training Loss: 3.206812949, Training Accuracy: 20.800\n",
            "Worker 6, [14/32]: Training Loss: 3.131406069, Training Accuracy: 21.936\n",
            "Worker 6, [15/32]: Training Loss: 3.068077968, Training Accuracy: 22.784\n",
            "Worker 6, [16/32]: Training Loss: 2.981185843, Training Accuracy: 24.320\n",
            "Worker 6, [17/32]: Training Loss: 2.955942765, Training Accuracy: 25.488\n",
            "Worker 6, [18/32]: Training Loss: 2.880956976, Training Accuracy: 26.944\n",
            "Worker 6, [19/32]: Training Loss: 2.819039948, Training Accuracy: 27.520\n",
            "Worker 6, [20/32]: Training Loss: 2.748156385, Training Accuracy: 28.720\n",
            "Worker 6, [21/32]: Training Loss: 2.688343055, Training Accuracy: 30.176\n",
            "Worker 6, [22/32]: Training Loss: 2.648844352, Training Accuracy: 30.992\n",
            "Worker 6, [23/32]: Training Loss: 2.587509209, Training Accuracy: 30.768\n",
            "Worker 6, [24/32]: Training Loss: 2.528522288, Training Accuracy: 32.880\n",
            "Worker 6, [25/32]: Training Loss: 2.518456777, Training Accuracy: 33.040\n",
            "Worker 6, [26/32]: Training Loss: 2.419271534, Training Accuracy: 35.520\n",
            "Worker 6, [27/32]: Training Loss: 2.375441645, Training Accuracy: 35.664\n",
            "Worker 6, [28/32]: Training Loss: 2.319834943, Training Accuracy: 37.360\n",
            "Worker 6, [29/32]: Training Loss: 2.274613956, Training Accuracy: 38.560\n",
            "Worker 6, [30/32]: Training Loss: 2.204128209, Training Accuracy: 39.568\n",
            "Worker 6, [31/32]: Training Loss: 2.207189750, Training Accuracy: 39.648\n",
            "Worker 6, [32/32]: Training Loss: 2.152923774, Training Accuracy: 41.504\n",
            "Time taken for training worker 6: 0:01:40.535566\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/32]: Training Loss: 4.592369449, Training Accuracy: 1.504\n",
            "Worker 7, [02/32]: Training Loss: 4.389790180, Training Accuracy: 3.296\n",
            "Worker 7, [03/32]: Training Loss: 4.170965492, Training Accuracy: 5.712\n",
            "Worker 7, [04/32]: Training Loss: 4.006992561, Training Accuracy: 8.160\n",
            "Worker 7, [05/32]: Training Loss: 3.882806279, Training Accuracy: 9.600\n",
            "Worker 7, [06/32]: Training Loss: 3.796470384, Training Accuracy: 11.456\n",
            "Worker 7, [07/32]: Training Loss: 3.691119637, Training Accuracy: 12.400\n",
            "Worker 7, [08/32]: Training Loss: 3.604696198, Training Accuracy: 13.888\n",
            "Worker 7, [09/32]: Training Loss: 3.516946145, Training Accuracy: 15.424\n",
            "Worker 7, [10/32]: Training Loss: 3.427997944, Training Accuracy: 16.768\n",
            "Worker 7, [11/32]: Training Loss: 3.378965400, Training Accuracy: 17.632\n",
            "Worker 7, [12/32]: Training Loss: 3.298955628, Training Accuracy: 19.360\n",
            "Worker 7, [13/32]: Training Loss: 3.251177489, Training Accuracy: 20.256\n",
            "Worker 7, [14/32]: Training Loss: 3.155738916, Training Accuracy: 21.680\n",
            "Worker 7, [15/32]: Training Loss: 3.128489898, Training Accuracy: 22.144\n",
            "Worker 7, [16/32]: Training Loss: 3.032538577, Training Accuracy: 23.744\n",
            "Worker 7, [17/32]: Training Loss: 2.960095046, Training Accuracy: 24.976\n",
            "Worker 7, [18/32]: Training Loss: 2.926369146, Training Accuracy: 25.776\n",
            "Worker 7, [19/32]: Training Loss: 2.842000115, Training Accuracy: 26.912\n",
            "Worker 7, [20/32]: Training Loss: 2.789001496, Training Accuracy: 28.048\n",
            "Worker 7, [21/32]: Training Loss: 2.754122921, Training Accuracy: 29.024\n",
            "Worker 7, [22/32]: Training Loss: 2.689543546, Training Accuracy: 29.584\n",
            "Worker 7, [23/32]: Training Loss: 2.639017382, Training Accuracy: 31.248\n",
            "Worker 7, [24/32]: Training Loss: 2.559607606, Training Accuracy: 31.744\n",
            "Worker 7, [25/32]: Training Loss: 2.510991329, Training Accuracy: 32.944\n",
            "Worker 7, [26/32]: Training Loss: 2.445261950, Training Accuracy: 35.328\n",
            "Worker 7, [27/32]: Training Loss: 2.419783677, Training Accuracy: 34.864\n",
            "Worker 7, [28/32]: Training Loss: 2.345547822, Training Accuracy: 37.040\n",
            "Worker 7, [29/32]: Training Loss: 2.306492124, Training Accuracy: 37.616\n",
            "Worker 7, [30/32]: Training Loss: 2.266474751, Training Accuracy: 38.768\n",
            "Worker 7, [31/32]: Training Loss: 2.198773507, Training Accuracy: 40.000\n",
            "Worker 7, [32/32]: Training Loss: 2.149082312, Training Accuracy: 40.736\n",
            "Time taken for training worker 7: 0:01:40.171279\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/32]: Training Loss: 4.588655963, Training Accuracy: 1.056\n",
            "Worker 8, [02/32]: Training Loss: 4.358470274, Training Accuracy: 4.224\n",
            "Worker 8, [03/32]: Training Loss: 4.133203752, Training Accuracy: 6.624\n",
            "Worker 8, [04/32]: Training Loss: 3.998097186, Training Accuracy: 7.968\n",
            "Worker 8, [05/32]: Training Loss: 3.877929481, Training Accuracy: 9.440\n",
            "Worker 8, [06/32]: Training Loss: 3.769677058, Training Accuracy: 11.008\n",
            "Worker 8, [07/32]: Training Loss: 3.677910598, Training Accuracy: 12.528\n",
            "Worker 8, [08/32]: Training Loss: 3.601031590, Training Accuracy: 13.824\n",
            "Worker 8, [09/32]: Training Loss: 3.522955783, Training Accuracy: 15.472\n",
            "Worker 8, [10/32]: Training Loss: 3.430024410, Training Accuracy: 17.248\n",
            "Worker 8, [11/32]: Training Loss: 3.346224787, Training Accuracy: 18.368\n",
            "Worker 8, [12/32]: Training Loss: 3.296296232, Training Accuracy: 18.240\n",
            "Worker 8, [13/32]: Training Loss: 3.221450329, Training Accuracy: 20.144\n",
            "Worker 8, [14/32]: Training Loss: 3.143276891, Training Accuracy: 22.272\n",
            "Worker 8, [15/32]: Training Loss: 3.096702744, Training Accuracy: 22.480\n",
            "Worker 8, [16/32]: Training Loss: 3.012273285, Training Accuracy: 24.352\n",
            "Worker 8, [17/32]: Training Loss: 2.963111624, Training Accuracy: 24.880\n",
            "Worker 8, [18/32]: Training Loss: 2.896663403, Training Accuracy: 26.496\n",
            "Worker 8, [19/32]: Training Loss: 2.822113915, Training Accuracy: 27.328\n",
            "Worker 8, [20/32]: Training Loss: 2.788182220, Training Accuracy: 28.432\n",
            "Worker 8, [21/32]: Training Loss: 2.709030212, Training Accuracy: 28.928\n",
            "Worker 8, [22/32]: Training Loss: 2.657244218, Training Accuracy: 30.176\n",
            "Worker 8, [23/32]: Training Loss: 2.611961336, Training Accuracy: 31.216\n",
            "Worker 8, [24/32]: Training Loss: 2.537285408, Training Accuracy: 33.440\n",
            "Worker 8, [25/32]: Training Loss: 2.517578179, Training Accuracy: 32.464\n",
            "Worker 8, [26/32]: Training Loss: 2.446185131, Training Accuracy: 34.240\n",
            "Worker 8, [27/32]: Training Loss: 2.386038980, Training Accuracy: 36.304\n",
            "Worker 8, [28/32]: Training Loss: 2.356123930, Training Accuracy: 36.672\n",
            "Worker 8, [29/32]: Training Loss: 2.296141693, Training Accuracy: 38.544\n",
            "Worker 8, [30/32]: Training Loss: 2.234421146, Training Accuracy: 39.248\n",
            "Worker 8, [31/32]: Training Loss: 2.207950225, Training Accuracy: 39.856\n",
            "Worker 8, [32/32]: Training Loss: 2.134521261, Training Accuracy: 41.856\n",
            "Time taken for training worker 8: 0:01:39.014498\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004657\n",
            "Global Update 01: Test Loss: 3.580823042, Test Accuracy: 22.820\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 3.220051887, Training Accuracy: 21.024\n",
            "Worker 1, [02/32]: Training Loss: 2.976115183, Training Accuracy: 25.376\n",
            "Worker 1, [03/32]: Training Loss: 2.849634504, Training Accuracy: 27.120\n",
            "Worker 1, [04/32]: Training Loss: 2.747285884, Training Accuracy: 28.960\n",
            "Worker 1, [05/32]: Training Loss: 2.698017833, Training Accuracy: 30.400\n",
            "Worker 1, [06/32]: Training Loss: 2.570218694, Training Accuracy: 32.704\n",
            "Worker 1, [07/32]: Training Loss: 2.511892804, Training Accuracy: 33.232\n",
            "Worker 1, [08/32]: Training Loss: 2.423811964, Training Accuracy: 35.680\n",
            "Worker 1, [09/32]: Training Loss: 2.342803456, Training Accuracy: 37.552\n",
            "Worker 1, [10/32]: Training Loss: 2.280731035, Training Accuracy: 38.448\n",
            "Worker 1, [11/32]: Training Loss: 2.224484772, Training Accuracy: 39.920\n",
            "Worker 1, [12/32]: Training Loss: 2.160764219, Training Accuracy: 40.944\n",
            "Worker 1, [13/32]: Training Loss: 2.091904100, Training Accuracy: 42.656\n",
            "Worker 1, [14/32]: Training Loss: 2.046481604, Training Accuracy: 43.696\n",
            "Worker 1, [15/32]: Training Loss: 2.004424517, Training Accuracy: 45.248\n",
            "Worker 1, [16/32]: Training Loss: 1.910423259, Training Accuracy: 47.024\n",
            "Worker 1, [17/32]: Training Loss: 1.861041874, Training Accuracy: 47.440\n",
            "Worker 1, [18/32]: Training Loss: 1.795556702, Training Accuracy: 48.656\n",
            "Worker 1, [19/32]: Training Loss: 1.789649570, Training Accuracy: 49.072\n",
            "Worker 1, [20/32]: Training Loss: 1.693856745, Training Accuracy: 51.648\n",
            "Worker 1, [21/32]: Training Loss: 1.639667968, Training Accuracy: 52.992\n",
            "Worker 1, [22/32]: Training Loss: 1.589394288, Training Accuracy: 54.176\n",
            "Worker 1, [23/32]: Training Loss: 1.559843515, Training Accuracy: 55.520\n",
            "Worker 1, [24/32]: Training Loss: 1.512528738, Training Accuracy: 55.920\n",
            "Worker 1, [25/32]: Training Loss: 1.489193807, Training Accuracy: 56.928\n",
            "Worker 1, [26/32]: Training Loss: 1.427814815, Training Accuracy: 58.464\n",
            "Worker 1, [27/32]: Training Loss: 1.392826522, Training Accuracy: 60.272\n",
            "Worker 1, [28/32]: Training Loss: 1.343283472, Training Accuracy: 60.688\n",
            "Worker 1, [29/32]: Training Loss: 1.293719522, Training Accuracy: 62.528\n",
            "Worker 1, [30/32]: Training Loss: 1.260472933, Training Accuracy: 63.152\n",
            "Worker 1, [31/32]: Training Loss: 1.235936718, Training Accuracy: 63.472\n",
            "Worker 1, [32/32]: Training Loss: 1.229974080, Training Accuracy: 63.552\n",
            "Time taken for training worker 1: 0:01:39.599187\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 3.154957567, Training Accuracy: 21.824\n",
            "Worker 2, [02/32]: Training Loss: 2.931509278, Training Accuracy: 25.904\n",
            "Worker 2, [03/32]: Training Loss: 2.786781637, Training Accuracy: 28.336\n",
            "Worker 2, [04/32]: Training Loss: 2.687697406, Training Accuracy: 30.080\n",
            "Worker 2, [05/32]: Training Loss: 2.610015838, Training Accuracy: 31.792\n",
            "Worker 2, [06/32]: Training Loss: 2.525858374, Training Accuracy: 32.640\n",
            "Worker 2, [07/32]: Training Loss: 2.433192195, Training Accuracy: 35.328\n",
            "Worker 2, [08/32]: Training Loss: 2.334397514, Training Accuracy: 38.000\n",
            "Worker 2, [09/32]: Training Loss: 2.305817896, Training Accuracy: 38.288\n",
            "Worker 2, [10/32]: Training Loss: 2.206459332, Training Accuracy: 39.952\n",
            "Worker 2, [11/32]: Training Loss: 2.178840645, Training Accuracy: 41.104\n",
            "Worker 2, [12/32]: Training Loss: 2.099777840, Training Accuracy: 42.864\n",
            "Worker 2, [13/32]: Training Loss: 2.052408460, Training Accuracy: 44.128\n",
            "Worker 2, [14/32]: Training Loss: 1.976500332, Training Accuracy: 44.448\n",
            "Worker 2, [15/32]: Training Loss: 1.954576489, Training Accuracy: 45.664\n",
            "Worker 2, [16/32]: Training Loss: 1.829148510, Training Accuracy: 48.624\n",
            "Worker 2, [17/32]: Training Loss: 1.789641094, Training Accuracy: 49.584\n",
            "Worker 2, [18/32]: Training Loss: 1.793875537, Training Accuracy: 49.616\n",
            "Worker 2, [19/32]: Training Loss: 1.709205434, Training Accuracy: 51.232\n",
            "Worker 2, [20/32]: Training Loss: 1.611735104, Training Accuracy: 54.112\n",
            "Worker 2, [21/32]: Training Loss: 1.613513554, Training Accuracy: 53.568\n",
            "Worker 2, [22/32]: Training Loss: 1.552008642, Training Accuracy: 55.824\n",
            "Worker 2, [23/32]: Training Loss: 1.542150458, Training Accuracy: 55.424\n",
            "Worker 2, [24/32]: Training Loss: 1.508787940, Training Accuracy: 56.592\n",
            "Worker 2, [25/32]: Training Loss: 1.467146844, Training Accuracy: 58.304\n",
            "Worker 2, [26/32]: Training Loss: 1.387292447, Training Accuracy: 59.328\n",
            "Worker 2, [27/32]: Training Loss: 1.353931855, Training Accuracy: 60.608\n",
            "Worker 2, [28/32]: Training Loss: 1.314864478, Training Accuracy: 61.696\n",
            "Worker 2, [29/32]: Training Loss: 1.289099117, Training Accuracy: 62.336\n",
            "Worker 2, [30/32]: Training Loss: 1.240490796, Training Accuracy: 62.864\n",
            "Worker 2, [31/32]: Training Loss: 1.242660644, Training Accuracy: 63.712\n",
            "Worker 2, [32/32]: Training Loss: 1.174024153, Training Accuracy: 65.520\n",
            "Time taken for training worker 2: 0:01:40.173455\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 3.194141923, Training Accuracy: 21.968\n",
            "Worker 3, [02/32]: Training Loss: 2.978681995, Training Accuracy: 25.216\n",
            "Worker 3, [03/32]: Training Loss: 2.818163322, Training Accuracy: 28.032\n",
            "Worker 3, [04/32]: Training Loss: 2.717078224, Training Accuracy: 29.824\n",
            "Worker 3, [05/32]: Training Loss: 2.593608080, Training Accuracy: 32.288\n",
            "Worker 3, [06/32]: Training Loss: 2.530823511, Training Accuracy: 33.552\n",
            "Worker 3, [07/32]: Training Loss: 2.483237258, Training Accuracy: 34.528\n",
            "Worker 3, [08/32]: Training Loss: 2.392038434, Training Accuracy: 36.544\n",
            "Worker 3, [09/32]: Training Loss: 2.305387239, Training Accuracy: 37.872\n",
            "Worker 3, [10/32]: Training Loss: 2.239392637, Training Accuracy: 39.936\n",
            "Worker 3, [11/32]: Training Loss: 2.156121068, Training Accuracy: 41.712\n",
            "Worker 3, [12/32]: Training Loss: 2.079776202, Training Accuracy: 43.136\n",
            "Worker 3, [13/32]: Training Loss: 2.049903878, Training Accuracy: 44.336\n",
            "Worker 3, [14/32]: Training Loss: 2.000626133, Training Accuracy: 44.960\n",
            "Worker 3, [15/32]: Training Loss: 1.910238570, Training Accuracy: 46.032\n",
            "Worker 3, [16/32]: Training Loss: 1.837580755, Training Accuracy: 47.424\n",
            "Worker 3, [17/32]: Training Loss: 1.821323817, Training Accuracy: 48.560\n",
            "Worker 3, [18/32]: Training Loss: 1.767727948, Training Accuracy: 50.544\n",
            "Worker 3, [19/32]: Training Loss: 1.741126809, Training Accuracy: 50.528\n",
            "Worker 3, [20/32]: Training Loss: 1.666121142, Training Accuracy: 53.104\n",
            "Worker 3, [21/32]: Training Loss: 1.594865483, Training Accuracy: 55.168\n",
            "Worker 3, [22/32]: Training Loss: 1.539308380, Training Accuracy: 55.072\n",
            "Worker 3, [23/32]: Training Loss: 1.533501726, Training Accuracy: 55.440\n",
            "Worker 3, [24/32]: Training Loss: 1.479733383, Training Accuracy: 57.440\n",
            "Worker 3, [25/32]: Training Loss: 1.433925433, Training Accuracy: 58.512\n",
            "Worker 3, [26/32]: Training Loss: 1.370410089, Training Accuracy: 60.624\n",
            "Worker 3, [27/32]: Training Loss: 1.386869175, Training Accuracy: 60.192\n",
            "Worker 3, [28/32]: Training Loss: 1.342420090, Training Accuracy: 60.880\n",
            "Worker 3, [29/32]: Training Loss: 1.296986370, Training Accuracy: 62.176\n",
            "Worker 3, [30/32]: Training Loss: 1.225829769, Training Accuracy: 63.456\n",
            "Worker 3, [31/32]: Training Loss: 1.229256133, Training Accuracy: 63.776\n",
            "Worker 3, [32/32]: Training Loss: 1.158488044, Training Accuracy: 65.408\n",
            "Time taken for training worker 3: 0:01:39.384287\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 3.205150857, Training Accuracy: 21.168\n",
            "Worker 4, [02/32]: Training Loss: 2.949564778, Training Accuracy: 25.616\n",
            "Worker 4, [03/32]: Training Loss: 2.833423583, Training Accuracy: 26.944\n",
            "Worker 4, [04/32]: Training Loss: 2.735544217, Training Accuracy: 29.984\n",
            "Worker 4, [05/32]: Training Loss: 2.659648348, Training Accuracy: 30.976\n",
            "Worker 4, [06/32]: Training Loss: 2.557240579, Training Accuracy: 34.256\n",
            "Worker 4, [07/32]: Training Loss: 2.458488953, Training Accuracy: 34.768\n",
            "Worker 4, [08/32]: Training Loss: 2.408330279, Training Accuracy: 35.296\n",
            "Worker 4, [09/32]: Training Loss: 2.331293807, Training Accuracy: 37.568\n",
            "Worker 4, [10/32]: Training Loss: 2.235632229, Training Accuracy: 39.568\n",
            "Worker 4, [11/32]: Training Loss: 2.213527403, Training Accuracy: 39.712\n",
            "Worker 4, [12/32]: Training Loss: 2.114543366, Training Accuracy: 42.640\n",
            "Worker 4, [13/32]: Training Loss: 2.053061179, Training Accuracy: 43.744\n",
            "Worker 4, [14/32]: Training Loss: 2.028873537, Training Accuracy: 44.080\n",
            "Worker 4, [15/32]: Training Loss: 1.991943400, Training Accuracy: 45.664\n",
            "Worker 4, [16/32]: Training Loss: 1.925046342, Training Accuracy: 47.008\n",
            "Worker 4, [17/32]: Training Loss: 1.868302391, Training Accuracy: 47.616\n",
            "Worker 4, [18/32]: Training Loss: 1.813322283, Training Accuracy: 49.392\n",
            "Worker 4, [19/32]: Training Loss: 1.752710043, Training Accuracy: 50.960\n",
            "Worker 4, [20/32]: Training Loss: 1.698884454, Training Accuracy: 51.984\n",
            "Worker 4, [21/32]: Training Loss: 1.637544903, Training Accuracy: 53.536\n",
            "Worker 4, [22/32]: Training Loss: 1.578281249, Training Accuracy: 55.152\n",
            "Worker 4, [23/32]: Training Loss: 1.546526669, Training Accuracy: 55.680\n",
            "Worker 4, [24/32]: Training Loss: 1.502337656, Training Accuracy: 56.784\n",
            "Worker 4, [25/32]: Training Loss: 1.447365896, Training Accuracy: 58.000\n",
            "Worker 4, [26/32]: Training Loss: 1.434220199, Training Accuracy: 58.560\n",
            "Worker 4, [27/32]: Training Loss: 1.412246408, Training Accuracy: 59.648\n",
            "Worker 4, [28/32]: Training Loss: 1.333569336, Training Accuracy: 61.888\n",
            "Worker 4, [29/32]: Training Loss: 1.340636971, Training Accuracy: 60.736\n",
            "Worker 4, [30/32]: Training Loss: 1.277787766, Training Accuracy: 61.968\n",
            "Worker 4, [31/32]: Training Loss: 1.196835868, Training Accuracy: 64.816\n",
            "Worker 4, [32/32]: Training Loss: 1.264786686, Training Accuracy: 63.152\n",
            "Time taken for training worker 4: 0:01:40.041023\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/32]: Training Loss: 3.206728286, Training Accuracy: 21.632\n",
            "Worker 5, [02/32]: Training Loss: 2.937592407, Training Accuracy: 25.776\n",
            "Worker 5, [03/32]: Training Loss: 2.807963846, Training Accuracy: 28.240\n",
            "Worker 5, [04/32]: Training Loss: 2.718595072, Training Accuracy: 29.024\n",
            "Worker 5, [05/32]: Training Loss: 2.586328453, Training Accuracy: 32.320\n",
            "Worker 5, [06/32]: Training Loss: 2.531915570, Training Accuracy: 33.856\n",
            "Worker 5, [07/32]: Training Loss: 2.451327714, Training Accuracy: 35.424\n",
            "Worker 5, [08/32]: Training Loss: 2.368240811, Training Accuracy: 37.040\n",
            "Worker 5, [09/32]: Training Loss: 2.286877759, Training Accuracy: 38.912\n",
            "Worker 5, [10/32]: Training Loss: 2.241640198, Training Accuracy: 39.568\n",
            "Worker 5, [11/32]: Training Loss: 2.160831714, Training Accuracy: 41.824\n",
            "Worker 5, [12/32]: Training Loss: 2.120630482, Training Accuracy: 41.568\n",
            "Worker 5, [13/32]: Training Loss: 2.037446341, Training Accuracy: 44.368\n",
            "Worker 5, [14/32]: Training Loss: 1.971258927, Training Accuracy: 45.536\n",
            "Worker 5, [15/32]: Training Loss: 1.934346815, Training Accuracy: 46.064\n",
            "Worker 5, [16/32]: Training Loss: 1.869956588, Training Accuracy: 48.448\n",
            "Worker 5, [17/32]: Training Loss: 1.798254795, Training Accuracy: 49.136\n",
            "Worker 5, [18/32]: Training Loss: 1.753953375, Training Accuracy: 50.352\n",
            "Worker 5, [19/32]: Training Loss: 1.740397077, Training Accuracy: 50.592\n",
            "Worker 5, [20/32]: Training Loss: 1.662037167, Training Accuracy: 52.320\n",
            "Worker 5, [21/32]: Training Loss: 1.636716657, Training Accuracy: 53.248\n",
            "Worker 5, [22/32]: Training Loss: 1.584211920, Training Accuracy: 54.976\n",
            "Worker 5, [23/32]: Training Loss: 1.504046762, Training Accuracy: 56.336\n",
            "Worker 5, [24/32]: Training Loss: 1.455480546, Training Accuracy: 57.920\n",
            "Worker 5, [25/32]: Training Loss: 1.438147910, Training Accuracy: 58.240\n",
            "Worker 5, [26/32]: Training Loss: 1.389569956, Training Accuracy: 58.976\n",
            "Worker 5, [27/32]: Training Loss: 1.335063280, Training Accuracy: 61.440\n",
            "Worker 5, [28/32]: Training Loss: 1.287153817, Training Accuracy: 62.160\n",
            "Worker 5, [29/32]: Training Loss: 1.290750990, Training Accuracy: 62.144\n",
            "Worker 5, [30/32]: Training Loss: 1.218083228, Training Accuracy: 63.472\n",
            "Worker 5, [31/32]: Training Loss: 1.216817568, Training Accuracy: 64.160\n",
            "Worker 5, [32/32]: Training Loss: 1.170984398, Training Accuracy: 65.392\n",
            "Time taken for training worker 5: 0:01:37.728362\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/32]: Training Loss: 3.176562207, Training Accuracy: 22.192\n",
            "Worker 6, [02/32]: Training Loss: 2.939128260, Training Accuracy: 25.472\n",
            "Worker 6, [03/32]: Training Loss: 2.803036811, Training Accuracy: 28.224\n",
            "Worker 6, [04/32]: Training Loss: 2.694812685, Training Accuracy: 30.288\n",
            "Worker 6, [05/32]: Training Loss: 2.605855703, Training Accuracy: 32.480\n",
            "Worker 6, [06/32]: Training Loss: 2.505862359, Training Accuracy: 34.032\n",
            "Worker 6, [07/32]: Training Loss: 2.439513981, Training Accuracy: 36.064\n",
            "Worker 6, [08/32]: Training Loss: 2.397490353, Training Accuracy: 36.048\n",
            "Worker 6, [09/32]: Training Loss: 2.350416012, Training Accuracy: 36.448\n",
            "Worker 6, [10/32]: Training Loss: 2.229289359, Training Accuracy: 39.792\n",
            "Worker 6, [11/32]: Training Loss: 2.169875293, Training Accuracy: 40.656\n",
            "Worker 6, [12/32]: Training Loss: 2.131336452, Training Accuracy: 41.856\n",
            "Worker 6, [13/32]: Training Loss: 2.069223434, Training Accuracy: 42.816\n",
            "Worker 6, [14/32]: Training Loss: 1.976783787, Training Accuracy: 45.136\n",
            "Worker 6, [15/32]: Training Loss: 1.919220761, Training Accuracy: 46.896\n",
            "Worker 6, [16/32]: Training Loss: 1.895527295, Training Accuracy: 46.896\n",
            "Worker 6, [17/32]: Training Loss: 1.800188146, Training Accuracy: 49.856\n",
            "Worker 6, [18/32]: Training Loss: 1.764639528, Training Accuracy: 49.984\n",
            "Worker 6, [19/32]: Training Loss: 1.721374697, Training Accuracy: 51.664\n",
            "Worker 6, [20/32]: Training Loss: 1.663240114, Training Accuracy: 52.512\n",
            "Worker 6, [21/32]: Training Loss: 1.617163343, Training Accuracy: 53.856\n",
            "Worker 6, [22/32]: Training Loss: 1.545569164, Training Accuracy: 55.776\n",
            "Worker 6, [23/32]: Training Loss: 1.552849194, Training Accuracy: 55.152\n",
            "Worker 6, [24/32]: Training Loss: 1.489173757, Training Accuracy: 56.848\n",
            "Worker 6, [25/32]: Training Loss: 1.459996967, Training Accuracy: 58.208\n",
            "Worker 6, [26/32]: Training Loss: 1.453908963, Training Accuracy: 58.224\n",
            "Worker 6, [27/32]: Training Loss: 1.342065652, Training Accuracy: 60.544\n",
            "Worker 6, [28/32]: Training Loss: 1.345181515, Training Accuracy: 60.848\n",
            "Worker 6, [29/32]: Training Loss: 1.321204064, Training Accuracy: 61.104\n",
            "Worker 6, [30/32]: Training Loss: 1.272972794, Training Accuracy: 63.088\n",
            "Worker 6, [31/32]: Training Loss: 1.224985655, Training Accuracy: 63.632\n",
            "Worker 6, [32/32]: Training Loss: 1.167564118, Training Accuracy: 65.280\n",
            "Time taken for training worker 6: 0:01:39.336124\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/32]: Training Loss: 3.165823263, Training Accuracy: 22.368\n",
            "Worker 7, [02/32]: Training Loss: 2.923474616, Training Accuracy: 26.480\n",
            "Worker 7, [03/32]: Training Loss: 2.819824652, Training Accuracy: 27.840\n",
            "Worker 7, [04/32]: Training Loss: 2.703896644, Training Accuracy: 30.336\n",
            "Worker 7, [05/32]: Training Loss: 2.604566808, Training Accuracy: 32.160\n",
            "Worker 7, [06/32]: Training Loss: 2.525940389, Training Accuracy: 34.448\n",
            "Worker 7, [07/32]: Training Loss: 2.443809671, Training Accuracy: 35.088\n",
            "Worker 7, [08/32]: Training Loss: 2.359172949, Training Accuracy: 36.752\n",
            "Worker 7, [09/32]: Training Loss: 2.271916089, Training Accuracy: 38.096\n",
            "Worker 7, [10/32]: Training Loss: 2.233209055, Training Accuracy: 39.232\n",
            "Worker 7, [11/32]: Training Loss: 2.204505992, Training Accuracy: 40.416\n",
            "Worker 7, [12/32]: Training Loss: 2.100744612, Training Accuracy: 42.464\n",
            "Worker 7, [13/32]: Training Loss: 2.045647458, Training Accuracy: 43.632\n",
            "Worker 7, [14/32]: Training Loss: 2.002145245, Training Accuracy: 44.576\n",
            "Worker 7, [15/32]: Training Loss: 1.903553882, Training Accuracy: 47.232\n",
            "Worker 7, [16/32]: Training Loss: 1.868614711, Training Accuracy: 46.768\n",
            "Worker 7, [17/32]: Training Loss: 1.812414038, Training Accuracy: 49.856\n",
            "Worker 7, [18/32]: Training Loss: 1.735815607, Training Accuracy: 51.264\n",
            "Worker 7, [19/32]: Training Loss: 1.697181932, Training Accuracy: 51.520\n",
            "Worker 7, [20/32]: Training Loss: 1.655895873, Training Accuracy: 52.208\n",
            "Worker 7, [21/32]: Training Loss: 1.605649645, Training Accuracy: 54.176\n",
            "Worker 7, [22/32]: Training Loss: 1.556182499, Training Accuracy: 55.024\n",
            "Worker 7, [23/32]: Training Loss: 1.489805093, Training Accuracy: 56.736\n",
            "Worker 7, [24/32]: Training Loss: 1.431084643, Training Accuracy: 58.672\n",
            "Worker 7, [25/32]: Training Loss: 1.370888934, Training Accuracy: 59.392\n",
            "Worker 7, [26/32]: Training Loss: 1.356740415, Training Accuracy: 60.112\n",
            "Worker 7, [27/32]: Training Loss: 1.344483666, Training Accuracy: 60.976\n",
            "Worker 7, [28/32]: Training Loss: 1.316190699, Training Accuracy: 61.808\n",
            "Worker 7, [29/32]: Training Loss: 1.274006081, Training Accuracy: 62.224\n",
            "Worker 7, [30/32]: Training Loss: 1.243697204, Training Accuracy: 62.816\n",
            "Worker 7, [31/32]: Training Loss: 1.186829159, Training Accuracy: 64.320\n",
            "Worker 7, [32/32]: Training Loss: 1.151111530, Training Accuracy: 65.712\n",
            "Time taken for training worker 7: 0:01:41.034518\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/32]: Training Loss: 3.150599470, Training Accuracy: 22.240\n",
            "Worker 8, [02/32]: Training Loss: 2.918179677, Training Accuracy: 25.712\n",
            "Worker 8, [03/32]: Training Loss: 2.797146965, Training Accuracy: 28.496\n",
            "Worker 8, [04/32]: Training Loss: 2.678895340, Training Accuracy: 31.168\n",
            "Worker 8, [05/32]: Training Loss: 2.592970787, Training Accuracy: 32.400\n",
            "Worker 8, [06/32]: Training Loss: 2.517867047, Training Accuracy: 33.776\n",
            "Worker 8, [07/32]: Training Loss: 2.442509768, Training Accuracy: 35.104\n",
            "Worker 8, [08/32]: Training Loss: 2.361996710, Training Accuracy: 36.576\n",
            "Worker 8, [09/32]: Training Loss: 2.295026724, Training Accuracy: 38.944\n",
            "Worker 8, [10/32]: Training Loss: 2.232581591, Training Accuracy: 39.728\n",
            "Worker 8, [11/32]: Training Loss: 2.145137782, Training Accuracy: 41.472\n",
            "Worker 8, [12/32]: Training Loss: 2.125020806, Training Accuracy: 42.448\n",
            "Worker 8, [13/32]: Training Loss: 2.045931337, Training Accuracy: 43.968\n",
            "Worker 8, [14/32]: Training Loss: 1.971087001, Training Accuracy: 45.888\n",
            "Worker 8, [15/32]: Training Loss: 1.917163402, Training Accuracy: 46.576\n",
            "Worker 8, [16/32]: Training Loss: 1.885023273, Training Accuracy: 48.096\n",
            "Worker 8, [17/32]: Training Loss: 1.800430401, Training Accuracy: 49.408\n",
            "Worker 8, [18/32]: Training Loss: 1.730796857, Training Accuracy: 50.800\n",
            "Worker 8, [19/32]: Training Loss: 1.692765100, Training Accuracy: 52.192\n",
            "Worker 8, [20/32]: Training Loss: 1.681505775, Training Accuracy: 51.824\n",
            "Worker 8, [21/32]: Training Loss: 1.603214395, Training Accuracy: 54.352\n",
            "Worker 8, [22/32]: Training Loss: 1.577096788, Training Accuracy: 54.560\n",
            "Worker 8, [23/32]: Training Loss: 1.509378445, Training Accuracy: 56.544\n",
            "Worker 8, [24/32]: Training Loss: 1.505706352, Training Accuracy: 56.752\n",
            "Worker 8, [25/32]: Training Loss: 1.419657822, Training Accuracy: 58.576\n",
            "Worker 8, [26/32]: Training Loss: 1.411980213, Training Accuracy: 58.288\n",
            "Worker 8, [27/32]: Training Loss: 1.372625915, Training Accuracy: 59.936\n",
            "Worker 8, [28/32]: Training Loss: 1.319098466, Training Accuracy: 61.984\n",
            "Worker 8, [29/32]: Training Loss: 1.287755782, Training Accuracy: 62.544\n",
            "Worker 8, [30/32]: Training Loss: 1.229716422, Training Accuracy: 63.568\n",
            "Worker 8, [31/32]: Training Loss: 1.205366483, Training Accuracy: 64.448\n",
            "Worker 8, [32/32]: Training Loss: 1.172630047, Training Accuracy: 65.472\n",
            "Time taken for training worker 8: 0:01:39.263038\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004248\n",
            "Global Update 02: Test Loss: 2.524331726, Test Accuracy: 36.370\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.487015049, Training Accuracy: 36.064\n",
            "Worker 1, [02/32]: Training Loss: 2.269429673, Training Accuracy: 40.624\n",
            "Worker 1, [03/32]: Training Loss: 2.090554211, Training Accuracy: 43.568\n",
            "Worker 1, [04/32]: Training Loss: 1.968198296, Training Accuracy: 46.000\n",
            "Worker 1, [05/32]: Training Loss: 1.813551236, Training Accuracy: 49.520\n",
            "Worker 1, [06/32]: Training Loss: 1.738086305, Training Accuracy: 51.552\n",
            "Worker 1, [07/32]: Training Loss: 1.649195135, Training Accuracy: 53.040\n",
            "Worker 1, [08/32]: Training Loss: 1.554279829, Training Accuracy: 55.536\n",
            "Worker 1, [09/32]: Training Loss: 1.480159134, Training Accuracy: 57.872\n",
            "Worker 1, [10/32]: Training Loss: 1.387460802, Training Accuracy: 60.016\n",
            "Worker 1, [11/32]: Training Loss: 1.371314696, Training Accuracy: 60.528\n",
            "Worker 1, [12/32]: Training Loss: 1.298376898, Training Accuracy: 62.000\n",
            "Worker 1, [13/32]: Training Loss: 1.197374445, Training Accuracy: 64.800\n",
            "Worker 1, [14/32]: Training Loss: 1.164720112, Training Accuracy: 65.968\n",
            "Worker 1, [15/32]: Training Loss: 1.154137693, Training Accuracy: 65.840\n",
            "Worker 1, [16/32]: Training Loss: 1.053321322, Training Accuracy: 69.008\n",
            "Worker 1, [17/32]: Training Loss: 1.008850069, Training Accuracy: 69.872\n",
            "Worker 1, [18/32]: Training Loss: 0.971729235, Training Accuracy: 70.976\n",
            "Worker 1, [19/32]: Training Loss: 0.953544230, Training Accuracy: 71.888\n",
            "Worker 1, [20/32]: Training Loss: 0.923342457, Training Accuracy: 71.872\n",
            "Worker 1, [21/32]: Training Loss: 0.874492565, Training Accuracy: 74.560\n",
            "Worker 1, [22/32]: Training Loss: 0.845403213, Training Accuracy: 74.432\n",
            "Worker 1, [23/32]: Training Loss: 0.781786482, Training Accuracy: 76.256\n",
            "Worker 1, [24/32]: Training Loss: 0.784528079, Training Accuracy: 76.048\n",
            "Worker 1, [25/32]: Training Loss: 0.743338861, Training Accuracy: 77.648\n",
            "Worker 1, [26/32]: Training Loss: 0.702987941, Training Accuracy: 79.216\n",
            "Worker 1, [27/32]: Training Loss: 0.688636947, Training Accuracy: 79.104\n",
            "Worker 1, [28/32]: Training Loss: 0.669478611, Training Accuracy: 80.224\n",
            "Worker 1, [29/32]: Training Loss: 0.667003164, Training Accuracy: 79.840\n",
            "Worker 1, [30/32]: Training Loss: 0.639291426, Training Accuracy: 80.384\n",
            "Worker 1, [31/32]: Training Loss: 0.613852973, Training Accuracy: 80.752\n",
            "Worker 1, [32/32]: Training Loss: 0.586848646, Training Accuracy: 82.896\n",
            "Time taken for training worker 1: 0:01:37.845737\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.422277406, Training Accuracy: 37.136\n",
            "Worker 2, [02/32]: Training Loss: 2.207302204, Training Accuracy: 41.984\n",
            "Worker 2, [03/32]: Training Loss: 2.054537104, Training Accuracy: 44.704\n",
            "Worker 2, [04/32]: Training Loss: 1.898801886, Training Accuracy: 47.856\n",
            "Worker 2, [05/32]: Training Loss: 1.838697443, Training Accuracy: 48.608\n",
            "Worker 2, [06/32]: Training Loss: 1.697278782, Training Accuracy: 52.000\n",
            "Worker 2, [07/32]: Training Loss: 1.615425285, Training Accuracy: 54.416\n",
            "Worker 2, [08/32]: Training Loss: 1.549127511, Training Accuracy: 55.664\n",
            "Worker 2, [09/32]: Training Loss: 1.469006562, Training Accuracy: 58.192\n",
            "Worker 2, [10/32]: Training Loss: 1.392480955, Training Accuracy: 60.048\n",
            "Worker 2, [11/32]: Training Loss: 1.316810000, Training Accuracy: 61.536\n",
            "Worker 2, [12/32]: Training Loss: 1.250101372, Training Accuracy: 62.720\n",
            "Worker 2, [13/32]: Training Loss: 1.199934182, Training Accuracy: 64.192\n",
            "Worker 2, [14/32]: Training Loss: 1.156956547, Training Accuracy: 65.616\n",
            "Worker 2, [15/32]: Training Loss: 1.106417171, Training Accuracy: 67.328\n",
            "Worker 2, [16/32]: Training Loss: 1.045321680, Training Accuracy: 68.992\n",
            "Worker 2, [17/32]: Training Loss: 1.025758189, Training Accuracy: 69.056\n",
            "Worker 2, [18/32]: Training Loss: 0.954269538, Training Accuracy: 71.632\n",
            "Worker 2, [19/32]: Training Loss: 0.919018800, Training Accuracy: 71.664\n",
            "Worker 2, [20/32]: Training Loss: 0.881769155, Training Accuracy: 73.808\n",
            "Worker 2, [21/32]: Training Loss: 0.876151995, Training Accuracy: 73.520\n",
            "Worker 2, [22/32]: Training Loss: 0.831937127, Training Accuracy: 75.200\n",
            "Worker 2, [23/32]: Training Loss: 0.783394583, Training Accuracy: 76.048\n",
            "Worker 2, [24/32]: Training Loss: 0.772199617, Training Accuracy: 76.352\n",
            "Worker 2, [25/32]: Training Loss: 0.735983816, Training Accuracy: 77.328\n",
            "Worker 2, [26/32]: Training Loss: 0.689907375, Training Accuracy: 79.104\n",
            "Worker 2, [27/32]: Training Loss: 0.703710922, Training Accuracy: 79.008\n",
            "Worker 2, [28/32]: Training Loss: 0.657001165, Training Accuracy: 79.920\n",
            "Worker 2, [29/32]: Training Loss: 0.653843384, Training Accuracy: 79.856\n",
            "Worker 2, [30/32]: Training Loss: 0.626472151, Training Accuracy: 81.152\n",
            "Worker 2, [31/32]: Training Loss: 0.592899903, Training Accuracy: 81.840\n",
            "Worker 2, [32/32]: Training Loss: 0.599389414, Training Accuracy: 82.112\n",
            "Time taken for training worker 2: 0:01:37.930896\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 2.487795835, Training Accuracy: 36.144\n",
            "Worker 3, [02/32]: Training Loss: 2.215299129, Training Accuracy: 41.328\n",
            "Worker 3, [03/32]: Training Loss: 2.059013828, Training Accuracy: 44.816\n",
            "Worker 3, [04/32]: Training Loss: 1.958825375, Training Accuracy: 46.960\n",
            "Worker 3, [05/32]: Training Loss: 1.847980969, Training Accuracy: 49.840\n",
            "Worker 3, [06/32]: Training Loss: 1.745533618, Training Accuracy: 51.232\n",
            "Worker 3, [07/32]: Training Loss: 1.639640601, Training Accuracy: 54.128\n",
            "Worker 3, [08/32]: Training Loss: 1.554981228, Training Accuracy: 55.856\n",
            "Worker 3, [09/32]: Training Loss: 1.469630892, Training Accuracy: 58.224\n",
            "Worker 3, [10/32]: Training Loss: 1.390461780, Training Accuracy: 59.936\n",
            "Worker 3, [11/32]: Training Loss: 1.332099584, Training Accuracy: 61.248\n",
            "Worker 3, [12/32]: Training Loss: 1.251957174, Training Accuracy: 63.888\n",
            "Worker 3, [13/32]: Training Loss: 1.230948488, Training Accuracy: 63.760\n",
            "Worker 3, [14/32]: Training Loss: 1.167244618, Training Accuracy: 65.056\n",
            "Worker 3, [15/32]: Training Loss: 1.089978139, Training Accuracy: 68.128\n",
            "Worker 3, [16/32]: Training Loss: 1.055605460, Training Accuracy: 69.040\n",
            "Worker 3, [17/32]: Training Loss: 1.019503247, Training Accuracy: 69.568\n",
            "Worker 3, [18/32]: Training Loss: 0.987657754, Training Accuracy: 70.080\n",
            "Worker 3, [19/32]: Training Loss: 0.939782806, Training Accuracy: 71.984\n",
            "Worker 3, [20/32]: Training Loss: 0.884136322, Training Accuracy: 72.912\n",
            "Worker 3, [21/32]: Training Loss: 0.849354569, Training Accuracy: 74.768\n",
            "Worker 3, [22/32]: Training Loss: 0.810772625, Training Accuracy: 74.992\n",
            "Worker 3, [23/32]: Training Loss: 0.806851325, Training Accuracy: 75.712\n",
            "Worker 3, [24/32]: Training Loss: 0.789865577, Training Accuracy: 76.448\n",
            "Worker 3, [25/32]: Training Loss: 0.745326507, Training Accuracy: 77.328\n",
            "Worker 3, [26/32]: Training Loss: 0.716687782, Training Accuracy: 78.000\n",
            "Worker 3, [27/32]: Training Loss: 0.703740441, Training Accuracy: 78.784\n",
            "Worker 3, [28/32]: Training Loss: 0.667849648, Training Accuracy: 79.968\n",
            "Worker 3, [29/32]: Training Loss: 0.663182847, Training Accuracy: 79.216\n",
            "Worker 3, [30/32]: Training Loss: 0.627978804, Training Accuracy: 80.800\n",
            "Worker 3, [31/32]: Training Loss: 0.609436226, Training Accuracy: 81.296\n",
            "Worker 3, [32/32]: Training Loss: 0.620040714, Training Accuracy: 81.104\n",
            "Time taken for training worker 3: 0:01:41.282238\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 2.471645287, Training Accuracy: 36.192\n",
            "Worker 4, [02/32]: Training Loss: 2.249170532, Training Accuracy: 40.992\n",
            "Worker 4, [03/32]: Training Loss: 2.070675026, Training Accuracy: 43.408\n",
            "Worker 4, [04/32]: Training Loss: 1.942296815, Training Accuracy: 46.528\n",
            "Worker 4, [05/32]: Training Loss: 1.826052808, Training Accuracy: 48.992\n",
            "Worker 4, [06/32]: Training Loss: 1.715494580, Training Accuracy: 52.592\n",
            "Worker 4, [07/32]: Training Loss: 1.646886531, Training Accuracy: 53.872\n",
            "Worker 4, [08/32]: Training Loss: 1.571722494, Training Accuracy: 55.728\n",
            "Worker 4, [09/32]: Training Loss: 1.475470407, Training Accuracy: 57.600\n",
            "Worker 4, [10/32]: Training Loss: 1.409053615, Training Accuracy: 60.560\n",
            "Worker 4, [11/32]: Training Loss: 1.299708900, Training Accuracy: 62.448\n",
            "Worker 4, [12/32]: Training Loss: 1.295228517, Training Accuracy: 62.352\n",
            "Worker 4, [13/32]: Training Loss: 1.216799839, Training Accuracy: 64.688\n",
            "Worker 4, [14/32]: Training Loss: 1.146487807, Training Accuracy: 66.816\n",
            "Worker 4, [15/32]: Training Loss: 1.092428501, Training Accuracy: 67.744\n",
            "Worker 4, [16/32]: Training Loss: 1.057635773, Training Accuracy: 68.560\n",
            "Worker 4, [17/32]: Training Loss: 1.020529395, Training Accuracy: 69.584\n",
            "Worker 4, [18/32]: Training Loss: 0.981273936, Training Accuracy: 71.248\n",
            "Worker 4, [19/32]: Training Loss: 0.930103470, Training Accuracy: 73.280\n",
            "Worker 4, [20/32]: Training Loss: 0.905024711, Training Accuracy: 72.816\n",
            "Worker 4, [21/32]: Training Loss: 0.878347534, Training Accuracy: 73.776\n",
            "Worker 4, [22/32]: Training Loss: 0.822165583, Training Accuracy: 75.344\n",
            "Worker 4, [23/32]: Training Loss: 0.822780850, Training Accuracy: 75.232\n",
            "Worker 4, [24/32]: Training Loss: 0.781300515, Training Accuracy: 76.592\n",
            "Worker 4, [25/32]: Training Loss: 0.743373216, Training Accuracy: 77.488\n",
            "Worker 4, [26/32]: Training Loss: 0.741056141, Training Accuracy: 77.168\n",
            "Worker 4, [27/32]: Training Loss: 0.701243140, Training Accuracy: 79.136\n",
            "Worker 4, [28/32]: Training Loss: 0.665545050, Training Accuracy: 79.504\n",
            "Worker 4, [29/32]: Training Loss: 0.673309267, Training Accuracy: 79.664\n",
            "Worker 4, [30/32]: Training Loss: 0.616320579, Training Accuracy: 80.720\n",
            "Worker 4, [31/32]: Training Loss: 0.604715454, Training Accuracy: 82.096\n",
            "Worker 4, [32/32]: Training Loss: 0.579664811, Training Accuracy: 82.480\n",
            "Time taken for training worker 4: 0:01:39.227740\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/32]: Training Loss: 2.457447043, Training Accuracy: 35.984\n",
            "Worker 5, [02/32]: Training Loss: 2.204050220, Training Accuracy: 41.856\n",
            "Worker 5, [03/32]: Training Loss: 2.074953456, Training Accuracy: 44.080\n",
            "Worker 5, [04/32]: Training Loss: 1.934412853, Training Accuracy: 47.344\n",
            "Worker 5, [05/32]: Training Loss: 1.805467202, Training Accuracy: 49.632\n",
            "Worker 5, [06/32]: Training Loss: 1.702091189, Training Accuracy: 53.088\n",
            "Worker 5, [07/32]: Training Loss: 1.610435206, Training Accuracy: 54.848\n",
            "Worker 5, [08/32]: Training Loss: 1.526362318, Training Accuracy: 56.576\n",
            "Worker 5, [09/32]: Training Loss: 1.481907287, Training Accuracy: 57.696\n",
            "Worker 5, [10/32]: Training Loss: 1.376221843, Training Accuracy: 59.728\n",
            "Worker 5, [11/32]: Training Loss: 1.317234034, Training Accuracy: 61.552\n",
            "Worker 5, [12/32]: Training Loss: 1.268520511, Training Accuracy: 62.800\n",
            "Worker 5, [13/32]: Training Loss: 1.190866641, Training Accuracy: 65.488\n",
            "Worker 5, [14/32]: Training Loss: 1.125963106, Training Accuracy: 66.528\n",
            "Worker 5, [15/32]: Training Loss: 1.084342702, Training Accuracy: 67.936\n",
            "Worker 5, [16/32]: Training Loss: 1.046767554, Training Accuracy: 68.960\n",
            "Worker 5, [17/32]: Training Loss: 1.000620585, Training Accuracy: 70.256\n",
            "Worker 5, [18/32]: Training Loss: 0.962638359, Training Accuracy: 70.704\n",
            "Worker 5, [19/32]: Training Loss: 0.918339786, Training Accuracy: 72.672\n",
            "Worker 5, [20/32]: Training Loss: 0.874918149, Training Accuracy: 73.616\n",
            "Worker 5, [21/32]: Training Loss: 0.876219164, Training Accuracy: 73.952\n",
            "Worker 5, [22/32]: Training Loss: 0.840978185, Training Accuracy: 75.120\n",
            "Worker 5, [23/32]: Training Loss: 0.797826889, Training Accuracy: 75.872\n",
            "Worker 5, [24/32]: Training Loss: 0.786128617, Training Accuracy: 76.544\n",
            "Worker 5, [25/32]: Training Loss: 0.726835233, Training Accuracy: 77.968\n",
            "Worker 5, [26/32]: Training Loss: 0.710538283, Training Accuracy: 78.544\n",
            "Worker 5, [27/32]: Training Loss: 0.685212872, Training Accuracy: 79.088\n",
            "Worker 5, [28/32]: Training Loss: 0.645733148, Training Accuracy: 80.688\n",
            "Worker 5, [29/32]: Training Loss: 0.634437493, Training Accuracy: 80.144\n",
            "Worker 5, [30/32]: Training Loss: 0.650674806, Training Accuracy: 80.512\n",
            "Worker 5, [31/32]: Training Loss: 0.652206764, Training Accuracy: 80.336\n",
            "Worker 5, [32/32]: Training Loss: 0.565889527, Training Accuracy: 82.880\n",
            "Time taken for training worker 5: 0:01:39.823679\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/32]: Training Loss: 2.447387690, Training Accuracy: 37.424\n",
            "Worker 6, [02/32]: Training Loss: 2.224308918, Training Accuracy: 41.024\n",
            "Worker 6, [03/32]: Training Loss: 2.065750454, Training Accuracy: 43.392\n",
            "Worker 6, [04/32]: Training Loss: 1.926423277, Training Accuracy: 47.632\n",
            "Worker 6, [05/32]: Training Loss: 1.806605472, Training Accuracy: 50.064\n",
            "Worker 6, [06/32]: Training Loss: 1.713721228, Training Accuracy: 51.728\n",
            "Worker 6, [07/32]: Training Loss: 1.656834735, Training Accuracy: 53.360\n",
            "Worker 6, [08/32]: Training Loss: 1.567650637, Training Accuracy: 55.600\n",
            "Worker 6, [09/32]: Training Loss: 1.462681815, Training Accuracy: 58.368\n",
            "Worker 6, [10/32]: Training Loss: 1.386873510, Training Accuracy: 59.568\n",
            "Worker 6, [11/32]: Training Loss: 1.326205389, Training Accuracy: 61.328\n",
            "Worker 6, [12/32]: Training Loss: 1.267918268, Training Accuracy: 63.120\n",
            "Worker 6, [13/32]: Training Loss: 1.224018257, Training Accuracy: 64.416\n",
            "Worker 6, [14/32]: Training Loss: 1.164401840, Training Accuracy: 65.680\n",
            "Worker 6, [15/32]: Training Loss: 1.093202663, Training Accuracy: 67.056\n",
            "Worker 6, [16/32]: Training Loss: 1.061999073, Training Accuracy: 68.864\n",
            "Worker 6, [17/32]: Training Loss: 1.001205947, Training Accuracy: 70.288\n",
            "Worker 6, [18/32]: Training Loss: 0.983142486, Training Accuracy: 70.720\n",
            "Worker 6, [19/32]: Training Loss: 0.944892827, Training Accuracy: 71.776\n",
            "Worker 6, [20/32]: Training Loss: 0.879029895, Training Accuracy: 73.968\n",
            "Worker 6, [21/32]: Training Loss: 0.839783397, Training Accuracy: 75.120\n",
            "Worker 6, [22/32]: Training Loss: 0.842208020, Training Accuracy: 75.152\n",
            "Worker 6, [23/32]: Training Loss: 0.817306316, Training Accuracy: 75.472\n",
            "Worker 6, [24/32]: Training Loss: 0.767456543, Training Accuracy: 76.672\n",
            "Worker 6, [25/32]: Training Loss: 0.714039719, Training Accuracy: 78.224\n",
            "Worker 6, [26/32]: Training Loss: 0.726552961, Training Accuracy: 78.096\n",
            "Worker 6, [27/32]: Training Loss: 0.703359877, Training Accuracy: 78.784\n",
            "Worker 6, [28/32]: Training Loss: 0.661355866, Training Accuracy: 80.032\n",
            "Worker 6, [29/32]: Training Loss: 0.674447124, Training Accuracy: 79.936\n",
            "Worker 6, [30/32]: Training Loss: 0.628000309, Training Accuracy: 80.816\n",
            "Worker 6, [31/32]: Training Loss: 0.609856839, Training Accuracy: 81.552\n",
            "Worker 6, [32/32]: Training Loss: 0.575804065, Training Accuracy: 82.480\n",
            "Time taken for training worker 6: 0:01:39.806699\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/32]: Training Loss: 2.436729968, Training Accuracy: 37.568\n",
            "Worker 7, [02/32]: Training Loss: 2.203609122, Training Accuracy: 41.424\n",
            "Worker 7, [03/32]: Training Loss: 2.023179274, Training Accuracy: 45.072\n",
            "Worker 7, [04/32]: Training Loss: 1.934349097, Training Accuracy: 47.024\n",
            "Worker 7, [05/32]: Training Loss: 1.803984795, Training Accuracy: 50.768\n",
            "Worker 7, [06/32]: Training Loss: 1.710962151, Training Accuracy: 52.144\n",
            "Worker 7, [07/32]: Training Loss: 1.628321687, Training Accuracy: 54.288\n",
            "Worker 7, [08/32]: Training Loss: 1.500366486, Training Accuracy: 57.328\n",
            "Worker 7, [09/32]: Training Loss: 1.448948427, Training Accuracy: 59.104\n",
            "Worker 7, [10/32]: Training Loss: 1.401486193, Training Accuracy: 60.032\n",
            "Worker 7, [11/32]: Training Loss: 1.331696796, Training Accuracy: 61.280\n",
            "Worker 7, [12/32]: Training Loss: 1.251334203, Training Accuracy: 63.888\n",
            "Worker 7, [13/32]: Training Loss: 1.185590573, Training Accuracy: 65.536\n",
            "Worker 7, [14/32]: Training Loss: 1.147948420, Training Accuracy: 66.272\n",
            "Worker 7, [15/32]: Training Loss: 1.089442551, Training Accuracy: 68.208\n",
            "Worker 7, [16/32]: Training Loss: 1.058689072, Training Accuracy: 68.448\n",
            "Worker 7, [17/32]: Training Loss: 1.017117800, Training Accuracy: 69.008\n",
            "Worker 7, [18/32]: Training Loss: 0.954664901, Training Accuracy: 71.840\n",
            "Worker 7, [19/32]: Training Loss: 0.920168849, Training Accuracy: 72.736\n",
            "Worker 7, [20/32]: Training Loss: 0.879626543, Training Accuracy: 74.112\n",
            "Worker 7, [21/32]: Training Loss: 0.846054007, Training Accuracy: 74.112\n",
            "Worker 7, [22/32]: Training Loss: 0.816285859, Training Accuracy: 75.808\n",
            "Worker 7, [23/32]: Training Loss: 0.772229106, Training Accuracy: 76.848\n",
            "Worker 7, [24/32]: Training Loss: 0.772874445, Training Accuracy: 76.384\n",
            "Worker 7, [25/32]: Training Loss: 0.759669146, Training Accuracy: 76.928\n",
            "Worker 7, [26/32]: Training Loss: 0.698348901, Training Accuracy: 78.704\n",
            "Worker 7, [27/32]: Training Loss: 0.690827434, Training Accuracy: 79.360\n",
            "Worker 7, [28/32]: Training Loss: 0.680696280, Training Accuracy: 79.072\n",
            "Worker 7, [29/32]: Training Loss: 0.633114997, Training Accuracy: 81.232\n",
            "Worker 7, [30/32]: Training Loss: 0.611006009, Training Accuracy: 81.520\n",
            "Worker 7, [31/32]: Training Loss: 0.630308671, Training Accuracy: 80.864\n",
            "Worker 7, [32/32]: Training Loss: 0.594338992, Training Accuracy: 81.904\n",
            "Time taken for training worker 7: 0:01:38.975088\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/32]: Training Loss: 2.433174328, Training Accuracy: 36.624\n",
            "Worker 8, [02/32]: Training Loss: 2.202649800, Training Accuracy: 41.184\n",
            "Worker 8, [03/32]: Training Loss: 2.035482765, Training Accuracy: 45.152\n",
            "Worker 8, [04/32]: Training Loss: 1.896837408, Training Accuracy: 47.888\n",
            "Worker 8, [05/32]: Training Loss: 1.798145084, Training Accuracy: 49.440\n",
            "Worker 8, [06/32]: Training Loss: 1.702133800, Training Accuracy: 52.432\n",
            "Worker 8, [07/32]: Training Loss: 1.615157754, Training Accuracy: 54.000\n",
            "Worker 8, [08/32]: Training Loss: 1.524415014, Training Accuracy: 56.720\n",
            "Worker 8, [09/32]: Training Loss: 1.460232976, Training Accuracy: 57.504\n",
            "Worker 8, [10/32]: Training Loss: 1.396670736, Training Accuracy: 59.920\n",
            "Worker 8, [11/32]: Training Loss: 1.305711291, Training Accuracy: 62.032\n",
            "Worker 8, [12/32]: Training Loss: 1.250084895, Training Accuracy: 63.408\n",
            "Worker 8, [13/32]: Training Loss: 1.198427056, Training Accuracy: 65.456\n",
            "Worker 8, [14/32]: Training Loss: 1.136722475, Training Accuracy: 67.216\n",
            "Worker 8, [15/32]: Training Loss: 1.094142144, Training Accuracy: 67.728\n",
            "Worker 8, [16/32]: Training Loss: 1.011229088, Training Accuracy: 70.112\n",
            "Worker 8, [17/32]: Training Loss: 0.970992853, Training Accuracy: 71.264\n",
            "Worker 8, [18/32]: Training Loss: 0.967801489, Training Accuracy: 71.648\n",
            "Worker 8, [19/32]: Training Loss: 0.908095581, Training Accuracy: 73.424\n",
            "Worker 8, [20/32]: Training Loss: 0.894141687, Training Accuracy: 72.848\n",
            "Worker 8, [21/32]: Training Loss: 0.843585495, Training Accuracy: 74.672\n",
            "Worker 8, [22/32]: Training Loss: 0.824134733, Training Accuracy: 75.200\n",
            "Worker 8, [23/32]: Training Loss: 0.817220208, Training Accuracy: 75.536\n",
            "Worker 8, [24/32]: Training Loss: 0.787117877, Training Accuracy: 76.160\n",
            "Worker 8, [25/32]: Training Loss: 0.740079694, Training Accuracy: 77.056\n",
            "Worker 8, [26/32]: Training Loss: 0.687908163, Training Accuracy: 79.008\n",
            "Worker 8, [27/32]: Training Loss: 0.676845542, Training Accuracy: 79.728\n",
            "Worker 8, [28/32]: Training Loss: 0.667209705, Training Accuracy: 79.216\n",
            "Worker 8, [29/32]: Training Loss: 0.694606041, Training Accuracy: 78.912\n",
            "Worker 8, [30/32]: Training Loss: 0.642426477, Training Accuracy: 80.720\n",
            "Worker 8, [31/32]: Training Loss: 0.595053537, Training Accuracy: 82.016\n",
            "Worker 8, [32/32]: Training Loss: 0.589088441, Training Accuracy: 82.080\n",
            "Time taken for training worker 8: 0:01:37.507684\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.011885\n",
            "Global Update 03: Test Loss: 2.660836660, Test Accuracy: 41.330\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.187651260, Training Accuracy: 43.888\n",
            "Worker 1, [02/32]: Training Loss: 1.913246539, Training Accuracy: 48.768\n",
            "Worker 1, [03/32]: Training Loss: 1.757815794, Training Accuracy: 52.144\n",
            "Worker 1, [04/32]: Training Loss: 1.663520217, Training Accuracy: 54.128\n",
            "Worker 1, [05/32]: Training Loss: 1.563486887, Training Accuracy: 56.048\n",
            "Worker 1, [06/32]: Training Loss: 1.494189518, Training Accuracy: 58.272\n",
            "Worker 1, [07/32]: Training Loss: 1.391351135, Training Accuracy: 60.848\n",
            "Worker 1, [08/32]: Training Loss: 1.291209610, Training Accuracy: 63.088\n",
            "Worker 1, [09/32]: Training Loss: 1.212850386, Training Accuracy: 65.200\n",
            "Worker 1, [10/32]: Training Loss: 1.149505728, Training Accuracy: 67.680\n",
            "Worker 1, [11/32]: Training Loss: 1.096134183, Training Accuracy: 69.120\n",
            "Worker 1, [12/32]: Training Loss: 1.055219063, Training Accuracy: 69.952\n",
            "Worker 1, [13/32]: Training Loss: 0.998751499, Training Accuracy: 71.760\n",
            "Worker 1, [14/32]: Training Loss: 0.952555743, Training Accuracy: 72.512\n",
            "Worker 1, [15/32]: Training Loss: 0.880355036, Training Accuracy: 75.056\n",
            "Worker 1, [16/32]: Training Loss: 0.834824173, Training Accuracy: 75.904\n",
            "Worker 1, [17/32]: Training Loss: 0.822451840, Training Accuracy: 76.608\n",
            "Worker 1, [18/32]: Training Loss: 0.782594709, Training Accuracy: 78.128\n",
            "Worker 1, [19/32]: Training Loss: 0.729089378, Training Accuracy: 79.328\n",
            "Worker 1, [20/32]: Training Loss: 0.700882260, Training Accuracy: 79.872\n",
            "Worker 1, [21/32]: Training Loss: 0.650671846, Training Accuracy: 81.888\n",
            "Worker 1, [22/32]: Training Loss: 0.631864314, Training Accuracy: 81.552\n",
            "Worker 1, [23/32]: Training Loss: 0.609696323, Training Accuracy: 82.480\n",
            "Worker 1, [24/32]: Training Loss: 0.580642718, Training Accuracy: 83.632\n",
            "Worker 1, [25/32]: Training Loss: 0.562877785, Training Accuracy: 84.480\n",
            "Worker 1, [26/32]: Training Loss: 0.533499343, Training Accuracy: 85.104\n",
            "Worker 1, [27/32]: Training Loss: 0.505165486, Training Accuracy: 85.648\n",
            "Worker 1, [28/32]: Training Loss: 0.475299286, Training Accuracy: 86.944\n",
            "Worker 1, [29/32]: Training Loss: 0.480741775, Training Accuracy: 86.512\n",
            "Worker 1, [30/32]: Training Loss: 0.461459752, Training Accuracy: 86.896\n",
            "Worker 1, [31/32]: Training Loss: 0.425008693, Training Accuracy: 88.560\n",
            "Worker 1, [32/32]: Training Loss: 0.402077351, Training Accuracy: 88.528\n",
            "Time taken for training worker 1: 0:01:40.076542\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.123485577, Training Accuracy: 44.608\n",
            "Worker 2, [02/32]: Training Loss: 1.855098796, Training Accuracy: 49.440\n",
            "Worker 2, [03/32]: Training Loss: 1.732352084, Training Accuracy: 52.656\n",
            "Worker 2, [04/32]: Training Loss: 1.621010612, Training Accuracy: 54.736\n",
            "Worker 2, [05/32]: Training Loss: 1.527374828, Training Accuracy: 57.152\n",
            "Worker 2, [06/32]: Training Loss: 1.417315089, Training Accuracy: 59.328\n",
            "Worker 2, [07/32]: Training Loss: 1.362896560, Training Accuracy: 61.376\n",
            "Worker 2, [08/32]: Training Loss: 1.254047938, Training Accuracy: 63.488\n",
            "Worker 2, [09/32]: Training Loss: 1.201732305, Training Accuracy: 65.232\n",
            "Worker 2, [10/32]: Training Loss: 1.161103272, Training Accuracy: 66.352\n",
            "Worker 2, [11/32]: Training Loss: 1.082701521, Training Accuracy: 68.256\n",
            "Worker 2, [12/32]: Training Loss: 1.017148273, Training Accuracy: 70.384\n",
            "Worker 2, [13/32]: Training Loss: 0.975024635, Training Accuracy: 71.936\n",
            "Worker 2, [14/32]: Training Loss: 0.919250172, Training Accuracy: 73.904\n",
            "Worker 2, [15/32]: Training Loss: 0.880476618, Training Accuracy: 74.608\n",
            "Worker 2, [16/32]: Training Loss: 0.827832576, Training Accuracy: 76.288\n",
            "Worker 2, [17/32]: Training Loss: 0.768096531, Training Accuracy: 77.280\n",
            "Worker 2, [18/32]: Training Loss: 0.746080039, Training Accuracy: 78.496\n",
            "Worker 2, [19/32]: Training Loss: 0.740430661, Training Accuracy: 79.072\n",
            "Worker 2, [20/32]: Training Loss: 0.699001329, Training Accuracy: 79.840\n",
            "Worker 2, [21/32]: Training Loss: 0.663566844, Training Accuracy: 81.008\n",
            "Worker 2, [22/32]: Training Loss: 0.627809909, Training Accuracy: 81.424\n",
            "Worker 2, [23/32]: Training Loss: 0.577926051, Training Accuracy: 83.072\n",
            "Worker 2, [24/32]: Training Loss: 0.567983150, Training Accuracy: 83.312\n",
            "Worker 2, [25/32]: Training Loss: 0.550403413, Training Accuracy: 84.128\n",
            "Worker 2, [26/32]: Training Loss: 0.533622996, Training Accuracy: 85.120\n",
            "Worker 2, [27/32]: Training Loss: 0.484098044, Training Accuracy: 86.144\n",
            "Worker 2, [28/32]: Training Loss: 0.492345068, Training Accuracy: 85.664\n",
            "Worker 2, [29/32]: Training Loss: 0.478542572, Training Accuracy: 86.320\n",
            "Worker 2, [30/32]: Training Loss: 0.445219236, Training Accuracy: 87.184\n",
            "Worker 2, [31/32]: Training Loss: 0.442059516, Training Accuracy: 87.168\n",
            "Worker 2, [32/32]: Training Loss: 0.408914479, Training Accuracy: 88.720\n",
            "Time taken for training worker 2: 0:01:39.013913\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/32]: Training Loss: 2.136479463, Training Accuracy: 44.448\n",
            "Worker 3, [02/32]: Training Loss: 1.895793179, Training Accuracy: 48.464\n",
            "Worker 3, [03/32]: Training Loss: 1.751527288, Training Accuracy: 52.464\n",
            "Worker 3, [04/32]: Training Loss: 1.626766603, Training Accuracy: 55.296\n",
            "Worker 3, [05/32]: Training Loss: 1.525702233, Training Accuracy: 57.664\n",
            "Worker 3, [06/32]: Training Loss: 1.452115674, Training Accuracy: 59.424\n",
            "Worker 3, [07/32]: Training Loss: 1.339463242, Training Accuracy: 62.384\n",
            "Worker 3, [08/32]: Training Loss: 1.295424573, Training Accuracy: 63.248\n",
            "Worker 3, [09/32]: Training Loss: 1.197742120, Training Accuracy: 65.568\n",
            "Worker 3, [10/32]: Training Loss: 1.140822738, Training Accuracy: 67.552\n",
            "Worker 3, [11/32]: Training Loss: 1.095556785, Training Accuracy: 69.024\n",
            "Worker 3, [12/32]: Training Loss: 1.026598044, Training Accuracy: 70.848\n",
            "Worker 3, [13/32]: Training Loss: 0.981980009, Training Accuracy: 71.744\n",
            "Worker 3, [14/32]: Training Loss: 0.934458903, Training Accuracy: 72.960\n",
            "Worker 3, [15/32]: Training Loss: 0.873152369, Training Accuracy: 75.216\n",
            "Worker 3, [16/32]: Training Loss: 0.849510566, Training Accuracy: 75.488\n",
            "Worker 3, [17/32]: Training Loss: 0.797681337, Training Accuracy: 77.424\n",
            "Worker 3, [18/32]: Training Loss: 0.751530679, Training Accuracy: 78.528\n",
            "Worker 3, [19/32]: Training Loss: 0.729345319, Training Accuracy: 79.376\n",
            "Worker 3, [20/32]: Training Loss: 0.679910033, Training Accuracy: 80.464\n",
            "Worker 3, [21/32]: Training Loss: 0.651162455, Training Accuracy: 81.680\n",
            "Worker 3, [22/32]: Training Loss: 0.616004621, Training Accuracy: 82.848\n",
            "Worker 3, [23/32]: Training Loss: 0.589948952, Training Accuracy: 83.184\n",
            "Worker 3, [24/32]: Training Loss: 0.566794498, Training Accuracy: 84.064\n",
            "Worker 3, [25/32]: Training Loss: 0.561733669, Training Accuracy: 83.920\n",
            "Worker 3, [26/32]: Training Loss: 0.525688842, Training Accuracy: 85.536\n",
            "Worker 3, [27/32]: Training Loss: 0.496257428, Training Accuracy: 86.176\n",
            "Worker 3, [28/32]: Training Loss: 0.483506315, Training Accuracy: 86.320\n",
            "Worker 3, [29/32]: Training Loss: 0.450234216, Training Accuracy: 87.360\n",
            "Worker 3, [30/32]: Training Loss: 0.453714042, Training Accuracy: 87.872\n",
            "Worker 3, [31/32]: Training Loss: 0.418673772, Training Accuracy: 88.432\n",
            "Worker 3, [32/32]: Training Loss: 0.410451266, Training Accuracy: 88.832\n",
            "Time taken for training worker 3: 0:01:40.121380\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/32]: Training Loss: 2.151837068, Training Accuracy: 44.400\n",
            "Worker 4, [02/32]: Training Loss: 1.879360321, Training Accuracy: 49.920\n",
            "Worker 4, [03/32]: Training Loss: 1.752723909, Training Accuracy: 52.256\n",
            "Worker 4, [04/32]: Training Loss: 1.636044943, Training Accuracy: 55.136\n",
            "Worker 4, [05/32]: Training Loss: 1.523531317, Training Accuracy: 57.840\n",
            "Worker 4, [06/32]: Training Loss: 1.447468265, Training Accuracy: 59.344\n",
            "Worker 4, [07/32]: Training Loss: 1.359554095, Training Accuracy: 61.376\n",
            "Worker 4, [08/32]: Training Loss: 1.281534012, Training Accuracy: 64.272\n",
            "Worker 4, [09/32]: Training Loss: 1.203668527, Training Accuracy: 66.336\n",
            "Worker 4, [10/32]: Training Loss: 1.151183190, Training Accuracy: 66.752\n",
            "Worker 4, [11/32]: Training Loss: 1.098311555, Training Accuracy: 68.880\n",
            "Worker 4, [12/32]: Training Loss: 1.035820572, Training Accuracy: 70.768\n",
            "Worker 4, [13/32]: Training Loss: 0.991131241, Training Accuracy: 71.472\n",
            "Worker 4, [14/32]: Training Loss: 0.925358471, Training Accuracy: 73.664\n",
            "Worker 4, [15/32]: Training Loss: 0.893298729, Training Accuracy: 75.104\n",
            "Worker 4, [16/32]: Training Loss: 0.844093575, Training Accuracy: 76.272\n",
            "Worker 4, [17/32]: Training Loss: 0.778061257, Training Accuracy: 77.440\n",
            "Worker 4, [18/32]: Training Loss: 0.770412097, Training Accuracy: 77.776\n",
            "Worker 4, [19/32]: Training Loss: 0.716574642, Training Accuracy: 79.728\n",
            "Worker 4, [20/32]: Training Loss: 0.683552688, Training Accuracy: 80.272\n",
            "Worker 4, [21/32]: Training Loss: 0.664258937, Training Accuracy: 81.744\n",
            "Worker 4, [22/32]: Training Loss: 0.632711092, Training Accuracy: 82.144\n",
            "Worker 4, [23/32]: Training Loss: 0.587861241, Training Accuracy: 83.456\n",
            "Worker 4, [24/32]: Training Loss: 0.562084567, Training Accuracy: 84.304\n",
            "Worker 4, [25/32]: Training Loss: 0.548926873, Training Accuracy: 84.352\n",
            "Worker 4, [26/32]: Training Loss: 0.535233705, Training Accuracy: 84.400\n",
            "Worker 4, [27/32]: Training Loss: 0.527886487, Training Accuracy: 85.936\n",
            "Worker 4, [28/32]: Training Loss: 0.488923018, Training Accuracy: 86.608\n",
            "Worker 4, [29/32]: Training Loss: 0.472895805, Training Accuracy: 86.544\n",
            "Worker 4, [30/32]: Training Loss: 0.466647036, Training Accuracy: 86.864\n",
            "Worker 4, [31/32]: Training Loss: 0.454133312, Training Accuracy: 87.040\n",
            "Worker 4, [32/32]: Training Loss: 0.418012302, Training Accuracy: 88.576\n",
            "Time taken for training worker 4: 0:01:42.863442\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/32]: Training Loss: 2.155905939, Training Accuracy: 43.264\n",
            "Worker 5, [02/32]: Training Loss: 1.885138727, Training Accuracy: 48.784\n",
            "Worker 5, [03/32]: Training Loss: 1.741831608, Training Accuracy: 52.016\n",
            "Worker 5, [04/32]: Training Loss: 1.646849233, Training Accuracy: 54.656\n",
            "Worker 5, [05/32]: Training Loss: 1.537517808, Training Accuracy: 57.120\n",
            "Worker 5, [06/32]: Training Loss: 1.426359495, Training Accuracy: 59.552\n",
            "Worker 5, [07/32]: Training Loss: 1.347276065, Training Accuracy: 62.208\n",
            "Worker 5, [08/32]: Training Loss: 1.285746455, Training Accuracy: 63.600\n",
            "Worker 5, [09/32]: Training Loss: 1.189842539, Training Accuracy: 66.544\n",
            "Worker 5, [10/32]: Training Loss: 1.135490453, Training Accuracy: 67.808\n",
            "Worker 5, [11/32]: Training Loss: 1.085779262, Training Accuracy: 68.816\n",
            "Worker 5, [12/32]: Training Loss: 1.001047755, Training Accuracy: 71.312\n",
            "Worker 5, [13/32]: Training Loss: 0.972833811, Training Accuracy: 72.448\n",
            "Worker 5, [14/32]: Training Loss: 0.934426404, Training Accuracy: 73.136\n",
            "Worker 5, [15/32]: Training Loss: 0.863367469, Training Accuracy: 75.536\n",
            "Worker 5, [16/32]: Training Loss: 0.819409732, Training Accuracy: 76.656\n",
            "Worker 5, [17/32]: Training Loss: 0.781187363, Training Accuracy: 77.664\n",
            "Worker 5, [18/32]: Training Loss: 0.752985286, Training Accuracy: 79.152\n",
            "Worker 5, [19/32]: Training Loss: 0.704829190, Training Accuracy: 79.952\n",
            "Worker 5, [20/32]: Training Loss: 0.689435135, Training Accuracy: 80.304\n",
            "Worker 5, [21/32]: Training Loss: 0.649508650, Training Accuracy: 80.928\n",
            "Worker 5, [22/32]: Training Loss: 0.623711755, Training Accuracy: 82.208\n",
            "Worker 5, [23/32]: Training Loss: 0.591280992, Training Accuracy: 83.856\n",
            "Worker 5, [24/32]: Training Loss: 0.563208973, Training Accuracy: 84.240\n",
            "Worker 5, [25/32]: Training Loss: 0.567057938, Training Accuracy: 84.000\n",
            "Worker 5, [26/32]: Training Loss: 0.532273077, Training Accuracy: 84.432\n",
            "Worker 5, [27/32]: Training Loss: 0.510597970, Training Accuracy: 85.728\n",
            "Worker 5, [28/32]: Training Loss: 0.492051040, Training Accuracy: 86.288\n",
            "Worker 5, [29/32]: Training Loss: 0.467106334, Training Accuracy: 86.944\n",
            "Worker 5, [30/32]: Training Loss: 0.439892227, Training Accuracy: 87.632\n",
            "Worker 5, [31/32]: Training Loss: 0.418438489, Training Accuracy: 88.592\n",
            "Worker 5, [32/32]: Training Loss: 0.401058620, Training Accuracy: 89.152\n",
            "Time taken for training worker 5: 0:01:40.252357\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/32]: Training Loss: 2.118151726, Training Accuracy: 45.152\n",
            "Worker 6, [02/32]: Training Loss: 1.865296223, Training Accuracy: 49.216\n",
            "Worker 6, [03/32]: Training Loss: 1.720950541, Training Accuracy: 52.528\n",
            "Worker 6, [04/32]: Training Loss: 1.637100201, Training Accuracy: 54.864\n",
            "Worker 6, [05/32]: Training Loss: 1.529540209, Training Accuracy: 57.648\n",
            "Worker 6, [06/32]: Training Loss: 1.427897154, Training Accuracy: 59.680\n",
            "Worker 6, [07/32]: Training Loss: 1.342548651, Training Accuracy: 62.032\n",
            "Worker 6, [08/32]: Training Loss: 1.258676979, Training Accuracy: 64.288\n",
            "Worker 6, [09/32]: Training Loss: 1.210356214, Training Accuracy: 64.848\n",
            "Worker 6, [10/32]: Training Loss: 1.134201187, Training Accuracy: 68.112\n",
            "Worker 6, [11/32]: Training Loss: 1.076532796, Training Accuracy: 69.376\n",
            "Worker 6, [12/32]: Training Loss: 1.026778674, Training Accuracy: 71.056\n",
            "Worker 6, [13/32]: Training Loss: 0.984172736, Training Accuracy: 71.952\n",
            "Worker 6, [14/32]: Training Loss: 0.909014133, Training Accuracy: 74.384\n",
            "Worker 6, [15/32]: Training Loss: 0.882317502, Training Accuracy: 74.368\n",
            "Worker 6, [16/32]: Training Loss: 0.831731501, Training Accuracy: 76.704\n",
            "Worker 6, [17/32]: Training Loss: 0.809795806, Training Accuracy: 77.168\n",
            "Worker 6, [18/32]: Training Loss: 0.750657054, Training Accuracy: 78.496\n",
            "Worker 6, [19/32]: Training Loss: 0.739527926, Training Accuracy: 79.248\n",
            "Worker 6, [20/32]: Training Loss: 0.705829979, Training Accuracy: 79.200\n",
            "Worker 6, [21/32]: Training Loss: 0.642057557, Training Accuracy: 81.600\n",
            "Worker 6, [22/32]: Training Loss: 0.626065350, Training Accuracy: 82.384\n",
            "Worker 6, [23/32]: Training Loss: 0.624995749, Training Accuracy: 81.664\n",
            "Worker 6, [24/32]: Training Loss: 0.579601566, Training Accuracy: 83.344\n",
            "Worker 6, [25/32]: Training Loss: 0.554408628, Training Accuracy: 84.240\n",
            "Worker 6, [26/32]: Training Loss: 0.526199262, Training Accuracy: 85.056\n",
            "Worker 6, [27/32]: Training Loss: 0.511879423, Training Accuracy: 85.536\n",
            "Worker 6, [28/32]: Training Loss: 0.490554248, Training Accuracy: 86.096\n",
            "Worker 6, [29/32]: Training Loss: 0.488426697, Training Accuracy: 86.128\n",
            "Worker 6, [30/32]: Training Loss: 0.439611519, Training Accuracy: 87.888\n",
            "Worker 6, [31/32]: Training Loss: 0.423774822, Training Accuracy: 88.272\n",
            "Worker 6, [32/32]: Training Loss: 0.401618215, Training Accuracy: 89.056\n",
            "Time taken for training worker 6: 0:01:39.307449\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/32]: Training Loss: 2.127767554, Training Accuracy: 44.800\n",
            "Worker 7, [02/32]: Training Loss: 1.882285782, Training Accuracy: 48.960\n",
            "Worker 7, [03/32]: Training Loss: 1.739330389, Training Accuracy: 52.528\n",
            "Worker 7, [04/32]: Training Loss: 1.614745921, Training Accuracy: 55.968\n",
            "Worker 7, [05/32]: Training Loss: 1.526534099, Training Accuracy: 57.856\n",
            "Worker 7, [06/32]: Training Loss: 1.436922853, Training Accuracy: 59.744\n",
            "Worker 7, [07/32]: Training Loss: 1.342109981, Training Accuracy: 62.192\n",
            "Worker 7, [08/32]: Training Loss: 1.290300978, Training Accuracy: 63.744\n",
            "Worker 7, [09/32]: Training Loss: 1.220047237, Training Accuracy: 65.376\n",
            "Worker 7, [10/32]: Training Loss: 1.143765409, Training Accuracy: 67.088\n",
            "Worker 7, [11/32]: Training Loss: 1.109858579, Training Accuracy: 69.040\n",
            "Worker 7, [12/32]: Training Loss: 1.039172891, Training Accuracy: 70.496\n",
            "Worker 7, [13/32]: Training Loss: 1.005633726, Training Accuracy: 71.072\n",
            "Worker 7, [14/32]: Training Loss: 0.948216147, Training Accuracy: 72.960\n",
            "Worker 7, [15/32]: Training Loss: 0.904449653, Training Accuracy: 74.560\n",
            "Worker 7, [16/32]: Training Loss: 0.851979531, Training Accuracy: 75.552\n",
            "Worker 7, [17/32]: Training Loss: 0.820299809, Training Accuracy: 76.448\n",
            "Worker 7, [18/32]: Training Loss: 0.760653801, Training Accuracy: 78.528\n",
            "Worker 7, [19/32]: Training Loss: 0.751440043, Training Accuracy: 78.688\n",
            "Worker 7, [20/32]: Training Loss: 0.698346515, Training Accuracy: 80.336\n",
            "Worker 7, [21/32]: Training Loss: 0.676143717, Training Accuracy: 81.232\n",
            "Worker 7, [22/32]: Training Loss: 0.645483368, Training Accuracy: 81.616\n",
            "Worker 7, [23/32]: Training Loss: 0.608210606, Training Accuracy: 82.256\n",
            "Worker 7, [24/32]: Training Loss: 0.584757582, Training Accuracy: 83.712\n",
            "Worker 7, [25/32]: Training Loss: 0.574083426, Training Accuracy: 83.760\n",
            "Worker 7, [26/32]: Training Loss: 0.545437174, Training Accuracy: 84.736\n",
            "Worker 7, [27/32]: Training Loss: 0.519320306, Training Accuracy: 85.536\n",
            "Worker 7, [28/32]: Training Loss: 0.499415358, Training Accuracy: 86.416\n",
            "Worker 7, [29/32]: Training Loss: 0.475602267, Training Accuracy: 86.464\n",
            "Worker 7, [30/32]: Training Loss: 0.463253075, Training Accuracy: 86.496\n",
            "Worker 7, [31/32]: Training Loss: 0.441642728, Training Accuracy: 87.568\n",
            "Worker 7, [32/32]: Training Loss: 0.444847030, Training Accuracy: 87.104\n",
            "Time taken for training worker 7: 0:01:38.500783\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/32]: Training Loss: 2.125059889, Training Accuracy: 44.208\n",
            "Worker 8, [02/32]: Training Loss: 1.860171619, Training Accuracy: 49.008\n",
            "Worker 8, [03/32]: Training Loss: 1.725745489, Training Accuracy: 52.000\n",
            "Worker 8, [04/32]: Training Loss: 1.626498670, Training Accuracy: 54.720\n",
            "Worker 8, [05/32]: Training Loss: 1.525878989, Training Accuracy: 57.456\n",
            "Worker 8, [06/32]: Training Loss: 1.435463340, Training Accuracy: 59.552\n",
            "Worker 8, [07/32]: Training Loss: 1.326153716, Training Accuracy: 62.400\n",
            "Worker 8, [08/32]: Training Loss: 1.282022323, Training Accuracy: 63.136\n",
            "Worker 8, [09/32]: Training Loss: 1.192281443, Training Accuracy: 66.304\n",
            "Worker 8, [10/32]: Training Loss: 1.142570629, Training Accuracy: 67.712\n",
            "Worker 8, [11/32]: Training Loss: 1.077348851, Training Accuracy: 69.040\n",
            "Worker 8, [12/32]: Training Loss: 1.014971571, Training Accuracy: 70.688\n",
            "Worker 8, [13/32]: Training Loss: 0.945515376, Training Accuracy: 72.816\n",
            "Worker 8, [14/32]: Training Loss: 0.912383259, Training Accuracy: 73.808\n",
            "Worker 8, [15/32]: Training Loss: 0.877231228, Training Accuracy: 75.056\n",
            "Worker 8, [16/32]: Training Loss: 0.832707149, Training Accuracy: 76.368\n",
            "Worker 8, [17/32]: Training Loss: 0.792060148, Training Accuracy: 77.152\n",
            "Worker 8, [18/32]: Training Loss: 0.747720137, Training Accuracy: 78.528\n",
            "Worker 8, [19/32]: Training Loss: 0.703537408, Training Accuracy: 79.920\n",
            "Worker 8, [20/32]: Training Loss: 0.693011294, Training Accuracy: 80.608\n",
            "Worker 8, [21/32]: Training Loss: 0.646627065, Training Accuracy: 81.744\n",
            "Worker 8, [22/32]: Training Loss: 0.617184956, Training Accuracy: 82.608\n",
            "Worker 8, [23/32]: Training Loss: 0.584192174, Training Accuracy: 83.696\n",
            "Worker 8, [24/32]: Training Loss: 0.570110253, Training Accuracy: 83.664\n",
            "Worker 8, [25/32]: Training Loss: 0.543276928, Training Accuracy: 85.200\n",
            "Worker 8, [26/32]: Training Loss: 0.515959692, Training Accuracy: 85.520\n",
            "Worker 8, [27/32]: Training Loss: 0.495725265, Training Accuracy: 85.984\n",
            "Worker 8, [28/32]: Training Loss: 0.480356828, Training Accuracy: 86.224\n",
            "Worker 8, [29/32]: Training Loss: 0.445962157, Training Accuracy: 87.760\n",
            "Worker 8, [30/32]: Training Loss: 0.441521136, Training Accuracy: 87.824\n",
            "Worker 8, [31/32]: Training Loss: 0.411429040, Training Accuracy: 88.560\n",
            "Worker 8, [32/32]: Training Loss: 0.405520106, Training Accuracy: 89.280\n",
            "Time taken for training worker 8: 0:01:38.804201\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.006164\n",
            "Global Update 04: Test Loss: 2.830345691, Test Accuracy: 43.520\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:53:11.899133\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:64\n",
            "==================================================\n",
            "Worker 1, [01/64]: Training Loss: 4.593810792, Training Accuracy: 1.136\n",
            "Worker 1, [02/64]: Training Loss: 4.416249660, Training Accuracy: 3.408\n",
            "Worker 1, [03/64]: Training Loss: 4.194186865, Training Accuracy: 5.344\n",
            "Worker 1, [04/64]: Training Loss: 4.066891544, Training Accuracy: 7.104\n",
            "Worker 1, [05/64]: Training Loss: 3.953594563, Training Accuracy: 8.304\n",
            "Worker 1, [06/64]: Training Loss: 3.837450397, Training Accuracy: 10.128\n",
            "Worker 1, [07/64]: Training Loss: 3.745696289, Training Accuracy: 11.472\n",
            "Worker 1, [08/64]: Training Loss: 3.645874709, Training Accuracy: 13.088\n",
            "Worker 1, [09/64]: Training Loss: 3.557200780, Training Accuracy: 15.264\n",
            "Worker 1, [10/64]: Training Loss: 3.491470843, Training Accuracy: 15.456\n",
            "Worker 1, [11/64]: Training Loss: 3.414976762, Training Accuracy: 17.552\n",
            "Worker 1, [12/64]: Training Loss: 3.326575287, Training Accuracy: 18.672\n",
            "Worker 1, [13/64]: Training Loss: 3.244805771, Training Accuracy: 19.968\n",
            "Worker 1, [14/64]: Training Loss: 3.163678464, Training Accuracy: 20.800\n",
            "Worker 1, [15/64]: Training Loss: 3.140726532, Training Accuracy: 21.408\n",
            "Worker 1, [16/64]: Training Loss: 3.040874282, Training Accuracy: 23.312\n",
            "Worker 1, [17/64]: Training Loss: 3.001861580, Training Accuracy: 23.792\n",
            "Worker 1, [18/64]: Training Loss: 2.921705042, Training Accuracy: 25.648\n",
            "Worker 1, [19/64]: Training Loss: 2.864751013, Training Accuracy: 26.752\n",
            "Worker 1, [20/64]: Training Loss: 2.803297697, Training Accuracy: 27.424\n",
            "Worker 1, [21/64]: Training Loss: 2.739976065, Training Accuracy: 29.008\n",
            "Worker 1, [22/64]: Training Loss: 2.687313289, Training Accuracy: 30.992\n",
            "Worker 1, [23/64]: Training Loss: 2.674421583, Training Accuracy: 29.680\n",
            "Worker 1, [24/64]: Training Loss: 2.577950952, Training Accuracy: 31.776\n",
            "Worker 1, [25/64]: Training Loss: 2.567440281, Training Accuracy: 32.384\n",
            "Worker 1, [26/64]: Training Loss: 2.473667202, Training Accuracy: 33.952\n",
            "Worker 1, [27/64]: Training Loss: 2.443225710, Training Accuracy: 34.736\n",
            "Worker 1, [28/64]: Training Loss: 2.330628264, Training Accuracy: 36.784\n",
            "Worker 1, [29/64]: Training Loss: 2.331828830, Training Accuracy: 37.280\n",
            "Worker 1, [30/64]: Training Loss: 2.277580691, Training Accuracy: 37.744\n",
            "Worker 1, [31/64]: Training Loss: 2.185460971, Training Accuracy: 40.560\n",
            "Worker 1, [32/64]: Training Loss: 2.139581836, Training Accuracy: 41.392\n",
            "Worker 1, [33/64]: Training Loss: 2.112027104, Training Accuracy: 41.248\n",
            "Worker 1, [34/64]: Training Loss: 2.043416832, Training Accuracy: 43.824\n",
            "Worker 1, [35/64]: Training Loss: 2.011475241, Training Accuracy: 44.144\n",
            "Worker 1, [36/64]: Training Loss: 1.929698447, Training Accuracy: 45.936\n",
            "Worker 1, [37/64]: Training Loss: 1.903094615, Training Accuracy: 46.672\n",
            "Worker 1, [38/64]: Training Loss: 1.856317279, Training Accuracy: 47.440\n",
            "Worker 1, [39/64]: Training Loss: 1.785205480, Training Accuracy: 48.880\n",
            "Worker 1, [40/64]: Training Loss: 1.778619320, Training Accuracy: 49.280\n",
            "Worker 1, [41/64]: Training Loss: 1.763617113, Training Accuracy: 49.936\n",
            "Worker 1, [42/64]: Training Loss: 1.689669824, Training Accuracy: 52.432\n",
            "Worker 1, [43/64]: Training Loss: 1.641097295, Training Accuracy: 53.184\n",
            "Worker 1, [44/64]: Training Loss: 1.576078246, Training Accuracy: 54.096\n",
            "Worker 1, [45/64]: Training Loss: 1.568437449, Training Accuracy: 54.624\n",
            "Worker 1, [46/64]: Training Loss: 1.543954760, Training Accuracy: 55.632\n",
            "Worker 1, [47/64]: Training Loss: 1.486303509, Training Accuracy: 56.816\n",
            "Worker 1, [48/64]: Training Loss: 1.453179271, Training Accuracy: 57.728\n",
            "Worker 1, [49/64]: Training Loss: 1.442118614, Training Accuracy: 58.768\n",
            "Worker 1, [50/64]: Training Loss: 1.402820852, Training Accuracy: 59.056\n",
            "Worker 1, [51/64]: Training Loss: 1.367699539, Training Accuracy: 60.560\n",
            "Worker 1, [52/64]: Training Loss: 1.384767452, Training Accuracy: 60.224\n",
            "Worker 1, [53/64]: Training Loss: 1.312606557, Training Accuracy: 61.568\n",
            "Worker 1, [54/64]: Training Loss: 1.307213912, Training Accuracy: 61.216\n",
            "Worker 1, [55/64]: Training Loss: 1.237892157, Training Accuracy: 63.440\n",
            "Worker 1, [56/64]: Training Loss: 1.246465114, Training Accuracy: 63.936\n",
            "Worker 1, [57/64]: Training Loss: 1.215865274, Training Accuracy: 64.352\n",
            "Worker 1, [58/64]: Training Loss: 1.183700388, Training Accuracy: 64.752\n",
            "Worker 1, [59/64]: Training Loss: 1.206917373, Training Accuracy: 64.080\n",
            "Worker 1, [60/64]: Training Loss: 1.135906243, Training Accuracy: 66.784\n",
            "Worker 1, [61/64]: Training Loss: 1.157984040, Training Accuracy: 65.888\n",
            "Worker 1, [62/64]: Training Loss: 1.117261977, Training Accuracy: 67.280\n",
            "Worker 1, [63/64]: Training Loss: 1.098496288, Training Accuracy: 67.216\n",
            "Worker 1, [64/64]: Training Loss: 1.032929139, Training Accuracy: 68.768\n",
            "Time taken for training worker 1: 0:03:19.819195\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/64]: Training Loss: 4.593876537, Training Accuracy: 1.600\n",
            "Worker 2, [02/64]: Training Loss: 4.396991637, Training Accuracy: 3.488\n",
            "Worker 2, [03/64]: Training Loss: 4.166819079, Training Accuracy: 5.616\n",
            "Worker 2, [04/64]: Training Loss: 4.011343044, Training Accuracy: 7.664\n",
            "Worker 2, [05/64]: Training Loss: 3.871886161, Training Accuracy: 9.808\n",
            "Worker 2, [06/64]: Training Loss: 3.783717445, Training Accuracy: 11.056\n",
            "Worker 2, [07/64]: Training Loss: 3.668075515, Training Accuracy: 12.992\n",
            "Worker 2, [08/64]: Training Loss: 3.598391141, Training Accuracy: 14.000\n",
            "Worker 2, [09/64]: Training Loss: 3.507555665, Training Accuracy: 15.472\n",
            "Worker 2, [10/64]: Training Loss: 3.407510592, Training Accuracy: 17.200\n",
            "Worker 2, [11/64]: Training Loss: 3.349804190, Training Accuracy: 17.952\n",
            "Worker 2, [12/64]: Training Loss: 3.286297504, Training Accuracy: 18.560\n",
            "Worker 2, [13/64]: Training Loss: 3.214770787, Training Accuracy: 19.984\n",
            "Worker 2, [14/64]: Training Loss: 3.141457594, Training Accuracy: 21.376\n",
            "Worker 2, [15/64]: Training Loss: 3.080282034, Training Accuracy: 22.544\n",
            "Worker 2, [16/64]: Training Loss: 3.009249174, Training Accuracy: 23.872\n",
            "Worker 2, [17/64]: Training Loss: 2.961088490, Training Accuracy: 24.512\n",
            "Worker 2, [18/64]: Training Loss: 2.882747667, Training Accuracy: 26.352\n",
            "Worker 2, [19/64]: Training Loss: 2.837197966, Training Accuracy: 26.736\n",
            "Worker 2, [20/64]: Training Loss: 2.766586255, Training Accuracy: 28.704\n",
            "Worker 2, [21/64]: Training Loss: 2.732224170, Training Accuracy: 28.848\n",
            "Worker 2, [22/64]: Training Loss: 2.658937503, Training Accuracy: 29.872\n",
            "Worker 2, [23/64]: Training Loss: 2.604695284, Training Accuracy: 31.392\n",
            "Worker 2, [24/64]: Training Loss: 2.523882810, Training Accuracy: 33.232\n",
            "Worker 2, [25/64]: Training Loss: 2.508740384, Training Accuracy: 33.120\n",
            "Worker 2, [26/64]: Training Loss: 2.440421405, Training Accuracy: 34.352\n",
            "Worker 2, [27/64]: Training Loss: 2.397071463, Training Accuracy: 35.024\n",
            "Worker 2, [28/64]: Training Loss: 2.315260864, Training Accuracy: 36.976\n",
            "Worker 2, [29/64]: Training Loss: 2.277763104, Training Accuracy: 37.808\n",
            "Worker 2, [30/64]: Training Loss: 2.228453078, Training Accuracy: 39.344\n",
            "Worker 2, [31/64]: Training Loss: 2.144858936, Training Accuracy: 41.136\n",
            "Worker 2, [32/64]: Training Loss: 2.112540391, Training Accuracy: 41.568\n",
            "Worker 2, [33/64]: Training Loss: 2.059577538, Training Accuracy: 42.992\n",
            "Worker 2, [34/64]: Training Loss: 2.019809270, Training Accuracy: 43.248\n",
            "Worker 2, [35/64]: Training Loss: 1.959971293, Training Accuracy: 45.200\n",
            "Worker 2, [36/64]: Training Loss: 1.912356923, Training Accuracy: 46.640\n",
            "Worker 2, [37/64]: Training Loss: 1.868864894, Training Accuracy: 47.840\n",
            "Worker 2, [38/64]: Training Loss: 1.830931591, Training Accuracy: 48.352\n",
            "Worker 2, [39/64]: Training Loss: 1.808295184, Training Accuracy: 48.816\n",
            "Worker 2, [40/64]: Training Loss: 1.733429213, Training Accuracy: 49.936\n",
            "Worker 2, [41/64]: Training Loss: 1.741165181, Training Accuracy: 50.800\n",
            "Worker 2, [42/64]: Training Loss: 1.709294929, Training Accuracy: 50.880\n",
            "Worker 2, [43/64]: Training Loss: 1.633051496, Training Accuracy: 53.904\n",
            "Worker 2, [44/64]: Training Loss: 1.610635720, Training Accuracy: 53.840\n",
            "Worker 2, [45/64]: Training Loss: 1.590159133, Training Accuracy: 53.824\n",
            "Worker 2, [46/64]: Training Loss: 1.551496129, Training Accuracy: 55.200\n",
            "Worker 2, [47/64]: Training Loss: 1.523210920, Training Accuracy: 56.816\n",
            "Worker 2, [48/64]: Training Loss: 1.485429367, Training Accuracy: 57.536\n",
            "Worker 2, [49/64]: Training Loss: 1.433081516, Training Accuracy: 58.032\n",
            "Worker 2, [50/64]: Training Loss: 1.427986519, Training Accuracy: 58.528\n",
            "Worker 2, [51/64]: Training Loss: 1.345879669, Training Accuracy: 60.624\n",
            "Worker 2, [52/64]: Training Loss: 1.387190348, Training Accuracy: 59.664\n",
            "Worker 2, [53/64]: Training Loss: 1.299588705, Training Accuracy: 62.064\n",
            "Worker 2, [54/64]: Training Loss: 1.295482963, Training Accuracy: 62.032\n",
            "Worker 2, [55/64]: Training Loss: 1.273323059, Training Accuracy: 61.904\n",
            "Worker 2, [56/64]: Training Loss: 1.253154611, Training Accuracy: 62.640\n",
            "Worker 2, [57/64]: Training Loss: 1.243621666, Training Accuracy: 63.456\n",
            "Worker 2, [58/64]: Training Loss: 1.222778919, Training Accuracy: 63.728\n",
            "Worker 2, [59/64]: Training Loss: 1.145268567, Training Accuracy: 66.272\n",
            "Worker 2, [60/64]: Training Loss: 1.148508379, Training Accuracy: 66.208\n",
            "Worker 2, [61/64]: Training Loss: 1.093479711, Training Accuracy: 67.152\n",
            "Worker 2, [62/64]: Training Loss: 1.089279006, Training Accuracy: 66.976\n",
            "Worker 2, [63/64]: Training Loss: 1.112633019, Training Accuracy: 67.472\n",
            "Worker 2, [64/64]: Training Loss: 1.108587887, Training Accuracy: 67.904\n",
            "Time taken for training worker 2: 0:03:16.027889\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/64]: Training Loss: 4.591477341, Training Accuracy: 1.648\n",
            "Worker 3, [02/64]: Training Loss: 4.364274813, Training Accuracy: 3.360\n",
            "Worker 3, [03/64]: Training Loss: 4.142757844, Training Accuracy: 6.112\n",
            "Worker 3, [04/64]: Training Loss: 3.990979959, Training Accuracy: 8.304\n",
            "Worker 3, [05/64]: Training Loss: 3.890137480, Training Accuracy: 9.984\n",
            "Worker 3, [06/64]: Training Loss: 3.782701984, Training Accuracy: 11.616\n",
            "Worker 3, [07/64]: Training Loss: 3.706333083, Training Accuracy: 12.464\n",
            "Worker 3, [08/64]: Training Loss: 3.624435729, Training Accuracy: 13.648\n",
            "Worker 3, [09/64]: Training Loss: 3.521731532, Training Accuracy: 15.248\n",
            "Worker 3, [10/64]: Training Loss: 3.436956921, Training Accuracy: 16.496\n",
            "Worker 3, [11/64]: Training Loss: 3.360073021, Training Accuracy: 17.632\n",
            "Worker 3, [12/64]: Training Loss: 3.281873029, Training Accuracy: 18.368\n",
            "Worker 3, [13/64]: Training Loss: 3.220921064, Training Accuracy: 20.016\n",
            "Worker 3, [14/64]: Training Loss: 3.159885195, Training Accuracy: 21.216\n",
            "Worker 3, [15/64]: Training Loss: 3.097515770, Training Accuracy: 22.064\n",
            "Worker 3, [16/64]: Training Loss: 3.025139113, Training Accuracy: 23.536\n",
            "Worker 3, [17/64]: Training Loss: 2.973809478, Training Accuracy: 24.224\n",
            "Worker 3, [18/64]: Training Loss: 2.924975368, Training Accuracy: 25.712\n",
            "Worker 3, [19/64]: Training Loss: 2.855871128, Training Accuracy: 26.048\n",
            "Worker 3, [20/64]: Training Loss: 2.803866479, Training Accuracy: 27.248\n",
            "Worker 3, [21/64]: Training Loss: 2.720048975, Training Accuracy: 30.176\n",
            "Worker 3, [22/64]: Training Loss: 2.685431688, Training Accuracy: 30.016\n",
            "Worker 3, [23/64]: Training Loss: 2.634960128, Training Accuracy: 30.912\n",
            "Worker 3, [24/64]: Training Loss: 2.597434931, Training Accuracy: 31.840\n",
            "Worker 3, [25/64]: Training Loss: 2.490193722, Training Accuracy: 33.776\n",
            "Worker 3, [26/64]: Training Loss: 2.447618273, Training Accuracy: 34.304\n",
            "Worker 3, [27/64]: Training Loss: 2.411167903, Training Accuracy: 35.744\n",
            "Worker 3, [28/64]: Training Loss: 2.371492436, Training Accuracy: 35.888\n",
            "Worker 3, [29/64]: Training Loss: 2.326459483, Training Accuracy: 37.488\n",
            "Worker 3, [30/64]: Training Loss: 2.236039898, Training Accuracy: 39.632\n",
            "Worker 3, [31/64]: Training Loss: 2.200863372, Training Accuracy: 39.552\n",
            "Worker 3, [32/64]: Training Loss: 2.196719325, Training Accuracy: 40.672\n",
            "Worker 3, [33/64]: Training Loss: 2.098876310, Training Accuracy: 43.424\n",
            "Worker 3, [34/64]: Training Loss: 2.061714042, Training Accuracy: 43.120\n",
            "Worker 3, [35/64]: Training Loss: 2.017863601, Training Accuracy: 43.680\n",
            "Worker 3, [36/64]: Training Loss: 1.917299138, Training Accuracy: 46.640\n",
            "Worker 3, [37/64]: Training Loss: 1.914276212, Training Accuracy: 46.096\n",
            "Worker 3, [38/64]: Training Loss: 1.891228199, Training Accuracy: 46.640\n",
            "Worker 3, [39/64]: Training Loss: 1.839146196, Training Accuracy: 48.704\n",
            "Worker 3, [40/64]: Training Loss: 1.801858868, Training Accuracy: 49.104\n",
            "Worker 3, [41/64]: Training Loss: 1.730143612, Training Accuracy: 51.152\n",
            "Worker 3, [42/64]: Training Loss: 1.724374184, Training Accuracy: 51.488\n",
            "Worker 3, [43/64]: Training Loss: 1.672631838, Training Accuracy: 52.304\n",
            "Worker 3, [44/64]: Training Loss: 1.613225571, Training Accuracy: 53.344\n",
            "Worker 3, [45/64]: Training Loss: 1.598159199, Training Accuracy: 54.304\n",
            "Worker 3, [46/64]: Training Loss: 1.590078540, Training Accuracy: 55.200\n",
            "Worker 3, [47/64]: Training Loss: 1.509307590, Training Accuracy: 56.736\n",
            "Worker 3, [48/64]: Training Loss: 1.485381503, Training Accuracy: 57.072\n",
            "Worker 3, [49/64]: Training Loss: 1.477711462, Training Accuracy: 57.088\n",
            "Worker 3, [50/64]: Training Loss: 1.446456369, Training Accuracy: 57.760\n",
            "Worker 3, [51/64]: Training Loss: 1.371373692, Training Accuracy: 60.592\n",
            "Worker 3, [52/64]: Training Loss: 1.357445121, Training Accuracy: 60.752\n",
            "Worker 3, [53/64]: Training Loss: 1.359423134, Training Accuracy: 60.528\n",
            "Worker 3, [54/64]: Training Loss: 1.336056828, Training Accuracy: 61.120\n",
            "Worker 3, [55/64]: Training Loss: 1.289970964, Training Accuracy: 62.960\n",
            "Worker 3, [56/64]: Training Loss: 1.224373771, Training Accuracy: 64.336\n",
            "Worker 3, [57/64]: Training Loss: 1.210379474, Training Accuracy: 64.880\n",
            "Worker 3, [58/64]: Training Loss: 1.231328957, Training Accuracy: 63.376\n",
            "Worker 3, [59/64]: Training Loss: 1.189772729, Training Accuracy: 65.168\n",
            "Worker 3, [60/64]: Training Loss: 1.164883944, Training Accuracy: 65.264\n",
            "Worker 3, [61/64]: Training Loss: 1.147607173, Training Accuracy: 65.520\n",
            "Worker 3, [62/64]: Training Loss: 1.100291124, Training Accuracy: 67.408\n",
            "Worker 3, [63/64]: Training Loss: 1.128159786, Training Accuracy: 67.616\n",
            "Worker 3, [64/64]: Training Loss: 1.071327749, Training Accuracy: 67.872\n",
            "Time taken for training worker 3: 0:03:18.297124\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/64]: Training Loss: 4.595183051, Training Accuracy: 1.456\n",
            "Worker 4, [02/64]: Training Loss: 4.397448686, Training Accuracy: 3.456\n",
            "Worker 4, [03/64]: Training Loss: 4.169993498, Training Accuracy: 5.392\n",
            "Worker 4, [04/64]: Training Loss: 4.029446271, Training Accuracy: 7.616\n",
            "Worker 4, [05/64]: Training Loss: 3.900356967, Training Accuracy: 9.024\n",
            "Worker 4, [06/64]: Training Loss: 3.801716036, Training Accuracy: 10.352\n",
            "Worker 4, [07/64]: Training Loss: 3.711785307, Training Accuracy: 12.080\n",
            "Worker 4, [08/64]: Training Loss: 3.626785436, Training Accuracy: 13.424\n",
            "Worker 4, [09/64]: Training Loss: 3.547996234, Training Accuracy: 14.656\n",
            "Worker 4, [10/64]: Training Loss: 3.491301870, Training Accuracy: 15.200\n",
            "Worker 4, [11/64]: Training Loss: 3.409455703, Training Accuracy: 16.928\n",
            "Worker 4, [12/64]: Training Loss: 3.331473329, Training Accuracy: 18.000\n",
            "Worker 4, [13/64]: Training Loss: 3.271619838, Training Accuracy: 19.008\n",
            "Worker 4, [14/64]: Training Loss: 3.183990048, Training Accuracy: 21.232\n",
            "Worker 4, [15/64]: Training Loss: 3.131470780, Training Accuracy: 21.744\n",
            "Worker 4, [16/64]: Training Loss: 3.054791288, Training Accuracy: 22.752\n",
            "Worker 4, [17/64]: Training Loss: 2.985133390, Training Accuracy: 24.256\n",
            "Worker 4, [18/64]: Training Loss: 2.949744864, Training Accuracy: 25.136\n",
            "Worker 4, [19/64]: Training Loss: 2.884814360, Training Accuracy: 26.512\n",
            "Worker 4, [20/64]: Training Loss: 2.857628443, Training Accuracy: 27.248\n",
            "Worker 4, [21/64]: Training Loss: 2.773384853, Training Accuracy: 28.560\n",
            "Worker 4, [22/64]: Training Loss: 2.723685442, Training Accuracy: 29.328\n",
            "Worker 4, [23/64]: Training Loss: 2.666525396, Training Accuracy: 31.504\n",
            "Worker 4, [24/64]: Training Loss: 2.618590421, Training Accuracy: 32.336\n",
            "Worker 4, [25/64]: Training Loss: 2.566497379, Training Accuracy: 32.512\n",
            "Worker 4, [26/64]: Training Loss: 2.495290621, Training Accuracy: 33.952\n",
            "Worker 4, [27/64]: Training Loss: 2.421797661, Training Accuracy: 34.560\n",
            "Worker 4, [28/64]: Training Loss: 2.369231930, Training Accuracy: 35.616\n",
            "Worker 4, [29/64]: Training Loss: 2.350904286, Training Accuracy: 36.768\n",
            "Worker 4, [30/64]: Training Loss: 2.284330896, Training Accuracy: 38.320\n",
            "Worker 4, [31/64]: Training Loss: 2.221494175, Training Accuracy: 39.568\n",
            "Worker 4, [32/64]: Training Loss: 2.171087114, Training Accuracy: 41.024\n",
            "Worker 4, [33/64]: Training Loss: 2.143831816, Training Accuracy: 41.968\n",
            "Worker 4, [34/64]: Training Loss: 2.093703682, Training Accuracy: 42.656\n",
            "Worker 4, [35/64]: Training Loss: 2.025526651, Training Accuracy: 43.680\n",
            "Worker 4, [36/64]: Training Loss: 1.959347603, Training Accuracy: 45.312\n",
            "Worker 4, [37/64]: Training Loss: 1.970714061, Training Accuracy: 44.384\n",
            "Worker 4, [38/64]: Training Loss: 1.892878649, Training Accuracy: 47.024\n",
            "Worker 4, [39/64]: Training Loss: 1.840264122, Training Accuracy: 47.504\n",
            "Worker 4, [40/64]: Training Loss: 1.826945922, Training Accuracy: 48.224\n",
            "Worker 4, [41/64]: Training Loss: 1.776332074, Training Accuracy: 49.680\n",
            "Worker 4, [42/64]: Training Loss: 1.745419398, Training Accuracy: 51.280\n",
            "Worker 4, [43/64]: Training Loss: 1.722917642, Training Accuracy: 50.848\n",
            "Worker 4, [44/64]: Training Loss: 1.652477958, Training Accuracy: 52.912\n",
            "Worker 4, [45/64]: Training Loss: 1.595176093, Training Accuracy: 54.416\n",
            "Worker 4, [46/64]: Training Loss: 1.608086340, Training Accuracy: 53.472\n",
            "Worker 4, [47/64]: Training Loss: 1.520926718, Training Accuracy: 56.912\n",
            "Worker 4, [48/64]: Training Loss: 1.516771798, Training Accuracy: 56.896\n",
            "Worker 4, [49/64]: Training Loss: 1.482127229, Training Accuracy: 58.080\n",
            "Worker 4, [50/64]: Training Loss: 1.446035859, Training Accuracy: 58.432\n",
            "Worker 4, [51/64]: Training Loss: 1.380097839, Training Accuracy: 59.744\n",
            "Worker 4, [52/64]: Training Loss: 1.386367880, Training Accuracy: 60.448\n",
            "Worker 4, [53/64]: Training Loss: 1.343963362, Training Accuracy: 60.624\n",
            "Worker 4, [54/64]: Training Loss: 1.382207974, Training Accuracy: 60.432\n",
            "Worker 4, [55/64]: Training Loss: 1.345987844, Training Accuracy: 60.624\n",
            "Worker 4, [56/64]: Training Loss: 1.257273270, Training Accuracy: 63.520\n",
            "Worker 4, [57/64]: Training Loss: 1.241675685, Training Accuracy: 63.616\n",
            "Worker 4, [58/64]: Training Loss: 1.260636496, Training Accuracy: 63.472\n",
            "Worker 4, [59/64]: Training Loss: 1.232243392, Training Accuracy: 64.176\n",
            "Worker 4, [60/64]: Training Loss: 1.193790090, Training Accuracy: 64.928\n",
            "Worker 4, [61/64]: Training Loss: 1.178034078, Training Accuracy: 65.392\n",
            "Worker 4, [62/64]: Training Loss: 1.124379100, Training Accuracy: 66.864\n",
            "Worker 4, [63/64]: Training Loss: 1.126427046, Training Accuracy: 67.120\n",
            "Worker 4, [64/64]: Training Loss: 1.119789716, Training Accuracy: 67.552\n",
            "Time taken for training worker 4: 0:03:18.678540\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/64]: Training Loss: 4.592857195, Training Accuracy: 1.248\n",
            "Worker 5, [02/64]: Training Loss: 4.399714480, Training Accuracy: 3.680\n",
            "Worker 5, [03/64]: Training Loss: 4.170149548, Training Accuracy: 5.840\n",
            "Worker 5, [04/64]: Training Loss: 4.029404438, Training Accuracy: 8.496\n",
            "Worker 5, [05/64]: Training Loss: 3.902161214, Training Accuracy: 9.936\n",
            "Worker 5, [06/64]: Training Loss: 3.799592084, Training Accuracy: 10.960\n",
            "Worker 5, [07/64]: Training Loss: 3.707585921, Training Accuracy: 12.912\n",
            "Worker 5, [08/64]: Training Loss: 3.631930446, Training Accuracy: 13.424\n",
            "Worker 5, [09/64]: Training Loss: 3.519323454, Training Accuracy: 15.104\n",
            "Worker 5, [10/64]: Training Loss: 3.457964973, Training Accuracy: 16.544\n",
            "Worker 5, [11/64]: Training Loss: 3.377848496, Training Accuracy: 18.048\n",
            "Worker 5, [12/64]: Training Loss: 3.321698488, Training Accuracy: 18.336\n",
            "Worker 5, [13/64]: Training Loss: 3.211108570, Training Accuracy: 20.288\n",
            "Worker 5, [14/64]: Training Loss: 3.159389184, Training Accuracy: 21.600\n",
            "Worker 5, [15/64]: Training Loss: 3.105125330, Training Accuracy: 22.288\n",
            "Worker 5, [16/64]: Training Loss: 3.012856644, Training Accuracy: 23.712\n",
            "Worker 5, [17/64]: Training Loss: 2.970113044, Training Accuracy: 24.656\n",
            "Worker 5, [18/64]: Training Loss: 2.879842984, Training Accuracy: 26.640\n",
            "Worker 5, [19/64]: Training Loss: 2.870992884, Training Accuracy: 26.560\n",
            "Worker 5, [20/64]: Training Loss: 2.818503127, Training Accuracy: 26.944\n",
            "Worker 5, [21/64]: Training Loss: 2.702792725, Training Accuracy: 29.824\n",
            "Worker 5, [22/64]: Training Loss: 2.643023912, Training Accuracy: 31.072\n",
            "Worker 5, [23/64]: Training Loss: 2.582778636, Training Accuracy: 31.344\n",
            "Worker 5, [24/64]: Training Loss: 2.575983726, Training Accuracy: 32.768\n",
            "Worker 5, [25/64]: Training Loss: 2.481944741, Training Accuracy: 34.080\n",
            "Worker 5, [26/64]: Training Loss: 2.424259914, Training Accuracy: 35.264\n",
            "Worker 5, [27/64]: Training Loss: 2.400870641, Training Accuracy: 35.136\n",
            "Worker 5, [28/64]: Training Loss: 2.341922946, Training Accuracy: 36.944\n",
            "Worker 5, [29/64]: Training Loss: 2.278165013, Training Accuracy: 38.144\n",
            "Worker 5, [30/64]: Training Loss: 2.220029523, Training Accuracy: 39.552\n",
            "Worker 5, [31/64]: Training Loss: 2.201264437, Training Accuracy: 39.888\n",
            "Worker 5, [32/64]: Training Loss: 2.117261466, Training Accuracy: 41.904\n",
            "Worker 5, [33/64]: Training Loss: 2.046060903, Training Accuracy: 43.936\n",
            "Worker 5, [34/64]: Training Loss: 2.002076244, Training Accuracy: 44.480\n",
            "Worker 5, [35/64]: Training Loss: 1.973530153, Training Accuracy: 46.096\n",
            "Worker 5, [36/64]: Training Loss: 1.942216113, Training Accuracy: 45.168\n",
            "Worker 5, [37/64]: Training Loss: 1.893812540, Training Accuracy: 47.456\n",
            "Worker 5, [38/64]: Training Loss: 1.856289478, Training Accuracy: 47.360\n",
            "Worker 5, [39/64]: Training Loss: 1.805796193, Training Accuracy: 50.096\n",
            "Worker 5, [40/64]: Training Loss: 1.728176926, Training Accuracy: 51.472\n",
            "Worker 5, [41/64]: Training Loss: 1.710184850, Training Accuracy: 51.872\n",
            "Worker 5, [42/64]: Training Loss: 1.638599117, Training Accuracy: 53.472\n",
            "Worker 5, [43/64]: Training Loss: 1.653922349, Training Accuracy: 53.392\n",
            "Worker 5, [44/64]: Training Loss: 1.619221886, Training Accuracy: 53.328\n",
            "Worker 5, [45/64]: Training Loss: 1.541608955, Training Accuracy: 55.120\n",
            "Worker 5, [46/64]: Training Loss: 1.540479327, Training Accuracy: 55.856\n",
            "Worker 5, [47/64]: Training Loss: 1.478588028, Training Accuracy: 57.088\n",
            "Worker 5, [48/64]: Training Loss: 1.463355173, Training Accuracy: 58.064\n",
            "Worker 5, [49/64]: Training Loss: 1.442238468, Training Accuracy: 58.544\n",
            "Worker 5, [50/64]: Training Loss: 1.430297800, Training Accuracy: 58.592\n",
            "Worker 5, [51/64]: Training Loss: 1.364052891, Training Accuracy: 59.984\n",
            "Worker 5, [52/64]: Training Loss: 1.360231504, Training Accuracy: 60.208\n",
            "Worker 5, [53/64]: Training Loss: 1.282117735, Training Accuracy: 62.192\n",
            "Worker 5, [54/64]: Training Loss: 1.306780526, Training Accuracy: 61.568\n",
            "Worker 5, [55/64]: Training Loss: 1.271504610, Training Accuracy: 62.608\n",
            "Worker 5, [56/64]: Training Loss: 1.224857757, Training Accuracy: 63.872\n",
            "Worker 5, [57/64]: Training Loss: 1.238590099, Training Accuracy: 63.648\n",
            "Worker 5, [58/64]: Training Loss: 1.184542872, Training Accuracy: 66.208\n",
            "Worker 5, [59/64]: Training Loss: 1.197928526, Training Accuracy: 64.752\n",
            "Worker 5, [60/64]: Training Loss: 1.186679745, Training Accuracy: 64.976\n",
            "Worker 5, [61/64]: Training Loss: 1.132531406, Training Accuracy: 66.240\n",
            "Worker 5, [62/64]: Training Loss: 1.116890723, Training Accuracy: 67.344\n",
            "Worker 5, [63/64]: Training Loss: 1.145232392, Training Accuracy: 66.016\n",
            "Worker 5, [64/64]: Training Loss: 1.062817864, Training Accuracy: 68.688\n",
            "Time taken for training worker 5: 0:03:16.753304\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/64]: Training Loss: 4.589878763, Training Accuracy: 1.696\n",
            "Worker 6, [02/64]: Training Loss: 4.364540796, Training Accuracy: 4.224\n",
            "Worker 6, [03/64]: Training Loss: 4.126939606, Training Accuracy: 6.336\n",
            "Worker 6, [04/64]: Training Loss: 3.974288077, Training Accuracy: 8.992\n",
            "Worker 6, [05/64]: Training Loss: 3.848202472, Training Accuracy: 10.112\n",
            "Worker 6, [06/64]: Training Loss: 3.772083660, Training Accuracy: 11.488\n",
            "Worker 6, [07/64]: Training Loss: 3.670751314, Training Accuracy: 13.856\n",
            "Worker 6, [08/64]: Training Loss: 3.582482155, Training Accuracy: 14.944\n",
            "Worker 6, [09/64]: Training Loss: 3.507961791, Training Accuracy: 15.728\n",
            "Worker 6, [10/64]: Training Loss: 3.424357069, Training Accuracy: 16.848\n",
            "Worker 6, [11/64]: Training Loss: 3.347233553, Training Accuracy: 18.480\n",
            "Worker 6, [12/64]: Training Loss: 3.258943507, Training Accuracy: 20.064\n",
            "Worker 6, [13/64]: Training Loss: 3.202289732, Training Accuracy: 20.816\n",
            "Worker 6, [14/64]: Training Loss: 3.132729107, Training Accuracy: 22.112\n",
            "Worker 6, [15/64]: Training Loss: 3.073354084, Training Accuracy: 23.280\n",
            "Worker 6, [16/64]: Training Loss: 3.020141336, Training Accuracy: 23.552\n",
            "Worker 6, [17/64]: Training Loss: 2.962202131, Training Accuracy: 24.624\n",
            "Worker 6, [18/64]: Training Loss: 2.856591923, Training Accuracy: 26.896\n",
            "Worker 6, [19/64]: Training Loss: 2.845369896, Training Accuracy: 26.320\n",
            "Worker 6, [20/64]: Training Loss: 2.766191838, Training Accuracy: 28.640\n",
            "Worker 6, [21/64]: Training Loss: 2.730450635, Training Accuracy: 28.720\n",
            "Worker 6, [22/64]: Training Loss: 2.652635934, Training Accuracy: 30.000\n",
            "Worker 6, [23/64]: Training Loss: 2.606308747, Training Accuracy: 31.408\n",
            "Worker 6, [24/64]: Training Loss: 2.584570092, Training Accuracy: 31.520\n",
            "Worker 6, [25/64]: Training Loss: 2.465992104, Training Accuracy: 34.736\n",
            "Worker 6, [26/64]: Training Loss: 2.431614802, Training Accuracy: 34.992\n",
            "Worker 6, [27/64]: Training Loss: 2.403464019, Training Accuracy: 34.912\n",
            "Worker 6, [28/64]: Training Loss: 2.349832227, Training Accuracy: 36.304\n",
            "Worker 6, [29/64]: Training Loss: 2.282678416, Training Accuracy: 38.096\n",
            "Worker 6, [30/64]: Training Loss: 2.268540905, Training Accuracy: 38.352\n",
            "Worker 6, [31/64]: Training Loss: 2.167237754, Training Accuracy: 41.616\n",
            "Worker 6, [32/64]: Training Loss: 2.140133512, Training Accuracy: 41.056\n",
            "Worker 6, [33/64]: Training Loss: 2.111441939, Training Accuracy: 42.272\n",
            "Worker 6, [34/64]: Training Loss: 2.059124205, Training Accuracy: 43.024\n",
            "Worker 6, [35/64]: Training Loss: 1.983081094, Training Accuracy: 44.608\n",
            "Worker 6, [36/64]: Training Loss: 1.913627666, Training Accuracy: 46.224\n",
            "Worker 6, [37/64]: Training Loss: 1.891371468, Training Accuracy: 47.680\n",
            "Worker 6, [38/64]: Training Loss: 1.825519423, Training Accuracy: 48.032\n",
            "Worker 6, [39/64]: Training Loss: 1.768742991, Training Accuracy: 50.064\n",
            "Worker 6, [40/64]: Training Loss: 1.776538456, Training Accuracy: 50.544\n",
            "Worker 6, [41/64]: Training Loss: 1.685156022, Training Accuracy: 52.128\n",
            "Worker 6, [42/64]: Training Loss: 1.692125665, Training Accuracy: 51.248\n",
            "Worker 6, [43/64]: Training Loss: 1.641282301, Training Accuracy: 53.424\n",
            "Worker 6, [44/64]: Training Loss: 1.645663823, Training Accuracy: 51.776\n",
            "Worker 6, [45/64]: Training Loss: 1.616044023, Training Accuracy: 53.744\n",
            "Worker 6, [46/64]: Training Loss: 1.566575318, Training Accuracy: 55.104\n",
            "Worker 6, [47/64]: Training Loss: 1.475532026, Training Accuracy: 57.008\n",
            "Worker 6, [48/64]: Training Loss: 1.500732786, Training Accuracy: 57.392\n",
            "Worker 6, [49/64]: Training Loss: 1.465266830, Training Accuracy: 58.528\n",
            "Worker 6, [50/64]: Training Loss: 1.423106281, Training Accuracy: 58.304\n",
            "Worker 6, [51/64]: Training Loss: 1.370465912, Training Accuracy: 59.936\n",
            "Worker 6, [52/64]: Training Loss: 1.389182596, Training Accuracy: 59.408\n",
            "Worker 6, [53/64]: Training Loss: 1.357719427, Training Accuracy: 59.952\n",
            "Worker 6, [54/64]: Training Loss: 1.263025766, Training Accuracy: 63.248\n",
            "Worker 6, [55/64]: Training Loss: 1.281510432, Training Accuracy: 62.704\n",
            "Worker 6, [56/64]: Training Loss: 1.212740704, Training Accuracy: 65.024\n",
            "Worker 6, [57/64]: Training Loss: 1.230361610, Training Accuracy: 63.856\n",
            "Worker 6, [58/64]: Training Loss: 1.180816757, Training Accuracy: 65.088\n",
            "Worker 6, [59/64]: Training Loss: 1.175704468, Training Accuracy: 65.664\n",
            "Worker 6, [60/64]: Training Loss: 1.161788406, Training Accuracy: 66.176\n",
            "Worker 6, [61/64]: Training Loss: 1.107332337, Training Accuracy: 66.848\n",
            "Worker 6, [62/64]: Training Loss: 1.090692358, Training Accuracy: 67.792\n",
            "Worker 6, [63/64]: Training Loss: 1.089817421, Training Accuracy: 67.968\n",
            "Worker 6, [64/64]: Training Loss: 1.073204127, Training Accuracy: 67.664\n",
            "Time taken for training worker 6: 0:03:32.986687\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/64]: Training Loss: 4.592359655, Training Accuracy: 1.856\n",
            "Worker 7, [02/64]: Training Loss: 4.387655346, Training Accuracy: 3.568\n",
            "Worker 7, [03/64]: Training Loss: 4.157711070, Training Accuracy: 5.808\n",
            "Worker 7, [04/64]: Training Loss: 3.987987183, Training Accuracy: 7.904\n",
            "Worker 7, [05/64]: Training Loss: 3.886674297, Training Accuracy: 9.344\n",
            "Worker 7, [06/64]: Training Loss: 3.795288828, Training Accuracy: 10.784\n",
            "Worker 7, [07/64]: Training Loss: 3.693867231, Training Accuracy: 12.608\n",
            "Worker 7, [08/64]: Training Loss: 3.610435423, Training Accuracy: 14.112\n",
            "Worker 7, [09/64]: Training Loss: 3.520011912, Training Accuracy: 14.880\n",
            "Worker 7, [10/64]: Training Loss: 3.466333983, Training Accuracy: 16.400\n",
            "Worker 7, [11/64]: Training Loss: 3.371212463, Training Accuracy: 17.152\n",
            "Worker 7, [12/64]: Training Loss: 3.268130220, Training Accuracy: 19.232\n",
            "Worker 7, [13/64]: Training Loss: 3.200025220, Training Accuracy: 20.304\n",
            "Worker 7, [14/64]: Training Loss: 3.163248875, Training Accuracy: 21.296\n",
            "Worker 7, [15/64]: Training Loss: 3.089549863, Training Accuracy: 22.832\n",
            "Worker 7, [16/64]: Training Loss: 3.000321945, Training Accuracy: 23.760\n",
            "Worker 7, [17/64]: Training Loss: 2.932671148, Training Accuracy: 24.816\n",
            "Worker 7, [18/64]: Training Loss: 2.894690939, Training Accuracy: 25.616\n",
            "Worker 7, [19/64]: Training Loss: 2.818997062, Training Accuracy: 27.568\n",
            "Worker 7, [20/64]: Training Loss: 2.798056235, Training Accuracy: 27.808\n",
            "Worker 7, [21/64]: Training Loss: 2.698212225, Training Accuracy: 29.376\n",
            "Worker 7, [22/64]: Training Loss: 2.683153415, Training Accuracy: 30.384\n",
            "Worker 7, [23/64]: Training Loss: 2.584798175, Training Accuracy: 32.032\n",
            "Worker 7, [24/64]: Training Loss: 2.556330511, Training Accuracy: 32.528\n",
            "Worker 7, [25/64]: Training Loss: 2.461467654, Training Accuracy: 34.336\n",
            "Worker 7, [26/64]: Training Loss: 2.437257974, Training Accuracy: 35.248\n",
            "Worker 7, [27/64]: Training Loss: 2.373756083, Training Accuracy: 36.208\n",
            "Worker 7, [28/64]: Training Loss: 2.301182457, Training Accuracy: 38.480\n",
            "Worker 7, [29/64]: Training Loss: 2.252434962, Training Accuracy: 38.624\n",
            "Worker 7, [30/64]: Training Loss: 2.211564097, Training Accuracy: 39.680\n",
            "Worker 7, [31/64]: Training Loss: 2.194133496, Training Accuracy: 39.392\n",
            "Worker 7, [32/64]: Training Loss: 2.159737126, Training Accuracy: 40.800\n",
            "Worker 7, [33/64]: Training Loss: 2.078753317, Training Accuracy: 43.296\n",
            "Worker 7, [34/64]: Training Loss: 2.009130001, Training Accuracy: 44.448\n",
            "Worker 7, [35/64]: Training Loss: 1.936383087, Training Accuracy: 45.952\n",
            "Worker 7, [36/64]: Training Loss: 1.905786559, Training Accuracy: 46.768\n",
            "Worker 7, [37/64]: Training Loss: 1.857721284, Training Accuracy: 47.840\n",
            "Worker 7, [38/64]: Training Loss: 1.825512999, Training Accuracy: 48.256\n",
            "Worker 7, [39/64]: Training Loss: 1.749366768, Training Accuracy: 50.304\n",
            "Worker 7, [40/64]: Training Loss: 1.748984271, Training Accuracy: 49.824\n",
            "Worker 7, [41/64]: Training Loss: 1.680051566, Training Accuracy: 52.128\n",
            "Worker 7, [42/64]: Training Loss: 1.671503842, Training Accuracy: 52.592\n",
            "Worker 7, [43/64]: Training Loss: 1.589407976, Training Accuracy: 54.752\n",
            "Worker 7, [44/64]: Training Loss: 1.542920693, Training Accuracy: 55.872\n",
            "Worker 7, [45/64]: Training Loss: 1.562139661, Training Accuracy: 54.400\n",
            "Worker 7, [46/64]: Training Loss: 1.511133508, Training Accuracy: 57.008\n",
            "Worker 7, [47/64]: Training Loss: 1.450437493, Training Accuracy: 58.128\n",
            "Worker 7, [48/64]: Training Loss: 1.425857263, Training Accuracy: 58.832\n",
            "Worker 7, [49/64]: Training Loss: 1.431738542, Training Accuracy: 59.136\n",
            "Worker 7, [50/64]: Training Loss: 1.379558450, Training Accuracy: 60.160\n",
            "Worker 7, [51/64]: Training Loss: 1.356987677, Training Accuracy: 60.704\n",
            "Worker 7, [52/64]: Training Loss: 1.322900726, Training Accuracy: 62.128\n",
            "Worker 7, [53/64]: Training Loss: 1.308096459, Training Accuracy: 61.632\n",
            "Worker 7, [54/64]: Training Loss: 1.279393251, Training Accuracy: 62.560\n",
            "Worker 7, [55/64]: Training Loss: 1.264725087, Training Accuracy: 63.136\n",
            "Worker 7, [56/64]: Training Loss: 1.220798578, Training Accuracy: 64.816\n",
            "Worker 7, [57/64]: Training Loss: 1.207296483, Training Accuracy: 64.720\n",
            "Worker 7, [58/64]: Training Loss: 1.147743623, Training Accuracy: 66.768\n",
            "Worker 7, [59/64]: Training Loss: 1.154932657, Training Accuracy: 66.160\n",
            "Worker 7, [60/64]: Training Loss: 1.136254829, Training Accuracy: 66.096\n",
            "Worker 7, [61/64]: Training Loss: 1.129463733, Training Accuracy: 66.288\n",
            "Worker 7, [62/64]: Training Loss: 1.039889365, Training Accuracy: 69.600\n",
            "Worker 7, [63/64]: Training Loss: 1.093271565, Training Accuracy: 67.760\n",
            "Worker 7, [64/64]: Training Loss: 1.016580739, Training Accuracy: 69.840\n",
            "Time taken for training worker 7: 0:03:18.169038\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/64]: Training Loss: 4.587044249, Training Accuracy: 1.968\n",
            "Worker 8, [02/64]: Training Loss: 4.363392144, Training Accuracy: 4.208\n",
            "Worker 8, [03/64]: Training Loss: 4.141953485, Training Accuracy: 5.920\n",
            "Worker 8, [04/64]: Training Loss: 4.012510261, Training Accuracy: 8.320\n",
            "Worker 8, [05/64]: Training Loss: 3.900073027, Training Accuracy: 9.008\n",
            "Worker 8, [06/64]: Training Loss: 3.767834929, Training Accuracy: 11.616\n",
            "Worker 8, [07/64]: Training Loss: 3.704887222, Training Accuracy: 12.768\n",
            "Worker 8, [08/64]: Training Loss: 3.603692400, Training Accuracy: 14.544\n",
            "Worker 8, [09/64]: Training Loss: 3.510950249, Training Accuracy: 16.624\n",
            "Worker 8, [10/64]: Training Loss: 3.419152802, Training Accuracy: 16.928\n",
            "Worker 8, [11/64]: Training Loss: 3.357432942, Training Accuracy: 18.240\n",
            "Worker 8, [12/64]: Training Loss: 3.288711589, Training Accuracy: 19.664\n",
            "Worker 8, [13/64]: Training Loss: 3.222413204, Training Accuracy: 20.256\n",
            "Worker 8, [14/64]: Training Loss: 3.160456567, Training Accuracy: 20.960\n",
            "Worker 8, [15/64]: Training Loss: 3.059015223, Training Accuracy: 23.472\n",
            "Worker 8, [16/64]: Training Loss: 3.038674678, Training Accuracy: 23.600\n",
            "Worker 8, [17/64]: Training Loss: 2.927774675, Training Accuracy: 25.280\n",
            "Worker 8, [18/64]: Training Loss: 2.892573729, Training Accuracy: 25.744\n",
            "Worker 8, [19/64]: Training Loss: 2.820491606, Training Accuracy: 27.776\n",
            "Worker 8, [20/64]: Training Loss: 2.769867688, Training Accuracy: 28.976\n",
            "Worker 8, [21/64]: Training Loss: 2.693357526, Training Accuracy: 29.904\n",
            "Worker 8, [22/64]: Training Loss: 2.624974012, Training Accuracy: 30.800\n",
            "Worker 8, [23/64]: Training Loss: 2.589024848, Training Accuracy: 31.872\n",
            "Worker 8, [24/64]: Training Loss: 2.537637501, Training Accuracy: 33.488\n",
            "Worker 8, [25/64]: Training Loss: 2.475653370, Training Accuracy: 34.400\n",
            "Worker 8, [26/64]: Training Loss: 2.433735399, Training Accuracy: 34.640\n",
            "Worker 8, [27/64]: Training Loss: 2.369801001, Training Accuracy: 36.816\n",
            "Worker 8, [28/64]: Training Loss: 2.313637168, Training Accuracy: 37.408\n",
            "Worker 8, [29/64]: Training Loss: 2.234251745, Training Accuracy: 39.408\n",
            "Worker 8, [30/64]: Training Loss: 2.210806241, Training Accuracy: 40.032\n",
            "Worker 8, [31/64]: Training Loss: 2.124631031, Training Accuracy: 41.936\n",
            "Worker 8, [32/64]: Training Loss: 2.108048715, Training Accuracy: 42.176\n",
            "Worker 8, [33/64]: Training Loss: 2.057489275, Training Accuracy: 43.088\n",
            "Worker 8, [34/64]: Training Loss: 2.007358327, Training Accuracy: 44.336\n",
            "Worker 8, [35/64]: Training Loss: 1.959114937, Training Accuracy: 46.032\n",
            "Worker 8, [36/64]: Training Loss: 1.895688169, Training Accuracy: 47.232\n",
            "Worker 8, [37/64]: Training Loss: 1.891509409, Training Accuracy: 46.928\n",
            "Worker 8, [38/64]: Training Loss: 1.838216473, Training Accuracy: 48.112\n",
            "Worker 8, [39/64]: Training Loss: 1.752671665, Training Accuracy: 50.160\n",
            "Worker 8, [40/64]: Training Loss: 1.754760820, Training Accuracy: 50.240\n",
            "Worker 8, [41/64]: Training Loss: 1.695467432, Training Accuracy: 52.368\n",
            "Worker 8, [42/64]: Training Loss: 1.633921311, Training Accuracy: 53.184\n",
            "Worker 8, [43/64]: Training Loss: 1.628415939, Training Accuracy: 54.272\n",
            "Worker 8, [44/64]: Training Loss: 1.651080483, Training Accuracy: 52.816\n",
            "Worker 8, [45/64]: Training Loss: 1.521766927, Training Accuracy: 56.048\n",
            "Worker 8, [46/64]: Training Loss: 1.563571383, Training Accuracy: 55.184\n",
            "Worker 8, [47/64]: Training Loss: 1.516761998, Training Accuracy: 56.928\n",
            "Worker 8, [48/64]: Training Loss: 1.417951882, Training Accuracy: 58.640\n",
            "Worker 8, [49/64]: Training Loss: 1.414994016, Training Accuracy: 58.208\n",
            "Worker 8, [50/64]: Training Loss: 1.384218657, Training Accuracy: 60.256\n",
            "Worker 8, [51/64]: Training Loss: 1.386967750, Training Accuracy: 60.256\n",
            "Worker 8, [52/64]: Training Loss: 1.304482676, Training Accuracy: 61.632\n",
            "Worker 8, [53/64]: Training Loss: 1.302242489, Training Accuracy: 62.224\n",
            "Worker 8, [54/64]: Training Loss: 1.252899886, Training Accuracy: 63.584\n",
            "Worker 8, [55/64]: Training Loss: 1.222870581, Training Accuracy: 64.688\n",
            "Worker 8, [56/64]: Training Loss: 1.265049995, Training Accuracy: 63.344\n",
            "Worker 8, [57/64]: Training Loss: 1.212102227, Training Accuracy: 64.416\n",
            "Worker 8, [58/64]: Training Loss: 1.232807527, Training Accuracy: 63.632\n",
            "Worker 8, [59/64]: Training Loss: 1.199775679, Training Accuracy: 65.648\n",
            "Worker 8, [60/64]: Training Loss: 1.147722939, Training Accuracy: 65.792\n",
            "Worker 8, [61/64]: Training Loss: 1.112894786, Training Accuracy: 67.296\n",
            "Worker 8, [62/64]: Training Loss: 1.126057738, Training Accuracy: 67.168\n",
            "Worker 8, [63/64]: Training Loss: 1.063500862, Training Accuracy: 68.592\n",
            "Worker 8, [64/64]: Training Loss: 1.020285813, Training Accuracy: 70.208\n",
            "Time taken for training worker 8: 0:03:18.924705\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004226\n",
            "Global Update 01: Test Loss: 3.836596018, Test Accuracy: 19.100\n",
            "**************************************************\n",
            "Worker 1, [01/64]: Training Loss: 3.305642826, Training Accuracy: 20.576\n",
            "Worker 1, [02/64]: Training Loss: 2.881924804, Training Accuracy: 26.160\n",
            "Worker 1, [03/64]: Training Loss: 2.697428112, Training Accuracy: 30.656\n",
            "Worker 1, [04/64]: Training Loss: 2.555239755, Training Accuracy: 33.152\n",
            "Worker 1, [05/64]: Training Loss: 2.443490079, Training Accuracy: 35.152\n",
            "Worker 1, [06/64]: Training Loss: 2.335106982, Training Accuracy: 37.600\n",
            "Worker 1, [07/64]: Training Loss: 2.243877400, Training Accuracy: 40.176\n",
            "Worker 1, [08/64]: Training Loss: 2.151498477, Training Accuracy: 40.912\n",
            "Worker 1, [09/64]: Training Loss: 2.067275404, Training Accuracy: 43.568\n",
            "Worker 1, [10/64]: Training Loss: 1.979026339, Training Accuracy: 45.392\n",
            "Worker 1, [11/64]: Training Loss: 1.899994614, Training Accuracy: 47.184\n",
            "Worker 1, [12/64]: Training Loss: 1.842830311, Training Accuracy: 47.712\n",
            "Worker 1, [13/64]: Training Loss: 1.790506778, Training Accuracy: 49.056\n",
            "Worker 1, [14/64]: Training Loss: 1.704691987, Training Accuracy: 50.800\n",
            "Worker 1, [15/64]: Training Loss: 1.647929621, Training Accuracy: 53.376\n",
            "Worker 1, [16/64]: Training Loss: 1.572217218, Training Accuracy: 55.136\n",
            "Worker 1, [17/64]: Training Loss: 1.523645346, Training Accuracy: 56.608\n",
            "Worker 1, [18/64]: Training Loss: 1.449349020, Training Accuracy: 58.848\n",
            "Worker 1, [19/64]: Training Loss: 1.401121843, Training Accuracy: 59.376\n",
            "Worker 1, [20/64]: Training Loss: 1.380705995, Training Accuracy: 59.632\n",
            "Worker 1, [21/64]: Training Loss: 1.306610625, Training Accuracy: 61.408\n",
            "Worker 1, [22/64]: Training Loss: 1.239383713, Training Accuracy: 63.648\n",
            "Worker 1, [23/64]: Training Loss: 1.216994644, Training Accuracy: 64.368\n",
            "Worker 1, [24/64]: Training Loss: 1.184707876, Training Accuracy: 64.560\n",
            "Worker 1, [25/64]: Training Loss: 1.131472859, Training Accuracy: 67.280\n",
            "Worker 1, [26/64]: Training Loss: 1.117860882, Training Accuracy: 66.720\n",
            "Worker 1, [27/64]: Training Loss: 1.040453624, Training Accuracy: 69.024\n",
            "Worker 1, [28/64]: Training Loss: 0.999597588, Training Accuracy: 69.200\n",
            "Worker 1, [29/64]: Training Loss: 0.986106117, Training Accuracy: 70.608\n",
            "Worker 1, [30/64]: Training Loss: 0.914659430, Training Accuracy: 72.400\n",
            "Worker 1, [31/64]: Training Loss: 0.914169249, Training Accuracy: 72.800\n",
            "Worker 1, [32/64]: Training Loss: 0.878310756, Training Accuracy: 73.392\n",
            "Worker 1, [33/64]: Training Loss: 0.873626051, Training Accuracy: 72.768\n",
            "Worker 1, [34/64]: Training Loss: 0.831297940, Training Accuracy: 75.024\n",
            "Worker 1, [35/64]: Training Loss: 0.839604066, Training Accuracy: 74.224\n",
            "Worker 1, [36/64]: Training Loss: 0.765849429, Training Accuracy: 76.448\n",
            "Worker 1, [37/64]: Training Loss: 0.703629517, Training Accuracy: 78.384\n",
            "Worker 1, [38/64]: Training Loss: 0.734868777, Training Accuracy: 77.776\n",
            "Worker 1, [39/64]: Training Loss: 0.726143328, Training Accuracy: 77.536\n",
            "Worker 1, [40/64]: Training Loss: 0.700287348, Training Accuracy: 78.176\n",
            "Worker 1, [41/64]: Training Loss: 0.705580839, Training Accuracy: 78.560\n",
            "Worker 1, [42/64]: Training Loss: 0.627291969, Training Accuracy: 81.312\n",
            "Worker 1, [43/64]: Training Loss: 0.620539334, Training Accuracy: 80.400\n",
            "Worker 1, [44/64]: Training Loss: 0.633089105, Training Accuracy: 80.512\n",
            "Worker 1, [45/64]: Training Loss: 0.574544781, Training Accuracy: 82.432\n",
            "Worker 1, [46/64]: Training Loss: 0.571964496, Training Accuracy: 82.416\n",
            "Worker 1, [47/64]: Training Loss: 0.560279216, Training Accuracy: 82.640\n",
            "Worker 1, [48/64]: Training Loss: 0.517702292, Training Accuracy: 84.016\n",
            "Worker 1, [49/64]: Training Loss: 0.551570519, Training Accuracy: 83.120\n",
            "Worker 1, [50/64]: Training Loss: 0.516164173, Training Accuracy: 84.608\n",
            "Worker 1, [51/64]: Training Loss: 0.489402795, Training Accuracy: 85.232\n",
            "Worker 1, [52/64]: Training Loss: 0.494331447, Training Accuracy: 84.576\n",
            "Worker 1, [53/64]: Training Loss: 0.519626625, Training Accuracy: 83.872\n",
            "Worker 1, [54/64]: Training Loss: 0.467500807, Training Accuracy: 85.600\n",
            "Worker 1, [55/64]: Training Loss: 0.467982896, Training Accuracy: 85.536\n",
            "Worker 1, [56/64]: Training Loss: 0.468859079, Training Accuracy: 85.472\n",
            "Worker 1, [57/64]: Training Loss: 0.469283714, Training Accuracy: 85.616\n",
            "Worker 1, [58/64]: Training Loss: 0.453064713, Training Accuracy: 86.032\n",
            "Worker 1, [59/64]: Training Loss: 0.418000182, Training Accuracy: 87.136\n",
            "Worker 1, [60/64]: Training Loss: 0.435431374, Training Accuracy: 86.592\n",
            "Worker 1, [61/64]: Training Loss: 0.393713296, Training Accuracy: 87.552\n",
            "Worker 1, [62/64]: Training Loss: 0.396895935, Training Accuracy: 87.792\n",
            "Worker 1, [63/64]: Training Loss: 0.399548321, Training Accuracy: 87.936\n",
            "Worker 1, [64/64]: Training Loss: 0.363265640, Training Accuracy: 88.752\n",
            "Time taken for training worker 1: 0:03:17.556156\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/64]: Training Loss: 3.275816141, Training Accuracy: 21.552\n",
            "Worker 2, [02/64]: Training Loss: 2.823697450, Training Accuracy: 27.904\n",
            "Worker 2, [03/64]: Training Loss: 2.627116600, Training Accuracy: 32.640\n",
            "Worker 2, [04/64]: Training Loss: 2.498812239, Training Accuracy: 34.864\n",
            "Worker 2, [05/64]: Training Loss: 2.386186904, Training Accuracy: 36.384\n",
            "Worker 2, [06/64]: Training Loss: 2.296910198, Training Accuracy: 39.248\n",
            "Worker 2, [07/64]: Training Loss: 2.195467678, Training Accuracy: 40.784\n",
            "Worker 2, [08/64]: Training Loss: 2.128330620, Training Accuracy: 42.384\n",
            "Worker 2, [09/64]: Training Loss: 2.049133703, Training Accuracy: 44.240\n",
            "Worker 2, [10/64]: Training Loss: 1.958260882, Training Accuracy: 45.696\n",
            "Worker 2, [11/64]: Training Loss: 1.913644682, Training Accuracy: 46.848\n",
            "Worker 2, [12/64]: Training Loss: 1.835796630, Training Accuracy: 48.512\n",
            "Worker 2, [13/64]: Training Loss: 1.772762679, Training Accuracy: 49.536\n",
            "Worker 2, [14/64]: Training Loss: 1.719847149, Training Accuracy: 50.560\n",
            "Worker 2, [15/64]: Training Loss: 1.627912032, Training Accuracy: 53.472\n",
            "Worker 2, [16/64]: Training Loss: 1.592974759, Training Accuracy: 53.936\n",
            "Worker 2, [17/64]: Training Loss: 1.535906013, Training Accuracy: 56.320\n",
            "Worker 2, [18/64]: Training Loss: 1.510752815, Training Accuracy: 56.800\n",
            "Worker 2, [19/64]: Training Loss: 1.414875186, Training Accuracy: 59.328\n",
            "Worker 2, [20/64]: Training Loss: 1.339309317, Training Accuracy: 60.464\n",
            "Worker 2, [21/64]: Training Loss: 1.310599564, Training Accuracy: 61.696\n",
            "Worker 2, [22/64]: Training Loss: 1.287050886, Training Accuracy: 62.336\n",
            "Worker 2, [23/64]: Training Loss: 1.245022392, Training Accuracy: 63.136\n",
            "Worker 2, [24/64]: Training Loss: 1.177725429, Training Accuracy: 64.800\n",
            "Worker 2, [25/64]: Training Loss: 1.142134170, Training Accuracy: 65.344\n",
            "Worker 2, [26/64]: Training Loss: 1.106409057, Training Accuracy: 67.024\n",
            "Worker 2, [27/64]: Training Loss: 1.055209457, Training Accuracy: 68.752\n",
            "Worker 2, [28/64]: Training Loss: 1.005220253, Training Accuracy: 69.408\n",
            "Worker 2, [29/64]: Training Loss: 0.964822824, Training Accuracy: 70.560\n",
            "Worker 2, [30/64]: Training Loss: 0.919292566, Training Accuracy: 72.432\n",
            "Worker 2, [31/64]: Training Loss: 0.921901233, Training Accuracy: 72.352\n",
            "Worker 2, [32/64]: Training Loss: 0.915302464, Training Accuracy: 72.096\n",
            "Worker 2, [33/64]: Training Loss: 0.871971721, Training Accuracy: 74.128\n",
            "Worker 2, [34/64]: Training Loss: 0.815396922, Training Accuracy: 75.456\n",
            "Worker 2, [35/64]: Training Loss: 0.830214730, Training Accuracy: 75.024\n",
            "Worker 2, [36/64]: Training Loss: 0.762129162, Training Accuracy: 76.864\n",
            "Worker 2, [37/64]: Training Loss: 0.732767822, Training Accuracy: 77.808\n",
            "Worker 2, [38/64]: Training Loss: 0.716632902, Training Accuracy: 78.384\n",
            "Worker 2, [39/64]: Training Loss: 0.691736545, Training Accuracy: 79.184\n",
            "Worker 2, [40/64]: Training Loss: 0.702980991, Training Accuracy: 78.464\n",
            "Worker 2, [41/64]: Training Loss: 0.646110275, Training Accuracy: 80.608\n",
            "Worker 2, [42/64]: Training Loss: 0.675809019, Training Accuracy: 79.600\n",
            "Worker 2, [43/64]: Training Loss: 0.627067020, Training Accuracy: 80.928\n",
            "Worker 2, [44/64]: Training Loss: 0.638349859, Training Accuracy: 80.192\n",
            "Worker 2, [45/64]: Training Loss: 0.587570856, Training Accuracy: 81.872\n",
            "Worker 2, [46/64]: Training Loss: 0.569401918, Training Accuracy: 81.920\n",
            "Worker 2, [47/64]: Training Loss: 0.561827813, Training Accuracy: 83.104\n",
            "Worker 2, [48/64]: Training Loss: 0.566534807, Training Accuracy: 82.256\n",
            "Worker 2, [49/64]: Training Loss: 0.532392363, Training Accuracy: 83.840\n",
            "Worker 2, [50/64]: Training Loss: 0.527627825, Training Accuracy: 83.760\n",
            "Worker 2, [51/64]: Training Loss: 0.552943145, Training Accuracy: 83.200\n",
            "Worker 2, [52/64]: Training Loss: 0.521073836, Training Accuracy: 83.744\n",
            "Worker 2, [53/64]: Training Loss: 0.472502877, Training Accuracy: 85.568\n",
            "Worker 2, [54/64]: Training Loss: 0.484566837, Training Accuracy: 84.656\n",
            "Worker 2, [55/64]: Training Loss: 0.466512527, Training Accuracy: 85.088\n",
            "Worker 2, [56/64]: Training Loss: 0.490607768, Training Accuracy: 84.944\n",
            "Worker 2, [57/64]: Training Loss: 0.437543062, Training Accuracy: 86.336\n",
            "Worker 2, [58/64]: Training Loss: 0.444950585, Training Accuracy: 86.368\n",
            "Worker 2, [59/64]: Training Loss: 0.405838877, Training Accuracy: 87.600\n",
            "Worker 2, [60/64]: Training Loss: 0.379313925, Training Accuracy: 88.512\n",
            "Worker 2, [61/64]: Training Loss: 0.423629428, Training Accuracy: 87.136\n",
            "Worker 2, [62/64]: Training Loss: 0.418131272, Training Accuracy: 87.200\n",
            "Worker 2, [63/64]: Training Loss: 0.399994113, Training Accuracy: 87.856\n",
            "Worker 2, [64/64]: Training Loss: 0.416139651, Training Accuracy: 87.408\n",
            "Time taken for training worker 2: 0:03:17.211879\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/64]: Training Loss: 3.262026505, Training Accuracy: 21.152\n",
            "Worker 3, [02/64]: Training Loss: 2.845895456, Training Accuracy: 28.064\n",
            "Worker 3, [03/64]: Training Loss: 2.694068721, Training Accuracy: 31.152\n",
            "Worker 3, [04/64]: Training Loss: 2.537988693, Training Accuracy: 33.984\n",
            "Worker 3, [05/64]: Training Loss: 2.411714915, Training Accuracy: 36.256\n",
            "Worker 3, [06/64]: Training Loss: 2.344630446, Training Accuracy: 37.760\n",
            "Worker 3, [07/64]: Training Loss: 2.225023237, Training Accuracy: 39.904\n",
            "Worker 3, [08/64]: Training Loss: 2.146081795, Training Accuracy: 42.032\n",
            "Worker 3, [09/64]: Training Loss: 2.039509867, Training Accuracy: 44.048\n",
            "Worker 3, [10/64]: Training Loss: 1.989549347, Training Accuracy: 44.480\n",
            "Worker 3, [11/64]: Training Loss: 1.911014486, Training Accuracy: 46.896\n",
            "Worker 3, [12/64]: Training Loss: 1.822408891, Training Accuracy: 48.816\n",
            "Worker 3, [13/64]: Training Loss: 1.774508238, Training Accuracy: 50.368\n",
            "Worker 3, [14/64]: Training Loss: 1.718957440, Training Accuracy: 51.856\n",
            "Worker 3, [15/64]: Training Loss: 1.655790196, Training Accuracy: 53.136\n",
            "Worker 3, [16/64]: Training Loss: 1.582417935, Training Accuracy: 55.360\n",
            "Worker 3, [17/64]: Training Loss: 1.526334264, Training Accuracy: 56.448\n",
            "Worker 3, [18/64]: Training Loss: 1.483814691, Training Accuracy: 57.712\n",
            "Worker 3, [19/64]: Training Loss: 1.424031567, Training Accuracy: 58.752\n",
            "Worker 3, [20/64]: Training Loss: 1.367274908, Training Accuracy: 60.432\n",
            "Worker 3, [21/64]: Training Loss: 1.284992265, Training Accuracy: 62.016\n",
            "Worker 3, [22/64]: Training Loss: 1.235046270, Training Accuracy: 64.656\n",
            "Worker 3, [23/64]: Training Loss: 1.231695774, Training Accuracy: 64.304\n",
            "Worker 3, [24/64]: Training Loss: 1.199517481, Training Accuracy: 64.464\n",
            "Worker 3, [25/64]: Training Loss: 1.150976450, Training Accuracy: 66.128\n",
            "Worker 3, [26/64]: Training Loss: 1.094835876, Training Accuracy: 67.888\n",
            "Worker 3, [27/64]: Training Loss: 1.078526372, Training Accuracy: 67.904\n",
            "Worker 3, [28/64]: Training Loss: 1.007599430, Training Accuracy: 70.288\n",
            "Worker 3, [29/64]: Training Loss: 0.957970112, Training Accuracy: 71.664\n",
            "Worker 3, [30/64]: Training Loss: 0.944343280, Training Accuracy: 71.872\n",
            "Worker 3, [31/64]: Training Loss: 0.906141314, Training Accuracy: 72.768\n",
            "Worker 3, [32/64]: Training Loss: 0.898377384, Training Accuracy: 73.392\n",
            "Worker 3, [33/64]: Training Loss: 0.841779679, Training Accuracy: 74.832\n",
            "Worker 3, [34/64]: Training Loss: 0.879640889, Training Accuracy: 73.808\n",
            "Worker 3, [35/64]: Training Loss: 0.772032358, Training Accuracy: 76.432\n",
            "Worker 3, [36/64]: Training Loss: 0.767836026, Training Accuracy: 76.704\n",
            "Worker 3, [37/64]: Training Loss: 0.767702158, Training Accuracy: 77.072\n",
            "Worker 3, [38/64]: Training Loss: 0.723279421, Training Accuracy: 78.464\n",
            "Worker 3, [39/64]: Training Loss: 0.716705476, Training Accuracy: 78.768\n",
            "Worker 3, [40/64]: Training Loss: 0.693831435, Training Accuracy: 78.944\n",
            "Worker 3, [41/64]: Training Loss: 0.689720381, Training Accuracy: 79.232\n",
            "Worker 3, [42/64]: Training Loss: 0.649919309, Training Accuracy: 80.384\n",
            "Worker 3, [43/64]: Training Loss: 0.639056919, Training Accuracy: 80.336\n",
            "Worker 3, [44/64]: Training Loss: 0.619198461, Training Accuracy: 81.136\n",
            "Worker 3, [45/64]: Training Loss: 0.576147135, Training Accuracy: 82.096\n",
            "Worker 3, [46/64]: Training Loss: 0.557777033, Training Accuracy: 83.440\n",
            "Worker 3, [47/64]: Training Loss: 0.552691846, Training Accuracy: 83.520\n",
            "Worker 3, [48/64]: Training Loss: 0.553253285, Training Accuracy: 82.720\n",
            "Worker 3, [49/64]: Training Loss: 0.547964550, Training Accuracy: 83.696\n",
            "Worker 3, [50/64]: Training Loss: 0.527956312, Training Accuracy: 83.840\n",
            "Worker 3, [51/64]: Training Loss: 0.529260417, Training Accuracy: 83.904\n",
            "Worker 3, [52/64]: Training Loss: 0.483870894, Training Accuracy: 85.504\n",
            "Worker 3, [53/64]: Training Loss: 0.493222501, Training Accuracy: 84.752\n",
            "Worker 3, [54/64]: Training Loss: 0.474280444, Training Accuracy: 85.056\n",
            "Worker 3, [55/64]: Training Loss: 0.466764621, Training Accuracy: 86.592\n",
            "Worker 3, [56/64]: Training Loss: 0.446619375, Training Accuracy: 85.888\n",
            "Worker 3, [57/64]: Training Loss: 0.441841908, Training Accuracy: 86.960\n",
            "Worker 3, [58/64]: Training Loss: 0.421252388, Training Accuracy: 86.512\n",
            "Worker 3, [59/64]: Training Loss: 0.421483764, Training Accuracy: 86.864\n",
            "Worker 3, [60/64]: Training Loss: 0.431315030, Training Accuracy: 86.672\n",
            "Worker 3, [61/64]: Training Loss: 0.414928805, Training Accuracy: 87.248\n",
            "Worker 3, [62/64]: Training Loss: 0.417246831, Training Accuracy: 87.408\n",
            "Worker 3, [63/64]: Training Loss: 0.371232222, Training Accuracy: 88.544\n",
            "Worker 3, [64/64]: Training Loss: 0.380857670, Training Accuracy: 87.504\n",
            "Time taken for training worker 3: 0:03:20.215741\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/64]: Training Loss: 3.284005241, Training Accuracy: 20.848\n",
            "Worker 4, [02/64]: Training Loss: 2.842844652, Training Accuracy: 27.952\n",
            "Worker 4, [03/64]: Training Loss: 2.678026204, Training Accuracy: 30.800\n",
            "Worker 4, [04/64]: Training Loss: 2.541227774, Training Accuracy: 33.984\n",
            "Worker 4, [05/64]: Training Loss: 2.425428313, Training Accuracy: 35.824\n",
            "Worker 4, [06/64]: Training Loss: 2.323267751, Training Accuracy: 38.112\n",
            "Worker 4, [07/64]: Training Loss: 2.226604960, Training Accuracy: 40.016\n",
            "Worker 4, [08/64]: Training Loss: 2.125887910, Training Accuracy: 42.464\n",
            "Worker 4, [09/64]: Training Loss: 2.070253259, Training Accuracy: 43.952\n",
            "Worker 4, [10/64]: Training Loss: 1.996371289, Training Accuracy: 45.696\n",
            "Worker 4, [11/64]: Training Loss: 1.930363015, Training Accuracy: 46.128\n",
            "Worker 4, [12/64]: Training Loss: 1.832637362, Training Accuracy: 48.720\n",
            "Worker 4, [13/64]: Training Loss: 1.769153698, Training Accuracy: 50.576\n",
            "Worker 4, [14/64]: Training Loss: 1.688610452, Training Accuracy: 52.656\n",
            "Worker 4, [15/64]: Training Loss: 1.604089389, Training Accuracy: 54.432\n",
            "Worker 4, [16/64]: Training Loss: 1.587629234, Training Accuracy: 54.128\n",
            "Worker 4, [17/64]: Training Loss: 1.509261561, Training Accuracy: 57.520\n",
            "Worker 4, [18/64]: Training Loss: 1.461170390, Training Accuracy: 57.536\n",
            "Worker 4, [19/64]: Training Loss: 1.424652188, Training Accuracy: 58.864\n",
            "Worker 4, [20/64]: Training Loss: 1.367143003, Training Accuracy: 60.080\n",
            "Worker 4, [21/64]: Training Loss: 1.321189879, Training Accuracy: 61.472\n",
            "Worker 4, [22/64]: Training Loss: 1.251709363, Training Accuracy: 63.584\n",
            "Worker 4, [23/64]: Training Loss: 1.203314085, Training Accuracy: 64.496\n",
            "Worker 4, [24/64]: Training Loss: 1.192414495, Training Accuracy: 65.040\n",
            "Worker 4, [25/64]: Training Loss: 1.146571594, Training Accuracy: 66.752\n",
            "Worker 4, [26/64]: Training Loss: 1.096881654, Training Accuracy: 67.360\n",
            "Worker 4, [27/64]: Training Loss: 1.042384610, Training Accuracy: 69.216\n",
            "Worker 4, [28/64]: Training Loss: 1.049210028, Training Accuracy: 68.976\n",
            "Worker 4, [29/64]: Training Loss: 1.001864197, Training Accuracy: 70.544\n",
            "Worker 4, [30/64]: Training Loss: 0.985977987, Training Accuracy: 69.904\n",
            "Worker 4, [31/64]: Training Loss: 0.950538523, Training Accuracy: 71.312\n",
            "Worker 4, [32/64]: Training Loss: 0.881908507, Training Accuracy: 73.232\n",
            "Worker 4, [33/64]: Training Loss: 0.876466638, Training Accuracy: 73.664\n",
            "Worker 4, [34/64]: Training Loss: 0.820291442, Training Accuracy: 75.248\n",
            "Worker 4, [35/64]: Training Loss: 0.815849690, Training Accuracy: 75.552\n",
            "Worker 4, [36/64]: Training Loss: 0.828327015, Training Accuracy: 75.456\n",
            "Worker 4, [37/64]: Training Loss: 0.794498125, Training Accuracy: 75.824\n",
            "Worker 4, [38/64]: Training Loss: 0.733839567, Training Accuracy: 77.488\n",
            "Worker 4, [39/64]: Training Loss: 0.710011462, Training Accuracy: 78.272\n",
            "Worker 4, [40/64]: Training Loss: 0.713355908, Training Accuracy: 78.272\n",
            "Worker 4, [41/64]: Training Loss: 0.670616158, Training Accuracy: 79.504\n",
            "Worker 4, [42/64]: Training Loss: 0.686420833, Training Accuracy: 79.024\n",
            "Worker 4, [43/64]: Training Loss: 0.708006186, Training Accuracy: 78.544\n",
            "Worker 4, [44/64]: Training Loss: 0.644867712, Training Accuracy: 80.672\n",
            "Worker 4, [45/64]: Training Loss: 0.630413914, Training Accuracy: 80.848\n",
            "Worker 4, [46/64]: Training Loss: 0.566909421, Training Accuracy: 82.032\n",
            "Worker 4, [47/64]: Training Loss: 0.583346956, Training Accuracy: 81.872\n",
            "Worker 4, [48/64]: Training Loss: 0.590686054, Training Accuracy: 82.096\n",
            "Worker 4, [49/64]: Training Loss: 0.549184378, Training Accuracy: 83.184\n",
            "Worker 4, [50/64]: Training Loss: 0.536255469, Training Accuracy: 83.600\n",
            "Worker 4, [51/64]: Training Loss: 0.535238947, Training Accuracy: 83.872\n",
            "Worker 4, [52/64]: Training Loss: 0.499066065, Training Accuracy: 84.896\n",
            "Worker 4, [53/64]: Training Loss: 0.526825046, Training Accuracy: 84.416\n",
            "Worker 4, [54/64]: Training Loss: 0.494974857, Training Accuracy: 84.880\n",
            "Worker 4, [55/64]: Training Loss: 0.472802421, Training Accuracy: 85.328\n",
            "Worker 4, [56/64]: Training Loss: 0.467817557, Training Accuracy: 85.984\n",
            "Worker 4, [57/64]: Training Loss: 0.485796794, Training Accuracy: 84.992\n",
            "Worker 4, [58/64]: Training Loss: 0.503441396, Training Accuracy: 84.848\n",
            "Worker 4, [59/64]: Training Loss: 0.443441805, Training Accuracy: 86.080\n",
            "Worker 4, [60/64]: Training Loss: 0.436510847, Training Accuracy: 86.576\n",
            "Worker 4, [61/64]: Training Loss: 0.421486458, Training Accuracy: 86.624\n",
            "Worker 4, [62/64]: Training Loss: 0.398706637, Training Accuracy: 87.536\n",
            "Worker 4, [63/64]: Training Loss: 0.417173296, Training Accuracy: 86.848\n",
            "Worker 4, [64/64]: Training Loss: 0.384786784, Training Accuracy: 88.336\n",
            "Time taken for training worker 4: 0:03:21.553227\n",
            "--------------------------------------------------\n",
            "Worker 5, [01/64]: Training Loss: 3.279641881, Training Accuracy: 21.760\n",
            "Worker 5, [02/64]: Training Loss: 2.835247003, Training Accuracy: 28.176\n",
            "Worker 5, [03/64]: Training Loss: 2.665838327, Training Accuracy: 31.264\n",
            "Worker 5, [04/64]: Training Loss: 2.510538369, Training Accuracy: 34.672\n",
            "Worker 5, [05/64]: Training Loss: 2.410555230, Training Accuracy: 37.136\n",
            "Worker 5, [06/64]: Training Loss: 2.307050665, Training Accuracy: 38.032\n",
            "Worker 5, [07/64]: Training Loss: 2.225345949, Training Accuracy: 40.096\n",
            "Worker 5, [08/64]: Training Loss: 2.140533107, Training Accuracy: 42.688\n",
            "Worker 5, [09/64]: Training Loss: 2.072628999, Training Accuracy: 43.296\n",
            "Worker 5, [10/64]: Training Loss: 1.984142456, Training Accuracy: 44.800\n",
            "Worker 5, [11/64]: Training Loss: 1.909054250, Training Accuracy: 46.672\n",
            "Worker 5, [12/64]: Training Loss: 1.832767623, Training Accuracy: 49.312\n",
            "Worker 5, [13/64]: Training Loss: 1.773551408, Training Accuracy: 50.528\n",
            "Worker 5, [14/64]: Training Loss: 1.703896867, Training Accuracy: 51.888\n",
            "Worker 5, [15/64]: Training Loss: 1.627396969, Training Accuracy: 54.000\n",
            "Worker 5, [16/64]: Training Loss: 1.571836745, Training Accuracy: 55.264\n",
            "Worker 5, [17/64]: Training Loss: 1.516060501, Training Accuracy: 55.632\n",
            "Worker 5, [18/64]: Training Loss: 1.471712583, Training Accuracy: 58.112\n",
            "Worker 5, [19/64]: Training Loss: 1.402148872, Training Accuracy: 59.520\n",
            "Worker 5, [20/64]: Training Loss: 1.344395719, Training Accuracy: 60.800\n",
            "Worker 5, [21/64]: Training Loss: 1.305118558, Training Accuracy: 61.664\n",
            "Worker 5, [22/64]: Training Loss: 1.281343110, Training Accuracy: 62.352\n",
            "Worker 5, [23/64]: Training Loss: 1.206645964, Training Accuracy: 64.736\n",
            "Worker 5, [24/64]: Training Loss: 1.181926573, Training Accuracy: 65.296\n",
            "Worker 5, [25/64]: Training Loss: 1.117042253, Training Accuracy: 67.328\n",
            "Worker 5, [26/64]: Training Loss: 1.083586384, Training Accuracy: 67.520\n",
            "Worker 5, [27/64]: Training Loss: 1.046002010, Training Accuracy: 68.624\n",
            "Worker 5, [28/64]: Training Loss: 1.024600485, Training Accuracy: 69.360\n",
            "Worker 5, [29/64]: Training Loss: 0.977251499, Training Accuracy: 70.720\n",
            "Worker 5, [30/64]: Training Loss: 0.932728173, Training Accuracy: 72.400\n",
            "Worker 5, [31/64]: Training Loss: 0.938256758, Training Accuracy: 72.048\n",
            "Worker 5, [32/64]: Training Loss: 0.882033462, Training Accuracy: 73.472\n",
            "Worker 5, [33/64]: Training Loss: 0.870592803, Training Accuracy: 73.952\n",
            "Worker 5, [34/64]: Training Loss: 0.843562692, Training Accuracy: 74.832\n",
            "Worker 5, [35/64]: Training Loss: 0.811825643, Training Accuracy: 75.856\n",
            "Worker 5, [36/64]: Training Loss: 0.735273486, Training Accuracy: 77.984\n",
            "Worker 5, [37/64]: Training Loss: 0.777359949, Training Accuracy: 76.160\n",
            "Worker 5, [38/64]: Training Loss: 0.717344908, Training Accuracy: 77.888\n",
            "Worker 5, [39/64]: Training Loss: 0.716799286, Training Accuracy: 78.208\n",
            "Worker 5, [40/64]: Training Loss: 0.700019950, Training Accuracy: 79.184\n",
            "Worker 5, [41/64]: Training Loss: 0.703549821, Training Accuracy: 78.464\n",
            "Worker 5, [42/64]: Training Loss: 0.649424261, Training Accuracy: 80.464\n",
            "Worker 5, [43/64]: Training Loss: 0.646537283, Training Accuracy: 80.576\n",
            "Worker 5, [44/64]: Training Loss: 0.609500026, Training Accuracy: 82.096\n",
            "Worker 5, [45/64]: Training Loss: 0.590858464, Training Accuracy: 82.304\n",
            "Worker 5, [46/64]: Training Loss: 0.533581605, Training Accuracy: 83.392\n",
            "Worker 5, [47/64]: Training Loss: 0.588483296, Training Accuracy: 81.760\n",
            "Worker 5, [48/64]: Training Loss: 0.582090012, Training Accuracy: 82.560\n",
            "Worker 5, [49/64]: Training Loss: 0.522916815, Training Accuracy: 84.208\n",
            "Worker 5, [50/64]: Training Loss: 0.560809992, Training Accuracy: 82.816\n",
            "Worker 5, [51/64]: Training Loss: 0.513791420, Training Accuracy: 84.080\n",
            "Worker 5, [52/64]: Training Loss: 0.482430210, Training Accuracy: 85.520\n",
            "Worker 5, [53/64]: Training Loss: 0.488155753, Training Accuracy: 84.800\n",
            "Worker 5, [54/64]: Training Loss: 0.492490176, Training Accuracy: 85.136\n",
            "Worker 5, [55/64]: Training Loss: 0.459300326, Training Accuracy: 85.872\n",
            "Worker 5, [56/64]: Training Loss: 0.457558539, Training Accuracy: 86.096\n",
            "Worker 5, [57/64]: Training Loss: 0.446614799, Training Accuracy: 86.208\n",
            "Worker 5, [58/64]: Training Loss: 0.456519214, Training Accuracy: 86.160\n",
            "Worker 5, [59/64]: Training Loss: 0.453183954, Training Accuracy: 86.592\n",
            "Worker 5, [60/64]: Training Loss: 0.411492507, Training Accuracy: 87.280\n",
            "Worker 5, [61/64]: Training Loss: 0.424471558, Training Accuracy: 87.248\n",
            "Worker 5, [62/64]: Training Loss: 0.420085934, Training Accuracy: 87.296\n",
            "Worker 5, [63/64]: Training Loss: 0.395223094, Training Accuracy: 88.144\n",
            "Worker 5, [64/64]: Training Loss: 0.396975224, Training Accuracy: 87.664\n",
            "Time taken for training worker 5: 0:03:18.004414\n",
            "--------------------------------------------------\n",
            "Worker 6, [01/64]: Training Loss: 3.275011007, Training Accuracy: 21.664\n",
            "Worker 6, [02/64]: Training Loss: 2.799274817, Training Accuracy: 29.056\n",
            "Worker 6, [03/64]: Training Loss: 2.650130355, Training Accuracy: 31.376\n",
            "Worker 6, [04/64]: Training Loss: 2.517358740, Training Accuracy: 34.304\n",
            "Worker 6, [05/64]: Training Loss: 2.391178501, Training Accuracy: 35.888\n",
            "Worker 6, [06/64]: Training Loss: 2.303502891, Training Accuracy: 38.560\n",
            "Worker 6, [07/64]: Training Loss: 2.211163931, Training Accuracy: 40.336\n",
            "Worker 6, [08/64]: Training Loss: 2.139601136, Training Accuracy: 41.920\n",
            "Worker 6, [09/64]: Training Loss: 2.044020372, Training Accuracy: 44.448\n",
            "Worker 6, [10/64]: Training Loss: 1.979690267, Training Accuracy: 45.264\n",
            "Worker 6, [11/64]: Training Loss: 1.933262624, Training Accuracy: 46.320\n",
            "Worker 6, [12/64]: Training Loss: 1.850286419, Training Accuracy: 47.936\n",
            "Worker 6, [13/64]: Training Loss: 1.731666522, Training Accuracy: 51.408\n",
            "Worker 6, [14/64]: Training Loss: 1.733335244, Training Accuracy: 51.360\n",
            "Worker 6, [15/64]: Training Loss: 1.665082676, Training Accuracy: 52.784\n",
            "Worker 6, [16/64]: Training Loss: 1.596696955, Training Accuracy: 54.768\n",
            "Worker 6, [17/64]: Training Loss: 1.531314203, Training Accuracy: 55.792\n",
            "Worker 6, [18/64]: Training Loss: 1.475439310, Training Accuracy: 57.568\n",
            "Worker 6, [19/64]: Training Loss: 1.405096200, Training Accuracy: 59.056\n",
            "Worker 6, [20/64]: Training Loss: 1.361349693, Training Accuracy: 60.304\n",
            "Worker 6, [21/64]: Training Loss: 1.312516050, Training Accuracy: 61.248\n",
            "Worker 6, [22/64]: Training Loss: 1.287589473, Training Accuracy: 62.384\n",
            "Worker 6, [23/64]: Training Loss: 1.246824808, Training Accuracy: 63.440\n",
            "Worker 6, [24/64]: Training Loss: 1.185834368, Training Accuracy: 65.296\n",
            "Worker 6, [25/64]: Training Loss: 1.148767315, Training Accuracy: 65.248\n",
            "Worker 6, [26/64]: Training Loss: 1.085357567, Training Accuracy: 67.744\n",
            "Worker 6, [27/64]: Training Loss: 1.054896432, Training Accuracy: 68.560\n",
            "Worker 6, [28/64]: Training Loss: 1.028680033, Training Accuracy: 68.896\n",
            "Worker 6, [29/64]: Training Loss: 0.992630356, Training Accuracy: 70.128\n",
            "Worker 6, [30/64]: Training Loss: 0.948354941, Training Accuracy: 71.440\n",
            "Worker 6, [31/64]: Training Loss: 0.891991468, Training Accuracy: 73.504\n",
            "Worker 6, [32/64]: Training Loss: 0.884167399, Training Accuracy: 73.088\n",
            "Worker 6, [33/64]: Training Loss: 0.828943768, Training Accuracy: 74.864\n",
            "Worker 6, [34/64]: Training Loss: 0.819652043, Training Accuracy: 74.832\n",
            "Worker 6, [35/64]: Training Loss: 0.818118100, Training Accuracy: 75.216\n",
            "Worker 6, [36/64]: Training Loss: 0.774617681, Training Accuracy: 76.960\n",
            "Worker 6, [37/64]: Training Loss: 0.763853851, Training Accuracy: 77.104\n",
            "Worker 6, [38/64]: Training Loss: 0.709681769, Training Accuracy: 78.672\n",
            "Worker 6, [39/64]: Training Loss: 0.719952324, Training Accuracy: 78.032\n",
            "Worker 6, [40/64]: Training Loss: 0.723932648, Training Accuracy: 77.872\n",
            "Worker 6, [41/64]: Training Loss: 0.659988468, Training Accuracy: 80.112\n",
            "Worker 6, [42/64]: Training Loss: 0.641517526, Training Accuracy: 80.272\n",
            "Worker 6, [43/64]: Training Loss: 0.619034389, Training Accuracy: 81.504\n",
            "Worker 6, [44/64]: Training Loss: 0.607959941, Training Accuracy: 81.360\n",
            "Worker 6, [45/64]: Training Loss: 0.637116599, Training Accuracy: 80.544\n",
            "Worker 6, [46/64]: Training Loss: 0.632702406, Training Accuracy: 80.960\n",
            "Worker 6, [47/64]: Training Loss: 0.582287926, Training Accuracy: 82.400\n",
            "Worker 6, [48/64]: Training Loss: 0.542790467, Training Accuracy: 83.216\n",
            "Worker 6, [49/64]: Training Loss: 0.534387768, Training Accuracy: 83.648\n",
            "Worker 6, [50/64]: Training Loss: 0.509096431, Training Accuracy: 84.192\n",
            "Worker 6, [51/64]: Training Loss: 0.535299203, Training Accuracy: 83.648\n",
            "Worker 6, [52/64]: Training Loss: 0.517916325, Training Accuracy: 84.352\n",
            "Worker 6, [53/64]: Training Loss: 0.478704906, Training Accuracy: 85.136\n",
            "Worker 6, [54/64]: Training Loss: 0.451304581, Training Accuracy: 86.208\n",
            "Worker 6, [55/64]: Training Loss: 0.469843877, Training Accuracy: 85.632\n",
            "Worker 6, [56/64]: Training Loss: 0.477436035, Training Accuracy: 85.584\n",
            "Worker 6, [57/64]: Training Loss: 0.436792015, Training Accuracy: 86.848\n",
            "Worker 6, [58/64]: Training Loss: 0.452880954, Training Accuracy: 86.144\n",
            "Worker 6, [59/64]: Training Loss: 0.443768795, Training Accuracy: 86.448\n",
            "Worker 6, [60/64]: Training Loss: 0.412881221, Training Accuracy: 87.328\n",
            "Worker 6, [61/64]: Training Loss: 0.451908349, Training Accuracy: 86.400\n",
            "Worker 6, [62/64]: Training Loss: 0.394338321, Training Accuracy: 87.904\n",
            "Worker 6, [63/64]: Training Loss: 0.406729102, Training Accuracy: 87.616\n",
            "Worker 6, [64/64]: Training Loss: 0.407207655, Training Accuracy: 87.616\n",
            "Time taken for training worker 6: 0:03:18.011189\n",
            "--------------------------------------------------\n",
            "Worker 7, [01/64]: Training Loss: 3.270383360, Training Accuracy: 20.992\n",
            "Worker 7, [02/64]: Training Loss: 2.849035171, Training Accuracy: 28.256\n",
            "Worker 7, [03/64]: Training Loss: 2.658890948, Training Accuracy: 31.632\n",
            "Worker 7, [04/64]: Training Loss: 2.502149027, Training Accuracy: 34.608\n",
            "Worker 7, [05/64]: Training Loss: 2.382258421, Training Accuracy: 37.600\n",
            "Worker 7, [06/64]: Training Loss: 2.284430528, Training Accuracy: 39.184\n",
            "Worker 7, [07/64]: Training Loss: 2.205545391, Training Accuracy: 41.056\n",
            "Worker 7, [08/64]: Training Loss: 2.124441908, Training Accuracy: 41.792\n",
            "Worker 7, [09/64]: Training Loss: 2.040706264, Training Accuracy: 44.336\n",
            "Worker 7, [10/64]: Training Loss: 1.954768226, Training Accuracy: 45.856\n",
            "Worker 7, [11/64]: Training Loss: 1.904274169, Training Accuracy: 47.280\n",
            "Worker 7, [12/64]: Training Loss: 1.834892183, Training Accuracy: 48.944\n",
            "Worker 7, [13/64]: Training Loss: 1.750645998, Training Accuracy: 51.264\n",
            "Worker 7, [14/64]: Training Loss: 1.691007598, Training Accuracy: 52.192\n",
            "Worker 7, [15/64]: Training Loss: 1.609075444, Training Accuracy: 55.168\n",
            "Worker 7, [16/64]: Training Loss: 1.566475924, Training Accuracy: 55.360\n",
            "Worker 7, [17/64]: Training Loss: 1.511028237, Training Accuracy: 57.040\n",
            "Worker 7, [18/64]: Training Loss: 1.464964474, Training Accuracy: 57.456\n",
            "Worker 7, [19/64]: Training Loss: 1.409833314, Training Accuracy: 59.424\n",
            "Worker 7, [20/64]: Training Loss: 1.338215959, Training Accuracy: 60.688\n",
            "Worker 7, [21/64]: Training Loss: 1.271221197, Training Accuracy: 62.464\n",
            "Worker 7, [22/64]: Training Loss: 1.268995910, Training Accuracy: 62.192\n",
            "Worker 7, [23/64]: Training Loss: 1.219557473, Training Accuracy: 63.872\n",
            "Worker 7, [24/64]: Training Loss: 1.189812087, Training Accuracy: 64.736\n",
            "Worker 7, [25/64]: Training Loss: 1.113850119, Training Accuracy: 66.480\n",
            "Worker 7, [26/64]: Training Loss: 1.068195883, Training Accuracy: 68.368\n",
            "Worker 7, [27/64]: Training Loss: 1.025715939, Training Accuracy: 69.296\n",
            "Worker 7, [28/64]: Training Loss: 1.007825417, Training Accuracy: 69.680\n",
            "Worker 7, [29/64]: Training Loss: 0.947190583, Training Accuracy: 71.024\n",
            "Worker 7, [30/64]: Training Loss: 0.948175969, Training Accuracy: 71.120\n",
            "Worker 7, [31/64]: Training Loss: 0.906740067, Training Accuracy: 72.160\n",
            "Worker 7, [32/64]: Training Loss: 0.864800511, Training Accuracy: 74.576\n",
            "Worker 7, [33/64]: Training Loss: 0.839783194, Training Accuracy: 74.896\n",
            "Worker 7, [34/64]: Training Loss: 0.831160676, Training Accuracy: 74.752\n",
            "Worker 7, [35/64]: Training Loss: 0.794723855, Training Accuracy: 76.064\n",
            "Worker 7, [36/64]: Training Loss: 0.728772140, Training Accuracy: 77.888\n",
            "Worker 7, [37/64]: Training Loss: 0.734735636, Training Accuracy: 78.048\n",
            "Worker 7, [38/64]: Training Loss: 0.731090374, Training Accuracy: 77.840\n",
            "Worker 7, [39/64]: Training Loss: 0.736631743, Training Accuracy: 77.632\n",
            "Worker 7, [40/64]: Training Loss: 0.684856146, Training Accuracy: 79.360\n",
            "Worker 7, [41/64]: Training Loss: 0.674614229, Training Accuracy: 79.072\n",
            "Worker 7, [42/64]: Training Loss: 0.667955316, Training Accuracy: 79.472\n",
            "Worker 7, [43/64]: Training Loss: 0.615068829, Training Accuracy: 81.296\n",
            "Worker 7, [44/64]: Training Loss: 0.595548341, Training Accuracy: 81.936\n",
            "Worker 7, [45/64]: Training Loss: 0.614076811, Training Accuracy: 81.072\n",
            "Worker 7, [46/64]: Training Loss: 0.610455460, Training Accuracy: 81.168\n",
            "Worker 7, [47/64]: Training Loss: 0.564780026, Training Accuracy: 82.656\n",
            "Worker 7, [48/64]: Training Loss: 0.535602400, Training Accuracy: 83.824\n",
            "Worker 7, [49/64]: Training Loss: 0.550563457, Training Accuracy: 82.976\n",
            "Worker 7, [50/64]: Training Loss: 0.542191712, Training Accuracy: 83.808\n",
            "Worker 7, [51/64]: Training Loss: 0.501583369, Training Accuracy: 84.608\n",
            "Worker 7, [52/64]: Training Loss: 0.493205265, Training Accuracy: 84.544\n",
            "Worker 7, [53/64]: Training Loss: 0.500157101, Training Accuracy: 84.512\n",
            "Worker 7, [54/64]: Training Loss: 0.479595816, Training Accuracy: 85.440\n",
            "Worker 7, [55/64]: Training Loss: 0.455440147, Training Accuracy: 85.888\n",
            "Worker 7, [56/64]: Training Loss: 0.473134067, Training Accuracy: 85.136\n",
            "Worker 7, [57/64]: Training Loss: 0.454540882, Training Accuracy: 85.952\n",
            "Worker 7, [58/64]: Training Loss: 0.432162462, Training Accuracy: 86.880\n",
            "Worker 7, [59/64]: Training Loss: 0.433130912, Training Accuracy: 87.024\n",
            "Worker 7, [60/64]: Training Loss: 0.381496731, Training Accuracy: 88.016\n",
            "Worker 7, [61/64]: Training Loss: 0.394124897, Training Accuracy: 87.968\n",
            "Worker 7, [62/64]: Training Loss: 0.376214991, Training Accuracy: 88.592\n",
            "Worker 7, [63/64]: Training Loss: 0.382619347, Training Accuracy: 88.144\n",
            "Worker 7, [64/64]: Training Loss: 0.390418946, Training Accuracy: 88.320\n",
            "Time taken for training worker 7: 0:03:17.836924\n",
            "--------------------------------------------------\n",
            "Worker 8, [01/64]: Training Loss: 3.253129431, Training Accuracy: 22.080\n",
            "Worker 8, [02/64]: Training Loss: 2.828013641, Training Accuracy: 28.416\n",
            "Worker 8, [03/64]: Training Loss: 2.643749405, Training Accuracy: 31.936\n",
            "Worker 8, [04/64]: Training Loss: 2.506994690, Training Accuracy: 34.320\n",
            "Worker 8, [05/64]: Training Loss: 2.393931618, Training Accuracy: 36.720\n",
            "Worker 8, [06/64]: Training Loss: 2.278564514, Training Accuracy: 39.664\n",
            "Worker 8, [07/64]: Training Loss: 2.216857661, Training Accuracy: 40.256\n",
            "Worker 8, [08/64]: Training Loss: 2.115388042, Training Accuracy: 42.832\n",
            "Worker 8, [09/64]: Training Loss: 2.028744885, Training Accuracy: 43.744\n",
            "Worker 8, [10/64]: Training Loss: 1.957975529, Training Accuracy: 46.416\n",
            "Worker 8, [11/64]: Training Loss: 1.899009681, Training Accuracy: 47.488\n",
            "Worker 8, [12/64]: Training Loss: 1.822885000, Training Accuracy: 50.176\n",
            "Worker 8, [13/64]: Training Loss: 1.737207374, Training Accuracy: 51.584\n",
            "Worker 8, [14/64]: Training Loss: 1.701744581, Training Accuracy: 51.952\n",
            "Worker 8, [15/64]: Training Loss: 1.640369471, Training Accuracy: 53.664\n",
            "Worker 8, [16/64]: Training Loss: 1.564174561, Training Accuracy: 55.296\n",
            "Worker 8, [17/64]: Training Loss: 1.521527592, Training Accuracy: 57.008\n",
            "Worker 8, [18/64]: Training Loss: 1.443818493, Training Accuracy: 58.416\n",
            "Worker 8, [19/64]: Training Loss: 1.408136674, Training Accuracy: 59.664\n",
            "Worker 8, [20/64]: Training Loss: 1.347295614, Training Accuracy: 60.880\n",
            "Worker 8, [21/64]: Training Loss: 1.289287197, Training Accuracy: 62.944\n",
            "Worker 8, [22/64]: Training Loss: 1.275944999, Training Accuracy: 63.072\n",
            "Worker 8, [23/64]: Training Loss: 1.187759255, Training Accuracy: 64.928\n",
            "Worker 8, [24/64]: Training Loss: 1.195241562, Training Accuracy: 64.432\n",
            "Worker 8, [25/64]: Training Loss: 1.140133594, Training Accuracy: 66.048\n",
            "Worker 8, [26/64]: Training Loss: 1.100743149, Training Accuracy: 67.168\n",
            "Worker 8, [27/64]: Training Loss: 1.076850821, Training Accuracy: 67.728\n",
            "Worker 8, [28/64]: Training Loss: 1.067879574, Training Accuracy: 69.008\n",
            "Worker 8, [29/64]: Training Loss: 0.968243869, Training Accuracy: 71.328\n",
            "Worker 8, [30/64]: Training Loss: 0.968246939, Training Accuracy: 70.912\n",
            "Worker 8, [31/64]: Training Loss: 0.940242844, Training Accuracy: 71.392\n",
            "Worker 8, [32/64]: Training Loss: 0.877644474, Training Accuracy: 72.816\n",
            "Worker 8, [33/64]: Training Loss: 0.873163496, Training Accuracy: 74.096\n",
            "Worker 8, [34/64]: Training Loss: 0.836670019, Training Accuracy: 74.992\n",
            "Worker 8, [35/64]: Training Loss: 0.841133660, Training Accuracy: 74.592\n",
            "Worker 8, [36/64]: Training Loss: 0.749567898, Training Accuracy: 77.552\n",
            "Worker 8, [37/64]: Training Loss: 0.785916336, Training Accuracy: 76.352\n",
            "Worker 8, [38/64]: Training Loss: 0.744423593, Training Accuracy: 76.880\n",
            "Worker 8, [39/64]: Training Loss: 0.750258560, Training Accuracy: 77.072\n",
            "Worker 8, [40/64]: Training Loss: 0.678764869, Training Accuracy: 79.248\n",
            "Worker 8, [41/64]: Training Loss: 0.685458321, Training Accuracy: 79.520\n",
            "Worker 8, [42/64]: Training Loss: 0.693737753, Training Accuracy: 79.392\n",
            "Worker 8, [43/64]: Training Loss: 0.635074155, Training Accuracy: 81.152\n",
            "Worker 8, [44/64]: Training Loss: 0.614707710, Training Accuracy: 81.024\n",
            "Worker 8, [45/64]: Training Loss: 0.590502310, Training Accuracy: 82.160\n",
            "Worker 8, [46/64]: Training Loss: 0.584411832, Training Accuracy: 82.352\n",
            "Worker 8, [47/64]: Training Loss: 0.575632627, Training Accuracy: 82.400\n",
            "Worker 8, [48/64]: Training Loss: 0.560396444, Training Accuracy: 83.264\n",
            "Worker 8, [49/64]: Training Loss: 0.571573282, Training Accuracy: 82.544\n",
            "Worker 8, [50/64]: Training Loss: 0.543814809, Training Accuracy: 82.928\n",
            "Worker 8, [51/64]: Training Loss: 0.518659509, Training Accuracy: 83.904\n",
            "Worker 8, [52/64]: Training Loss: 0.494417003, Training Accuracy: 85.008\n",
            "Worker 8, [53/64]: Training Loss: 0.521314477, Training Accuracy: 84.160\n",
            "Worker 8, [54/64]: Training Loss: 0.445290042, Training Accuracy: 86.256\n",
            "Worker 8, [55/64]: Training Loss: 0.464945081, Training Accuracy: 85.824\n",
            "Worker 8, [56/64]: Training Loss: 0.446516792, Training Accuracy: 86.320\n",
            "Worker 8, [57/64]: Training Loss: 0.451788323, Training Accuracy: 85.744\n",
            "Worker 8, [58/64]: Training Loss: 0.426923016, Training Accuracy: 86.848\n",
            "Worker 8, [59/64]: Training Loss: 0.441042265, Training Accuracy: 86.528\n",
            "Worker 8, [60/64]: Training Loss: 0.421169336, Training Accuracy: 87.408\n",
            "Worker 8, [61/64]: Training Loss: 0.437228757, Training Accuracy: 86.480\n",
            "Worker 8, [62/64]: Training Loss: 0.414637451, Training Accuracy: 86.992\n",
            "Worker 8, [63/64]: Training Loss: 0.409627253, Training Accuracy: 87.776\n",
            "Worker 8, [64/64]: Training Loss: 0.382375437, Training Accuracy: 88.368\n",
            "Time taken for training worker 8: 0:03:19.135818\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.004257\n",
            "Global Update 02: Test Loss: 2.854822108, Test Accuracy: 37.720\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:53:11.239793\n",
            "//////////////////////////////////////////////////\n"
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [2, 4, 8]\n",
        "J = [4, 8, 16, 32, 64]\n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    local_SGD(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SlowMo Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "class SlowMoOptimizer(Optimizer):\n",
        "    def __init__(self, global_model, momentum, lr=0.01, beta=0.5, alpha=1.0):\n",
        "        self.global_model = global_model\n",
        "        self.lr = lr\n",
        "        self.beta = beta\n",
        "        self.alpha = alpha\n",
        "        self.momentum = momentum\n",
        "        params = list(global_model.parameters())\n",
        "        super(SlowMoOptimizer, self).__init__(params, {'lr': lr})\n",
        "        \n",
        "    def step(self, local_models):\n",
        "        # Calculate exact average of local models\n",
        "        avg_state_dict = {key: torch.zeros_like(value) for key, value in local_models[0].state_dict().items()}\n",
        "        for model in local_models:\n",
        "            for key, param in model.state_dict().items():\n",
        "                avg_state_dict[key] += param\n",
        "        \n",
        "        # Averaging the models\n",
        "        for key in avg_state_dict:\n",
        "            avg_state_dict[key] /= len(local_models)\n",
        "        \n",
        "        # Perform SlowMo momentum update\n",
        "        for key in self.global_model.state_dict().keys():\n",
        "            self.momentum[key] = self.beta * self.momentum[key] + (1.0 / self.lr) * (self.global_model.state_dict()[key] - avg_state_dict[key])\n",
        "        \n",
        "        # Update global model parameters with outer update\n",
        "        with torch.no_grad():\n",
        "            for key, param in self.global_model.state_dict().items():\n",
        "                param.copy_(param - self.alpha * self.lr * self.momentum[key])\n",
        "\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def local_SGD_SlowMo(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, beta, alpha):\n",
        "\n",
        "  total_start_time = time.time()\n",
        "\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "  \n",
        "  momentum = {key: torch.zeros_like(value) for key, value in global_model.state_dict().items()}\n",
        "  \n",
        "  global_optimizer = SlowMoOptimizer(global_model, momentum , lr, beta, alpha)\n",
        "  \n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "\n",
        "  checkpoint = load_checkpoint('slowmo', 64, {'k': k, 'j': j})\n",
        "  if checkpoint is not None:\n",
        "      global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      global_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      \n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn)\n",
        "      print(f'Global Update: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      \n",
        "      return None\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for loca_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    global_optimizer.step(local_models)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    for local_optimizer in local_optimizers:\n",
        "        local_optimizer.param_groups[0]['lr'] = global_optimizer.param_groups[0]['lr']    \n",
        "\n",
        "    for local_model in local_models:\n",
        "       local_model.load_state_dict(global_model.state_dict())\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Global Update {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "  # Save checkpoint\n",
        "  save_checkpoint({\n",
        "              'epoch': num_epochs,\n",
        "              'model_state_dict': global_model.state_dict(),\n",
        "              'optimizer_state_dict': global_optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss': loss_fn\n",
        "              }, 149, 64, 'slowmo', {'k': k, 'j': j})\n",
        "\n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for local_SGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:1\n",
            "==================================================\n",
            "Worker 1, [01/01]: Training Loss: 4.342150847, Training Accuracy: 3.840\n",
            "Time taken for training worker 1: 0:00:16.096309\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 4.335856533, Training Accuracy: 4.268\n",
            "Time taken for training worker 2: 0:00:13.447262\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002601\n",
            "Local Step 01: Test Loss: 3.988315181, Test Accuracy: 8.490\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.885550671, Training Accuracy: 9.632\n",
            "Time taken for training worker 1: 0:00:13.360038\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.893093649, Training Accuracy: 9.688\n",
            "Time taken for training worker 2: 0:00:11.400283\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002186\n",
            "Local Step 02: Test Loss: 4.405346876, Test Accuracy: 13.270\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.636666330, Training Accuracy: 14.136\n",
            "Time taken for training worker 1: 0:00:11.572736\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.642945243, Training Accuracy: 13.908\n",
            "Time taken for training worker 2: 0:00:11.209035\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002093\n",
            "Local Step 03: Test Loss: 3.589729356, Test Accuracy: 19.140\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.401977267, Training Accuracy: 17.744\n",
            "Time taken for training worker 1: 0:00:11.515425\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.423771399, Training Accuracy: 17.316\n",
            "Time taken for training worker 2: 0:00:11.156817\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002269\n",
            "Local Step 04: Test Loss: 3.191830026, Test Accuracy: 22.790\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.214541640, Training Accuracy: 21.100\n",
            "Time taken for training worker 1: 0:00:12.406378\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.225668302, Training Accuracy: 20.976\n",
            "Time taken for training worker 2: 0:00:12.957925\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002431\n",
            "Local Step 05: Test Loss: 2.974782200, Test Accuracy: 25.870\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.063640211, Training Accuracy: 24.252\n",
            "Time taken for training worker 1: 0:00:14.528041\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.080212228, Training Accuracy: 23.532\n",
            "Time taken for training worker 2: 0:00:11.684328\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002128\n",
            "Local Step 06: Test Loss: 2.816568451, Test Accuracy: 29.340\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.917613297, Training Accuracy: 26.812\n",
            "Time taken for training worker 1: 0:00:12.491698\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.931252772, Training Accuracy: 26.168\n",
            "Time taken for training worker 2: 0:00:12.444097\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002022\n",
            "Local Step 07: Test Loss: 2.643130053, Test Accuracy: 32.060\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.798180551, Training Accuracy: 29.188\n",
            "Time taken for training worker 1: 0:00:12.313934\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.797623556, Training Accuracy: 29.148\n",
            "Time taken for training worker 2: 0:00:12.901108\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003036\n",
            "Local Step 08: Test Loss: 2.516031843, Test Accuracy: 34.860\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.684763879, Training Accuracy: 31.556\n",
            "Time taken for training worker 1: 0:00:14.874031\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.700527600, Training Accuracy: 31.044\n",
            "Time taken for training worker 2: 0:00:12.256020\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002091\n",
            "Local Step 09: Test Loss: 2.425662513, Test Accuracy: 36.960\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.594341393, Training Accuracy: 33.428\n",
            "Time taken for training worker 1: 0:00:12.018347\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.603496471, Training Accuracy: 32.992\n",
            "Time taken for training worker 2: 0:00:11.579730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002051\n",
            "Local Step 10: Test Loss: 2.384847217, Test Accuracy: 38.400\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.533320336, Training Accuracy: 34.856\n",
            "Time taken for training worker 1: 0:00:11.528155\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.508706339, Training Accuracy: 35.220\n",
            "Time taken for training worker 2: 0:00:11.807590\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002370\n",
            "Local Step 11: Test Loss: 2.337506016, Test Accuracy: 39.240\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.462063040, Training Accuracy: 35.932\n",
            "Time taken for training worker 1: 0:00:11.849007\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.462700415, Training Accuracy: 36.032\n",
            "Time taken for training worker 2: 0:00:13.675144\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003323\n",
            "Local Step 12: Test Loss: 2.264010406, Test Accuracy: 40.280\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.387078104, Training Accuracy: 37.508\n",
            "Time taken for training worker 1: 0:00:13.115171\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.414554125, Training Accuracy: 37.432\n",
            "Time taken for training worker 2: 0:00:12.565362\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002599\n",
            "Local Step 13: Test Loss: 2.255074221, Test Accuracy: 41.200\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.351296381, Training Accuracy: 38.600\n",
            "Time taken for training worker 1: 0:00:12.414035\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.353050244, Training Accuracy: 38.572\n",
            "Time taken for training worker 2: 0:00:12.690956\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002109\n",
            "Local Step 14: Test Loss: 2.227291813, Test Accuracy: 41.630\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.295852557, Training Accuracy: 39.160\n",
            "Time taken for training worker 1: 0:00:12.194208\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.309027452, Training Accuracy: 39.384\n",
            "Time taken for training worker 2: 0:00:12.374757\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002318\n",
            "Local Step 15: Test Loss: 2.172928948, Test Accuracy: 43.140\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.253449748, Training Accuracy: 39.948\n",
            "Time taken for training worker 1: 0:00:12.544717\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.242832228, Training Accuracy: 40.796\n",
            "Time taken for training worker 2: 0:00:12.890338\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002242\n",
            "Local Step 16: Test Loss: 2.146246197, Test Accuracy: 43.750\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.204257865, Training Accuracy: 41.792\n",
            "Time taken for training worker 1: 0:00:12.517730\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.218964791, Training Accuracy: 41.372\n",
            "Time taken for training worker 2: 0:00:13.245379\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002225\n",
            "Local Step 17: Test Loss: 2.104229381, Test Accuracy: 44.850\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.187148295, Training Accuracy: 41.852\n",
            "Time taken for training worker 1: 0:00:11.329906\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.202615164, Training Accuracy: 41.728\n",
            "Time taken for training worker 2: 0:00:11.110857\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002095\n",
            "Local Step 18: Test Loss: 2.120728019, Test Accuracy: 44.270\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.142160650, Training Accuracy: 42.816\n",
            "Time taken for training worker 1: 0:00:11.729687\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.141435139, Training Accuracy: 43.128\n",
            "Time taken for training worker 2: 0:00:11.758638\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002037\n",
            "Local Step 19: Test Loss: 2.066050614, Test Accuracy: 45.460\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.107840878, Training Accuracy: 43.612\n",
            "Time taken for training worker 1: 0:00:12.079833\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.112824154, Training Accuracy: 43.464\n",
            "Time taken for training worker 2: 0:00:12.565708\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002490\n",
            "Local Step 20: Test Loss: 2.045240723, Test Accuracy: 46.050\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.085572857, Training Accuracy: 44.128\n",
            "Time taken for training worker 1: 0:00:13.689653\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.078232356, Training Accuracy: 44.232\n",
            "Time taken for training worker 2: 0:00:11.943304\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003093\n",
            "Local Step 21: Test Loss: 2.031107082, Test Accuracy: 46.000\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.039732321, Training Accuracy: 45.276\n",
            "Time taken for training worker 1: 0:00:13.886522\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.056317286, Training Accuracy: 45.156\n",
            "Time taken for training worker 2: 0:00:13.188002\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002756\n",
            "Local Step 22: Test Loss: 2.004109735, Test Accuracy: 46.740\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.017385835, Training Accuracy: 45.324\n",
            "Time taken for training worker 1: 0:00:14.854947\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.024548960, Training Accuracy: 45.716\n",
            "Time taken for training worker 2: 0:00:12.912815\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002194\n",
            "Local Step 23: Test Loss: 1.976576418, Test Accuracy: 47.740\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.007683161, Training Accuracy: 45.840\n",
            "Time taken for training worker 1: 0:00:13.726082\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 1.994170839, Training Accuracy: 46.244\n",
            "Time taken for training worker 2: 0:00:13.256460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002326\n",
            "Local Step 24: Test Loss: 1.999550436, Test Accuracy: 46.830\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.982028726, Training Accuracy: 46.488\n",
            "Time taken for training worker 1: 0:00:12.191587\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 1.959712650, Training Accuracy: 47.188\n",
            "Time taken for training worker 2: 0:00:12.869685\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002208\n",
            "Local Step 25: Test Loss: 1.978570656, Test Accuracy: 47.820\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.951214600, Training Accuracy: 47.164\n",
            "Time taken for training worker 1: 0:00:12.271810\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "beta = 0.4\n",
        "alpha = 1\n",
        "parameters = {'lr': lr, 'wd': wd, 'beta': beta}\n",
        "K = [2, 4, 8]\n",
        "J = [4, 8, 16, 32 , 64]\n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize slow model\n",
        "slow_model = LeNet5()\n",
        "slow_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "for k in K:\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    local_SGD_SlowMo(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, beta, alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Personal Contribution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SHAT (Asyncronous Approach)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class SHAT_PS_Optimizer(Optimizer):\n",
        "    def __init__(self, global_model, lr=0.01):\n",
        "        self.global_model = global_model\n",
        "        self.lr = lr\n",
        "        params = list(global_model.parameters())\n",
        "        super(SHAT_PS_Optimizer, self).__init__(params, {'lr': lr})\n",
        "        \n",
        "    def step(self, gradients_model):\n",
        "        # Update global model parameters with gradients \n",
        "        with torch.no_grad():\n",
        "            for key, param in self.global_model.state_dict().items():\n",
        "                param.copy_(param -  self.lr * gradients_model.state_dict()[key])\n",
        "\n",
        "        return None\n",
        "\n",
        "# For example total iterations 150*8 = 1200 to hacve the same number of iterations as local SGD in other words 150 true epochs\n",
        "# For example: Worker 1 has 15 iterations, Worker 2 has 15 iterations, Worker 3 has 15 iterations, Worker 4 has 15 iterations\n",
        "# All have the same number of iterations\n",
        "\n",
        "def generate_computation_latency_sequence(K, each_worker_iteration):\n",
        "    \"\"\"\n",
        "    Simulates a sequence of operations for K workers with random computation latencies.\n",
        "\n",
        "    The function generates a list of operations, each performed by a worker, sorted by\n",
        "    their latencies. Each worker performs exactly t operations, and the latencies are\n",
        "    scaled so that the fastest worker's latency is normalized to 1.\n",
        "\n",
        "    Parameters:\n",
        "    K (int): The number of workers in the simulation.\n",
        "    t (int): The number of operations each computer will perform.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - result (list of dicts): A list of dictionaries where each dictionary represents\n",
        "          an operation performed by a computer. The keys in the dictionary are:\n",
        "              - \"total_iterations\" (int): The total number of iterations performed.\n",
        "              - \"computer\" (int): The ID of the computer performing the operation.\n",
        "              - \"value\" (int): The latency value associated with that operation.\n",
        "        - computation_powers (list of ints): The original random computation latencies for each computer.\n",
        "        - scaled_powers (list of floats): The computation latencies scaled such that the fastest\n",
        "          worker's latency is 1.\n",
        "    \"\"\"\n",
        "    # Generate random computation powers\n",
        "    computation_powers = [5370, 9830] #[random.randint(5000, 10000) for _ in range(K)]\n",
        "\n",
        "    # Find the minimum computation power\n",
        "    min_power = min(computation_powers)\n",
        "\n",
        "    # Scale the computation powers so that the fastest (smallest value) is equal to 1\n",
        "    scaled_powers = [power / min_power for power in computation_powers] #[1.0, 1.8305400372439478]\n",
        "\n",
        "    # Initialize a list to store the dictionaries\n",
        "    result = []\n",
        "\n",
        "    # Store the number of turns taken by each worker\n",
        "    turns_taken = [0] * K\n",
        "    total_number_of_iterations = 0\n",
        "    # Generate the dictionaries until all computers have 15 turns\n",
        "    while any(turn < each_worker_iteration for turn in turns_taken):\n",
        "        total_number_of_iterations += 1\n",
        "        # Generate the next possible dictionaries for each worker\n",
        "        possible_entries = []\n",
        "        for index in range(K):\n",
        "            if turns_taken[index] < each_worker_iteration:  # Only consider computers with less than 15 turns\n",
        "                operation_value = computation_powers[index] * (turns_taken[index] + 1)\n",
        "                possible_entries.append({\"total_iterations\": total_number_of_iterations,\"worker\": index, \"value\": operation_value})\n",
        "\n",
        "        # Sort the possible dictionaries by their operation value\n",
        "        possible_entries.sort(key=lambda x: x[\"value\"])\n",
        "\n",
        "        # Add the dictionary with the smallest value to the result\n",
        "        chosen_entry = possible_entries[0]\n",
        "        result.append(chosen_entry)\n",
        "\n",
        "        # Update the turn count for the chosen worker\n",
        "        chosen_index = chosen_entry[\"worker\"] \n",
        "        turns_taken[chosen_index] += 1\n",
        "\n",
        "    return result, computation_powers, scaled_powers\n",
        "\n",
        "\n",
        "# For example total iterations 150*8 = 1200 to have the same number of iterations as local SGD in other words 150 true epochs\n",
        "# For example: Worker 1 has 15 iterations, Worker 2 has 30 iterations, Worker 3 has 45 iterations, Worker 4 has 60 iterations\n",
        "'''def generate_computation_latency_sequence(K, total_iterations):\n",
        "    \"\"\"\n",
        "    Simulates a sequence of operations for K workers with random computation latencies.\n",
        "\n",
        "    The function generates a list of operations, each performed by a worker, sorted by\n",
        "    their latencies. The total number of iterations is limited by `total_iterations`,\n",
        "    and the latencies are scaled so that the fastest worker's latency is normalized to 1.\n",
        "\n",
        "    Parameters:\n",
        "    K (int): The number of workers in the simulation.\n",
        "    total_iterations (int): The total number of iterations across all workers.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - result (list of dicts): A list of dictionaries where each dictionary represents\n",
        "          an operation performed by a worker. The keys in the dictionary are:\n",
        "              - \"total_iterations\" (int): The total number of iterations performed.\n",
        "              - \"worker\" (int): The ID of the worker performing the operation.\n",
        "              - \"value\" (int): The latency value associated with that operation.\n",
        "        - computation_powers (list of ints): The original random computation latencies for each worker.\n",
        "        - scaled_powers (list of floats): The computation latencies scaled such that the fastest\n",
        "          worker's latency is 1.\n",
        "    \"\"\"\n",
        "    # Generate random computation powers\n",
        "    computation_powers = [random.randint(5000, 10000) for _ in range(K)]\n",
        "\n",
        "    # Find the minimum computation power\n",
        "    min_power = min(computation_powers)\n",
        "\n",
        "    # Scale the computation powers so that the fastest (smallest value) is equal to 1\n",
        "    scaled_powers = [power / min_power for power in computation_powers]\n",
        "\n",
        "    # Initialize a list to store the dictionaries\n",
        "    result = []\n",
        "\n",
        "    # Store the number of turns taken by each worker\n",
        "    turns_taken = [0] * K\n",
        "    total_number_of_iterations_performed = 0\n",
        "\n",
        "    # Generate the dictionaries until the total number of iterations is reached\n",
        "    while total_number_of_iterations_performed < total_iterations:\n",
        "        total_number_of_iterations_performed += 1\n",
        "\n",
        "        # Generate the next possible dictionaries for each worker\n",
        "        possible_entries = []\n",
        "        for index in range(K):\n",
        "            # Calculate the operation value based on the current number of turns for this worker\n",
        "            operation_value = computation_powers[index] * (turns_taken[index] + 1)\n",
        "            possible_entries.append({\n",
        "                \"total_iterations\": total_number_of_iterations_performed,\n",
        "                \"worker\": index,  # Now starting index from 0\n",
        "                \"value\": operation_value\n",
        "            })\n",
        "\n",
        "        # Sort the possible dictionaries by their operation value\n",
        "        possible_entries.sort(key=lambda x: x[\"value\"])\n",
        "\n",
        "        # Add the dictionary with the smallest value to the result\n",
        "        chosen_entry = possible_entries[0]\n",
        "        result.append(chosen_entry)\n",
        "\n",
        "        # Update the turn count for the chosen worker\n",
        "        chosen_index = chosen_entry[\"worker\"]\n",
        "        turns_taken[chosen_index] += 1\n",
        "\n",
        "    return result, computation_powers, scaled_powers\n",
        "'''\n",
        "\n",
        "def calculate_gradients_model(global_model, local_model, lr):\n",
        "    gradients_dict = {key: torch.zeros_like(value) for key, value in local_model.state_dict().items()}\n",
        "    \n",
        "    for key, value in local_model.state_dict().items():\n",
        "        gradients_dict[key] += ((global_model.state_dict()[key] - value)/lr)\n",
        "    # TODO: lr ro check konim \n",
        "    \n",
        "    return gradients_dict\n",
        "\n",
        "def update_global_model(global_model, gradients_model, lr):\n",
        "    new_weights = {}\n",
        "    for key, value in global_model.state_dict().items():\n",
        "        new_weights[key] = value - lr * gradients_model.state_dict()[key]\n",
        "\n",
        "    return new_weights\n",
        "\n",
        "\n",
        "def calculate_s_i(k,ci):\n",
        "    return float(k/ci)\n",
        "\n",
        "def calculate_alpha_i(si, k):\n",
        "    alpha = 1 - (si / math.log(k))\n",
        "    return alpha\n",
        "\n",
        "def updatel_local_model(global_model, local_model, alpha_i):\n",
        "    new_weights = {}\n",
        "    for key, value in local_model.state_dict().items():\n",
        "        new_weights[key] = (1 - alpha_i) * value + alpha_i * global_model.state_dict()[key]\n",
        "    \n",
        "    return new_weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def SHAT_PS(shard_loaders, loss_fn, k, lr, wd, initial_state_dict, num_epochs):  \n",
        "    # SHAT Parameter Server to manage workers\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    # Initialize a model with same value of param for each chunk\n",
        "    local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "    for model in local_models:\n",
        "      model.load_state_dict(initial_state_dict)\n",
        "\n",
        "    # # Initialize the global model\n",
        "    global_model = LeNet5().to(device)\n",
        "    global_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "    global_optimizer = SHAT_PS_Optimizer(global_model, lr)\n",
        "  \n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=num_epochs)\n",
        "    \n",
        "    checkpoint = load_checkpoint('shat', 64, {'k': k})\n",
        "    if checkpoint is not None:\n",
        "      global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      global_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      \n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn)\n",
        "      print(f'Global Update: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      \n",
        "      return None\n",
        "    # Generate a sequence of computation latency to simulate the difference of computation latency (Lower computation Latency means higher computation power)\n",
        "    computation_latency_sequence, computation_latency, scaled_computation_latency = generate_computation_latency_sequence(k, num_epochs)\n",
        "\n",
        "    # Print the original and scaled computation latency\n",
        "    print(\"Original Computation Latency:\", computation_latency)\n",
        "    print(\"Scaled Computation Latency:\", scaled_computation_latency)\n",
        "\n",
        "    # print sequence of workers based on their computation latency\n",
        "    print(f'workers simulated orders based on computation latency:{[entry[\"worker\"]+1 for entry in computation_latency_sequence]}')\n",
        "\n",
        "    C = [1 for _ in range(k)] # Staleness counter\n",
        "    local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "    \n",
        "    #This specifies turn of the model\n",
        "    for iteration_index, worker in enumerate([entry['worker'] for entry in computation_latency_sequence]):\n",
        "      iteration_start_time = time.time()\n",
        "      print('*'*50)\n",
        "\n",
        "      train_loss, train_accuracy = train(local_models[worker], shard_loaders[worker], local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "      print(f'Worker {worker+1}, [{iteration_index+1:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      print('*'*50)\n",
        "      \n",
        "      '''PS server: receive model from the worker and calculate diff model (gradient)'''\n",
        "      gradients_model = LeNet5().to(device)\n",
        "      gradients_model.load_state_dict(calculate_gradients_model(global_model, local_models[worker], lr))\n",
        "      \n",
        "      # Computing the staleness of each worker\n",
        "      for i in C :\n",
        "        if i != worker:\n",
        "          i += 1\n",
        "          \n",
        "      '''PS Server update global model'''\n",
        "      global_optimizer.step(gradients_model)\n",
        "      \n",
        "      '''send updated model to the worker'''\n",
        "      '''calucale the staleness of the worker αi ← si − logn ,    s ← n/ci'''\n",
        "      s_i = calculate_s_i(k,C[worker])\n",
        "      alpha_i = calculate_alpha_i(s_i, k)\n",
        "      '''update worker local model ba w ← (1 − α )w + α w̃'''\n",
        "      local_models[worker].load_state_dict(updatel_local_model(global_model, local_models[worker], alpha_i))\n",
        "\n",
        "      '''continue outer loop in PS'''\n",
        "      '''ci = 0 ya 1'''\n",
        "      C[worker] = 1\n",
        "      \n",
        "      iteration_end_time = time.time()\n",
        "     \n",
        "      print(f'Time taken for worker {worker+1} : {str(timedelta(seconds=iteration_end_time - iteration_start_time))}')\n",
        "      print('-'*50)\n",
        "      print(f'Iteration: {iteration_index+1:02}/{k*num_epochs:02} - True epochs: {num_epochs}')\n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "      print(f'Global Update {iteration_index+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      print('-'*50)\n",
        "    \n",
        "    # Save checkpoint\n",
        "    save_checkpoint({\n",
        "              'epoch': num_epochs,\n",
        "              'model_state_dict': global_model.state_dict(),\n",
        "              'optimizer_state_dict': global_optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss': loss_fn\n",
        "              }, 149, 64, 'shat', {'k': k})\n",
        "    \n",
        "    total_end_time = time.time()\n",
        "    print('/'*50)\n",
        "    print(f'Total time taken for SHAT: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "    print('/'*50)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2\n",
            "==================================================\n",
            "Original Computation Latency: [5370, 9830]\n",
            "Scaled Computation Latency: [1.0, 1.8305400372439478]\n",
            "workers simulated orders based on computation latency:[1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "**************************************************\n",
            "Worker 1, [01]: Training Loss: 4.316138252, Training Accuracy: 4.696\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.667041\n",
            "--------------------------------------------------\n",
            "Iteration: 01/300 - True epochs: 150\n",
            "Global Update 01: Test Loss: 3.975807231, Test Accuracy: 9.000\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [02]: Training Loss: 4.329453920, Training Accuracy: 4.180\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.505983\n",
            "--------------------------------------------------\n",
            "Iteration: 02/300 - True epochs: 150\n",
            "Global Update 02: Test Loss: 3.972512134, Test Accuracy: 9.410\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [03]: Training Loss: 3.836991175, Training Accuracy: 10.528\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.112250\n",
            "--------------------------------------------------\n",
            "Iteration: 03/300 - True epochs: 150\n",
            "Global Update 03: Test Loss: 3.632532524, Test Accuracy: 14.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [04]: Training Loss: 3.618038215, Training Accuracy: 14.008\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.515699\n",
            "--------------------------------------------------\n",
            "Iteration: 04/300 - True epochs: 150\n",
            "Global Update 04: Test Loss: 3.429692271, Test Accuracy: 18.340\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [05]: Training Loss: 3.858631433, Training Accuracy: 10.496\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.228417\n",
            "--------------------------------------------------\n",
            "Iteration: 05/300 - True epochs: 150\n",
            "Global Update 05: Test Loss: 3.660878534, Test Accuracy: 13.870\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [06]: Training Loss: 3.414191735, Training Accuracy: 17.612\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.891479\n",
            "--------------------------------------------------\n",
            "Iteration: 06/300 - True epochs: 150\n",
            "Global Update 06: Test Loss: 3.246058077, Test Accuracy: 20.830\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [07]: Training Loss: 3.253835350, Training Accuracy: 20.392\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.068308\n",
            "--------------------------------------------------\n",
            "Iteration: 07/300 - True epochs: 150\n",
            "Global Update 07: Test Loss: 3.083960554, Test Accuracy: 24.290\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [08]: Training Loss: 3.622754240, Training Accuracy: 13.980\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.040350\n",
            "--------------------------------------------------\n",
            "Iteration: 08/300 - True epochs: 150\n",
            "Global Update 08: Test Loss: 3.437970435, Test Accuracy: 17.380\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [09]: Training Loss: 3.120094610, Training Accuracy: 22.736\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.734624\n",
            "--------------------------------------------------\n",
            "Iteration: 09/300 - True epochs: 150\n",
            "Global Update 09: Test Loss: 3.016988590, Test Accuracy: 25.150\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [10]: Training Loss: 2.984104911, Training Accuracy: 25.308\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.255636\n",
            "--------------------------------------------------\n",
            "Iteration: 10/300 - True epochs: 150\n",
            "Global Update 10: Test Loss: 2.908172238, Test Accuracy: 27.060\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [11]: Training Loss: 3.402254808, Training Accuracy: 17.924\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.537503\n",
            "--------------------------------------------------\n",
            "Iteration: 11/300 - True epochs: 150\n",
            "Global Update 11: Test Loss: 3.237796903, Test Accuracy: 21.180\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [12]: Training Loss: 2.866282683, Training Accuracy: 27.828\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.006991\n",
            "--------------------------------------------------\n",
            "Iteration: 12/300 - True epochs: 150\n",
            "Global Update 12: Test Loss: 2.819842363, Test Accuracy: 29.360\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [13]: Training Loss: 2.776137770, Training Accuracy: 29.636\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.253642\n",
            "--------------------------------------------------\n",
            "Iteration: 13/300 - True epochs: 150\n",
            "Global Update 13: Test Loss: 2.688713400, Test Accuracy: 31.790\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [14]: Training Loss: 3.238975392, Training Accuracy: 20.840\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.169064\n",
            "--------------------------------------------------\n",
            "Iteration: 14/300 - True epochs: 150\n",
            "Global Update 14: Test Loss: 3.086980874, Test Accuracy: 24.310\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [15]: Training Loss: 2.685191630, Training Accuracy: 30.916\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.626431\n",
            "--------------------------------------------------\n",
            "Iteration: 15/300 - True epochs: 150\n",
            "Global Update 15: Test Loss: 2.661002721, Test Accuracy: 31.930\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [16]: Training Loss: 3.095067416, Training Accuracy: 23.388\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.438638\n",
            "--------------------------------------------------\n",
            "Iteration: 16/300 - True epochs: 150\n",
            "Global Update 16: Test Loss: 2.910315693, Test Accuracy: 27.130\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [17]: Training Loss: 2.617654759, Training Accuracy: 32.700\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.103067\n",
            "--------------------------------------------------\n",
            "Iteration: 17/300 - True epochs: 150\n",
            "Global Update 17: Test Loss: 2.639010981, Test Accuracy: 32.130\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [18]: Training Loss: 2.531724761, Training Accuracy: 34.328\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.510805\n",
            "--------------------------------------------------\n",
            "Iteration: 18/300 - True epochs: 150\n",
            "Global Update 18: Test Loss: 2.565222997, Test Accuracy: 34.330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [19]: Training Loss: 2.946598632, Training Accuracy: 25.780\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.292272\n",
            "--------------------------------------------------\n",
            "Iteration: 19/300 - True epochs: 150\n",
            "Global Update 19: Test Loss: 2.949476805, Test Accuracy: 26.120\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [20]: Training Loss: 2.475083508, Training Accuracy: 35.428\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.204478\n",
            "--------------------------------------------------\n",
            "Iteration: 20/300 - True epochs: 150\n",
            "Global Update 20: Test Loss: 2.489591006, Test Accuracy: 35.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [21]: Training Loss: 2.431714295, Training Accuracy: 36.396\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.549945\n",
            "--------------------------------------------------\n",
            "Iteration: 21/300 - True epochs: 150\n",
            "Global Update 21: Test Loss: 2.495057530, Test Accuracy: 36.020\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [22]: Training Loss: 2.843231023, Training Accuracy: 27.800\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.269226\n",
            "--------------------------------------------------\n",
            "Iteration: 22/300 - True epochs: 150\n",
            "Global Update 22: Test Loss: 2.819718488, Test Accuracy: 28.910\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [23]: Training Loss: 2.379955557, Training Accuracy: 37.644\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.998146\n",
            "--------------------------------------------------\n",
            "Iteration: 23/300 - True epochs: 150\n",
            "Global Update 23: Test Loss: 2.450878770, Test Accuracy: 37.120\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [24]: Training Loss: 2.332721216, Training Accuracy: 38.636\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.762926\n",
            "--------------------------------------------------\n",
            "Iteration: 24/300 - True epochs: 150\n",
            "Global Update 24: Test Loss: 2.390652502, Test Accuracy: 38.110\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [25]: Training Loss: 2.758907427, Training Accuracy: 29.524\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:10.863055\n",
            "--------------------------------------------------\n",
            "Iteration: 25/300 - True epochs: 150\n",
            "Global Update 25: Test Loss: 2.739135718, Test Accuracy: 31.060\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [26]: Training Loss: 2.272783875, Training Accuracy: 40.012\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.867576\n",
            "--------------------------------------------------\n",
            "Iteration: 26/300 - True epochs: 150\n",
            "Global Update 26: Test Loss: 2.473722601, Test Accuracy: 37.220\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [27]: Training Loss: 2.231427574, Training Accuracy: 40.740\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.601060\n",
            "--------------------------------------------------\n",
            "Iteration: 27/300 - True epochs: 150\n",
            "Global Update 27: Test Loss: 2.388413335, Test Accuracy: 39.610\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [28]: Training Loss: 2.653471935, Training Accuracy: 31.808\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.351117\n",
            "--------------------------------------------------\n",
            "Iteration: 28/300 - True epochs: 150\n",
            "Global Update 28: Test Loss: 2.667235596, Test Accuracy: 32.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [29]: Training Loss: 2.197013154, Training Accuracy: 41.408\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.297390\n",
            "--------------------------------------------------\n",
            "Iteration: 29/300 - True epochs: 150\n",
            "Global Update 29: Test Loss: 2.374346902, Test Accuracy: 38.980\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [30]: Training Loss: 2.160509014, Training Accuracy: 42.340\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.091779\n",
            "--------------------------------------------------\n",
            "Iteration: 30/300 - True epochs: 150\n",
            "Global Update 30: Test Loss: 2.341440397, Test Accuracy: 40.330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [31]: Training Loss: 2.611322434, Training Accuracy: 32.964\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.262375\n",
            "--------------------------------------------------\n",
            "Iteration: 31/300 - True epochs: 150\n",
            "Global Update 31: Test Loss: 2.606453214, Test Accuracy: 33.200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [32]: Training Loss: 2.126850358, Training Accuracy: 42.760\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.384848\n",
            "--------------------------------------------------\n",
            "Iteration: 32/300 - True epochs: 150\n",
            "Global Update 32: Test Loss: 2.378417058, Test Accuracy: 39.460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [33]: Training Loss: 2.532118220, Training Accuracy: 34.360\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.770795\n",
            "--------------------------------------------------\n",
            "Iteration: 33/300 - True epochs: 150\n",
            "Global Update 33: Test Loss: 2.557158661, Test Accuracy: 34.300\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [34]: Training Loss: 2.098583099, Training Accuracy: 43.548\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.889500\n",
            "--------------------------------------------------\n",
            "Iteration: 34/300 - True epochs: 150\n",
            "Global Update 34: Test Loss: 2.371856371, Test Accuracy: 39.610\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [35]: Training Loss: 2.073999657, Training Accuracy: 44.304\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.623743\n",
            "--------------------------------------------------\n",
            "Iteration: 35/300 - True epochs: 150\n",
            "Global Update 35: Test Loss: 2.407574376, Test Accuracy: 39.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [36]: Training Loss: 2.460184660, Training Accuracy: 35.784\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.866451\n",
            "--------------------------------------------------\n",
            "Iteration: 36/300 - True epochs: 150\n",
            "Global Update 36: Test Loss: 2.520238143, Test Accuracy: 35.330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [37]: Training Loss: 2.050690645, Training Accuracy: 44.900\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.998221\n",
            "--------------------------------------------------\n",
            "Iteration: 37/300 - True epochs: 150\n",
            "Global Update 37: Test Loss: 2.338403539, Test Accuracy: 40.520\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [38]: Training Loss: 2.005932141, Training Accuracy: 45.588\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.348949\n",
            "--------------------------------------------------\n",
            "Iteration: 38/300 - True epochs: 150\n",
            "Global Update 38: Test Loss: 2.361860282, Test Accuracy: 40.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [39]: Training Loss: 2.402351194, Training Accuracy: 37.040\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.484094\n",
            "--------------------------------------------------\n",
            "Iteration: 39/300 - True epochs: 150\n",
            "Global Update 39: Test Loss: 2.503434579, Test Accuracy: 36.260\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [40]: Training Loss: 1.994910215, Training Accuracy: 46.104\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.367583\n",
            "--------------------------------------------------\n",
            "Iteration: 40/300 - True epochs: 150\n",
            "Global Update 40: Test Loss: 2.447367743, Test Accuracy: 38.420\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [41]: Training Loss: 1.964797944, Training Accuracy: 46.760\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.136974\n",
            "--------------------------------------------------\n",
            "Iteration: 41/300 - True epochs: 150\n",
            "Global Update 41: Test Loss: 2.345293606, Test Accuracy: 40.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [42]: Training Loss: 2.361893484, Training Accuracy: 38.104\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.671055\n",
            "--------------------------------------------------\n",
            "Iteration: 42/300 - True epochs: 150\n",
            "Global Update 42: Test Loss: 2.507612555, Test Accuracy: 35.680\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [43]: Training Loss: 1.923018138, Training Accuracy: 47.600\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.615024\n",
            "--------------------------------------------------\n",
            "Iteration: 43/300 - True epochs: 150\n",
            "Global Update 43: Test Loss: 2.362430249, Test Accuracy: 40.760\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [44]: Training Loss: 1.912922054, Training Accuracy: 48.040\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.294842\n",
            "--------------------------------------------------\n",
            "Iteration: 44/300 - True epochs: 150\n",
            "Global Update 44: Test Loss: 2.382212295, Test Accuracy: 40.730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [45]: Training Loss: 2.321800609, Training Accuracy: 38.656\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.224221\n",
            "--------------------------------------------------\n",
            "Iteration: 45/300 - True epochs: 150\n",
            "Global Update 45: Test Loss: 2.427226087, Test Accuracy: 37.510\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [46]: Training Loss: 1.892754872, Training Accuracy: 47.932\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.131398\n",
            "--------------------------------------------------\n",
            "Iteration: 46/300 - True epochs: 150\n",
            "Global Update 46: Test Loss: 2.399494384, Test Accuracy: 40.670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [47]: Training Loss: 1.868417685, Training Accuracy: 48.508\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.538805\n",
            "--------------------------------------------------\n",
            "Iteration: 47/300 - True epochs: 150\n",
            "Global Update 47: Test Loss: 2.413973480, Test Accuracy: 39.940\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [48]: Training Loss: 2.268863169, Training Accuracy: 39.752\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.920640\n",
            "--------------------------------------------------\n",
            "Iteration: 48/300 - True epochs: 150\n",
            "Global Update 48: Test Loss: 2.422993883, Test Accuracy: 37.820\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [49]: Training Loss: 1.867405598, Training Accuracy: 48.560\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.001229\n",
            "--------------------------------------------------\n",
            "Iteration: 49/300 - True epochs: 150\n",
            "Global Update 49: Test Loss: 2.287775385, Test Accuracy: 42.380\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [50]: Training Loss: 2.216568122, Training Accuracy: 40.648\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.170150\n",
            "--------------------------------------------------\n",
            "Iteration: 50/300 - True epochs: 150\n",
            "Global Update 50: Test Loss: 2.370671874, Test Accuracy: 39.230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [51]: Training Loss: 1.823930742, Training Accuracy: 49.824\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.195551\n",
            "--------------------------------------------------\n",
            "Iteration: 51/300 - True epochs: 150\n",
            "Global Update 51: Test Loss: 2.284418865, Test Accuracy: 42.190\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [52]: Training Loss: 1.800729085, Training Accuracy: 50.216\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.371146\n",
            "--------------------------------------------------\n",
            "Iteration: 52/300 - True epochs: 150\n",
            "Global Update 52: Test Loss: 2.363187909, Test Accuracy: 41.650\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [53]: Training Loss: 2.182685631, Training Accuracy: 41.932\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.434406\n",
            "--------------------------------------------------\n",
            "Iteration: 53/300 - True epochs: 150\n",
            "Global Update 53: Test Loss: 2.394151860, Test Accuracy: 38.830\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [54]: Training Loss: 1.809204883, Training Accuracy: 50.212\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.416776\n",
            "--------------------------------------------------\n",
            "Iteration: 54/300 - True epochs: 150\n",
            "Global Update 54: Test Loss: 2.312577093, Test Accuracy: 42.360\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [55]: Training Loss: 1.773283204, Training Accuracy: 50.936\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.286328\n",
            "--------------------------------------------------\n",
            "Iteration: 55/300 - True epochs: 150\n",
            "Global Update 55: Test Loss: 2.352393050, Test Accuracy: 42.050\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [56]: Training Loss: 2.166724631, Training Accuracy: 42.208\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.705461\n",
            "--------------------------------------------------\n",
            "Iteration: 56/300 - True epochs: 150\n",
            "Global Update 56: Test Loss: 2.399756820, Test Accuracy: 38.950\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [57]: Training Loss: 1.777806404, Training Accuracy: 51.000\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.228401\n",
            "--------------------------------------------------\n",
            "Iteration: 57/300 - True epochs: 150\n",
            "Global Update 57: Test Loss: 2.376094653, Test Accuracy: 40.680\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [58]: Training Loss: 1.757168186, Training Accuracy: 51.344\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.420995\n",
            "--------------------------------------------------\n",
            "Iteration: 58/300 - True epochs: 150\n",
            "Global Update 58: Test Loss: 2.333054682, Test Accuracy: 42.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [59]: Training Loss: 2.126482616, Training Accuracy: 42.872\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.361129\n",
            "--------------------------------------------------\n",
            "Iteration: 59/300 - True epochs: 150\n",
            "Global Update 59: Test Loss: 2.362659913, Test Accuracy: 39.610\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [60]: Training Loss: 1.724814029, Training Accuracy: 52.264\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.350277\n",
            "--------------------------------------------------\n",
            "Iteration: 60/300 - True epochs: 150\n",
            "Global Update 60: Test Loss: 2.357208572, Test Accuracy: 42.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [61]: Training Loss: 1.725930137, Training Accuracy: 52.244\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.449508\n",
            "--------------------------------------------------\n",
            "Iteration: 61/300 - True epochs: 150\n",
            "Global Update 61: Test Loss: 2.283835901, Test Accuracy: 42.890\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [62]: Training Loss: 2.078036781, Training Accuracy: 44.032\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.071234\n",
            "--------------------------------------------------\n",
            "Iteration: 62/300 - True epochs: 150\n",
            "Global Update 62: Test Loss: 2.404818180, Test Accuracy: 40.020\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [63]: Training Loss: 1.691285531, Training Accuracy: 53.256\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.567309\n",
            "--------------------------------------------------\n",
            "Iteration: 63/300 - True epochs: 150\n",
            "Global Update 63: Test Loss: 2.320308923, Test Accuracy: 41.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [64]: Training Loss: 1.691031609, Training Accuracy: 53.084\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.530213\n",
            "--------------------------------------------------\n",
            "Iteration: 64/300 - True epochs: 150\n",
            "Global Update 64: Test Loss: 2.339951171, Test Accuracy: 41.980\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [65]: Training Loss: 2.061682583, Training Accuracy: 44.224\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.232989\n",
            "--------------------------------------------------\n",
            "Iteration: 65/300 - True epochs: 150\n",
            "Global Update 65: Test Loss: 2.378964725, Test Accuracy: 40.630\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [66]: Training Loss: 1.692352859, Training Accuracy: 53.068\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.818666\n",
            "--------------------------------------------------\n",
            "Iteration: 66/300 - True epochs: 150\n",
            "Global Update 66: Test Loss: 2.298893975, Test Accuracy: 42.940\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [67]: Training Loss: 2.025143274, Training Accuracy: 45.352\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.397511\n",
            "--------------------------------------------------\n",
            "Iteration: 67/300 - True epochs: 150\n",
            "Global Update 67: Test Loss: 2.296705142, Test Accuracy: 41.690\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [68]: Training Loss: 1.679219170, Training Accuracy: 53.480\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.702529\n",
            "--------------------------------------------------\n",
            "Iteration: 68/300 - True epochs: 150\n",
            "Global Update 68: Test Loss: 2.308738833, Test Accuracy: 43.050\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [69]: Training Loss: 1.660299158, Training Accuracy: 53.796\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.451378\n",
            "--------------------------------------------------\n",
            "Iteration: 69/300 - True epochs: 150\n",
            "Global Update 69: Test Loss: 2.366027255, Test Accuracy: 43.090\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [70]: Training Loss: 1.995216186, Training Accuracy: 46.208\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.689269\n",
            "--------------------------------------------------\n",
            "Iteration: 70/300 - True epochs: 150\n",
            "Global Update 70: Test Loss: 2.392522758, Test Accuracy: 40.100\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [71]: Training Loss: 1.645369129, Training Accuracy: 53.872\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.988653\n",
            "--------------------------------------------------\n",
            "Iteration: 71/300 - True epochs: 150\n",
            "Global Update 71: Test Loss: 2.334212001, Test Accuracy: 42.260\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [72]: Training Loss: 1.651052115, Training Accuracy: 53.780\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.874294\n",
            "--------------------------------------------------\n",
            "Iteration: 72/300 - True epochs: 150\n",
            "Global Update 72: Test Loss: 2.368443895, Test Accuracy: 42.100\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [73]: Training Loss: 1.975179085, Training Accuracy: 46.148\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.152046\n",
            "--------------------------------------------------\n",
            "Iteration: 73/300 - True epochs: 150\n",
            "Global Update 73: Test Loss: 2.337784133, Test Accuracy: 41.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [74]: Training Loss: 1.631080172, Training Accuracy: 54.520\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.337342\n",
            "--------------------------------------------------\n",
            "Iteration: 74/300 - True epochs: 150\n",
            "Global Update 74: Test Loss: 2.436195863, Test Accuracy: 41.840\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [75]: Training Loss: 1.613323090, Training Accuracy: 55.052\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.260736\n",
            "--------------------------------------------------\n",
            "Iteration: 75/300 - True epochs: 150\n",
            "Global Update 75: Test Loss: 2.300718769, Test Accuracy: 43.590\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [76]: Training Loss: 1.936310600, Training Accuracy: 47.312\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.990712\n",
            "--------------------------------------------------\n",
            "Iteration: 76/300 - True epochs: 150\n",
            "Global Update 76: Test Loss: 2.395428646, Test Accuracy: 39.800\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [77]: Training Loss: 1.599914946, Training Accuracy: 55.180\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.384436\n",
            "--------------------------------------------------\n",
            "Iteration: 77/300 - True epochs: 150\n",
            "Global Update 77: Test Loss: 2.282156458, Test Accuracy: 43.490\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [78]: Training Loss: 1.608788187, Training Accuracy: 54.700\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.487170\n",
            "--------------------------------------------------\n",
            "Iteration: 78/300 - True epochs: 150\n",
            "Global Update 78: Test Loss: 2.340676127, Test Accuracy: 42.830\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [79]: Training Loss: 1.938165330, Training Accuracy: 46.964\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.038627\n",
            "--------------------------------------------------\n",
            "Iteration: 79/300 - True epochs: 150\n",
            "Global Update 79: Test Loss: 2.352092558, Test Accuracy: 41.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [80]: Training Loss: 1.586032391, Training Accuracy: 55.096\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.312376\n",
            "--------------------------------------------------\n",
            "Iteration: 80/300 - True epochs: 150\n",
            "Global Update 80: Test Loss: 2.336152469, Test Accuracy: 42.420\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [81]: Training Loss: 1.605724025, Training Accuracy: 54.996\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.539784\n",
            "--------------------------------------------------\n",
            "Iteration: 81/300 - True epochs: 150\n",
            "Global Update 81: Test Loss: 2.303625322, Test Accuracy: 43.670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [82]: Training Loss: 1.912488924, Training Accuracy: 47.836\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.709650\n",
            "--------------------------------------------------\n",
            "Iteration: 82/300 - True epochs: 150\n",
            "Global Update 82: Test Loss: 2.344473776, Test Accuracy: 41.210\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [83]: Training Loss: 1.572026900, Training Accuracy: 55.820\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.015839\n",
            "--------------------------------------------------\n",
            "Iteration: 83/300 - True epochs: 150\n",
            "Global Update 83: Test Loss: 2.397986870, Test Accuracy: 41.870\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [84]: Training Loss: 1.889167925, Training Accuracy: 48.684\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.417445\n",
            "--------------------------------------------------\n",
            "Iteration: 84/300 - True epochs: 150\n",
            "Global Update 84: Test Loss: 2.348603394, Test Accuracy: 41.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [85]: Training Loss: 1.584378986, Training Accuracy: 55.640\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.951653\n",
            "--------------------------------------------------\n",
            "Iteration: 85/300 - True epochs: 150\n",
            "Global Update 85: Test Loss: 2.364032265, Test Accuracy: 44.380\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [86]: Training Loss: 1.549381569, Training Accuracy: 56.352\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.359756\n",
            "--------------------------------------------------\n",
            "Iteration: 86/300 - True epochs: 150\n",
            "Global Update 86: Test Loss: 2.336683926, Test Accuracy: 43.680\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [87]: Training Loss: 1.878443418, Training Accuracy: 48.988\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.623087\n",
            "--------------------------------------------------\n",
            "Iteration: 87/300 - True epochs: 150\n",
            "Global Update 87: Test Loss: 2.307770759, Test Accuracy: 42.030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [88]: Training Loss: 1.541199810, Training Accuracy: 56.416\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.499515\n",
            "--------------------------------------------------\n",
            "Iteration: 88/300 - True epochs: 150\n",
            "Global Update 88: Test Loss: 2.376873098, Test Accuracy: 42.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [89]: Training Loss: 1.541309864, Training Accuracy: 56.260\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.667814\n",
            "--------------------------------------------------\n",
            "Iteration: 89/300 - True epochs: 150\n",
            "Global Update 89: Test Loss: 2.418202458, Test Accuracy: 42.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [90]: Training Loss: 1.839348989, Training Accuracy: 49.056\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.921324\n",
            "--------------------------------------------------\n",
            "Iteration: 90/300 - True epochs: 150\n",
            "Global Update 90: Test Loss: 2.312930505, Test Accuracy: 41.140\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [91]: Training Loss: 1.536791792, Training Accuracy: 56.724\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.599077\n",
            "--------------------------------------------------\n",
            "Iteration: 91/300 - True epochs: 150\n",
            "Global Update 91: Test Loss: 2.362759534, Test Accuracy: 43.670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [92]: Training Loss: 1.545726330, Training Accuracy: 56.352\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.576864\n",
            "--------------------------------------------------\n",
            "Iteration: 92/300 - True epochs: 150\n",
            "Global Update 92: Test Loss: 2.376499812, Test Accuracy: 42.610\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [93]: Training Loss: 1.836121774, Training Accuracy: 49.456\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.970155\n",
            "--------------------------------------------------\n",
            "Iteration: 93/300 - True epochs: 150\n",
            "Global Update 93: Test Loss: 2.351608145, Test Accuracy: 40.820\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [94]: Training Loss: 1.527345386, Training Accuracy: 56.936\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.291177\n",
            "--------------------------------------------------\n",
            "Iteration: 94/300 - True epochs: 150\n",
            "Global Update 94: Test Loss: 2.382404747, Test Accuracy: 43.820\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [95]: Training Loss: 1.512389899, Training Accuracy: 57.112\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.221977\n",
            "--------------------------------------------------\n",
            "Iteration: 95/300 - True epochs: 150\n",
            "Global Update 95: Test Loss: 2.263063306, Test Accuracy: 44.690\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [96]: Training Loss: 1.814522245, Training Accuracy: 50.036\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.785551\n",
            "--------------------------------------------------\n",
            "Iteration: 96/300 - True epochs: 150\n",
            "Global Update 96: Test Loss: 2.337836869, Test Accuracy: 42.020\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [97]: Training Loss: 1.542983404, Training Accuracy: 56.424\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.376011\n",
            "--------------------------------------------------\n",
            "Iteration: 97/300 - True epochs: 150\n",
            "Global Update 97: Test Loss: 2.366075632, Test Accuracy: 44.330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [98]: Training Loss: 1.508174523, Training Accuracy: 57.448\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.951624\n",
            "--------------------------------------------------\n",
            "Iteration: 98/300 - True epochs: 150\n",
            "Global Update 98: Test Loss: 2.339341638, Test Accuracy: 43.380\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [99]: Training Loss: 1.810244707, Training Accuracy: 50.160\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.210520\n",
            "--------------------------------------------------\n",
            "Iteration: 99/300 - True epochs: 150\n",
            "Global Update 99: Test Loss: 2.249091046, Test Accuracy: 43.230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [100]: Training Loss: 1.501446632, Training Accuracy: 57.684\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.102642\n",
            "--------------------------------------------------\n",
            "Iteration: 100/300 - True epochs: 150\n",
            "Global Update 100: Test Loss: 2.329049700, Test Accuracy: 43.920\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [101]: Training Loss: 1.758178526, Training Accuracy: 51.076\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.078691\n",
            "--------------------------------------------------\n",
            "Iteration: 101/300 - True epochs: 150\n",
            "Global Update 101: Test Loss: 2.430589826, Test Accuracy: 40.640\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [102]: Training Loss: 1.469667379, Training Accuracy: 58.524\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.099757\n",
            "--------------------------------------------------\n",
            "Iteration: 102/300 - True epochs: 150\n",
            "Global Update 102: Test Loss: 2.376917007, Test Accuracy: 44.280\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [103]: Training Loss: 1.488919301, Training Accuracy: 57.636\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.447638\n",
            "--------------------------------------------------\n",
            "Iteration: 103/300 - True epochs: 150\n",
            "Global Update 103: Test Loss: 2.371384340, Test Accuracy: 43.460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [104]: Training Loss: 1.771329990, Training Accuracy: 51.100\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.592653\n",
            "--------------------------------------------------\n",
            "Iteration: 104/300 - True epochs: 150\n",
            "Global Update 104: Test Loss: 2.345658868, Test Accuracy: 41.370\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [105]: Training Loss: 1.496519282, Training Accuracy: 57.872\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.690442\n",
            "--------------------------------------------------\n",
            "Iteration: 105/300 - True epochs: 150\n",
            "Global Update 105: Test Loss: 2.353771493, Test Accuracy: 42.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [106]: Training Loss: 1.467665409, Training Accuracy: 58.328\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.549362\n",
            "--------------------------------------------------\n",
            "Iteration: 106/300 - True epochs: 150\n",
            "Global Update 106: Test Loss: 2.426212337, Test Accuracy: 43.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [107]: Training Loss: 1.743885154, Training Accuracy: 51.712\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.639429\n",
            "--------------------------------------------------\n",
            "Iteration: 107/300 - True epochs: 150\n",
            "Global Update 107: Test Loss: 2.293003227, Test Accuracy: 43.190\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [108]: Training Loss: 1.458791452, Training Accuracy: 58.396\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.557187\n",
            "--------------------------------------------------\n",
            "Iteration: 108/300 - True epochs: 150\n",
            "Global Update 108: Test Loss: 2.342103323, Test Accuracy: 44.200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [109]: Training Loss: 1.467125476, Training Accuracy: 58.460\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.037053\n",
            "--------------------------------------------------\n",
            "Iteration: 109/300 - True epochs: 150\n",
            "Global Update 109: Test Loss: 2.358055073, Test Accuracy: 43.760\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [110]: Training Loss: 1.722566135, Training Accuracy: 52.004\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.533767\n",
            "--------------------------------------------------\n",
            "Iteration: 110/300 - True epochs: 150\n",
            "Global Update 110: Test Loss: 2.406690484, Test Accuracy: 41.250\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [111]: Training Loss: 1.452863529, Training Accuracy: 58.708\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.831047\n",
            "--------------------------------------------------\n",
            "Iteration: 111/300 - True epochs: 150\n",
            "Global Update 111: Test Loss: 2.377354102, Test Accuracy: 44.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [112]: Training Loss: 1.438409614, Training Accuracy: 59.164\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.789065\n",
            "--------------------------------------------------\n",
            "Iteration: 112/300 - True epochs: 150\n",
            "Global Update 112: Test Loss: 2.454786690, Test Accuracy: 43.300\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [113]: Training Loss: 1.703456334, Training Accuracy: 52.460\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.765360\n",
            "--------------------------------------------------\n",
            "Iteration: 113/300 - True epochs: 150\n",
            "Global Update 113: Test Loss: 2.369138029, Test Accuracy: 42.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [114]: Training Loss: 1.444736774, Training Accuracy: 58.792\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.238016\n",
            "--------------------------------------------------\n",
            "Iteration: 114/300 - True epochs: 150\n",
            "Global Update 114: Test Loss: 2.429452758, Test Accuracy: 43.510\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [115]: Training Loss: 1.448253147, Training Accuracy: 58.788\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.425858\n",
            "--------------------------------------------------\n",
            "Iteration: 115/300 - True epochs: 150\n",
            "Global Update 115: Test Loss: 2.479166767, Test Accuracy: 43.040\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [116]: Training Loss: 1.704307019, Training Accuracy: 52.584\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.239649\n",
            "--------------------------------------------------\n",
            "Iteration: 116/300 - True epochs: 150\n",
            "Global Update 116: Test Loss: 2.294360843, Test Accuracy: 43.230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [117]: Training Loss: 1.441684899, Training Accuracy: 58.620\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.678122\n",
            "--------------------------------------------------\n",
            "Iteration: 117/300 - True epochs: 150\n",
            "Global Update 117: Test Loss: 2.428173640, Test Accuracy: 43.660\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [118]: Training Loss: 1.695194653, Training Accuracy: 52.788\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.474607\n",
            "--------------------------------------------------\n",
            "Iteration: 118/300 - True epochs: 150\n",
            "Global Update 118: Test Loss: 2.372399597, Test Accuracy: 41.210\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [119]: Training Loss: 1.424401033, Training Accuracy: 59.756\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.095497\n",
            "--------------------------------------------------\n",
            "Iteration: 119/300 - True epochs: 150\n",
            "Global Update 119: Test Loss: 2.329219893, Test Accuracy: 44.100\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [120]: Training Loss: 1.439006140, Training Accuracy: 59.156\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.549009\n",
            "--------------------------------------------------\n",
            "Iteration: 120/300 - True epochs: 150\n",
            "Global Update 120: Test Loss: 2.409859524, Test Accuracy: 42.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [121]: Training Loss: 1.681295749, Training Accuracy: 53.156\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.003148\n",
            "--------------------------------------------------\n",
            "Iteration: 121/300 - True epochs: 150\n",
            "Global Update 121: Test Loss: 2.301618880, Test Accuracy: 43.760\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [122]: Training Loss: 1.400242731, Training Accuracy: 59.988\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.135503\n",
            "--------------------------------------------------\n",
            "Iteration: 122/300 - True epochs: 150\n",
            "Global Update 122: Test Loss: 2.389557010, Test Accuracy: 43.670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [123]: Training Loss: 1.407974453, Training Accuracy: 59.944\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.070759\n",
            "--------------------------------------------------\n",
            "Iteration: 123/300 - True epochs: 150\n",
            "Global Update 123: Test Loss: 2.427785766, Test Accuracy: 43.570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [124]: Training Loss: 1.667610897, Training Accuracy: 53.576\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.035205\n",
            "--------------------------------------------------\n",
            "Iteration: 124/300 - True epochs: 150\n",
            "Global Update 124: Test Loss: 2.375614625, Test Accuracy: 42.300\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [125]: Training Loss: 1.416449466, Training Accuracy: 59.268\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.607653\n",
            "--------------------------------------------------\n",
            "Iteration: 125/300 - True epochs: 150\n",
            "Global Update 125: Test Loss: 2.387929381, Test Accuracy: 43.730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [126]: Training Loss: 1.404092328, Training Accuracy: 59.996\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.421048\n",
            "--------------------------------------------------\n",
            "Iteration: 126/300 - True epochs: 150\n",
            "Global Update 126: Test Loss: 2.443424160, Test Accuracy: 42.680\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [127]: Training Loss: 1.644649932, Training Accuracy: 53.372\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.436975\n",
            "--------------------------------------------------\n",
            "Iteration: 127/300 - True epochs: 150\n",
            "Global Update 127: Test Loss: 2.337199169, Test Accuracy: 41.950\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [128]: Training Loss: 1.413952161, Training Accuracy: 60.112\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.377135\n",
            "--------------------------------------------------\n",
            "Iteration: 128/300 - True epochs: 150\n",
            "Global Update 128: Test Loss: 2.507497413, Test Accuracy: 42.740\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [129]: Training Loss: 1.406345548, Training Accuracy: 59.804\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.168864\n",
            "--------------------------------------------------\n",
            "Iteration: 129/300 - True epochs: 150\n",
            "Global Update 129: Test Loss: 2.422292119, Test Accuracy: 43.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [130]: Training Loss: 1.643948959, Training Accuracy: 53.884\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.764295\n",
            "--------------------------------------------------\n",
            "Iteration: 130/300 - True epochs: 150\n",
            "Global Update 130: Test Loss: 2.363883518, Test Accuracy: 43.790\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [131]: Training Loss: 1.393487448, Training Accuracy: 60.248\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.860305\n",
            "--------------------------------------------------\n",
            "Iteration: 131/300 - True epochs: 150\n",
            "Global Update 131: Test Loss: 2.370230607, Test Accuracy: 43.340\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [132]: Training Loss: 1.403282350, Training Accuracy: 60.244\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.116641\n",
            "--------------------------------------------------\n",
            "Iteration: 132/300 - True epochs: 150\n",
            "Global Update 132: Test Loss: 2.367217839, Test Accuracy: 44.490\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [133]: Training Loss: 1.619537434, Training Accuracy: 54.644\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.888651\n",
            "--------------------------------------------------\n",
            "Iteration: 133/300 - True epochs: 150\n",
            "Global Update 133: Test Loss: 2.331833553, Test Accuracy: 43.140\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [134]: Training Loss: 1.366576165, Training Accuracy: 60.764\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.363855\n",
            "--------------------------------------------------\n",
            "Iteration: 134/300 - True epochs: 150\n",
            "Global Update 134: Test Loss: 2.429396489, Test Accuracy: 42.830\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [135]: Training Loss: 1.609795622, Training Accuracy: 54.812\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.812050\n",
            "--------------------------------------------------\n",
            "Iteration: 135/300 - True epochs: 150\n",
            "Global Update 135: Test Loss: 2.337123266, Test Accuracy: 43.630\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [136]: Training Loss: 1.406623230, Training Accuracy: 60.008\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.776353\n",
            "--------------------------------------------------\n",
            "Iteration: 136/300 - True epochs: 150\n",
            "Global Update 136: Test Loss: 2.469695980, Test Accuracy: 42.370\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [137]: Training Loss: 1.392989513, Training Accuracy: 60.084\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.789863\n",
            "--------------------------------------------------\n",
            "Iteration: 137/300 - True epochs: 150\n",
            "Global Update 137: Test Loss: 2.355607277, Test Accuracy: 44.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [138]: Training Loss: 1.611623463, Training Accuracy: 54.840\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.450928\n",
            "--------------------------------------------------\n",
            "Iteration: 138/300 - True epochs: 150\n",
            "Global Update 138: Test Loss: 2.351587834, Test Accuracy: 42.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [139]: Training Loss: 1.399718881, Training Accuracy: 59.864\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.940151\n",
            "--------------------------------------------------\n",
            "Iteration: 139/300 - True epochs: 150\n",
            "Global Update 139: Test Loss: 2.421588241, Test Accuracy: 43.580\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [140]: Training Loss: 1.397501756, Training Accuracy: 59.984\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.466860\n",
            "--------------------------------------------------\n",
            "Iteration: 140/300 - True epochs: 150\n",
            "Global Update 140: Test Loss: 2.491808721, Test Accuracy: 43.060\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [141]: Training Loss: 1.595669527, Training Accuracy: 55.016\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.502890\n",
            "--------------------------------------------------\n",
            "Iteration: 141/300 - True epochs: 150\n",
            "Global Update 141: Test Loss: 2.453644767, Test Accuracy: 42.540\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [142]: Training Loss: 1.366340020, Training Accuracy: 60.572\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.294993\n",
            "--------------------------------------------------\n",
            "Iteration: 142/300 - True epochs: 150\n",
            "Global Update 142: Test Loss: 2.408151428, Test Accuracy: 44.300\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [143]: Training Loss: 1.377704201, Training Accuracy: 60.512\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.633692\n",
            "--------------------------------------------------\n",
            "Iteration: 143/300 - True epochs: 150\n",
            "Global Update 143: Test Loss: 2.402787983, Test Accuracy: 42.920\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [144]: Training Loss: 1.576281516, Training Accuracy: 55.708\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.334970\n",
            "--------------------------------------------------\n",
            "Iteration: 144/300 - True epochs: 150\n",
            "Global Update 144: Test Loss: 2.348765489, Test Accuracy: 42.260\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [145]: Training Loss: 1.366169835, Training Accuracy: 60.804\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.248031\n",
            "--------------------------------------------------\n",
            "Iteration: 145/300 - True epochs: 150\n",
            "Global Update 145: Test Loss: 2.441253875, Test Accuracy: 43.820\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [146]: Training Loss: 1.349410826, Training Accuracy: 61.592\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.275477\n",
            "--------------------------------------------------\n",
            "Iteration: 146/300 - True epochs: 150\n",
            "Global Update 146: Test Loss: 2.453373847, Test Accuracy: 42.890\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [147]: Training Loss: 1.592421795, Training Accuracy: 55.000\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.055001\n",
            "--------------------------------------------------\n",
            "Iteration: 147/300 - True epochs: 150\n",
            "Global Update 147: Test Loss: 2.440508177, Test Accuracy: 41.230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [148]: Training Loss: 1.355361849, Training Accuracy: 61.236\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.181037\n",
            "--------------------------------------------------\n",
            "Iteration: 148/300 - True epochs: 150\n",
            "Global Update 148: Test Loss: 2.482073008, Test Accuracy: 43.650\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [149]: Training Loss: 1.378585550, Training Accuracy: 60.816\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.744004\n",
            "--------------------------------------------------\n",
            "Iteration: 149/300 - True epochs: 150\n",
            "Global Update 149: Test Loss: 2.412064372, Test Accuracy: 43.920\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [150]: Training Loss: 1.572277238, Training Accuracy: 55.496\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.204581\n",
            "--------------------------------------------------\n",
            "Iteration: 150/300 - True epochs: 150\n",
            "Global Update 150: Test Loss: 2.367253572, Test Accuracy: 42.830\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [151]: Training Loss: 1.365364361, Training Accuracy: 60.808\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.818422\n",
            "--------------------------------------------------\n",
            "Iteration: 151/300 - True epochs: 150\n",
            "Global Update 151: Test Loss: 2.401569014, Test Accuracy: 43.660\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [152]: Training Loss: 1.561714777, Training Accuracy: 55.852\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.488381\n",
            "--------------------------------------------------\n",
            "Iteration: 152/300 - True epochs: 150\n",
            "Global Update 152: Test Loss: 2.360664024, Test Accuracy: 42.890\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [153]: Training Loss: 1.371420832, Training Accuracy: 60.632\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.580754\n",
            "--------------------------------------------------\n",
            "Iteration: 153/300 - True epochs: 150\n",
            "Global Update 153: Test Loss: 2.422626914, Test Accuracy: 43.850\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [154]: Training Loss: 1.356039762, Training Accuracy: 60.900\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.947428\n",
            "--------------------------------------------------\n",
            "Iteration: 154/300 - True epochs: 150\n",
            "Global Update 154: Test Loss: 2.419685546, Test Accuracy: 44.210\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [155]: Training Loss: 1.557062028, Training Accuracy: 56.172\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.438508\n",
            "--------------------------------------------------\n",
            "Iteration: 155/300 - True epochs: 150\n",
            "Global Update 155: Test Loss: 2.384785307, Test Accuracy: 43.290\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [156]: Training Loss: 1.353404430, Training Accuracy: 61.232\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.215378\n",
            "--------------------------------------------------\n",
            "Iteration: 156/300 - True epochs: 150\n",
            "Global Update 156: Test Loss: 2.353711436, Test Accuracy: 43.530\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [157]: Training Loss: 1.347858867, Training Accuracy: 61.400\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.915090\n",
            "--------------------------------------------------\n",
            "Iteration: 157/300 - True epochs: 150\n",
            "Global Update 157: Test Loss: 2.520182009, Test Accuracy: 42.790\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [158]: Training Loss: 1.556730891, Training Accuracy: 56.252\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.055111\n",
            "--------------------------------------------------\n",
            "Iteration: 158/300 - True epochs: 150\n",
            "Global Update 158: Test Loss: 2.365036318, Test Accuracy: 43.880\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [159]: Training Loss: 1.337164520, Training Accuracy: 61.576\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.097938\n",
            "--------------------------------------------------\n",
            "Iteration: 159/300 - True epochs: 150\n",
            "Global Update 159: Test Loss: 2.465923722, Test Accuracy: 43.510\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [160]: Training Loss: 1.351905073, Training Accuracy: 61.272\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.433360\n",
            "--------------------------------------------------\n",
            "Iteration: 160/300 - True epochs: 150\n",
            "Global Update 160: Test Loss: 2.437394314, Test Accuracy: 44.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [161]: Training Loss: 1.544909681, Training Accuracy: 56.064\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.345052\n",
            "--------------------------------------------------\n",
            "Iteration: 161/300 - True epochs: 150\n",
            "Global Update 161: Test Loss: 2.386978907, Test Accuracy: 42.290\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [162]: Training Loss: 1.353532773, Training Accuracy: 60.972\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.860724\n",
            "--------------------------------------------------\n",
            "Iteration: 162/300 - True epochs: 150\n",
            "Global Update 162: Test Loss: 2.448235511, Test Accuracy: 43.630\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [163]: Training Loss: 1.354984628, Training Accuracy: 61.260\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.480967\n",
            "--------------------------------------------------\n",
            "Iteration: 163/300 - True epochs: 150\n",
            "Global Update 163: Test Loss: 2.410107800, Test Accuracy: 43.620\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [164]: Training Loss: 1.521354818, Training Accuracy: 57.016\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.572608\n",
            "--------------------------------------------------\n",
            "Iteration: 164/300 - True epochs: 150\n",
            "Global Update 164: Test Loss: 2.469179805, Test Accuracy: 41.280\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [165]: Training Loss: 1.328247848, Training Accuracy: 61.796\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.484166\n",
            "--------------------------------------------------\n",
            "Iteration: 165/300 - True epochs: 150\n",
            "Global Update 165: Test Loss: 2.492983611, Test Accuracy: 41.920\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [166]: Training Loss: 1.335649559, Training Accuracy: 61.400\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.477638\n",
            "--------------------------------------------------\n",
            "Iteration: 166/300 - True epochs: 150\n",
            "Global Update 166: Test Loss: 2.427794504, Test Accuracy: 43.410\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [167]: Training Loss: 1.527547444, Training Accuracy: 57.040\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.890242\n",
            "--------------------------------------------------\n",
            "Iteration: 167/300 - True epochs: 150\n",
            "Global Update 167: Test Loss: 2.360734040, Test Accuracy: 43.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [168]: Training Loss: 1.333976579, Training Accuracy: 61.564\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.496433\n",
            "--------------------------------------------------\n",
            "Iteration: 168/300 - True epochs: 150\n",
            "Global Update 168: Test Loss: 2.415899692, Test Accuracy: 44.100\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [169]: Training Loss: 1.498495553, Training Accuracy: 57.560\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.950406\n",
            "--------------------------------------------------\n",
            "Iteration: 169/300 - True epochs: 150\n",
            "Global Update 169: Test Loss: 2.390593401, Test Accuracy: 42.910\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [170]: Training Loss: 1.332368592, Training Accuracy: 61.900\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.554949\n",
            "--------------------------------------------------\n",
            "Iteration: 170/300 - True epochs: 150\n",
            "Global Update 170: Test Loss: 2.572377480, Test Accuracy: 41.720\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [171]: Training Loss: 1.335034484, Training Accuracy: 61.976\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.452091\n",
            "--------------------------------------------------\n",
            "Iteration: 171/300 - True epochs: 150\n",
            "Global Update 171: Test Loss: 2.494293777, Test Accuracy: 42.550\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [172]: Training Loss: 1.506922050, Training Accuracy: 57.400\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.033988\n",
            "--------------------------------------------------\n",
            "Iteration: 172/300 - True epochs: 150\n",
            "Global Update 172: Test Loss: 2.421464612, Test Accuracy: 42.770\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [173]: Training Loss: 1.332531612, Training Accuracy: 61.788\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.720005\n",
            "--------------------------------------------------\n",
            "Iteration: 173/300 - True epochs: 150\n",
            "Global Update 173: Test Loss: 2.436763181, Test Accuracy: 42.760\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [174]: Training Loss: 1.291649072, Training Accuracy: 62.988\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.667861\n",
            "--------------------------------------------------\n",
            "Iteration: 174/300 - True epochs: 150\n",
            "Global Update 174: Test Loss: 2.448778740, Test Accuracy: 43.870\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [175]: Training Loss: 1.481377734, Training Accuracy: 58.128\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.036012\n",
            "--------------------------------------------------\n",
            "Iteration: 175/300 - True epochs: 150\n",
            "Global Update 175: Test Loss: 2.333741563, Test Accuracy: 43.790\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [176]: Training Loss: 1.329983374, Training Accuracy: 61.548\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.943758\n",
            "--------------------------------------------------\n",
            "Iteration: 176/300 - True epochs: 150\n",
            "Global Update 176: Test Loss: 2.447532077, Test Accuracy: 43.530\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [177]: Training Loss: 1.317279156, Training Accuracy: 62.120\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.919709\n",
            "--------------------------------------------------\n",
            "Iteration: 177/300 - True epochs: 150\n",
            "Global Update 177: Test Loss: 2.494138453, Test Accuracy: 43.320\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [178]: Training Loss: 1.495884503, Training Accuracy: 57.596\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.648539\n",
            "--------------------------------------------------\n",
            "Iteration: 178/300 - True epochs: 150\n",
            "Global Update 178: Test Loss: 2.383947562, Test Accuracy: 43.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [179]: Training Loss: 1.301381250, Training Accuracy: 62.452\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.572173\n",
            "--------------------------------------------------\n",
            "Iteration: 179/300 - True epochs: 150\n",
            "Global Update 179: Test Loss: 2.412716225, Test Accuracy: 44.110\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [180]: Training Loss: 1.310366376, Training Accuracy: 62.176\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.377317\n",
            "--------------------------------------------------\n",
            "Iteration: 180/300 - True epochs: 150\n",
            "Global Update 180: Test Loss: 2.485363085, Test Accuracy: 43.410\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [181]: Training Loss: 1.486392656, Training Accuracy: 57.956\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.371289\n",
            "--------------------------------------------------\n",
            "Iteration: 181/300 - True epochs: 150\n",
            "Global Update 181: Test Loss: 2.398947117, Test Accuracy: 42.530\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [182]: Training Loss: 1.312309915, Training Accuracy: 62.240\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.317991\n",
            "--------------------------------------------------\n",
            "Iteration: 182/300 - True epochs: 150\n",
            "Global Update 182: Test Loss: 2.560551538, Test Accuracy: 41.720\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [183]: Training Loss: 1.474399551, Training Accuracy: 58.216\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.846908\n",
            "--------------------------------------------------\n",
            "Iteration: 183/300 - True epochs: 150\n",
            "Global Update 183: Test Loss: 2.413751905, Test Accuracy: 42.520\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [184]: Training Loss: 1.310064652, Training Accuracy: 62.376\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.474268\n",
            "--------------------------------------------------\n",
            "Iteration: 184/300 - True epochs: 150\n",
            "Global Update 184: Test Loss: 2.397376684, Test Accuracy: 43.980\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [185]: Training Loss: 1.286278742, Training Accuracy: 62.820\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.225725\n",
            "--------------------------------------------------\n",
            "Iteration: 185/300 - True epochs: 150\n",
            "Global Update 185: Test Loss: 2.461037908, Test Accuracy: 44.280\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [186]: Training Loss: 1.496343151, Training Accuracy: 57.260\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.227706\n",
            "--------------------------------------------------\n",
            "Iteration: 186/300 - True epochs: 150\n",
            "Global Update 186: Test Loss: 2.384066890, Test Accuracy: 43.140\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [187]: Training Loss: 1.318757407, Training Accuracy: 61.936\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.833301\n",
            "--------------------------------------------------\n",
            "Iteration: 187/300 - True epochs: 150\n",
            "Global Update 187: Test Loss: 2.444964079, Test Accuracy: 43.340\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [188]: Training Loss: 1.290906844, Training Accuracy: 63.016\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.223465\n",
            "--------------------------------------------------\n",
            "Iteration: 188/300 - True epochs: 150\n",
            "Global Update 188: Test Loss: 2.498270925, Test Accuracy: 43.190\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [189]: Training Loss: 1.470919018, Training Accuracy: 58.012\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:13.009838\n",
            "--------------------------------------------------\n",
            "Iteration: 189/300 - True epochs: 150\n",
            "Global Update 189: Test Loss: 2.399805497, Test Accuracy: 42.970\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [190]: Training Loss: 1.314017584, Training Accuracy: 61.836\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.996978\n",
            "--------------------------------------------------\n",
            "Iteration: 190/300 - True epochs: 150\n",
            "Global Update 190: Test Loss: 2.483902580, Test Accuracy: 43.700\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [191]: Training Loss: 1.298878304, Training Accuracy: 62.824\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.167057\n",
            "--------------------------------------------------\n",
            "Iteration: 191/300 - True epochs: 150\n",
            "Global Update 191: Test Loss: 2.442789504, Test Accuracy: 44.500\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [192]: Training Loss: 1.481675036, Training Accuracy: 57.928\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.694863\n",
            "--------------------------------------------------\n",
            "Iteration: 192/300 - True epochs: 150\n",
            "Global Update 192: Test Loss: 2.437249074, Test Accuracy: 43.870\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [193]: Training Loss: 1.297849889, Training Accuracy: 62.692\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.343909\n",
            "--------------------------------------------------\n",
            "Iteration: 193/300 - True epochs: 150\n",
            "Global Update 193: Test Loss: 2.583290542, Test Accuracy: 41.840\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [194]: Training Loss: 1.301964501, Training Accuracy: 62.584\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.280974\n",
            "--------------------------------------------------\n",
            "Iteration: 194/300 - True epochs: 150\n",
            "Global Update 194: Test Loss: 2.461426760, Test Accuracy: 44.470\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [195]: Training Loss: 1.471644532, Training Accuracy: 58.420\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.183688\n",
            "--------------------------------------------------\n",
            "Iteration: 195/300 - True epochs: 150\n",
            "Global Update 195: Test Loss: 2.401670300, Test Accuracy: 42.750\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [196]: Training Loss: 1.294971527, Training Accuracy: 62.604\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.003072\n",
            "--------------------------------------------------\n",
            "Iteration: 196/300 - True epochs: 150\n",
            "Global Update 196: Test Loss: 2.516165919, Test Accuracy: 43.150\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [197]: Training Loss: 1.299257349, Training Accuracy: 62.472\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.792831\n",
            "--------------------------------------------------\n",
            "Iteration: 197/300 - True epochs: 150\n",
            "Global Update 197: Test Loss: 2.530946199, Test Accuracy: 43.900\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [198]: Training Loss: 1.458915125, Training Accuracy: 58.404\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.236358\n",
            "--------------------------------------------------\n",
            "Iteration: 198/300 - True epochs: 150\n",
            "Global Update 198: Test Loss: 2.443392904, Test Accuracy: 42.190\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [199]: Training Loss: 1.290232076, Training Accuracy: 62.756\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.793118\n",
            "--------------------------------------------------\n",
            "Iteration: 199/300 - True epochs: 150\n",
            "Global Update 199: Test Loss: 2.497372401, Test Accuracy: 43.430\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [200]: Training Loss: 1.442445313, Training Accuracy: 58.772\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.199500\n",
            "--------------------------------------------------\n",
            "Iteration: 200/300 - True epochs: 150\n",
            "Global Update 200: Test Loss: 2.430327730, Test Accuracy: 42.380\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [201]: Training Loss: 1.284479139, Training Accuracy: 62.652\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.678795\n",
            "--------------------------------------------------\n",
            "Iteration: 201/300 - True epochs: 150\n",
            "Global Update 201: Test Loss: 2.459918604, Test Accuracy: 43.550\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [202]: Training Loss: 1.272292713, Training Accuracy: 62.804\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.392865\n",
            "--------------------------------------------------\n",
            "Iteration: 202/300 - True epochs: 150\n",
            "Global Update 202: Test Loss: 2.493921067, Test Accuracy: 43.730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [203]: Training Loss: 1.449913517, Training Accuracy: 58.840\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.337669\n",
            "--------------------------------------------------\n",
            "Iteration: 203/300 - True epochs: 150\n",
            "Global Update 203: Test Loss: 2.369029196, Test Accuracy: 43.960\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [204]: Training Loss: 1.298304060, Training Accuracy: 62.848\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.685245\n",
            "--------------------------------------------------\n",
            "Iteration: 204/300 - True epochs: 150\n",
            "Global Update 204: Test Loss: 2.559429641, Test Accuracy: 43.270\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [205]: Training Loss: 1.280671421, Training Accuracy: 62.852\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.065267\n",
            "--------------------------------------------------\n",
            "Iteration: 205/300 - True epochs: 150\n",
            "Global Update 205: Test Loss: 2.516567986, Test Accuracy: 43.640\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [206]: Training Loss: 1.448409707, Training Accuracy: 58.836\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.369818\n",
            "--------------------------------------------------\n",
            "Iteration: 206/300 - True epochs: 150\n",
            "Global Update 206: Test Loss: 2.352359567, Test Accuracy: 44.460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [207]: Training Loss: 1.296190381, Training Accuracy: 62.580\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.805698\n",
            "--------------------------------------------------\n",
            "Iteration: 207/300 - True epochs: 150\n",
            "Global Update 207: Test Loss: 2.513166321, Test Accuracy: 43.600\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [208]: Training Loss: 1.265776230, Training Accuracy: 63.164\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.390284\n",
            "--------------------------------------------------\n",
            "Iteration: 208/300 - True epochs: 150\n",
            "Global Update 208: Test Loss: 2.575885190, Test Accuracy: 43.220\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [209]: Training Loss: 1.417343033, Training Accuracy: 59.696\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.831504\n",
            "--------------------------------------------------\n",
            "Iteration: 209/300 - True epochs: 150\n",
            "Global Update 209: Test Loss: 2.437920043, Test Accuracy: 43.750\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [210]: Training Loss: 1.289764598, Training Accuracy: 62.920\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:10.990066\n",
            "--------------------------------------------------\n",
            "Iteration: 210/300 - True epochs: 150\n",
            "Global Update 210: Test Loss: 2.562922668, Test Accuracy: 43.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [211]: Training Loss: 1.269398121, Training Accuracy: 63.560\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:13.034173\n",
            "--------------------------------------------------\n",
            "Iteration: 211/300 - True epochs: 150\n",
            "Global Update 211: Test Loss: 2.469244083, Test Accuracy: 43.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [212]: Training Loss: 1.435406071, Training Accuracy: 58.944\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.399258\n",
            "--------------------------------------------------\n",
            "Iteration: 212/300 - True epochs: 150\n",
            "Global Update 212: Test Loss: 2.353327999, Test Accuracy: 43.450\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [213]: Training Loss: 1.245878369, Training Accuracy: 64.024\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.327526\n",
            "--------------------------------------------------\n",
            "Iteration: 213/300 - True epochs: 150\n",
            "Global Update 213: Test Loss: 2.541426846, Test Accuracy: 43.200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [214]: Training Loss: 1.269733509, Training Accuracy: 63.004\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.139136\n",
            "--------------------------------------------------\n",
            "Iteration: 214/300 - True epochs: 150\n",
            "Global Update 214: Test Loss: 2.528439369, Test Accuracy: 43.750\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [215]: Training Loss: 1.406017738, Training Accuracy: 59.772\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.858691\n",
            "--------------------------------------------------\n",
            "Iteration: 215/300 - True epochs: 150\n",
            "Global Update 215: Test Loss: 2.400047874, Test Accuracy: 44.340\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [216]: Training Loss: 1.273913290, Training Accuracy: 63.264\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.073995\n",
            "--------------------------------------------------\n",
            "Iteration: 216/300 - True epochs: 150\n",
            "Global Update 216: Test Loss: 2.531057900, Test Accuracy: 43.190\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [217]: Training Loss: 1.423166112, Training Accuracy: 59.260\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.812275\n",
            "--------------------------------------------------\n",
            "Iteration: 217/300 - True epochs: 150\n",
            "Global Update 217: Test Loss: 2.430894105, Test Accuracy: 42.800\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [218]: Training Loss: 1.272147731, Training Accuracy: 63.056\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.384795\n",
            "--------------------------------------------------\n",
            "Iteration: 218/300 - True epochs: 150\n",
            "Global Update 218: Test Loss: 2.522261268, Test Accuracy: 43.460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [219]: Training Loss: 1.247604307, Training Accuracy: 63.868\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.543410\n",
            "--------------------------------------------------\n",
            "Iteration: 219/300 - True epochs: 150\n",
            "Global Update 219: Test Loss: 2.478418769, Test Accuracy: 44.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [220]: Training Loss: 1.413695785, Training Accuracy: 59.616\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.927133\n",
            "--------------------------------------------------\n",
            "Iteration: 220/300 - True epochs: 150\n",
            "Global Update 220: Test Loss: 2.406037825, Test Accuracy: 42.770\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [221]: Training Loss: 1.270346493, Training Accuracy: 63.328\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.299116\n",
            "--------------------------------------------------\n",
            "Iteration: 221/300 - True epochs: 150\n",
            "Global Update 221: Test Loss: 2.462190932, Test Accuracy: 44.200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [222]: Training Loss: 1.258509249, Training Accuracy: 63.528\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.370722\n",
            "--------------------------------------------------\n",
            "Iteration: 222/300 - True epochs: 150\n",
            "Global Update 222: Test Loss: 2.515432974, Test Accuracy: 43.640\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [223]: Training Loss: 1.412504714, Training Accuracy: 59.488\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.966325\n",
            "--------------------------------------------------\n",
            "Iteration: 223/300 - True epochs: 150\n",
            "Global Update 223: Test Loss: 2.458389520, Test Accuracy: 43.740\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [224]: Training Loss: 1.253977939, Training Accuracy: 63.988\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.434760\n",
            "--------------------------------------------------\n",
            "Iteration: 224/300 - True epochs: 150\n",
            "Global Update 224: Test Loss: 2.484682195, Test Accuracy: 42.560\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [225]: Training Loss: 1.291508590, Training Accuracy: 62.812\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.893885\n",
            "--------------------------------------------------\n",
            "Iteration: 225/300 - True epochs: 150\n",
            "Global Update 225: Test Loss: 2.509062056, Test Accuracy: 43.440\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [226]: Training Loss: 1.416602431, Training Accuracy: 58.988\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.828009\n",
            "--------------------------------------------------\n",
            "Iteration: 226/300 - True epochs: 150\n",
            "Global Update 226: Test Loss: 2.406116296, Test Accuracy: 43.670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [227]: Training Loss: 1.250409606, Training Accuracy: 63.452\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:12.044666\n",
            "--------------------------------------------------\n",
            "Iteration: 227/300 - True epochs: 150\n",
            "Global Update 227: Test Loss: 2.522692643, Test Accuracy: 43.450\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [228]: Training Loss: 1.276538121, Training Accuracy: 62.756\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.296758\n",
            "--------------------------------------------------\n",
            "Iteration: 228/300 - True epochs: 150\n",
            "Global Update 228: Test Loss: 2.502799635, Test Accuracy: 43.880\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [229]: Training Loss: 1.393069456, Training Accuracy: 60.028\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.113127\n",
            "--------------------------------------------------\n",
            "Iteration: 229/300 - True epochs: 150\n",
            "Global Update 229: Test Loss: 2.434358662, Test Accuracy: 43.470\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [230]: Training Loss: 1.276834872, Training Accuracy: 63.108\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.583761\n",
            "--------------------------------------------------\n",
            "Iteration: 230/300 - True epochs: 150\n",
            "Global Update 230: Test Loss: 2.464660371, Test Accuracy: 43.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 1, [231]: Training Loss: 1.266660519, Training Accuracy: 63.088\n",
            "**************************************************\n",
            "Time taken for worker 1 : 0:00:11.385540\n",
            "--------------------------------------------------\n",
            "Iteration: 231/300 - True epochs: 150\n",
            "Global Update 231: Test Loss: 2.500851468, Test Accuracy: 44.490\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [232]: Training Loss: 1.386626027, Training Accuracy: 60.368\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.737047\n",
            "--------------------------------------------------\n",
            "Iteration: 232/300 - True epochs: 150\n",
            "Global Update 232: Test Loss: 2.459403390, Test Accuracy: 42.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [233]: Training Loss: 1.399065541, Training Accuracy: 59.944\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.072688\n",
            "--------------------------------------------------\n",
            "Iteration: 233/300 - True epochs: 150\n",
            "Global Update 233: Test Loss: 2.397542540, Test Accuracy: 43.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [234]: Training Loss: 1.385842155, Training Accuracy: 60.724\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.694269\n",
            "--------------------------------------------------\n",
            "Iteration: 234/300 - True epochs: 150\n",
            "Global Update 234: Test Loss: 2.486011102, Test Accuracy: 41.830\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [235]: Training Loss: 1.386374014, Training Accuracy: 60.212\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.799620\n",
            "--------------------------------------------------\n",
            "Iteration: 235/300 - True epochs: 150\n",
            "Global Update 235: Test Loss: 2.393514348, Test Accuracy: 44.270\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [236]: Training Loss: 1.391075044, Training Accuracy: 60.240\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.357210\n",
            "--------------------------------------------------\n",
            "Iteration: 236/300 - True epochs: 150\n",
            "Global Update 236: Test Loss: 2.438095754, Test Accuracy: 43.040\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [237]: Training Loss: 1.387339046, Training Accuracy: 60.368\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.155132\n",
            "--------------------------------------------------\n",
            "Iteration: 237/300 - True epochs: 150\n",
            "Global Update 237: Test Loss: 2.449415172, Test Accuracy: 43.510\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [238]: Training Loss: 1.355693920, Training Accuracy: 61.256\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.240244\n",
            "--------------------------------------------------\n",
            "Iteration: 238/300 - True epochs: 150\n",
            "Global Update 238: Test Loss: 2.442054207, Test Accuracy: 43.620\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [239]: Training Loss: 1.354832054, Training Accuracy: 60.960\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.736959\n",
            "--------------------------------------------------\n",
            "Iteration: 239/300 - True epochs: 150\n",
            "Global Update 239: Test Loss: 2.444694382, Test Accuracy: 43.280\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [240]: Training Loss: 1.371950324, Training Accuracy: 60.880\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.199937\n",
            "--------------------------------------------------\n",
            "Iteration: 240/300 - True epochs: 150\n",
            "Global Update 240: Test Loss: 2.434116857, Test Accuracy: 42.980\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [241]: Training Loss: 1.356997914, Training Accuracy: 60.784\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.437450\n",
            "--------------------------------------------------\n",
            "Iteration: 241/300 - True epochs: 150\n",
            "Global Update 241: Test Loss: 2.394194153, Test Accuracy: 43.910\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [242]: Training Loss: 1.355628750, Training Accuracy: 61.016\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.296114\n",
            "--------------------------------------------------\n",
            "Iteration: 242/300 - True epochs: 150\n",
            "Global Update 242: Test Loss: 2.401716749, Test Accuracy: 45.020\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [243]: Training Loss: 1.343840825, Training Accuracy: 61.588\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.035954\n",
            "--------------------------------------------------\n",
            "Iteration: 243/300 - True epochs: 150\n",
            "Global Update 243: Test Loss: 2.473536396, Test Accuracy: 42.760\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [244]: Training Loss: 1.349029709, Training Accuracy: 61.112\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.652744\n",
            "--------------------------------------------------\n",
            "Iteration: 244/300 - True epochs: 150\n",
            "Global Update 244: Test Loss: 2.536183525, Test Accuracy: 43.040\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [245]: Training Loss: 1.354054744, Training Accuracy: 60.884\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.973266\n",
            "--------------------------------------------------\n",
            "Iteration: 245/300 - True epochs: 150\n",
            "Global Update 245: Test Loss: 2.494626903, Test Accuracy: 43.040\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [246]: Training Loss: 1.332650026, Training Accuracy: 61.828\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.503853\n",
            "--------------------------------------------------\n",
            "Iteration: 246/300 - True epochs: 150\n",
            "Global Update 246: Test Loss: 2.446792738, Test Accuracy: 44.150\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [247]: Training Loss: 1.351988045, Training Accuracy: 60.968\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.404897\n",
            "--------------------------------------------------\n",
            "Iteration: 247/300 - True epochs: 150\n",
            "Global Update 247: Test Loss: 2.404070994, Test Accuracy: 42.700\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [248]: Training Loss: 1.351893976, Training Accuracy: 61.132\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.619031\n",
            "--------------------------------------------------\n",
            "Iteration: 248/300 - True epochs: 150\n",
            "Global Update 248: Test Loss: 2.392720631, Test Accuracy: 44.010\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [249]: Training Loss: 1.364026097, Training Accuracy: 60.632\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.126750\n",
            "--------------------------------------------------\n",
            "Iteration: 249/300 - True epochs: 150\n",
            "Global Update 249: Test Loss: 2.423605390, Test Accuracy: 44.080\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [250]: Training Loss: 1.332228500, Training Accuracy: 61.564\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.687105\n",
            "--------------------------------------------------\n",
            "Iteration: 250/300 - True epochs: 150\n",
            "Global Update 250: Test Loss: 2.444874817, Test Accuracy: 44.060\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [251]: Training Loss: 1.332580244, Training Accuracy: 61.648\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.648654\n",
            "--------------------------------------------------\n",
            "Iteration: 251/300 - True epochs: 150\n",
            "Global Update 251: Test Loss: 2.475750859, Test Accuracy: 43.320\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [252]: Training Loss: 1.340929781, Training Accuracy: 61.236\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.279655\n",
            "--------------------------------------------------\n",
            "Iteration: 252/300 - True epochs: 150\n",
            "Global Update 252: Test Loss: 2.431049838, Test Accuracy: 44.150\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [253]: Training Loss: 1.329111752, Training Accuracy: 61.864\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.857764\n",
            "--------------------------------------------------\n",
            "Iteration: 253/300 - True epochs: 150\n",
            "Global Update 253: Test Loss: 2.460643409, Test Accuracy: 43.810\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [254]: Training Loss: 1.334323222, Training Accuracy: 61.588\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.811579\n",
            "--------------------------------------------------\n",
            "Iteration: 254/300 - True epochs: 150\n",
            "Global Update 254: Test Loss: 2.518360879, Test Accuracy: 42.610\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [255]: Training Loss: 1.331831988, Training Accuracy: 61.460\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.432184\n",
            "--------------------------------------------------\n",
            "Iteration: 255/300 - True epochs: 150\n",
            "Global Update 255: Test Loss: 2.519038824, Test Accuracy: 42.780\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [256]: Training Loss: 1.315746873, Training Accuracy: 62.348\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.350134\n",
            "--------------------------------------------------\n",
            "Iteration: 256/300 - True epochs: 150\n",
            "Global Update 256: Test Loss: 2.394013148, Test Accuracy: 43.350\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [257]: Training Loss: 1.328395953, Training Accuracy: 61.780\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:13.203979\n",
            "--------------------------------------------------\n",
            "Iteration: 257/300 - True epochs: 150\n",
            "Global Update 257: Test Loss: 2.472336172, Test Accuracy: 44.140\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [258]: Training Loss: 1.302845412, Training Accuracy: 62.492\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:13.064220\n",
            "--------------------------------------------------\n",
            "Iteration: 258/300 - True epochs: 150\n",
            "Global Update 258: Test Loss: 2.400246326, Test Accuracy: 43.920\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [259]: Training Loss: 1.327192293, Training Accuracy: 61.940\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.520988\n",
            "--------------------------------------------------\n",
            "Iteration: 259/300 - True epochs: 150\n",
            "Global Update 259: Test Loss: 2.403335969, Test Accuracy: 43.670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [260]: Training Loss: 1.327986487, Training Accuracy: 61.724\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:13.133898\n",
            "--------------------------------------------------\n",
            "Iteration: 260/300 - True epochs: 150\n",
            "Global Update 260: Test Loss: 2.544368063, Test Accuracy: 42.820\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [261]: Training Loss: 1.329326440, Training Accuracy: 61.668\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.525032\n",
            "--------------------------------------------------\n",
            "Iteration: 261/300 - True epochs: 150\n",
            "Global Update 261: Test Loss: 2.543079523, Test Accuracy: 42.640\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [262]: Training Loss: 1.313712779, Training Accuracy: 62.112\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.458423\n",
            "--------------------------------------------------\n",
            "Iteration: 262/300 - True epochs: 150\n",
            "Global Update 262: Test Loss: 2.474827690, Test Accuracy: 43.690\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [263]: Training Loss: 1.301865341, Training Accuracy: 62.632\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:13.162564\n",
            "--------------------------------------------------\n",
            "Iteration: 263/300 - True epochs: 150\n",
            "Global Update 263: Test Loss: 2.515361795, Test Accuracy: 43.410\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [264]: Training Loss: 1.321624122, Training Accuracy: 61.816\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:13.058591\n",
            "--------------------------------------------------\n",
            "Iteration: 264/300 - True epochs: 150\n",
            "Global Update 264: Test Loss: 2.455473494, Test Accuracy: 43.240\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [265]: Training Loss: 1.280449599, Training Accuracy: 62.956\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.510327\n",
            "--------------------------------------------------\n",
            "Iteration: 265/300 - True epochs: 150\n",
            "Global Update 265: Test Loss: 2.543858350, Test Accuracy: 41.990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [266]: Training Loss: 1.310851176, Training Accuracy: 62.256\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.442880\n",
            "--------------------------------------------------\n",
            "Iteration: 266/300 - True epochs: 150\n",
            "Global Update 266: Test Loss: 2.574978137, Test Accuracy: 42.880\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [267]: Training Loss: 1.329969862, Training Accuracy: 62.296\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.177221\n",
            "--------------------------------------------------\n",
            "Iteration: 267/300 - True epochs: 150\n",
            "Global Update 267: Test Loss: 2.513228848, Test Accuracy: 42.430\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [268]: Training Loss: 1.305603458, Training Accuracy: 62.584\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.495413\n",
            "--------------------------------------------------\n",
            "Iteration: 268/300 - True epochs: 150\n",
            "Global Update 268: Test Loss: 2.478035458, Test Accuracy: 42.510\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [269]: Training Loss: 1.295728157, Training Accuracy: 62.336\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.201445\n",
            "--------------------------------------------------\n",
            "Iteration: 269/300 - True epochs: 150\n",
            "Global Update 269: Test Loss: 2.519204326, Test Accuracy: 43.020\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [270]: Training Loss: 1.294166140, Training Accuracy: 62.676\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.949895\n",
            "--------------------------------------------------\n",
            "Iteration: 270/300 - True epochs: 150\n",
            "Global Update 270: Test Loss: 2.468178606, Test Accuracy: 42.930\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [271]: Training Loss: 1.297582550, Training Accuracy: 62.452\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.274169\n",
            "--------------------------------------------------\n",
            "Iteration: 271/300 - True epochs: 150\n",
            "Global Update 271: Test Loss: 2.541739485, Test Accuracy: 42.680\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [272]: Training Loss: 1.298040869, Training Accuracy: 62.208\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.783111\n",
            "--------------------------------------------------\n",
            "Iteration: 272/300 - True epochs: 150\n",
            "Global Update 272: Test Loss: 2.505008330, Test Accuracy: 42.760\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [273]: Training Loss: 1.301338982, Training Accuracy: 62.472\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.443528\n",
            "--------------------------------------------------\n",
            "Iteration: 273/300 - True epochs: 150\n",
            "Global Update 273: Test Loss: 2.524549368, Test Accuracy: 41.740\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [274]: Training Loss: 1.294157809, Training Accuracy: 62.632\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.796996\n",
            "--------------------------------------------------\n",
            "Iteration: 274/300 - True epochs: 150\n",
            "Global Update 274: Test Loss: 2.538432957, Test Accuracy: 43.840\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [275]: Training Loss: 1.299017358, Training Accuracy: 62.352\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.225357\n",
            "--------------------------------------------------\n",
            "Iteration: 275/300 - True epochs: 150\n",
            "Global Update 275: Test Loss: 2.547148686, Test Accuracy: 42.200\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [276]: Training Loss: 1.303310401, Training Accuracy: 62.604\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.925895\n",
            "--------------------------------------------------\n",
            "Iteration: 276/300 - True epochs: 150\n",
            "Global Update 276: Test Loss: 2.460141411, Test Accuracy: 43.050\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [277]: Training Loss: 1.260608439, Training Accuracy: 63.280\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.807157\n",
            "--------------------------------------------------\n",
            "Iteration: 277/300 - True epochs: 150\n",
            "Global Update 277: Test Loss: 2.473756173, Test Accuracy: 43.060\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [278]: Training Loss: 1.284778800, Training Accuracy: 63.284\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.251010\n",
            "--------------------------------------------------\n",
            "Iteration: 278/300 - True epochs: 150\n",
            "Global Update 278: Test Loss: 2.454613088, Test Accuracy: 44.070\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [279]: Training Loss: 1.269870223, Training Accuracy: 63.252\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.888548\n",
            "--------------------------------------------------\n",
            "Iteration: 279/300 - True epochs: 150\n",
            "Global Update 279: Test Loss: 2.536650646, Test Accuracy: 42.900\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [280]: Training Loss: 1.299750953, Training Accuracy: 62.716\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.658567\n",
            "--------------------------------------------------\n",
            "Iteration: 280/300 - True epochs: 150\n",
            "Global Update 280: Test Loss: 2.475026128, Test Accuracy: 43.400\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [281]: Training Loss: 1.274883219, Training Accuracy: 63.040\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.975002\n",
            "--------------------------------------------------\n",
            "Iteration: 281/300 - True epochs: 150\n",
            "Global Update 281: Test Loss: 2.493293676, Test Accuracy: 42.910\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [282]: Training Loss: 1.273760621, Training Accuracy: 63.272\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.854072\n",
            "--------------------------------------------------\n",
            "Iteration: 282/300 - True epochs: 150\n",
            "Global Update 282: Test Loss: 2.541708389, Test Accuracy: 42.090\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [283]: Training Loss: 1.266272209, Training Accuracy: 63.436\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.045461\n",
            "--------------------------------------------------\n",
            "Iteration: 283/300 - True epochs: 150\n",
            "Global Update 283: Test Loss: 2.580619010, Test Accuracy: 43.620\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [284]: Training Loss: 1.266989800, Training Accuracy: 62.912\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.885480\n",
            "--------------------------------------------------\n",
            "Iteration: 284/300 - True epochs: 150\n",
            "Global Update 284: Test Loss: 2.486130868, Test Accuracy: 42.870\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [285]: Training Loss: 1.271562007, Training Accuracy: 63.288\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.977639\n",
            "--------------------------------------------------\n",
            "Iteration: 285/300 - True epochs: 150\n",
            "Global Update 285: Test Loss: 2.564394438, Test Accuracy: 42.680\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [286]: Training Loss: 1.271506412, Training Accuracy: 63.244\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.713164\n",
            "--------------------------------------------------\n",
            "Iteration: 286/300 - True epochs: 150\n",
            "Global Update 286: Test Loss: 2.472968848, Test Accuracy: 42.860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [287]: Training Loss: 1.257168181, Training Accuracy: 63.748\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.153228\n",
            "--------------------------------------------------\n",
            "Iteration: 287/300 - True epochs: 150\n",
            "Global Update 287: Test Loss: 2.488573174, Test Accuracy: 43.630\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [288]: Training Loss: 1.278343910, Training Accuracy: 62.928\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.174524\n",
            "--------------------------------------------------\n",
            "Iteration: 288/300 - True epochs: 150\n",
            "Global Update 288: Test Loss: 2.505949881, Test Accuracy: 43.230\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [289]: Training Loss: 1.275910571, Training Accuracy: 63.580\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.416260\n",
            "--------------------------------------------------\n",
            "Iteration: 289/300 - True epochs: 150\n",
            "Global Update 289: Test Loss: 2.562177490, Test Accuracy: 42.470\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [290]: Training Loss: 1.270410756, Training Accuracy: 63.060\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.133510\n",
            "--------------------------------------------------\n",
            "Iteration: 290/300 - True epochs: 150\n",
            "Global Update 290: Test Loss: 2.478066599, Test Accuracy: 43.540\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [291]: Training Loss: 1.274037694, Training Accuracy: 63.136\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.484247\n",
            "--------------------------------------------------\n",
            "Iteration: 291/300 - True epochs: 150\n",
            "Global Update 291: Test Loss: 2.440995921, Test Accuracy: 43.900\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [292]: Training Loss: 1.259451986, Training Accuracy: 63.308\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.760285\n",
            "--------------------------------------------------\n",
            "Iteration: 292/300 - True epochs: 150\n",
            "Global Update 292: Test Loss: 2.444358596, Test Accuracy: 43.870\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [293]: Training Loss: 1.257389098, Training Accuracy: 63.492\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.102662\n",
            "--------------------------------------------------\n",
            "Iteration: 293/300 - True epochs: 150\n",
            "Global Update 293: Test Loss: 2.524437093, Test Accuracy: 42.710\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [294]: Training Loss: 1.277252541, Training Accuracy: 62.984\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.492015\n",
            "--------------------------------------------------\n",
            "Iteration: 294/300 - True epochs: 150\n",
            "Global Update 294: Test Loss: 2.517949169, Test Accuracy: 43.270\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [295]: Training Loss: 1.276354654, Training Accuracy: 63.316\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.621122\n",
            "--------------------------------------------------\n",
            "Iteration: 295/300 - True epochs: 150\n",
            "Global Update 295: Test Loss: 2.503501053, Test Accuracy: 43.530\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [296]: Training Loss: 1.271649374, Training Accuracy: 63.304\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.706540\n",
            "--------------------------------------------------\n",
            "Iteration: 296/300 - True epochs: 150\n",
            "Global Update 296: Test Loss: 2.477577879, Test Accuracy: 42.770\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [297]: Training Loss: 1.241776460, Training Accuracy: 64.196\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.097135\n",
            "--------------------------------------------------\n",
            "Iteration: 297/300 - True epochs: 150\n",
            "Global Update 297: Test Loss: 2.575617622, Test Accuracy: 42.240\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [298]: Training Loss: 1.239890515, Training Accuracy: 63.756\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.312851\n",
            "--------------------------------------------------\n",
            "Iteration: 298/300 - True epochs: 150\n",
            "Global Update 298: Test Loss: 2.572263329, Test Accuracy: 41.160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [299]: Training Loss: 1.254970633, Training Accuracy: 63.860\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:11.410192\n",
            "--------------------------------------------------\n",
            "Iteration: 299/300 - True epochs: 150\n",
            "Global Update 299: Test Loss: 2.511494826, Test Accuracy: 43.980\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Worker 2, [300]: Training Loss: 1.256832471, Training Accuracy: 63.872\n",
            "**************************************************\n",
            "Time taken for worker 2 : 0:00:12.439738\n",
            "--------------------------------------------------\n",
            "Iteration: 300/300 - True epochs: 150\n",
            "Global Update 300: Test Loss: 2.560581903, Test Accuracy: 42.860\n",
            "--------------------------------------------------\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for SHAT: 1:05:17.087721\n",
            "//////////////////////////////////////////////////\n"
          ]
        }
      ],
      "source": [
        "# SHAT\n",
        "lr = 1e-02\n",
        "wd = 1e-03 \n",
        "K = [2]\n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  print('='*50)\n",
        "  print(f'Number of Workers:{k}')\n",
        "  print('='*50)\n",
        "  SHAT_PS(shard_loaders, loss_fn, k, lr, wd, initial_state_dict, num_epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def LocalAdaScale(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs):\n",
        "  total_start_time = time.time()\n",
        "\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "  # global_optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "  # Initialize a scheduler for each optimizer\n",
        "  # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for loca_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    # global_model = synchronize(global_model, local_models, 1)\n",
        "    global_model = synchronize(local_models)\n",
        "\n",
        "    # for local_model in local_models:\n",
        "    #   local_model.load_state_dict(global_model.state_dict())\n",
        "    # scheduler.step()\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Local Step {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "# TODO: Schedule ro check konim \n",
        "\n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for local_SGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Our Approach 1: Layerwise Masking Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "def lock_conv_and_fc_layers(model, ratio):\n",
        "    \"\"\"\n",
        "    Locks the last `ratio` portion of the Conv2d and Linear layers in the model.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The model whose layers you want to lock.\n",
        "    - ratio (float): The ratio of Conv2d and Linear layers to lock. For example, 0.25 will lock the last 25% of such layers.\n",
        "    \"\"\"\n",
        "    # Flatten the model layers into a list (recursively)\n",
        "    layers = []\n",
        "    def get_layers(module):\n",
        "        for layer in module.children():\n",
        "            if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "                layers.append(layer)\n",
        "            if len(list(layer.children())) > 0:  # If the layer has sub-layers\n",
        "                get_layers(layer)\n",
        "    \n",
        "    get_layers(model)\n",
        "    num_layers = len(layers)\n",
        "    \n",
        "    # Calculate the number of layers to lock\n",
        "    layers_to_lock = int(ratio * num_layers)\n",
        "    if ratio * num_layers != layers_to_lock:  # If not an exact integer, round up\n",
        "        layers_to_lock += 1\n",
        "\n",
        "            \n",
        "    # Lock the last `layers_to_lock` Conv2d and Linear layers\n",
        "    count = 0\n",
        "    for layer in reversed(layers):  # Reverse the layers to lock the last ones\n",
        "        if count < layers_to_lock:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "            count += 1\n",
        "        else:\n",
        "            break\n",
        "    return layers_to_lock\n",
        "\n",
        "def synchronize(models): # average models over only unlocked parameters\n",
        "    for params in zip(*[model.parameters() for model in models]):\n",
        "        # Filter out the parameters that are locked (i.e., requires_grad is False)\n",
        "        unlocked_params = [param.data for param in params if param.requires_grad]\n",
        "        \n",
        "        # If there are any unlocked parameters, average them\n",
        "        if unlocked_params:\n",
        "            param_avg = torch.mean(torch.stack(unlocked_params), dim=0)\n",
        "            for param in params:\n",
        "                if param.requires_grad:  # Update only the unlocked parameters\n",
        "                    param.data = param_avg\n",
        "    \n",
        "    return models[0]\n",
        "\n",
        "class LMOptimizer(Optimizer):\n",
        "    def __init__(self, global_model, lr=0.01):\n",
        "        self.global_model = global_model\n",
        "        self.lr = lr\n",
        "        params = list(global_model.parameters())\n",
        "        super(LMOptimizer, self).__init__(params, {'lr': lr})\n",
        "\n",
        "    def step(self, local_models):\n",
        "        # Get global model parameters\n",
        "        global_params = list(self.global_model.parameters())\n",
        "        \n",
        "        # Initialize deltas for each parameter, same size as global parameters\n",
        "        deltas = [torch.zeros_like(param) for param in global_params]\n",
        "        \n",
        "        # Track the count of active local models for each parameter\n",
        "        active_model_counts = [0] * len(global_params)\n",
        "        \n",
        "        # Sum up differences between global model and active local models\n",
        "        for local_model in local_models:\n",
        "            local_params = list(local_model.parameters())\n",
        "            for i, param in enumerate(local_params):\n",
        "                if param.requires_grad:  # Only consider active (unlocked) parameters\n",
        "                    deltas[i] += (global_params[i] - param)\n",
        "                    active_model_counts[i] += 1  # Count this local model as active for this parameter\n",
        "        \n",
        "        # Average the delta over the number of active local models for each parameter\n",
        "        for i, delta in enumerate(deltas):\n",
        "            if active_model_counts[i] > 0:  # Only update if there are active models\n",
        "                deltas[i] /= self.lr\n",
        "                deltas[i] /= active_model_counts[i]\n",
        "        \n",
        "        # Update global model parameters\n",
        "        with torch.no_grad():\n",
        "            for i, param in enumerate(global_params):\n",
        "                if active_model_counts[i] > 0:  # Only update active parameters\n",
        "                    param.copy_(param - self.lr * deltas[i])\n",
        "        \n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def LayerwiseMaskingApproach(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, warmups):\n",
        "  total_start_time = time.time()\n",
        "  print('Start Time:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_start_time)))\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "  global_optimizer = LMOptimizer(global_model, lr=lr)\n",
        "  \n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "  \n",
        "  checkpoint = load_checkpoint('layerwise-masking', 64, {'k': k, 'j': j})\n",
        "  if checkpoint is not None:\n",
        "      global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      global_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      \n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn)\n",
        "      print(f'Global Update: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      \n",
        "      return None\n",
        "  workers_warmup_time = {}\n",
        "  print('-'*50)\n",
        "  print('Warmup rounds')\n",
        "  print('-'*50)\n",
        "\n",
        "  for warmup in range(warmups):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for local_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{local_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      time_diff = train_end_time - train_start_time\n",
        "      \n",
        "      # mock time difference for warmup\n",
        "      if worker == 1:\n",
        "        time_diff = workers_warmup_time[0] * 1.5\n",
        "      elif worker == 2:\n",
        "        time_diff = workers_warmup_time[0] * 1.75\n",
        "      elif worker == 3:\n",
        "        time_diff = workers_warmup_time[0] * 2\n",
        "        \n",
        "      workers_warmup_time[worker] = time_diff      \n",
        "      print(f'Time taken for warmup {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "\n",
        "  print(f'workers_warmup_time: {workers_warmup_time}')\n",
        "  print('Warmup finished')\n",
        "  print('-'*50)\n",
        "\n",
        "  min_time = min(workers_warmup_time.values())\n",
        "  workers_warmup_time_ratio = {worker:  (1-(min_time/time)) for worker, time in workers_warmup_time.items()}\n",
        "  print(f'scaled_workers_warmup_time: {workers_warmup_time_ratio}')\n",
        "\n",
        "  # lock models based on ratio of computation power calculated in warmup phase\n",
        "  for worker, local_model in enumerate(local_models):\n",
        "    locked_layers = lock_conv_and_fc_layers(local_model, workers_warmup_time_ratio[worker])\n",
        "\n",
        "    print(f'Worker {worker+1} - Locked last layers : {locked_layers}') \n",
        "  \n",
        "  for iteration in range(iterations):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for local_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{local_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    global_optimizer.step(local_models)\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "    for local_model in local_models:\n",
        "       local_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Global Update {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "  \n",
        "  # Save checkpoint\n",
        "  save_checkpoint({\n",
        "              'epoch': num_epochs,\n",
        "              'model_state_dict': global_model.state_dict(),\n",
        "              'optimizer_state_dict': global_optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss': loss_fn\n",
        "              }, 149, 64, 'layerwise-masking', {'k': k, 'j': j})  \n",
        "\n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for HeteroCompSGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:4\n",
            "==================================================\n",
            "Start Time: 2024-09-10 00:57:27\n",
            "--------------------------------------------------\n",
            "Warmup rounds\n",
            "--------------------------------------------------\n",
            "Worker 1, [01/04]: Training Loss: 4.515324364, Training Accuracy: 2.424\n",
            "Worker 1, [02/04]: Training Loss: 4.120817585, Training Accuracy: 6.840\n",
            "Worker 1, [03/04]: Training Loss: 3.916717316, Training Accuracy: 9.264\n",
            "Worker 1, [04/04]: Training Loss: 3.746108011, Training Accuracy: 11.808\n",
            "Time taken for warmup 1: 0:00:23.109754\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.518111667, Training Accuracy: 2.360\n",
            "Worker 2, [02/04]: Training Loss: 4.137024254, Training Accuracy: 6.288\n",
            "Worker 2, [03/04]: Training Loss: 3.903139393, Training Accuracy: 9.568\n",
            "Worker 2, [04/04]: Training Loss: 3.747285134, Training Accuracy: 11.840\n",
            "Time taken for warmup 2: 0:00:23.026878\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 4.502692269, Training Accuracy: 2.504\n",
            "Worker 3, [02/04]: Training Loss: 4.093973903, Training Accuracy: 6.848\n",
            "Worker 3, [03/04]: Training Loss: 3.888114960, Training Accuracy: 9.800\n",
            "Worker 3, [04/04]: Training Loss: 3.731901080, Training Accuracy: 11.984\n",
            "Time taken for warmup 3: 0:00:23.304460\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 4.509439320, Training Accuracy: 2.192\n",
            "Worker 4, [02/04]: Training Loss: 4.116653271, Training Accuracy: 6.616\n",
            "Worker 4, [03/04]: Training Loss: 3.909165985, Training Accuracy: 9.056\n",
            "Worker 4, [04/04]: Training Loss: 3.741253990, Training Accuracy: 11.640\n",
            "Time taken for warmup 4: 0:00:23.389056\n",
            "--------------------------------------------------\n",
            "workers_warmup_time: {0: 23.109753847122192, 1: 34.66463077068329, 2: 40.44206923246384, 3: 46.219507694244385}\n",
            "Warmup finished\n",
            "--------------------------------------------------\n",
            "scaled_workers_warmup_time: {0: 0.0, 1: 0.33333333333333337, 2: 0.4285714285714286, 3: 0.5}\n",
            "Worker 1 - Locked last layers : 0\n",
            "Worker 2 - Locked last layers : 2\n",
            "Worker 3 - Locked last layers : 3\n",
            "Worker 4 - Locked last layers : 3\n",
            "Worker 1, [01/04]: Training Loss: 3.629623530, Training Accuracy: 13.680\n",
            "Worker 1, [02/04]: Training Loss: 3.491228938, Training Accuracy: 16.184\n",
            "Worker 1, [03/04]: Training Loss: 3.363798717, Training Accuracy: 18.704\n",
            "Worker 1, [04/04]: Training Loss: 3.249348688, Training Accuracy: 20.128\n",
            "Time taken for training worker 1: 0:00:23.399304\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.600569790, Training Accuracy: 14.160\n",
            "Worker 2, [02/04]: Training Loss: 3.520281758, Training Accuracy: 15.840\n",
            "Worker 2, [03/04]: Training Loss: 3.459108729, Training Accuracy: 16.768\n",
            "Worker 2, [04/04]: Training Loss: 3.402321907, Training Accuracy: 17.872\n",
            "Time taken for training worker 2: 0:00:24.360008\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.670789788, Training Accuracy: 12.312\n",
            "Worker 3, [02/04]: Training Loss: 3.624234915, Training Accuracy: 13.072\n",
            "Worker 3, [03/04]: Training Loss: 3.619186165, Training Accuracy: 13.024\n",
            "Worker 3, [04/04]: Training Loss: 3.586114061, Training Accuracy: 13.496\n",
            "Time taken for training worker 3: 0:00:23.753531\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.712531771, Training Accuracy: 11.632\n",
            "Worker 4, [02/04]: Training Loss: 3.679469272, Training Accuracy: 12.480\n",
            "Worker 4, [03/04]: Training Loss: 3.651832744, Training Accuracy: 12.400\n",
            "Worker 4, [04/04]: Training Loss: 3.644296874, Training Accuracy: 12.304\n",
            "Time taken for training worker 4: 0:00:23.249321\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.010664\n",
            "Global Update 01: Test Loss: 3.428147831, Test Accuracy: 18.470\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.286558943, Training Accuracy: 19.616\n",
            "Worker 1, [02/04]: Training Loss: 3.143627389, Training Accuracy: 21.776\n",
            "Worker 1, [03/04]: Training Loss: 3.059061278, Training Accuracy: 23.872\n",
            "Worker 1, [04/04]: Training Loss: 2.987463057, Training Accuracy: 25.400\n",
            "Time taken for training worker 1: 0:00:24.194330\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.356038883, Training Accuracy: 18.600\n",
            "Worker 2, [02/04]: Training Loss: 3.252782863, Training Accuracy: 20.256\n",
            "Worker 2, [03/04]: Training Loss: 3.185217165, Training Accuracy: 21.928\n",
            "Worker 2, [04/04]: Training Loss: 3.110024255, Training Accuracy: 22.800\n",
            "Time taken for training worker 2: 0:00:23.879056\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.366092459, Training Accuracy: 18.720\n",
            "Worker 3, [02/04]: Training Loss: 3.306685296, Training Accuracy: 19.856\n",
            "Worker 3, [03/04]: Training Loss: 3.280343299, Training Accuracy: 20.368\n",
            "Worker 3, [04/04]: Training Loss: 3.271180813, Training Accuracy: 20.712\n",
            "Time taken for training worker 3: 0:00:23.712334\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.374999009, Training Accuracy: 18.496\n",
            "Worker 4, [02/04]: Training Loss: 3.312718270, Training Accuracy: 19.632\n",
            "Worker 4, [03/04]: Training Loss: 3.289481528, Training Accuracy: 20.168\n",
            "Worker 4, [04/04]: Training Loss: 3.278447682, Training Accuracy: 20.296\n",
            "Time taken for training worker 4: 0:00:23.832658\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002479\n",
            "Global Update 02: Test Loss: 3.234721314, Test Accuracy: 24.220\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.984909930, Training Accuracy: 25.584\n",
            "Worker 1, [02/04]: Training Loss: 2.857902955, Training Accuracy: 27.296\n",
            "Worker 1, [03/04]: Training Loss: 2.772728420, Training Accuracy: 29.352\n",
            "Worker 1, [04/04]: Training Loss: 2.737800855, Training Accuracy: 29.952\n",
            "Time taken for training worker 1: 0:00:23.891972\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.078024407, Training Accuracy: 24.296\n",
            "Worker 2, [02/04]: Training Loss: 3.007265862, Training Accuracy: 25.016\n",
            "Worker 2, [03/04]: Training Loss: 2.926728745, Training Accuracy: 26.680\n",
            "Worker 2, [04/04]: Training Loss: 2.890778121, Training Accuracy: 27.040\n",
            "Time taken for training worker 2: 0:00:24.146157\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.107417574, Training Accuracy: 23.488\n",
            "Worker 3, [02/04]: Training Loss: 3.046130790, Training Accuracy: 25.024\n",
            "Worker 3, [03/04]: Training Loss: 3.038096554, Training Accuracy: 24.920\n",
            "Worker 3, [04/04]: Training Loss: 3.032391501, Training Accuracy: 24.800\n",
            "Time taken for training worker 3: 0:00:23.839038\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.103047392, Training Accuracy: 23.640\n",
            "Worker 4, [02/04]: Training Loss: 3.071065656, Training Accuracy: 23.920\n",
            "Worker 4, [03/04]: Training Loss: 3.031689854, Training Accuracy: 25.128\n",
            "Worker 4, [04/04]: Training Loss: 3.004662307, Training Accuracy: 25.288\n",
            "Time taken for training worker 4: 0:00:24.219979\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.009655\n",
            "Global Update 03: Test Loss: 2.780726079, Test Accuracy: 30.460\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.699481267, Training Accuracy: 30.344\n",
            "Worker 1, [02/04]: Training Loss: 2.616966546, Training Accuracy: 32.424\n",
            "Worker 1, [03/04]: Training Loss: 2.555979703, Training Accuracy: 33.656\n",
            "Worker 1, [04/04]: Training Loss: 2.471749100, Training Accuracy: 34.800\n",
            "Time taken for training worker 1: 0:00:25.763750\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.884777421, Training Accuracy: 28.024\n",
            "Worker 2, [02/04]: Training Loss: 2.806284160, Training Accuracy: 29.696\n",
            "Worker 2, [03/04]: Training Loss: 2.739924141, Training Accuracy: 30.664\n",
            "Worker 2, [04/04]: Training Loss: 2.698584987, Training Accuracy: 31.760\n",
            "Time taken for training worker 2: 0:00:23.780483\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.912498890, Training Accuracy: 27.592\n",
            "Worker 3, [02/04]: Training Loss: 2.890189042, Training Accuracy: 28.344\n",
            "Worker 3, [03/04]: Training Loss: 2.877910536, Training Accuracy: 28.632\n",
            "Worker 3, [04/04]: Training Loss: 2.829624740, Training Accuracy: 29.032\n",
            "Time taken for training worker 3: 0:00:25.114278\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.929754044, Training Accuracy: 27.168\n",
            "Worker 4, [02/04]: Training Loss: 2.877741688, Training Accuracy: 27.960\n",
            "Worker 4, [03/04]: Training Loss: 2.878532534, Training Accuracy: 28.040\n",
            "Worker 4, [04/04]: Training Loss: 2.855135234, Training Accuracy: 28.528\n",
            "Time taken for training worker 4: 0:00:23.844606\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002671\n",
            "Global Update 04: Test Loss: 2.634995391, Test Accuracy: 33.630\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.483706340, Training Accuracy: 35.088\n",
            "Worker 1, [02/04]: Training Loss: 2.425278957, Training Accuracy: 36.088\n",
            "Worker 1, [03/04]: Training Loss: 2.375368916, Training Accuracy: 37.232\n",
            "Worker 1, [04/04]: Training Loss: 2.320760198, Training Accuracy: 38.072\n",
            "Time taken for training worker 1: 0:00:25.522019\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.750951136, Training Accuracy: 30.544\n",
            "Worker 2, [02/04]: Training Loss: 2.686722614, Training Accuracy: 31.776\n",
            "Worker 2, [03/04]: Training Loss: 2.633291474, Training Accuracy: 33.120\n",
            "Worker 2, [04/04]: Training Loss: 2.589543217, Training Accuracy: 34.072\n",
            "Time taken for training worker 2: 0:00:25.030627\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.796286228, Training Accuracy: 30.408\n",
            "Worker 3, [02/04]: Training Loss: 2.767980712, Training Accuracy: 30.712\n",
            "Worker 3, [03/04]: Training Loss: 2.743569419, Training Accuracy: 31.112\n",
            "Worker 3, [04/04]: Training Loss: 2.725110599, Training Accuracy: 31.672\n",
            "Time taken for training worker 3: 0:00:24.938687\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.812392626, Training Accuracy: 29.224\n",
            "Worker 4, [02/04]: Training Loss: 2.781201526, Training Accuracy: 29.688\n",
            "Worker 4, [03/04]: Training Loss: 2.751882965, Training Accuracy: 30.600\n",
            "Worker 4, [04/04]: Training Loss: 2.728880270, Training Accuracy: 31.120\n",
            "Time taken for training worker 4: 0:00:24.057921\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002786\n",
            "Global Update 05: Test Loss: 2.663928117, Test Accuracy: 34.180\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.335017623, Training Accuracy: 37.784\n",
            "Worker 1, [02/04]: Training Loss: 2.272947014, Training Accuracy: 39.168\n",
            "Worker 1, [03/04]: Training Loss: 2.222669205, Training Accuracy: 40.288\n",
            "Worker 1, [04/04]: Training Loss: 2.161688948, Training Accuracy: 41.832\n",
            "Time taken for training worker 1: 0:00:24.567053\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.666359998, Training Accuracy: 32.672\n",
            "Worker 2, [02/04]: Training Loss: 2.578817160, Training Accuracy: 34.544\n",
            "Worker 2, [03/04]: Training Loss: 2.522457865, Training Accuracy: 35.296\n",
            "Worker 2, [04/04]: Training Loss: 2.449183941, Training Accuracy: 36.024\n",
            "Time taken for training worker 2: 0:00:24.496498\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.716595304, Training Accuracy: 31.408\n",
            "Worker 3, [02/04]: Training Loss: 2.705920760, Training Accuracy: 31.872\n",
            "Worker 3, [03/04]: Training Loss: 2.685761155, Training Accuracy: 32.408\n",
            "Worker 3, [04/04]: Training Loss: 2.664631665, Training Accuracy: 32.464\n",
            "Time taken for training worker 3: 0:00:24.973289\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.747675825, Training Accuracy: 31.048\n",
            "Worker 4, [02/04]: Training Loss: 2.731034072, Training Accuracy: 31.392\n",
            "Worker 4, [03/04]: Training Loss: 2.706104406, Training Accuracy: 31.944\n",
            "Worker 4, [04/04]: Training Loss: 2.689804699, Training Accuracy: 32.248\n",
            "Time taken for training worker 4: 0:00:24.014317\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002286\n",
            "Global Update 06: Test Loss: 2.543846873, Test Accuracy: 36.690\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.178472393, Training Accuracy: 41.360\n",
            "Worker 1, [02/04]: Training Loss: 2.131950971, Training Accuracy: 41.776\n",
            "Worker 1, [03/04]: Training Loss: 2.086315459, Training Accuracy: 43.552\n",
            "Worker 1, [04/04]: Training Loss: 2.043878602, Training Accuracy: 43.872\n",
            "Time taken for training worker 1: 0:00:24.528030\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.579895901, Training Accuracy: 34.672\n",
            "Worker 2, [02/04]: Training Loss: 2.495861141, Training Accuracy: 36.288\n",
            "Worker 2, [03/04]: Training Loss: 2.426641350, Training Accuracy: 37.648\n",
            "Worker 2, [04/04]: Training Loss: 2.389319294, Training Accuracy: 38.384\n",
            "Time taken for training worker 2: 0:00:25.658078\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.687859590, Training Accuracy: 32.768\n",
            "Worker 3, [02/04]: Training Loss: 2.663103308, Training Accuracy: 32.712\n",
            "Worker 3, [03/04]: Training Loss: 2.631590139, Training Accuracy: 33.720\n",
            "Worker 3, [04/04]: Training Loss: 2.610130766, Training Accuracy: 34.216\n",
            "Time taken for training worker 3: 0:00:24.801422\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.711242842, Training Accuracy: 31.656\n",
            "Worker 4, [02/04]: Training Loss: 2.664494595, Training Accuracy: 32.720\n",
            "Worker 4, [03/04]: Training Loss: 2.630806961, Training Accuracy: 33.664\n",
            "Worker 4, [04/04]: Training Loss: 2.620726130, Training Accuracy: 33.200\n",
            "Time taken for training worker 4: 0:00:25.561377\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002722\n",
            "Global Update 07: Test Loss: 2.523888144, Test Accuracy: 37.210\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.075822196, Training Accuracy: 43.360\n",
            "Worker 1, [02/04]: Training Loss: 2.002243389, Training Accuracy: 45.416\n",
            "Worker 1, [03/04]: Training Loss: 1.969762288, Training Accuracy: 45.968\n",
            "Worker 1, [04/04]: Training Loss: 1.915479582, Training Accuracy: 46.776\n",
            "Time taken for training worker 1: 0:00:24.749301\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.514141125, Training Accuracy: 36.128\n",
            "Worker 2, [02/04]: Training Loss: 2.432666109, Training Accuracy: 37.256\n",
            "Worker 2, [03/04]: Training Loss: 2.375917270, Training Accuracy: 38.616\n",
            "Worker 2, [04/04]: Training Loss: 2.344528296, Training Accuracy: 39.112\n",
            "Time taken for training worker 2: 0:00:24.307301\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.659300484, Training Accuracy: 32.832\n",
            "Worker 3, [02/04]: Training Loss: 2.604900478, Training Accuracy: 33.912\n",
            "Worker 3, [03/04]: Training Loss: 2.591099978, Training Accuracy: 34.648\n",
            "Worker 3, [04/04]: Training Loss: 2.569146313, Training Accuracy: 35.008\n",
            "Time taken for training worker 3: 0:00:24.423695\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.652342019, Training Accuracy: 32.800\n",
            "Worker 4, [02/04]: Training Loss: 2.631288132, Training Accuracy: 33.672\n",
            "Worker 4, [03/04]: Training Loss: 2.594691522, Training Accuracy: 33.904\n",
            "Worker 4, [04/04]: Training Loss: 2.583605811, Training Accuracy: 34.232\n",
            "Time taken for training worker 4: 0:00:24.033137\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002778\n",
            "Global Update 08: Test Loss: 2.460706963, Test Accuracy: 38.700\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.968675838, Training Accuracy: 46.448\n",
            "Worker 1, [02/04]: Training Loss: 1.925625622, Training Accuracy: 46.272\n",
            "Worker 1, [03/04]: Training Loss: 1.857494097, Training Accuracy: 48.088\n",
            "Worker 1, [04/04]: Training Loss: 1.842659990, Training Accuracy: 49.000\n",
            "Time taken for training worker 1: 0:00:25.274357\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.458073260, Training Accuracy: 37.024\n",
            "Worker 2, [02/04]: Training Loss: 2.399081612, Training Accuracy: 38.424\n",
            "Worker 2, [03/04]: Training Loss: 2.320260179, Training Accuracy: 40.272\n",
            "Worker 2, [04/04]: Training Loss: 2.279743253, Training Accuracy: 40.304\n",
            "Time taken for training worker 2: 0:00:24.321778\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.620475906, Training Accuracy: 34.000\n",
            "Worker 3, [02/04]: Training Loss: 2.590878018, Training Accuracy: 34.456\n",
            "Worker 3, [03/04]: Training Loss: 2.580902496, Training Accuracy: 34.464\n",
            "Worker 3, [04/04]: Training Loss: 2.560714728, Training Accuracy: 35.280\n",
            "Time taken for training worker 3: 0:00:24.949740\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.638388010, Training Accuracy: 33.592\n",
            "Worker 4, [02/04]: Training Loss: 2.607516789, Training Accuracy: 34.032\n",
            "Worker 4, [03/04]: Training Loss: 2.590798337, Training Accuracy: 34.264\n",
            "Worker 4, [04/04]: Training Loss: 2.580204613, Training Accuracy: 34.160\n",
            "Time taken for training worker 4: 0:00:24.172258\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002673\n",
            "Global Update 09: Test Loss: 2.535239934, Test Accuracy: 37.940\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.871170478, Training Accuracy: 47.240\n",
            "Worker 1, [02/04]: Training Loss: 1.813362980, Training Accuracy: 48.944\n",
            "Worker 1, [03/04]: Training Loss: 1.809912649, Training Accuracy: 49.224\n",
            "Worker 1, [04/04]: Training Loss: 1.763354654, Training Accuracy: 50.648\n",
            "Time taken for training worker 1: 0:00:25.553260\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.427652927, Training Accuracy: 37.128\n",
            "Worker 2, [02/04]: Training Loss: 2.345011440, Training Accuracy: 39.336\n",
            "Worker 2, [03/04]: Training Loss: 2.282106762, Training Accuracy: 40.816\n",
            "Worker 2, [04/04]: Training Loss: 2.245850245, Training Accuracy: 41.552\n",
            "Time taken for training worker 2: 0:00:23.813835\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.642287303, Training Accuracy: 33.808\n",
            "Worker 3, [02/04]: Training Loss: 2.564707613, Training Accuracy: 35.176\n",
            "Worker 3, [03/04]: Training Loss: 2.552832265, Training Accuracy: 35.920\n",
            "Worker 3, [04/04]: Training Loss: 2.557463283, Training Accuracy: 35.440\n",
            "Time taken for training worker 3: 0:00:25.654556\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.650345496, Training Accuracy: 33.520\n",
            "Worker 4, [02/04]: Training Loss: 2.591405971, Training Accuracy: 33.952\n",
            "Worker 4, [03/04]: Training Loss: 2.575031633, Training Accuracy: 34.752\n",
            "Worker 4, [04/04]: Training Loss: 2.556103533, Training Accuracy: 35.216\n",
            "Time taken for training worker 4: 0:00:24.098513\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002681\n",
            "Global Update 10: Test Loss: 2.481351144, Test Accuracy: 39.300\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.791266246, Training Accuracy: 49.656\n",
            "Worker 1, [02/04]: Training Loss: 1.740475316, Training Accuracy: 51.312\n",
            "Worker 1, [03/04]: Training Loss: 1.724514928, Training Accuracy: 51.400\n",
            "Worker 1, [04/04]: Training Loss: 1.665714595, Training Accuracy: 52.696\n",
            "Time taken for training worker 1: 0:00:25.397319\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.377551172, Training Accuracy: 38.408\n",
            "Worker 2, [02/04]: Training Loss: 2.279512530, Training Accuracy: 40.512\n",
            "Worker 2, [03/04]: Training Loss: 2.236079810, Training Accuracy: 41.480\n",
            "Worker 2, [04/04]: Training Loss: 2.193151333, Training Accuracy: 42.472\n",
            "Time taken for training worker 2: 0:00:25.247222\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.600140915, Training Accuracy: 34.464\n",
            "Worker 3, [02/04]: Training Loss: 2.544185962, Training Accuracy: 35.600\n",
            "Worker 3, [03/04]: Training Loss: 2.537292712, Training Accuracy: 35.712\n",
            "Worker 3, [04/04]: Training Loss: 2.525377489, Training Accuracy: 35.704\n",
            "Time taken for training worker 3: 0:00:25.989433\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.620681779, Training Accuracy: 33.880\n",
            "Worker 4, [02/04]: Training Loss: 2.578203235, Training Accuracy: 34.368\n",
            "Worker 4, [03/04]: Training Loss: 2.569881224, Training Accuracy: 34.528\n",
            "Worker 4, [04/04]: Training Loss: 2.532381119, Training Accuracy: 35.320\n",
            "Time taken for training worker 4: 0:00:24.114594\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002970\n",
            "Global Update 11: Test Loss: 2.456256255, Test Accuracy: 40.270\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.731743586, Training Accuracy: 51.128\n",
            "Worker 1, [02/04]: Training Loss: 1.673334511, Training Accuracy: 52.560\n",
            "Worker 1, [03/04]: Training Loss: 1.632555361, Training Accuracy: 53.616\n",
            "Worker 1, [04/04]: Training Loss: 1.608571520, Training Accuracy: 53.848\n",
            "Time taken for training worker 1: 0:00:24.309042\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.335316150, Training Accuracy: 39.440\n",
            "Worker 2, [02/04]: Training Loss: 2.273339644, Training Accuracy: 41.288\n",
            "Worker 2, [03/04]: Training Loss: 2.206414116, Training Accuracy: 42.432\n",
            "Worker 2, [04/04]: Training Loss: 2.148561278, Training Accuracy: 43.544\n",
            "Time taken for training worker 2: 0:00:25.031456\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.615794689, Training Accuracy: 34.584\n",
            "Worker 3, [02/04]: Training Loss: 2.572334028, Training Accuracy: 35.104\n",
            "Worker 3, [03/04]: Training Loss: 2.535574214, Training Accuracy: 36.512\n",
            "Worker 3, [04/04]: Training Loss: 2.524686931, Training Accuracy: 36.600\n",
            "Time taken for training worker 3: 0:00:24.398204\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.622956874, Training Accuracy: 34.504\n",
            "Worker 4, [02/04]: Training Loss: 2.580542901, Training Accuracy: 34.496\n",
            "Worker 4, [03/04]: Training Loss: 2.548014272, Training Accuracy: 35.456\n",
            "Worker 4, [04/04]: Training Loss: 2.539023819, Training Accuracy: 35.648\n",
            "Time taken for training worker 4: 0:00:24.653754\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002488\n",
            "Global Update 12: Test Loss: 2.469026942, Test Accuracy: 40.120\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.647934802, Training Accuracy: 53.360\n",
            "Worker 1, [02/04]: Training Loss: 1.605892616, Training Accuracy: 54.480\n",
            "Worker 1, [03/04]: Training Loss: 1.568432088, Training Accuracy: 55.144\n",
            "Worker 1, [04/04]: Training Loss: 1.562368345, Training Accuracy: 55.720\n",
            "Time taken for training worker 1: 0:00:24.417201\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.316973180, Training Accuracy: 40.016\n",
            "Worker 2, [02/04]: Training Loss: 2.233320551, Training Accuracy: 42.144\n",
            "Worker 2, [03/04]: Training Loss: 2.165724133, Training Accuracy: 43.464\n",
            "Worker 2, [04/04]: Training Loss: 2.100415198, Training Accuracy: 44.880\n",
            "Time taken for training worker 2: 0:00:24.074472\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.603845061, Training Accuracy: 34.920\n",
            "Worker 3, [02/04]: Training Loss: 2.547434458, Training Accuracy: 36.512\n",
            "Worker 3, [03/04]: Training Loss: 2.530604292, Training Accuracy: 36.184\n",
            "Worker 3, [04/04]: Training Loss: 2.512113634, Training Accuracy: 36.408\n",
            "Time taken for training worker 3: 0:00:24.026650\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.591118207, Training Accuracy: 34.976\n",
            "Worker 4, [02/04]: Training Loss: 2.551451007, Training Accuracy: 35.656\n",
            "Worker 4, [03/04]: Training Loss: 2.540731007, Training Accuracy: 35.936\n",
            "Worker 4, [04/04]: Training Loss: 2.522935746, Training Accuracy: 36.152\n",
            "Time taken for training worker 4: 0:00:24.370152\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002493\n",
            "Global Update 13: Test Loss: 2.392368937, Test Accuracy: 40.950\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.600688899, Training Accuracy: 54.216\n",
            "Worker 1, [02/04]: Training Loss: 1.570842853, Training Accuracy: 55.216\n",
            "Worker 1, [03/04]: Training Loss: 1.521284692, Training Accuracy: 55.792\n",
            "Worker 1, [04/04]: Training Loss: 1.498883583, Training Accuracy: 56.600\n",
            "Time taken for training worker 1: 0:00:24.318138\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.286006441, Training Accuracy: 41.216\n",
            "Worker 2, [02/04]: Training Loss: 2.198197927, Training Accuracy: 42.760\n",
            "Worker 2, [03/04]: Training Loss: 2.150538581, Training Accuracy: 44.080\n",
            "Worker 2, [04/04]: Training Loss: 2.092862034, Training Accuracy: 44.832\n",
            "Time taken for training worker 2: 0:00:25.217740\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.571133468, Training Accuracy: 35.688\n",
            "Worker 3, [02/04]: Training Loss: 2.525582997, Training Accuracy: 36.288\n",
            "Worker 3, [03/04]: Training Loss: 2.511152287, Training Accuracy: 36.528\n",
            "Worker 3, [04/04]: Training Loss: 2.501602794, Training Accuracy: 37.040\n",
            "Time taken for training worker 3: 0:00:24.835038\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.601277092, Training Accuracy: 34.584\n",
            "Worker 4, [02/04]: Training Loss: 2.524102260, Training Accuracy: 36.600\n",
            "Worker 4, [03/04]: Training Loss: 2.514652014, Training Accuracy: 35.984\n",
            "Worker 4, [04/04]: Training Loss: 2.514852920, Training Accuracy: 35.904\n",
            "Time taken for training worker 4: 0:00:24.351673\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002775\n",
            "Global Update 14: Test Loss: 2.424897217, Test Accuracy: 40.880\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.540858259, Training Accuracy: 56.176\n",
            "Worker 1, [02/04]: Training Loss: 1.482970033, Training Accuracy: 57.528\n",
            "Worker 1, [03/04]: Training Loss: 1.492462301, Training Accuracy: 56.688\n",
            "Worker 1, [04/04]: Training Loss: 1.485047315, Training Accuracy: 57.704\n",
            "Time taken for training worker 1: 0:00:25.096242\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.273529586, Training Accuracy: 41.120\n",
            "Worker 2, [02/04]: Training Loss: 2.182990810, Training Accuracy: 43.216\n",
            "Worker 2, [03/04]: Training Loss: 2.141111555, Training Accuracy: 44.112\n",
            "Worker 2, [04/04]: Training Loss: 2.070503150, Training Accuracy: 45.688\n",
            "Time taken for training worker 2: 0:00:25.730916\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.591168360, Training Accuracy: 35.504\n",
            "Worker 3, [02/04]: Training Loss: 2.539031496, Training Accuracy: 35.976\n",
            "Worker 3, [03/04]: Training Loss: 2.532294679, Training Accuracy: 36.376\n",
            "Worker 3, [04/04]: Training Loss: 2.514493408, Training Accuracy: 36.896\n",
            "Time taken for training worker 3: 0:00:24.989654\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.572926422, Training Accuracy: 35.200\n",
            "Worker 4, [02/04]: Training Loss: 2.556815855, Training Accuracy: 36.024\n",
            "Worker 4, [03/04]: Training Loss: 2.540732129, Training Accuracy: 35.928\n",
            "Worker 4, [04/04]: Training Loss: 2.512199511, Training Accuracy: 36.400\n",
            "Time taken for training worker 4: 0:00:25.487104\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002811\n",
            "Global Update 15: Test Loss: 2.473015901, Test Accuracy: 40.430\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.499425184, Training Accuracy: 57.344\n",
            "Worker 1, [02/04]: Training Loss: 1.486041382, Training Accuracy: 57.184\n",
            "Worker 1, [03/04]: Training Loss: 1.441256696, Training Accuracy: 58.664\n",
            "Worker 1, [04/04]: Training Loss: 1.443520159, Training Accuracy: 59.040\n",
            "Time taken for training worker 1: 0:00:25.149286\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.256325151, Training Accuracy: 41.704\n",
            "Worker 2, [02/04]: Training Loss: 2.151885658, Training Accuracy: 43.920\n",
            "Worker 2, [03/04]: Training Loss: 2.129771470, Training Accuracy: 44.520\n",
            "Worker 2, [04/04]: Training Loss: 2.030890083, Training Accuracy: 46.480\n",
            "Time taken for training worker 2: 0:00:25.468589\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.565417386, Training Accuracy: 36.272\n",
            "Worker 3, [02/04]: Training Loss: 2.516308620, Training Accuracy: 36.872\n",
            "Worker 3, [03/04]: Training Loss: 2.495519439, Training Accuracy: 36.816\n",
            "Worker 3, [04/04]: Training Loss: 2.477173194, Training Accuracy: 37.200\n",
            "Time taken for training worker 3: 0:00:24.286835\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.580540467, Training Accuracy: 35.648\n",
            "Worker 4, [02/04]: Training Loss: 2.530325915, Training Accuracy: 36.504\n",
            "Worker 4, [03/04]: Training Loss: 2.503212528, Training Accuracy: 36.624\n",
            "Worker 4, [04/04]: Training Loss: 2.477706033, Training Accuracy: 36.752\n",
            "Time taken for training worker 4: 0:00:25.176782\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002422\n",
            "Global Update 16: Test Loss: 2.450263493, Test Accuracy: 40.960\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.465412142, Training Accuracy: 56.984\n",
            "Worker 1, [02/04]: Training Loss: 1.393200092, Training Accuracy: 59.816\n",
            "Worker 1, [03/04]: Training Loss: 1.408132815, Training Accuracy: 59.272\n",
            "Worker 1, [04/04]: Training Loss: 1.382731437, Training Accuracy: 60.424\n",
            "Time taken for training worker 1: 0:00:25.869172\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.222058434, Training Accuracy: 42.032\n",
            "Worker 2, [02/04]: Training Loss: 2.142242202, Training Accuracy: 44.136\n",
            "Worker 2, [03/04]: Training Loss: 2.092487670, Training Accuracy: 44.992\n",
            "Worker 2, [04/04]: Training Loss: 2.026065274, Training Accuracy: 46.440\n",
            "Time taken for training worker 2: 0:00:25.955590\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.561777941, Training Accuracy: 35.688\n",
            "Worker 3, [02/04]: Training Loss: 2.535005054, Training Accuracy: 35.936\n",
            "Worker 3, [03/04]: Training Loss: 2.500326319, Training Accuracy: 37.280\n",
            "Worker 3, [04/04]: Training Loss: 2.484991137, Training Accuracy: 37.312\n",
            "Time taken for training worker 3: 0:00:23.974506\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.567960180, Training Accuracy: 35.712\n",
            "Worker 4, [02/04]: Training Loss: 2.546112568, Training Accuracy: 35.672\n",
            "Worker 4, [03/04]: Training Loss: 2.510734246, Training Accuracy: 35.920\n",
            "Worker 4, [04/04]: Training Loss: 2.495423361, Training Accuracy: 37.368\n",
            "Time taken for training worker 4: 0:00:25.452214\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002715\n",
            "Global Update 17: Test Loss: 2.425823553, Test Accuracy: 41.720\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.427125531, Training Accuracy: 58.720\n",
            "Worker 1, [02/04]: Training Loss: 1.381338865, Training Accuracy: 60.208\n",
            "Worker 1, [03/04]: Training Loss: 1.380677987, Training Accuracy: 60.288\n",
            "Worker 1, [04/04]: Training Loss: 1.332806839, Training Accuracy: 61.368\n",
            "Time taken for training worker 1: 0:00:25.529337\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.174806904, Training Accuracy: 43.200\n",
            "Worker 2, [02/04]: Training Loss: 2.120557420, Training Accuracy: 44.624\n",
            "Worker 2, [03/04]: Training Loss: 2.041549972, Training Accuracy: 46.656\n",
            "Worker 2, [04/04]: Training Loss: 2.012371902, Training Accuracy: 47.016\n",
            "Time taken for training worker 2: 0:00:23.817091\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.563301726, Training Accuracy: 36.360\n",
            "Worker 3, [02/04]: Training Loss: 2.522043923, Training Accuracy: 37.064\n",
            "Worker 3, [03/04]: Training Loss: 2.507069753, Training Accuracy: 36.832\n",
            "Worker 3, [04/04]: Training Loss: 2.481329101, Training Accuracy: 38.096\n",
            "Time taken for training worker 3: 0:00:25.374404\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.554348187, Training Accuracy: 36.552\n",
            "Worker 4, [02/04]: Training Loss: 2.517318535, Training Accuracy: 36.632\n",
            "Worker 4, [03/04]: Training Loss: 2.474732298, Training Accuracy: 37.272\n",
            "Worker 4, [04/04]: Training Loss: 2.491387070, Training Accuracy: 37.104\n",
            "Time taken for training worker 4: 0:00:24.353098\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002812\n",
            "Global Update 18: Test Loss: 2.384620252, Test Accuracy: 42.080\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.359634285, Training Accuracy: 60.360\n",
            "Worker 1, [02/04]: Training Loss: 1.369354352, Training Accuracy: 60.216\n",
            "Worker 1, [03/04]: Training Loss: 1.329494874, Training Accuracy: 60.872\n",
            "Worker 1, [04/04]: Training Loss: 1.292966984, Training Accuracy: 62.328\n",
            "Time taken for training worker 1: 0:00:26.324919\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.179800178, Training Accuracy: 43.232\n",
            "Worker 2, [02/04]: Training Loss: 2.100817853, Training Accuracy: 44.800\n",
            "Worker 2, [03/04]: Training Loss: 2.031035591, Training Accuracy: 46.712\n",
            "Worker 2, [04/04]: Training Loss: 1.987156751, Training Accuracy: 47.056\n",
            "Time taken for training worker 2: 0:00:24.392665\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.529810475, Training Accuracy: 37.224\n",
            "Worker 3, [02/04]: Training Loss: 2.497790796, Training Accuracy: 37.056\n",
            "Worker 3, [03/04]: Training Loss: 2.470471824, Training Accuracy: 37.504\n",
            "Worker 3, [04/04]: Training Loss: 2.469716873, Training Accuracy: 37.616\n",
            "Time taken for training worker 3: 0:00:24.081921\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.556356183, Training Accuracy: 36.088\n",
            "Worker 4, [02/04]: Training Loss: 2.513016700, Training Accuracy: 37.104\n",
            "Worker 4, [03/04]: Training Loss: 2.484249159, Training Accuracy: 37.024\n",
            "Worker 4, [04/04]: Training Loss: 2.474630589, Training Accuracy: 37.600\n",
            "Time taken for training worker 4: 0:00:25.462811\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002318\n",
            "Global Update 19: Test Loss: 2.395134394, Test Accuracy: 42.030\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.342415861, Training Accuracy: 60.656\n",
            "Worker 1, [02/04]: Training Loss: 1.347202776, Training Accuracy: 60.744\n",
            "Worker 1, [03/04]: Training Loss: 1.332739031, Training Accuracy: 61.712\n",
            "Worker 1, [04/04]: Training Loss: 1.242175641, Training Accuracy: 64.392\n",
            "Time taken for training worker 1: 0:00:26.468324\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.158894323, Training Accuracy: 43.440\n",
            "Worker 2, [02/04]: Training Loss: 2.053378220, Training Accuracy: 46.496\n",
            "Worker 2, [03/04]: Training Loss: 2.004855345, Training Accuracy: 47.400\n",
            "Worker 2, [04/04]: Training Loss: 1.957900233, Training Accuracy: 48.576\n",
            "Time taken for training worker 2: 0:00:26.090472\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.574797884, Training Accuracy: 36.200\n",
            "Worker 3, [02/04]: Training Loss: 2.494713117, Training Accuracy: 37.712\n",
            "Worker 3, [03/04]: Training Loss: 2.464750835, Training Accuracy: 38.048\n",
            "Worker 3, [04/04]: Training Loss: 2.477471698, Training Accuracy: 37.904\n",
            "Time taken for training worker 3: 0:00:24.046720\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.569766132, Training Accuracy: 35.664\n",
            "Worker 4, [02/04]: Training Loss: 2.507344040, Training Accuracy: 37.000\n",
            "Worker 4, [03/04]: Training Loss: 2.487686110, Training Accuracy: 37.312\n",
            "Worker 4, [04/04]: Training Loss: 2.463662539, Training Accuracy: 37.952\n",
            "Time taken for training worker 4: 0:00:24.030012\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002869\n",
            "Global Update 20: Test Loss: 2.488446514, Test Accuracy: 41.230\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.332984058, Training Accuracy: 61.488\n",
            "Worker 1, [02/04]: Training Loss: 1.313852294, Training Accuracy: 61.408\n",
            "Worker 1, [03/04]: Training Loss: 1.289697676, Training Accuracy: 61.504\n",
            "Worker 1, [04/04]: Training Loss: 1.268687917, Training Accuracy: 63.512\n",
            "Time taken for training worker 1: 0:00:26.336587\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.128502471, Training Accuracy: 44.264\n",
            "Worker 2, [02/04]: Training Loss: 2.065314423, Training Accuracy: 45.888\n",
            "Worker 2, [03/04]: Training Loss: 1.977710916, Training Accuracy: 47.992\n",
            "Worker 2, [04/04]: Training Loss: 1.958668459, Training Accuracy: 48.344\n",
            "Time taken for training worker 2: 0:00:26.387848\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.557245531, Training Accuracy: 36.544\n",
            "Worker 3, [02/04]: Training Loss: 2.514038448, Training Accuracy: 36.808\n",
            "Worker 3, [03/04]: Training Loss: 2.487501417, Training Accuracy: 37.368\n",
            "Worker 3, [04/04]: Training Loss: 2.477318651, Training Accuracy: 37.744\n",
            "Time taken for training worker 3: 0:00:25.794543\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.568823566, Training Accuracy: 36.040\n",
            "Worker 4, [02/04]: Training Loss: 2.515080034, Training Accuracy: 36.328\n",
            "Worker 4, [03/04]: Training Loss: 2.526213033, Training Accuracy: 36.608\n",
            "Worker 4, [04/04]: Training Loss: 2.485808749, Training Accuracy: 37.520\n",
            "Time taken for training worker 4: 0:00:26.046172\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002846\n",
            "Global Update 21: Test Loss: 2.361314039, Test Accuracy: 42.040\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.281555713, Training Accuracy: 62.240\n",
            "Worker 1, [02/04]: Training Loss: 1.301985132, Training Accuracy: 62.224\n",
            "Worker 1, [03/04]: Training Loss: 1.250361536, Training Accuracy: 63.760\n",
            "Worker 1, [04/04]: Training Loss: 1.209242689, Training Accuracy: 64.488\n",
            "Time taken for training worker 1: 0:00:24.074513\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.115039658, Training Accuracy: 44.312\n",
            "Worker 2, [02/04]: Training Loss: 2.038379265, Training Accuracy: 46.504\n",
            "Worker 2, [03/04]: Training Loss: 1.973044991, Training Accuracy: 48.032\n",
            "Worker 2, [04/04]: Training Loss: 1.932999936, Training Accuracy: 48.680\n",
            "Time taken for training worker 2: 0:00:23.660677\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.547039714, Training Accuracy: 36.272\n",
            "Worker 3, [02/04]: Training Loss: 2.502379750, Training Accuracy: 37.584\n",
            "Worker 3, [03/04]: Training Loss: 2.484115653, Training Accuracy: 37.096\n",
            "Worker 3, [04/04]: Training Loss: 2.461462779, Training Accuracy: 37.848\n",
            "Time taken for training worker 3: 0:00:23.707321\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.563989805, Training Accuracy: 36.168\n",
            "Worker 4, [02/04]: Training Loss: 2.514337588, Training Accuracy: 36.944\n",
            "Worker 4, [03/04]: Training Loss: 2.500415107, Training Accuracy: 37.048\n",
            "Worker 4, [04/04]: Training Loss: 2.457824455, Training Accuracy: 37.872\n",
            "Time taken for training worker 4: 0:00:24.393996\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002390\n",
            "Global Update 22: Test Loss: 2.435969354, Test Accuracy: 41.790\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.250422813, Training Accuracy: 63.736\n",
            "Worker 1, [02/04]: Training Loss: 1.242791674, Training Accuracy: 63.816\n",
            "Worker 1, [03/04]: Training Loss: 1.214050409, Training Accuracy: 64.128\n",
            "Worker 1, [04/04]: Training Loss: 1.181182650, Training Accuracy: 65.400\n",
            "Time taken for training worker 1: 0:00:24.184356\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.090163497, Training Accuracy: 45.008\n",
            "Worker 2, [02/04]: Training Loss: 2.027136315, Training Accuracy: 46.928\n",
            "Worker 2, [03/04]: Training Loss: 1.958670714, Training Accuracy: 48.312\n",
            "Worker 2, [04/04]: Training Loss: 1.894892283, Training Accuracy: 50.088\n",
            "Time taken for training worker 2: 0:00:24.621349\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.542514147, Training Accuracy: 36.792\n",
            "Worker 3, [02/04]: Training Loss: 2.509857939, Training Accuracy: 36.976\n",
            "Worker 3, [03/04]: Training Loss: 2.480696065, Training Accuracy: 37.632\n",
            "Worker 3, [04/04]: Training Loss: 2.469410561, Training Accuracy: 37.816\n",
            "Time taken for training worker 3: 0:00:24.574213\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.548813379, Training Accuracy: 36.368\n",
            "Worker 4, [02/04]: Training Loss: 2.528418436, Training Accuracy: 36.432\n",
            "Worker 4, [03/04]: Training Loss: 2.491284887, Training Accuracy: 36.808\n",
            "Worker 4, [04/04]: Training Loss: 2.485674440, Training Accuracy: 37.000\n",
            "Time taken for training worker 4: 0:00:24.903374\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002716\n",
            "Global Update 23: Test Loss: 2.352755315, Test Accuracy: 42.830\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.213371941, Training Accuracy: 64.440\n",
            "Worker 1, [02/04]: Training Loss: 1.211058622, Training Accuracy: 64.280\n",
            "Worker 1, [03/04]: Training Loss: 1.180076101, Training Accuracy: 65.192\n",
            "Worker 1, [04/04]: Training Loss: 1.204361512, Training Accuracy: 64.560\n",
            "Time taken for training worker 1: 0:00:26.835533\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.070172843, Training Accuracy: 45.680\n",
            "Worker 2, [02/04]: Training Loss: 2.007071369, Training Accuracy: 46.984\n",
            "Worker 2, [03/04]: Training Loss: 1.946784630, Training Accuracy: 48.336\n",
            "Worker 2, [04/04]: Training Loss: 1.920523060, Training Accuracy: 49.032\n",
            "Time taken for training worker 2: 0:00:23.813023\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.518323400, Training Accuracy: 37.400\n",
            "Worker 3, [02/04]: Training Loss: 2.478032441, Training Accuracy: 37.544\n",
            "Worker 3, [03/04]: Training Loss: 2.464181800, Training Accuracy: 37.800\n",
            "Worker 3, [04/04]: Training Loss: 2.427899440, Training Accuracy: 38.400\n",
            "Time taken for training worker 3: 0:00:23.739217\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.519672559, Training Accuracy: 37.088\n",
            "Worker 4, [02/04]: Training Loss: 2.460267249, Training Accuracy: 38.424\n",
            "Worker 4, [03/04]: Training Loss: 2.461369940, Training Accuracy: 37.784\n",
            "Worker 4, [04/04]: Training Loss: 2.445184475, Training Accuracy: 38.248\n",
            "Time taken for training worker 4: 0:00:24.008653\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002938\n",
            "Global Update 24: Test Loss: 2.422102905, Test Accuracy: 42.090\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.212453890, Training Accuracy: 64.344\n",
            "Worker 1, [02/04]: Training Loss: 1.207028464, Training Accuracy: 64.848\n",
            "Worker 1, [03/04]: Training Loss: 1.196381458, Training Accuracy: 64.520\n",
            "Worker 1, [04/04]: Training Loss: 1.179934071, Training Accuracy: 65.384\n",
            "Time taken for training worker 1: 0:00:26.777754\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.089679979, Training Accuracy: 45.808\n",
            "Worker 2, [02/04]: Training Loss: 1.968320067, Training Accuracy: 47.528\n",
            "Worker 2, [03/04]: Training Loss: 1.936245330, Training Accuracy: 48.888\n",
            "Worker 2, [04/04]: Training Loss: 1.886310920, Training Accuracy: 50.264\n",
            "Time taken for training worker 2: 0:00:24.080003\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.566848164, Training Accuracy: 36.640\n",
            "Worker 3, [02/04]: Training Loss: 2.507699845, Training Accuracy: 37.192\n",
            "Worker 3, [03/04]: Training Loss: 2.505517028, Training Accuracy: 37.648\n",
            "Worker 3, [04/04]: Training Loss: 2.454718395, Training Accuracy: 38.632\n",
            "Time taken for training worker 3: 0:00:24.171282\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.575115895, Training Accuracy: 36.592\n",
            "Worker 4, [02/04]: Training Loss: 2.518896600, Training Accuracy: 37.384\n",
            "Worker 4, [03/04]: Training Loss: 2.493083771, Training Accuracy: 37.112\n",
            "Worker 4, [04/04]: Training Loss: 2.467507309, Training Accuracy: 38.096\n",
            "Time taken for training worker 4: 0:00:25.669682\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002746\n",
            "Global Update 25: Test Loss: 2.413399892, Test Accuracy: 42.700\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.216073380, Training Accuracy: 64.560\n",
            "Worker 1, [02/04]: Training Loss: 1.184400019, Training Accuracy: 65.184\n",
            "Worker 1, [03/04]: Training Loss: 1.137106377, Training Accuracy: 66.048\n",
            "Worker 1, [04/04]: Training Loss: 1.154431266, Training Accuracy: 65.824\n",
            "Time taken for training worker 1: 0:00:24.743171\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.073711231, Training Accuracy: 45.808\n",
            "Worker 2, [02/04]: Training Loss: 1.965505965, Training Accuracy: 48.424\n",
            "Worker 2, [03/04]: Training Loss: 1.941827876, Training Accuracy: 48.688\n",
            "Worker 2, [04/04]: Training Loss: 1.885191023, Training Accuracy: 49.456\n",
            "Time taken for training worker 2: 0:00:24.608489\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.511859758, Training Accuracy: 38.008\n",
            "Worker 3, [02/04]: Training Loss: 2.468321980, Training Accuracy: 37.896\n",
            "Worker 3, [03/04]: Training Loss: 2.459209291, Training Accuracy: 38.640\n",
            "Worker 3, [04/04]: Training Loss: 2.442103127, Training Accuracy: 38.496\n",
            "Time taken for training worker 3: 0:00:24.117898\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.525057688, Training Accuracy: 37.256\n",
            "Worker 4, [02/04]: Training Loss: 2.486489271, Training Accuracy: 37.752\n",
            "Worker 4, [03/04]: Training Loss: 2.471953840, Training Accuracy: 37.840\n",
            "Worker 4, [04/04]: Training Loss: 2.447701898, Training Accuracy: 38.200\n",
            "Time taken for training worker 4: 0:00:23.463089\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002255\n",
            "Global Update 26: Test Loss: 2.382736245, Test Accuracy: 43.830\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.176159211, Training Accuracy: 65.424\n",
            "Worker 1, [02/04]: Training Loss: 1.193803053, Training Accuracy: 64.608\n",
            "Worker 1, [03/04]: Training Loss: 1.172449242, Training Accuracy: 65.568\n",
            "Worker 1, [04/04]: Training Loss: 1.102868555, Training Accuracy: 67.544\n",
            "Time taken for training worker 1: 0:00:24.536507\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.065207438, Training Accuracy: 45.848\n",
            "Worker 2, [02/04]: Training Loss: 1.979748340, Training Accuracy: 47.704\n",
            "Worker 2, [03/04]: Training Loss: 1.923989025, Training Accuracy: 49.424\n",
            "Worker 2, [04/04]: Training Loss: 1.879295246, Training Accuracy: 49.712\n",
            "Time taken for training worker 2: 0:00:24.065789\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.536454213, Training Accuracy: 37.136\n",
            "Worker 3, [02/04]: Training Loss: 2.501464491, Training Accuracy: 37.376\n",
            "Worker 3, [03/04]: Training Loss: 2.458800646, Training Accuracy: 37.936\n",
            "Worker 3, [04/04]: Training Loss: 2.438869455, Training Accuracy: 38.712\n",
            "Time taken for training worker 3: 0:00:24.608599\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.548557897, Training Accuracy: 36.584\n",
            "Worker 4, [02/04]: Training Loss: 2.473966996, Training Accuracy: 37.320\n",
            "Worker 4, [03/04]: Training Loss: 2.462427996, Training Accuracy: 38.240\n",
            "Worker 4, [04/04]: Training Loss: 2.456461391, Training Accuracy: 37.696\n",
            "Time taken for training worker 4: 0:00:25.221704\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002802\n",
            "Global Update 27: Test Loss: 2.434567423, Test Accuracy: 42.530\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.140487733, Training Accuracy: 66.536\n",
            "Worker 1, [02/04]: Training Loss: 1.135225090, Training Accuracy: 66.352\n",
            "Worker 1, [03/04]: Training Loss: 1.149915948, Training Accuracy: 66.440\n",
            "Worker 1, [04/04]: Training Loss: 1.118875827, Training Accuracy: 67.296\n",
            "Time taken for training worker 1: 0:00:25.662721\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.043243741, Training Accuracy: 46.672\n",
            "Worker 2, [02/04]: Training Loss: 1.939387421, Training Accuracy: 48.448\n",
            "Worker 2, [03/04]: Training Loss: 1.894184179, Training Accuracy: 49.256\n",
            "Worker 2, [04/04]: Training Loss: 1.847346837, Training Accuracy: 51.264\n",
            "Time taken for training worker 2: 0:00:24.374520\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.533359501, Training Accuracy: 36.560\n",
            "Worker 3, [02/04]: Training Loss: 2.483995385, Training Accuracy: 37.184\n",
            "Worker 3, [03/04]: Training Loss: 2.454730348, Training Accuracy: 38.072\n",
            "Worker 3, [04/04]: Training Loss: 2.427769489, Training Accuracy: 38.448\n",
            "Time taken for training worker 3: 0:00:24.798102\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.548749034, Training Accuracy: 36.424\n",
            "Worker 4, [02/04]: Training Loss: 2.486125831, Training Accuracy: 37.520\n",
            "Worker 4, [03/04]: Training Loss: 2.440410338, Training Accuracy: 38.392\n",
            "Worker 4, [04/04]: Training Loss: 2.450625066, Training Accuracy: 38.528\n",
            "Time taken for training worker 4: 0:00:24.413861\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002267\n",
            "Global Update 28: Test Loss: 2.368876629, Test Accuracy: 43.210\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.155563390, Training Accuracy: 65.696\n",
            "Worker 1, [02/04]: Training Loss: 1.110934684, Training Accuracy: 66.992\n",
            "Worker 1, [03/04]: Training Loss: 1.092861718, Training Accuracy: 67.376\n",
            "Worker 1, [04/04]: Training Loss: 1.100457498, Training Accuracy: 67.032\n",
            "Time taken for training worker 1: 0:00:24.162050\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.039955461, Training Accuracy: 46.088\n",
            "Worker 2, [02/04]: Training Loss: 1.941622392, Training Accuracy: 48.560\n",
            "Worker 2, [03/04]: Training Loss: 1.857560738, Training Accuracy: 51.072\n",
            "Worker 2, [04/04]: Training Loss: 1.842502912, Training Accuracy: 50.984\n",
            "Time taken for training worker 2: 0:00:23.471114\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.530101609, Training Accuracy: 37.336\n",
            "Worker 3, [02/04]: Training Loss: 2.486172260, Training Accuracy: 37.640\n",
            "Worker 3, [03/04]: Training Loss: 2.448675475, Training Accuracy: 38.048\n",
            "Worker 3, [04/04]: Training Loss: 2.423557333, Training Accuracy: 39.104\n",
            "Time taken for training worker 3: 0:00:23.934834\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.535624811, Training Accuracy: 37.152\n",
            "Worker 4, [02/04]: Training Loss: 2.473366487, Training Accuracy: 38.072\n",
            "Worker 4, [03/04]: Training Loss: 2.446034173, Training Accuracy: 38.168\n",
            "Worker 4, [04/04]: Training Loss: 2.443142134, Training Accuracy: 38.488\n",
            "Time taken for training worker 4: 0:00:23.015161\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002410\n",
            "Global Update 29: Test Loss: 2.424564104, Test Accuracy: 42.580\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.114333576, Training Accuracy: 66.448\n",
            "Worker 1, [02/04]: Training Loss: 1.095583911, Training Accuracy: 67.256\n",
            "Worker 1, [03/04]: Training Loss: 1.117626822, Training Accuracy: 67.136\n",
            "Worker 1, [04/04]: Training Loss: 1.089884935, Training Accuracy: 67.616\n",
            "Time taken for training worker 1: 0:00:24.869342\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.027017149, Training Accuracy: 46.984\n",
            "Worker 2, [02/04]: Training Loss: 1.912786683, Training Accuracy: 49.688\n",
            "Worker 2, [03/04]: Training Loss: 1.898347091, Training Accuracy: 49.544\n",
            "Worker 2, [04/04]: Training Loss: 1.844176732, Training Accuracy: 50.864\n",
            "Time taken for training worker 2: 0:00:24.731630\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.500608786, Training Accuracy: 37.376\n",
            "Worker 3, [02/04]: Training Loss: 2.450939275, Training Accuracy: 38.272\n",
            "Worker 3, [03/04]: Training Loss: 2.444162012, Training Accuracy: 37.816\n",
            "Worker 3, [04/04]: Training Loss: 2.405152203, Training Accuracy: 38.616\n",
            "Time taken for training worker 3: 0:00:24.935674\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.529783309, Training Accuracy: 36.512\n",
            "Worker 4, [02/04]: Training Loss: 2.482570318, Training Accuracy: 37.320\n",
            "Worker 4, [03/04]: Training Loss: 2.446853404, Training Accuracy: 38.168\n",
            "Worker 4, [04/04]: Training Loss: 2.457478376, Training Accuracy: 38.008\n",
            "Time taken for training worker 4: 0:00:25.350713\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003435\n",
            "Global Update 30: Test Loss: 2.360202268, Test Accuracy: 43.630\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.134447912, Training Accuracy: 66.560\n",
            "Worker 1, [02/04]: Training Loss: 1.120412519, Training Accuracy: 66.848\n",
            "Worker 1, [03/04]: Training Loss: 1.076766259, Training Accuracy: 68.056\n",
            "Worker 1, [04/04]: Training Loss: 1.068865116, Training Accuracy: 68.048\n",
            "Time taken for training worker 1: 0:00:24.626498\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.992196240, Training Accuracy: 47.752\n",
            "Worker 2, [02/04]: Training Loss: 1.931040084, Training Accuracy: 49.424\n",
            "Worker 2, [03/04]: Training Loss: 1.869355595, Training Accuracy: 50.664\n",
            "Worker 2, [04/04]: Training Loss: 1.813497082, Training Accuracy: 51.776\n",
            "Time taken for training worker 2: 0:00:24.139868\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.515314433, Training Accuracy: 37.424\n",
            "Worker 3, [02/04]: Training Loss: 2.466752540, Training Accuracy: 37.984\n",
            "Worker 3, [03/04]: Training Loss: 2.451072939, Training Accuracy: 38.392\n",
            "Worker 3, [04/04]: Training Loss: 2.412069831, Training Accuracy: 39.408\n",
            "Time taken for training worker 3: 0:00:25.459577\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.545881816, Training Accuracy: 36.488\n",
            "Worker 4, [02/04]: Training Loss: 2.470688886, Training Accuracy: 37.856\n",
            "Worker 4, [03/04]: Training Loss: 2.439865069, Training Accuracy: 38.640\n",
            "Worker 4, [04/04]: Training Loss: 2.431133180, Training Accuracy: 38.800\n",
            "Time taken for training worker 4: 0:00:24.813017\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002213\n",
            "Global Update 31: Test Loss: 2.356332210, Test Accuracy: 43.250\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.124437841, Training Accuracy: 66.360\n",
            "Worker 1, [02/04]: Training Loss: 1.074564372, Training Accuracy: 67.896\n",
            "Worker 1, [03/04]: Training Loss: 1.079191543, Training Accuracy: 68.192\n",
            "Worker 1, [04/04]: Training Loss: 1.087623707, Training Accuracy: 68.264\n",
            "Time taken for training worker 1: 0:00:25.369674\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.012471399, Training Accuracy: 46.960\n",
            "Worker 2, [02/04]: Training Loss: 1.898017640, Training Accuracy: 49.480\n",
            "Worker 2, [03/04]: Training Loss: 1.828588792, Training Accuracy: 51.480\n",
            "Worker 2, [04/04]: Training Loss: 1.818783440, Training Accuracy: 51.528\n",
            "Time taken for training worker 2: 0:00:25.215147\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.507216719, Training Accuracy: 37.848\n",
            "Worker 3, [02/04]: Training Loss: 2.472812049, Training Accuracy: 37.888\n",
            "Worker 3, [03/04]: Training Loss: 2.439271872, Training Accuracy: 38.656\n",
            "Worker 3, [04/04]: Training Loss: 2.429287268, Training Accuracy: 38.512\n",
            "Time taken for training worker 3: 0:00:24.790343\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.529807926, Training Accuracy: 37.008\n",
            "Worker 4, [02/04]: Training Loss: 2.454135583, Training Accuracy: 37.904\n",
            "Worker 4, [03/04]: Training Loss: 2.438111455, Training Accuracy: 38.808\n",
            "Worker 4, [04/04]: Training Loss: 2.434177102, Training Accuracy: 38.720\n",
            "Time taken for training worker 4: 0:00:25.704105\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002740\n",
            "Global Update 32: Test Loss: 2.442125431, Test Accuracy: 41.880\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.104538200, Training Accuracy: 66.960\n",
            "Worker 1, [02/04]: Training Loss: 1.053991209, Training Accuracy: 68.648\n",
            "Worker 1, [03/04]: Training Loss: 1.055833069, Training Accuracy: 68.712\n",
            "Worker 1, [04/04]: Training Loss: 1.028623969, Training Accuracy: 69.200\n",
            "Time taken for training worker 1: 0:00:25.210986\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.977056023, Training Accuracy: 47.320\n",
            "Worker 2, [02/04]: Training Loss: 1.909115240, Training Accuracy: 49.576\n",
            "Worker 2, [03/04]: Training Loss: 1.848218953, Training Accuracy: 50.488\n",
            "Worker 2, [04/04]: Training Loss: 1.783006592, Training Accuracy: 52.400\n",
            "Time taken for training worker 2: 0:00:26.376356\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.534029107, Training Accuracy: 36.880\n",
            "Worker 3, [02/04]: Training Loss: 2.462714614, Training Accuracy: 37.672\n",
            "Worker 3, [03/04]: Training Loss: 2.452951712, Training Accuracy: 38.352\n",
            "Worker 3, [04/04]: Training Loss: 2.438867898, Training Accuracy: 38.712\n",
            "Time taken for training worker 3: 0:00:23.513212\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.539827613, Training Accuracy: 36.824\n",
            "Worker 4, [02/04]: Training Loss: 2.473539216, Training Accuracy: 38.088\n",
            "Worker 4, [03/04]: Training Loss: 2.466519406, Training Accuracy: 38.000\n",
            "Worker 4, [04/04]: Training Loss: 2.439976475, Training Accuracy: 38.192\n",
            "Time taken for training worker 4: 0:00:24.580354\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003220\n",
            "Global Update 33: Test Loss: 2.366240923, Test Accuracy: 43.460\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.087184713, Training Accuracy: 68.048\n",
            "Worker 1, [02/04]: Training Loss: 1.044375821, Training Accuracy: 68.960\n",
            "Worker 1, [03/04]: Training Loss: 1.075313265, Training Accuracy: 68.384\n",
            "Worker 1, [04/04]: Training Loss: 1.072067126, Training Accuracy: 68.312\n",
            "Time taken for training worker 1: 0:00:25.729347\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.991724841, Training Accuracy: 47.688\n",
            "Worker 2, [02/04]: Training Loss: 1.902904293, Training Accuracy: 50.080\n",
            "Worker 2, [03/04]: Training Loss: 1.850359821, Training Accuracy: 51.040\n",
            "Worker 2, [04/04]: Training Loss: 1.779826363, Training Accuracy: 52.072\n",
            "Time taken for training worker 2: 0:00:25.632211\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.499287883, Training Accuracy: 38.112\n",
            "Worker 3, [02/04]: Training Loss: 2.436363309, Training Accuracy: 38.912\n",
            "Worker 3, [03/04]: Training Loss: 2.432045658, Training Accuracy: 38.880\n",
            "Worker 3, [04/04]: Training Loss: 2.408184826, Training Accuracy: 39.528\n",
            "Time taken for training worker 3: 0:00:25.902560\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.501584124, Training Accuracy: 37.928\n",
            "Worker 4, [02/04]: Training Loss: 2.466928346, Training Accuracy: 37.944\n",
            "Worker 4, [03/04]: Training Loss: 2.422923696, Training Accuracy: 39.000\n",
            "Worker 4, [04/04]: Training Loss: 2.428346283, Training Accuracy: 39.120\n",
            "Time taken for training worker 4: 0:00:25.464979\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003013\n",
            "Global Update 34: Test Loss: 2.411992967, Test Accuracy: 43.330\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.066281617, Training Accuracy: 67.824\n",
            "Worker 1, [02/04]: Training Loss: 1.084923087, Training Accuracy: 67.808\n",
            "Worker 1, [03/04]: Training Loss: 1.057985845, Training Accuracy: 68.104\n",
            "Worker 1, [04/04]: Training Loss: 1.049608737, Training Accuracy: 68.792\n",
            "Time taken for training worker 1: 0:00:25.480886\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.970663251, Training Accuracy: 47.872\n",
            "Worker 2, [02/04]: Training Loss: 1.889768864, Training Accuracy: 49.368\n",
            "Worker 2, [03/04]: Training Loss: 1.835745030, Training Accuracy: 50.688\n",
            "Worker 2, [04/04]: Training Loss: 1.801632758, Training Accuracy: 52.072\n",
            "Time taken for training worker 2: 0:00:25.539651\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.514024932, Training Accuracy: 37.368\n",
            "Worker 3, [02/04]: Training Loss: 2.452330081, Training Accuracy: 38.752\n",
            "Worker 3, [03/04]: Training Loss: 2.431382464, Training Accuracy: 38.496\n",
            "Worker 3, [04/04]: Training Loss: 2.405056995, Training Accuracy: 38.992\n",
            "Time taken for training worker 3: 0:00:26.120994\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.503597247, Training Accuracy: 38.104\n",
            "Worker 4, [02/04]: Training Loss: 2.454268485, Training Accuracy: 38.328\n",
            "Worker 4, [03/04]: Training Loss: 2.431863481, Training Accuracy: 38.816\n",
            "Worker 4, [04/04]: Training Loss: 2.412500917, Training Accuracy: 39.056\n",
            "Time taken for training worker 4: 0:00:25.617491\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002813\n",
            "Global Update 35: Test Loss: 2.413035082, Test Accuracy: 42.940\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.072375692, Training Accuracy: 68.224\n",
            "Worker 1, [02/04]: Training Loss: 1.044186145, Training Accuracy: 68.904\n",
            "Worker 1, [03/04]: Training Loss: 1.054175574, Training Accuracy: 69.240\n",
            "Worker 1, [04/04]: Training Loss: 1.017555830, Training Accuracy: 69.624\n",
            "Time taken for training worker 1: 0:00:25.975597\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.977312168, Training Accuracy: 48.008\n",
            "Worker 2, [02/04]: Training Loss: 1.877775135, Training Accuracy: 50.072\n",
            "Worker 2, [03/04]: Training Loss: 1.805990552, Training Accuracy: 51.712\n",
            "Worker 2, [04/04]: Training Loss: 1.761248392, Training Accuracy: 52.600\n",
            "Time taken for training worker 2: 0:00:26.015549\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.500040639, Training Accuracy: 37.544\n",
            "Worker 3, [02/04]: Training Loss: 2.464133813, Training Accuracy: 38.560\n",
            "Worker 3, [03/04]: Training Loss: 2.437576757, Training Accuracy: 39.288\n",
            "Worker 3, [04/04]: Training Loss: 2.414715448, Training Accuracy: 39.312\n",
            "Time taken for training worker 3: 0:00:25.169094\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.513428014, Training Accuracy: 37.576\n",
            "Worker 4, [02/04]: Training Loss: 2.451228341, Training Accuracy: 38.592\n",
            "Worker 4, [03/04]: Training Loss: 2.445766195, Training Accuracy: 38.336\n",
            "Worker 4, [04/04]: Training Loss: 2.441018867, Training Accuracy: 38.456\n",
            "Time taken for training worker 4: 0:00:26.893808\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003378\n",
            "Global Update 36: Test Loss: 2.382335695, Test Accuracy: 43.390\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.055771694, Training Accuracy: 68.408\n",
            "Worker 1, [02/04]: Training Loss: 1.038976759, Training Accuracy: 68.936\n",
            "Worker 1, [03/04]: Training Loss: 1.032144952, Training Accuracy: 69.248\n",
            "Worker 1, [04/04]: Training Loss: 1.001831938, Training Accuracy: 70.696\n",
            "Time taken for training worker 1: 0:00:25.571505\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.928017324, Training Accuracy: 48.856\n",
            "Worker 2, [02/04]: Training Loss: 1.847247858, Training Accuracy: 50.664\n",
            "Worker 2, [03/04]: Training Loss: 1.825397820, Training Accuracy: 51.384\n",
            "Worker 2, [04/04]: Training Loss: 1.787151942, Training Accuracy: 52.416\n",
            "Time taken for training worker 2: 0:00:25.194629\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.499705056, Training Accuracy: 37.808\n",
            "Worker 3, [02/04]: Training Loss: 2.453355038, Training Accuracy: 38.360\n",
            "Worker 3, [03/04]: Training Loss: 2.425798461, Training Accuracy: 38.920\n",
            "Worker 3, [04/04]: Training Loss: 2.418129935, Training Accuracy: 39.072\n",
            "Time taken for training worker 3: 0:00:25.540526\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 2.532965059, Training Accuracy: 37.024\n",
            "Worker 4, [02/04]: Training Loss: 2.478771646, Training Accuracy: 37.800\n",
            "Worker 4, [03/04]: Training Loss: 2.456029218, Training Accuracy: 38.520\n",
            "Worker 4, [04/04]: Training Loss: 2.405266753, Training Accuracy: 39.320\n",
            "Time taken for training worker 4: 0:00:23.320269\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002325\n",
            "Global Update 37: Test Loss: 2.400287570, Test Accuracy: 42.830\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for HeteroCompSGD: 1:03:26.665899\n",
            "//////////////////////////////////////////////////\n"
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [4]\n",
        "J = [4] # TODO: Bar asase local sgd behtarin meghdare J ro peyda konim.\n",
        "\n",
        "num_epochs = 150\n",
        "warmups = 1\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    LayerwiseMaskingApproach(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, warmups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Our Approach 1: Layerwise Masking Approach Farzad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "from itertools import combinations\n",
        "\n",
        "def freeze_layers(model, layers_to_freeze):\n",
        "    \"\"\"\n",
        "    Freezes the specified layers in the model, skipping non-trainable layers.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The model whose layers you want to freeze.\n",
        "    - layers_to_freeze (list of nn.Module): The layers to freeze.\n",
        "    \"\"\"\n",
        "\n",
        "    for idx, layer in enumerate(model.children()):\n",
        "        if idx in layers_to_freeze and any(p.requires_grad for p in layer.parameters()):\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "def build_model_list(model, num_layers_to_freeze):\n",
        "    \"\"\"\n",
        "    Builds a list of models with different combinations of frozen layers.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The model to use as a base.\n",
        "    - num_layers_to_freeze (int): The number of layers to freeze in each model.\n",
        "\n",
        "    Returns:\n",
        "    - list of nn.Module: A list of models with different combinations of frozen layers.\n",
        "    \"\"\"\n",
        "    model_list = []\n",
        "    trainable_layers = [idx for idx, layer in enumerate(model.children()) if any(p.requires_grad for p in layer.parameters())]\n",
        "    num_trainable_layers = len(trainable_layers)\n",
        "\n",
        "    # Generate all possible combinations of trainable layers to freeze\n",
        "    layer_combinations = list(combinations(trainable_layers, num_layers_to_freeze))\n",
        "\n",
        "    for layers in layer_combinations:\n",
        "        new_model = copy.deepcopy(model)\n",
        "        freeze_layers(new_model, layers)\n",
        "        model_list.append(new_model)\n",
        "\n",
        "    return model_list\n",
        "\n",
        "def train_select_best_model(model, num_layers_to_freeze, num_epochs, train_loader, test_loader, loss_fn, optimizer, device):\n",
        "    \"\"\"\n",
        "    Trains the model with different combinations of frozen layers and selects the best one based on test accuracy.\n",
        "    \"\"\"\n",
        "    model_list = build_model_list(model, num_layers_to_freeze)\n",
        "    best_accuracy = 0.0\n",
        "    best_layers = []\n",
        "\n",
        "    for candidate_model in model_list:\n",
        "        candidate_model.to(device)\n",
        "        optimizer_copy = copy.deepcopy(optimizer)\n",
        "\n",
        "        # Update only the parameters that require gradients\n",
        "        for param_group in optimizer_copy.param_groups:\n",
        "            param_group['params'] = [param for param in candidate_model.parameters() if param.requires_grad]\n",
        "\n",
        "        candidate_model.train()\n",
        "\n",
        "        for _ in range(num_epochs):\n",
        "            train(candidate_model, train_loader, optimizer_copy, loss_fn)\n",
        "\n",
        "        test_loss, test_accuracy = test(candidate_model, test_loader, loss_fn, is_wandb=False)\n",
        "\n",
        "        if test_accuracy > best_accuracy:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_layers = [name for idx, (name, layer) in enumerate(candidate_model.named_children()) \n",
        "                           if not any(p.requires_grad for p in layer.parameters()) and any(p.requires_grad for p in model.get_submodule(name).parameters())]\n",
        "\n",
        "    best_layer_indices = [idx for idx, (name, layer) in enumerate(model.named_children()) if name in best_layers]\n",
        "    freeze_layers(model, best_layer_indices)\n",
        "\n",
        "    return {tuple(best_layers): copy.deepcopy(model)}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class PerDLMaskOptimizer(Optimizer):\n",
        "    def __init__(self, global_model, lr=0.01):\n",
        "        self.global_model = global_model\n",
        "        self.lr = lr\n",
        "        params = list(global_model.parameters())\n",
        "        super(PerDLMaskOptimizer, self).__init__(params, {'lr': lr})\n",
        "\n",
        "    def expand_mask(self, model, layer_mask):\n",
        "        \"\"\"\n",
        "        Expands the layer-based mask to match the number of parameters.\n",
        "\n",
        "        Args:\n",
        "        - model (nn.Module): The model whose parameters are being masked.\n",
        "        - layer_mask (list of bool): The mask corresponding to each layer (True for trainable, False for frozen).\n",
        "\n",
        "        Returns:\n",
        "        - list of bool: Expanded mask corresponding to each parameter in the model.\n",
        "        \"\"\"\n",
        "        expanded_mask = []\n",
        "        for is_trainable, layer in zip(layer_mask, model.children()):\n",
        "            # Append True/False for each parameter in the layer\n",
        "            expanded_mask.extend([is_trainable] * len(list(layer.parameters())))\n",
        "        return expanded_mask\n",
        "\n",
        "    def step(self, local_models, masks):\n",
        "        \"\"\"\n",
        "        Perform a step of Local SGD optimization using the mask for each worker.\n",
        "\n",
        "        Args:\n",
        "        - local_models (list of nn.Module): The list of local models (from different workers).\n",
        "        - masks (list of list of bool): The mask for each worker indicating whether each layer is trainable or not.\n",
        "        \"\"\"\n",
        "        # Get global model parameters\n",
        "        global_params = list(self.global_model.parameters())\n",
        "        \n",
        "        # Initialize deltas for each parameter, same size as global parameters\n",
        "        deltas = [torch.zeros_like(param) for param in global_params]\n",
        "\n",
        "        # Iterate over local models and accumulate deltas based on the expanded mask\n",
        "        for worker_idx, local_model in enumerate(local_models):\n",
        "            local_params = list(local_model.parameters())\n",
        "            \n",
        "            # Expand the mask to match the number of parameters in the model\n",
        "            expanded_mask = self.expand_mask(local_model, masks[worker_idx])\n",
        "\n",
        "            # Ensure mask length matches local_params length\n",
        "            assert len(expanded_mask) == len(local_params), f\"Expanded mask size {len(expanded_mask)} doesn't match number of parameters {len(local_params)}\"\n",
        "\n",
        "            for i, param in enumerate(local_params):\n",
        "                if expanded_mask[i]:  # Only consider non-frozen parameters\n",
        "                    deltas[i] += (global_params[i] - param)\n",
        "\n",
        "        # Adjust deltas based on the number of workers that updated each layer\n",
        "        for i, delta in enumerate(deltas):\n",
        "            num_active_workers = sum(expanded_mask[i] for expanded_mask in [self.expand_mask(m, masks[w_idx]) for w_idx, m in enumerate(local_models)])  # Count workers with trainable parameters\n",
        "            if num_active_workers > 0:\n",
        "                deltas[i] /= num_active_workers  # Average only over active workers\n",
        "            deltas[i] /= self.lr  # Apply learning rate scaling\n",
        "        \n",
        "        # Update global model parameters\n",
        "        with torch.no_grad():\n",
        "            for i, param in enumerate(global_params):\n",
        "                param.copy_(param - self.lr * deltas[i])\n",
        "        \n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def PerDLMask(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, index_weaker_worker, n_freeze_layer, is_wandb=False):\n",
        "      \n",
        "  total_start_time = time.time()\n",
        "  \n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "  \n",
        "  global_optimizer = PerDLMaskOptimizer(global_model, lr)\n",
        "  \n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "  \n",
        "  checkpoint = load_checkpoint('PerDLMask_finetuning', 64, {'k': k, 'j': j, 'weaker_worker': index_weaker_worker, 'n_freeze_layer': n_freeze_layer})\n",
        "  if checkpoint is not None:\n",
        "      global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      global_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      \n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn)\n",
        "      print(f'Global Update: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      \n",
        "      return None\n",
        "  \n",
        "  # select to freeze layers for the weaker worker with lower bais\n",
        "  freeze_model = train_select_best_model(local_models[index_weaker_worker], n_freeze_layer, 5, shard_loaders[index_weaker_worker], original_test_loader, loss_fn, local_optimizers[index_weaker_worker], device)\n",
        "  print(f'The following layers of worker {index_weaker_worker+1} are frozen: {list(freeze_model.keys())[0]}')\n",
        "  local_models[index_weaker_worker] = list(freeze_model.values())[0]\n",
        "  \n",
        "  # Initialize masks for each worker\n",
        "  masks = [[True for _ in model.named_children()] for model in local_models]\n",
        "  # Freeze the layers for the weaker worker in the masks\n",
        "  masks[index_weaker_worker] = [name not in list(freeze_model.keys())[0] for name, layer in list(freeze_model.values())[0].named_children()]\n",
        "\n",
        "  for iteration in range(iterations-1):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for loca_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    global_optimizer.step(local_models, masks)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    for local_optimizer in local_optimizers:\n",
        "        local_optimizer.param_groups[0]['lr'] = global_optimizer.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "    for local_model in local_models:\n",
        "      local_model.load_state_dict(global_model.state_dict())\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Global Update {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "  \n",
        "  # Unfreeze all layers for worker 2 (the weaker worker)\n",
        "  for name, layer in local_models[index_weaker_worker].named_children():\n",
        "    for param in layer.parameters():\n",
        "        if name in list(freeze_model.keys())[0]:\n",
        "            param.requires_grad = True\n",
        "  masks = [[True for _ in model.named_children()] for model in local_models]  \n",
        "  \n",
        "  for worker in range(k):\n",
        "    train_start_time = time.time()\n",
        "    for _ in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "    total_end_time = time.time()\n",
        "    print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "    print('-'*50)\n",
        "  sync_start_time = time.time()\n",
        "\n",
        "  global_optimizer.step(local_models, masks)\n",
        "  scheduler.step()\n",
        "  for local_optimizer in local_optimizers:\n",
        "    local_optimizer.param_groups[0]['lr'] = global_optimizer.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "  for local_model in local_models:\n",
        "    local_model.load_state_dict(global_model.state_dict())\n",
        "  sync_end_time = time.time()\n",
        "  print('*'*50)\n",
        "  print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "  test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "  print(f'Global Update {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "  print('*'*50)\n",
        "\n",
        "  \n",
        "  # Save checkpoint\n",
        "  save_checkpoint({\n",
        "              'epoch': num_epochs,\n",
        "              'model_state_dict': global_model.state_dict(),\n",
        "              'optimizer_state_dict': global_optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss': loss_fn\n",
        "              }, 149, 64, 'PerDLMask_finetuning', {'k': k, 'j': j, 'weaker_worker': index_weaker_worker, 'n_freeze_layer': n_freeze_layer})\n",
        " \n",
        "\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for local_SGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:4\n",
            "==================================================\n",
            "The following layers of worker 2 are frozen: ('conv1', 'fc2')\n",
            "Worker 1, [01/04]: Training Loss: 4.325985642, Training Accuracy: 4.264\n",
            "Worker 1, [02/04]: Training Loss: 3.868384989, Training Accuracy: 10.252\n",
            "Worker 1, [03/04]: Training Loss: 3.624024439, Training Accuracy: 13.772\n",
            "Worker 1, [04/04]: Training Loss: 3.404914563, Training Accuracy: 17.408\n",
            "Time taken for training worker 1: 0:00:49.061982\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.606343958, Training Accuracy: 0.824\n",
            "Worker 2, [02/04]: Training Loss: 4.606307727, Training Accuracy: 0.800\n",
            "Worker 2, [03/04]: Training Loss: 4.606329551, Training Accuracy: 0.836\n",
            "Worker 2, [04/04]: Training Loss: 4.606335538, Training Accuracy: 0.868\n",
            "Time taken for training worker 2: 0:00:45.812945\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002335\n",
            "Global Update 01: Test Loss: 4.052325751, Test Accuracy: 19.510\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.397919449, Training Accuracy: 17.776\n",
            "Worker 1, [02/04]: Training Loss: 3.180810970, Training Accuracy: 21.872\n",
            "Worker 1, [03/04]: Training Loss: 3.039010358, Training Accuracy: 24.096\n",
            "Worker 1, [04/04]: Training Loss: 2.901271932, Training Accuracy: 26.752\n",
            "Time taken for training worker 1: 0:00:51.643842\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.091401148, Training Accuracy: 17.536\n",
            "Worker 2, [02/04]: Training Loss: 4.090463296, Training Accuracy: 17.956\n",
            "Worker 2, [03/04]: Training Loss: 4.090386021, Training Accuracy: 17.512\n",
            "Worker 2, [04/04]: Training Loss: 4.090892074, Training Accuracy: 17.492\n",
            "Time taken for training worker 2: 0:00:46.745342\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002887\n",
            "Global Update 02: Test Loss: 3.256559654, Test Accuracy: 28.620\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.950611057, Training Accuracy: 25.628\n",
            "Worker 1, [02/04]: Training Loss: 2.812428623, Training Accuracy: 28.632\n",
            "Worker 1, [03/04]: Training Loss: 2.694353755, Training Accuracy: 30.912\n",
            "Worker 1, [04/04]: Training Loss: 2.618809583, Training Accuracy: 32.400\n",
            "Time taken for training worker 1: 0:00:52.002093\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.357726063, Training Accuracy: 26.432\n",
            "Worker 2, [02/04]: Training Loss: 3.363657214, Training Accuracy: 26.140\n",
            "Worker 2, [03/04]: Training Loss: 3.359049859, Training Accuracy: 26.092\n",
            "Worker 2, [04/04]: Training Loss: 3.366217086, Training Accuracy: 25.984\n",
            "Time taken for training worker 2: 0:00:46.577865\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002541\n",
            "Global Update 03: Test Loss: 2.763523503, Test Accuracy: 33.570\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.642755342, Training Accuracy: 31.800\n",
            "Worker 1, [02/04]: Training Loss: 2.567214155, Training Accuracy: 33.420\n",
            "Worker 1, [03/04]: Training Loss: 2.502111344, Training Accuracy: 34.868\n",
            "Worker 1, [04/04]: Training Loss: 2.439100523, Training Accuracy: 36.084\n",
            "Time taken for training worker 1: 0:00:44.773876\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.905349242, Training Accuracy: 30.684\n",
            "Worker 2, [02/04]: Training Loss: 2.901437189, Training Accuracy: 30.756\n",
            "Worker 2, [03/04]: Training Loss: 2.910174098, Training Accuracy: 30.412\n",
            "Worker 2, [04/04]: Training Loss: 2.905379345, Training Accuracy: 30.304\n",
            "Time taken for training worker 2: 0:00:41.382694\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002090\n",
            "Global Update 04: Test Loss: 2.461300543, Test Accuracy: 37.570\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.456060107, Training Accuracy: 35.600\n",
            "Worker 1, [02/04]: Training Loss: 2.395885102, Training Accuracy: 36.924\n",
            "Worker 1, [03/04]: Training Loss: 2.336378274, Training Accuracy: 38.116\n",
            "Worker 1, [04/04]: Training Loss: 2.287495130, Training Accuracy: 39.528\n",
            "Time taken for training worker 1: 0:00:47.626531\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.624845178, Training Accuracy: 34.656\n",
            "Worker 2, [02/04]: Training Loss: 2.626099465, Training Accuracy: 34.628\n",
            "Worker 2, [03/04]: Training Loss: 2.617715104, Training Accuracy: 34.764\n",
            "Worker 2, [04/04]: Training Loss: 2.629470634, Training Accuracy: 34.540\n",
            "Time taken for training worker 2: 0:00:49.474155\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002547\n",
            "Global Update 05: Test Loss: 2.334820484, Test Accuracy: 39.290\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.288727963, Training Accuracy: 39.216\n",
            "Worker 1, [02/04]: Training Loss: 2.254242378, Training Accuracy: 39.740\n",
            "Worker 1, [03/04]: Training Loss: 2.196366524, Training Accuracy: 41.316\n",
            "Worker 1, [04/04]: Training Loss: 2.169758370, Training Accuracy: 42.024\n",
            "Time taken for training worker 1: 0:00:44.560617\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.511065306, Training Accuracy: 36.408\n",
            "Worker 2, [02/04]: Training Loss: 2.503584604, Training Accuracy: 36.552\n",
            "Worker 2, [03/04]: Training Loss: 2.509466917, Training Accuracy: 36.532\n",
            "Worker 2, [04/04]: Training Loss: 2.506284466, Training Accuracy: 36.096\n",
            "Time taken for training worker 2: 0:00:40.285606\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002118\n",
            "Global Update 06: Test Loss: 2.210874593, Test Accuracy: 41.870\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.145274494, Training Accuracy: 42.492\n",
            "Worker 1, [02/04]: Training Loss: 2.135335006, Training Accuracy: 42.344\n",
            "Worker 1, [03/04]: Training Loss: 2.083093702, Training Accuracy: 43.416\n",
            "Worker 1, [04/04]: Training Loss: 2.064696996, Training Accuracy: 44.356\n",
            "Time taken for training worker 1: 0:00:45.655663\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.397863845, Training Accuracy: 38.092\n",
            "Worker 2, [02/04]: Training Loss: 2.396685154, Training Accuracy: 38.256\n",
            "Worker 2, [03/04]: Training Loss: 2.396642241, Training Accuracy: 38.316\n",
            "Worker 2, [04/04]: Training Loss: 2.406902316, Training Accuracy: 38.196\n",
            "Time taken for training worker 2: 0:00:40.823256\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002167\n",
            "Global Update 07: Test Loss: 2.175152704, Test Accuracy: 43.290\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.037627313, Training Accuracy: 44.784\n",
            "Worker 1, [02/04]: Training Loss: 2.031709461, Training Accuracy: 44.688\n",
            "Worker 1, [03/04]: Training Loss: 1.998676323, Training Accuracy: 45.680\n",
            "Worker 1, [04/04]: Training Loss: 1.975867749, Training Accuracy: 46.188\n",
            "Time taken for training worker 1: 0:00:41.808722\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.360304464, Training Accuracy: 39.644\n",
            "Worker 2, [02/04]: Training Loss: 2.366523254, Training Accuracy: 39.240\n",
            "Worker 2, [03/04]: Training Loss: 2.363936142, Training Accuracy: 39.384\n",
            "Worker 2, [04/04]: Training Loss: 2.376680761, Training Accuracy: 39.388\n",
            "Time taken for training worker 2: 0:00:40.358157\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002112\n",
            "Global Update 08: Test Loss: 2.111722298, Test Accuracy: 45.250\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.933786007, Training Accuracy: 46.844\n",
            "Worker 1, [02/04]: Training Loss: 1.921295240, Training Accuracy: 47.164\n",
            "Worker 1, [03/04]: Training Loss: 1.907743102, Training Accuracy: 47.884\n",
            "Worker 1, [04/04]: Training Loss: 1.882660904, Training Accuracy: 48.364\n",
            "Time taken for training worker 1: 0:00:46.100038\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.312263519, Training Accuracy: 40.760\n",
            "Worker 2, [02/04]: Training Loss: 2.312661684, Training Accuracy: 40.772\n",
            "Worker 2, [03/04]: Training Loss: 2.310898538, Training Accuracy: 40.820\n",
            "Worker 2, [04/04]: Training Loss: 2.311670702, Training Accuracy: 40.676\n",
            "Time taken for training worker 2: 0:00:40.734083\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002505\n",
            "Global Update 09: Test Loss: 2.109314508, Test Accuracy: 45.200\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.843658392, Training Accuracy: 49.172\n",
            "Worker 1, [02/04]: Training Loss: 1.840162098, Training Accuracy: 49.288\n",
            "Worker 1, [03/04]: Training Loss: 1.836771360, Training Accuracy: 49.440\n",
            "Worker 1, [04/04]: Training Loss: 1.782170078, Training Accuracy: 50.608\n",
            "Time taken for training worker 1: 0:00:41.887810\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.308156331, Training Accuracy: 41.060\n",
            "Worker 2, [02/04]: Training Loss: 2.305518006, Training Accuracy: 40.984\n",
            "Worker 2, [03/04]: Training Loss: 2.304554293, Training Accuracy: 40.972\n",
            "Worker 2, [04/04]: Training Loss: 2.303612151, Training Accuracy: 40.920\n",
            "Time taken for training worker 2: 0:00:41.762851\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002244\n",
            "Global Update 10: Test Loss: 2.074639991, Test Accuracy: 45.840\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.762146744, Training Accuracy: 51.276\n",
            "Worker 1, [02/04]: Training Loss: 1.765373184, Training Accuracy: 50.672\n",
            "Worker 1, [03/04]: Training Loss: 1.755715993, Training Accuracy: 51.116\n",
            "Worker 1, [04/04]: Training Loss: 1.731740387, Training Accuracy: 51.964\n",
            "Time taken for training worker 1: 0:00:43.191869\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.283599345, Training Accuracy: 41.668\n",
            "Worker 2, [02/04]: Training Loss: 2.291039632, Training Accuracy: 42.020\n",
            "Worker 2, [03/04]: Training Loss: 2.276336330, Training Accuracy: 42.116\n",
            "Worker 2, [04/04]: Training Loss: 2.278945597, Training Accuracy: 41.852\n",
            "Time taken for training worker 2: 0:00:44.738420\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003410\n",
            "Global Update 11: Test Loss: 2.081502526, Test Accuracy: 45.990\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.655594611, Training Accuracy: 53.596\n",
            "Worker 1, [02/04]: Training Loss: 1.671982808, Training Accuracy: 52.936\n",
            "Worker 1, [03/04]: Training Loss: 1.679222575, Training Accuracy: 53.024\n",
            "Worker 1, [04/04]: Training Loss: 1.644154708, Training Accuracy: 54.036\n",
            "Time taken for training worker 1: 0:00:45.771894\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.285419300, Training Accuracy: 42.068\n",
            "Worker 2, [02/04]: Training Loss: 2.292909252, Training Accuracy: 41.648\n",
            "Worker 2, [03/04]: Training Loss: 2.295117097, Training Accuracy: 41.580\n",
            "Worker 2, [04/04]: Training Loss: 2.283547037, Training Accuracy: 41.944\n",
            "Time taken for training worker 2: 0:00:47.063347\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002224\n",
            "Global Update 12: Test Loss: 2.071238276, Test Accuracy: 46.900\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.581702887, Training Accuracy: 55.588\n",
            "Worker 1, [02/04]: Training Loss: 1.584567873, Training Accuracy: 55.240\n",
            "Worker 1, [03/04]: Training Loss: 1.595609221, Training Accuracy: 54.820\n",
            "Worker 1, [04/04]: Training Loss: 1.586822385, Training Accuracy: 55.056\n",
            "Time taken for training worker 1: 0:00:46.250898\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.275963863, Training Accuracy: 42.404\n",
            "Worker 2, [02/04]: Training Loss: 2.277502813, Training Accuracy: 42.416\n",
            "Worker 2, [03/04]: Training Loss: 2.260361159, Training Accuracy: 43.016\n",
            "Worker 2, [04/04]: Training Loss: 2.278714934, Training Accuracy: 42.296\n",
            "Time taken for training worker 2: 0:00:41.648042\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002326\n",
            "Global Update 13: Test Loss: 2.074464288, Test Accuracy: 47.090\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.509505503, Training Accuracy: 57.132\n",
            "Worker 1, [02/04]: Training Loss: 1.528153858, Training Accuracy: 56.868\n",
            "Worker 1, [03/04]: Training Loss: 1.515531503, Training Accuracy: 56.908\n",
            "Worker 1, [04/04]: Training Loss: 1.512269441, Training Accuracy: 57.232\n",
            "Time taken for training worker 1: 0:00:46.811993\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.268362561, Training Accuracy: 42.740\n",
            "Worker 2, [02/04]: Training Loss: 2.271126634, Training Accuracy: 42.668\n",
            "Worker 2, [03/04]: Training Loss: 2.278848640, Training Accuracy: 42.540\n",
            "Worker 2, [04/04]: Training Loss: 2.268925027, Training Accuracy: 42.688\n",
            "Time taken for training worker 2: 0:00:47.373117\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002530\n",
            "Global Update 14: Test Loss: 2.084872706, Test Accuracy: 47.070\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.422547855, Training Accuracy: 59.228\n",
            "Worker 1, [02/04]: Training Loss: 1.457096943, Training Accuracy: 58.448\n",
            "Worker 1, [03/04]: Training Loss: 1.438869520, Training Accuracy: 58.728\n",
            "Worker 1, [04/04]: Training Loss: 1.425137539, Training Accuracy: 59.012\n",
            "Time taken for training worker 1: 0:00:44.669893\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.283440630, Training Accuracy: 43.360\n",
            "Worker 2, [02/04]: Training Loss: 2.278658549, Training Accuracy: 43.256\n",
            "Worker 2, [03/04]: Training Loss: 2.277919016, Training Accuracy: 42.880\n",
            "Worker 2, [04/04]: Training Loss: 2.283837981, Training Accuracy: 42.716\n",
            "Time taken for training worker 2: 0:00:42.126452\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002363\n",
            "Global Update 15: Test Loss: 2.090106043, Test Accuracy: 47.170\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.345780015, Training Accuracy: 60.972\n",
            "Worker 1, [02/04]: Training Loss: 1.364263871, Training Accuracy: 60.748\n",
            "Worker 1, [03/04]: Training Loss: 1.392710428, Training Accuracy: 59.604\n",
            "Worker 1, [04/04]: Training Loss: 1.353489245, Training Accuracy: 60.844\n",
            "Time taken for training worker 1: 0:00:48.310757\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.279597296, Training Accuracy: 43.272\n",
            "Worker 2, [02/04]: Training Loss: 2.295081286, Training Accuracy: 43.292\n",
            "Worker 2, [03/04]: Training Loss: 2.280937563, Training Accuracy: 43.548\n",
            "Worker 2, [04/04]: Training Loss: 2.279718624, Training Accuracy: 43.596\n",
            "Time taken for training worker 2: 0:00:42.134181\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002202\n",
            "Global Update 16: Test Loss: 2.115189412, Test Accuracy: 47.670\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.294309581, Training Accuracy: 62.520\n",
            "Worker 1, [02/04]: Training Loss: 1.297440638, Training Accuracy: 62.012\n",
            "Worker 1, [03/04]: Training Loss: 1.293439674, Training Accuracy: 62.600\n",
            "Worker 1, [04/04]: Training Loss: 1.278430249, Training Accuracy: 62.668\n",
            "Time taken for training worker 1: 0:00:44.222674\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.305396654, Training Accuracy: 43.284\n",
            "Worker 2, [02/04]: Training Loss: 2.300569353, Training Accuracy: 43.672\n",
            "Worker 2, [03/04]: Training Loss: 2.315239963, Training Accuracy: 43.520\n",
            "Worker 2, [04/04]: Training Loss: 2.310639188, Training Accuracy: 43.608\n",
            "Time taken for training worker 2: 0:00:42.734881\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002135\n",
            "Global Update 17: Test Loss: 2.124318003, Test Accuracy: 47.630\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.200808937, Training Accuracy: 64.784\n",
            "Worker 1, [02/04]: Training Loss: 1.216336486, Training Accuracy: 64.568\n",
            "Worker 1, [03/04]: Training Loss: 1.227887701, Training Accuracy: 64.168\n",
            "Worker 1, [04/04]: Training Loss: 1.212830406, Training Accuracy: 64.276\n",
            "Time taken for training worker 1: 0:00:47.232056\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.313724945, Training Accuracy: 43.596\n",
            "Worker 2, [02/04]: Training Loss: 2.312358364, Training Accuracy: 43.392\n",
            "Worker 2, [03/04]: Training Loss: 2.304860411, Training Accuracy: 43.512\n",
            "Worker 2, [04/04]: Training Loss: 2.296856001, Training Accuracy: 43.592\n",
            "Time taken for training worker 2: 0:00:41.824114\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002195\n",
            "Global Update 18: Test Loss: 2.184813420, Test Accuracy: 46.810\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.120650878, Training Accuracy: 67.112\n",
            "Worker 1, [02/04]: Training Loss: 1.140330060, Training Accuracy: 66.480\n",
            "Worker 1, [03/04]: Training Loss: 1.148503480, Training Accuracy: 66.060\n",
            "Worker 1, [04/04]: Training Loss: 1.149491032, Training Accuracy: 66.200\n",
            "Time taken for training worker 1: 0:00:45.843573\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.378497918, Training Accuracy: 42.832\n",
            "Worker 2, [02/04]: Training Loss: 2.357500643, Training Accuracy: 43.144\n",
            "Worker 2, [03/04]: Training Loss: 2.378639582, Training Accuracy: 43.220\n",
            "Worker 2, [04/04]: Training Loss: 2.371573894, Training Accuracy: 43.316\n",
            "Time taken for training worker 2: 0:00:41.513169\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002221\n",
            "Global Update 19: Test Loss: 2.191096666, Test Accuracy: 47.820\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.058119986, Training Accuracy: 68.728\n",
            "Worker 1, [02/04]: Training Loss: 1.068049696, Training Accuracy: 68.288\n",
            "Worker 1, [03/04]: Training Loss: 1.074576403, Training Accuracy: 68.412\n",
            "Worker 1, [04/04]: Training Loss: 1.061155038, Training Accuracy: 68.472\n",
            "Time taken for training worker 1: 0:00:49.291208\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.379989527, Training Accuracy: 43.804\n",
            "Worker 2, [02/04]: Training Loss: 2.380847371, Training Accuracy: 43.560\n",
            "Worker 2, [03/04]: Training Loss: 2.375481840, Training Accuracy: 43.900\n",
            "Worker 2, [04/04]: Training Loss: 2.375736835, Training Accuracy: 43.836\n",
            "Time taken for training worker 2: 0:00:44.306234\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002202\n",
            "Global Update 20: Test Loss: 2.227714975, Test Accuracy: 47.780\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.977997207, Training Accuracy: 70.972\n",
            "Worker 1, [02/04]: Training Loss: 1.007659810, Training Accuracy: 70.364\n",
            "Worker 1, [03/04]: Training Loss: 0.990143790, Training Accuracy: 70.520\n",
            "Worker 1, [04/04]: Training Loss: 0.992702702, Training Accuracy: 70.296\n",
            "Time taken for training worker 1: 0:00:46.609422\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.405117529, Training Accuracy: 43.872\n",
            "Worker 2, [02/04]: Training Loss: 2.392275315, Training Accuracy: 43.852\n",
            "Worker 2, [03/04]: Training Loss: 2.396690601, Training Accuracy: 44.052\n",
            "Worker 2, [04/04]: Training Loss: 2.399132379, Training Accuracy: 43.596\n",
            "Time taken for training worker 2: 0:00:48.035991\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002649\n",
            "Global Update 21: Test Loss: 2.254812251, Test Accuracy: 48.010\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.909146500, Training Accuracy: 72.976\n",
            "Worker 1, [02/04]: Training Loss: 0.926802287, Training Accuracy: 72.392\n",
            "Worker 1, [03/04]: Training Loss: 0.922877549, Training Accuracy: 72.232\n",
            "Worker 1, [04/04]: Training Loss: 0.922465818, Training Accuracy: 72.440\n",
            "Time taken for training worker 1: 0:00:47.850827\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.424025094, Training Accuracy: 44.028\n",
            "Worker 2, [02/04]: Training Loss: 2.412629162, Training Accuracy: 44.172\n",
            "Worker 2, [03/04]: Training Loss: 2.435704378, Training Accuracy: 43.584\n",
            "Worker 2, [04/04]: Training Loss: 2.420502654, Training Accuracy: 44.100\n",
            "Time taken for training worker 2: 0:00:40.823744\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002313\n",
            "Global Update 22: Test Loss: 2.310457195, Test Accuracy: 47.570\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.845006300, Training Accuracy: 74.516\n",
            "Worker 1, [02/04]: Training Loss: 0.865575101, Training Accuracy: 73.792\n",
            "Worker 1, [03/04]: Training Loss: 0.851330572, Training Accuracy: 74.596\n",
            "Worker 1, [04/04]: Training Loss: 0.860897801, Training Accuracy: 74.280\n",
            "Time taken for training worker 1: 0:00:44.865112\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.481349502, Training Accuracy: 43.880\n",
            "Worker 2, [02/04]: Training Loss: 2.489679654, Training Accuracy: 43.792\n",
            "Worker 2, [03/04]: Training Loss: 2.490333158, Training Accuracy: 43.964\n",
            "Worker 2, [04/04]: Training Loss: 2.484470864, Training Accuracy: 43.976\n",
            "Time taken for training worker 2: 0:00:42.880182\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002421\n",
            "Global Update 23: Test Loss: 2.323558418, Test Accuracy: 48.210\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.768391133, Training Accuracy: 76.892\n",
            "Worker 1, [02/04]: Training Loss: 0.782765735, Training Accuracy: 76.492\n",
            "Worker 1, [03/04]: Training Loss: 0.768314543, Training Accuracy: 76.684\n",
            "Worker 1, [04/04]: Training Loss: 0.774265052, Training Accuracy: 76.532\n",
            "Time taken for training worker 1: 0:00:42.907682\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.511019312, Training Accuracy: 44.232\n",
            "Worker 2, [02/04]: Training Loss: 2.504618674, Training Accuracy: 44.016\n",
            "Worker 2, [03/04]: Training Loss: 2.509571175, Training Accuracy: 44.040\n",
            "Worker 2, [04/04]: Training Loss: 2.509413954, Training Accuracy: 43.924\n",
            "Time taken for training worker 2: 0:00:39.119414\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002179\n",
            "Global Update 24: Test Loss: 2.374701779, Test Accuracy: 48.320\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.697500072, Training Accuracy: 78.900\n",
            "Worker 1, [02/04]: Training Loss: 0.720830799, Training Accuracy: 78.392\n",
            "Worker 1, [03/04]: Training Loss: 0.710187012, Training Accuracy: 78.384\n",
            "Worker 1, [04/04]: Training Loss: 0.707477175, Training Accuracy: 78.344\n",
            "Time taken for training worker 1: 0:00:43.036650\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.559580713, Training Accuracy: 44.044\n",
            "Worker 2, [02/04]: Training Loss: 2.548429363, Training Accuracy: 44.132\n",
            "Worker 2, [03/04]: Training Loss: 2.545333059, Training Accuracy: 43.740\n",
            "Worker 2, [04/04]: Training Loss: 2.538072097, Training Accuracy: 44.204\n",
            "Time taken for training worker 2: 0:00:41.820700\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002120\n",
            "Global Update 25: Test Loss: 2.401973173, Test Accuracy: 48.580\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.646056274, Training Accuracy: 80.640\n",
            "Worker 1, [02/04]: Training Loss: 0.642349374, Training Accuracy: 80.664\n",
            "Worker 1, [03/04]: Training Loss: 0.655146914, Training Accuracy: 79.916\n",
            "Worker 1, [04/04]: Training Loss: 0.641067256, Training Accuracy: 80.784\n",
            "Time taken for training worker 1: 0:00:44.440874\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.585537427, Training Accuracy: 44.440\n",
            "Worker 2, [02/04]: Training Loss: 2.577922478, Training Accuracy: 44.264\n",
            "Worker 2, [03/04]: Training Loss: 2.575438018, Training Accuracy: 44.828\n",
            "Worker 2, [04/04]: Training Loss: 2.596296699, Training Accuracy: 44.496\n",
            "Time taken for training worker 2: 0:00:41.966419\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002208\n",
            "Global Update 26: Test Loss: 2.480898446, Test Accuracy: 47.990\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.592377150, Training Accuracy: 82.488\n",
            "Worker 1, [02/04]: Training Loss: 0.577370894, Training Accuracy: 82.576\n",
            "Worker 1, [03/04]: Training Loss: 0.588018556, Training Accuracy: 82.296\n",
            "Worker 1, [04/04]: Training Loss: 0.576148854, Training Accuracy: 82.616\n",
            "Time taken for training worker 1: 0:00:41.893786\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.652460218, Training Accuracy: 44.232\n",
            "Worker 2, [02/04]: Training Loss: 2.668252306, Training Accuracy: 44.232\n",
            "Worker 2, [03/04]: Training Loss: 2.642218130, Training Accuracy: 44.476\n",
            "Worker 2, [04/04]: Training Loss: 2.659669703, Training Accuracy: 43.972\n",
            "Time taken for training worker 2: 0:00:40.277279\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002139\n",
            "Global Update 27: Test Loss: 2.517506322, Test Accuracy: 47.830\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.530688599, Training Accuracy: 84.244\n",
            "Worker 1, [02/04]: Training Loss: 0.536488606, Training Accuracy: 84.108\n",
            "Worker 1, [03/04]: Training Loss: 0.532471360, Training Accuracy: 83.912\n",
            "Worker 1, [04/04]: Training Loss: 0.525237225, Training Accuracy: 84.000\n",
            "Time taken for training worker 1: 0:00:42.393175\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.695672784, Training Accuracy: 44.288\n",
            "Worker 2, [02/04]: Training Loss: 2.712048603, Training Accuracy: 44.400\n",
            "Worker 2, [03/04]: Training Loss: 2.706409440, Training Accuracy: 44.340\n",
            "Worker 2, [04/04]: Training Loss: 2.703363929, Training Accuracy: 43.984\n",
            "Time taken for training worker 2: 0:00:38.589224\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002114\n",
            "Global Update 28: Test Loss: 2.563092891, Test Accuracy: 48.480\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.480828944, Training Accuracy: 85.904\n",
            "Worker 1, [02/04]: Training Loss: 0.486324355, Training Accuracy: 85.496\n",
            "Worker 1, [03/04]: Training Loss: 0.478638964, Training Accuracy: 85.656\n",
            "Worker 1, [04/04]: Training Loss: 0.480500118, Training Accuracy: 85.680\n",
            "Time taken for training worker 1: 0:00:42.284543\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.763241793, Training Accuracy: 44.336\n",
            "Worker 2, [02/04]: Training Loss: 2.754932654, Training Accuracy: 44.700\n",
            "Worker 2, [03/04]: Training Loss: 2.760534574, Training Accuracy: 44.616\n",
            "Worker 2, [04/04]: Training Loss: 2.767212449, Training Accuracy: 44.264\n",
            "Time taken for training worker 2: 0:00:39.938340\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002389\n",
            "Global Update 29: Test Loss: 2.608207534, Test Accuracy: 48.460\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.438998438, Training Accuracy: 86.952\n",
            "Worker 1, [02/04]: Training Loss: 0.433484206, Training Accuracy: 87.348\n",
            "Worker 1, [03/04]: Training Loss: 0.436652080, Training Accuracy: 87.184\n",
            "Worker 1, [04/04]: Training Loss: 0.434368271, Training Accuracy: 87.108\n",
            "Time taken for training worker 1: 0:00:42.115343\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.802857234, Training Accuracy: 44.488\n",
            "Worker 2, [02/04]: Training Loss: 2.803416859, Training Accuracy: 44.368\n",
            "Worker 2, [03/04]: Training Loss: 2.805161520, Training Accuracy: 44.464\n",
            "Worker 2, [04/04]: Training Loss: 2.809323324, Training Accuracy: 44.412\n",
            "Time taken for training worker 2: 0:00:43.943480\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002359\n",
            "Global Update 30: Test Loss: 2.632891272, Test Accuracy: 48.270\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.398996676, Training Accuracy: 88.512\n",
            "Worker 1, [02/04]: Training Loss: 0.402407407, Training Accuracy: 88.324\n",
            "Worker 1, [03/04]: Training Loss: 0.407614748, Training Accuracy: 88.160\n",
            "Worker 1, [04/04]: Training Loss: 0.393694724, Training Accuracy: 88.672\n",
            "Time taken for training worker 1: 0:00:41.760411\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.821423592, Training Accuracy: 44.800\n",
            "Worker 2, [02/04]: Training Loss: 2.831561893, Training Accuracy: 44.580\n",
            "Worker 2, [03/04]: Training Loss: 2.828391134, Training Accuracy: 44.280\n",
            "Worker 2, [04/04]: Training Loss: 2.819139916, Training Accuracy: 44.724\n",
            "Time taken for training worker 2: 0:00:40.702521\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002063\n",
            "Global Update 31: Test Loss: 2.688733127, Test Accuracy: 48.220\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.379372207, Training Accuracy: 88.968\n",
            "Worker 1, [02/04]: Training Loss: 0.373733332, Training Accuracy: 89.296\n",
            "Worker 1, [03/04]: Training Loss: 0.373741464, Training Accuracy: 89.444\n",
            "Worker 1, [04/04]: Training Loss: 0.367598191, Training Accuracy: 89.536\n",
            "Time taken for training worker 1: 0:00:45.843253\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.880441227, Training Accuracy: 44.632\n",
            "Worker 2, [02/04]: Training Loss: 2.872810021, Training Accuracy: 44.380\n",
            "Worker 2, [03/04]: Training Loss: 2.850652012, Training Accuracy: 44.492\n",
            "Worker 2, [04/04]: Training Loss: 2.879657114, Training Accuracy: 44.300\n",
            "Time taken for training worker 2: 0:00:41.545612\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002373\n",
            "Global Update 32: Test Loss: 2.702869119, Test Accuracy: 48.740\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.349291283, Training Accuracy: 90.456\n",
            "Worker 1, [02/04]: Training Loss: 0.344230892, Training Accuracy: 90.108\n",
            "Worker 1, [03/04]: Training Loss: 0.350214688, Training Accuracy: 90.128\n",
            "Worker 1, [04/04]: Training Loss: 0.346617780, Training Accuracy: 90.364\n",
            "Time taken for training worker 1: 0:00:39.594458\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.886087475, Training Accuracy: 44.716\n",
            "Worker 2, [02/04]: Training Loss: 2.880195123, Training Accuracy: 44.400\n",
            "Worker 2, [03/04]: Training Loss: 2.884673656, Training Accuracy: 44.892\n",
            "Worker 2, [04/04]: Training Loss: 2.887375784, Training Accuracy: 45.244\n",
            "Time taken for training worker 2: 0:00:41.184257\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002417\n",
            "Global Update 33: Test Loss: 2.723349621, Test Accuracy: 48.820\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.334111507, Training Accuracy: 90.788\n",
            "Worker 1, [02/04]: Training Loss: 0.338997126, Training Accuracy: 90.592\n",
            "Worker 1, [03/04]: Training Loss: 0.330663179, Training Accuracy: 90.700\n",
            "Worker 1, [04/04]: Training Loss: 0.332895397, Training Accuracy: 90.684\n",
            "Time taken for training worker 1: 0:00:39.856913\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.907892514, Training Accuracy: 45.048\n",
            "Worker 2, [02/04]: Training Loss: 2.886841286, Training Accuracy: 44.828\n",
            "Worker 2, [03/04]: Training Loss: 2.911826805, Training Accuracy: 44.516\n",
            "Worker 2, [04/04]: Training Loss: 2.907158169, Training Accuracy: 44.800\n",
            "Time taken for training worker 2: 0:00:41.417498\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002170\n",
            "Global Update 34: Test Loss: 2.744929475, Test Accuracy: 48.710\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.321461916, Training Accuracy: 91.172\n",
            "Worker 1, [02/04]: Training Loss: 0.317137044, Training Accuracy: 91.252\n",
            "Worker 1, [03/04]: Training Loss: 0.318732448, Training Accuracy: 91.184\n",
            "Worker 1, [04/04]: Training Loss: 0.313865175, Training Accuracy: 91.348\n",
            "Time taken for training worker 1: 0:00:41.024398\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.913869377, Training Accuracy: 44.592\n",
            "Worker 2, [02/04]: Training Loss: 2.911201428, Training Accuracy: 44.972\n",
            "Worker 2, [03/04]: Training Loss: 2.903261243, Training Accuracy: 44.860\n",
            "Worker 2, [04/04]: Training Loss: 2.910728523, Training Accuracy: 45.036\n",
            "Time taken for training worker 2: 0:00:39.180142\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002100\n",
            "Global Update 35: Test Loss: 2.756003313, Test Accuracy: 48.740\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.311396068, Training Accuracy: 91.412\n",
            "Worker 1, [02/04]: Training Loss: 0.317961811, Training Accuracy: 91.208\n",
            "Worker 1, [03/04]: Training Loss: 0.312477937, Training Accuracy: 91.520\n",
            "Worker 1, [04/04]: Training Loss: 0.311096002, Training Accuracy: 91.328\n",
            "Time taken for training worker 1: 0:00:41.584378\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.945533731, Training Accuracy: 44.740\n",
            "Worker 2, [02/04]: Training Loss: 2.944628848, Training Accuracy: 44.528\n",
            "Worker 2, [03/04]: Training Loss: 2.927207515, Training Accuracy: 44.928\n",
            "Worker 2, [04/04]: Training Loss: 2.913634145, Training Accuracy: 44.788\n",
            "Time taken for training worker 2: 0:00:38.527107\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002144\n",
            "Global Update 36: Test Loss: 2.759072326, Test Accuracy: 48.960\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.313552081, Training Accuracy: 91.392\n",
            "Worker 1, [02/04]: Training Loss: 0.309598758, Training Accuracy: 91.404\n",
            "Worker 1, [03/04]: Training Loss: 0.299233171, Training Accuracy: 91.816\n",
            "Worker 1, [04/04]: Training Loss: 0.311146806, Training Accuracy: 91.552\n",
            "Time taken for training worker 1: 0:00:43.338712\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.933374654, Training Accuracy: 44.852\n",
            "Worker 2, [02/04]: Training Loss: 2.945981561, Training Accuracy: 44.864\n",
            "Worker 2, [03/04]: Training Loss: 2.949779913, Training Accuracy: 44.864\n",
            "Worker 2, [04/04]: Training Loss: 2.944696195, Training Accuracy: 44.804\n",
            "Time taken for training worker 2: 0:00:41.332581\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002235\n",
            "Global Update 37: Test Loss: 2.760830911, Test Accuracy: 48.840\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 1:04:28.965587\n",
            "//////////////////////////////////////////////////\n"
          ]
        }
      ],
      "source": [
        "# Without Fine-tuning\n",
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [2]\n",
        "J = [4]\n",
        "\n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    PerDLMask(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, index_weaker_worker=1, n_freeze_layer=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:4\n",
            "==================================================\n",
            "The following layers of worker 2 are frozen: ('conv1', 'fc2')\n",
            "Worker 1, [01/04]: Training Loss: 4.330617891, Training Accuracy: 4.248\n",
            "Worker 1, [02/04]: Training Loss: 3.867122097, Training Accuracy: 9.944\n",
            "Worker 1, [03/04]: Training Loss: 3.627806635, Training Accuracy: 13.604\n",
            "Worker 1, [04/04]: Training Loss: 3.402276817, Training Accuracy: 17.288\n",
            "Time taken for training worker 1: 0:00:45.707038\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.606128061, Training Accuracy: 0.776\n",
            "Worker 2, [02/04]: Training Loss: 4.606151497, Training Accuracy: 0.808\n",
            "Worker 2, [03/04]: Training Loss: 4.606094134, Training Accuracy: 0.816\n",
            "Worker 2, [04/04]: Training Loss: 4.606063303, Training Accuracy: 0.720\n",
            "Time taken for training worker 2: 0:00:43.457638\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.010548\n",
            "Global Update 01: Test Loss: 4.003255266, Test Accuracy: 17.050\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.417710412, Training Accuracy: 17.348\n",
            "Worker 1, [02/04]: Training Loss: 3.172522452, Training Accuracy: 21.596\n",
            "Worker 1, [03/04]: Training Loss: 3.049093383, Training Accuracy: 23.896\n",
            "Worker 1, [04/04]: Training Loss: 2.894638669, Training Accuracy: 26.840\n",
            "Time taken for training worker 1: 0:00:45.384331\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.049954731, Training Accuracy: 15.672\n",
            "Worker 2, [02/04]: Training Loss: 4.049800867, Training Accuracy: 15.508\n",
            "Worker 2, [03/04]: Training Loss: 4.050086937, Training Accuracy: 15.568\n",
            "Worker 2, [04/04]: Training Loss: 4.050202643, Training Accuracy: 15.664\n",
            "Time taken for training worker 2: 0:00:43.022293\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002371\n",
            "Global Update 02: Test Loss: 3.204459567, Test Accuracy: 28.220\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.942260532, Training Accuracy: 26.128\n",
            "Worker 1, [02/04]: Training Loss: 2.804094638, Training Accuracy: 28.960\n",
            "Worker 1, [03/04]: Training Loss: 2.687112561, Training Accuracy: 31.004\n",
            "Worker 1, [04/04]: Training Loss: 2.619325018, Training Accuracy: 32.816\n",
            "Time taken for training worker 1: 0:00:47.524965\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.320729976, Training Accuracy: 25.152\n",
            "Worker 2, [02/04]: Training Loss: 3.315218534, Training Accuracy: 25.184\n",
            "Worker 2, [03/04]: Training Loss: 3.320069284, Training Accuracy: 25.412\n",
            "Worker 2, [04/04]: Training Loss: 3.319730545, Training Accuracy: 25.204\n",
            "Time taken for training worker 2: 0:00:48.305165\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002162\n",
            "Global Update 03: Test Loss: 2.696189047, Test Accuracy: 34.780\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.649002648, Training Accuracy: 32.168\n",
            "Worker 1, [02/04]: Training Loss: 2.569098105, Training Accuracy: 33.600\n",
            "Worker 1, [03/04]: Training Loss: 2.489228577, Training Accuracy: 35.120\n",
            "Worker 1, [04/04]: Training Loss: 2.432529960, Training Accuracy: 36.624\n",
            "Time taken for training worker 1: 0:00:49.936866\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.839873024, Training Accuracy: 31.396\n",
            "Worker 2, [02/04]: Training Loss: 2.834758303, Training Accuracy: 31.588\n",
            "Worker 2, [03/04]: Training Loss: 2.835337824, Training Accuracy: 31.316\n",
            "Worker 2, [04/04]: Training Loss: 2.837919998, Training Accuracy: 31.352\n",
            "Time taken for training worker 2: 0:00:47.156527\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002177\n",
            "Global Update 04: Test Loss: 2.479120071, Test Accuracy: 37.330\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.446268178, Training Accuracy: 36.120\n",
            "Worker 1, [02/04]: Training Loss: 2.399106149, Training Accuracy: 37.020\n",
            "Worker 1, [03/04]: Training Loss: 2.341433047, Training Accuracy: 38.584\n",
            "Worker 1, [04/04]: Training Loss: 2.284110000, Training Accuracy: 39.340\n",
            "Time taken for training worker 1: 0:00:45.040859\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.641636842, Training Accuracy: 34.372\n",
            "Worker 2, [02/04]: Training Loss: 2.638006424, Training Accuracy: 34.216\n",
            "Worker 2, [03/04]: Training Loss: 2.638900560, Training Accuracy: 34.404\n",
            "Worker 2, [04/04]: Training Loss: 2.638147983, Training Accuracy: 34.268\n",
            "Time taken for training worker 2: 0:00:45.923201\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002623\n",
            "Global Update 05: Test Loss: 2.367074573, Test Accuracy: 39.850\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.299147053, Training Accuracy: 39.260\n",
            "Worker 1, [02/04]: Training Loss: 2.240206517, Training Accuracy: 40.448\n",
            "Worker 1, [03/04]: Training Loss: 2.212334927, Training Accuracy: 40.952\n",
            "Worker 1, [04/04]: Training Loss: 2.165580735, Training Accuracy: 42.116\n",
            "Time taken for training worker 1: 0:00:48.172668\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.547931373, Training Accuracy: 35.872\n",
            "Worker 2, [02/04]: Training Loss: 2.548411367, Training Accuracy: 35.792\n",
            "Worker 2, [03/04]: Training Loss: 2.545030693, Training Accuracy: 35.840\n",
            "Worker 2, [04/04]: Training Loss: 2.541664914, Training Accuracy: 35.748\n",
            "Time taken for training worker 2: 0:00:47.334968\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002745\n",
            "Global Update 06: Test Loss: 2.229542792, Test Accuracy: 41.780\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.153942680, Training Accuracy: 42.368\n",
            "Worker 1, [02/04]: Training Loss: 2.136941763, Training Accuracy: 42.772\n",
            "Worker 1, [03/04]: Training Loss: 2.092398874, Training Accuracy: 43.564\n",
            "Worker 1, [04/04]: Training Loss: 2.052643391, Training Accuracy: 44.616\n",
            "Time taken for training worker 1: 0:00:42.989593\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.411989049, Training Accuracy: 38.464\n",
            "Worker 2, [02/04]: Training Loss: 2.408435570, Training Accuracy: 38.432\n",
            "Worker 2, [03/04]: Training Loss: 2.407929143, Training Accuracy: 38.340\n",
            "Worker 2, [04/04]: Training Loss: 2.409575181, Training Accuracy: 38.544\n",
            "Time taken for training worker 2: 0:00:41.287601\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002301\n",
            "Global Update 07: Test Loss: 2.185602834, Test Accuracy: 42.960\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.034574720, Training Accuracy: 45.180\n",
            "Worker 1, [02/04]: Training Loss: 2.015285410, Training Accuracy: 45.488\n",
            "Worker 1, [03/04]: Training Loss: 1.987875858, Training Accuracy: 45.956\n",
            "Worker 1, [04/04]: Training Loss: 1.949729122, Training Accuracy: 46.788\n",
            "Time taken for training worker 1: 0:00:46.552982\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.383477178, Training Accuracy: 38.884\n",
            "Worker 2, [02/04]: Training Loss: 2.377553932, Training Accuracy: 39.504\n",
            "Worker 2, [03/04]: Training Loss: 2.379217982, Training Accuracy: 39.148\n",
            "Worker 2, [04/04]: Training Loss: 2.378790643, Training Accuracy: 38.980\n",
            "Time taken for training worker 2: 0:00:45.074854\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002908\n",
            "Global Update 08: Test Loss: 2.159826403, Test Accuracy: 43.910\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.927621184, Training Accuracy: 47.328\n",
            "Worker 1, [02/04]: Training Loss: 1.917638589, Training Accuracy: 47.528\n",
            "Worker 1, [03/04]: Training Loss: 1.892422486, Training Accuracy: 47.896\n",
            "Worker 1, [04/04]: Training Loss: 1.875752962, Training Accuracy: 48.772\n",
            "Time taken for training worker 1: 0:00:44.476732\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.355880524, Training Accuracy: 39.428\n",
            "Worker 2, [02/04]: Training Loss: 2.355207611, Training Accuracy: 39.668\n",
            "Worker 2, [03/04]: Training Loss: 2.356184652, Training Accuracy: 39.436\n",
            "Worker 2, [04/04]: Training Loss: 2.338321177, Training Accuracy: 39.804\n",
            "Time taken for training worker 2: 0:00:48.212582\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002682\n",
            "Global Update 09: Test Loss: 2.129771520, Test Accuracy: 44.650\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.831805206, Training Accuracy: 49.864\n",
            "Worker 1, [02/04]: Training Loss: 1.836107692, Training Accuracy: 49.316\n",
            "Worker 1, [03/04]: Training Loss: 1.807115665, Training Accuracy: 50.172\n",
            "Worker 1, [04/04]: Training Loss: 1.787682475, Training Accuracy: 50.780\n",
            "Time taken for training worker 1: 0:00:43.127002\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.317151038, Training Accuracy: 40.968\n",
            "Worker 2, [02/04]: Training Loss: 2.301189926, Training Accuracy: 41.116\n",
            "Worker 2, [03/04]: Training Loss: 2.306386326, Training Accuracy: 41.320\n",
            "Worker 2, [04/04]: Training Loss: 2.312171161, Training Accuracy: 40.776\n",
            "Time taken for training worker 2: 0:00:41.844260\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002249\n",
            "Global Update 10: Test Loss: 2.185733864, Test Accuracy: 43.990\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.753160617, Training Accuracy: 51.452\n",
            "Worker 1, [02/04]: Training Loss: 1.743989488, Training Accuracy: 51.696\n",
            "Worker 1, [03/04]: Training Loss: 1.741229692, Training Accuracy: 51.404\n",
            "Worker 1, [04/04]: Training Loss: 1.711714567, Training Accuracy: 52.236\n",
            "Time taken for training worker 1: 0:00:47.093923\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.368874311, Training Accuracy: 39.960\n",
            "Worker 2, [02/04]: Training Loss: 2.354935256, Training Accuracy: 40.696\n",
            "Worker 2, [03/04]: Training Loss: 2.373125487, Training Accuracy: 40.156\n",
            "Worker 2, [04/04]: Training Loss: 2.374800362, Training Accuracy: 40.128\n",
            "Time taken for training worker 2: 0:00:41.287708\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002300\n",
            "Global Update 11: Test Loss: 2.099648587, Test Accuracy: 45.890\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.655063455, Training Accuracy: 53.632\n",
            "Worker 1, [02/04]: Training Loss: 1.657841210, Training Accuracy: 53.516\n",
            "Worker 1, [03/04]: Training Loss: 1.656230810, Training Accuracy: 53.692\n",
            "Worker 1, [04/04]: Training Loss: 1.645350893, Training Accuracy: 53.840\n",
            "Time taken for training worker 1: 0:00:50.675129\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.289595203, Training Accuracy: 41.864\n",
            "Worker 2, [02/04]: Training Loss: 2.284962560, Training Accuracy: 42.148\n",
            "Worker 2, [03/04]: Training Loss: 2.301288246, Training Accuracy: 41.620\n",
            "Worker 2, [04/04]: Training Loss: 2.292992299, Training Accuracy: 41.936\n",
            "Time taken for training worker 2: 0:00:45.800060\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002662\n",
            "Global Update 12: Test Loss: 2.125074503, Test Accuracy: 46.020\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.570890167, Training Accuracy: 55.620\n",
            "Worker 1, [02/04]: Training Loss: 1.590289176, Training Accuracy: 55.116\n",
            "Worker 1, [03/04]: Training Loss: 1.580236445, Training Accuracy: 55.056\n",
            "Worker 1, [04/04]: Training Loss: 1.570085603, Training Accuracy: 55.524\n",
            "Time taken for training worker 1: 0:00:43.597169\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.325589850, Training Accuracy: 41.672\n",
            "Worker 2, [02/04]: Training Loss: 2.324356919, Training Accuracy: 41.604\n",
            "Worker 2, [03/04]: Training Loss: 2.313514590, Training Accuracy: 41.868\n",
            "Worker 2, [04/04]: Training Loss: 2.323224152, Training Accuracy: 41.408\n",
            "Time taken for training worker 2: 0:00:46.087288\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002397\n",
            "Global Update 13: Test Loss: 2.080972495, Test Accuracy: 46.650\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.503392496, Training Accuracy: 57.436\n",
            "Worker 1, [02/04]: Training Loss: 1.520998243, Training Accuracy: 57.032\n",
            "Worker 1, [03/04]: Training Loss: 1.512710848, Training Accuracy: 57.192\n",
            "Worker 1, [04/04]: Training Loss: 1.480834257, Training Accuracy: 57.748\n",
            "Time taken for training worker 1: 0:00:44.937790\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.295370292, Training Accuracy: 42.152\n",
            "Worker 2, [02/04]: Training Loss: 2.272794403, Training Accuracy: 42.360\n",
            "Worker 2, [03/04]: Training Loss: 2.287007873, Training Accuracy: 42.076\n",
            "Worker 2, [04/04]: Training Loss: 2.279287762, Training Accuracy: 42.480\n",
            "Time taken for training worker 2: 0:00:41.472541\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002294\n",
            "Global Update 14: Test Loss: 2.111752540, Test Accuracy: 46.660\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.420743057, Training Accuracy: 59.116\n",
            "Worker 1, [02/04]: Training Loss: 1.442777592, Training Accuracy: 59.244\n",
            "Worker 1, [03/04]: Training Loss: 1.432191435, Training Accuracy: 58.828\n",
            "Worker 1, [04/04]: Training Loss: 1.419402671, Training Accuracy: 59.492\n",
            "Time taken for training worker 1: 0:00:44.798690\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.302280803, Training Accuracy: 42.712\n",
            "Worker 2, [02/04]: Training Loss: 2.313453234, Training Accuracy: 42.604\n",
            "Worker 2, [03/04]: Training Loss: 2.308731271, Training Accuracy: 42.320\n",
            "Worker 2, [04/04]: Training Loss: 2.311157148, Training Accuracy: 42.408\n",
            "Time taken for training worker 2: 0:00:44.073436\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002298\n",
            "Global Update 15: Test Loss: 2.112467682, Test Accuracy: 47.040\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.337697952, Training Accuracy: 61.572\n",
            "Worker 1, [02/04]: Training Loss: 1.361379836, Training Accuracy: 60.800\n",
            "Worker 1, [03/04]: Training Loss: 1.361044769, Training Accuracy: 60.988\n",
            "Worker 1, [04/04]: Training Loss: 1.346687455, Training Accuracy: 61.420\n",
            "Time taken for training worker 1: 0:00:46.659472\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.308303305, Training Accuracy: 42.560\n",
            "Worker 2, [02/04]: Training Loss: 2.297675798, Training Accuracy: 42.996\n",
            "Worker 2, [03/04]: Training Loss: 2.293049100, Training Accuracy: 43.320\n",
            "Worker 2, [04/04]: Training Loss: 2.293316947, Training Accuracy: 42.936\n",
            "Time taken for training worker 2: 0:00:41.211239\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002244\n",
            "Global Update 16: Test Loss: 2.148763108, Test Accuracy: 46.740\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.255125967, Training Accuracy: 63.540\n",
            "Worker 1, [02/04]: Training Loss: 1.282553910, Training Accuracy: 62.988\n",
            "Worker 1, [03/04]: Training Loss: 1.291306458, Training Accuracy: 62.668\n",
            "Worker 1, [04/04]: Training Loss: 1.274614838, Training Accuracy: 62.948\n",
            "Time taken for training worker 1: 0:00:46.717667\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.322263796, Training Accuracy: 42.596\n",
            "Worker 2, [02/04]: Training Loss: 2.328519352, Training Accuracy: 42.728\n",
            "Worker 2, [03/04]: Training Loss: 2.332720014, Training Accuracy: 42.792\n",
            "Worker 2, [04/04]: Training Loss: 2.335161387, Training Accuracy: 42.580\n",
            "Time taken for training worker 2: 0:00:45.158685\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002580\n",
            "Global Update 17: Test Loss: 2.140540698, Test Accuracy: 47.950\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.195107610, Training Accuracy: 65.176\n",
            "Worker 1, [02/04]: Training Loss: 1.202428760, Training Accuracy: 64.980\n",
            "Worker 1, [03/04]: Training Loss: 1.208816005, Training Accuracy: 64.820\n",
            "Worker 1, [04/04]: Training Loss: 1.206160703, Training Accuracy: 64.828\n",
            "Time taken for training worker 1: 0:00:45.859526\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.334473483, Training Accuracy: 43.428\n",
            "Worker 2, [02/04]: Training Loss: 2.331085341, Training Accuracy: 43.740\n",
            "Worker 2, [03/04]: Training Loss: 2.329874133, Training Accuracy: 43.772\n",
            "Worker 2, [04/04]: Training Loss: 2.326597385, Training Accuracy: 43.508\n",
            "Time taken for training worker 2: 0:00:43.645792\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002562\n",
            "Global Update 18: Test Loss: 2.207414962, Test Accuracy: 47.070\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.114795453, Training Accuracy: 67.108\n",
            "Worker 1, [02/04]: Training Loss: 1.145531190, Training Accuracy: 66.472\n",
            "Worker 1, [03/04]: Training Loss: 1.141859942, Training Accuracy: 66.432\n",
            "Worker 1, [04/04]: Training Loss: 1.135997811, Training Accuracy: 66.492\n",
            "Time taken for training worker 1: 0:00:45.443226\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.406403866, Training Accuracy: 42.704\n",
            "Worker 2, [02/04]: Training Loss: 2.420389574, Training Accuracy: 42.360\n",
            "Worker 2, [03/04]: Training Loss: 2.418086850, Training Accuracy: 42.552\n",
            "Worker 2, [04/04]: Training Loss: 2.413521049, Training Accuracy: 42.588\n",
            "Time taken for training worker 2: 0:00:42.847332\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002191\n",
            "Global Update 19: Test Loss: 2.196667522, Test Accuracy: 48.250\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.047448338, Training Accuracy: 68.900\n",
            "Worker 1, [02/04]: Training Loss: 1.065433208, Training Accuracy: 68.552\n",
            "Worker 1, [03/04]: Training Loss: 1.048128628, Training Accuracy: 68.672\n",
            "Worker 1, [04/04]: Training Loss: 1.060093137, Training Accuracy: 68.768\n",
            "Time taken for training worker 1: 0:00:45.143061\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.399943361, Training Accuracy: 43.548\n",
            "Worker 2, [02/04]: Training Loss: 2.409168707, Training Accuracy: 43.664\n",
            "Worker 2, [03/04]: Training Loss: 2.399450010, Training Accuracy: 43.568\n",
            "Worker 2, [04/04]: Training Loss: 2.390169979, Training Accuracy: 43.748\n",
            "Time taken for training worker 2: 0:00:44.069218\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002330\n",
            "Global Update 20: Test Loss: 2.207076730, Test Accuracy: 48.200\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.969594505, Training Accuracy: 71.364\n",
            "Worker 1, [02/04]: Training Loss: 0.983508906, Training Accuracy: 70.708\n",
            "Worker 1, [03/04]: Training Loss: 0.989310134, Training Accuracy: 70.536\n",
            "Worker 1, [04/04]: Training Loss: 0.981003375, Training Accuracy: 70.592\n",
            "Time taken for training worker 1: 0:00:44.434216\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.404149789, Training Accuracy: 44.256\n",
            "Worker 2, [02/04]: Training Loss: 2.403618992, Training Accuracy: 44.200\n",
            "Worker 2, [03/04]: Training Loss: 2.398206185, Training Accuracy: 44.152\n",
            "Worker 2, [04/04]: Training Loss: 2.388871548, Training Accuracy: 44.348\n",
            "Time taken for training worker 2: 0:00:43.420514\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002595\n",
            "Global Update 21: Test Loss: 2.261794099, Test Accuracy: 48.160\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.894823589, Training Accuracy: 73.204\n",
            "Worker 1, [02/04]: Training Loss: 0.913586663, Training Accuracy: 72.628\n",
            "Worker 1, [03/04]: Training Loss: 0.914273663, Training Accuracy: 72.720\n",
            "Worker 1, [04/04]: Training Loss: 0.913974359, Training Accuracy: 72.676\n",
            "Time taken for training worker 1: 0:00:46.736590\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.452221514, Training Accuracy: 44.020\n",
            "Worker 2, [02/04]: Training Loss: 2.471243192, Training Accuracy: 43.872\n",
            "Worker 2, [03/04]: Training Loss: 2.470564888, Training Accuracy: 44.360\n",
            "Worker 2, [04/04]: Training Loss: 2.458356628, Training Accuracy: 44.128\n",
            "Time taken for training worker 2: 0:00:43.884837\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002494\n",
            "Global Update 22: Test Loss: 2.289482078, Test Accuracy: 48.030\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.833506616, Training Accuracy: 74.956\n",
            "Worker 1, [02/04]: Training Loss: 0.845469043, Training Accuracy: 74.780\n",
            "Worker 1, [03/04]: Training Loss: 0.835896201, Training Accuracy: 74.768\n",
            "Worker 1, [04/04]: Training Loss: 0.833586816, Training Accuracy: 74.540\n",
            "Time taken for training worker 1: 0:00:45.190812\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.472643913, Training Accuracy: 43.636\n",
            "Worker 2, [02/04]: Training Loss: 2.474934751, Training Accuracy: 44.136\n",
            "Worker 2, [03/04]: Training Loss: 2.477440183, Training Accuracy: 43.912\n",
            "Worker 2, [04/04]: Training Loss: 2.468293867, Training Accuracy: 44.236\n",
            "Time taken for training worker 2: 0:00:46.871818\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002216\n",
            "Global Update 23: Test Loss: 2.334639097, Test Accuracy: 48.110\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.758313246, Training Accuracy: 77.304\n",
            "Worker 1, [02/04]: Training Loss: 0.780885342, Training Accuracy: 76.556\n",
            "Worker 1, [03/04]: Training Loss: 0.772502640, Training Accuracy: 76.536\n",
            "Worker 1, [04/04]: Training Loss: 0.765275782, Training Accuracy: 77.084\n",
            "Time taken for training worker 1: 0:00:47.176959\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.536897045, Training Accuracy: 43.984\n",
            "Worker 2, [02/04]: Training Loss: 2.530158949, Training Accuracy: 44.060\n",
            "Worker 2, [03/04]: Training Loss: 2.535217519, Training Accuracy: 43.700\n",
            "Worker 2, [04/04]: Training Loss: 2.535348113, Training Accuracy: 43.584\n",
            "Time taken for training worker 2: 0:00:46.812345\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002688\n",
            "Global Update 24: Test Loss: 2.351665589, Test Accuracy: 48.900\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.687483458, Training Accuracy: 79.240\n",
            "Worker 1, [02/04]: Training Loss: 0.698505490, Training Accuracy: 78.728\n",
            "Worker 1, [03/04]: Training Loss: 0.697154609, Training Accuracy: 78.736\n",
            "Worker 1, [04/04]: Training Loss: 0.700114364, Training Accuracy: 78.692\n",
            "Time taken for training worker 1: 0:00:44.536570\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.540926331, Training Accuracy: 44.364\n",
            "Worker 2, [02/04]: Training Loss: 2.547048211, Training Accuracy: 44.228\n",
            "Worker 2, [03/04]: Training Loss: 2.545391493, Training Accuracy: 44.204\n",
            "Worker 2, [04/04]: Training Loss: 2.547431930, Training Accuracy: 44.440\n",
            "Time taken for training worker 2: 0:00:43.470198\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002275\n",
            "Global Update 25: Test Loss: 2.424110139, Test Accuracy: 48.200\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.624375412, Training Accuracy: 81.460\n",
            "Worker 1, [02/04]: Training Loss: 0.630895011, Training Accuracy: 80.852\n",
            "Worker 1, [03/04]: Training Loss: 0.644330053, Training Accuracy: 80.572\n",
            "Worker 1, [04/04]: Training Loss: 0.635961232, Training Accuracy: 80.876\n",
            "Time taken for training worker 1: 0:00:49.266011\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.615826226, Training Accuracy: 44.240\n",
            "Worker 2, [02/04]: Training Loss: 2.631391855, Training Accuracy: 44.224\n",
            "Worker 2, [03/04]: Training Loss: 2.614237846, Training Accuracy: 44.176\n",
            "Worker 2, [04/04]: Training Loss: 2.634831268, Training Accuracy: 43.896\n",
            "Time taken for training worker 2: 0:00:41.059971\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002268\n",
            "Global Update 26: Test Loss: 2.452308902, Test Accuracy: 48.800\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.570815705, Training Accuracy: 83.020\n",
            "Worker 1, [02/04]: Training Loss: 0.574493121, Training Accuracy: 82.832\n",
            "Worker 1, [03/04]: Training Loss: 0.577702341, Training Accuracy: 82.816\n",
            "Worker 1, [04/04]: Training Loss: 0.567022419, Training Accuracy: 82.832\n",
            "Time taken for training worker 1: 0:00:43.787158\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.658202823, Training Accuracy: 44.364\n",
            "Worker 2, [02/04]: Training Loss: 2.665821164, Training Accuracy: 44.284\n",
            "Worker 2, [03/04]: Training Loss: 2.670804455, Training Accuracy: 44.496\n",
            "Worker 2, [04/04]: Training Loss: 2.652006642, Training Accuracy: 44.416\n",
            "Time taken for training worker 2: 0:00:42.426480\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002217\n",
            "Global Update 27: Test Loss: 2.490934823, Test Accuracy: 49.030\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.517337638, Training Accuracy: 84.688\n",
            "Worker 1, [02/04]: Training Loss: 0.527825067, Training Accuracy: 84.120\n",
            "Worker 1, [03/04]: Training Loss: 0.523268529, Training Accuracy: 84.324\n",
            "Worker 1, [04/04]: Training Loss: 0.513916219, Training Accuracy: 84.324\n",
            "Time taken for training worker 1: 0:00:48.065244\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.704390058, Training Accuracy: 44.444\n",
            "Worker 2, [02/04]: Training Loss: 2.697723206, Training Accuracy: 44.392\n",
            "Worker 2, [03/04]: Training Loss: 2.706318284, Training Accuracy: 44.492\n",
            "Worker 2, [04/04]: Training Loss: 2.712896144, Training Accuracy: 44.536\n",
            "Time taken for training worker 2: 0:00:44.458673\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002561\n",
            "Global Update 28: Test Loss: 2.548858922, Test Accuracy: 48.860\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.460616647, Training Accuracy: 86.400\n",
            "Worker 1, [02/04]: Training Loss: 0.465544574, Training Accuracy: 86.260\n",
            "Worker 1, [03/04]: Training Loss: 0.469414331, Training Accuracy: 86.300\n",
            "Worker 1, [04/04]: Training Loss: 0.463524824, Training Accuracy: 86.172\n",
            "Time taken for training worker 1: 0:00:50.319780\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.757937405, Training Accuracy: 44.824\n",
            "Worker 2, [02/04]: Training Loss: 2.768289378, Training Accuracy: 44.692\n",
            "Worker 2, [03/04]: Training Loss: 2.773738043, Training Accuracy: 44.084\n",
            "Worker 2, [04/04]: Training Loss: 2.754239089, Training Accuracy: 44.556\n",
            "Time taken for training worker 2: 0:00:44.889805\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002564\n",
            "Global Update 29: Test Loss: 2.586586554, Test Accuracy: 49.010\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.425759175, Training Accuracy: 87.480\n",
            "Worker 1, [02/04]: Training Loss: 0.419647288, Training Accuracy: 87.848\n",
            "Worker 1, [03/04]: Training Loss: 0.428621679, Training Accuracy: 87.420\n",
            "Worker 1, [04/04]: Training Loss: 0.428880318, Training Accuracy: 87.840\n",
            "Time taken for training worker 1: 0:00:43.340628\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.797838189, Training Accuracy: 44.500\n",
            "Worker 2, [02/04]: Training Loss: 2.794114733, Training Accuracy: 44.620\n",
            "Worker 2, [03/04]: Training Loss: 2.793004945, Training Accuracy: 44.428\n",
            "Worker 2, [04/04]: Training Loss: 2.785320118, Training Accuracy: 44.728\n",
            "Time taken for training worker 2: 0:00:43.243135\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002619\n",
            "Global Update 30: Test Loss: 2.635350821, Test Accuracy: 48.930\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.397377260, Training Accuracy: 88.616\n",
            "Worker 1, [02/04]: Training Loss: 0.390847933, Training Accuracy: 88.756\n",
            "Worker 1, [03/04]: Training Loss: 0.400662135, Training Accuracy: 88.204\n",
            "Worker 1, [04/04]: Training Loss: 0.378506220, Training Accuracy: 88.992\n",
            "Time taken for training worker 1: 0:00:45.047242\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.845680237, Training Accuracy: 44.632\n",
            "Worker 2, [02/04]: Training Loss: 2.841875587, Training Accuracy: 44.476\n",
            "Worker 2, [03/04]: Training Loss: 2.856440870, Training Accuracy: 44.816\n",
            "Worker 2, [04/04]: Training Loss: 2.854091608, Training Accuracy: 44.376\n",
            "Time taken for training worker 2: 0:00:42.509866\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002282\n",
            "Global Update 31: Test Loss: 2.670308863, Test Accuracy: 48.900\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.366089467, Training Accuracy: 89.608\n",
            "Worker 1, [02/04]: Training Loss: 0.365634675, Training Accuracy: 89.540\n",
            "Worker 1, [03/04]: Training Loss: 0.358482014, Training Accuracy: 89.660\n",
            "Worker 1, [04/04]: Training Loss: 0.359091125, Training Accuracy: 89.888\n",
            "Time taken for training worker 1: 0:00:44.244145\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.890666258, Training Accuracy: 44.700\n",
            "Worker 2, [02/04]: Training Loss: 2.884674978, Training Accuracy: 44.620\n",
            "Worker 2, [03/04]: Training Loss: 2.892043115, Training Accuracy: 44.544\n",
            "Worker 2, [04/04]: Training Loss: 2.886340104, Training Accuracy: 44.708\n",
            "Time taken for training worker 2: 0:00:42.486183\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002165\n",
            "Global Update 32: Test Loss: 2.705469657, Test Accuracy: 49.150\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.345152472, Training Accuracy: 90.380\n",
            "Worker 1, [02/04]: Training Loss: 0.340219339, Training Accuracy: 90.408\n",
            "Worker 1, [03/04]: Training Loss: 0.344349261, Training Accuracy: 90.076\n",
            "Worker 1, [04/04]: Training Loss: 0.330282234, Training Accuracy: 90.644\n",
            "Time taken for training worker 1: 0:00:45.128665\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.910251633, Training Accuracy: 44.736\n",
            "Worker 2, [02/04]: Training Loss: 2.917440017, Training Accuracy: 44.584\n",
            "Worker 2, [03/04]: Training Loss: 2.926663493, Training Accuracy: 44.432\n",
            "Worker 2, [04/04]: Training Loss: 2.909593316, Training Accuracy: 45.112\n",
            "Time taken for training worker 2: 0:00:41.598578\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002280\n",
            "Global Update 33: Test Loss: 2.719047173, Test Accuracy: 48.930\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.325640468, Training Accuracy: 91.024\n",
            "Worker 1, [02/04]: Training Loss: 0.319722521, Training Accuracy: 91.176\n",
            "Worker 1, [03/04]: Training Loss: 0.319843589, Training Accuracy: 91.336\n",
            "Worker 1, [04/04]: Training Loss: 0.319751289, Training Accuracy: 91.076\n",
            "Time taken for training worker 1: 0:00:41.735544\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.918157985, Training Accuracy: 45.008\n",
            "Worker 2, [02/04]: Training Loss: 2.941432048, Training Accuracy: 44.640\n",
            "Worker 2, [03/04]: Training Loss: 2.929111801, Training Accuracy: 44.712\n",
            "Worker 2, [04/04]: Training Loss: 2.937212374, Training Accuracy: 44.716\n",
            "Time taken for training worker 2: 0:00:38.342247\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002123\n",
            "Global Update 34: Test Loss: 2.735910437, Test Accuracy: 48.860\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.311963116, Training Accuracy: 91.372\n",
            "Worker 1, [02/04]: Training Loss: 0.316767076, Training Accuracy: 91.304\n",
            "Worker 1, [03/04]: Training Loss: 0.317832516, Training Accuracy: 91.160\n",
            "Worker 1, [04/04]: Training Loss: 0.308035041, Training Accuracy: 91.712\n",
            "Time taken for training worker 1: 0:00:44.999326\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.950925973, Training Accuracy: 44.528\n",
            "Worker 2, [02/04]: Training Loss: 2.942691626, Training Accuracy: 44.672\n",
            "Worker 2, [03/04]: Training Loss: 2.937952768, Training Accuracy: 44.880\n",
            "Worker 2, [04/04]: Training Loss: 2.951747107, Training Accuracy: 44.552\n",
            "Time taken for training worker 2: 0:00:38.148169\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002177\n",
            "Global Update 35: Test Loss: 2.745666642, Test Accuracy: 49.040\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 0.307144130, Training Accuracy: 91.764\n",
            "Worker 1, [02/04]: Training Loss: 0.305067943, Training Accuracy: 91.808\n",
            "Worker 1, [03/04]: Training Loss: 0.300716196, Training Accuracy: 91.800\n",
            "Worker 1, [04/04]: Training Loss: 0.302247779, Training Accuracy: 91.856\n",
            "Time taken for training worker 1: 0:00:43.727121\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.975002293, Training Accuracy: 44.520\n",
            "Worker 2, [02/04]: Training Loss: 2.942796383, Training Accuracy: 45.284\n",
            "Worker 2, [03/04]: Training Loss: 2.957948082, Training Accuracy: 45.044\n",
            "Worker 2, [04/04]: Training Loss: 2.973068567, Training Accuracy: 44.812\n",
            "Time taken for training worker 2: 0:00:39.964844\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002121\n",
            "Global Update 36: Test Loss: 2.751714091, Test Accuracy: 49.080\n",
            "**************************************************\n",
            "Worker 1, [04/04]: Training Loss: 2.862783036, Training Accuracy: 44.896\n",
            "Worker 1, [04/04]: Training Loss: 2.736489902, Training Accuracy: 44.752\n",
            "Worker 1, [04/04]: Training Loss: 2.637911931, Training Accuracy: 44.656\n",
            "Worker 1, [04/04]: Training Loss: 2.581400952, Training Accuracy: 44.308\n",
            "Time taken for training worker 1: -1 day, 23:59:59.181458\n",
            "--------------------------------------------------\n",
            "Worker 2, [04/04]: Training Loss: 2.987989635, Training Accuracy: 44.776\n",
            "Worker 2, [04/04]: Training Loss: 2.953604646, Training Accuracy: 45.148\n",
            "Worker 2, [04/04]: Training Loss: 2.972935549, Training Accuracy: 44.668\n",
            "Worker 2, [04/04]: Training Loss: 2.978928691, Training Accuracy: 44.676\n",
            "Time taken for training worker 2: -1 day, 23:59:18.056221\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002215\n",
            "Global Update 36: Test Loss: 2.543037563, Test Accuracy: 49.050\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 1:04:52.042812\n",
            "//////////////////////////////////////////////////\n"
          ]
        }
      ],
      "source": [
        "# With Fine-tuning\n",
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [2]\n",
        "J = [4]\n",
        "\n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    PerDLMask(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, index_weaker_worker=1, n_freeze_layer=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Our Approach 2: Confidence Interval Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def update_CI_Acc(accuracies, C_level=0.8):\n",
        "    if len(accuracies) == 1:\n",
        "        return 0.0\n",
        "    mean = np.mean(accuracies)\n",
        "    std_dev = np.std(accuracies, ddof=1)\n",
        "    t_score = stats.t.ppf(1 - (1 - C_level) / 2, len(accuracies) - 1)\n",
        "    CI = t_score * (std_dev / np.sqrt(len(accuracies)))\n",
        "    # Compute relative error\n",
        "    RE = CI / mean\n",
        "    # Compute accuracy\n",
        "    accuracy = 1 - RE\n",
        "\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def ConfidenceInetrvalApproach(shard_loaders, loss_fn, k, lr, wd, initial_state_dict, num_epochs, ci_acc_threshold):\n",
        "  total_start_time = time.time()\n",
        "#   print('Start Time:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_start_time)))\n",
        "  \n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  train_accuracy_workers = {i: [] for i in range(k)}\n",
        "  steps_per_worker = {i: 0 for i in range(k)}\n",
        "  accuracy_workers = {i: 0 for i in range(k)}\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "  \n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "  global_optimizer = LocalSGDOptimizer(global_model, lr=lr)\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=num_epochs)\n",
        "\n",
        "  checkpoint = load_checkpoint('ci-strategy', 64, {'k': k, 'threshold': ci_acc_threshold})\n",
        "  if checkpoint is not None:\n",
        "      global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      global_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      \n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn)\n",
        "      print(f'Global Update: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      \n",
        "      return None\n",
        "  \n",
        "  total_epochs = 0\n",
        "  steps_per_worker = {i: 0 for i in range(k)}\n",
        "  while not all(value > num_epochs for value in steps_per_worker.values()): \n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      if steps_per_worker[worker] > num_epochs:\n",
        "        continue\n",
        "      \n",
        "      train_start_time = time.time()\n",
        "      step_counter = 0\n",
        "      while not accuracy_workers[worker] > ci_acc_threshold:\n",
        "        \n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        train_accuracy_workers[worker].append(train_accuracy)\n",
        "\n",
        "        accuracy_workers[worker] = update_CI_Acc(train_accuracy_workers[worker])\n",
        "        steps_per_worker[worker] += 1\n",
        "        step_counter += 1\n",
        "        print(accuracy_workers[worker])\n",
        "        print(f'Worker {worker+1}, [epoch: {step_counter:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "        if steps_per_worker[worker] == num_epochs:\n",
        "          break\n",
        "      train_accuracy_workers[worker] = []\n",
        "      accuracy_workers[worker] = 0  \n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time() \n",
        "\n",
        "    global_optimizer.step(local_models)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    for local_model in local_models:\n",
        "       local_model.load_state_dict(global_model.state_dict())\n",
        "    \n",
        "    for local_optimizer in local_optimizers:\n",
        "        local_optimizer.param_groups[0]['lr'] = global_optimizer.param_groups[0]['lr']\n",
        "\n",
        "    total_epochs += step_counter\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Global Model: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "  \n",
        "  # Save checkpoint\n",
        "  save_checkpoint({\n",
        "              'epoch': num_epochs,\n",
        "              'model_state_dict': global_model.state_dict(),\n",
        "              'optimizer_state_dict': global_optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss': loss_fn\n",
        "              }, 149, 64, 'ci-strategy', {'k': k, 'threshold': ci_acc_threshold}) \n",
        "  \n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for Confidence Interval: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2, CI Accuracy Threshold:0.8\n",
            "==================================================\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 4.319634652, Training Accuracy: 4.496\n",
            "-0.20474564017392738\n",
            "Worker 1, [epoch: 02]: Training Loss: 3.862335460, Training Accuracy: 10.280\n",
            "0.45577287919592036\n",
            "Worker 1, [epoch: 03]: Training Loss: 3.620782297, Training Accuracy: 14.020\n",
            "0.6149884133219583\n",
            "Worker 1, [epoch: 04]: Training Loss: 3.423515814, Training Accuracy: 16.972\n",
            "0.6795624333300601\n",
            "Worker 1, [epoch: 05]: Training Loss: 3.245273922, Training Accuracy: 20.600\n",
            "0.7233443707065708\n",
            "Worker 1, [epoch: 06]: Training Loss: 3.102693468, Training Accuracy: 23.100\n",
            "0.755013067617014\n",
            "Worker 1, [epoch: 07]: Training Loss: 2.974448443, Training Accuracy: 25.300\n",
            "0.7768931987200809\n",
            "Worker 1, [epoch: 08]: Training Loss: 2.860033102, Training Accuracy: 27.908\n",
            "0.7958419882048056\n",
            "Worker 1, [epoch: 09]: Training Loss: 2.768209924, Training Accuracy: 29.436\n",
            "0.8108080038803376\n",
            "Worker 1, [epoch: 10]: Training Loss: 2.675107944, Training Accuracy: 31.360\n",
            "Time taken for training worker 1: 0:01:56.191734\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 4.319740827, Training Accuracy: 4.424\n",
            "-0.2332413848632744\n",
            "Worker 2, [epoch: 02]: Training Loss: 3.861710054, Training Accuracy: 10.340\n",
            "0.4485900530514152\n",
            "Worker 2, [epoch: 03]: Training Loss: 3.609813855, Training Accuracy: 14.084\n",
            "0.6062304976768762\n",
            "Worker 2, [epoch: 04]: Training Loss: 3.413179124, Training Accuracy: 17.380\n",
            "0.6769040352758486\n",
            "Worker 2, [epoch: 05]: Training Loss: 3.234335804, Training Accuracy: 20.660\n",
            "0.7208549094678605\n",
            "Worker 2, [epoch: 06]: Training Loss: 3.102239981, Training Accuracy: 23.432\n",
            "0.7525392005648777\n",
            "Worker 2, [epoch: 07]: Training Loss: 2.965689730, Training Accuracy: 25.740\n",
            "0.7764632739300767\n",
            "Worker 2, [epoch: 08]: Training Loss: 2.854625733, Training Accuracy: 27.856\n",
            "0.794409163441575\n",
            "Worker 2, [epoch: 09]: Training Loss: 2.755446550, Training Accuracy: 30.128\n",
            "0.8096231234012554\n",
            "Worker 2, [epoch: 10]: Training Loss: 2.680755844, Training Accuracy: 31.768\n",
            "Time taken for training worker 2: 0:01:53.227984\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001556\n",
            "Global Model: Test Loss: 2.977438667, Test Accuracy: 31.330\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.790977798, Training Accuracy: 29.244\n",
            "0.8709397546970912\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.653506855, Training Accuracy: 31.804\n",
            "Time taken for training worker 1: 0:00:21.034096\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.784744902, Training Accuracy: 29.288\n",
            "0.8788631512513327\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.651078436, Training Accuracy: 31.688\n",
            "Time taken for training worker 2: 0:00:22.135402\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001764\n",
            "Global Model: Test Loss: 2.419150829, Test Accuracy: 37.950\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.603672387, Training Accuracy: 32.804\n",
            "0.9301318341371522\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.538803199, Training Accuracy: 34.328\n",
            "Time taken for training worker 1: 0:00:20.821431\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.599679349, Training Accuracy: 33.224\n",
            "0.9197063273695565\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.516485822, Training Accuracy: 35.004\n",
            "Time taken for training worker 2: 0:00:20.913339\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001661\n",
            "Global Model: Test Loss: 2.295766065, Test Accuracy: 40.120\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.473755859, Training Accuracy: 35.928\n",
            "0.9574233502757812\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.425681436, Training Accuracy: 36.936\n",
            "Time taken for training worker 1: 0:00:20.975170\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.460603404, Training Accuracy: 35.988\n",
            "0.935037470762886\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.402163342, Training Accuracy: 37.540\n",
            "Time taken for training worker 2: 0:00:20.416171\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001578\n",
            "Global Model: Test Loss: 2.215192807, Test Accuracy: 41.500\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.372168663, Training Accuracy: 37.716\n",
            "0.9524384292046826\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.318799976, Training Accuracy: 38.900\n",
            "Time taken for training worker 1: 0:00:20.672050\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.383469453, Training Accuracy: 38.164\n",
            "0.9622438532885631\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.317338220, Training Accuracy: 39.112\n",
            "Time taken for training worker 2: 0:00:20.881202\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001634\n",
            "Global Model: Test Loss: 2.124216333, Test Accuracy: 43.540\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.307553719, Training Accuracy: 39.060\n",
            "0.9374736978896835\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.240250301, Training Accuracy: 40.680\n",
            "Time taken for training worker 1: 0:00:23.127039\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.304887062, Training Accuracy: 39.492\n",
            "0.9654781616257111\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.248252892, Training Accuracy: 40.388\n",
            "Time taken for training worker 2: 0:00:21.955564\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001623\n",
            "Global Model: Test Loss: 2.101092462, Test Accuracy: 44.590\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.228800948, Training Accuracy: 41.060\n",
            "0.9750192852328288\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.195096499, Training Accuracy: 41.732\n",
            "Time taken for training worker 1: 0:00:20.431734\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.212260143, Training Accuracy: 41.748\n",
            "0.9810969301385606\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.180455955, Training Accuracy: 42.264\n",
            "Time taken for training worker 2: 0:00:20.434648\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001664\n",
            "Global Model: Test Loss: 2.098086404, Test Accuracy: 44.820\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.159056598, Training Accuracy: 42.560\n",
            "0.962709465521042\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.114652725, Training Accuracy: 43.604\n",
            "Time taken for training worker 1: 0:00:22.210961\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.169996746, Training Accuracy: 42.464\n",
            "0.9648911641935133\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.122512128, Training Accuracy: 43.444\n",
            "Time taken for training worker 2: 0:00:19.878117\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001647\n",
            "Global Model: Test Loss: 2.018465024, Test Accuracy: 46.950\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.114718986, Training Accuracy: 43.448\n",
            "0.980572668185222\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.075208890, Training Accuracy: 44.000\n",
            "Time taken for training worker 1: 0:00:20.146110\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.105928786, Training Accuracy: 43.840\n",
            "0.9749328942670857\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.065323953, Training Accuracy: 44.560\n",
            "Time taken for training worker 2: 0:00:21.296344\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001818\n",
            "Global Model: Test Loss: 2.014875177, Test Accuracy: 47.770\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.056091145, Training Accuracy: 44.848\n",
            "0.9845685654832285\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.026176872, Training Accuracy: 45.300\n",
            "Time taken for training worker 1: 0:00:20.639076\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.056731375, Training Accuracy: 44.824\n",
            "0.9766955725606634\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.028656315, Training Accuracy: 45.508\n",
            "Time taken for training worker 2: 0:00:19.903321\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001631\n",
            "Global Model: Test Loss: 1.938817878, Test Accuracy: 48.040\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.020949194, Training Accuracy: 45.528\n",
            "0.9807871503985368\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.986265012, Training Accuracy: 46.100\n",
            "Time taken for training worker 1: 0:00:20.249864\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.027315200, Training Accuracy: 45.616\n",
            "0.968612783437754\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.982483064, Training Accuracy: 46.556\n",
            "Time taken for training worker 2: 0:00:20.297255\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001628\n",
            "Global Model: Test Loss: 1.918680740, Test Accuracy: 48.790\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.979299313, Training Accuracy: 46.184\n",
            "0.9746214035756786\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.957014597, Training Accuracy: 46.952\n",
            "Time taken for training worker 1: 0:00:20.055515\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.986840284, Training Accuracy: 46.372\n",
            "0.9692494471517239\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.953518001, Training Accuracy: 47.308\n",
            "Time taken for training worker 2: 0:00:20.154300\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001608\n",
            "Global Model: Test Loss: 1.914520012, Test Accuracy: 48.870\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.948395450, Training Accuracy: 47.096\n",
            "0.9841369923304032\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.928208088, Training Accuracy: 47.584\n",
            "Time taken for training worker 1: 0:00:20.746018\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.953390591, Training Accuracy: 47.396\n",
            "0.9935201205633996\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.925858638, Training Accuracy: 47.596\n",
            "Time taken for training worker 2: 0:00:20.775854\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001684\n",
            "Global Model: Test Loss: 1.906948147, Test Accuracy: 49.240\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.910702125, Training Accuracy: 47.900\n",
            "0.9911586592253074\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.899486451, Training Accuracy: 48.176\n",
            "Time taken for training worker 1: 0:00:22.029306\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.923510865, Training Accuracy: 48.060\n",
            "0.9788851269971693\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.895788444, Training Accuracy: 48.724\n",
            "Time taken for training worker 2: 0:00:21.538505\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001774\n",
            "Global Model: Test Loss: 1.873784651, Test Accuracy: 49.870\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.886710266, Training Accuracy: 48.744\n",
            "0.9898052440799153\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.866726283, Training Accuracy: 49.068\n",
            "Time taken for training worker 1: 0:00:21.035379\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.901724501, Training Accuracy: 48.268\n",
            "0.9654504552116135\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.864276056, Training Accuracy: 49.364\n",
            "Time taken for training worker 2: 0:00:21.692526\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001755\n",
            "Global Model: Test Loss: 1.891762765, Test Accuracy: 50.240\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.866176663, Training Accuracy: 49.008\n",
            "0.9869928849207733\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.849476558, Training Accuracy: 49.424\n",
            "Time taken for training worker 1: 0:00:20.736809\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.868813706, Training Accuracy: 49.164\n",
            "0.9782447542508639\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.828478138, Training Accuracy: 49.864\n",
            "Time taken for training worker 2: 0:00:20.363383\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001647\n",
            "Global Model: Test Loss: 1.852337546, Test Accuracy: 50.910\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.852818051, Training Accuracy: 49.528\n",
            "0.9602531303389936\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.809980781, Training Accuracy: 50.824\n",
            "Time taken for training worker 1: 0:00:20.258628\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.856588157, Training Accuracy: 49.532\n",
            "0.9802443486338261\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.815111308, Training Accuracy: 50.172\n",
            "Time taken for training worker 2: 0:00:20.229938\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001862\n",
            "Global Model: Test Loss: 1.875629689, Test Accuracy: 50.000\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.814364647, Training Accuracy: 50.492\n",
            "0.9991468821621011\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.797165285, Training Accuracy: 50.520\n",
            "Time taken for training worker 1: 0:00:20.475331\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.822389997, Training Accuracy: 50.048\n",
            "0.9798397005185087\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.796337615, Training Accuracy: 50.708\n",
            "Time taken for training worker 2: 0:00:20.750744\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001606\n",
            "Global Model: Test Loss: 1.836170903, Test Accuracy: 51.220\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.811905309, Training Accuracy: 50.460\n",
            "0.9854310838475371\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.775861538, Training Accuracy: 50.940\n",
            "Time taken for training worker 1: 0:00:20.234640\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.792341030, Training Accuracy: 50.840\n",
            "0.9793153497811918\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.770899421, Training Accuracy: 51.528\n",
            "Time taken for training worker 2: 0:00:20.134090\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001613\n",
            "Global Model: Test Loss: 1.835899406, Test Accuracy: 51.160\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.763375512, Training Accuracy: 51.096\n",
            "0.9912309863699242\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.761983188, Training Accuracy: 51.388\n",
            "Time taken for training worker 1: 0:00:19.827853\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.778232757, Training Accuracy: 51.412\n",
            "0.9842768702333219\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.749314112, Training Accuracy: 51.940\n",
            "Time taken for training worker 2: 0:00:21.671707\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001628\n",
            "Global Model: Test Loss: 1.831301604, Test Accuracy: 51.640\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.774407337, Training Accuracy: 50.852\n",
            "0.9750269885410305\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.737799920, Training Accuracy: 51.684\n",
            "Time taken for training worker 1: 0:00:21.175545\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.744343520, Training Accuracy: 51.840\n",
            "0.9797117289437476\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.725226737, Training Accuracy: 52.528\n",
            "Time taken for training worker 2: 0:00:20.982007\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002549\n",
            "Global Model: Test Loss: 1.816183610, Test Accuracy: 52.050\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.744687830, Training Accuracy: 51.732\n",
            "0.974980101622515\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.719478906, Training Accuracy: 52.580\n",
            "Time taken for training worker 1: 0:00:19.962336\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.728774122, Training Accuracy: 52.384\n",
            "0.9954104822410009\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.730167749, Training Accuracy: 52.228\n",
            "Time taken for training worker 2: 0:00:20.144128\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001651\n",
            "Global Model: Test Loss: 1.833767150, Test Accuracy: 51.440\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.720091676, Training Accuracy: 52.120\n",
            "0.9707555726225028\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.696807403, Training Accuracy: 53.120\n",
            "Time taken for training worker 1: 0:00:21.569188\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.712241541, Training Accuracy: 52.960\n",
            "0.9961692921688283\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.686146581, Training Accuracy: 53.092\n",
            "Time taken for training worker 2: 0:00:21.424924\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001603\n",
            "Global Model: Test Loss: 1.789593533, Test Accuracy: 52.440\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.698607999, Training Accuracy: 53.304\n",
            "0.9936356508423345\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.686875822, Training Accuracy: 53.084\n",
            "Time taken for training worker 1: 0:00:20.429609\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.693462103, Training Accuracy: 53.140\n",
            "0.9980320976367197\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.680109063, Training Accuracy: 53.208\n",
            "Time taken for training worker 2: 0:00:20.529708\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001523\n",
            "Global Model: Test Loss: 1.796041232, Test Accuracy: 51.840\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.685518838, Training Accuracy: 52.940\n",
            "0.9883013045328369\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.673778529, Training Accuracy: 53.344\n",
            "Time taken for training worker 1: 0:00:23.042593\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.694905256, Training Accuracy: 53.184\n",
            "0.9912290404669344\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.680082473, Training Accuracy: 53.488\n",
            "Time taken for training worker 2: 0:00:21.918672\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001786\n",
            "Global Model: Test Loss: 1.804950924, Test Accuracy: 51.900\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.677514551, Training Accuracy: 53.552\n",
            "0.9966702920134667\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.659085451, Training Accuracy: 53.668\n",
            "Time taken for training worker 1: 0:00:22.837903\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.673709916, Training Accuracy: 53.820\n",
            "0.9898546674020706\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.656179202, Training Accuracy: 54.176\n",
            "Time taken for training worker 2: 0:00:22.875929\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001812\n",
            "Global Model: Test Loss: 1.820091216, Test Accuracy: 52.320\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.658207440, Training Accuracy: 53.880\n",
            "0.9932744424350807\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.655838392, Training Accuracy: 54.116\n",
            "Time taken for training worker 1: 0:00:21.166711\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.651718483, Training Accuracy: 53.860\n",
            "0.9830652960949826\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.631474057, Training Accuracy: 54.456\n",
            "Time taken for training worker 2: 0:00:22.843291\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001842\n",
            "Global Model: Test Loss: 1.807806004, Test Accuracy: 51.800\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.646232785, Training Accuracy: 54.112\n",
            "0.9921710538592687\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.634556618, Training Accuracy: 54.388\n",
            "Time taken for training worker 1: 0:00:20.020504\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.631586599, Training Accuracy: 54.936\n",
            "0.9979844901524506\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.618832821, Training Accuracy: 55.008\n",
            "Time taken for training worker 2: 0:00:21.108881\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001689\n",
            "Global Model: Test Loss: 1.798324236, Test Accuracy: 52.350\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.638757539, Training Accuracy: 54.164\n",
            "0.9734119146796604\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.609279944, Training Accuracy: 55.108\n",
            "Time taken for training worker 1: 0:00:23.057268\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.642197824, Training Accuracy: 54.556\n",
            "0.9824991385942837\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.601074036, Training Accuracy: 55.180\n",
            "Time taken for training worker 2: 0:00:22.823416\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001847\n",
            "Global Model: Test Loss: 1.772550632, Test Accuracy: 53.080\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.630414961, Training Accuracy: 54.836\n",
            "0.9805915868558222\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.589942573, Training Accuracy: 55.532\n",
            "Time taken for training worker 1: 0:00:21.711221\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.605248513, Training Accuracy: 55.460\n",
            "0.9936866894007684\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.592015096, Training Accuracy: 55.688\n",
            "Time taken for training worker 2: 0:00:22.006670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001794\n",
            "Global Model: Test Loss: 1.774140305, Test Accuracy: 53.180\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.606466497, Training Accuracy: 55.280\n",
            "0.9931118193882433\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.575289770, Training Accuracy: 55.528\n",
            "Time taken for training worker 1: 0:00:21.640645\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.602234808, Training Accuracy: 55.304\n",
            "0.9863706543626801\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.577837949, Training Accuracy: 55.796\n",
            "Time taken for training worker 2: 0:00:21.405740\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001554\n",
            "Global Model: Test Loss: 1.794419835, Test Accuracy: 53.290\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.596592760, Training Accuracy: 55.380\n",
            "0.9916864302074344\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.577092058, Training Accuracy: 55.680\n",
            "Time taken for training worker 1: 0:00:20.140532\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.591693448, Training Accuracy: 55.632\n",
            "0.9901840589567642\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.577430859, Training Accuracy: 55.988\n",
            "Time taken for training worker 2: 0:00:20.442120\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001721\n",
            "Global Model: Test Loss: 1.785122434, Test Accuracy: 52.980\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.582969681, Training Accuracy: 55.796\n",
            "0.9796165383164559\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.546783993, Training Accuracy: 56.540\n",
            "Time taken for training worker 1: 0:00:19.828050\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.570705205, Training Accuracy: 56.096\n",
            "0.9971443946910498\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.567679912, Training Accuracy: 55.992\n",
            "Time taken for training worker 2: 0:00:20.524169\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001719\n",
            "Global Model: Test Loss: 1.783272216, Test Accuracy: 52.830\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.584665192, Training Accuracy: 55.752\n",
            "0.981453787919839\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.551846580, Training Accuracy: 56.428\n",
            "Time taken for training worker 1: 0:00:19.910381\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.565799004, Training Accuracy: 56.280\n",
            "0.9816267435166159\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.539702024, Training Accuracy: 56.956\n",
            "Time taken for training worker 2: 0:00:19.769179\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001716\n",
            "Global Model: Test Loss: 1.793705444, Test Accuracy: 52.710\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.547243762, Training Accuracy: 56.588\n",
            "0.997174443080247\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.546507394, Training Accuracy: 56.692\n",
            "Time taken for training worker 1: 0:00:20.262012\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.561341460, Training Accuracy: 56.156\n",
            "0.9902753635526859\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.536936691, Training Accuracy: 56.512\n",
            "Time taken for training worker 2: 0:00:20.596621\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001562\n",
            "Global Model: Test Loss: 1.768936154, Test Accuracy: 53.500\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.557308368, Training Accuracy: 56.412\n",
            "0.9757509281626962\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.521741041, Training Accuracy: 57.308\n",
            "Time taken for training worker 1: 0:00:21.593436\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.539330852, Training Accuracy: 56.936\n",
            "0.9946934623553545\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.543707936, Training Accuracy: 56.740\n",
            "Time taken for training worker 2: 0:00:21.629598\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001607\n",
            "Global Model: Test Loss: 1.795588341, Test Accuracy: 52.990\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.523487415, Training Accuracy: 57.244\n",
            "0.9933187542259495\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.519744820, Training Accuracy: 56.996\n",
            "Time taken for training worker 1: 0:00:20.945994\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.523559914, Training Accuracy: 57.380\n",
            "0.9959181915952151\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.509396544, Training Accuracy: 57.228\n",
            "Time taken for training worker 2: 0:00:22.525730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001660\n",
            "Global Model: Test Loss: 1.784875067, Test Accuracy: 52.870\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.521105985, Training Accuracy: 57.272\n",
            "0.994742670617982\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.513567635, Training Accuracy: 57.468\n",
            "Time taken for training worker 1: 0:00:21.530069\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.512642539, Training Accuracy: 57.540\n",
            "0.9866859165201254\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.480087568, Training Accuracy: 58.040\n",
            "Time taken for training worker 2: 0:00:22.409888\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001803\n",
            "Global Model: Test Loss: 1.774135938, Test Accuracy: 53.670\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.498031683, Training Accuracy: 58.116\n",
            "0.9811381189679796\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.505402153, Training Accuracy: 57.408\n",
            "Time taken for training worker 1: 0:00:19.908963\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.508628679, Training Accuracy: 57.428\n",
            "0.9854916824589163\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.493248263, Training Accuracy: 57.972\n",
            "Time taken for training worker 2: 0:00:20.025086\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001681\n",
            "Global Model: Test Loss: 1.774756246, Test Accuracy: 53.740\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.510030081, Training Accuracy: 57.396\n",
            "0.9744749447463587\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.471461014, Training Accuracy: 58.356\n",
            "Time taken for training worker 1: 0:00:23.001108\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.495894737, Training Accuracy: 57.492\n",
            "0.9804253795075617\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.474088173, Training Accuracy: 58.228\n",
            "Time taken for training worker 2: 0:00:22.805755\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001834\n",
            "Global Model: Test Loss: 1.785517626, Test Accuracy: 53.000\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.490548080, Training Accuracy: 57.952\n",
            "0.9930057463860713\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.476775359, Training Accuracy: 58.216\n",
            "Time taken for training worker 1: 0:00:22.807610\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.492782472, Training Accuracy: 57.900\n",
            "0.9860307328618586\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.467456086, Training Accuracy: 58.428\n",
            "Time taken for training worker 2: 0:00:22.652752\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001815\n",
            "Global Model: Test Loss: 1.792147803, Test Accuracy: 52.450\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.485050284, Training Accuracy: 57.692\n",
            "0.9541742359735468\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.440795308, Training Accuracy: 59.436\n",
            "Time taken for training worker 1: 0:00:19.618325\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.482624812, Training Accuracy: 58.044\n",
            "0.9683037638713422\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.446933936, Training Accuracy: 59.252\n",
            "Time taken for training worker 2: 0:00:21.268576\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001843\n",
            "Global Model: Test Loss: 1.769148378, Test Accuracy: 53.200\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.465617581, Training Accuracy: 58.500\n",
            "0.9828401407458326\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.434239395, Training Accuracy: 59.156\n",
            "Time taken for training worker 1: 0:00:20.968458\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.449519185, Training Accuracy: 59.160\n",
            "0.9846288356616878\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.446473653, Training Accuracy: 58.572\n",
            "Time taken for training worker 2: 0:00:21.762923\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001787\n",
            "Global Model: Test Loss: 1.772012634, Test Accuracy: 53.650\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.459067177, Training Accuracy: 58.832\n",
            "0.9843856953737005\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.428004100, Training Accuracy: 59.432\n",
            "Time taken for training worker 1: 0:00:20.485957\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.462750971, Training Accuracy: 58.428\n",
            "0.9628442385180349\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.419392434, Training Accuracy: 59.856\n",
            "Time taken for training worker 2: 0:00:20.324263\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001742\n",
            "Global Model: Test Loss: 1.754412814, Test Accuracy: 54.320\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.439488808, Training Accuracy: 59.416\n",
            "0.9947255667059511\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.424783223, Training Accuracy: 59.620\n",
            "Time taken for training worker 1: 0:00:21.772827\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.450431210, Training Accuracy: 58.488\n",
            "0.9690575129294634\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.414539981, Training Accuracy: 59.676\n",
            "Time taken for training worker 2: 0:00:19.978662\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001583\n",
            "Global Model: Test Loss: 1.757238850, Test Accuracy: 54.150\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.430122635, Training Accuracy: 59.008\n",
            "0.9744483266548913\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.402015174, Training Accuracy: 59.996\n",
            "Time taken for training worker 1: 0:00:20.347330\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.425913851, Training Accuracy: 59.664\n",
            "0.9862374159385354\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.408322313, Training Accuracy: 60.200\n",
            "Time taken for training worker 2: 0:00:19.880749\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001548\n",
            "Global Model: Test Loss: 1.780120411, Test Accuracy: 53.840\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.427544615, Training Accuracy: 59.264\n",
            "0.977625470616299\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.399535416, Training Accuracy: 60.132\n",
            "Time taken for training worker 1: 0:00:23.119946\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.419163050, Training Accuracy: 59.928\n",
            "0.989150832891785\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.401578730, Training Accuracy: 60.352\n",
            "Time taken for training worker 2: 0:00:22.811583\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001842\n",
            "Global Model: Test Loss: 1.810202967, Test Accuracy: 53.250\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.415293975, Training Accuracy: 60.228\n",
            "0.989610679084088\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.376672418, Training Accuracy: 60.636\n",
            "Time taken for training worker 1: 0:00:20.430816\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.418028949, Training Accuracy: 59.772\n",
            "0.9901454999809236\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.406756135, Training Accuracy: 60.156\n",
            "Time taken for training worker 2: 0:00:19.770628\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001585\n",
            "Global Model: Test Loss: 1.746412826, Test Accuracy: 54.240\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.411198502, Training Accuracy: 60.096\n",
            "0.9905038131130244\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.377649132, Training Accuracy: 60.468\n",
            "Time taken for training worker 1: 0:00:21.482441\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.396509868, Training Accuracy: 60.012\n",
            "0.9820531342340277\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.369487892, Training Accuracy: 60.716\n",
            "Time taken for training worker 2: 0:00:21.650565\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001658\n",
            "Global Model: Test Loss: 1.765375080, Test Accuracy: 54.010\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.383899207, Training Accuracy: 60.564\n",
            "0.9926997030151229\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.368163182, Training Accuracy: 60.852\n",
            "Time taken for training worker 1: 0:00:22.707198\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.382134559, Training Accuracy: 60.536\n",
            "0.9955325258763148\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.371141434, Training Accuracy: 60.712\n",
            "Time taken for training worker 2: 0:00:20.638953\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001607\n",
            "Global Model: Test Loss: 1.793304569, Test Accuracy: 53.580\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.378058647, Training Accuracy: 60.588\n",
            "0.9912876634562254\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.355512524, Training Accuracy: 60.932\n",
            "Time taken for training worker 1: 0:00:22.647979\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.383407232, Training Accuracy: 60.252\n",
            "0.9640556610705997\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.345143048, Training Accuracy: 61.676\n",
            "Time taken for training worker 2: 0:00:21.713469\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001768\n",
            "Global Model: Test Loss: 1.740689221, Test Accuracy: 55.190\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.362233380, Training Accuracy: 60.632\n",
            "0.9666604367543398\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.325036684, Training Accuracy: 61.960\n",
            "Time taken for training worker 1: 0:00:20.024590\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.366424011, Training Accuracy: 60.916\n",
            "0.9822180517027652\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.335360668, Training Accuracy: 61.624\n",
            "Time taken for training worker 2: 0:00:20.387835\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001631\n",
            "Global Model: Test Loss: 1.769180894, Test Accuracy: 53.820\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.356918895, Training Accuracy: 61.132\n",
            "0.977510350561146\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.327091981, Training Accuracy: 62.032\n",
            "Time taken for training worker 1: 0:00:20.050817\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.343834282, Training Accuracy: 61.320\n",
            "0.9909921457447577\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.336876876, Training Accuracy: 61.680\n",
            "Time taken for training worker 2: 0:00:20.716137\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001670\n",
            "Global Model: Test Loss: 1.774613647, Test Accuracy: 53.730\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.345754441, Training Accuracy: 61.352\n",
            "0.9756142276570643\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.307660247, Training Accuracy: 62.332\n",
            "Time taken for training worker 1: 0:00:20.205311\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.334975172, Training Accuracy: 61.952\n",
            "0.9946253505268345\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.327726372, Training Accuracy: 61.736\n",
            "Time taken for training worker 2: 0:00:21.810431\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001738\n",
            "Global Model: Test Loss: 1.778520761, Test Accuracy: 54.060\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.333907387, Training Accuracy: 61.600\n",
            "0.9893450629798676\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.309900312, Training Accuracy: 62.028\n",
            "Time taken for training worker 1: 0:00:21.834677\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.318633339, Training Accuracy: 62.268\n",
            "0.9878889504726442\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.298794508, Training Accuracy: 62.760\n",
            "Time taken for training worker 2: 0:00:21.750499\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001750\n",
            "Global Model: Test Loss: 1.770584694, Test Accuracy: 54.040\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.308455932, Training Accuracy: 62.332\n",
            "0.9856495338390285\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.296736460, Training Accuracy: 62.916\n",
            "Time taken for training worker 1: 0:00:19.997020\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.319042931, Training Accuracy: 61.760\n",
            "0.971365793205317\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.282286412, Training Accuracy: 62.920\n",
            "Time taken for training worker 2: 0:00:20.257672\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001629\n",
            "Global Model: Test Loss: 1.767501001, Test Accuracy: 53.970\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.300496931, Training Accuracy: 62.484\n",
            "0.9968508705892867\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.292482896, Training Accuracy: 62.612\n",
            "Time taken for training worker 1: 0:00:20.214177\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.307762474, Training Accuracy: 62.252\n",
            "0.9867090855211589\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.293002369, Training Accuracy: 62.792\n",
            "Time taken for training worker 2: 0:00:20.262088\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001575\n",
            "Global Model: Test Loss: 1.742684660, Test Accuracy: 54.550\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.288436216, Training Accuracy: 63.052\n",
            "0.9965793743036971\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.282207699, Training Accuracy: 62.912\n",
            "Time taken for training worker 1: 0:00:21.918868\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.305070199, Training Accuracy: 62.504\n",
            "0.9826672822525124\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.270053820, Training Accuracy: 63.212\n",
            "Time taken for training worker 2: 0:00:19.983033\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001766\n",
            "Global Model: Test Loss: 1.752751378, Test Accuracy: 54.460\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.282199951, Training Accuracy: 63.032\n",
            "0.9918188159137513\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.264318906, Training Accuracy: 63.368\n",
            "Time taken for training worker 1: 0:00:20.153449\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.274741145, Training Accuracy: 63.172\n",
            "0.9888352541945397\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.264367180, Training Accuracy: 63.632\n",
            "Time taken for training worker 2: 0:00:19.874581\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001617\n",
            "Global Model: Test Loss: 1.747357217, Test Accuracy: 54.420\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.277080641, Training Accuracy: 63.064\n",
            "0.9834961137680485\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.248286086, Training Accuracy: 63.744\n",
            "Time taken for training worker 1: 0:00:20.261389\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.289465807, Training Accuracy: 62.900\n",
            "0.987330625084325\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.254946686, Training Accuracy: 63.420\n",
            "Time taken for training worker 2: 0:00:19.950586\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001744\n",
            "Global Model: Test Loss: 1.764402222, Test Accuracy: 54.380\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.262574199, Training Accuracy: 63.568\n",
            "0.9897700076844144\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.240354471, Training Accuracy: 63.992\n",
            "Time taken for training worker 1: 0:00:20.030912\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.266501651, Training Accuracy: 63.708\n",
            "0.9860538106536101\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.223138309, Training Accuracy: 64.288\n",
            "Time taken for training worker 2: 0:00:20.025117\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001717\n",
            "Global Model: Test Loss: 1.759867285, Test Accuracy: 54.880\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.242550184, Training Accuracy: 63.892\n",
            "0.9897260119651443\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.231285637, Training Accuracy: 64.320\n",
            "Time taken for training worker 1: 0:00:21.660022\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.249236865, Training Accuracy: 63.680\n",
            "0.9870058709140562\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.236631836, Training Accuracy: 64.220\n",
            "Time taken for training worker 2: 0:00:20.316087\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001653\n",
            "Global Model: Test Loss: 1.764763468, Test Accuracy: 54.580\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.239471117, Training Accuracy: 64.488\n",
            "0.9983782083589309\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.227886963, Training Accuracy: 64.556\n",
            "Time taken for training worker 1: 0:00:19.953693\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.234300386, Training Accuracy: 64.284\n",
            "0.9881722940986251\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.203571602, Training Accuracy: 64.780\n",
            "Time taken for training worker 2: 0:00:20.178636\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001606\n",
            "Global Model: Test Loss: 1.769461831, Test Accuracy: 54.240\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.224563796, Training Accuracy: 64.628\n",
            "0.9771225145593964\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.187376439, Training Accuracy: 65.596\n",
            "Time taken for training worker 1: 0:00:20.594861\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.227742694, Training Accuracy: 64.620\n",
            "0.9891791289402393\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.194481937, Training Accuracy: 65.076\n",
            "Time taken for training worker 2: 0:00:20.061320\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001717\n",
            "Global Model: Test Loss: 1.776458381, Test Accuracy: 54.820\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.223859741, Training Accuracy: 64.488\n",
            "0.9888727488689065\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.205529246, Training Accuracy: 64.956\n",
            "Time taken for training worker 1: 0:00:20.760107\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.207995444, Training Accuracy: 64.860\n",
            "0.9809483919307433\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.177935149, Training Accuracy: 65.668\n",
            "Time taken for training worker 2: 0:00:20.267423\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001642\n",
            "Global Model: Test Loss: 1.757265921, Test Accuracy: 55.250\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.199107597, Training Accuracy: 65.260\n",
            "0.9870385642048746\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.172749662, Training Accuracy: 65.812\n",
            "Time taken for training worker 1: 0:00:20.666834\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.191703383, Training Accuracy: 65.180\n",
            "0.9879594262252402\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.170923664, Training Accuracy: 65.692\n",
            "Time taken for training worker 2: 0:00:20.189631\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001703\n",
            "Global Model: Test Loss: 1.773836218, Test Accuracy: 54.740\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.189808229, Training Accuracy: 65.480\n",
            "0.990441394531539\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.168055455, Training Accuracy: 65.888\n",
            "Time taken for training worker 1: 0:00:20.581025\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.182306578, Training Accuracy: 65.628\n",
            "0.9898105426128954\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.156269186, Training Accuracy: 66.064\n",
            "Time taken for training worker 2: 0:00:21.177455\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001784\n",
            "Global Model: Test Loss: 1.782547346, Test Accuracy: 54.260\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.179762989, Training Accuracy: 65.564\n",
            "0.9821721543199982\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.143882765, Training Accuracy: 66.328\n",
            "Time taken for training worker 1: 0:00:20.048003\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.179287799, Training Accuracy: 65.280\n",
            "0.9808839531850446\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.155236447, Training Accuracy: 66.096\n",
            "Time taken for training worker 2: 0:00:19.996553\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001660\n",
            "Global Model: Test Loss: 1.751219545, Test Accuracy: 55.600\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.163249472, Training Accuracy: 66.228\n",
            "0.997399820800138\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.152389140, Training Accuracy: 66.340\n",
            "Time taken for training worker 1: 0:00:21.238768\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.154358150, Training Accuracy: 66.328\n",
            "0.9964775623632944\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.141385170, Training Accuracy: 66.480\n",
            "Time taken for training worker 2: 0:00:23.091041\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001854\n",
            "Global Model: Test Loss: 1.769534823, Test Accuracy: 55.590\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.160370015, Training Accuracy: 66.212\n",
            "0.9859339738512364\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.130657055, Training Accuracy: 66.820\n",
            "Time taken for training worker 1: 0:00:19.819590\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.156272878, Training Accuracy: 66.152\n",
            "0.9623170802647315\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.097171638, Training Accuracy: 67.792\n",
            "Time taken for training worker 2: 0:00:20.236860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001630\n",
            "Global Model: Test Loss: 1.770113261, Test Accuracy: 55.280\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.137379227, Training Accuracy: 67.156\n",
            "0.9947843626692183\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.110234302, Training Accuracy: 67.384\n",
            "Time taken for training worker 1: 0:00:21.918478\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.133973983, Training Accuracy: 66.888\n",
            "0.9950386616810193\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.121486145, Training Accuracy: 67.104\n",
            "Time taken for training worker 2: 0:00:21.833076\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001707\n",
            "Global Model: Test Loss: 1.772027490, Test Accuracy: 54.840\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.121510307, Training Accuracy: 67.072\n",
            "0.9842956696214624\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.098590726, Training Accuracy: 67.760\n",
            "Time taken for training worker 1: 0:00:21.944217\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.128251465, Training Accuracy: 67.168\n",
            "0.978975457723052\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.097410238, Training Accuracy: 68.092\n",
            "Time taken for training worker 2: 0:00:20.765422\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001595\n",
            "Global Model: Test Loss: 1.786572354, Test Accuracy: 55.280\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for Confidence Interval: 0:54:34.554846\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:2, CI Accuracy Threshold:0.9\n",
            "==================================================\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 4.325224478, Training Accuracy: 4.412\n",
            "-0.24914330475953772\n",
            "Worker 1, [epoch: 02]: Training Loss: 3.860925343, Training Accuracy: 10.440\n",
            "0.4612344445315506\n",
            "Worker 1, [epoch: 03]: Training Loss: 3.625500261, Training Accuracy: 13.692\n",
            "0.6080754721485151\n",
            "Worker 1, [epoch: 04]: Training Loss: 3.420857881, Training Accuracy: 17.360\n",
            "0.6774540806469775\n",
            "Worker 1, [epoch: 05]: Training Loss: 3.248735453, Training Accuracy: 20.548\n",
            "0.7216595552428198\n",
            "Worker 1, [epoch: 06]: Training Loss: 3.107374248, Training Accuracy: 23.204\n",
            "0.7536413870271399\n",
            "Worker 1, [epoch: 07]: Training Loss: 2.974336195, Training Accuracy: 25.420\n",
            "0.7772399966532627\n",
            "Worker 1, [epoch: 08]: Training Loss: 2.859468014, Training Accuracy: 27.612\n",
            "0.7967966219887518\n",
            "Worker 1, [epoch: 09]: Training Loss: 2.765690164, Training Accuracy: 29.160\n",
            "0.811454611241271\n",
            "Worker 1, [epoch: 10]: Training Loss: 2.679472277, Training Accuracy: 31.344\n",
            "0.8244094045005883\n",
            "Worker 1, [epoch: 11]: Training Loss: 2.600441691, Training Accuracy: 32.596\n",
            "0.8350379270971195\n",
            "Worker 1, [epoch: 12]: Training Loss: 2.535628008, Training Accuracy: 34.272\n",
            "0.8446464392347872\n",
            "Worker 1, [epoch: 13]: Training Loss: 2.484001704, Training Accuracy: 35.276\n",
            "0.8529512490520945\n",
            "Worker 1, [epoch: 14]: Training Loss: 2.420522707, Training Accuracy: 36.536\n",
            "0.8601270278141335\n",
            "Worker 1, [epoch: 15]: Training Loss: 2.364718209, Training Accuracy: 37.832\n",
            "0.8666816238877858\n",
            "Worker 1, [epoch: 16]: Training Loss: 2.318859067, Training Accuracy: 38.692\n",
            "0.8726512045436541\n",
            "Worker 1, [epoch: 17]: Training Loss: 2.288763353, Training Accuracy: 39.512\n",
            "0.878037830117049\n",
            "Worker 1, [epoch: 18]: Training Loss: 2.233991118, Training Accuracy: 40.388\n",
            "0.8829399656411056\n",
            "Worker 1, [epoch: 19]: Training Loss: 2.193503605, Training Accuracy: 41.184\n",
            "0.887194357396551\n",
            "Worker 1, [epoch: 20]: Training Loss: 2.143970738, Training Accuracy: 42.444\n",
            "0.8912169615233789\n",
            "Worker 1, [epoch: 21]: Training Loss: 2.117782423, Training Accuracy: 42.968\n",
            "0.8949699420597312\n",
            "Worker 1, [epoch: 22]: Training Loss: 2.093489514, Training Accuracy: 43.560\n",
            "0.8983178836742733\n",
            "Worker 1, [epoch: 23]: Training Loss: 2.054447516, Training Accuracy: 44.592\n",
            "0.9015079262403182\n",
            "Worker 1, [epoch: 24]: Training Loss: 2.027191588, Training Accuracy: 45.012\n",
            "Time taken for training worker 1: 0:04:03.378710\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 4.321295081, Training Accuracy: 4.400\n",
            "-0.25950314465754487\n",
            "Worker 2, [epoch: 02]: Training Loss: 3.844666915, Training Accuracy: 10.496\n",
            "0.43454622380893615\n",
            "Worker 2, [epoch: 03]: Training Loss: 3.602322855, Training Accuracy: 14.512\n",
            "0.5974030962073354\n",
            "Worker 2, [epoch: 04]: Training Loss: 3.392311600, Training Accuracy: 17.952\n",
            "0.6762005642666593\n",
            "Worker 2, [epoch: 05]: Training Loss: 3.232863760, Training Accuracy: 20.788\n",
            "0.723895265990071\n",
            "Worker 2, [epoch: 06]: Training Loss: 3.094344454, Training Accuracy: 23.324\n",
            "0.7547300842176525\n",
            "Worker 2, [epoch: 07]: Training Loss: 2.953175092, Training Accuracy: 26.008\n",
            "0.7776630272703244\n",
            "Worker 2, [epoch: 08]: Training Loss: 2.836741619, Training Accuracy: 28.284\n",
            "0.7964695047913615\n",
            "Worker 2, [epoch: 09]: Training Loss: 2.737373778, Training Accuracy: 30.044\n",
            "0.8118479380101921\n",
            "Worker 2, [epoch: 10]: Training Loss: 2.661707701, Training Accuracy: 31.748\n",
            "0.8254151219587669\n",
            "Worker 2, [epoch: 11]: Training Loss: 2.592024788, Training Accuracy: 32.848\n",
            "0.8364164215817953\n",
            "Worker 2, [epoch: 12]: Training Loss: 2.533696990, Training Accuracy: 34.508\n",
            "0.8458744580528786\n",
            "Worker 2, [epoch: 13]: Training Loss: 2.472802874, Training Accuracy: 35.816\n",
            "0.8541858205341579\n",
            "Worker 2, [epoch: 14]: Training Loss: 2.417124477, Training Accuracy: 36.952\n",
            "0.8616883728337523\n",
            "Worker 2, [epoch: 15]: Training Loss: 2.372804935, Training Accuracy: 37.828\n",
            "0.8680518211095998\n",
            "Worker 2, [epoch: 16]: Training Loss: 2.312899123, Training Accuracy: 39.232\n",
            "0.8739899918721472\n",
            "Worker 2, [epoch: 17]: Training Loss: 2.268745821, Training Accuracy: 39.860\n",
            "0.8794131637862252\n",
            "Worker 2, [epoch: 18]: Training Loss: 2.237199513, Training Accuracy: 40.596\n",
            "0.8842751149590837\n",
            "Worker 2, [epoch: 19]: Training Loss: 2.198315574, Training Accuracy: 41.500\n",
            "0.8887259529532354\n",
            "Worker 2, [epoch: 20]: Training Loss: 2.167425437, Training Accuracy: 42.220\n",
            "0.892736262911064\n",
            "Worker 2, [epoch: 21]: Training Loss: 2.127077647, Training Accuracy: 43.104\n",
            "0.8965691828858593\n",
            "Worker 2, [epoch: 22]: Training Loss: 2.104125550, Training Accuracy: 43.396\n",
            "0.8999812276527257\n",
            "Worker 2, [epoch: 23]: Training Loss: 2.065111331, Training Accuracy: 44.396\n",
            "0.9029983873139422\n",
            "Worker 2, [epoch: 24]: Training Loss: 2.032346898, Training Accuracy: 45.508\n",
            "Time taken for training worker 2: 0:04:12.590201\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001556\n",
            "Global Model: Test Loss: 3.042871223, Test Accuracy: 32.410\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.496691633, Training Accuracy: 35.304\n",
            "0.8649700105547439\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.319274790, Training Accuracy: 38.544\n",
            "0.9287796868351845\n",
            "Worker 1, [epoch: 03]: Training Loss: 2.244072084, Training Accuracy: 40.192\n",
            "Time taken for training worker 1: 0:00:30.066922\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.475285164, Training Accuracy: 35.656\n",
            "0.8388604537208479\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.295079120, Training Accuracy: 39.596\n",
            "0.9262490162497433\n",
            "Worker 2, [epoch: 03]: Training Loss: 2.227444826, Training Accuracy: 40.608\n",
            "Time taken for training worker 2: 0:00:30.144702\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001632\n",
            "Global Model: Test Loss: 2.142412773, Test Accuracy: 43.730\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.253129658, Training Accuracy: 40.272\n",
            "0.9276137667646676\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.177574147, Training Accuracy: 42.212\n",
            "Time taken for training worker 1: 0:00:20.216107\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.227637153, Training Accuracy: 40.836\n",
            "0.9519251052909397\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.173882673, Training Accuracy: 42.132\n",
            "Time taken for training worker 2: 0:00:20.255998\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001631\n",
            "Global Model: Test Loss: 2.049215946, Test Accuracy: 46.240\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.184813912, Training Accuracy: 42.080\n",
            "0.9635742884005086\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.119113921, Training Accuracy: 43.088\n",
            "Time taken for training worker 1: 0:00:20.977949\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.163709980, Training Accuracy: 42.644\n",
            "0.9699835805863997\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.114290997, Training Accuracy: 43.484\n",
            "Time taken for training worker 2: 0:00:20.882074\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001698\n",
            "Global Model: Test Loss: 2.014388696, Test Accuracy: 46.160\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.116732716, Training Accuracy: 43.316\n",
            "0.9596111884299714\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.059946752, Training Accuracy: 44.468\n",
            "Time taken for training worker 1: 0:00:21.975182\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.098687112, Training Accuracy: 43.740\n",
            "0.9515216364526223\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.046250537, Training Accuracy: 45.140\n",
            "Time taken for training worker 2: 0:00:20.988862\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001714\n",
            "Global Model: Test Loss: 1.967114039, Test Accuracy: 47.700\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.042295988, Training Accuracy: 44.992\n",
            "0.990045154937473\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.031670868, Training Accuracy: 45.284\n",
            "Time taken for training worker 1: 0:00:19.872995\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.050858457, Training Accuracy: 45.228\n",
            "0.9822740583260399\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.013150076, Training Accuracy: 45.752\n",
            "Time taken for training worker 2: 0:00:20.902754\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001848\n",
            "Global Model: Test Loss: 1.936958773, Test Accuracy: 48.900\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.028104213, Training Accuracy: 45.588\n",
            "0.9658183520379523\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.975401891, Training Accuracy: 46.612\n",
            "Time taken for training worker 1: 0:00:22.293674\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.013128299, Training Accuracy: 45.772\n",
            "0.964508206088434\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.957566341, Training Accuracy: 46.840\n",
            "Time taken for training worker 2: 0:00:19.911079\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001691\n",
            "Global Model: Test Loss: 1.896304050, Test Accuracy: 50.280\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.973134723, Training Accuracy: 46.724\n",
            "0.9926402648354411\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.951485796, Training Accuracy: 46.948\n",
            "Time taken for training worker 1: 0:00:20.309919\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.962519508, Training Accuracy: 47.080\n",
            "0.968169925580042\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.929275319, Training Accuracy: 48.064\n",
            "Time taken for training worker 2: 0:00:21.699974\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001651\n",
            "Global Model: Test Loss: 1.896428751, Test Accuracy: 49.670\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.943616094, Training Accuracy: 47.572\n",
            "0.9758653330693642\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.892932914, Training Accuracy: 48.324\n",
            "Time taken for training worker 1: 0:00:19.872549\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.922781235, Training Accuracy: 48.020\n",
            "0.9664022188523145\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.880971181, Training Accuracy: 49.080\n",
            "Time taken for training worker 2: 0:00:21.487141\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001637\n",
            "Global Model: Test Loss: 1.872751351, Test Accuracy: 50.640\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.907038098, Training Accuracy: 48.316\n",
            "0.9885767272208552\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.884756374, Training Accuracy: 48.676\n",
            "Time taken for training worker 1: 0:00:22.986627\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.896528454, Training Accuracy: 48.768\n",
            "0.9916921425989483\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.878713614, Training Accuracy: 49.032\n",
            "Time taken for training worker 2: 0:00:20.277583\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001722\n",
            "Global Model: Test Loss: 1.890836180, Test Accuracy: 49.820\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.864983694, Training Accuracy: 49.276\n",
            "0.9987503315181061\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.846595936, Training Accuracy: 49.236\n",
            "Time taken for training worker 1: 0:00:21.821539\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.871365549, Training Accuracy: 48.868\n",
            "0.9648848212699223\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.830207889, Training Accuracy: 49.996\n",
            "Time taken for training worker 2: 0:00:21.811094\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001748\n",
            "Global Model: Test Loss: 1.851397829, Test Accuracy: 51.210\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.853836793, Training Accuracy: 49.388\n",
            "0.9847461387324591\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.830635523, Training Accuracy: 49.880\n",
            "Time taken for training worker 1: 0:00:21.765981\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.829569494, Training Accuracy: 50.312\n",
            "0.9998776609477596\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.816585350, Training Accuracy: 50.316\n",
            "Time taken for training worker 2: 0:00:21.146226\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001750\n",
            "Global Model: Test Loss: 1.873987532, Test Accuracy: 50.170\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.826788102, Training Accuracy: 50.252\n",
            "0.992911942298362\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.808783961, Training Accuracy: 50.484\n",
            "Time taken for training worker 1: 0:00:21.081518\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.815123414, Training Accuracy: 50.584\n",
            "0.9919896240454408\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.786674271, Training Accuracy: 50.848\n",
            "Time taken for training worker 2: 0:00:20.672961\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001644\n",
            "Global Model: Test Loss: 1.860822700, Test Accuracy: 50.440\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.817243619, Training Accuracy: 50.384\n",
            "0.9843204780103548\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.782940846, Training Accuracy: 50.900\n",
            "Time taken for training worker 1: 0:00:20.101383\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.819820473, Training Accuracy: 50.452\n",
            "0.963352299439437\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.762648952, Training Accuracy: 51.668\n",
            "Time taken for training worker 2: 0:00:20.435372\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001815\n",
            "Global Model: Test Loss: 1.889195476, Test Accuracy: 50.390\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.798442909, Training Accuracy: 50.768\n",
            "0.9737936484996758\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.756299236, Training Accuracy: 51.640\n",
            "Time taken for training worker 1: 0:00:20.482798\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.763671291, Training Accuracy: 51.352\n",
            "0.9723223488297726\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.744451476, Training Accuracy: 52.284\n",
            "Time taken for training worker 2: 0:00:20.530630\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001604\n",
            "Global Model: Test Loss: 1.839928214, Test Accuracy: 51.620\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.769107139, Training Accuracy: 51.524\n",
            "0.9860856689264148\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.747435937, Training Accuracy: 51.992\n",
            "Time taken for training worker 1: 0:00:20.345645\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.746453688, Training Accuracy: 51.852\n",
            "0.981240111849107\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.724262657, Training Accuracy: 52.488\n",
            "Time taken for training worker 2: 0:00:20.273869\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001617\n",
            "Global Model: Test Loss: 1.811227106, Test Accuracy: 51.820\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.752000171, Training Accuracy: 51.840\n",
            "0.9921832096722961\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.724300446, Training Accuracy: 52.104\n",
            "Time taken for training worker 1: 0:00:20.278997\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.731541248, Training Accuracy: 52.344\n",
            "0.9977640810917696\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.721289274, Training Accuracy: 52.268\n",
            "Time taken for training worker 2: 0:00:19.784047\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001639\n",
            "Global Model: Test Loss: 1.801691307, Test Accuracy: 52.030\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.734764938, Training Accuracy: 51.936\n",
            "0.971467052018561\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.703547507, Training Accuracy: 52.908\n",
            "Time taken for training worker 1: 0:00:20.319325\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.708518324, Training Accuracy: 52.956\n",
            "0.9993027450074293\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.698053244, Training Accuracy: 52.980\n",
            "Time taken for training worker 2: 0:00:20.266819\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001646\n",
            "Global Model: Test Loss: 1.812448209, Test Accuracy: 51.940\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.734388926, Training Accuracy: 52.424\n",
            "0.9790094332867003\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.693048547, Training Accuracy: 53.144\n",
            "Time taken for training worker 1: 0:00:20.331075\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.702274046, Training Accuracy: 53.128\n",
            "0.9832916203191759\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.668948467, Training Accuracy: 53.708\n",
            "Time taken for training worker 2: 0:00:20.353980\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001608\n",
            "Global Model: Test Loss: 1.825020476, Test Accuracy: 51.730\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.695195248, Training Accuracy: 53.284\n",
            "0.99976897736547\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.684949340, Training Accuracy: 53.292\n",
            "Time taken for training worker 1: 0:00:22.472263\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.682156351, Training Accuracy: 53.764\n",
            "0.9997710056891957\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.663489757, Training Accuracy: 53.756\n",
            "Time taken for training worker 2: 0:00:23.039562\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001931\n",
            "Global Model: Test Loss: 1.813592568, Test Accuracy: 51.930\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.692536257, Training Accuracy: 53.380\n",
            "0.9957393538041424\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.668634863, Training Accuracy: 53.528\n",
            "Time taken for training worker 1: 0:00:21.291387\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.662119000, Training Accuracy: 53.740\n",
            "0.9950826592316006\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.648237469, Training Accuracy: 53.912\n",
            "Time taken for training worker 2: 0:00:20.084076\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001587\n",
            "Global Model: Test Loss: 1.778821396, Test Accuracy: 52.810\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.670074534, Training Accuracy: 53.840\n",
            "0.990085590116004\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.642737050, Training Accuracy: 54.188\n",
            "Time taken for training worker 1: 0:00:19.865967\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.655487001, Training Accuracy: 54.132\n",
            "0.9976102290892453\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.650109290, Training Accuracy: 54.048\n",
            "Time taken for training worker 2: 0:00:19.955925\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001611\n",
            "Global Model: Test Loss: 1.791261132, Test Accuracy: 52.690\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.652665548, Training Accuracy: 54.152\n",
            "0.9926292535308756\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.628409710, Training Accuracy: 54.412\n",
            "Time taken for training worker 1: 0:00:20.212954\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.638084618, Training Accuracy: 54.532\n",
            "0.9985333081863812\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.633609782, Training Accuracy: 54.584\n",
            "Time taken for training worker 2: 0:00:20.485065\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001622\n",
            "Global Model: Test Loss: 1.775201119, Test Accuracy: 52.660\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.635230927, Training Accuracy: 54.184\n",
            "0.9871037299403921\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.624041989, Training Accuracy: 54.640\n",
            "Time taken for training worker 1: 0:00:19.943899\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.623756887, Training Accuracy: 54.848\n",
            "0.9926108725552667\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.612766754, Training Accuracy: 55.112\n",
            "Time taken for training worker 2: 0:00:21.027246\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001653\n",
            "Global Model: Test Loss: 1.791389130, Test Accuracy: 52.720\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.625956660, Training Accuracy: 54.732\n",
            "0.996284214805259\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.623047765, Training Accuracy: 54.600\n",
            "Time taken for training worker 1: 0:00:20.476217\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.596198667, Training Accuracy: 56.020\n",
            "0.9995604251178735\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.585238985, Training Accuracy: 56.004\n",
            "Time taken for training worker 2: 0:00:21.171801\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001843\n",
            "Global Model: Test Loss: 1.783073866, Test Accuracy: 52.670\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.627706309, Training Accuracy: 54.356\n",
            "0.9620839946537251\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.593116934, Training Accuracy: 55.712\n",
            "Time taken for training worker 1: 0:00:21.869029\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.598279962, Training Accuracy: 55.484\n",
            "0.9968905557104055\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.578970283, Training Accuracy: 55.372\n",
            "Time taken for training worker 2: 0:00:21.958048\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001762\n",
            "Global Model: Test Loss: 1.774523873, Test Accuracy: 52.710\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.619274138, Training Accuracy: 54.852\n",
            "0.9789360062092242\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.585190366, Training Accuracy: 55.608\n",
            "Time taken for training worker 1: 0:00:19.989377\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.592160032, Training Accuracy: 55.572\n",
            "0.9842418723971244\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.561030669, Training Accuracy: 56.144\n",
            "Time taken for training worker 2: 0:00:22.998057\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001896\n",
            "Global Model: Test Loss: 1.811916151, Test Accuracy: 52.160\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.592924478, Training Accuracy: 55.432\n",
            "0.9894868959751631\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.578466327, Training Accuracy: 55.812\n",
            "Time taken for training worker 1: 0:00:22.846493\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.582881828, Training Accuracy: 55.832\n",
            "0.9772380111030001\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.559724660, Training Accuracy: 56.664\n",
            "Time taken for training worker 2: 0:00:21.545422\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001818\n",
            "Global Model: Test Loss: 1.800894890, Test Accuracy: 52.860\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.585777534, Training Accuracy: 55.468\n",
            "0.9797158187962521\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.563633378, Training Accuracy: 56.204\n",
            "Time taken for training worker 1: 0:00:20.248177\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.570353086, Training Accuracy: 56.464\n",
            "0.9965155012315791\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.552728572, Training Accuracy: 56.592\n",
            "Time taken for training worker 2: 0:00:20.221339\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001645\n",
            "Global Model: Test Loss: 1.793211455, Test Accuracy: 52.450\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.580231912, Training Accuracy: 55.572\n",
            "0.9842418723971244\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.543364416, Training Accuracy: 56.144\n",
            "Time taken for training worker 1: 0:00:20.528569\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.548357359, Training Accuracy: 56.728\n",
            "0.9980481183965282\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.525710635, Training Accuracy: 56.800\n",
            "Time taken for training worker 2: 0:00:20.458460\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001620\n",
            "Global Model: Test Loss: 1.758213084, Test Accuracy: 53.640\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.556452695, Training Accuracy: 56.552\n",
            "0.9982575040129043\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.544774727, Training Accuracy: 56.488\n",
            "Time taken for training worker 1: 0:00:21.967869\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.548405996, Training Accuracy: 56.920\n",
            "0.9987028588632166\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.521646594, Training Accuracy: 56.968\n",
            "Time taken for training worker 2: 0:00:21.010652\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001592\n",
            "Global Model: Test Loss: 1.758988993, Test Accuracy: 53.610\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.559238816, Training Accuracy: 56.240\n",
            "0.9898548694233039\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.541396483, Training Accuracy: 56.612\n",
            "Time taken for training worker 1: 0:00:21.956275\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.521452935, Training Accuracy: 56.804\n",
            "0.9739980653538511\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.500129380, Training Accuracy: 57.772\n",
            "Time taken for training worker 2: 0:00:21.789578\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001813\n",
            "Global Model: Test Loss: 1.797809503, Test Accuracy: 52.910\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.522898818, Training Accuracy: 57.356\n",
            "0.9906348687441126\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.530014470, Training Accuracy: 57.008\n",
            "Time taken for training worker 1: 0:00:20.299784\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.527668120, Training Accuracy: 57.032\n",
            "0.9935378985081023\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.505948995, Training Accuracy: 57.272\n",
            "Time taken for training worker 2: 0:00:20.736465\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001744\n",
            "Global Model: Test Loss: 1.776721617, Test Accuracy: 53.620\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.531243631, Training Accuracy: 57.044\n",
            "0.9925724821073995\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.507608629, Training Accuracy: 57.320\n",
            "Time taken for training worker 1: 0:00:21.369939\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.503380375, Training Accuracy: 58.044\n",
            "0.9850815146039371\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.499999270, Training Accuracy: 57.484\n",
            "Time taken for training worker 2: 0:00:20.754120\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002569\n",
            "Global Model: Test Loss: 1.749265448, Test Accuracy: 53.860\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.513931399, Training Accuracy: 57.096\n",
            "0.9762514513291576\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.494172893, Training Accuracy: 57.984\n",
            "Time taken for training worker 1: 0:00:21.941048\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.510948495, Training Accuracy: 57.472\n",
            "0.9870949957375283\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.485820578, Training Accuracy: 57.956\n",
            "Time taken for training worker 2: 0:00:21.440030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001734\n",
            "Global Model: Test Loss: 1.767068491, Test Accuracy: 53.210\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.510050510, Training Accuracy: 57.732\n",
            "0.9970117505013586\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.497937238, Training Accuracy: 57.620\n",
            "Time taken for training worker 1: 0:00:21.866221\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.489637739, Training Accuracy: 58.240\n",
            "0.9892572814730273\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.465654829, Training Accuracy: 58.648\n",
            "Time taken for training worker 2: 0:00:21.498401\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001789\n",
            "Global Model: Test Loss: 1.762554119, Test Accuracy: 53.640\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.496847389, Training Accuracy: 57.636\n",
            "0.9815288032742082\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.474835405, Training Accuracy: 58.332\n",
            "Time taken for training worker 1: 0:00:20.637190\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.480221486, Training Accuracy: 57.936\n",
            "0.9868818658526822\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.475160513, Training Accuracy: 58.432\n",
            "Time taken for training worker 2: 0:00:20.327355\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002624\n",
            "Global Model: Test Loss: 1.771952153, Test Accuracy: 53.650\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.474214003, Training Accuracy: 58.368\n",
            "0.993977135935014\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.474352855, Training Accuracy: 58.140\n",
            "Time taken for training worker 1: 0:00:21.384487\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.473335293, Training Accuracy: 58.540\n",
            "0.993994866613465\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.457995555, Training Accuracy: 58.312\n",
            "Time taken for training worker 2: 0:00:20.718395\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001654\n",
            "Global Model: Test Loss: 1.778750009, Test Accuracy: 53.830\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.483021013, Training Accuracy: 58.032\n",
            "0.9943886192345104\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.462701563, Training Accuracy: 58.244\n",
            "Time taken for training worker 1: 0:00:20.170257\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.453536094, Training Accuracy: 58.932\n",
            "0.9959318875461413\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.439557717, Training Accuracy: 59.088\n",
            "Time taken for training worker 2: 0:00:19.755427\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001666\n",
            "Global Model: Test Loss: 1.776066126, Test Accuracy: 53.860\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.460503038, Training Accuracy: 58.692\n",
            "0.9864262837618056\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.442259431, Training Accuracy: 59.212\n",
            "Time taken for training worker 1: 0:00:20.446523\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.457700061, Training Accuracy: 58.784\n",
            "0.9818874066334926\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.431582782, Training Accuracy: 59.480\n",
            "Time taken for training worker 2: 0:00:20.142434\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001692\n",
            "Global Model: Test Loss: 1.752205720, Test Accuracy: 54.190\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.442529868, Training Accuracy: 58.952\n",
            "0.9934359977370901\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.430990245, Training Accuracy: 59.204\n",
            "Time taken for training worker 1: 0:00:22.104444\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.443441343, Training Accuracy: 58.908\n",
            "0.9778001794201675\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.408421019, Training Accuracy: 59.764\n",
            "Time taken for training worker 2: 0:00:20.677840\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001604\n",
            "Global Model: Test Loss: 1.775578003, Test Accuracy: 54.090\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.458359287, Training Accuracy: 58.964\n",
            "0.9896997846528659\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.424810215, Training Accuracy: 59.360\n",
            "Time taken for training worker 1: 0:00:20.940524\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.414904944, Training Accuracy: 59.536\n",
            "0.99772375990659\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.424317191, Training Accuracy: 59.448\n",
            "Time taken for training worker 2: 0:00:20.300448\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001647\n",
            "Global Model: Test Loss: 1.743418515, Test Accuracy: 54.300\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.440911752, Training Accuracy: 58.888\n",
            "0.974191115725432\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.400443692, Training Accuracy: 59.884\n",
            "Time taken for training worker 1: 0:00:20.514137\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.414030637, Training Accuracy: 59.584\n",
            "0.988780726211309\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.405264684, Training Accuracy: 60.020\n",
            "Time taken for training worker 2: 0:00:20.455753\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001741\n",
            "Global Model: Test Loss: 1.782730846, Test Accuracy: 53.630\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.427128554, Training Accuracy: 59.204\n",
            "0.9794483734134978\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.398674748, Training Accuracy: 60.000\n",
            "Time taken for training worker 1: 0:00:22.172843\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.424327791, Training Accuracy: 59.752\n",
            "0.9799406100926866\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.396726149, Training Accuracy: 60.536\n",
            "Time taken for training worker 2: 0:00:19.967440\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001611\n",
            "Global Model: Test Loss: 1.759347210, Test Accuracy: 54.270\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.412246405, Training Accuracy: 59.952\n",
            "0.9878303923931641\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.395468073, Training Accuracy: 60.428\n",
            "Time taken for training worker 1: 0:00:20.874329\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.402165894, Training Accuracy: 60.388\n",
            "0.9817612748832092\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.359092266, Training Accuracy: 61.108\n",
            "Time taken for training worker 2: 0:00:20.150573\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001617\n",
            "Global Model: Test Loss: 1.741713736, Test Accuracy: 54.800\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.400344593, Training Accuracy: 60.120\n",
            "0.998464995742041\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.377789630, Training Accuracy: 60.180\n",
            "Time taken for training worker 1: 0:00:20.961972\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.398445857, Training Accuracy: 59.936\n",
            "0.9809143816462326\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.370334581, Training Accuracy: 60.684\n",
            "Time taken for training worker 2: 0:00:21.914519\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001803\n",
            "Global Model: Test Loss: 1.772817057, Test Accuracy: 54.110\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.405773387, Training Accuracy: 59.864\n",
            "0.9719812304190371\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.370887375, Training Accuracy: 60.964\n",
            "Time taken for training worker 1: 0:00:19.991036\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.378004840, Training Accuracy: 60.520\n",
            "0.9819013320687213\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.354280734, Training Accuracy: 61.236\n",
            "Time taken for training worker 2: 0:00:19.987990\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001671\n",
            "Global Model: Test Loss: 1.761280708, Test Accuracy: 54.620\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.372775941, Training Accuracy: 60.900\n",
            "0.9998989299682371\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.365252135, Training Accuracy: 60.904\n",
            "Time taken for training worker 1: 0:00:20.028193\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.358192873, Training Accuracy: 61.236\n",
            "0.9945623965773716\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.347284557, Training Accuracy: 61.020\n",
            "Time taken for training worker 2: 0:00:20.263424\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001747\n",
            "Global Model: Test Loss: 1.743043820, Test Accuracy: 54.600\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.367606869, Training Accuracy: 61.240\n",
            "0.9859923376069867\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.334533597, Training Accuracy: 61.800\n",
            "Time taken for training worker 1: 0:00:20.695686\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.374600881, Training Accuracy: 60.828\n",
            "0.9791946542502853\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.325583071, Training Accuracy: 61.656\n",
            "Time taken for training worker 2: 0:00:20.664867\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002606\n",
            "Global Model: Test Loss: 1.758613076, Test Accuracy: 54.240\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.365136168, Training Accuracy: 60.808\n",
            "0.9750989978719975\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.333736867, Training Accuracy: 61.800\n",
            "Time taken for training worker 1: 0:00:19.930886\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.356408556, Training Accuracy: 61.152\n",
            "0.9805956904056448\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.332570849, Training Accuracy: 61.928\n",
            "Time taken for training worker 2: 0:00:20.328685\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001735\n",
            "Global Model: Test Loss: 1.757844827, Test Accuracy: 54.790\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.357465375, Training Accuracy: 61.432\n",
            "0.9936006580123035\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.327810512, Training Accuracy: 61.688\n",
            "Time taken for training worker 1: 0:00:21.384328\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.338040781, Training Accuracy: 61.272\n",
            "0.9921840425896264\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.330793645, Training Accuracy: 61.584\n",
            "Time taken for training worker 2: 0:00:20.226384\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001704\n",
            "Global Model: Test Loss: 1.756916439, Test Accuracy: 54.880\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.325847671, Training Accuracy: 61.720\n",
            "0.9968053109773372\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.323274724, Training Accuracy: 61.592\n",
            "Time taken for training worker 1: 0:00:23.252012\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.336813326, Training Accuracy: 61.924\n",
            "0.9778938910942555\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.296772753, Training Accuracy: 62.820\n",
            "Time taken for training worker 2: 0:00:22.971037\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001842\n",
            "Global Model: Test Loss: 1.771989774, Test Accuracy: 55.100\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.326311072, Training Accuracy: 62.056\n",
            "0.9972251694023113\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.306982916, Training Accuracy: 62.168\n",
            "Time taken for training worker 1: 0:00:20.748067\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.330512843, Training Accuracy: 61.840\n",
            "0.9788454976243828\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.291100877, Training Accuracy: 62.696\n",
            "Time taken for training worker 2: 0:00:20.536812\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001583\n",
            "Global Model: Test Loss: 1.759766860, Test Accuracy: 54.720\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.324049064, Training Accuracy: 61.816\n",
            "0.9863196441366141\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.303567353, Training Accuracy: 62.368\n",
            "Time taken for training worker 1: 0:00:20.514862\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.293274276, Training Accuracy: 62.740\n",
            "0.9985276589680077\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.292078852, Training Accuracy: 62.680\n",
            "Time taken for training worker 2: 0:00:20.221311\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001626\n",
            "Global Model: Test Loss: 1.768803896, Test Accuracy: 54.710\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.307805292, Training Accuracy: 62.408\n",
            "0.9863510828040748\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.281638279, Training Accuracy: 62.964\n",
            "Time taken for training worker 1: 0:00:19.882695\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.296518960, Training Accuracy: 62.652\n",
            "0.9736034358140547\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.263295482, Training Accuracy: 63.736\n",
            "Time taken for training worker 2: 0:00:20.343860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001655\n",
            "Global Model: Test Loss: 1.749475853, Test Accuracy: 55.610\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.303985259, Training Accuracy: 62.748\n",
            "0.9962768376921591\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.275572821, Training Accuracy: 62.900\n",
            "Time taken for training worker 1: 0:00:22.363603\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.298182954, Training Accuracy: 62.604\n",
            "0.9768727290555106\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.253635942, Training Accuracy: 63.552\n",
            "Time taken for training worker 2: 0:00:20.042434\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001706\n",
            "Global Model: Test Loss: 1.730015966, Test Accuracy: 55.140\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.301949582, Training Accuracy: 62.456\n",
            "0.9682029272634296\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.245779875, Training Accuracy: 63.760\n",
            "Time taken for training worker 1: 0:00:23.118252\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.268287283, Training Accuracy: 63.128\n",
            "0.9858296500652052\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.246542191, Training Accuracy: 63.712\n",
            "Time taken for training worker 2: 0:00:21.809747\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001830\n",
            "Global Model: Test Loss: 1.773691206, Test Accuracy: 54.150\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.285765930, Training Accuracy: 63.060\n",
            "0.9811792142788327\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.244048996, Training Accuracy: 63.836\n",
            "Time taken for training worker 1: 0:00:23.019883\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.263251116, Training Accuracy: 63.416\n",
            "0.9824352962128621\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.238529399, Training Accuracy: 64.144\n",
            "Time taken for training worker 2: 0:00:22.532258\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001851\n",
            "Global Model: Test Loss: 1.770053353, Test Accuracy: 54.760\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.268058876, Training Accuracy: 63.088\n",
            "0.9873682244163348\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.250938684, Training Accuracy: 63.608\n",
            "Time taken for training worker 1: 0:00:21.535191\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.238314648, Training Accuracy: 64.256\n",
            "0.9968420274018885\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.226144196, Training Accuracy: 64.388\n",
            "Time taken for training worker 2: 0:00:21.370518\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001649\n",
            "Global Model: Test Loss: 1.773439952, Test Accuracy: 54.660\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.254232602, Training Accuracy: 63.508\n",
            "0.9863943132545446\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.235545427, Training Accuracy: 64.072\n",
            "Time taken for training worker 1: 0:00:22.974651\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.240003212, Training Accuracy: 63.908\n",
            "0.9884853144458147\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.227770021, Training Accuracy: 64.388\n",
            "Time taken for training worker 2: 0:00:21.071775\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001598\n",
            "Global Model: Test Loss: 1.792210344, Test Accuracy: 54.050\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.248931633, Training Accuracy: 63.712\n",
            "0.9702440519765092\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.209258590, Training Accuracy: 64.956\n",
            "Time taken for training worker 1: 0:00:22.180051\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.233014790, Training Accuracy: 64.172\n",
            "0.9974123421821501\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.223269537, Training Accuracy: 64.280\n",
            "Time taken for training worker 2: 0:00:19.823967\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001669\n",
            "Global Model: Test Loss: 1.738507221, Test Accuracy: 55.730\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.220516501, Training Accuracy: 64.468\n",
            "0.9974242132759492\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.210429674, Training Accuracy: 64.576\n",
            "Time taken for training worker 1: 0:00:22.539603\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.227746241, Training Accuracy: 64.232\n",
            "0.9742473140895014\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.191941914, Training Accuracy: 65.316\n",
            "Time taken for training worker 2: 0:00:21.717167\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001813\n",
            "Global Model: Test Loss: 1.771742714, Test Accuracy: 55.300\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.201843812, Training Accuracy: 64.832\n",
            "0.9940301714078605\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.202617966, Training Accuracy: 65.084\n",
            "Time taken for training worker 1: 0:00:22.907794\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.217150274, Training Accuracy: 64.740\n",
            "0.9800684071590543\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.177740123, Training Accuracy: 65.584\n",
            "Time taken for training worker 2: 0:00:20.406009\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001854\n",
            "Global Model: Test Loss: 1.759489488, Test Accuracy: 55.170\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.211240382, Training Accuracy: 64.520\n",
            "Time taken for training worker 1: 0:00:10.175651\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.192049307, Training Accuracy: 65.128\n",
            "Time taken for training worker 2: 0:00:10.230429\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001597\n",
            "Global Model: Test Loss: 1.771312363, Test Accuracy: 55.110\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.196617400, Training Accuracy: 65.128\n",
            "0.9827075311210993\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.165892699, Training Accuracy: 65.864\n",
            "Time taken for training worker 1: 0:00:19.821921\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.174453442, Training Accuracy: 65.376\n",
            "0.9963323965547989\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.178196556, Training Accuracy: 65.532\n",
            "Time taken for training worker 2: 0:00:20.362420\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001597\n",
            "Global Model: Test Loss: 1.758802217, Test Accuracy: 55.350\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for Confidence Interval: 0:53:58.018296\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:2, CI Accuracy Threshold:0.95\n",
            "==================================================\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 4.324506900, Training Accuracy: 4.424\n",
            "-0.25252643685829756\n",
            "Worker 1, [epoch: 02]: Training Loss: 3.861645224, Training Accuracy: 10.496\n",
            "0.4520744854352089\n",
            "Worker 1, [epoch: 03]: Training Loss: 3.631330752, Training Accuracy: 14.024\n",
            "0.6077278848179116\n",
            "Worker 1, [epoch: 04]: Training Loss: 3.426945262, Training Accuracy: 17.424\n",
            "0.6804151542980559\n",
            "Worker 1, [epoch: 05]: Training Loss: 3.259269752, Training Accuracy: 20.440\n",
            "0.7277742647770794\n",
            "Worker 1, [epoch: 06]: Training Loss: 3.102883097, Training Accuracy: 22.636\n",
            "0.7585626635568676\n",
            "Worker 1, [epoch: 07]: Training Loss: 2.991632235, Training Accuracy: 25.132\n",
            "0.7803432609004619\n",
            "Worker 1, [epoch: 08]: Training Loss: 2.863541324, Training Accuracy: 27.560\n",
            "0.7981986133738572\n",
            "Worker 1, [epoch: 09]: Training Loss: 2.777032199, Training Accuracy: 29.372\n",
            "0.8131041599499672\n",
            "Worker 1, [epoch: 10]: Training Loss: 2.693354585, Training Accuracy: 30.992\n",
            "0.8248753891364857\n",
            "Worker 1, [epoch: 11]: Training Loss: 2.607023482, Training Accuracy: 32.968\n",
            "0.8350754177676594\n",
            "Worker 1, [epoch: 12]: Training Loss: 2.541825063, Training Accuracy: 34.464\n",
            "0.8442547264179625\n",
            "Worker 1, [epoch: 13]: Training Loss: 2.482873517, Training Accuracy: 35.632\n",
            "0.8530563020445567\n",
            "Worker 1, [epoch: 14]: Training Loss: 2.439040643, Training Accuracy: 36.100\n",
            "0.8604115115338706\n",
            "Worker 1, [epoch: 15]: Training Loss: 2.377987830, Training Accuracy: 37.572\n",
            "0.8670300265468269\n",
            "Worker 1, [epoch: 16]: Training Loss: 2.336659386, Training Accuracy: 38.496\n",
            "0.8731824509810732\n",
            "Worker 1, [epoch: 17]: Training Loss: 2.309874607, Training Accuracy: 39.072\n",
            "0.8787089518680999\n",
            "Worker 1, [epoch: 18]: Training Loss: 2.258391405, Training Accuracy: 39.908\n",
            "0.8836411261198205\n",
            "Worker 1, [epoch: 19]: Training Loss: 2.222184078, Training Accuracy: 40.828\n",
            "0.888160154475462\n",
            "Worker 1, [epoch: 20]: Training Loss: 2.190527067, Training Accuracy: 41.516\n",
            "0.8922431308713128\n",
            "Worker 1, [epoch: 21]: Training Loss: 2.142523732, Training Accuracy: 42.348\n",
            "0.8959718829197878\n",
            "Worker 1, [epoch: 22]: Training Loss: 2.110938816, Training Accuracy: 43.100\n",
            "0.8992310540056968\n",
            "Worker 1, [epoch: 23]: Training Loss: 2.066899633, Training Accuracy: 44.304\n",
            "0.9024003313953542\n",
            "Worker 1, [epoch: 24]: Training Loss: 2.053378652, Training Accuracy: 44.540\n",
            "0.9054927836991421\n",
            "Worker 1, [epoch: 25]: Training Loss: 2.037776974, Training Accuracy: 44.612\n",
            "0.9083157854605506\n",
            "Worker 1, [epoch: 26]: Training Loss: 2.010774842, Training Accuracy: 45.360\n",
            "0.9108842793040937\n",
            "Worker 1, [epoch: 27]: Training Loss: 1.976648003, Training Accuracy: 46.176\n",
            "0.9132464447492551\n",
            "Worker 1, [epoch: 28]: Training Loss: 1.945591262, Training Accuracy: 46.912\n",
            "0.9154912334817684\n",
            "Worker 1, [epoch: 29]: Training Loss: 1.929112960, Training Accuracy: 47.328\n",
            "0.9176610284399367\n",
            "Worker 1, [epoch: 30]: Training Loss: 1.917064404, Training Accuracy: 47.536\n",
            "0.9196101536388601\n",
            "Worker 1, [epoch: 31]: Training Loss: 1.872797435, Training Accuracy: 48.528\n",
            "0.9215294567353971\n",
            "Worker 1, [epoch: 32]: Training Loss: 1.881656028, Training Accuracy: 48.572\n",
            "0.9233550027624745\n",
            "Worker 1, [epoch: 33]: Training Loss: 1.849857861, Training Accuracy: 48.948\n",
            "0.9250621585045686\n",
            "Worker 1, [epoch: 34]: Training Loss: 1.831559701, Training Accuracy: 49.520\n",
            "0.9266729865248193\n",
            "Worker 1, [epoch: 35]: Training Loss: 1.813332284, Training Accuracy: 50.008\n",
            "0.9281693317482147\n",
            "Worker 1, [epoch: 36]: Training Loss: 1.785838590, Training Accuracy: 50.680\n",
            "0.92966979542957\n",
            "Worker 1, [epoch: 37]: Training Loss: 1.779076937, Training Accuracy: 50.508\n",
            "0.9311028008308145\n",
            "Worker 1, [epoch: 38]: Training Loss: 1.775592163, Training Accuracy: 50.844\n",
            "0.9324019659151082\n",
            "Worker 1, [epoch: 39]: Training Loss: 1.740937896, Training Accuracy: 51.792\n",
            "0.9336879938918392\n",
            "Worker 1, [epoch: 40]: Training Loss: 1.728161159, Training Accuracy: 51.776\n",
            "0.9349229428171515\n",
            "Worker 1, [epoch: 41]: Training Loss: 1.721367584, Training Accuracy: 52.076\n",
            "0.9361146166246137\n",
            "Worker 1, [epoch: 42]: Training Loss: 1.705732337, Training Accuracy: 52.316\n",
            "0.9372113966081643\n",
            "Worker 1, [epoch: 43]: Training Loss: 1.689996741, Training Accuracy: 53.116\n",
            "0.9382965351951059\n",
            "Worker 1, [epoch: 44]: Training Loss: 1.664970412, Training Accuracy: 53.104\n",
            "0.9393557039148218\n",
            "Worker 1, [epoch: 45]: Training Loss: 1.673300249, Training Accuracy: 53.216\n",
            "0.9403800850797321\n",
            "Worker 1, [epoch: 46]: Training Loss: 1.665104523, Training Accuracy: 53.428\n",
            "0.9413542866060852\n",
            "Worker 1, [epoch: 47]: Training Loss: 1.650144648, Training Accuracy: 53.852\n",
            "0.9422733367651079\n",
            "Worker 1, [epoch: 48]: Training Loss: 1.636417921, Training Accuracy: 54.384\n",
            "0.9431752837335695\n",
            "Worker 1, [epoch: 49]: Training Loss: 1.624281212, Training Accuracy: 54.452\n",
            "0.9440494057748522\n",
            "Worker 1, [epoch: 50]: Training Loss: 1.607815331, Training Accuracy: 54.660\n",
            "0.9448648479848488\n",
            "Worker 1, [epoch: 51]: Training Loss: 1.594883453, Training Accuracy: 55.336\n",
            "0.9456870243975343\n",
            "Worker 1, [epoch: 52]: Training Loss: 1.588559824, Training Accuracy: 55.084\n",
            "0.9464904137697486\n",
            "Worker 1, [epoch: 53]: Training Loss: 1.593536951, Training Accuracy: 55.180\n",
            "0.9472679771572371\n",
            "Worker 1, [epoch: 54]: Training Loss: 1.583026101, Training Accuracy: 55.396\n",
            "0.9479839815292971\n",
            "Worker 1, [epoch: 55]: Training Loss: 1.560537033, Training Accuracy: 56.248\n",
            "0.9487465147434144\n",
            "Worker 1, [epoch: 56]: Training Loss: 1.582434700, Training Accuracy: 55.220\n",
            "0.9494388973682867\n",
            "Worker 1, [epoch: 57]: Training Loss: 1.549702534, Training Accuracy: 56.300\n",
            "0.950159956820288\n",
            "Worker 1, [epoch: 58]: Training Loss: 1.564734370, Training Accuracy: 55.508\n",
            "Time taken for training worker 1: 0:09:55.096636\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 4.335562293, Training Accuracy: 4.280\n",
            "-0.348102977957621\n",
            "Worker 2, [epoch: 02]: Training Loss: 3.850640525, Training Accuracy: 10.952\n",
            "0.4263515799558043\n",
            "Worker 2, [epoch: 03]: Training Loss: 3.618722597, Training Accuracy: 14.624\n",
            "0.6037068013691922\n",
            "Worker 2, [epoch: 04]: Training Loss: 3.416737783, Training Accuracy: 17.604\n",
            "0.6779401146648275\n",
            "Worker 2, [epoch: 05]: Training Loss: 3.219049689, Training Accuracy: 20.976\n",
            "0.7266644507866158\n",
            "Worker 2, [epoch: 06]: Training Loss: 3.072059124, Training Accuracy: 23.132\n",
            "0.7549954405501829\n",
            "Worker 2, [epoch: 07]: Training Loss: 2.936178458, Training Accuracy: 26.356\n",
            "0.778409969582708\n",
            "Worker 2, [epoch: 08]: Training Loss: 2.833638055, Training Accuracy: 28.220\n",
            "0.7970434236387309\n",
            "Worker 2, [epoch: 09]: Training Loss: 2.744318945, Training Accuracy: 30.108\n",
            "0.8125861535429021\n",
            "Worker 2, [epoch: 10]: Training Loss: 2.672532172, Training Accuracy: 31.672\n",
            "0.8257609935354291\n",
            "Worker 2, [epoch: 11]: Training Loss: 2.595417847, Training Accuracy: 33.056\n",
            "0.836888600113926\n",
            "Worker 2, [epoch: 12]: Training Loss: 2.519303416, Training Accuracy: 34.440\n",
            "0.8461604077653851\n",
            "Worker 2, [epoch: 13]: Training Loss: 2.463968950, Training Accuracy: 35.972\n",
            "0.8544895099614189\n",
            "Worker 2, [epoch: 14]: Training Loss: 2.400211674, Training Accuracy: 36.960\n",
            "0.861990235605576\n",
            "Worker 2, [epoch: 15]: Training Loss: 2.377938788, Training Accuracy: 37.844\n",
            "0.8685797654765108\n",
            "Worker 2, [epoch: 16]: Training Loss: 2.306050264, Training Accuracy: 38.916\n",
            "0.8746570038955597\n",
            "Worker 2, [epoch: 17]: Training Loss: 2.271747898, Training Accuracy: 39.544\n",
            "0.8795466440640967\n",
            "Worker 2, [epoch: 18]: Training Loss: 2.218670110, Training Accuracy: 41.416\n",
            "0.8843585890784718\n",
            "Worker 2, [epoch: 19]: Training Loss: 2.185222966, Training Accuracy: 41.644\n",
            "0.8886052309686433\n",
            "Worker 2, [epoch: 20]: Training Loss: 2.144413807, Training Accuracy: 42.748\n",
            "0.892608443170242\n",
            "Worker 2, [epoch: 21]: Training Loss: 2.115874454, Training Accuracy: 43.268\n",
            "0.8963497666969407\n",
            "Worker 2, [epoch: 22]: Training Loss: 2.080074714, Training Accuracy: 43.816\n",
            "0.8998219514299454\n",
            "Worker 2, [epoch: 23]: Training Loss: 2.057616492, Training Accuracy: 44.412\n",
            "0.9029872734879666\n",
            "Worker 2, [epoch: 24]: Training Loss: 2.029807435, Training Accuracy: 45.200\n",
            "0.9059570575230229\n",
            "Worker 2, [epoch: 25]: Training Loss: 2.012831348, Training Accuracy: 45.708\n",
            "0.9087620220812741\n",
            "Worker 2, [epoch: 26]: Training Loss: 1.980568344, Training Accuracy: 46.128\n",
            "0.9111760329079682\n",
            "Worker 2, [epoch: 27]: Training Loss: 1.943458783, Training Accuracy: 47.532\n",
            "0.9135862055548928\n",
            "Worker 2, [epoch: 28]: Training Loss: 1.914382211, Training Accuracy: 47.488\n",
            "0.9159187601754039\n",
            "Worker 2, [epoch: 29]: Training Loss: 1.899478672, Training Accuracy: 47.652\n",
            "0.9180543751903881\n",
            "Worker 2, [epoch: 30]: Training Loss: 1.886621990, Training Accuracy: 48.428\n",
            "0.9200132115268509\n",
            "Worker 2, [epoch: 31]: Training Loss: 1.862893931, Training Accuracy: 49.212\n",
            "0.9219638734573694\n",
            "Worker 2, [epoch: 32]: Training Loss: 1.856386583, Training Accuracy: 49.096\n",
            "0.9238514413072476\n",
            "Worker 2, [epoch: 33]: Training Loss: 1.839077667, Training Accuracy: 49.224\n",
            "0.9255553849774978\n",
            "Worker 2, [epoch: 34]: Training Loss: 1.809004303, Training Accuracy: 50.192\n",
            "0.927225650000766\n",
            "Worker 2, [epoch: 35]: Training Loss: 1.805243742, Training Accuracy: 50.216\n",
            "0.9288108494706578\n",
            "Worker 2, [epoch: 36]: Training Loss: 1.789047391, Training Accuracy: 50.600\n",
            "0.9303176412272725\n",
            "Worker 2, [epoch: 37]: Training Loss: 1.764730333, Training Accuracy: 50.972\n",
            "0.9316580495943801\n",
            "Worker 2, [epoch: 38]: Training Loss: 1.738914147, Training Accuracy: 52.132\n",
            "0.9330273398803172\n",
            "Worker 2, [epoch: 39]: Training Loss: 1.738395366, Training Accuracy: 51.736\n",
            "0.9342575002131711\n",
            "Worker 2, [epoch: 40]: Training Loss: 1.705359872, Training Accuracy: 52.800\n",
            "0.9355286542663994\n",
            "Worker 2, [epoch: 41]: Training Loss: 1.718246174, Training Accuracy: 52.240\n",
            "0.9367011627385176\n",
            "Worker 2, [epoch: 42]: Training Loss: 1.684589869, Training Accuracy: 53.012\n",
            "0.9378155287994797\n",
            "Worker 2, [epoch: 43]: Training Loss: 1.671747679, Training Accuracy: 53.440\n",
            "0.9388942928199427\n",
            "Worker 2, [epoch: 44]: Training Loss: 1.671541389, Training Accuracy: 53.656\n",
            "0.9399744343224592\n",
            "Worker 2, [epoch: 45]: Training Loss: 1.667185392, Training Accuracy: 53.424\n",
            "0.9409848137260569\n",
            "Worker 2, [epoch: 46]: Training Loss: 1.642466282, Training Accuracy: 54.044\n",
            "0.9419461379002946\n",
            "Worker 2, [epoch: 47]: Training Loss: 1.631635124, Training Accuracy: 54.464\n",
            "0.9428954819820395\n",
            "Worker 2, [epoch: 48]: Training Loss: 1.626740582, Training Accuracy: 54.420\n",
            "0.9438188354584394\n",
            "Worker 2, [epoch: 49]: Training Loss: 1.616894603, Training Accuracy: 54.548\n",
            "0.9447091677211801\n",
            "Worker 2, [epoch: 50]: Training Loss: 1.611970793, Training Accuracy: 54.788\n",
            "0.9455593059769054\n",
            "Worker 2, [epoch: 51]: Training Loss: 1.591524231, Training Accuracy: 55.164\n",
            "0.9463883556024422\n",
            "Worker 2, [epoch: 52]: Training Loss: 1.590373643, Training Accuracy: 55.268\n",
            "0.947188947679299\n",
            "Worker 2, [epoch: 53]: Training Loss: 1.587670343, Training Accuracy: 55.500\n",
            "0.9479517999041187\n",
            "Worker 2, [epoch: 54]: Training Loss: 1.574717520, Training Accuracy: 55.916\n",
            "0.94871662459699\n",
            "Worker 2, [epoch: 55]: Training Loss: 1.572260149, Training Accuracy: 55.648\n",
            "0.9494475398652846\n",
            "Worker 2, [epoch: 56]: Training Loss: 1.567747760, Training Accuracy: 56.024\n",
            "0.9501333959825071\n",
            "Worker 2, [epoch: 57]: Training Loss: 1.546647733, Training Accuracy: 56.652\n",
            "Time taken for training worker 2: 0:09:51.978917\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001579\n",
            "Global Model: Test Loss: 3.253428626, Test Accuracy: 30.790\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.441484963, Training Accuracy: 36.104\n",
            "0.7675227522209498\n",
            "Worker 1, [epoch: 02]: Training Loss: 2.165434759, Training Accuracy: 42.004\n",
            "0.8776492149936265\n",
            "Worker 1, [epoch: 03]: Training Loss: 2.030348130, Training Accuracy: 45.212\n",
            "0.9080333638807184\n",
            "Worker 1, [epoch: 04]: Training Loss: 1.951788014, Training Accuracy: 46.984\n",
            "0.9236228563351786\n",
            "Worker 1, [epoch: 05]: Training Loss: 1.903716667, Training Accuracy: 48.308\n",
            "0.9331771673082321\n",
            "Worker 1, [epoch: 06]: Training Loss: 1.844704571, Training Accuracy: 49.512\n",
            "0.9403749177031684\n",
            "Worker 1, [epoch: 07]: Training Loss: 1.790373365, Training Accuracy: 50.212\n",
            "0.9459728355976624\n",
            "Worker 1, [epoch: 08]: Training Loss: 1.786750795, Training Accuracy: 50.808\n",
            "0.95041458022056\n",
            "Worker 1, [epoch: 09]: Training Loss: 1.744008150, Training Accuracy: 51.364\n",
            "Time taken for training worker 1: 0:01:33.819965\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.441927424, Training Accuracy: 36.452\n",
            "0.7939177809630484\n",
            "Worker 2, [epoch: 02]: Training Loss: 2.176125673, Training Accuracy: 41.684\n",
            "0.8882856184121966\n",
            "Worker 2, [epoch: 03]: Training Loss: 2.050606169, Training Accuracy: 44.768\n",
            "0.913412006742371\n",
            "Worker 2, [epoch: 04]: Training Loss: 1.957003621, Training Accuracy: 46.740\n",
            "0.9265437878952598\n",
            "Worker 2, [epoch: 05]: Training Loss: 1.901119569, Training Accuracy: 48.200\n",
            "0.935788052177893\n",
            "Worker 2, [epoch: 06]: Training Loss: 1.864293356, Training Accuracy: 49.072\n",
            "0.9416390959298614\n",
            "Worker 2, [epoch: 07]: Training Loss: 1.811094739, Training Accuracy: 50.340\n",
            "0.946203024791823\n",
            "Worker 2, [epoch: 08]: Training Loss: 1.774153195, Training Accuracy: 51.172\n",
            "0.9507537333393766\n",
            "Worker 2, [epoch: 09]: Training Loss: 1.766800430, Training Accuracy: 51.116\n",
            "Time taken for training worker 2: 0:01:33.754628\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001671\n",
            "Global Model: Test Loss: 2.108477715, Test Accuracy: 45.240\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 2.008879468, Training Accuracy: 45.896\n",
            "0.9452013203441856\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.921455961, Training Accuracy: 47.560\n",
            "0.9684767051075752\n",
            "Worker 1, [epoch: 03]: Training Loss: 1.872884180, Training Accuracy: 48.616\n",
            "Time taken for training worker 1: 0:00:30.404234\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 2.032884202, Training Accuracy: 45.000\n",
            "0.915136913443747\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.933482372, Training Accuracy: 47.552\n",
            "0.9571945305973592\n",
            "Worker 2, [epoch: 03]: Training Loss: 1.888775834, Training Accuracy: 48.596\n",
            "Time taken for training worker 2: 0:00:30.636280\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001638\n",
            "Global Model: Test Loss: 1.909428676, Test Accuracy: 49.750\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.934426798, Training Accuracy: 47.420\n",
            "0.9475590591686743\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.866821820, Training Accuracy: 49.064\n",
            "0.9724195080248894\n",
            "Worker 1, [epoch: 03]: Training Loss: 1.827620597, Training Accuracy: 49.840\n",
            "Time taken for training worker 1: 0:00:32.832382\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.937639687, Training Accuracy: 47.348\n",
            "0.9611069728584073\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.878201275, Training Accuracy: 48.560\n",
            "Time taken for training worker 2: 0:00:22.237737\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001743\n",
            "Global Model: Test Loss: 1.879833094, Test Accuracy: 50.170\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.858549459, Training Accuracy: 49.188\n",
            "0.9780085589566113\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.830778632, Training Accuracy: 49.896\n",
            "Time taken for training worker 1: 0:00:19.972413\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.918164616, Training Accuracy: 48.220\n",
            "0.9722959145608735\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.860238680, Training Accuracy: 49.096\n",
            "Time taken for training worker 2: 0:00:22.355768\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001753\n",
            "Global Model: Test Loss: 1.851969724, Test Accuracy: 51.070\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.832954184, Training Accuracy: 49.456\n",
            "0.9735937664596879\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.806881298, Training Accuracy: 50.312\n",
            "Time taken for training worker 1: 0:00:19.908225\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.859610205, Training Accuracy: 49.724\n",
            "0.9770239601797331\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.799156951, Training Accuracy: 50.472\n",
            "Time taken for training worker 2: 0:00:20.701233\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001721\n",
            "Global Model: Test Loss: 1.845314842, Test Accuracy: 50.990\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.814840208, Training Accuracy: 50.536\n",
            "0.9832826762129152\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.760856888, Training Accuracy: 51.088\n",
            "Time taken for training worker 1: 0:00:20.429287\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.831565712, Training Accuracy: 50.112\n",
            "0.9874000793222146\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.791312370, Training Accuracy: 50.524\n",
            "Time taken for training worker 2: 0:00:21.625130\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001892\n",
            "Global Model: Test Loss: 1.834418466, Test Accuracy: 51.570\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.788457951, Training Accuracy: 51.292\n",
            "0.9783127457111731\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.754022096, Training Accuracy: 52.020\n",
            "Time taken for training worker 1: 0:00:20.521717\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.811351999, Training Accuracy: 50.404\n",
            "0.9637947980094762\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.768563676, Training Accuracy: 51.604\n",
            "Time taken for training worker 2: 0:00:20.834117\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001644\n",
            "Global Model: Test Loss: 1.820758780, Test Accuracy: 52.180\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.760126105, Training Accuracy: 51.468\n",
            "0.970975308263755\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.718824731, Training Accuracy: 52.448\n",
            "Time taken for training worker 1: 0:00:19.931957\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.778509757, Training Accuracy: 50.988\n",
            "0.9967439577013202\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.774283988, Training Accuracy: 51.096\n",
            "Time taken for training worker 2: 0:00:20.043574\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001649\n",
            "Global Model: Test Loss: 1.856049447, Test Accuracy: 51.310\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.726118579, Training Accuracy: 52.632\n",
            "0.9932018567614421\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.714282723, Training Accuracy: 52.400\n",
            "Time taken for training worker 1: 0:00:20.564225\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.776599798, Training Accuracy: 50.976\n",
            "0.9748496558948696\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.742977291, Training Accuracy: 51.816\n",
            "Time taken for training worker 2: 0:00:23.103424\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001895\n",
            "Global Model: Test Loss: 1.808863468, Test Accuracy: 51.970\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.724624241, Training Accuracy: 52.012\n",
            "0.9638607557287779\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.691517297, Training Accuracy: 53.248\n",
            "Time taken for training worker 1: 0:00:19.861562\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.751709350, Training Accuracy: 51.760\n",
            "0.9821470472083909\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.718690128, Training Accuracy: 52.364\n",
            "Time taken for training worker 2: 0:00:20.707950\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001713\n",
            "Global Model: Test Loss: 1.794640571, Test Accuracy: 52.340\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.700940563, Training Accuracy: 52.896\n",
            "0.9827583995546881\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.684544260, Training Accuracy: 53.492\n",
            "Time taken for training worker 1: 0:00:21.797808\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.723142938, Training Accuracy: 52.540\n",
            "0.9891429439380827\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.682588752, Training Accuracy: 52.912\n",
            "Time taken for training worker 2: 0:00:20.588081\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001674\n",
            "Global Model: Test Loss: 1.820190894, Test Accuracy: 51.910\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.695614801, Training Accuracy: 53.108\n",
            "0.9916775627327062\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.670810274, Training Accuracy: 53.396\n",
            "Time taken for training worker 1: 0:00:21.987602\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.710762081, Training Accuracy: 53.244\n",
            "0.996759602201932\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.685517853, Training Accuracy: 53.132\n",
            "Time taken for training worker 2: 0:00:21.525898\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001772\n",
            "Global Model: Test Loss: 1.796985030, Test Accuracy: 53.070\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.688069032, Training Accuracy: 53.272\n",
            "0.9872276190739123\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.659220286, Training Accuracy: 53.716\n",
            "Time taken for training worker 1: 0:00:21.885314\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.676651874, Training Accuracy: 53.432\n",
            "0.9817915653054498\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.667490141, Training Accuracy: 54.068\n",
            "Time taken for training worker 2: 0:00:21.776783\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001796\n",
            "Global Model: Test Loss: 1.811456108, Test Accuracy: 52.140\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.678334790, Training Accuracy: 53.092\n",
            "0.963907884508378\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.630721565, Training Accuracy: 54.352\n",
            "Time taken for training worker 1: 0:00:20.715232\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.677051069, Training Accuracy: 53.192\n",
            "0.9700906503626155\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.640359608, Training Accuracy: 54.236\n",
            "Time taken for training worker 2: 0:00:20.955973\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001555\n",
            "Global Model: Test Loss: 1.804896040, Test Accuracy: 52.450\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.641280684, Training Accuracy: 54.284\n",
            "0.9837577354922699\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.625080826, Training Accuracy: 54.860\n",
            "Time taken for training worker 1: 0:00:20.911097\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.661457667, Training Accuracy: 54.016\n",
            "0.9830012323963041\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.626943878, Training Accuracy: 54.616\n",
            "Time taken for training worker 2: 0:00:21.288825\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001635\n",
            "Global Model: Test Loss: 1.798471048, Test Accuracy: 52.320\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.651693212, Training Accuracy: 54.104\n",
            "0.9749488250619589\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.616958901, Training Accuracy: 54.992\n",
            "Time taken for training worker 1: 0:00:21.611513\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.648689214, Training Accuracy: 53.960\n",
            "0.975780529640271\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.615151349, Training Accuracy: 54.816\n",
            "Time taken for training worker 2: 0:00:22.531350\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001809\n",
            "Global Model: Test Loss: 1.798706229, Test Accuracy: 52.610\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.643312836, Training Accuracy: 54.608\n",
            "0.9858626275724321\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.598782350, Training Accuracy: 55.112\n",
            "Time taken for training worker 1: 0:00:20.114929\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.633057352, Training Accuracy: 54.272\n",
            "0.9791599335901524\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.622009099, Training Accuracy: 55.012\n",
            "Time taken for training worker 2: 0:00:22.151165\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001807\n",
            "Global Model: Test Loss: 1.781219914, Test Accuracy: 53.130\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.619233348, Training Accuracy: 54.896\n",
            "0.9950584558591029\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.613648323, Training Accuracy: 54.720\n",
            "Time taken for training worker 1: 0:00:19.686071\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.609591069, Training Accuracy: 55.212\n",
            "0.9918829835537352\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.581451256, Training Accuracy: 55.504\n",
            "Time taken for training worker 2: 0:00:20.365106\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001684\n",
            "Global Model: Test Loss: 1.768167619, Test Accuracy: 52.740\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.601743190, Training Accuracy: 55.332\n",
            "0.9857160936751285\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.585657247, Training Accuracy: 55.848\n",
            "Time taken for training worker 1: 0:00:20.026505\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.583642718, Training Accuracy: 55.832\n",
            "0.9832620128176345\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.605410608, Training Accuracy: 55.228\n",
            "Time taken for training worker 2: 0:00:19.903378\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001707\n",
            "Global Model: Test Loss: 1.773224673, Test Accuracy: 53.060\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.605934520, Training Accuracy: 55.088\n",
            "0.9784746338518165\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.574847334, Training Accuracy: 55.864\n",
            "Time taken for training worker 1: 0:00:20.992276\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.593124435, Training Accuracy: 55.260\n",
            "0.9932202269412561\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.586823515, Training Accuracy: 55.504\n",
            "Time taken for training worker 2: 0:00:20.841860\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001704\n",
            "Global Model: Test Loss: 1.781857139, Test Accuracy: 52.380\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.588589429, Training Accuracy: 55.952\n",
            "0.9950574298142696\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.570746014, Training Accuracy: 56.132\n",
            "Time taken for training worker 1: 0:00:23.255652\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.584735237, Training Accuracy: 55.380\n",
            "0.9721342040971415\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.553235895, Training Accuracy: 56.392\n",
            "Time taken for training worker 2: 0:00:20.821997\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001627\n",
            "Global Model: Test Loss: 1.798010355, Test Accuracy: 52.360\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.575172582, Training Accuracy: 55.672\n",
            "0.978262775901864\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.556511796, Training Accuracy: 56.464\n",
            "Time taken for training worker 1: 0:00:21.657529\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.580936305, Training Accuracy: 55.680\n",
            "0.9877777942603102\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.567465261, Training Accuracy: 56.124\n",
            "Time taken for training worker 2: 0:00:20.439820\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001635\n",
            "Global Model: Test Loss: 1.773985280, Test Accuracy: 53.120\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.556317695, Training Accuracy: 56.368\n",
            "0.9937881816642172\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.548754917, Training Accuracy: 56.596\n",
            "Time taken for training worker 1: 0:00:22.049342\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.570847260, Training Accuracy: 55.708\n",
            "0.9735995112131312\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.543035571, Training Accuracy: 56.672\n",
            "Time taken for training worker 2: 0:00:22.298536\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001873\n",
            "Global Model: Test Loss: 1.784452558, Test Accuracy: 53.250\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.544126874, Training Accuracy: 56.716\n",
            "0.9794095220285584\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.523196620, Training Accuracy: 57.480\n",
            "Time taken for training worker 1: 0:00:22.939735\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.548460636, Training Accuracy: 56.516\n",
            "0.9946724664103159\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.536464466, Training Accuracy: 56.712\n",
            "Time taken for training worker 2: 0:00:22.495833\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001861\n",
            "Global Model: Test Loss: 1.769428227, Test Accuracy: 53.400\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.529839505, Training Accuracy: 57.048\n",
            "0.9918215402507766\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.522074504, Training Accuracy: 57.352\n",
            "Time taken for training worker 1: 0:00:22.710814\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.540652479, Training Accuracy: 56.992\n",
            "0.9967632859105294\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.512797122, Training Accuracy: 57.112\n",
            "Time taken for training worker 2: 0:00:20.142295\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001635\n",
            "Global Model: Test Loss: 1.773251961, Test Accuracy: 53.320\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.544408699, Training Accuracy: 56.540\n",
            "0.9791310500410672\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.518022960, Training Accuracy: 57.312\n",
            "Time taken for training worker 1: 0:00:19.747637\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.525005530, Training Accuracy: 57.400\n",
            "0.9993567163233036\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.513598614, Training Accuracy: 57.424\n",
            "Time taken for training worker 2: 0:00:19.815907\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001678\n",
            "Global Model: Test Loss: 1.793588507, Test Accuracy: 52.550\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.536267667, Training Accuracy: 56.632\n",
            "0.9808812695719725\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.510715955, Training Accuracy: 57.340\n",
            "Time taken for training worker 1: 0:00:23.001047\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.531033838, Training Accuracy: 57.304\n",
            "0.9932476384174095\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.502211562, Training Accuracy: 57.556\n",
            "Time taken for training worker 2: 0:00:21.761189\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001706\n",
            "Global Model: Test Loss: 1.785655555, Test Accuracy: 53.880\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.503418472, Training Accuracy: 57.616\n",
            "0.9985035920844305\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.501849858, Training Accuracy: 57.560\n",
            "Time taken for training worker 1: 0:00:20.202127\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.516429890, Training Accuracy: 57.140\n",
            "0.9841389663797355\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.500835417, Training Accuracy: 57.732\n",
            "Time taken for training worker 2: 0:00:20.672117\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001605\n",
            "Global Model: Test Loss: 1.797995920, Test Accuracy: 52.800\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.496814352, Training Accuracy: 57.856\n",
            "0.9984049596428113\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.476653894, Training Accuracy: 57.916\n",
            "Time taken for training worker 1: 0:00:19.911617\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.505192033, Training Accuracy: 56.924\n",
            "0.969699619376246\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.481338887, Training Accuracy: 58.056\n",
            "Time taken for training worker 2: 0:00:20.134357\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001550\n",
            "Global Model: Test Loss: 1.766152875, Test Accuracy: 54.080\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.502893168, Training Accuracy: 57.472\n",
            "0.9865640591196666\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.479892968, Training Accuracy: 57.976\n",
            "Time taken for training worker 1: 0:00:20.152375\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.491124954, Training Accuracy: 57.660\n",
            "0.9798497631954002\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.469387951, Training Accuracy: 58.420\n",
            "Time taken for training worker 2: 0:00:20.285073\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001630\n",
            "Global Model: Test Loss: 1.783663742, Test Accuracy: 53.650\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.493832426, Training Accuracy: 58.104\n",
            "0.9956504420728165\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.480028918, Training Accuracy: 57.940\n",
            "Time taken for training worker 1: 0:00:20.377031\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.499918575, Training Accuracy: 57.748\n",
            "0.97641257075309\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.462990102, Training Accuracy: 58.640\n",
            "Time taken for training worker 2: 0:00:21.638049\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001769\n",
            "Global Model: Test Loss: 1.760832695, Test Accuracy: 53.700\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.490589227, Training Accuracy: 58.092\n",
            "0.9987290177422227\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.464335003, Training Accuracy: 58.140\n",
            "Time taken for training worker 1: 0:00:22.871501\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.472636267, Training Accuracy: 58.444\n",
            "0.9887717911118268\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.452920584, Training Accuracy: 58.872\n",
            "Time taken for training worker 2: 0:00:22.918869\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001897\n",
            "Global Model: Test Loss: 1.797673720, Test Accuracy: 53.020\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.487504560, Training Accuracy: 57.772\n",
            "0.9787322724592165\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.465596909, Training Accuracy: 58.576\n",
            "Time taken for training worker 1: 0:00:20.623811\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.473099349, Training Accuracy: 58.272\n",
            "0.9764150132532354\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.439943372, Training Accuracy: 59.172\n",
            "Time taken for training worker 2: 0:00:19.820730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001689\n",
            "Global Model: Test Loss: 1.750163048, Test Accuracy: 53.700\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.476976897, Training Accuracy: 58.264\n",
            "0.9845479872956883\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.449551930, Training Accuracy: 58.852\n",
            "Time taken for training worker 1: 0:00:20.346937\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.463793832, Training Accuracy: 58.552\n",
            "0.9839992666691768\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.444989393, Training Accuracy: 59.164\n",
            "Time taken for training worker 2: 0:00:20.796330\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001644\n",
            "Global Model: Test Loss: 1.751528814, Test Accuracy: 53.960\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.464264037, Training Accuracy: 58.676\n",
            "0.9931962918777504\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.435929061, Training Accuracy: 58.936\n",
            "Time taken for training worker 1: 0:00:22.591523\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.450259608, Training Accuracy: 58.740\n",
            "0.9678543835399456\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.407468505, Training Accuracy: 59.980\n",
            "Time taken for training worker 2: 0:00:22.909105\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001855\n",
            "Global Model: Test Loss: 1.748978841, Test Accuracy: 54.070\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.438405338, Training Accuracy: 59.168\n",
            "0.9954293885636174\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.426430671, Training Accuracy: 59.344\n",
            "Time taken for training worker 1: 0:00:20.118106\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.439390111, Training Accuracy: 59.388\n",
            "0.9897730785693427\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.417187800, Training Accuracy: 59.784\n",
            "Time taken for training worker 2: 0:00:19.823226\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001560\n",
            "Global Model: Test Loss: 1.755615721, Test Accuracy: 54.420\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.441891383, Training Accuracy: 59.052\n",
            "0.9807334027355731\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.409276637, Training Accuracy: 59.796\n",
            "Time taken for training worker 1: 0:00:21.045859\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.421679126, Training Accuracy: 59.440\n",
            "0.9897819957012786\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.415988240, Training Accuracy: 59.836\n",
            "Time taken for training worker 2: 0:00:21.863626\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001741\n",
            "Global Model: Test Loss: 1.776595078, Test Accuracy: 53.230\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.424630497, Training Accuracy: 59.536\n",
            "0.9822170518039828\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.389844383, Training Accuracy: 60.228\n",
            "Time taken for training worker 1: 0:00:21.119724\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.424719790, Training Accuracy: 59.620\n",
            "0.9960817483779852\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.413118224, Training Accuracy: 59.772\n",
            "Time taken for training worker 2: 0:00:22.870153\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001916\n",
            "Global Model: Test Loss: 1.752809509, Test Accuracy: 54.110\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.423762483, Training Accuracy: 59.640\n",
            "0.9788848728213946\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.394086417, Training Accuracy: 60.464\n",
            "Time taken for training worker 1: 0:00:21.654538\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.417741158, Training Accuracy: 59.848\n",
            "0.9823092251075871\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.379970144, Training Accuracy: 60.540\n",
            "Time taken for training worker 2: 0:00:20.864424\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001621\n",
            "Global Model: Test Loss: 1.788494676, Test Accuracy: 53.940\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.414928264, Training Accuracy: 60.356\n",
            "0.9864955986127598\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.373630021, Training Accuracy: 60.888\n",
            "Time taken for training worker 1: 0:00:22.967996\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.409793353, Training Accuracy: 59.704\n",
            "0.9865531620126664\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.372553105, Training Accuracy: 60.228\n",
            "Time taken for training worker 2: 0:00:22.749154\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001873\n",
            "Global Model: Test Loss: 1.773774066, Test Accuracy: 53.830\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.387387900, Training Accuracy: 60.556\n",
            "0.9990854441894255\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.378557527, Training Accuracy: 60.592\n",
            "Time taken for training worker 1: 0:00:22.838798\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.393348250, Training Accuracy: 60.108\n",
            "0.9793512325064221\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.359934779, Training Accuracy: 60.920\n",
            "Time taken for training worker 2: 0:00:21.657456\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001786\n",
            "Global Model: Test Loss: 1.786574001, Test Accuracy: 53.690\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.382218327, Training Accuracy: 60.764\n",
            "Time taken for training worker 1: 0:00:11.496508\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.373296689, Training Accuracy: 60.584\n",
            "0.9904790939084601\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.362589234, Training Accuracy: 60.960\n",
            "Time taken for training worker 2: 0:00:22.649792\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001860\n",
            "Global Model: Test Loss: 1.752551184, Test Accuracy: 54.160\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 1, [epoch: 01]: Training Loss: 1.401801781, Training Accuracy: 60.224\n",
            "0.9869727680964748\n",
            "Worker 1, [epoch: 02]: Training Loss: 1.376395844, Training Accuracy: 60.736\n",
            "Time taken for training worker 1: 0:00:19.646126\n",
            "--------------------------------------------------\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.346642144, Training Accuracy: 61.096\n",
            "Time taken for training worker 2: 0:00:10.969795\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001700\n",
            "Global Model: Test Loss: 1.764284981, Test Accuracy: 53.970\n",
            "**************************************************\n",
            "0.0\n",
            "Worker 2, [epoch: 01]: Training Loss: 1.357741918, Training Accuracy: 61.028\n",
            "0.9895460484724797\n",
            "Worker 2, [epoch: 02]: Training Loss: 1.353532180, Training Accuracy: 61.444\n",
            "Time taken for training worker 2: 0:00:19.633066\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001731\n",
            "Global Model: Test Loss: 1.742772651, Test Accuracy: 54.220\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for Confidence Interval: 0:53:46.348835\n",
            "//////////////////////////////////////////////////\n"
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [2]\n",
        "num_epochs = 150\n",
        "ci_acc_thresholds= [0.8, 0.9, 0.95]\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "    for ci_acc_threshold in ci_acc_thresholds:\n",
        "        shard_loaders = data.iid_shards(num_shards=k)\n",
        "        print('='*50)\n",
        "        print(f'Number of Workers:{k}, CI Accuracy Threshold:{ci_acc_threshold}')\n",
        "        print('='*50)\n",
        "        ConfidenceInetrvalApproach(shard_loaders, loss_fn, k, lr, wd, initial_state_dict, num_epochs, ci_acc_threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Our Approach 3: Heuristic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class HeuristicOptimizer(Optimizer):\n",
        "    def __init__(self, global_model, lr=0.01):\n",
        "        self.global_model = global_model\n",
        "        self.lr = lr\n",
        "        params = list(global_model.parameters())\n",
        "        super(HeuristicOptimizer, self).__init__(params, {'lr': lr})\n",
        "\n",
        "    def step(self, local_models, train_accuracies, local_optimizers):\n",
        "        # Get global model parameters\n",
        "        global_params = list(self.global_model.parameters())\n",
        "        \n",
        "        # Initialize deltas for each parameter, same size as global parameters\n",
        "        deltas = [torch.zeros_like(param) for param in global_params]\n",
        "        \n",
        "        # TODO: Mitoonim begim agar tafavot az ye meghdari bishtar bood average begirim\n",
        "        \n",
        "        # Sum up differences between global model and local models\n",
        "        for i, local_model in enumerate(local_models):\n",
        "            if train_accuracies[i] == max(train_accuracies):\n",
        "                local_params = list(local_models[i].parameters())\n",
        "                for j, param in enumerate(local_params):\n",
        "                    deltas[j] += (global_params[j] - param)/local_optimizers[i].param_groups[0]['lr']\n",
        "        # Average the delta over the number of local models\n",
        "        # for i, delta in enumerate(deltas):\n",
        "        #     deltas[i] /= self.lr\n",
        "        \n",
        "        # Update global model parameters\n",
        "        with torch.no_grad():\n",
        "            for i, param in enumerate(global_params):\n",
        "                param.copy_(param - self.lr * deltas[i])\n",
        "        \n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def local_SGD(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs):\n",
        "  total_start_time = time.time()\n",
        "\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = []\n",
        "  for i, model in enumerate(local_models):\n",
        "    new_lr = lr * (1.5 ** i)\n",
        "    local_optimizers.append(torch.optim.SGD(model.parameters(), lr=new_lr, momentum=0.9, weight_decay=wd))\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "  \n",
        "  global_optimizer = HeuristicOptimizer(global_model, lr)\n",
        "  \n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    train_accuracies = []\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for loca_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      print(f\"lr: {local_optimizers[worker].param_groups[0]['lr']}\")\n",
        "      train_accuracies.append(train_accuracy) # Add last train accuracy to the list\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    global_optimizer.step(local_models, train_accuracies, local_optimizers)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # for local_optimizer in local_optimizers:\n",
        "    #     # print(f\"{local_optimizer.param_groups[0]['lr']}-->{global_optimizer.param_groups[0]['lr']}\") \n",
        "    #     local_optimizer.param_groups[0]['lr'] = global_optimizer.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "    for local_model in local_models:\n",
        "      local_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Local Step {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "# TODO: Schedule ro check konim \n",
        "\n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for local_SGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:1\n",
            "==================================================\n",
            "Worker 1, [01/01]: Training Loss: 4.586346840, Training Accuracy: 1.520\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.827075\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 4.560910171, Training Accuracy: 1.856\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.608530\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 4.495338233, Training Accuracy: 2.424\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.685312\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 4.421388424, Training Accuracy: 3.488\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.647228\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001866\n",
            "Local Step 01: Test Loss: 4.552944527, Test Accuracy: 4.320\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 4.238873107, Training Accuracy: 5.792\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.668817\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 4.229189398, Training Accuracy: 5.528\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.664886\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 4.202291133, Training Accuracy: 5.680\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.496758\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 4.188538775, Training Accuracy: 5.832\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.446569\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002201\n",
            "Local Step 02: Test Loss: 4.419528989, Test Accuracy: 8.230\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 4.041437251, Training Accuracy: 8.168\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.835174\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 4.053155989, Training Accuracy: 7.512\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.726874\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 4.055333668, Training Accuracy: 7.632\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.624241\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 4.040313016, Training Accuracy: 7.600\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.722114\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001797\n",
            "Local Step 03: Test Loss: 3.880050497, Test Accuracy: 9.990\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.837097140, Training Accuracy: 10.912\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.734418\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.893875205, Training Accuracy: 9.552\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.589071\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.904006689, Training Accuracy: 9.312\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.815733\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.934977205, Training Accuracy: 9.168\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.040617\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002611\n",
            "Local Step 04: Test Loss: 3.745804683, Test Accuracy: 12.700\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.733228001, Training Accuracy: 12.656\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.299437\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.814418996, Training Accuracy: 10.696\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.879947\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.828548347, Training Accuracy: 10.976\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.064773\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.869865956, Training Accuracy: 10.496\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.205706\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002110\n",
            "Local Step 05: Test Loss: 3.660509987, Test Accuracy: 14.090\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.627154313, Training Accuracy: 14.096\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.193443\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.731151167, Training Accuracy: 12.504\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.253446\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.760326607, Training Accuracy: 11.504\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.211702\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.790160337, Training Accuracy: 11.392\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.383617\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002052\n",
            "Local Step 06: Test Loss: 3.561030368, Test Accuracy: 15.990\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.548142121, Training Accuracy: 15.552\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.139436\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.663883922, Training Accuracy: 13.608\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.747692\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.695458924, Training Accuracy: 12.728\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.323115\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.762245962, Training Accuracy: 11.480\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.780305\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001869\n",
            "Local Step 07: Test Loss: 3.481307988, Test Accuracy: 16.530\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.462028351, Training Accuracy: 16.952\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.747732\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.599913839, Training Accuracy: 14.632\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.758427\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.636808018, Training Accuracy: 13.592\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.959821\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.703629963, Training Accuracy: 12.912\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.708544\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001957\n",
            "Local Step 08: Test Loss: 3.408596517, Test Accuracy: 18.180\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.365793089, Training Accuracy: 18.296\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.355616\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.527673286, Training Accuracy: 15.648\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.206416\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.586825123, Training Accuracy: 14.640\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.995698\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.653142119, Training Accuracy: 13.888\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.344692\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002218\n",
            "Local Step 09: Test Loss: 3.400472986, Test Accuracy: 18.320\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.297297991, Training Accuracy: 19.672\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.933089\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.480865458, Training Accuracy: 16.480\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.881073\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.524877963, Training Accuracy: 15.568\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.244519\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.610625464, Training Accuracy: 14.400\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.697063\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001835\n",
            "Local Step 10: Test Loss: 3.260277797, Test Accuracy: 21.350\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.225343541, Training Accuracy: 20.904\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.352153\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.408713349, Training Accuracy: 17.848\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.247905\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.470000580, Training Accuracy: 16.712\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.462412\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.563211620, Training Accuracy: 15.552\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.164420\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002164\n",
            "Local Step 11: Test Loss: 3.238366168, Test Accuracy: 21.340\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.141627972, Training Accuracy: 22.288\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.471639\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.364014679, Training Accuracy: 18.992\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.803910\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.428920292, Training Accuracy: 17.760\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.885431\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.499099263, Training Accuracy: 16.272\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.792001\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001921\n",
            "Local Step 12: Test Loss: 3.149087267, Test Accuracy: 23.490\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.066436495, Training Accuracy: 23.504\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.893106\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.316231016, Training Accuracy: 19.400\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.906193\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.380692427, Training Accuracy: 18.192\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.418573\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.468166350, Training Accuracy: 17.016\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.527088\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002077\n",
            "Local Step 13: Test Loss: 3.065372876, Test Accuracy: 24.560\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 3.009260029, Training Accuracy: 24.976\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.844380\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.272598343, Training Accuracy: 20.240\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.850889\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.369685163, Training Accuracy: 18.656\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.757449\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.445702001, Training Accuracy: 17.152\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.677514\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002046\n",
            "Local Step 14: Test Loss: 3.033493440, Test Accuracy: 24.880\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.944891599, Training Accuracy: 25.696\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.952399\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.245366230, Training Accuracy: 20.432\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.433778\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.332582033, Training Accuracy: 19.232\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.243472\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.417022357, Training Accuracy: 17.304\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.309169\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002283\n",
            "Local Step 15: Test Loss: 3.043893451, Test Accuracy: 24.790\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.898780649, Training Accuracy: 27.272\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.852639\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.208635165, Training Accuracy: 21.640\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.795391\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.282663074, Training Accuracy: 20.224\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.178797\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.401291308, Training Accuracy: 18.488\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.757184\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002193\n",
            "Local Step 16: Test Loss: 2.985281279, Test Accuracy: 26.210\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.831527760, Training Accuracy: 28.552\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.639520\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.179779235, Training Accuracy: 21.872\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.259150\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.261571235, Training Accuracy: 20.408\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.768376\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.412187095, Training Accuracy: 18.048\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.640722\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002548\n",
            "Local Step 17: Test Loss: 3.002070093, Test Accuracy: 26.330\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.787735080, Training Accuracy: 28.712\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.915722\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.162633570, Training Accuracy: 23.104\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.947252\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.228881285, Training Accuracy: 20.944\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.855971\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.366698404, Training Accuracy: 18.904\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.800594\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002128\n",
            "Local Step 18: Test Loss: 2.882841399, Test Accuracy: 28.060\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.727944614, Training Accuracy: 30.736\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.566070\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.130175982, Training Accuracy: 23.144\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.430210\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.201088945, Training Accuracy: 21.264\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.375264\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.326829982, Training Accuracy: 19.720\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.768447\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002237\n",
            "Local Step 19: Test Loss: 2.866264752, Test Accuracy: 28.130\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.659194478, Training Accuracy: 31.840\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.733409\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.089820602, Training Accuracy: 23.184\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.774412\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.186812964, Training Accuracy: 21.728\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.744696\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.301502301, Training Accuracy: 20.224\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.015778\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002019\n",
            "Local Step 20: Test Loss: 2.894322954, Test Accuracy: 27.660\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.619244876, Training Accuracy: 32.760\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.821869\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.070992849, Training Accuracy: 24.288\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.730241\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.167443048, Training Accuracy: 22.472\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.769038\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.284179177, Training Accuracy: 20.256\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.790156\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002119\n",
            "Local Step 21: Test Loss: 2.857052964, Test Accuracy: 29.400\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.556701090, Training Accuracy: 33.432\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.782000\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.055977409, Training Accuracy: 25.024\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.873821\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.150574008, Training Accuracy: 22.592\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.184248\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.272885270, Training Accuracy: 20.096\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.861620\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002257\n",
            "Local Step 22: Test Loss: 2.806169762, Test Accuracy: 30.230\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.504534403, Training Accuracy: 34.688\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.173445\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.018905204, Training Accuracy: 25.408\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.920792\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.128770084, Training Accuracy: 23.688\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.766385\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.277641481, Training Accuracy: 20.576\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.823677\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001906\n",
            "Local Step 23: Test Loss: 2.820078214, Test Accuracy: 29.700\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.477829267, Training Accuracy: 35.360\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.234325\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.016137212, Training Accuracy: 25.704\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.708417\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.107488007, Training Accuracy: 23.688\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.908420\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.278325355, Training Accuracy: 21.072\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.965801\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002867\n",
            "Local Step 24: Test Loss: 2.792787968, Test Accuracy: 31.040\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.413510792, Training Accuracy: 36.544\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.755123\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.993146510, Training Accuracy: 26.168\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.300951\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.077812122, Training Accuracy: 23.952\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.251318\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.208278714, Training Accuracy: 21.960\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.856373\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001977\n",
            "Local Step 25: Test Loss: 2.752799950, Test Accuracy: 31.030\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.378313952, Training Accuracy: 38.008\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.742217\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.969827596, Training Accuracy: 26.360\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.754860\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.065756451, Training Accuracy: 24.144\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.833470\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.220449350, Training Accuracy: 21.968\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.784705\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002079\n",
            "Local Step 26: Test Loss: 2.747285258, Test Accuracy: 31.210\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.324136764, Training Accuracy: 37.880\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.922425\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.953031630, Training Accuracy: 26.440\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.098951\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.075871554, Training Accuracy: 24.600\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.506135\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.170918315, Training Accuracy: 22.736\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.693717\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002170\n",
            "Local Step 27: Test Loss: 2.698561075, Test Accuracy: 32.670\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.267792957, Training Accuracy: 39.720\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.719990\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.944219976, Training Accuracy: 26.984\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.717384\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.038546302, Training Accuracy: 25.008\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.890694\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.173790876, Training Accuracy: 22.312\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.842510\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001934\n",
            "Local Step 28: Test Loss: 2.728461100, Test Accuracy: 32.240\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.238834340, Training Accuracy: 40.496\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.356859\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.942142103, Training Accuracy: 27.056\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.493065\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.032378985, Training Accuracy: 25.272\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.252945\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.174909196, Training Accuracy: 22.968\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.991941\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001817\n",
            "Local Step 29: Test Loss: 2.754624332, Test Accuracy: 32.080\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.186121425, Training Accuracy: 41.096\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.322116\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.896217153, Training Accuracy: 28.272\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.300915\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.025181213, Training Accuracy: 25.440\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.859863\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.179321611, Training Accuracy: 22.112\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.136498\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002210\n",
            "Local Step 30: Test Loss: 2.714450862, Test Accuracy: 32.770\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.158068000, Training Accuracy: 41.888\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.169848\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.907265803, Training Accuracy: 27.464\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.750681\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.012810297, Training Accuracy: 26.024\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.273653\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.159651699, Training Accuracy: 22.944\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.000415\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001976\n",
            "Local Step 31: Test Loss: 2.678353869, Test Accuracy: 33.330\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.123792603, Training Accuracy: 42.640\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.053573\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.888653789, Training Accuracy: 28.128\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.851579\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.981203883, Training Accuracy: 26.632\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.782177\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.158486428, Training Accuracy: 23.344\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.721285\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001805\n",
            "Local Step 32: Test Loss: 2.652456760, Test Accuracy: 34.800\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.088431794, Training Accuracy: 44.000\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.405336\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.890316816, Training Accuracy: 28.000\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.344206\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.006041011, Training Accuracy: 26.136\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.974372\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.166195033, Training Accuracy: 23.120\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.207772\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002870\n",
            "Local Step 33: Test Loss: 2.665081640, Test Accuracy: 34.040\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 2.016513224, Training Accuracy: 44.600\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.752968\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.881543336, Training Accuracy: 28.408\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.791389\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.963884196, Training Accuracy: 26.832\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.900707\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.136918238, Training Accuracy: 23.576\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.828629\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002174\n",
            "Local Step 34: Test Loss: 2.683732201, Test Accuracy: 34.300\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.990126918, Training Accuracy: 45.304\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.584135\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.870718929, Training Accuracy: 29.032\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.801511\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.970735284, Training Accuracy: 26.472\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.203194\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.167117148, Training Accuracy: 23.120\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.845084\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002105\n",
            "Local Step 35: Test Loss: 2.676239857, Test Accuracy: 34.720\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.967322024, Training Accuracy: 46.296\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.770281\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.865685638, Training Accuracy: 28.624\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.679808\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.976517779, Training Accuracy: 26.672\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.904309\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.148775952, Training Accuracy: 23.464\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.904701\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002090\n",
            "Local Step 36: Test Loss: 2.666108769, Test Accuracy: 34.840\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.943832780, Training Accuracy: 46.816\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.453128\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.858068407, Training Accuracy: 28.880\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.337381\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.987706726, Training Accuracy: 26.592\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.125243\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.136128044, Training Accuracy: 23.608\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.820124\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002072\n",
            "Local Step 37: Test Loss: 2.718582269, Test Accuracy: 33.940\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.881603231, Training Accuracy: 47.648\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.921230\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.882401294, Training Accuracy: 28.696\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.818426\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.977719331, Training Accuracy: 26.584\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.075843\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.115345737, Training Accuracy: 24.144\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.386038\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002361\n",
            "Local Step 38: Test Loss: 2.714848292, Test Accuracy: 34.660\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.867411089, Training Accuracy: 47.944\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.837528\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.856714173, Training Accuracy: 28.944\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.245115\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.958275625, Training Accuracy: 27.216\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.388594\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.128096512, Training Accuracy: 23.760\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.859243\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001837\n",
            "Local Step 39: Test Loss: 2.692166460, Test Accuracy: 34.070\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.817996830, Training Accuracy: 49.632\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.817794\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.841997286, Training Accuracy: 29.336\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.776978\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.975516429, Training Accuracy: 27.104\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.968942\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.120257992, Training Accuracy: 23.912\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.840282\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001844\n",
            "Local Step 40: Test Loss: 2.737485388, Test Accuracy: 34.550\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.789740084, Training Accuracy: 50.224\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.899345\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.852909806, Training Accuracy: 29.536\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.788349\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.958695558, Training Accuracy: 27.272\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.849397\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.120458020, Training Accuracy: 24.136\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.159600\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001869\n",
            "Local Step 41: Test Loss: 2.739255418, Test Accuracy: 35.200\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.762458238, Training Accuracy: 51.528\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.839782\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.866324692, Training Accuracy: 29.088\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.785076\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.960572724, Training Accuracy: 26.576\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.573323\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.174955391, Training Accuracy: 23.032\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.336704\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002059\n",
            "Local Step 42: Test Loss: 2.732215026, Test Accuracy: 35.430\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.724484752, Training Accuracy: 51.032\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.801658\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.859041305, Training Accuracy: 29.296\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.889473\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.973609160, Training Accuracy: 26.376\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.818334\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.128057358, Training Accuracy: 24.008\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.836836\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001830\n",
            "Local Step 43: Test Loss: 2.786204605, Test Accuracy: 34.790\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.706587608, Training Accuracy: 52.112\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.169926\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.853326407, Training Accuracy: 29.640\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.713178\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.966806580, Training Accuracy: 26.968\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.548836\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.139759701, Training Accuracy: 24.136\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.756116\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002103\n",
            "Local Step 44: Test Loss: 2.709553448, Test Accuracy: 35.790\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.664581802, Training Accuracy: 53.072\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.957723\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.836334955, Training Accuracy: 30.056\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.825025\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.953407611, Training Accuracy: 27.016\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.798379\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.122247936, Training Accuracy: 24.104\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.840778\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002294\n",
            "Local Step 45: Test Loss: 2.797840812, Test Accuracy: 34.390\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.632919167, Training Accuracy: 53.312\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.784867\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.876810032, Training Accuracy: 28.656\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.143167\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.961108934, Training Accuracy: 27.216\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.387296\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.125108081, Training Accuracy: 23.880\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.353189\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002042\n",
            "Local Step 46: Test Loss: 2.772296650, Test Accuracy: 35.220\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.609924616, Training Accuracy: 53.952\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.665255\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.868741125, Training Accuracy: 29.312\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.621734\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.953386394, Training Accuracy: 27.368\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.497513\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.159478101, Training Accuracy: 23.984\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.028495\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001964\n",
            "Local Step 47: Test Loss: 2.807318271, Test Accuracy: 34.380\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.582559226, Training Accuracy: 54.920\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.830679\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.879614356, Training Accuracy: 29.440\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.892879\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.968132687, Training Accuracy: 27.776\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.806417\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.136248949, Training Accuracy: 24.224\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.707053\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002149\n",
            "Local Step 48: Test Loss: 2.934555552, Test Accuracy: 33.860\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.556183490, Training Accuracy: 55.752\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.869880\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.875041423, Training Accuracy: 29.760\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.346877\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.983833385, Training Accuracy: 27.488\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.115167\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.144998200, Training Accuracy: 24.240\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.260065\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002205\n",
            "Local Step 49: Test Loss: 2.900073225, Test Accuracy: 33.260\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.499432767, Training Accuracy: 57.016\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.638921\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.851882958, Training Accuracy: 29.576\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.237090\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.988416909, Training Accuracy: 27.368\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.700050\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.158472640, Training Accuracy: 24.032\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.743442\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002416\n",
            "Local Step 50: Test Loss: 2.816829769, Test Accuracy: 34.890\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.505778898, Training Accuracy: 57.136\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.634905\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.854942536, Training Accuracy: 29.912\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.230632\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.030467424, Training Accuracy: 26.240\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.055423\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.162850409, Training Accuracy: 23.584\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.913281\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002150\n",
            "Local Step 51: Test Loss: 2.954345907, Test Accuracy: 34.740\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.482771207, Training Accuracy: 57.848\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.279109\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.869648922, Training Accuracy: 29.808\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.283187\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.962984263, Training Accuracy: 27.392\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.227614\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.145615562, Training Accuracy: 24.328\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.702562\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002434\n",
            "Local Step 52: Test Loss: 2.844545487, Test Accuracy: 35.900\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.436368713, Training Accuracy: 59.040\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.946651\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.859674369, Training Accuracy: 29.504\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.830588\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.985912323, Training Accuracy: 27.544\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.759553\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.167289190, Training Accuracy: 23.864\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.045432\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001816\n",
            "Local Step 53: Test Loss: 2.993644584, Test Accuracy: 34.380\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.411316332, Training Accuracy: 59.272\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.722829\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.875465347, Training Accuracy: 29.656\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.873926\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.995769663, Training Accuracy: 27.008\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.860914\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.165079333, Training Accuracy: 23.680\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.724402\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001832\n",
            "Local Step 54: Test Loss: 2.925689087, Test Accuracy: 35.220\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.394399050, Training Accuracy: 59.176\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.239831\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.898791514, Training Accuracy: 29.080\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.290980\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.995445534, Training Accuracy: 27.000\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.473122\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.168238550, Training Accuracy: 23.760\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.313447\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002256\n",
            "Local Step 55: Test Loss: 2.973705923, Test Accuracy: 34.430\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.350968471, Training Accuracy: 60.848\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.002975\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.905021396, Training Accuracy: 28.608\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.172248\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.009964012, Training Accuracy: 26.680\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.178715\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.193933343, Training Accuracy: 23.536\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.795163\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001964\n",
            "Local Step 56: Test Loss: 2.974895108, Test Accuracy: 35.220\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.334500467, Training Accuracy: 61.256\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.711979\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.886720781, Training Accuracy: 29.848\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.557750\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 2.988406000, Training Accuracy: 27.120\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.135312\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.184431116, Training Accuracy: 23.496\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.210951\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002085\n",
            "Local Step 57: Test Loss: 3.031885770, Test Accuracy: 35.210\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.319827509, Training Accuracy: 61.432\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.732297\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.912899893, Training Accuracy: 29.568\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.811501\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.040720656, Training Accuracy: 26.424\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.740437\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.193504297, Training Accuracy: 24.032\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.171578\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002075\n",
            "Local Step 58: Test Loss: 3.085131969, Test Accuracy: 35.030\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.290910520, Training Accuracy: 62.072\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.351697\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.923770930, Training Accuracy: 29.008\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.103607\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.016634193, Training Accuracy: 26.784\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.116162\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.222382766, Training Accuracy: 22.792\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.938708\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001927\n",
            "Local Step 59: Test Loss: 3.020801432, Test Accuracy: 35.530\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.283351886, Training Accuracy: 62.416\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.995576\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.928606328, Training Accuracy: 29.024\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.667235\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.020218816, Training Accuracy: 27.064\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.625265\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.217150098, Training Accuracy: 23.328\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.256092\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002341\n",
            "Local Step 60: Test Loss: 3.169629669, Test Accuracy: 34.300\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.245740452, Training Accuracy: 63.480\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.833021\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.915949847, Training Accuracy: 28.760\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.018058\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.054083345, Training Accuracy: 26.312\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.097953\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.192847763, Training Accuracy: 23.656\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.934159\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002012\n",
            "Local Step 61: Test Loss: 3.150544949, Test Accuracy: 35.090\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.224754361, Training Accuracy: 63.896\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.706594\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.927833658, Training Accuracy: 29.152\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.081187\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.039272502, Training Accuracy: 26.976\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.218500\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.214986858, Training Accuracy: 23.712\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.230540\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002083\n",
            "Local Step 62: Test Loss: 3.104408049, Test Accuracy: 34.460\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.212253506, Training Accuracy: 64.256\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.540910\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.926360517, Training Accuracy: 28.936\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.755092\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.016089675, Training Accuracy: 27.456\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.053195\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.204015367, Training Accuracy: 23.336\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.956096\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001858\n",
            "Local Step 63: Test Loss: 3.106025918, Test Accuracy: 35.900\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.211456074, Training Accuracy: 64.208\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.217798\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.909629789, Training Accuracy: 29.104\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.250304\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.025099481, Training Accuracy: 27.104\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.452445\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.218203093, Training Accuracy: 23.488\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.222030\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002060\n",
            "Local Step 64: Test Loss: 3.132010013, Test Accuracy: 35.220\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.168787723, Training Accuracy: 65.448\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.418947\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.955276441, Training Accuracy: 29.136\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.179815\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.038541722, Training Accuracy: 26.752\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.062506\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.244852534, Training Accuracy: 22.792\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.929278\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001955\n",
            "Local Step 65: Test Loss: 3.143817504, Test Accuracy: 34.700\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.158158959, Training Accuracy: 65.704\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.290153\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.920662127, Training Accuracy: 29.352\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.002401\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.034673593, Training Accuracy: 27.072\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.267088\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.192598732, Training Accuracy: 24.016\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.788501\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001879\n",
            "Local Step 66: Test Loss: 3.235335933, Test Accuracy: 34.940\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.158921676, Training Accuracy: 66.144\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.491059\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.949400757, Training Accuracy: 29.264\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.272423\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.040113431, Training Accuracy: 26.904\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.210794\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.214702792, Training Accuracy: 24.072\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.845407\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001851\n",
            "Local Step 67: Test Loss: 3.227079293, Test Accuracy: 34.900\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.109146269, Training Accuracy: 66.816\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.722215\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.948052434, Training Accuracy: 28.984\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.153477\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.030163201, Training Accuracy: 27.176\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.491802\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.224734636, Training Accuracy: 23.280\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.949953\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002048\n",
            "Local Step 68: Test Loss: 3.302957186, Test Accuracy: 34.120\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.093017068, Training Accuracy: 67.600\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.945805\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.954411756, Training Accuracy: 28.648\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.861705\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.072239169, Training Accuracy: 26.184\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.247038\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.250924755, Training Accuracy: 22.944\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.069900\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002368\n",
            "Local Step 69: Test Loss: 3.252037952, Test Accuracy: 34.490\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.060712600, Training Accuracy: 68.752\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.254054\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.935574986, Training Accuracy: 28.792\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.281743\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.046233098, Training Accuracy: 26.744\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.587502\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.230427547, Training Accuracy: 23.528\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.214294\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002182\n",
            "Local Step 70: Test Loss: 3.304698807, Test Accuracy: 33.990\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.048844816, Training Accuracy: 68.696\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.007910\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.984754477, Training Accuracy: 28.336\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.857490\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.051530575, Training Accuracy: 27.136\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.855674\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.255733670, Training Accuracy: 22.704\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.496313\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001685\n",
            "Local Step 71: Test Loss: 3.305226513, Test Accuracy: 34.860\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.066013238, Training Accuracy: 68.392\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.799054\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.957869904, Training Accuracy: 29.248\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.737389\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.047313761, Training Accuracy: 26.960\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.689597\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.234291749, Training Accuracy: 23.464\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.461520\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002224\n",
            "Local Step 72: Test Loss: 3.288673686, Test Accuracy: 35.030\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.033733511, Training Accuracy: 68.944\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.805915\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.952173481, Training Accuracy: 28.920\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.456000\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.097152839, Training Accuracy: 26.192\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.834897\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.252233415, Training Accuracy: 23.152\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.098719\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002239\n",
            "Local Step 73: Test Loss: 3.396405855, Test Accuracy: 34.870\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.024970424, Training Accuracy: 69.440\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.808692\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.959937329, Training Accuracy: 28.824\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.837704\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.066283514, Training Accuracy: 26.424\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.793242\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.273278174, Training Accuracy: 23.112\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.470660\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002241\n",
            "Local Step 74: Test Loss: 3.275639649, Test Accuracy: 34.900\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 1.003675656, Training Accuracy: 69.888\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.771533\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.963749247, Training Accuracy: 29.056\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.797409\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.061598050, Training Accuracy: 26.920\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.853060\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.246388061, Training Accuracy: 23.192\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.168367\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002988\n",
            "Local Step 75: Test Loss: 3.431638614, Test Accuracy: 34.080\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.989177615, Training Accuracy: 70.776\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.892636\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.986953216, Training Accuracy: 28.760\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.832544\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.089922448, Training Accuracy: 26.288\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.252396\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.296065498, Training Accuracy: 22.192\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.345192\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.003347\n",
            "Local Step 76: Test Loss: 3.407350536, Test Accuracy: 34.960\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.964833142, Training Accuracy: 71.264\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.170023\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.981175457, Training Accuracy: 28.664\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.821911\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.122105809, Training Accuracy: 25.784\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.779613\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.287062206, Training Accuracy: 22.416\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.306993\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002185\n",
            "Local Step 77: Test Loss: 3.428389839, Test Accuracy: 33.880\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.953946397, Training Accuracy: 71.392\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.844700\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.012099363, Training Accuracy: 28.128\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.852931\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.082514290, Training Accuracy: 26.528\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.771193\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.258994776, Training Accuracy: 23.240\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.795821\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001801\n",
            "Local Step 78: Test Loss: 3.488400455, Test Accuracy: 34.690\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.903589438, Training Accuracy: 72.688\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.396345\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.019414261, Training Accuracy: 28.264\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.859479\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.106423558, Training Accuracy: 25.896\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.904509\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.313438119, Training Accuracy: 22.128\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.832905\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002106\n",
            "Local Step 79: Test Loss: 3.531683750, Test Accuracy: 34.330\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.921664004, Training Accuracy: 72.152\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.840355\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.043540439, Training Accuracy: 27.648\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.506920\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.116111242, Training Accuracy: 26.160\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.991295\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.266108244, Training Accuracy: 23.016\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.898842\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002007\n",
            "Local Step 80: Test Loss: 3.577532527, Test Accuracy: 34.560\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.938467335, Training Accuracy: 71.976\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.808372\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 2.998893604, Training Accuracy: 29.136\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.747089\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.143246084, Training Accuracy: 26.064\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.186447\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.254351469, Training Accuracy: 23.792\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.040073\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002020\n",
            "Local Step 81: Test Loss: 3.486436235, Test Accuracy: 33.870\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.927951295, Training Accuracy: 72.512\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.113336\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.021247679, Training Accuracy: 28.008\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.242740\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.084253067, Training Accuracy: 26.448\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.743067\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.285073165, Training Accuracy: 23.080\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.614003\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002464\n",
            "Local Step 82: Test Loss: 3.533830910, Test Accuracy: 34.250\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.888089468, Training Accuracy: 73.120\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.984798\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.005052785, Training Accuracy: 28.864\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.832339\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.125366853, Training Accuracy: 26.008\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.734001\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.295125357, Training Accuracy: 23.040\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.925728\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002050\n",
            "Local Step 83: Test Loss: 3.487647134, Test Accuracy: 34.610\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.884712845, Training Accuracy: 73.272\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.963836\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.007784511, Training Accuracy: 28.568\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.802893\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.104829146, Training Accuracy: 26.112\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.743100\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.284059036, Training Accuracy: 22.832\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.954376\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001892\n",
            "Local Step 84: Test Loss: 3.619002049, Test Accuracy: 33.760\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.867699741, Training Accuracy: 73.408\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.736348\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.043869491, Training Accuracy: 27.472\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.720685\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.116189103, Training Accuracy: 26.040\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.018465\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.295030088, Training Accuracy: 22.664\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.475905\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002105\n",
            "Local Step 85: Test Loss: 3.605731817, Test Accuracy: 34.420\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.877826397, Training Accuracy: 73.448\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.604466\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.027852568, Training Accuracy: 27.928\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.008766\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.120224870, Training Accuracy: 26.112\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.791646\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.309648363, Training Accuracy: 22.632\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.028498\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002098\n",
            "Local Step 86: Test Loss: 3.609938579, Test Accuracy: 34.910\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.814908479, Training Accuracy: 75.168\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.807458\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.013229212, Training Accuracy: 28.888\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.701100\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.115665332, Training Accuracy: 26.624\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.727454\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.346724244, Training Accuracy: 22.088\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.851965\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001820\n",
            "Local Step 87: Test Loss: 3.642745073, Test Accuracy: 34.700\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.835345514, Training Accuracy: 75.160\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.902103\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.045307836, Training Accuracy: 27.944\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.909256\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.149070198, Training Accuracy: 25.784\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.817143\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.334523471, Training Accuracy: 21.896\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.835511\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002001\n",
            "Local Step 88: Test Loss: 3.712562546, Test Accuracy: 34.110\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.843866526, Training Accuracy: 73.848\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.600111\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.049083571, Training Accuracy: 27.960\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.763499\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.143084833, Training Accuracy: 25.512\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.743004\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.310122910, Training Accuracy: 22.392\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.765717\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001913\n",
            "Local Step 89: Test Loss: 3.693081400, Test Accuracy: 34.820\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.846632425, Training Accuracy: 74.296\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.812080\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.058080841, Training Accuracy: 27.400\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.351271\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.154050135, Training Accuracy: 25.544\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.919328\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.411890313, Training Accuracy: 20.752\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.329732\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002260\n",
            "Local Step 90: Test Loss: 3.635013438, Test Accuracy: 35.570\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.807061116, Training Accuracy: 75.424\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.903537\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.025012258, Training Accuracy: 28.184\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.803043\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.170870242, Training Accuracy: 25.816\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.706494\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.305669797, Training Accuracy: 23.152\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.706828\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002303\n",
            "Local Step 91: Test Loss: 3.675678548, Test Accuracy: 35.380\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.793946309, Training Accuracy: 75.968\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.739915\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.054717432, Training Accuracy: 27.896\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.589852\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.147428319, Training Accuracy: 26.384\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.586722\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.359183158, Training Accuracy: 22.328\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.564868\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002158\n",
            "Local Step 92: Test Loss: 3.668232016, Test Accuracy: 34.540\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.781920780, Training Accuracy: 76.640\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.852706\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.025148597, Training Accuracy: 28.464\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.253462\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.128764459, Training Accuracy: 26.336\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.482588\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.309943839, Training Accuracy: 23.216\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.237737\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002062\n",
            "Local Step 93: Test Loss: 3.742862237, Test Accuracy: 34.950\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.783010560, Training Accuracy: 76.432\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.871699\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.030872020, Training Accuracy: 28.112\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.765543\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.162128656, Training Accuracy: 25.984\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.118123\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.343964057, Training Accuracy: 21.992\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.151394\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002034\n",
            "Local Step 94: Test Loss: 3.739582583, Test Accuracy: 34.460\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.764328287, Training Accuracy: 76.800\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.629623\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.052076658, Training Accuracy: 28.152\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.580474\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.151176874, Training Accuracy: 25.992\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.493804\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.363785292, Training Accuracy: 21.536\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.883918\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002916\n",
            "Local Step 95: Test Loss: 3.808362950, Test Accuracy: 34.670\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.755784762, Training Accuracy: 76.976\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.873892\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.063358864, Training Accuracy: 28.144\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.862917\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.189243694, Training Accuracy: 24.976\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.154195\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.340801133, Training Accuracy: 22.536\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.823483\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001800\n",
            "Local Step 96: Test Loss: 3.797866803, Test Accuracy: 34.630\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.761545281, Training Accuracy: 76.648\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.872630\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.064409258, Training Accuracy: 27.816\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.652503\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.182236982, Training Accuracy: 25.344\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.677616\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.321352113, Training Accuracy: 22.536\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.779505\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002076\n",
            "Local Step 97: Test Loss: 3.824453620, Test Accuracy: 34.210\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.745252381, Training Accuracy: 77.104\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.653098\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.097762382, Training Accuracy: 27.392\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.359770\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.176189733, Training Accuracy: 25.016\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.764410\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.348702275, Training Accuracy: 22.152\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.926594\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001939\n",
            "Local Step 98: Test Loss: 3.832705530, Test Accuracy: 34.410\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.763254063, Training Accuracy: 76.432\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.820860\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.062908461, Training Accuracy: 28.048\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.746538\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.176070437, Training Accuracy: 25.208\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.840396\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.391610852, Training Accuracy: 21.096\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.893853\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001820\n",
            "Local Step 99: Test Loss: 3.805819558, Test Accuracy: 34.980\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.738973253, Training Accuracy: 77.440\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.950907\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.100178419, Training Accuracy: 27.184\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.499497\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.163658070, Training Accuracy: 25.040\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.767897\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.361101182, Training Accuracy: 22.024\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.834231\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002128\n",
            "Local Step 100: Test Loss: 3.826038906, Test Accuracy: 35.620\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.724482744, Training Accuracy: 77.992\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.908457\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.056467182, Training Accuracy: 27.912\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.767417\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.195747590, Training Accuracy: 25.584\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.790470\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.386137329, Training Accuracy: 21.416\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.721647\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001927\n",
            "Local Step 101: Test Loss: 3.851360253, Test Accuracy: 34.590\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.714190707, Training Accuracy: 78.256\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.815195\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.079598277, Training Accuracy: 28.208\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.701936\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.162370341, Training Accuracy: 26.008\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.605959\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.375863618, Training Accuracy: 21.520\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.650073\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002232\n",
            "Local Step 102: Test Loss: 3.796686906, Test Accuracy: 34.920\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.711128953, Training Accuracy: 78.040\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.568296\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.086935242, Training Accuracy: 27.616\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.833477\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.173994367, Training Accuracy: 25.616\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.331552\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.405673608, Training Accuracy: 21.264\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.908936\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002440\n",
            "Local Step 103: Test Loss: 3.852452866, Test Accuracy: 34.860\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.718932035, Training Accuracy: 77.768\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.318768\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.093724135, Training Accuracy: 27.336\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.107747\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.189388163, Training Accuracy: 25.096\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.707052\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.406510076, Training Accuracy: 20.880\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.678021\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001820\n",
            "Local Step 104: Test Loss: 3.967234053, Test Accuracy: 34.580\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.700536809, Training Accuracy: 78.480\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.875559\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.105810685, Training Accuracy: 27.264\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.279251\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.213025226, Training Accuracy: 24.984\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.913676\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.416585117, Training Accuracy: 20.592\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.077080\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002042\n",
            "Local Step 105: Test Loss: 3.816607744, Test Accuracy: 35.080\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.673457172, Training Accuracy: 79.424\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.899227\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.095524723, Training Accuracy: 27.256\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.027222\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.169345407, Training Accuracy: 25.312\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.730360\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.393027543, Training Accuracy: 21.184\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.414056\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002282\n",
            "Local Step 106: Test Loss: 3.874275867, Test Accuracy: 34.480\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.697477746, Training Accuracy: 78.864\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.947419\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.097768336, Training Accuracy: 27.240\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.801297\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.189417968, Training Accuracy: 25.496\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.754296\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.397769377, Training Accuracy: 21.472\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.550178\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002246\n",
            "Local Step 107: Test Loss: 3.819933648, Test Accuracy: 35.520\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.679723129, Training Accuracy: 79.120\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.494543\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.088293757, Training Accuracy: 27.360\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.866930\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.231130528, Training Accuracy: 24.664\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.244298\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.356208065, Training Accuracy: 22.392\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.510598\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002114\n",
            "Local Step 108: Test Loss: 3.941793824, Test Accuracy: 34.630\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.679107186, Training Accuracy: 79.080\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.782873\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.092821583, Training Accuracy: 28.136\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.720199\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.191707140, Training Accuracy: 25.456\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.931083\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.388467028, Training Accuracy: 22.088\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.726736\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002333\n",
            "Local Step 109: Test Loss: 3.791930750, Test Accuracy: 35.440\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.663017109, Training Accuracy: 79.936\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.759220\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.096707658, Training Accuracy: 27.504\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.865770\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.235993451, Training Accuracy: 24.480\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.056993\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.364882546, Training Accuracy: 21.952\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.944790\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002277\n",
            "Local Step 110: Test Loss: 3.898574882, Test Accuracy: 35.120\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.646668298, Training Accuracy: 79.824\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.851715\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.072187801, Training Accuracy: 28.032\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.885496\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.203370037, Training Accuracy: 24.768\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.227190\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.417098196, Training Accuracy: 21.384\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.773019\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002854\n",
            "Local Step 111: Test Loss: 3.909602382, Test Accuracy: 35.450\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.643420710, Training Accuracy: 79.680\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.717615\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.091921708, Training Accuracy: 27.440\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.082034\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.204386434, Training Accuracy: 24.976\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.880313\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.415707797, Training Accuracy: 21.064\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.786787\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001978\n",
            "Local Step 112: Test Loss: 3.918425428, Test Accuracy: 35.170\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.632650385, Training Accuracy: 80.784\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.954837\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.095593032, Training Accuracy: 27.816\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.137926\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.239761790, Training Accuracy: 24.496\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.722907\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.414197649, Training Accuracy: 21.232\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.942246\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002279\n",
            "Local Step 113: Test Loss: 3.886858495, Test Accuracy: 35.030\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.653608665, Training Accuracy: 79.840\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.905167\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.120450627, Training Accuracy: 27.160\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.771829\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.224421838, Training Accuracy: 24.864\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.832897\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.400142997, Training Accuracy: 21.480\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.926447\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001996\n",
            "Local Step 114: Test Loss: 3.934096013, Test Accuracy: 35.030\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.620470662, Training Accuracy: 80.896\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.091237\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.109736505, Training Accuracy: 27.944\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.907666\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.221021220, Training Accuracy: 24.896\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.849007\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.433227091, Training Accuracy: 21.328\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.284225\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001981\n",
            "Local Step 115: Test Loss: 3.962014838, Test Accuracy: 35.200\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.636957756, Training Accuracy: 80.208\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.980111\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.129959921, Training Accuracy: 27.744\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.841153\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.220599487, Training Accuracy: 24.944\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.695509\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.418747323, Training Accuracy: 21.240\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.725979\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001997\n",
            "Local Step 116: Test Loss: 3.991538760, Test Accuracy: 34.860\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.622777121, Training Accuracy: 81.024\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.925785\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.128727747, Training Accuracy: 27.040\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.012328\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.205824764, Training Accuracy: 25.040\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.092545\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.457556343, Training Accuracy: 20.320\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.883538\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001999\n",
            "Local Step 117: Test Loss: 3.973757881, Test Accuracy: 34.690\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.601626046, Training Accuracy: 81.400\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.887556\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.099111729, Training Accuracy: 27.896\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.846003\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.235465540, Training Accuracy: 24.488\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.393890\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.400643189, Training Accuracy: 22.096\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.150114\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002011\n",
            "Local Step 118: Test Loss: 3.916149190, Test Accuracy: 35.210\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.603437925, Training Accuracy: 81.584\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.578214\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.084693113, Training Accuracy: 28.000\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.910580\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.200938602, Training Accuracy: 25.248\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.737898\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.409934433, Training Accuracy: 21.080\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.752728\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002986\n",
            "Local Step 119: Test Loss: 3.961407285, Test Accuracy: 35.070\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.615445398, Training Accuracy: 81.832\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.081641\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.103206021, Training Accuracy: 27.520\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.951707\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.242399213, Training Accuracy: 24.624\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.710134\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.406753309, Training Accuracy: 21.304\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.787651\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001947\n",
            "Local Step 120: Test Loss: 4.011067501, Test Accuracy: 35.090\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.594105127, Training Accuracy: 81.232\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.240105\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.138706041, Training Accuracy: 27.608\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.187834\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.207735104, Training Accuracy: 25.280\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.664225\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.483536940, Training Accuracy: 20.032\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:07.092804\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002167\n",
            "Local Step 121: Test Loss: 4.027843202, Test Accuracy: 34.620\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.603318672, Training Accuracy: 81.064\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.258771\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.127653645, Training Accuracy: 27.904\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.869390\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.216757447, Training Accuracy: 24.960\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.765758\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.463799370, Training Accuracy: 20.736\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.156422\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002269\n",
            "Local Step 122: Test Loss: 3.971212080, Test Accuracy: 35.740\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.626109255, Training Accuracy: 80.744\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.816168\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.116715381, Training Accuracy: 27.504\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.994308\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.219594588, Training Accuracy: 24.976\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.665532\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.473005539, Training Accuracy: 20.968\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.826427\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001918\n",
            "Local Step 123: Test Loss: 3.955220780, Test Accuracy: 35.070\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.581917835, Training Accuracy: 81.552\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.822833\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.141687135, Training Accuracy: 27.312\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.845977\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.262149278, Training Accuracy: 24.136\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.778914\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.394364272, Training Accuracy: 21.856\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.740730\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002019\n",
            "Local Step 124: Test Loss: 4.031928069, Test Accuracy: 35.210\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.571997759, Training Accuracy: 82.576\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:07.192799\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.137203804, Training Accuracy: 27.904\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:07.074416\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.279382512, Training Accuracy: 24.200\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.563925\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.475868403, Training Accuracy: 20.344\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.944449\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002639\n",
            "Local Step 125: Test Loss: 4.025106615, Test Accuracy: 34.500\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.601875591, Training Accuracy: 81.056\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.736920\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.100056581, Training Accuracy: 27.984\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.710692\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.271145322, Training Accuracy: 23.696\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.708266\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.452295708, Training Accuracy: 20.528\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.784869\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002541\n",
            "Local Step 126: Test Loss: 4.038464250, Test Accuracy: 34.670\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.574627217, Training Accuracy: 82.352\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.967039\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.122712329, Training Accuracy: 27.640\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.381840\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.231430370, Training Accuracy: 24.984\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.978015\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.436497548, Training Accuracy: 21.496\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.776534\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002272\n",
            "Local Step 127: Test Loss: 4.052840705, Test Accuracy: 35.680\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.557674807, Training Accuracy: 82.784\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.866577\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.130912121, Training Accuracy: 27.448\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.740602\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.224332590, Training Accuracy: 25.360\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.689209\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.435834369, Training Accuracy: 21.352\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.761036\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002427\n",
            "Local Step 128: Test Loss: 4.045810719, Test Accuracy: 35.190\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.571807357, Training Accuracy: 82.256\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.987348\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.114600784, Training Accuracy: 27.640\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.947740\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.253132969, Training Accuracy: 24.256\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.417330\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.459183770, Training Accuracy: 20.656\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.874512\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002090\n",
            "Local Step 129: Test Loss: 4.037635448, Test Accuracy: 34.770\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.580295697, Training Accuracy: 82.192\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.059141\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.151358756, Training Accuracy: 26.784\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.802377\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.262040000, Training Accuracy: 24.672\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.786559\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.451095605, Training Accuracy: 21.640\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.711455\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001841\n",
            "Local Step 130: Test Loss: 4.095170897, Test Accuracy: 34.990\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.569850991, Training Accuracy: 82.144\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.979508\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.153405377, Training Accuracy: 26.216\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.871132\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.231344845, Training Accuracy: 24.976\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.760592\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.398289994, Training Accuracy: 21.720\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.165120\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001796\n",
            "Local Step 131: Test Loss: 4.057124463, Test Accuracy: 34.660\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.562608646, Training Accuracy: 82.688\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.540552\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.108052186, Training Accuracy: 27.712\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.694057\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.235602211, Training Accuracy: 24.832\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.991184\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.494442626, Training Accuracy: 19.728\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.820042\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001921\n",
            "Local Step 132: Test Loss: 4.121145417, Test Accuracy: 35.060\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.540748373, Training Accuracy: 83.392\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.840517\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.107858018, Training Accuracy: 28.104\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.783297\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.231318054, Training Accuracy: 24.856\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.729627\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.433439071, Training Accuracy: 20.944\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.517535\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002223\n",
            "Local Step 133: Test Loss: 4.089206758, Test Accuracy: 35.950\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.550301920, Training Accuracy: 83.280\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.693881\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.171515111, Training Accuracy: 26.512\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.862716\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.275369928, Training Accuracy: 24.312\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.549139\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.457905938, Training Accuracy: 20.520\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.613571\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002230\n",
            "Local Step 134: Test Loss: 4.117685251, Test Accuracy: 34.470\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.538814136, Training Accuracy: 83.416\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.055560\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.144566478, Training Accuracy: 27.176\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.863587\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.271557762, Training Accuracy: 24.328\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.796041\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.522237336, Training Accuracy: 19.608\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.969153\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002163\n",
            "Local Step 135: Test Loss: 4.156150914, Test Accuracy: 35.110\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.570957066, Training Accuracy: 82.192\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.780810\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.150835304, Training Accuracy: 27.016\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.806683\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.263301284, Training Accuracy: 24.408\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.011285\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.478116908, Training Accuracy: 20.304\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.848543\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002066\n",
            "Local Step 136: Test Loss: 4.007879192, Test Accuracy: 35.160\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.533382024, Training Accuracy: 83.272\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.956932\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.168392807, Training Accuracy: 26.160\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.729863\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.229562876, Training Accuracy: 25.128\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.465212\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.477871946, Training Accuracy: 20.640\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.661801\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002253\n",
            "Local Step 137: Test Loss: 4.101416734, Test Accuracy: 35.360\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.531599388, Training Accuracy: 83.488\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.854951\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.149537835, Training Accuracy: 26.936\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.627030\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.298449828, Training Accuracy: 23.720\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.450535\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.488030379, Training Accuracy: 20.392\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.561350\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001737\n",
            "Local Step 138: Test Loss: 4.122192111, Test Accuracy: 34.900\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.525220698, Training Accuracy: 83.632\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.388041\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.134186454, Training Accuracy: 27.392\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.844606\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.274776077, Training Accuracy: 24.520\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.825614\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.466232199, Training Accuracy: 20.432\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.586960\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002410\n",
            "Local Step 139: Test Loss: 3.984357433, Test Accuracy: 35.700\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.525990700, Training Accuracy: 83.984\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.836508\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.147855074, Training Accuracy: 27.592\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.756302\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.274161384, Training Accuracy: 24.360\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.763463\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.419384466, Training Accuracy: 21.344\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.077145\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002024\n",
            "Local Step 140: Test Loss: 4.194710353, Test Accuracy: 34.210\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.549836653, Training Accuracy: 82.896\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.787842\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.130376999, Training Accuracy: 27.216\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.655797\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.284060073, Training Accuracy: 23.880\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.670249\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.505530576, Training Accuracy: 19.976\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.834923\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001939\n",
            "Local Step 141: Test Loss: 4.129570092, Test Accuracy: 36.150\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.537321724, Training Accuracy: 83.712\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.731194\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.148316292, Training Accuracy: 27.392\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.002191\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.287429238, Training Accuracy: 24.088\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.965789\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.395448047, Training Accuracy: 21.784\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.088625\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002021\n",
            "Local Step 142: Test Loss: 4.135377977, Test Accuracy: 35.230\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.501744480, Training Accuracy: 84.576\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.298185\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.156803117, Training Accuracy: 27.056\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.720570\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.279290642, Training Accuracy: 23.888\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.680741\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.436753516, Training Accuracy: 21.288\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.087862\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001878\n",
            "Local Step 143: Test Loss: 4.163011631, Test Accuracy: 34.740\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.475006633, Training Accuracy: 85.952\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.358614\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.140830243, Training Accuracy: 27.408\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.706093\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.283844443, Training Accuracy: 23.920\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.666721\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.465769492, Training Accuracy: 20.640\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.715464\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002147\n",
            "Local Step 144: Test Loss: 4.112195112, Test Accuracy: 35.100\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.523015603, Training Accuracy: 84.000\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.823751\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.176950878, Training Accuracy: 26.056\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:05.698837\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.247116114, Training Accuracy: 24.976\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.662587\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.418243922, Training Accuracy: 21.584\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.694821\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001910\n",
            "Local Step 145: Test Loss: 4.050939718, Test Accuracy: 35.460\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.508805204, Training Accuracy: 84.248\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.850311\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.135844385, Training Accuracy: 27.304\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.008533\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.270986297, Training Accuracy: 24.520\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.776858\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.462440408, Training Accuracy: 20.544\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.737235\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001885\n",
            "Local Step 146: Test Loss: 4.145346535, Test Accuracy: 35.260\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.492440541, Training Accuracy: 84.880\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.142640\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.162911427, Training Accuracy: 26.816\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.401328\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.250391475, Training Accuracy: 24.464\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.570150\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.529024127, Training Accuracy: 19.888\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.421674\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002050\n",
            "Local Step 147: Test Loss: 4.090663646, Test Accuracy: 35.670\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.499265238, Training Accuracy: 84.360\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.614962\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.160073509, Training Accuracy: 26.792\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.216709\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.260784991, Training Accuracy: 25.032\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.755357\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.466015057, Training Accuracy: 20.456\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:05.864837\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001868\n",
            "Local Step 148: Test Loss: 4.149304846, Test Accuracy: 35.480\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.485904997, Training Accuracy: 84.976\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:05.912911\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.177121853, Training Accuracy: 26.872\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.400377\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.271740992, Training Accuracy: 23.768\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:05.700325\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.434708932, Training Accuracy: 21.456\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.120619\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001892\n",
            "Local Step 149: Test Loss: 4.199677891, Test Accuracy: 35.890\n",
            "**************************************************\n",
            "Worker 1, [01/01]: Training Loss: 0.490901900, Training Accuracy: 84.928\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:06.609976\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/01]: Training Loss: 3.198731347, Training Accuracy: 26.264\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:06.439840\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/01]: Training Loss: 3.276568875, Training Accuracy: 24.464\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:06.287939\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/01]: Training Loss: 3.478044705, Training Accuracy: 20.344\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:06.164942\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002147\n",
            "Local Step 150: Test Loss: 4.142382291, Test Accuracy: 34.690\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 1:03:40.335871\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:4, Number of Local Steps:4\n",
            "==================================================\n",
            "Worker 1, [01/04]: Training Loss: 4.585535551, Training Accuracy: 1.696\n",
            "Worker 1, [02/04]: Training Loss: 4.304203937, Training Accuracy: 4.880\n",
            "Worker 1, [03/04]: Training Loss: 4.052859945, Training Accuracy: 7.720\n",
            "Worker 1, [04/04]: Training Loss: 3.898440018, Training Accuracy: 10.016\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:23.450686\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.558748063, Training Accuracy: 1.696\n",
            "Worker 2, [02/04]: Training Loss: 4.203795607, Training Accuracy: 5.344\n",
            "Worker 2, [03/04]: Training Loss: 3.990294165, Training Accuracy: 8.072\n",
            "Worker 2, [04/04]: Training Loss: 3.831576716, Training Accuracy: 10.464\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:24.956687\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 4.506574227, Training Accuracy: 2.304\n",
            "Worker 3, [02/04]: Training Loss: 4.120665407, Training Accuracy: 6.120\n",
            "Worker 3, [03/04]: Training Loss: 3.887865194, Training Accuracy: 9.648\n",
            "Worker 3, [04/04]: Training Loss: 3.743055299, Training Accuracy: 11.792\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:24.706496\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 4.432729287, Training Accuracy: 3.296\n",
            "Worker 4, [02/04]: Training Loss: 4.021989595, Training Accuracy: 7.632\n",
            "Worker 4, [03/04]: Training Loss: 3.832680199, Training Accuracy: 10.344\n",
            "Worker 4, [04/04]: Training Loss: 3.684434449, Training Accuracy: 12.632\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:26.002588\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002219\n",
            "Local Step 01: Test Loss: 4.514672410, Test Accuracy: 8.050\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 4.019365234, Training Accuracy: 9.288\n",
            "Worker 1, [02/04]: Training Loss: 3.679698933, Training Accuracy: 13.320\n",
            "Worker 1, [03/04]: Training Loss: 3.540707355, Training Accuracy: 15.672\n",
            "Worker 1, [04/04]: Training Loss: 3.426821661, Training Accuracy: 17.520\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:23.857138\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.980148739, Training Accuracy: 9.632\n",
            "Worker 2, [02/04]: Training Loss: 3.662619337, Training Accuracy: 12.992\n",
            "Worker 2, [03/04]: Training Loss: 3.517311846, Training Accuracy: 15.496\n",
            "Worker 2, [04/04]: Training Loss: 3.396457457, Training Accuracy: 17.808\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:24.781115\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.973857207, Training Accuracy: 8.968\n",
            "Worker 3, [02/04]: Training Loss: 3.655858987, Training Accuracy: 13.624\n",
            "Worker 3, [03/04]: Training Loss: 3.469114678, Training Accuracy: 16.416\n",
            "Worker 3, [04/04]: Training Loss: 3.349035177, Training Accuracy: 18.728\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:23.666587\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.924156898, Training Accuracy: 9.992\n",
            "Worker 4, [02/04]: Training Loss: 3.651106759, Training Accuracy: 13.720\n",
            "Worker 4, [03/04]: Training Loss: 3.492321475, Training Accuracy: 16.064\n",
            "Worker 4, [04/04]: Training Loss: 3.364291625, Training Accuracy: 18.560\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:23.581670\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001688\n",
            "Local Step 02: Test Loss: 4.074842606, Test Accuracy: 18.940\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.501518145, Training Accuracy: 16.808\n",
            "Worker 1, [02/04]: Training Loss: 3.280372320, Training Accuracy: 20.152\n",
            "Worker 1, [03/04]: Training Loss: 3.184795066, Training Accuracy: 21.936\n",
            "Worker 1, [04/04]: Training Loss: 3.078840566, Training Accuracy: 23.808\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:23.191144\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.531250777, Training Accuracy: 15.640\n",
            "Worker 2, [02/04]: Training Loss: 3.320898732, Training Accuracy: 18.648\n",
            "Worker 2, [03/04]: Training Loss: 3.201465172, Training Accuracy: 20.728\n",
            "Worker 2, [04/04]: Training Loss: 3.107062075, Training Accuracy: 22.864\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:25.143239\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.519194064, Training Accuracy: 15.984\n",
            "Worker 3, [02/04]: Training Loss: 3.314147196, Training Accuracy: 19.024\n",
            "Worker 3, [03/04]: Training Loss: 3.190638700, Training Accuracy: 21.528\n",
            "Worker 3, [04/04]: Training Loss: 3.076729087, Training Accuracy: 23.616\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:25.321217\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.609391874, Training Accuracy: 14.640\n",
            "Worker 4, [02/04]: Training Loss: 3.393471077, Training Accuracy: 18.408\n",
            "Worker 4, [03/04]: Training Loss: 3.253692485, Training Accuracy: 20.056\n",
            "Worker 4, [04/04]: Training Loss: 3.136175321, Training Accuracy: 22.440\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:22.858970\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001897\n",
            "Local Step 03: Test Loss: 3.036743384, Test Accuracy: 25.200\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.000852996, Training Accuracy: 25.080\n",
            "Worker 1, [02/04]: Training Loss: 2.926887818, Training Accuracy: 26.328\n",
            "Worker 1, [03/04]: Training Loss: 2.857369638, Training Accuracy: 27.688\n",
            "Worker 1, [04/04]: Training Loss: 2.773570775, Training Accuracy: 29.600\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:23.031707\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.201963606, Training Accuracy: 21.984\n",
            "Worker 2, [02/04]: Training Loss: 3.080622512, Training Accuracy: 23.600\n",
            "Worker 2, [03/04]: Training Loss: 2.963441823, Training Accuracy: 25.504\n",
            "Worker 2, [04/04]: Training Loss: 2.887564163, Training Accuracy: 27.240\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:25.371213\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.236847856, Training Accuracy: 20.968\n",
            "Worker 3, [02/04]: Training Loss: 3.098974961, Training Accuracy: 22.664\n",
            "Worker 3, [03/04]: Training Loss: 2.999917208, Training Accuracy: 24.952\n",
            "Worker 3, [04/04]: Training Loss: 2.912160553, Training Accuracy: 26.520\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:23.682699\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.374863251, Training Accuracy: 18.544\n",
            "Worker 4, [02/04]: Training Loss: 3.232423820, Training Accuracy: 20.640\n",
            "Worker 4, [03/04]: Training Loss: 3.123067328, Training Accuracy: 23.360\n",
            "Worker 4, [04/04]: Training Loss: 2.998792636, Training Accuracy: 24.888\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:24.146573\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002288\n",
            "Local Step 04: Test Loss: 2.829949124, Test Accuracy: 28.930\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.741343329, Training Accuracy: 30.240\n",
            "Worker 1, [02/04]: Training Loss: 2.639527991, Training Accuracy: 32.088\n",
            "Worker 1, [03/04]: Training Loss: 2.590809318, Training Accuracy: 33.400\n",
            "Worker 1, [04/04]: Training Loss: 2.546806579, Training Accuracy: 33.992\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:23.767160\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.040750561, Training Accuracy: 24.824\n",
            "Worker 2, [02/04]: Training Loss: 2.902722443, Training Accuracy: 27.224\n",
            "Worker 2, [03/04]: Training Loss: 2.809482343, Training Accuracy: 28.448\n",
            "Worker 2, [04/04]: Training Loss: 2.733711665, Training Accuracy: 30.056\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:26.107988\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.144396030, Training Accuracy: 23.056\n",
            "Worker 3, [02/04]: Training Loss: 2.984244858, Training Accuracy: 24.992\n",
            "Worker 3, [03/04]: Training Loss: 2.877399920, Training Accuracy: 27.672\n",
            "Worker 3, [04/04]: Training Loss: 2.789163516, Training Accuracy: 29.464\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:25.278430\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.284351826, Training Accuracy: 20.568\n",
            "Worker 4, [02/04]: Training Loss: 3.098736163, Training Accuracy: 23.312\n",
            "Worker 4, [03/04]: Training Loss: 2.985323664, Training Accuracy: 25.288\n",
            "Worker 4, [04/04]: Training Loss: 2.914304862, Training Accuracy: 26.896\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:24.480862\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002969\n",
            "Local Step 05: Test Loss: 2.672721968, Test Accuracy: 32.590\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.480222597, Training Accuracy: 35.072\n",
            "Worker 1, [02/04]: Training Loss: 2.424760381, Training Accuracy: 36.360\n",
            "Worker 1, [03/04]: Training Loss: 2.379612018, Training Accuracy: 36.936\n",
            "Worker 1, [04/04]: Training Loss: 2.357664077, Training Accuracy: 38.168\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:25.300181\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.947105966, Training Accuracy: 26.472\n",
            "Worker 2, [02/04]: Training Loss: 2.791600924, Training Accuracy: 29.344\n",
            "Worker 2, [03/04]: Training Loss: 2.708758483, Training Accuracy: 31.576\n",
            "Worker 2, [04/04]: Training Loss: 2.622037402, Training Accuracy: 32.624\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:26.975851\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 3.046959827, Training Accuracy: 24.448\n",
            "Worker 3, [02/04]: Training Loss: 2.877722019, Training Accuracy: 27.912\n",
            "Worker 3, [03/04]: Training Loss: 2.794663542, Training Accuracy: 29.456\n",
            "Worker 3, [04/04]: Training Loss: 2.689102794, Training Accuracy: 31.168\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:25.952426\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.193729311, Training Accuracy: 22.112\n",
            "Worker 4, [02/04]: Training Loss: 3.030062054, Training Accuracy: 24.976\n",
            "Worker 4, [03/04]: Training Loss: 2.920783655, Training Accuracy: 26.536\n",
            "Worker 4, [04/04]: Training Loss: 2.832685461, Training Accuracy: 28.496\n",
            "lr: 0.016875\n",
            "Time taken for training worker 4: 0:00:24.849044\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.002403\n",
            "Local Step 06: Test Loss: 2.708444931, Test Accuracy: 33.070\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.276682168, Training Accuracy: 39.464\n",
            "Worker 1, [02/04]: Training Loss: 2.232488593, Training Accuracy: 40.128\n",
            "Worker 1, [03/04]: Training Loss: 2.208203299, Training Accuracy: 40.768\n",
            "Worker 1, [04/04]: Training Loss: 2.161799774, Training Accuracy: 41.576\n",
            "lr: 0.005\n",
            "Time taken for training worker 1: 0:00:26.115231\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.870625328, Training Accuracy: 28.584\n",
            "Worker 2, [02/04]: Training Loss: 2.731954426, Training Accuracy: 30.648\n",
            "Worker 2, [03/04]: Training Loss: 2.625628085, Training Accuracy: 32.896\n",
            "Worker 2, [04/04]: Training Loss: 2.559842461, Training Accuracy: 34.080\n",
            "lr: 0.0075\n",
            "Time taken for training worker 2: 0:00:25.149843\n",
            "--------------------------------------------------\n",
            "Worker 3, [01/04]: Training Loss: 2.967612642, Training Accuracy: 26.128\n",
            "Worker 3, [02/04]: Training Loss: 2.810647490, Training Accuracy: 28.728\n",
            "Worker 3, [03/04]: Training Loss: 2.697146234, Training Accuracy: 30.880\n",
            "Worker 3, [04/04]: Training Loss: 2.619303593, Training Accuracy: 32.320\n",
            "lr: 0.01125\n",
            "Time taken for training worker 3: 0:00:26.647886\n",
            "--------------------------------------------------\n",
            "Worker 4, [01/04]: Training Loss: 3.142585302, Training Accuracy: 23.176\n",
            "Worker 4, [02/04]: Training Loss: 2.979288486, Training Accuracy: 25.776\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Workers:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Number of Local Steps:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mlocal_SGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[16], line 31\u001b[0m, in \u001b[0;36mlocal_SGD\u001b[0;34m(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m train_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loca_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(j):\n\u001b[0;32m---> 31\u001b[0m   train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mis_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorker \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworker\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloca_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.9f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_optimizers[worker]\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn, accumulation_steps, device, is_wandb)\u001b[0m\n\u001b[1;32m      5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Initialize gradients to zero at the start of each epoch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     10\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:471\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:664\u001b[0m, in \u001b[0;36mRandomCrop.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m    img (PIL Image or Tensor): Image to be cropped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    PIL Image or Tensor: Cropped image.\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 664\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m width, height \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mget_image_size(img)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# pad the width if needed\u001b[39;00m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:485\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    483\u001b[0m     _log_api_usage_once(pad)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mpad(img, padding\u001b[38;5;241m=\u001b[39mpadding, fill\u001b[38;5;241m=\u001b[39mfill, padding_mode\u001b[38;5;241m=\u001b[39mpadding_mode)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py:201\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# RGB image\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 201\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_top\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_bottom\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_right\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Grayscale image\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/numpy/lib/arraypad.py:748\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`pad_width` must be of integral type.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Broadcast to shape (array.ndim, 2)\u001b[39;00m\n\u001b[0;32m--> 748\u001b[0m pad_width \u001b[38;5;241m=\u001b[39m \u001b[43m_as_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(mode):\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# Old behavior: Use user-supplied function with np.apply_along_axis\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     function \u001b[38;5;241m=\u001b[39m mode\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/numpy/lib/arraypad.py:522\u001b[0m, in \u001b[0;36m_as_pairs\u001b[0;34m(x, ndim, as_index)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain negative values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# Converting the array with `tolist` seems to improve performance\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# when iterating and indexing the result (see usage in `pad`)\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/numpy/lib/stride_tricks.py:413\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[0;34m(array, shape, subok)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_broadcast_to_dispatcher, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbroadcast_to\u001b[39m(array, shape, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    369\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Broadcast an array to a new shape.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m           [1, 2, 3]])\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreadonly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/numpy/lib/stride_tricks.py:349\u001b[0m, in \u001b[0;36m_broadcast_to\u001b[0;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall elements of broadcast shape must be non-\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    347\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    348\u001b[0m extras \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 349\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnditer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulti_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrefs_ok\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzerosize_ok\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mextras\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_flags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreadonly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitershape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m it:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;66;03m# never really has writebackifcopy semantics\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     broadcast \u001b[38;5;241m=\u001b[39m it\u001b[38;5;241m.\u001b[39mitviews[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 5e-03\n",
        "wd = 1e-03\n",
        "K = [4, 8]\n",
        "J = [1, 4, 8, 16, 32, 64] # TODO: 1 ro hazf konim\n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    local_SGD(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Discard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LocalAdaScale (Choice of LR in LocalSGD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def synchronize(models):\n",
        "#   for params in zip(*[model.parameters() for model in models]):\n",
        "#     param_avg = torch.mean(torch.stack([param.data for param in params]), dim=0)\n",
        "#     for param in params:\n",
        "#       param.data = param_avg\n",
        "  \n",
        "#   return models[0]\n",
        "\n",
        "# def synchronize(global_model, local_models):\n",
        "#         # Initialize a state dict with zeros, same shape as the model parameters\n",
        "#         delta = {key: torch.zeros_like(value) for key, value in local_models[0].state_dict().items()}\n",
        "      \n",
        "#         # Sum up all the model parameters\n",
        "#         for local_model in local_models:\n",
        "#             for key, value in local_model.state_dict().items():\n",
        "#                 delta[key] += (global_model.state_dict()[key] - value)\n",
        "      \n",
        "#         # Divide each parameter by the number of models to get the average\n",
        "#         for key in delta:\n",
        "#             delta[key] /= len(local_models)\n",
        "      \n",
        "#         new_weights = {}\n",
        "#         for key, value in global_model.state_dict().items():\n",
        "#             new_weights[key] = value -  delta [key] # TODO: az TA beporsim ke learning rate ro chetor hesab konim.\n",
        "      \n",
        "#         global_model.load_state_dict(new_weights)\n",
        "#         return global_model\n",
        "\n",
        "def average_models(local_models):\n",
        "    \"\"\"Calculate the average model from a list of local models.\"\"\"\n",
        "    num_models = len(local_models)\n",
        "    \n",
        "    # Initialize the averaged model as a copy of the first local model\n",
        "    avg_model = local_models[0]\n",
        "    \n",
        "    # Zero the parameters of the avg_model to start accumulating\n",
        "    for param in avg_model.parameters():\n",
        "        param.data.zero_()\n",
        "    \n",
        "    # Accumulate the parameters from all local models\n",
        "    for local_model in local_models:\n",
        "        for avg_param, local_param in zip(avg_model.parameters(), local_model.parameters()):\n",
        "            avg_param.data.add_(local_param.data)\n",
        "    \n",
        "    # Divide by the number of models to compute the average\n",
        "    for param in avg_model.parameters():\n",
        "        param.data.div_(num_models)\n",
        "    \n",
        "    return avg_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# (data_loader, loss_fn,  num_workers, scale_inv_budget, lr_init, wd,initial_state_dict, num_epochs)\n",
        "# def local_adascale_local_sgd(data_loader, num_epochs, lr_init, num_workers, num_local_steps, scale_inv_budget):\n",
        "#     # Initialize a model and save its initial parameters\n",
        "#     model = LeNet5().to(device='cuda').load_state_dict(initial_state_dict)\n",
        "    \n",
        "#     optimizer = torch.optim.SGD(model.parameters(), lr=lr_init)\n",
        "    \n",
        "#     # Add a learning rate scheduler\n",
        "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "    \n",
        "#     grad_cache = [None] * num_workers\n",
        "#     scale_inv_counter = 0\n",
        "    \n",
        "#     for epoch in range(num_epochs):\n",
        "#         for i, (inputs, targets) in enumerate(data_loader):\n",
        "#             # Simulate local steps on different workers\n",
        "#             for worker_id in range(num_workers):\n",
        "#                 outputs = model(inputs)\n",
        "#                 loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
        "#                 optimizer.zero_grad()\n",
        "#                 loss.backward()\n",
        "                \n",
        "#                 if (i + 1) % num_local_steps == 0:\n",
        "#                     grad_cache[worker_id] = [param.grad.clone() for param in model.parameters()]\n",
        "                    \n",
        "#             # Synchronize and average gradients after num_local_steps\n",
        "#             if (i + 1) % num_local_steps == 0:\n",
        "#                 avg_grad = [torch.mean(torch.stack([grad_cache[worker_id][j] for worker_id in range(num_workers)]), dim=0)\n",
        "#                             for j in range(len(grad_cache[0]))]\n",
        "                \n",
        "#                 # Compute gradient statistics\n",
        "#                 grad_variance = [torch.var(torch.stack([grad_cache[worker_id][j] for worker_id in range(num_workers)]), dim=0)\n",
        "#                                  for j in range(len(grad_cache[0]))]\n",
        "#                 grad_mean = [torch.mean(avg_grad[j]) for j in range(len(avg_grad))]\n",
        "                \n",
        "#                 gain_ratio = compute_gain_ratio(grad_mean, grad_variance, num_workers, num_local_steps)\n",
        "                \n",
        "#                 # Apply scaled gradients\n",
        "#                 for param, grad in zip(model.parameters(), avg_grad):\n",
        "#                     param.grad = gain_ratio * grad\n",
        "                \n",
        "#                 # Scale invariant iteration counter\n",
        "#                 scale_inv_counter += gain_ratio\n",
        "                \n",
        "#                 # Update model parameters\n",
        "#                 optimizer.step()\n",
        "\n",
        "#             # Check if scale invariant budget is exhausted\n",
        "#             if scale_inv_counter >= scale_inv_budget:\n",
        "#                 break\n",
        "        \n",
        "#         # Step the scheduler at the end of each epoch\n",
        "#         scheduler.step()\n",
        "    \n",
        "#     return model\n",
        "\n",
        "# def compute_gain_ratio(grad_mean, grad_variance, num_workers, num_local_steps):\n",
        "#     gain_ratio = []\n",
        "#     for g_mean, g_var in zip(grad_mean, grad_variance):\n",
        "#         g2_mean = g_mean ** 2\n",
        "#         term1 = g2_mean + g_var / num_workers\n",
        "#         term2 = (g2_mean + g_var / num_workers) ** 2\n",
        "#         term3 = 3 * (num_local_steps - 1) * g2_mean * g_var\n",
        "#         gain_ratio.append(2 * term1 / (term1 + torch.sqrt(term2 + term3)))\n",
        "    \n",
        "#     return torch.tensor(gain_ratio).mean().item()\n",
        "\n",
        "# Example usage:\n",
        "# model = MyModel()\n",
        "# data_loader = DataLoader(my_dataset, batch_size=64, shuffle=True)\n",
        "# trained_model = local_adascale_local_sgd(model, data_loader, num_epochs=10, lr_init=0.1, num_workers=4, num_local_steps=5, scale_inv_budget=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "    \n",
        "class CustomSGDLocalAdaScaleOptimizer(Optimizer):\n",
        "    def __init__(self, params, lr=0.01, gain_ratio=1.0, momentum=0.9, weight_decay=0.0):\n",
        "        # Store default settings\n",
        "        defaults = dict(lr=lr, gain_ratio=gain_ratio, momentum=momentum, weight_decay=weight_decay)\n",
        "        super(CustomSGDLocalAdaScaleOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            gain_ratio = group['gain_ratio']\n",
        "            momentum = group['momentum']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for param in group['params']:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                d_p = param.grad.data\n",
        "\n",
        "                # Apply weight decay if specified\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(param.data, alpha=weight_decay)\n",
        "\n",
        "                # If momentum is used\n",
        "                if momentum != 0:\n",
        "                    if 'momentum_buffer' not in self.state[param]:\n",
        "                        buf = self.state[param]['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = self.state[param]['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(d_p)\n",
        "                    d_p = buf\n",
        "\n",
        "                # Custom update rule\n",
        "                try:\n",
        "                    param.data.add_(d_p, alpha=-gain_ratio * lr)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "import math\n",
        "def local_adascale(shard_loaders, loss_fn, K, j, lr, wd, initial_state_dict, scale_invariant_budget):\n",
        "    total_start_time = time.time()\n",
        "    global_model = LeNet5().to(device)\n",
        "    global_model.load_state_dict(initial_state_dict)\n",
        "    local_models = [LeNet5().to(device) for _ in range(K)]\n",
        "    for model in local_models:\n",
        "      model.load_state_dict(initial_state_dict)\n",
        "    # local_optimizers = [CustomOptimizer(model.parameters(), lr=0.01, gain_ratio=1) for model in local_models]\n",
        "    # local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9) for model in local_models]\n",
        "    local_optimizers = [CustomSGDLocalAdaScaleOptimizer(model.parameters(), lr, gain_ratio=1, momentum=0.9, weight_decay= wd) for model in local_models]\n",
        "\n",
        "\n",
        "    num_epochs =150\n",
        "    scale_invariant_budget = 150\n",
        "    # for s in scale_invariant_budget: # Bejaye scale_invariant_budget, num_epochs ro dar nazar gereftim vali goftim age s>scale_invariant_budget shod break kon.\n",
        "    scale_inv_counter = 0\n",
        "    gain_ratio = 1\n",
        "    for epoch in range(num_epochs):\n",
        "        grad_cache = []\n",
        "        for hi, h in enumerate(range(j)):\n",
        "            for k in range(K):\n",
        "                # calculate gradient of all workers g^t_k\n",
        "                train_loss, train_accuracy, gradient =local_update_with_gradient(local_models[k], local_optimizers[k], shard_loaders[k], loss_fn, lr, gain_ratio)\n",
        "                print(f'Worker {k+1}, [{h+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "                if hi == 0: # Only store the gradients for the first local step\n",
        "                    grad_cache.append(gradient)\n",
        "                \n",
        "\n",
        "            # ------------\n",
        "            \n",
        "            # varriance bar va G bar bayad adad bashan va faghat tooye synchronization point (after H steps mohasebe mishan)\n",
        "            # fek konam in mishe g bar_t\n",
        "            # list ba len=10 (ehtemalan tedade layer ha) va dakhele harkodoom 2 ta tensor ke ehtemalen weights and bias\n",
        "            # TA Inja Kar mikone\n",
        "            # TODO: variance bar, G bar, rho, lr tebghe paper dar zamane syncronization update mishan.\n",
        "        \n",
        "        global_model = average_models(local_models)\n",
        "        for local_model in local_models:\n",
        "            local_model.load_state_dict(global_model.state_dict())\n",
        "        # To check if averaging is done correctly (for only two workers)\n",
        "        # with torch.no_grad():\n",
        "        #     for param1, param2, avg_param in zip(local_models[0].parameters(), local_models[1].parameters(), average_model.parameters()):\n",
        "        #         calculated_avg = (param1.data + param2.data) / 2\n",
        "        #         assert torch.allclose(avg_param.data, calculated_avg), \"Averaging failed!\"\n",
        "        \n",
        "        # test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "        # print(f'Local Step {epoch+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "        print(50*'-')\n",
        "        g_bar = calculate_g_bar(grad_cache, K)\n",
        "        variance_bar = calculate_variance_bar(g_bar, grad_cache, K)\n",
        "        G_bar = calculate_G_bar(g_bar=g_bar, num_workers=K, variance_bar=variance_bar)\n",
        "        L = estimate_lipschitz_constant(global_model, shard_loaders[0], loss_fn,device='cuda')\n",
        "        gain_ratio_new = compute_gain_ratio(G_bar,variance_bar, K, j)\n",
        "        lr_new = compute_optimal_learning_rate(G_bar,variance_bar, K, j, L)\n",
        "        if not (lr_new < 0 or math.isnan(lr_new) or gain_ratio_new < 0 or math.isnan(gain_ratio_new)):\n",
        "        # lr = lr_new\n",
        "            gain_ratio = gain_ratio_new\n",
        "        print(f'variance_bar: {variance_bar}, G_bar: {G_bar}, gain_ratio: {gain_ratio}, L: {L}, lr: {lr}')\n",
        "        # Scale invariant iteration counter\n",
        "        scale_inv_counter += gain_ratio\n",
        "        if scale_inv_counter > scale_invariant_budget:\n",
        "            break \n",
        "        print (f'scale_inv_counter: {scale_inv_counter} : scale_invariant_budget: {scale_invariant_budget}')\n",
        "        print (f'gain_ratio: {gain_ratio} lr: {lr}')\n",
        "\n",
        "    \n",
        "    total_end_time = time.time()\n",
        "    print('/'*50)\n",
        "    print(f'Total time taken for LocalAdaScale: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "    print('/'*50)\n",
        "\n",
        "def calculate_g_bar(grad_cache, num_workers):\n",
        "    \"\"\"Calculate ḡ: Element-wise average of all gradients in grad_cache.\"\"\"\n",
        "    gbar = [torch.mean(torch.stack([grad_cache[worker_id][i] for worker_id in range(num_workers)]), dim=0)\n",
        "            for i in range(len(grad_cache[0]))]\n",
        "    \n",
        "    return gbar\n",
        "\n",
        "\n",
        "'''def compute_gain_ratio(grad_mean, grad_variance, num_workers, num_local_steps):\n",
        "    gain_ratio = []\n",
        "    for g_mean, g_var in zip(grad_mean, grad_variance):\n",
        "        g2_mean = g_mean ** 2\n",
        "        term1 = g2_mean + g_var / num_workers\n",
        "        term2 = (g2_mean + g_var / num_workers) ** 2\n",
        "        term3 = 3 * (num_local_steps - 1) * g2_mean * g_var\n",
        "        gain_ratio.append(2 * term1 / (term1 + torch.sqrt(term2 + term3)))\n",
        "    \n",
        "    return torch.tensor(gain_ratio).mean().item()'''\n",
        "\n",
        "def compute_gain_ratio(G_bar, variance_bar, num_workers, num_local_steps):\n",
        "    # Use Ḡ and variancē to compute the gain ratio\n",
        "    term0 = 2 * (G_bar + variance_bar)\n",
        "    term1 = G_bar + variance_bar / num_workers\n",
        "    term2 = term1 ** 2\n",
        "    term3 = 3 * (num_local_steps - 1) * G_bar * variance_bar\n",
        "    \n",
        "    gain_ratio = term0 / (term1 + torch.sqrt(term2 + term3))\n",
        "    # print(f'variance_bar/G_bar: {variance_bar/G_bar}') # Vaghti variance_bar/G_bar miad taghriban zire 10 hame chi be ham mirize va gain ratio nan mishe.\n",
        "    if torch.isnan(gain_ratio):\n",
        "        pass\n",
        "        pass\n",
        "    # Ke variance_bar/G_bar tabe'e tedade worker e, va shayad tedade iteration\n",
        "\n",
        "        # time.sleep(2)\n",
        "\n",
        "    return gain_ratio.item()\n",
        "\n",
        "def compute_optimal_learning_rate(G_bar, variance_bar, num_workers, num_local_steps, L):\n",
        "    # Use Ḡ and variancē to compute the adaptive learning rate (t)\n",
        "    term0 = 2 * (G_bar)\n",
        "    term1 = G_bar + variance_bar / num_workers\n",
        "    term2 = term1 ** 2\n",
        "    term3 = 3 * (num_local_steps - 1) * G_bar * variance_bar\n",
        "    \n",
        "    lr = term0 / (term1 + torch.sqrt(term2 + term3))\n",
        "    lr = lr / L\n",
        "    return lr.item()\n",
        "\n",
        "def calculate_G_bar(g_bar, variance_bar, num_workers):\n",
        "    \"\"\"Calculate Ḡ as a scalar.\"\"\"\n",
        "    g_bar_norm = sum(torch.norm(g_bar[i])**2 for i in range(len(g_bar)))\n",
        "    G_bar = g_bar_norm - (1 / num_workers) * variance_bar\n",
        "    # print(f'G_bar: {G_bar}')\n",
        "    # print(G_bar.item())\n",
        "    if G_bar.item() < 0:\n",
        "        print('Gbaaaaaaaaaaaaar is negative')\n",
        "        # G_bar = 0.00\n",
        "    #     G_bar = 0.02* variance_bar # Choon bazi vaghta adad mishe kamtar az sefr choon taghrib mizanim. vali in kari ke kardam ham dorost nist.\n",
        "    #     print(f'heeeeeeeey: g_bar_norm: {g_bar_norm}, variance_bar: {variance_bar}, (1 / num_workers) * variance_bar: {(1 / num_workers) * variance_bar}')\n",
        "    #     print('-')\n",
        "\n",
        "    return G_bar\n",
        "\n",
        "def calculate_variance_bar(g_bar, grad_cache, num_workers): # Hamishe mosbate.\n",
        "    \"\"\"Calculate variancē as a scalar.\"\"\"\n",
        "    sum_norms = sum(torch.norm(grad_cache[worker_id][i])**2 for worker_id in range(num_workers) for i in range(len(g_bar)))\n",
        "    nomrs_sum = 0\n",
        "    for worker_id in range(num_workers):\n",
        "        for i in range(len(g_bar)):\n",
        "            nomrs_sum(torch.norm(grad_cache[worker_id][i])**2)\n",
        "    g_bar_norm = sum(torch.norm(g_bar[i])**2 for i in range(len(g_bar)))\n",
        "    variance_bar = (1 / (num_workers - 1)) * sum_norms - (num_workers / (num_workers - 1)) * g_bar_norm\n",
        "    print(f'sum(norm(g^t_k)**2): {sum_norms}, g_bar_norm: {g_bar_norm}')\n",
        "    return variance_bar\n",
        "\n",
        "def local_update_with_gradient(model, optimizer, dataloader, loss_fn, lr, gain_ratio, device=device, is_wandb=False):# (X, y, model, criterion, learning_rate): # forward_backward_pass_manual\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Update optimizer parameters\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "        param_group['gain_ratio'] = gain_ratio\n",
        "        \n",
        "        \n",
        "    \n",
        "    ## model.zero_grad()  # Initialize gradients to zero at the start of each epoch\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "    \n",
        "    initial_params = [param.clone() for param in model.parameters()] \n",
        "\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        loss.backward()  # Backpropagation\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        optimizer.step() \n",
        "        if i == (len(dataloader) - 1):\n",
        "            gradient = [param.grad.clone() for param in model.parameters()] \n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    final_params = [param.clone() for param in model.parameters()]  # Capture final parameters\n",
        "\n",
        "    # Calculate delta as the difference between final and initial parameters\n",
        "    # gradient = [(final - initial)/(gain_ratio * lr) for final, initial in zip(final_params, initial_params)]\n",
        "    \n",
        "    # TODO: in faghat gradient e akhar ro hesab mikone  behtare ke \n",
        "    #gradient = [param.grad.clone() for param in model.parameters()] \n",
        "    \n",
        "    # Update parameters using custom rule\n",
        "    # with torch.no_grad():  # Disable gradient tracking for manual update\n",
        "    #     for param in model.parameters():\n",
        "    #         if param.grad is not None:\n",
        "    #             param -= gain_ratio * lr * param.grad\n",
        "\n",
        "    # model.zero_grad()\n",
        "    len_dataloader = len(dataloader)\n",
        "    train_loss = running_loss / len_dataloader\n",
        "    train_accuracy = 100. * correct / total\n",
        "    if math.isnan(train_loss) or train_loss > 10:\n",
        "        pass\n",
        "    if is_wandb:\n",
        "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": train_accuracy})\n",
        "\n",
        "    return train_loss, train_accuracy, gradient\n",
        "\n",
        "def estimate_lipschitz_constant(model, dataloader, loss_fn, device='cuda'):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    max_grad_norm = 0\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        loss.backward()  # Compute gradients\n",
        "        \n",
        "        # Compute the gradient norm\n",
        "        total_norm = 0\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                param_norm = p.grad.data.norm(2)\n",
        "                total_norm += param_norm.item() ** 2\n",
        "        \n",
        "        total_norm = total_norm ** 0.5\n",
        "        max_grad_norm = max(max_grad_norm, total_norm)\n",
        "    \n",
        "    # Estimate L as the maximum observed gradient norm\n",
        "    lipschitz_constant = max_grad_norm\n",
        "    return lipschitz_constant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:2\n",
            "==================================================\n",
            "Worker 1, [01/02]: Training Loss: 4.585575668, Training Accuracy: 1.888\n",
            "Worker 2, [01/02]: Training Loss: 4.587634257, Training Accuracy: 1.920\n",
            "Worker 3, [01/02]: Training Loss: 4.590183788, Training Accuracy: 1.088\n",
            "Worker 4, [01/02]: Training Loss: 4.585954588, Training Accuracy: 1.760\n",
            "Worker 5, [01/02]: Training Loss: 4.590105193, Training Accuracy: 1.456\n",
            "Worker 6, [01/02]: Training Loss: 4.593763317, Training Accuracy: 1.296\n",
            "Worker 7, [01/02]: Training Loss: 4.591303373, Training Accuracy: 1.584\n",
            "Worker 8, [01/02]: Training Loss: 4.588327033, Training Accuracy: 1.648\n",
            "Worker 1, [02/02]: Training Loss: 4.349892193, Training Accuracy: 4.400\n",
            "Worker 2, [02/02]: Training Loss: 4.383812447, Training Accuracy: 3.632\n",
            "Worker 3, [02/02]: Training Loss: 4.370596905, Training Accuracy: 4.336\n",
            "Worker 4, [02/02]: Training Loss: 4.384550151, Training Accuracy: 3.760\n",
            "Worker 5, [02/02]: Training Loss: 4.385340963, Training Accuracy: 3.744\n",
            "Worker 6, [02/02]: Training Loss: 4.420571517, Training Accuracy: 3.488\n",
            "Worker 7, [02/02]: Training Loss: 4.383726125, Training Accuracy: 4.064\n",
            "Worker 8, [02/02]: Training Loss: 4.385157790, Training Accuracy: 4.128\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'int' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# (, loss_fn, , j, lr, wd, initial_state_dict, num_epochs)           \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# local_adascale_local_sgd(shard_loaders, loss_fn,  k, scale_inv_budget, lr, wd,initial_state_dict, num_epochs)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mlocal_adascale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_invariant_budget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# LocalAdaScale(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[40], line 53\u001b[0m, in \u001b[0;36mlocal_adascale\u001b[0;34m(shard_loaders, loss_fn, K, j, lr, wd, initial_state_dict, scale_invariant_budget)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m50\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m g_bar \u001b[38;5;241m=\u001b[39m calculate_g_bar(grad_cache, K)\n\u001b[0;32m---> 53\u001b[0m variance_bar \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_variance_bar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m G_bar \u001b[38;5;241m=\u001b[39m calculate_G_bar(g_bar\u001b[38;5;241m=\u001b[39mg_bar, num_workers\u001b[38;5;241m=\u001b[39mK, variance_bar\u001b[38;5;241m=\u001b[39mvariance_bar)\n\u001b[1;32m     55\u001b[0m L \u001b[38;5;241m=\u001b[39m estimate_lipschitz_constant(global_model, shard_loaders[\u001b[38;5;241m0\u001b[39m], loss_fn,device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[40], line 144\u001b[0m, in \u001b[0;36mcalculate_variance_bar\u001b[0;34m(g_bar, grad_cache, num_workers)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m worker_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_workers):\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(g_bar)):\n\u001b[0;32m--> 144\u001b[0m         \u001b[43mnomrs_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m g_bar_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(torch\u001b[38;5;241m.\u001b[39mnorm(g_bar[i])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(g_bar)))\n\u001b[1;32m    146\u001b[0m variance_bar \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (num_workers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m sum_norms \u001b[38;5;241m-\u001b[39m (num_workers \u001b[38;5;241m/\u001b[39m (num_workers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m g_bar_norm\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [8, 2, 4, 8]\n",
        "J = [2, 4, 8, 16, 32, 64] \n",
        "scale_invariant_budget = 150 # It uses as a unit instead of number of epochs\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    # (, loss_fn, , j, lr, wd, initial_state_dict, num_epochs)           \n",
        "    # local_adascale_local_sgd(shard_loaders, loss_fn,  k, scale_inv_budget, lr, wd,initial_state_dict, num_epochs)\n",
        "    local_adascale(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, scale_invariant_budget)\n",
        "    # LocalAdaScale(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BGJAcU7dTxJ1",
        "V2mj0Wd-T73T",
        "ybM87poAtHnl",
        "5fjzE9q4UOPU",
        "SEseiEKFt0mR",
        "taX_5ElNuKxC",
        "aeMWj_f0sbQE",
        "3GXZaFXruZI_"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06d3de2006b14793bd57a5ca89d44e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68d86018628f420e8d7e516ba5827e35",
            "placeholder": "​",
            "style": "IPY_MODEL_8877fd11846246f989e31835e5d3e7ae",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "07dc9b3c563e4615bec4dbd3233bf4ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07ff6351429c48c2b835305d3111af40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5de35a9063345b1a12e212718a02575",
            "placeholder": "​",
            "style": "IPY_MODEL_e86e517239b54ca4a6767a300aa7e01d",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "09b762f5ba784066a9408ffcfdea5b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a9ee7dc8817456aab4af03cbfa4c6ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b42ea1995bb410b99e653780dad3916": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b4b10bb8bff4d40afa8df981440fe43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d710da02bfe4c8e80d205158d9d64c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ef1563b6ba74b86a7bf7af3fcc3d5d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fdaf43c468043139e5d72397b118371": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "104402d4cba5477499593d972bc48e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13e7a0db2aa74e35b7d1948224358d76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1574e69b88de479da2aadc2dfdd43261": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15a0bbd1e2b14cef8a2f9accf120c1ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "175043eb44e44b4f9aefaf51aae6fb10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71e710b571d142a3a08cd233590b7171",
            "placeholder": "​",
            "style": "IPY_MODEL_2671c3bf3cfd49aa9ad6841c289068cc",
            "value": "0.019 MB of 0.019 MB uploaded\r"
          }
        },
        "17ac4efa9cf148e5be69edbc5b0bdecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18cb73ac1b224757920e7a8187b3ba4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19ed5f5d4f714246bbdf44513ecc50f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f9d7ac8fa8a4bcdb76fe2b2cb443f66",
              "IPY_MODEL_71c0cc303cfc44ba8d989d5f8feca119"
            ],
            "layout": "IPY_MODEL_07dc9b3c563e4615bec4dbd3233bf4ba"
          }
        },
        "1b093d744b374154afe2058e4a6de822": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b1acf7a1f5f43739e1fe0894ba48950": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d534596e41340438fc8d083fff6aa8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d74e9350c2c4c61b2e95924ec133012": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f9d7ac8fa8a4bcdb76fe2b2cb443f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ccc0b32e9a4ceebd6045dff63e6621",
            "placeholder": "​",
            "style": "IPY_MODEL_104402d4cba5477499593d972bc48e9d",
            "value": "0.030 MB of 0.030 MB uploaded\r"
          }
        },
        "1feabb0772134ace893d71cb905e9b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b1acf7a1f5f43739e1fe0894ba48950",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d710da02bfe4c8e80d205158d9d64c7",
            "value": 1
          }
        },
        "1fef0782f0804736881f03c2e31c3d64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2402e482be7e484fa98ae3f60f68d95a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2671c3bf3cfd49aa9ad6841c289068cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2991ead42a2a4637b2902ca26837d74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e0bb90e33ec4e198857c255080ef6f0",
              "IPY_MODEL_77790c4dcd124f73907619689fb8d37c"
            ],
            "layout": "IPY_MODEL_1fef0782f0804736881f03c2e31c3d64"
          }
        },
        "2bc1423c729a4faa9a184ae092eda8ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bd5fda76227433aa9332e6e48cd415b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd6480fe59104c7aa15d39bf6ea4a63b",
              "IPY_MODEL_3cb7b5c42e844747ae133750b7d42882"
            ],
            "layout": "IPY_MODEL_b81cc89707e44dedb51081d13a3ba424"
          }
        },
        "2d65b320aaed40cd87b8f78c99c614e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f39d5f73c7647f1804e4212cb39924d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e7ede02663f4eb0927872bf17858f18",
              "IPY_MODEL_a562d76741f74b10b8095be14da779d1"
            ],
            "layout": "IPY_MODEL_f4e7759ba0f544d8a28a9922e06e890b"
          }
        },
        "30d70d57987c4349b55560e5d9626201": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36ac25551e574241bed4d7e5a4301d95",
              "IPY_MODEL_6898349b9c754ca7be91bea29cabbba5"
            ],
            "layout": "IPY_MODEL_623f947ef06c41569be7fcdda4141174"
          }
        },
        "32c759d7fff64701a6ad61b38ac63187": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67aa8f8c2b244c9a9a3bb11caf491247",
            "placeholder": "​",
            "style": "IPY_MODEL_09b762f5ba784066a9408ffcfdea5b2e",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "33050a2d80c24235aecc36cb34e79684": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33a438b911c04f1f96c67b4f7ba7477d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff7b6956e7e34db68cd38dee19c798f1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d71c8b5d30df474ca6f30976d0a33c71",
            "value": 1
          }
        },
        "3445c5fe76514de0b6b9cc31200c8544": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0748d32c99741d39b18512400e07f30",
            "placeholder": "​",
            "style": "IPY_MODEL_dc43eb867d6b446cb0cb8e5debae57e7",
            "value": "0.030 MB of 0.030 MB uploaded\r"
          }
        },
        "346856a22f0e45b2be0c14aa7902261f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d90a90d5854bcaa2bc5f385cdad931": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3606009883cf4212b7e1ed6e4f182845": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36a546b812d6403cbf5ed693318a9744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a5f301338a4e5185c042a22684ed4a",
            "placeholder": "​",
            "style": "IPY_MODEL_48c7de7682f242fd86a234d1607187f4",
            "value": "0.020 MB of 0.020 MB uploaded\r"
          }
        },
        "36ac25551e574241bed4d7e5a4301d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d268f74266044451b230f20756bab2f6",
            "placeholder": "​",
            "style": "IPY_MODEL_6896790e80d1450c821918c7ef41e42f",
            "value": "0.027 MB of 0.027 MB uploaded\r"
          }
        },
        "371471be80ca47dbb005c853b7832f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1574e69b88de479da2aadc2dfdd43261",
            "placeholder": "​",
            "style": "IPY_MODEL_f523ae2aca8c46878f0a6ddc1f798ef5",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "397b6d4daaf04a1181413adc5e30db0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39aecb8ffe0645fbadccf8b0f8ed2486": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b0b3657d30547d7abe702d6c1e7b7c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cb7b5c42e844747ae133750b7d42882": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fdaf43c468043139e5d72397b118371",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f63fffe56ff64f1eb2b6e8f14974f011",
            "value": 1
          }
        },
        "3ee2c346d4d74faa915ef8315e3303c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "403dcd5f10de42ddbb67c8108e3bc34c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4302f47b1ff043aca2523212b015e080": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_371471be80ca47dbb005c853b7832f04",
              "IPY_MODEL_e75a0d8ae24e462aa3075a813aad302d"
            ],
            "layout": "IPY_MODEL_fd2cd8045a5c4387b98d080782289c48"
          }
        },
        "431cf75dd1cc4840b14f678876f45bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5f3871113994f1eb993c126d793d147",
              "IPY_MODEL_7903555c1e884408b70c9951ecae84ec"
            ],
            "layout": "IPY_MODEL_2d65b320aaed40cd87b8f78c99c614e7"
          }
        },
        "45bdcbbed9ae47ffb24bd2a962345eaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46c9d83bf8af443bb376ed4858ce35f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b0b3657d30547d7abe702d6c1e7b7c4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17ac4efa9cf148e5be69edbc5b0bdecf",
            "value": 1
          }
        },
        "48c7de7682f242fd86a234d1607187f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49727de0590f41dca6addf3f4ffe33f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "497de066b1964cd4b931476e0bd50c66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a4cfb14c56e477cbfef5a652c05fabc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b733fc6800e4a0ab36bba52ff84f1a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e2cc873f917444c833e152f7c2555de",
            "placeholder": "​",
            "style": "IPY_MODEL_49727de0590f41dca6addf3f4ffe33f2",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "5055c91b4ed34c1baf249fa25948e84f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33050a2d80c24235aecc36cb34e79684",
            "placeholder": "​",
            "style": "IPY_MODEL_2bc1423c729a4faa9a184ae092eda8ad",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "50b4ba3147b540abad020ae780353f27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "557cf2de74314915b7204d9055fa6987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a345ff7386643eeb7a5b190d69d2e0e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f4783e2060a4295bb68036a4a7310d9",
            "value": 1
          }
        },
        "5630d5fcb50a46f2887e0cce74a005a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "567cc1542f09433f8cc8b3a39237f198": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "584e5a6aada04535b64c40c3ece566e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cf516644d1a4ebf82c7e9dfa13e560b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36a546b812d6403cbf5ed693318a9744",
              "IPY_MODEL_aed05a015df946c7bae33226c3a8ddc6"
            ],
            "layout": "IPY_MODEL_d601ef3ebb6c4b688ac53a65979aa445"
          }
        },
        "5e0116dc735e4ad995a5b1a039c6a55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76dfea14213f4e66b269b901150c6cba",
            "placeholder": "​",
            "style": "IPY_MODEL_d1e4a03094b94c34ac235ffbd0c7fa06",
            "value": "0.030 MB of 0.030 MB uploaded\r"
          }
        },
        "5f3a33bc6a334c9e8c5886caa5f015ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8040d3a83de41319a2a836def914b1b",
            "placeholder": "​",
            "style": "IPY_MODEL_0b4b10bb8bff4d40afa8df981440fe43",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "60957598b6c64ab0987d583b788dd674": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623f947ef06c41569be7fcdda4141174": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63dd47b50bdc44e88edd97d14585f7ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e0116dc735e4ad995a5b1a039c6a55a",
              "IPY_MODEL_8cf1e952ed294ccdbc3bb2275b5d3efd"
            ],
            "layout": "IPY_MODEL_7421027bfcb34262a1b99d297e49bf8e"
          }
        },
        "64224c777f01418e904dcd30ecd7c5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99929754d3094d03a5f311177ac26cb3",
            "placeholder": "​",
            "style": "IPY_MODEL_9ea517158b974c5da17899e3fea4fc3b",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "67aa8f8c2b244c9a9a3bb11caf491247": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6896790e80d1450c821918c7ef41e42f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6898349b9c754ca7be91bea29cabbba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a30700c8bfb449cda98a40d75828f3d3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77beee57b67d4bad826f616e5483b87a",
            "value": 1
          }
        },
        "68d86018628f420e8d7e516ba5827e35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68ff4108835c462b929d0b5c78497555": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "696917cb66a74c41b87f9b436f8ce42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77a9b4df16ba408a941d67a51eed303d",
              "IPY_MODEL_b3b0d42308df44e6b43a06f7424d489d"
            ],
            "layout": "IPY_MODEL_fd23196beb964eb0a631b1fcb3893545"
          }
        },
        "6a345ff7386643eeb7a5b190d69d2e0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa92cf922724ac08c45385ff8a58447": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6abbcbe71a314356ba04f8349d1fc127": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3326a99729a416abca13d3122196e64",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d534596e41340438fc8d083fff6aa8b",
            "value": 1
          }
        },
        "6c30f9a9e6c44d78ad39f43a5f11a6d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e51a8e2e7e6463486d7b11454333941": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e7ede02663f4eb0927872bf17858f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a9ee7dc8817456aab4af03cbfa4c6ed",
            "placeholder": "​",
            "style": "IPY_MODEL_567cc1542f09433f8cc8b3a39237f198",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "7064cd1e5d794c258dbeeb63a9ae9f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71c0cc303cfc44ba8d989d5f8feca119": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80fe0a51b14b4e548454d4f83215336c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e13a8bef6e14973912966984e83cd4c",
            "value": 1
          }
        },
        "71e710b571d142a3a08cd233590b7171": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7421027bfcb34262a1b99d297e49bf8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76dfea14213f4e66b269b901150c6cba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77790c4dcd124f73907619689fb8d37c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13e7a0db2aa74e35b7d1948224358d76",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7064cd1e5d794c258dbeeb63a9ae9f9b",
            "value": 1
          }
        },
        "77a9b4df16ba408a941d67a51eed303d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e51a8e2e7e6463486d7b11454333941",
            "placeholder": "​",
            "style": "IPY_MODEL_18cb73ac1b224757920e7a8187b3ba4d",
            "value": "0.020 MB of 0.020 MB uploaded\r"
          }
        },
        "77beee57b67d4bad826f616e5483b87a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77c702b6d9c04f00b8e624f31a591f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6d67121f0ab4daf96d0bdd2c433259b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ee2c346d4d74faa915ef8315e3303c8",
            "value": 1
          }
        },
        "7903555c1e884408b70c9951ecae84ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac87babb2f2a4781ab863c3b8f613807",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3970cfd36ed4450b7c8f958a6499dc4",
            "value": 1
          }
        },
        "7e2cc873f917444c833e152f7c2555de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "804c12a3f13840c7bdb2e6df69e62c10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68ff4108835c462b929d0b5c78497555",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a63e1d987556448280ca217f17b60b49",
            "value": 1
          }
        },
        "80fe0a51b14b4e548454d4f83215336c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81408f178c46447f90d36d1d18cdad82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06d3de2006b14793bd57a5ca89d44e4a",
              "IPY_MODEL_804c12a3f13840c7bdb2e6df69e62c10"
            ],
            "layout": "IPY_MODEL_4a4cfb14c56e477cbfef5a652c05fabc"
          }
        },
        "81500a36ef5e4a9e9b36aef01bee3697": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "829bba2a5bb947a0bcf121c527902255": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "829eee71e4d74403a22941b15dfcf9bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8587c890f2a847e293a101405d64fc0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_175043eb44e44b4f9aefaf51aae6fb10",
              "IPY_MODEL_db36d0cc7691440a9792ac779e161e62"
            ],
            "layout": "IPY_MODEL_c6a68af25a2847e98201847781419670"
          }
        },
        "8795a05c814c469083b31be62e79289f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8877fd11846246f989e31835e5d3e7ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "889653c19d8e43cfb6f56ed46af5b76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cf1e952ed294ccdbc3bb2275b5d3efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8e410ff55fc448bbe87c8975a552e88",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f49251ce876f4d9292a4179b48ebb22a",
            "value": 1
          }
        },
        "8dc245e02cac40f1b601fa42a0e6e77a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e13a8bef6e14973912966984e83cd4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f4783e2060a4295bb68036a4a7310d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "904f899f441149e9b707699a0ebf31b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90b585318084475b9543b3226230d373": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f3a33bc6a334c9e8c5886caa5f015ca",
              "IPY_MODEL_46c9d83bf8af443bb376ed4858ce35f7"
            ],
            "layout": "IPY_MODEL_2402e482be7e484fa98ae3f60f68d95a"
          }
        },
        "9387a738135842cf965db860499510f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5630d5fcb50a46f2887e0cce74a005a6",
            "placeholder": "​",
            "style": "IPY_MODEL_fed90f96881140119ee97a0d2120b615",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "97c3921b8e454ccdb7b4db95d4440c64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99929754d3094d03a5f311177ac26cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d88657b39fd4b169d61b1b8d240e6ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e0bb90e33ec4e198857c255080ef6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3606009883cf4212b7e1ed6e4f182845",
            "placeholder": "​",
            "style": "IPY_MODEL_39aecb8ffe0645fbadccf8b0f8ed2486",
            "value": "0.021 MB of 0.021 MB uploaded\r"
          }
        },
        "9ea517158b974c5da17899e3fea4fc3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1d013e4d3e942b6b362ab5ade6d1a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a30700c8bfb449cda98a40d75828f3d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ccc0b32e9a4ceebd6045dff63e6621": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a562d76741f74b10b8095be14da779d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dc245e02cac40f1b601fa42a0e6e77a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b093d744b374154afe2058e4a6de822",
            "value": 1
          }
        },
        "a5758bfac16a4388a036005a15812d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5f3871113994f1eb993c126d793d147": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_346856a22f0e45b2be0c14aa7902261f",
            "placeholder": "​",
            "style": "IPY_MODEL_397b6d4daaf04a1181413adc5e30db0c",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "a63e1d987556448280ca217f17b60b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6ba8ab59be54b8ca096631627ee9713": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa6aa819c1fd4776ac47af588aaaaacd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64224c777f01418e904dcd30ecd7c5ef",
              "IPY_MODEL_c1dbd12595384bc6a0240aaeb16014dc"
            ],
            "layout": "IPY_MODEL_6aa92cf922724ac08c45385ff8a58447"
          }
        },
        "ab0b891f55f648cdbace9bc4540c51c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab580ef13b18428c994fc4f8b80885b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb268a732bc74b2d81ec3c3a6ecd0362",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2915a147a4246dba7984a90f23c34ae",
            "value": 1
          }
        },
        "ab62d64100a84b1faae744ee0f99501a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b4ba3147b540abad020ae780353f27",
            "placeholder": "​",
            "style": "IPY_MODEL_584e5a6aada04535b64c40c3ece566e8",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "ac87babb2f2a4781ab863c3b8f613807": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed05a015df946c7bae33226c3a8ddc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_403dcd5f10de42ddbb67c8108e3bc34c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1d013e4d3e942b6b362ab5ade6d1a80",
            "value": 1
          }
        },
        "b0748d32c99741d39b18512400e07f30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3970cfd36ed4450b7c8f958a6499dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3b0d42308df44e6b43a06f7424d489d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ef1563b6ba74b86a7bf7af3fcc3d5d6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_889653c19d8e43cfb6f56ed46af5b76f",
            "value": 1
          }
        },
        "b81cc89707e44dedb51081d13a3ba424": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba6f9560801c461a90a0fd2f8c20d918": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5055c91b4ed34c1baf249fa25948e84f",
              "IPY_MODEL_1feabb0772134ace893d71cb905e9b12"
            ],
            "layout": "IPY_MODEL_fb5f2e328dff44018f41180c2a2ec871"
          }
        },
        "bb2ee36dd3c04926b10b832928d7d0a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcbeb5bacc824f3aacd20e50d38bb70a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1dbd12595384bc6a0240aaeb16014dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb2ee36dd3c04926b10b832928d7d0a0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d88657b39fd4b169d61b1b8d240e6ff",
            "value": 1
          }
        },
        "c20d73b37f6a497a8847e2fd20a98d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15a0bbd1e2b14cef8a2f9accf120c1ad",
            "placeholder": "​",
            "style": "IPY_MODEL_e181391aa4424c04804ac665abd6f594",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "c59ee0d8b9fb4694ada362115bf965a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab62d64100a84b1faae744ee0f99501a",
              "IPY_MODEL_6abbcbe71a314356ba04f8349d1fc127"
            ],
            "layout": "IPY_MODEL_97c3921b8e454ccdb7b4db95d4440c64"
          }
        },
        "c5bb24dc0f56483b88a0efbfa2a1c8ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5de35a9063345b1a12e212718a02575": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6a68af25a2847e98201847781419670": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8e410ff55fc448bbe87c8975a552e88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbe10db0bb8d45609a0f3d59a30c8f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07ff6351429c48c2b835305d3111af40",
              "IPY_MODEL_ab580ef13b18428c994fc4f8b80885b6"
            ],
            "layout": "IPY_MODEL_497de066b1964cd4b931476e0bd50c66"
          }
        },
        "d1e4a03094b94c34ac235ffbd0c7fa06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d268f74266044451b230f20756bab2f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3326a99729a416abca13d3122196e64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d601ef3ebb6c4b688ac53a65979aa445": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d71c8b5d30df474ca6f30976d0a33c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d770fa4de3bd479886dbf8e1f6369543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c20d73b37f6a497a8847e2fd20a98d16",
              "IPY_MODEL_edd61f4569b54b869669aebf91420508"
            ],
            "layout": "IPY_MODEL_a6ba8ab59be54b8ca096631627ee9713"
          }
        },
        "db36d0cc7691440a9792ac779e161e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45bdcbbed9ae47ffb24bd2a962345eaa",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bcbeb5bacc824f3aacd20e50d38bb70a",
            "value": 1
          }
        },
        "dc43eb867d6b446cb0cb8e5debae57e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcbb358118e644a2a75d44d8adb6747b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60957598b6c64ab0987d583b788dd674",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4ad17a7648245069c70f2f3fb79105a",
            "value": 1
          }
        },
        "df97494c2f654ffbaceb9ca801ff3113": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1e5353cb6a1427a8b0b9d91d6067ef7",
              "IPY_MODEL_77c702b6d9c04f00b8e624f31a591f01"
            ],
            "layout": "IPY_MODEL_8795a05c814c469083b31be62e79289f"
          }
        },
        "e181391aa4424c04804ac665abd6f594": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1e5353cb6a1427a8b0b9d91d6067ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_829eee71e4d74403a22941b15dfcf9bc",
            "placeholder": "​",
            "style": "IPY_MODEL_ab0b891f55f648cdbace9bc4540c51c6",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "e4a5f301338a4e5185c042a22684ed4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4ad17a7648245069c70f2f3fb79105a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6a773393e084de9a117fbbf43b7a59e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6d67121f0ab4daf96d0bdd2c433259b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e75a0d8ae24e462aa3075a813aad302d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b42ea1995bb410b99e653780dad3916",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3ebd9b7bf7a4d24b54ab6673322f74b",
            "value": 1
          }
        },
        "e8040d3a83de41319a2a836def914b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e851c69b66ca4e8a80779a69435b855f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b733fc6800e4a0ab36bba52ff84f1a5",
              "IPY_MODEL_dcbb358118e644a2a75d44d8adb6747b"
            ],
            "layout": "IPY_MODEL_829bba2a5bb947a0bcf121c527902255"
          }
        },
        "e86e517239b54ca4a6767a300aa7e01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eafc1e77174c4758a3780b481d694ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c30f9a9e6c44d78ad39f43a5f11a6d0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81500a36ef5e4a9e9b36aef01bee3697",
            "value": 1
          }
        },
        "eb268a732bc74b2d81ec3c3a6ecd0362": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edd61f4569b54b869669aebf91420508": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8ce66183b2543e7bf19b71d90d1c53e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_904f899f441149e9b707699a0ebf31b5",
            "value": 1
          }
        },
        "f2915a147a4246dba7984a90f23c34ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3ebd9b7bf7a4d24b54ab6673322f74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f49251ce876f4d9292a4179b48ebb22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4e7759ba0f544d8a28a9922e06e890b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f523ae2aca8c46878f0a6ddc1f798ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5fd8db807e144578a2e20762d7369d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32c759d7fff64701a6ad61b38ac63187",
              "IPY_MODEL_557cf2de74314915b7204d9055fa6987"
            ],
            "layout": "IPY_MODEL_c5bb24dc0f56483b88a0efbfa2a1c8ee"
          }
        },
        "f63fffe56ff64f1eb2b6e8f14974f011": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f76ed5acf91a4edf92950dcb88356f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9387a738135842cf965db860499510f5",
              "IPY_MODEL_33a438b911c04f1f96c67b4f7ba7477d"
            ],
            "layout": "IPY_MODEL_35d90a90d5854bcaa2bc5f385cdad931"
          }
        },
        "f8ce66183b2543e7bf19b71d90d1c53e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9dca98d1b8c402db8fa03f1354a051b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3445c5fe76514de0b6b9cc31200c8544",
              "IPY_MODEL_eafc1e77174c4758a3780b481d694ae0"
            ],
            "layout": "IPY_MODEL_e6a773393e084de9a117fbbf43b7a59e"
          }
        },
        "fb5f2e328dff44018f41180c2a2ec871": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd23196beb964eb0a631b1fcb3893545": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2cd8045a5c4387b98d080782289c48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd6480fe59104c7aa15d39bf6ea4a63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d74e9350c2c4c61b2e95924ec133012",
            "placeholder": "​",
            "style": "IPY_MODEL_a5758bfac16a4388a036005a15812d1d",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "fed90f96881140119ee97a0d2120b615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff7b6956e7e34db68cd38dee19c798f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVF2_gqB0EDy",
        "outputId": "d30ab9fc-a12a-41d6-f66a-a1fd1c26307d"
      },
      "outputs": [],
      "source": [
        "# Install missing dependencies\n",
        "# !pip install -q torchinfo torchmetrics wandb\n",
        "# !pip install torch\n",
        "# !pip install torchvision\n",
        "# !pip install numpy\n",
        "# !pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGJAcU7dTxJ1"
      },
      "source": [
        "### Import important libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bnVg3M4l0EyA"
      },
      "outputs": [],
      "source": [
        "# Import required libraries/code\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import wandb\n",
        "import pickle\n",
        "\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import  random_split, DataLoader, Subset\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2mj0Wd-T73T"
      },
      "source": [
        "### Build the directory for checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQg3CXCtgB-5",
        "outputId": "7b5a2e28-b427-4b64-f013-8c1977e97423"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybM87poAtHnl"
      },
      "source": [
        "### Set device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FC7BMY9z0H3L",
        "outputId": "3a9750af-17c1-4e3c-f816-20ff79567d9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QH26cxftUt3"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "knmNFhHg0CwT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define a class for handling CIFAR100 data\n",
        "class CIFAR100Data:\n",
        "    \"\"\"\n",
        "    A class used to represent the CIFAR100 dataset.\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    batch_size : int\n",
        "        the number of samples that will be propagated through the network simultaneously\n",
        "    original_train_set : torchvision.datasets.CIFAR100\n",
        "        the original training set downloaded from CIFAR100\n",
        "    original_test_set : torchvision.datasets.CIFAR100\n",
        "        the original test set downloaded from CIFAR100\n",
        "    train_set : torch.utils.data.Subset\n",
        "        the training set after splitting the original training set\n",
        "    validation_set : torch.utils.data.Subset\n",
        "        the validation set after splitting the original training set\n",
        "    test_set : torchvision.datasets.CIFAR100\n",
        "        the test set, same as the original test set\n",
        "    original_train_loader : torch.utils.data.DataLoader\n",
        "        data loader for the original training set\n",
        "    original_test_loader : torch.utils.data.DataLoader\n",
        "        data loader for the original test set\n",
        "    train_loader : torch.utils.data.DataLoader\n",
        "        data loader for the training set\n",
        "    validation_loader : torch.utils.data.DataLoader\n",
        "        data loader for the validation set\n",
        "    test_loader : torch.utils.data.DataLoader\n",
        "        data loader for the test set\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    compute_mean_std(loader)\n",
        "        Computes the mean and standard deviation of the images in the loader.\n",
        "    download_data()\n",
        "        Downloads the CIFAR100 dataset.\n",
        "    split_data(original_train_set, validation_ratio=0.2)\n",
        "        Splits the original training set into a training set and a validation set.\n",
        "    compute_statistics(train_set)\n",
        "        Computes the mean and standard deviation of the training set.\n",
        "    apply_transforms(train_mean, train_std, is_validation_set_available = False)\n",
        "        Defines and applies the transformations for the training set, validation set, and test set.\n",
        "    save_data(data_loader, data_set, file_name: str)\n",
        "        Saves the data loader to Google Drive.\n",
        "    load_data(file_name: str)\n",
        "        Loads the data loader from Google Drive.\n",
        "    create_and_save_data_loaders(train_set, test_set, validation_set=None)\n",
        "        Creates data loaders for the training, validation, and test sets and saves them to Google Drive.\n",
        "    prepare_data(validation_ratio = None)\n",
        "        Prepares the data by downloading it, splitting it, computing statistics, applying transforms, and creating and saving data loaders.\n",
        "    train_valid_test(validation_ratio=0.2)\n",
        "        Loads or prepares the data loaders for the training, validation, and test sets and returns them.\n",
        "    train_test()\n",
        "        Loads or prepares the data loaders for the original training and test sets and returns them.\n",
        "    iid_shards(num_shards=2)\n",
        "        Loads or prepares the data loaders for the shards of the original training set and returns them.\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size=64):\n",
        "        \"\"\"\n",
        "        Initialize the CIFAR100Data object with the given batch size.\n",
        "\n",
        "        Parameters:\n",
        "        batch_size (int): The size of the batches for the data loaders.\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.original_train_set = None\n",
        "        self.original_test_set = None\n",
        "        self.train_set = None\n",
        "        self.validation_set = None\n",
        "        self.test_set = None\n",
        "\n",
        "        self.original_train_loader = None\n",
        "        self.original_test_loader = None\n",
        "        self.train_loader = None\n",
        "        self.validation_loader = None\n",
        "        self.test_loader = None\n",
        "\n",
        "    def compute_mean_std(self, loader):\n",
        "        \"\"\"\n",
        "        Compute the mean and standard deviation of the images in the loader.\n",
        "\n",
        "        Parameters:\n",
        "        loader (DataLoader): The DataLoader object containing the image data.\n",
        "\n",
        "        Returns:\n",
        "        mean (Tensor): The mean of the images.\n",
        "        std (Tensor): The standard deviation of the images.\n",
        "        \"\"\"\n",
        "        channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "        for data, _ in loader:\n",
        "            channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
        "            channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
        "            num_batches += 1\n",
        "        mean = channels_sum / num_batches\n",
        "        std = torch.sqrt((channels_squared_sum / num_batches) - mean**2)\n",
        "        return mean, std\n",
        "\n",
        "    def download_data(self):\n",
        "        \"\"\"\n",
        "        Download the CIFAR100 dataset and store it in instance variables.\n",
        "        \"\"\"\n",
        "        self.original_train_set = CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "        self.original_test_set = CIFAR100(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "    def split_data(self, original_train_set, validation_ratio=0.2):\n",
        "        \"\"\"\n",
        "        Split the original training set into a training set and a validation set.\n",
        "\n",
        "        Parameters:\n",
        "        original_train_set (Dataset): The original training set.\n",
        "        validation_ratio (float): The ratio of the original training set to use for validation.\n",
        "\n",
        "        Returns:\n",
        "        train_set (Subset): The new training set.\n",
        "        validation_set (Subset): The new validation set.\n",
        "        \"\"\"\n",
        "        train_len = int(len(original_train_set) * (1 - validation_ratio))\n",
        "        val_len = len(original_train_set) - train_len\n",
        "        train_set, validation_set = random_split(original_train_set, [train_len, val_len])\n",
        "\n",
        "        return train_set, validation_set\n",
        "\n",
        "    def compute_statistics(self, train_set):\n",
        "        \"\"\"\n",
        "        Compute the mean and standard deviation of the train set.\n",
        "\n",
        "        Parameters:\n",
        "        train_set (Dataset/Subset): The training set.\n",
        "\n",
        "        Returns:\n",
        "        train_mean (Tensor): The mean of the training set.\n",
        "        train_std (Tensor): The standard deviation of the training set.\n",
        "        \"\"\"\n",
        "        trainloader_tmp = DataLoader(train_set, batch_size=self.batch_size, shuffle=True)\n",
        "        train_mean, train_std = self.compute_mean_std(trainloader_tmp)\n",
        "\n",
        "        return train_mean, train_std\n",
        "\n",
        "    def apply_transforms(self, train_mean, train_std, is_validation_set_available = False):\n",
        "        \"\"\"\n",
        "        Define the transformations for the training set, validation set, and test set\n",
        "        and apply them to the datasets.\n",
        "\n",
        "        Parameters:\n",
        "        train_mean (Tensor): The mean of the training set.\n",
        "        train_std (Tensor): The standard deviation of the training set.\n",
        "        is_validation_set_available (bool): Whether a validation set is available.\n",
        "        \"\"\"\n",
        "\n",
        "        # Transformations for the training set\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(train_mean.tolist(), train_std.tolist())\n",
        "        ])\n",
        "\n",
        "        # Transformations for the validation and test sets\n",
        "        test_val_transforms = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(train_mean.tolist(), train_std.tolist())\n",
        "        ])\n",
        "\n",
        "        # Apply the transformations to the datasets\n",
        "        if is_validation_set_available:\n",
        "            self.train_set.transform = train_transforms\n",
        "            self.validation_set.transform = test_val_transforms\n",
        "            self.test_set.transform = test_val_transforms\n",
        "        else:\n",
        "            self.original_train_set.transform = train_transforms\n",
        "            self.original_test_set.transform = test_val_transforms\n",
        "\n",
        "    def save_data(self, data_loader, file_name: str):\n",
        "        \"\"\"\n",
        "        Save the given data loader and data set to Google Drive.\n",
        "\n",
        "        Parameters:\n",
        "        data_loader (DataLoader): The data loader to save.\n",
        "        file_name (str): The name of the file to save the data loader and data set to.\n",
        "        \"\"\"\n",
        "        # Check if the directory exists, if not, create it\n",
        "        # if not os.path.exists('/content/gdrive/MyDrive/data/data_loaders/'):\n",
        "        #     os.makedirs('/content/gdrive/MyDrive/data/data_loaders/')\n",
        "\n",
        "        # Open each file in write-binary mode on Google Drive and dump (pickle) the data loader into it\n",
        "        # with open(f'/content/gdrive/MyDrive/data/data_loaders/{self.batch_size}_{file_name}_loader.pkl', 'wb') as f:\n",
        "        #     pickle.dump(data_loader, f)\n",
        "        if not os.path.exists(f'./data/data_loaders/{self.batch_size}/'):\n",
        "            os.makedirs(f'./data/data_loaders/{self.batch_size}/')\n",
        "        \n",
        "        open(f'./data/data_loaders/{self.batch_size}/{file_name}_loader.pkl', 'wb').write(pickle.dumps(data_loader))\n",
        "\n",
        "    def load_data(self, file_name: str):\n",
        "        \"\"\"\n",
        "        Load a data loader from Google Drive.\n",
        "\n",
        "        Parameters:\n",
        "        file_name (str): The name of the file to load the data loader from.\n",
        "\n",
        "        Returns:\n",
        "        data_loader (DataLoader): The loaded data loader, or None if the file does not exist.\n",
        "        \"\"\"\n",
        "        # Check if the file exists\n",
        "        # if os.path.exists(f'/content/gdrive/MyDrive/data/data_loaders/{self.batch_size}_{file_name}_loader.pkl'):\n",
        "        #     # If it exists, open the file in read-binary mode and load (unpickle) the data loader from it\n",
        "        #     with open(f'/content/gdrive/MyDrive/data/data_loaders/{self.batch_size}_{file_name}_loader.pkl', 'rb') as f:\n",
        "        #         return pickle.load(f)\n",
        "        # else:\n",
        "        #     return None\n",
        "        if os.path.exists(f'./data/data_loaders/{self.batch_size}/{file_name}_loader.pkl'):\n",
        "            return pickle.loads(open(f'./data/data_loaders/{self.batch_size}/{file_name}_loader.pkl', 'rb').read())\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def create_and_save_data_loaders(self, train_set, test_set, train_name: str, test_name: str, validation_set=None):\n",
        "        \"\"\"\n",
        "        Create data loaders for the training, validation, and test sets and save them to Google Drive.\n",
        "\n",
        "        Parameters:\n",
        "        train_set (Subset): The training set.\n",
        "        test_set (Subset): The test set.\n",
        "        validation_set (Subset, optional): The validation set.\n",
        "\n",
        "        Returns:\n",
        "        train_loader (DataLoader): The data loader for the training set.\n",
        "        validation_loader (DataLoader): The data loader for the validation set, if it exists.\n",
        "        test_loader (DataLoader): The data loader for the test set.\n",
        "        \"\"\"\n",
        "        train_loader = DataLoader(train_set, batch_size=self.batch_size, shuffle=True, num_workers =8)\n",
        "        test_loader = DataLoader(test_set, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "        # Save the newly created data loaders to Google Drive\n",
        "        self.save_data(train_loader, train_name)\n",
        "        self.save_data(test_loader, test_name)\n",
        "\n",
        "        if validation_set is not None:\n",
        "            validation_loader = DataLoader(validation_set, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
        "            self.save_data(validation_loader, 'validation')\n",
        "            return train_loader, validation_loader, test_loader\n",
        "\n",
        "        return train_loader, test_loader\n",
        "\n",
        "    def prepare_data(self, validation_ratio = None):\n",
        "        \"\"\"\n",
        "        Prepare the data by downloading it, splitting it into training, validation, and test sets,\n",
        "        computing statistics, applying transformations, and creating and saving data loaders.\n",
        "\n",
        "        Parameters:\n",
        "        validation_ratio (float, optional): The ratio of the original training set to use for validation.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.download_data()\n",
        "        except IOError:\n",
        "            print(\"Error downloading data\")\n",
        "            return\n",
        "\n",
        "        if validation_ratio is not None:\n",
        "            self.train_set, self.validation_set = self.split_data(self.original_train_set, validation_ratio)\n",
        "            if self.validation_set is None:\n",
        "                print(\"Validation set is not available\")\n",
        "                return\n",
        "            self.test_set = self.original_test_set\n",
        "            train_mean, train_std = self.compute_statistics(self.train_set)\n",
        "            self.apply_transforms(train_mean, train_std, is_validation_set_available = True)\n",
        "\n",
        "            # Create and save data loaders\n",
        "            self.train_loader, self.validation_loader, self.test_loader = self.create_and_save_data_loaders(self.train_set, self.test_set, 'train', 'test', self.validation_set)\n",
        "\n",
        "        else:\n",
        "            train_mean, train_std = self.compute_statistics(self.original_train_set)\n",
        "            self.apply_transforms(train_mean, train_std)\n",
        "\n",
        "            # Create and save data loaders\n",
        "            self.original_train_loader, self.original_test_loader = self.create_and_save_data_loaders(self.original_train_set, self.original_test_set, 'original_train', 'original_test')\n",
        "\n",
        "    def train_valid_test(self, validation_ratio=0.2):\n",
        "        \"\"\"\n",
        "        Load the training, validation, and test data loaders from Google Drive, or prepare the data if they do not exist.\n",
        "\n",
        "        Parameters:\n",
        "        validation_ratio (float): The ratio of the original training set to use for validation.\n",
        "\n",
        "        Returns:\n",
        "        train_loader (DataLoader): The data loader for the training set.\n",
        "        validation_loader (DataLoader): The data loader for the validation set.\n",
        "        test_loader (DataLoader): The data loader for the test set.\n",
        "        \"\"\"\n",
        "        self.train_loader = self.load_data('train')\n",
        "        self.validation_loader = self.load_data('validation')\n",
        "        self.test_loader = self.load_data('test')\n",
        "\n",
        "        if self.train_loader is None or self.validation_loader is None or self.test_loader is None:\n",
        "            self.prepare_data(validation_ratio)\n",
        "\n",
        "        # Return the data loaders\n",
        "        return self.train_loader, self.validation_loader, self.test_loader\n",
        "\n",
        "    def train_test(self):\n",
        "        \"\"\"\n",
        "        Load the original training and test data loaders from Google Drive, or prepare the data if they do not exist.\n",
        "\n",
        "        Returns:\n",
        "        original_train_loader (DataLoader): The data loader for the original training set.\n",
        "        original_test_loader (DataLoader): The data loader for the original test set.\n",
        "        \"\"\"\n",
        "        self.original_train_loader = self.load_data('original_train')\n",
        "        self.original_test_loader = self.load_data('original_test')\n",
        "\n",
        "        if self.original_train_loader is None or self.original_test_loader is None:\n",
        "            self.prepare_data()\n",
        "\n",
        "        # Return the data loaders\n",
        "        return self.original_train_loader, self.original_test_loader\n",
        "\n",
        "    def iid_shards(self, num_shards=2):\n",
        "        \"\"\"\n",
        "        Create or load independent and identically distributed (IID) shards of the original training set.\n",
        "\n",
        "        Parameters:\n",
        "        num_shards (int): The number of shards to create.\n",
        "\n",
        "        Returns:\n",
        "        shard_loaders (list of DataLoader): The data loaders for the shards.\n",
        "        \"\"\"\n",
        "        # Try to load the shard datasets and their corresponding data loaders from Google Drive\n",
        "        shard_loaders = []\n",
        "        for i in range(num_shards):\n",
        "            shard_loader = self.load_data(f'iid_sharding/{num_shards}_chunk_{i+1}')\n",
        "            if shard_loader is None:\n",
        "                break\n",
        "            shard_loaders.append(shard_loader)\n",
        "\n",
        "        # If all shard data loaders were successfully loaded, return them\n",
        "        if len(shard_loaders) == num_shards:\n",
        "            return shard_loaders\n",
        "\n",
        "        # If not all shard data loaders were successfully loaded, create them\n",
        "        if self.original_train_set is None:\n",
        "            self.download_data()\n",
        "            train_mean, train_std = self.compute_statistics(self.original_train_set)\n",
        "            self.apply_transforms(train_mean, train_std)\n",
        "\n",
        "        # Shuffle the indices\n",
        "        indices = torch.randperm(len(self.original_train_set))\n",
        "\n",
        "        # Split the indices into K chunks\n",
        "        shard_size = len(indices) // num_shards\n",
        "        shards = [indices[i*shard_size:(i+1)*shard_size] for i in range(num_shards)]\n",
        "\n",
        "        # Create subsets for each shard\n",
        "        shard_datasets = [Subset(self.original_train_set, shard) for shard in shards]\n",
        "\n",
        "        # Create data loaders for each shard\n",
        "        shard_loaders = [DataLoader(shard_dataset, batch_size=self.batch_size, shuffle=True) for shard_dataset in shard_datasets]\n",
        "\n",
        "        # Save each shard dataset and its corresponding data loader\n",
        "        for i, shard_loader in enumerate(shard_loaders):\n",
        "            self.save_data(shard_loader, f'iid_sharding_{num_shards}_chunk_{i+1}')\n",
        "\n",
        "        return shard_loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KE_ErnYB06JF"
      },
      "outputs": [],
      "source": [
        "data = CIFAR100Data()\n",
        "train_loader, validation_loader, test_loader = data.train_valid_test(validation_ratio=0.2)\n",
        "original_train_loader, original_test_loader = data.train_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fjzE9q4UOPU"
      },
      "source": [
        "### Define the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KaLfzyjo0Pzk"
      },
      "outputs": [],
      "source": [
        "# Define the model architecture\n",
        "# Check if it is LeNet-5 or similar to LeNet-5 we want similar.\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=0) # 28x28\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=0) # 10x10\n",
        "        # TODO: be nazaram inja 2 ta pool tarif konim behtar bashe\n",
        "        self.pool = nn.MaxPool2d(2, 2) # 14x14 for conv1 and 5x5 for conv2\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(5 * 5 * 64, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 14x14\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 5x5\n",
        "        x = self.flatten(x) # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-4HtbSQ20ea5"
      },
      "outputs": [],
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEseiEKFt0mR"
      },
      "source": [
        "### Define some basic functions for train and test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JZNSgcBp0jEl"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, loss_fn, accumulation_steps=1, device=device, is_wandb=False):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    optimizer.zero_grad()  # Initialize gradients to zero at the start of each epoch\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        loss.backward()  # Backpropagation\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # Gradient accumulation\n",
        "        if (i+1) % accumulation_steps == 0 or i+1 == len(dataloader):\n",
        "            optimizer.step()  # Update model parameters\n",
        "            optimizer.zero_grad()  # Reset gradients to zero\n",
        "\n",
        "    train_loss = running_loss / len(dataloader)\n",
        "    train_accuracy = 100. * correct / total\n",
        "\n",
        "    if is_wandb:\n",
        "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": train_accuracy})\n",
        "\n",
        "    return train_loss, train_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m6dkKxXh0i7n"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader, loss_fn, device = device, is_wandb= False):\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if is_wandb:\n",
        "              # Log the loss and accuracy values at each step\n",
        "              wandb.log({\n",
        "                  'Test Loss': test_loss / (batch_idx + 1),\n",
        "                  'Test Accuracy': 100 * correct / total\n",
        "              })\n",
        "\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_accuracy = 100. * correct / total\n",
        "\n",
        "    return test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZD7Vge_wYwvX"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, epoch, batch_size, optimizer_name, hyperparameters):\n",
        "    # dir_path = f\"/content/gdrive/MyDrive/{optimizer_name}/{batch_size}/\"\n",
        "    dir_path = f\"./train/{optimizer_name}/{batch_size}/\"\n",
        "    for key, value in hyperparameters.items():\n",
        "        dir_path = os.path.join(dir_path, f\"{key}_{value}/\")\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "    path = os.path.join(dir_path, f\"epoch_{epoch:03}.pt\")\n",
        "    torch.save(state, path)\n",
        "\n",
        "    # Get list of all files\n",
        "    list_of_files = glob.glob(os.path.join(dir_path, f\"epoch_*.pt\"))\n",
        "    # Sort files by creation time\n",
        "    list_of_files.sort(key=os.path.getctime)\n",
        "    # If there are more than 2 files, delete the second last one\n",
        "    if len(list_of_files) > 1:\n",
        "        os.remove(list_of_files[-2])\n",
        "\n",
        "def load_checkpoint(optimizer_name, batch_size, hyperparameters):\n",
        "    # dir_path = f\"/content/gdrive/MyDrive/{optimizer_name}/{batch_size}/\"\n",
        "    dir_path = f\"./train/{optimizer_name}/{batch_size}/\"\n",
        "    for key, value in hyperparameters.items():\n",
        "        dir_path = os.path.join(dir_path, f\"{key}_{value}/\")\n",
        "    list_of_files = glob.glob(os.path.join(dir_path, f\"epoch_*.pt\")) # * means all if need specific format then *.csv\n",
        "    if not list_of_files:  # I'm using glob which can return an empty list\n",
        "        return None\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    if os.path.isfile(latest_file):\n",
        "        return torch.load(latest_file)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9Yjx-yZf-Sbz"
      },
      "outputs": [],
      "source": [
        "def run_training(\n",
        "    num_epochs,\n",
        "    model,\n",
        "    trainloader,\n",
        "    validationloader,\n",
        "    testloader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    optimizer_name: str,\n",
        "    accumulation_steps=1,\n",
        "    hyperparameters=None,\n",
        "    is_wandb = False,\n",
        "    n_epochs_stop = None\n",
        "  ):\n",
        "\n",
        "  best_accuracy = 0\n",
        "  epochs_no_improve = 0\n",
        "  n_epochs_stop = n_epochs_stop  # number of epochs to wait before stopping\n",
        "\n",
        "  start_epoch = 0\n",
        "  run_id = None\n",
        "  run_name = None\n",
        "  # Load checkpoint if available\n",
        "  checkpoint = load_checkpoint(optimizer_name, trainloader.batch_size, hyperparameters)\n",
        "  if checkpoint is not None:\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "      start_epoch = checkpoint['epoch'] + 1\n",
        "      run_id = checkpoint.get('wandb_run_id', None)\n",
        "      run_name = checkpoint.get('wandb_run_name', None)\n",
        "\n",
        "  else:\n",
        "    run_name = \" \".join([f\"{key}={value}\" for key, value in hyperparameters.items()])\n",
        "\n",
        "  if is_wandb:\n",
        "    # Initialize a wandb run with the given hyperparameters\n",
        "    wandb.init(id=run_id, name=run_name, project=f'cifar100-training-mldl2024-baseline-{optimizer_name}',\n",
        "                   config=hyperparameters if hyperparameters is not None else {},\n",
        "                   resume=\"allow\", reinit=True)\n",
        "\n",
        "  for epoch in range(start_epoch, num_epochs):\n",
        "      # Call the training function for each epoch\n",
        "      train_loss, train_acc = train(model, trainloader, optimizer, loss_fn, accumulation_steps, device, is_wandb=is_wandb)\n",
        "      print(f'[{epoch+1}/{num_epochs}]: Training Loss: {train_loss}, Training Accuracy: {train_acc}')\n",
        "      scheduler.step() # Update learning rate based on scheduler\n",
        "\n",
        "      # Save checkpoint\n",
        "      save_checkpoint({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),\n",
        "                    'loss': criterion,\n",
        "                    'wandb_run_id': wandb.run.id if is_wandb else None,\n",
        "                    'wandb_run_name': wandb.run.name if is_wandb else None,\n",
        "                    }, epoch, trainloader.batch_size, optimizer_name, hyperparameters)\n",
        "\n",
        "      if validationloader is not None:\n",
        "        val_loss, val_acc = test(model, validationloader, criterion)\n",
        "        print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_acc}')\n",
        "\n",
        "\n",
        "        if n_epochs_stop is not None:\n",
        "          if val_acc > best_accuracy:\n",
        "                best_accuracy = val_acc\n",
        "                epochs_no_improve = 0\n",
        "          else:\n",
        "              epochs_no_improve += 1\n",
        "              if epochs_no_improve == n_epochs_stop:\n",
        "                  print('Early stopping!')\n",
        "                  break\n",
        "\n",
        "  print('*'*70)\n",
        "  test_loss, test_acc = test(model, testloader, criterion, is_wandb = is_wandb)\n",
        "  print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')\n",
        "\n",
        "\n",
        "  # Finish the wandb run after all epochs\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taX_5ElNuKxC"
      },
      "source": [
        "### Centeralised baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "8hxWmieIE7jF"
      },
      "outputs": [],
      "source": [
        "\n",
        "learning_rates = [1e-03, 1e-02]\n",
        "weight_decays = [1e-04, 1e-03, 4e-04]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SGDM (Stochastic Gradient Descent with Momentum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8587c890f2a847e293a101405d64fc0d",
            "175043eb44e44b4f9aefaf51aae6fb10",
            "db36d0cc7691440a9792ac779e161e62",
            "c6a68af25a2847e98201847781419670",
            "71e710b571d142a3a08cd233590b7171",
            "2671c3bf3cfd49aa9ad6841c289068cc",
            "45bdcbbed9ae47ffb24bd2a962345eaa",
            "bcbeb5bacc824f3aacd20e50d38bb70a",
            "2991ead42a2a4637b2902ca26837d74b",
            "9e0bb90e33ec4e198857c255080ef6f0",
            "77790c4dcd124f73907619689fb8d37c",
            "1fef0782f0804736881f03c2e31c3d64",
            "3606009883cf4212b7e1ed6e4f182845",
            "39aecb8ffe0645fbadccf8b0f8ed2486",
            "13e7a0db2aa74e35b7d1948224358d76",
            "7064cd1e5d794c258dbeeb63a9ae9f9b",
            "5cf516644d1a4ebf82c7e9dfa13e560b",
            "36a546b812d6403cbf5ed693318a9744",
            "aed05a015df946c7bae33226c3a8ddc6",
            "d601ef3ebb6c4b688ac53a65979aa445",
            "e4a5f301338a4e5185c042a22684ed4a",
            "48c7de7682f242fd86a234d1607187f4",
            "403dcd5f10de42ddbb67c8108e3bc34c",
            "a1d013e4d3e942b6b362ab5ade6d1a80",
            "e851c69b66ca4e8a80779a69435b855f",
            "4b733fc6800e4a0ab36bba52ff84f1a5",
            "dcbb358118e644a2a75d44d8adb6747b",
            "829bba2a5bb947a0bcf121c527902255",
            "7e2cc873f917444c833e152f7c2555de",
            "49727de0590f41dca6addf3f4ffe33f2",
            "60957598b6c64ab0987d583b788dd674",
            "e4ad17a7648245069c70f2f3fb79105a",
            "431cf75dd1cc4840b14f678876f45bc9",
            "a5f3871113994f1eb993c126d793d147",
            "7903555c1e884408b70c9951ecae84ec",
            "2d65b320aaed40cd87b8f78c99c614e7",
            "346856a22f0e45b2be0c14aa7902261f",
            "397b6d4daaf04a1181413adc5e30db0c",
            "ac87babb2f2a4781ab863c3b8f613807",
            "b3970cfd36ed4450b7c8f958a6499dc4",
            "df97494c2f654ffbaceb9ca801ff3113",
            "e1e5353cb6a1427a8b0b9d91d6067ef7",
            "77c702b6d9c04f00b8e624f31a591f01",
            "8795a05c814c469083b31be62e79289f",
            "829eee71e4d74403a22941b15dfcf9bc",
            "ab0b891f55f648cdbace9bc4540c51c6",
            "e6d67121f0ab4daf96d0bdd2c433259b",
            "3ee2c346d4d74faa915ef8315e3303c8"
          ]
        },
        "id": "9rGKhc7FJB8u",
        "outputId": "9b4eddc5-dc51-4af2-fdbe-4ba3175d3642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240611_235508-gw8sn909</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909' target=\"_blank\">learning_rate=0.001 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10/150]: Training Loss: 3.7955315692901612, Training Accuracy: 12.3825\n",
            "Validation Loss: 3.7704276780413974, Validation Accuracy: 12.97\n",
            "[11/150]: Training Loss: 3.693796951675415, Training Accuracy: 14.2675\n",
            "Validation Loss: 3.6661312990127857, Validation Accuracy: 14.76\n",
            "[12/150]: Training Loss: 3.5919596023559572, Training Accuracy: 16.0125\n",
            "Validation Loss: 3.5912161389733575, Validation Accuracy: 16.26\n",
            "[13/150]: Training Loss: 3.5116733703613283, Training Accuracy: 17.3775\n",
            "Validation Loss: 3.5196323546634356, Validation Accuracy: 16.95\n",
            "[14/150]: Training Loss: 3.4402521675109865, Training Accuracy: 18.835\n",
            "Validation Loss: 3.4998342930131656, Validation Accuracy: 17.78\n",
            "[15/150]: Training Loss: 3.374294019317627, Training Accuracy: 19.835\n",
            "Validation Loss: 3.412280123704558, Validation Accuracy: 19.05\n",
            "[16/150]: Training Loss: 3.3126377914428713, Training Accuracy: 20.9225\n",
            "Validation Loss: 3.3567575497232425, Validation Accuracy: 20.21\n",
            "[17/150]: Training Loss: 3.264142190551758, Training Accuracy: 21.7475\n",
            "Validation Loss: 3.3158142976700122, Validation Accuracy: 20.54\n",
            "[18/150]: Training Loss: 3.212448757171631, Training Accuracy: 22.925\n",
            "Validation Loss: 3.288792388454364, Validation Accuracy: 21.5\n",
            "[19/150]: Training Loss: 3.16608261680603, Training Accuracy: 23.6325\n",
            "Validation Loss: 3.247776625262704, Validation Accuracy: 22.28\n",
            "[20/150]: Training Loss: 3.1185284183502198, Training Accuracy: 24.565\n",
            "Validation Loss: 3.2615917640127194, Validation Accuracy: 21.82\n",
            "[21/150]: Training Loss: 3.0692719429016115, Training Accuracy: 25.24\n",
            "Validation Loss: 3.188312981538712, Validation Accuracy: 23.06\n",
            "[22/150]: Training Loss: 3.0227535221099853, Training Accuracy: 26.3125\n",
            "Validation Loss: 3.189725389905796, Validation Accuracy: 23.3\n",
            "[23/150]: Training Loss: 2.9822758083343506, Training Accuracy: 26.9425\n",
            "Validation Loss: 3.1865678501736587, Validation Accuracy: 24.06\n",
            "[24/150]: Training Loss: 2.9383605140686035, Training Accuracy: 27.7575\n",
            "Validation Loss: 3.1210442075304163, Validation Accuracy: 25.28\n",
            "[25/150]: Training Loss: 2.8941220123291016, Training Accuracy: 28.8275\n",
            "Validation Loss: 3.0783458576080904, Validation Accuracy: 26.38\n",
            "[26/150]: Training Loss: 2.846640188598633, Training Accuracy: 29.6625\n",
            "Validation Loss: 3.0489486524253895, Validation Accuracy: 26.15\n",
            "[27/150]: Training Loss: 2.803463589859009, Training Accuracy: 30.53\n",
            "Validation Loss: 3.0650304320511546, Validation Accuracy: 26.3\n",
            "[28/150]: Training Loss: 2.7623793058395387, Training Accuracy: 31.4175\n",
            "Validation Loss: 3.016696295161156, Validation Accuracy: 27.16\n",
            "[29/150]: Training Loss: 2.724277519607544, Training Accuracy: 31.9075\n",
            "Validation Loss: 3.001434786304547, Validation Accuracy: 27.8\n",
            "[30/150]: Training Loss: 2.6789874454498293, Training Accuracy: 33.0275\n",
            "Validation Loss: 3.016883801502787, Validation Accuracy: 27.3\n",
            "[31/150]: Training Loss: 2.639483213806152, Training Accuracy: 33.66\n",
            "Validation Loss: 2.98876147361318, Validation Accuracy: 28.67\n",
            "[32/150]: Training Loss: 2.5927667430877683, Training Accuracy: 34.5325\n",
            "Validation Loss: 2.994789396881298, Validation Accuracy: 27.54\n",
            "[33/150]: Training Loss: 2.5531089670181273, Training Accuracy: 35.2525\n",
            "Validation Loss: 2.997880709399084, Validation Accuracy: 28.54\n",
            "[34/150]: Training Loss: 2.5080887552261353, Training Accuracy: 36.16\n",
            "Validation Loss: 2.9557720460709493, Validation Accuracy: 29.05\n",
            "[35/150]: Training Loss: 2.470765545463562, Training Accuracy: 37.03\n",
            "Validation Loss: 2.960252163516488, Validation Accuracy: 29.2\n",
            "[36/150]: Training Loss: 2.4253519346237185, Training Accuracy: 37.9225\n",
            "Validation Loss: 2.9324972644733016, Validation Accuracy: 29.32\n",
            "[37/150]: Training Loss: 2.3800090463638304, Training Accuracy: 39.06\n",
            "Validation Loss: 2.9341223923264037, Validation Accuracy: 30.21\n",
            "[38/150]: Training Loss: 2.3413858253479005, Training Accuracy: 39.57\n",
            "Validation Loss: 2.9640257130762575, Validation Accuracy: 29.88\n",
            "[39/150]: Training Loss: 2.301597819519043, Training Accuracy: 40.4175\n",
            "Validation Loss: 3.0218158800890493, Validation Accuracy: 28.21\n",
            "[40/150]: Training Loss: 2.2570854791641235, Training Accuracy: 41.3725\n",
            "Validation Loss: 3.0180870347721562, Validation Accuracy: 29.27\n",
            "[41/150]: Training Loss: 2.2198365371704103, Training Accuracy: 42.375\n",
            "Validation Loss: 2.938889775306556, Validation Accuracy: 30.41\n",
            "[42/150]: Training Loss: 2.1788083906173705, Training Accuracy: 43.24\n",
            "Validation Loss: 2.931537726882157, Validation Accuracy: 30.24\n",
            "[43/150]: Training Loss: 2.130757416725159, Training Accuracy: 44.2975\n",
            "Validation Loss: 2.935111481672639, Validation Accuracy: 30.94\n",
            "[44/150]: Training Loss: 2.0930950580596925, Training Accuracy: 45.0275\n",
            "Validation Loss: 2.9872301232283283, Validation Accuracy: 30.88\n",
            "[45/150]: Training Loss: 2.0451603315353393, Training Accuracy: 46.14\n",
            "Validation Loss: 2.9818537857881777, Validation Accuracy: 30.2\n",
            "[46/150]: Training Loss: 2.004084638786316, Training Accuracy: 47.0275\n",
            "Validation Loss: 2.995785168022107, Validation Accuracy: 30.23\n",
            "[47/150]: Training Loss: 1.9692718086242675, Training Accuracy: 47.845\n",
            "Validation Loss: 3.0542795536624396, Validation Accuracy: 30.14\n",
            "[48/150]: Training Loss: 1.9216952280044555, Training Accuracy: 48.8675\n",
            "Validation Loss: 2.9834766873888148, Validation Accuracy: 31.12\n",
            "[49/150]: Training Loss: 1.8784988208770752, Training Accuracy: 49.9625\n",
            "Validation Loss: 3.0277192516691365, Validation Accuracy: 31.11\n",
            "[50/150]: Training Loss: 1.8381427211761474, Training Accuracy: 50.9275\n",
            "Validation Loss: 3.062338437244391, Validation Accuracy: 31.08\n",
            "[51/150]: Training Loss: 1.7901163187026978, Training Accuracy: 52.055\n",
            "Validation Loss: 3.1062804225144114, Validation Accuracy: 30.8\n",
            "[52/150]: Training Loss: 1.7526374937057496, Training Accuracy: 52.695\n",
            "Validation Loss: 3.121828522651818, Validation Accuracy: 31.1\n",
            "[53/150]: Training Loss: 1.7035991048812866, Training Accuracy: 53.87\n",
            "Validation Loss: 3.1324675629852683, Validation Accuracy: 30.77\n",
            "[54/150]: Training Loss: 1.6695509649276734, Training Accuracy: 54.6\n",
            "Validation Loss: 3.1344476976212423, Validation Accuracy: 30.74\n",
            "[55/150]: Training Loss: 1.6191298002243042, Training Accuracy: 55.795\n",
            "Validation Loss: 3.2295576159361823, Validation Accuracy: 29.87\n",
            "[56/150]: Training Loss: 1.5762285459518433, Training Accuracy: 56.7875\n",
            "Validation Loss: 3.242111048121361, Validation Accuracy: 30.56\n",
            "[57/150]: Training Loss: 1.5393764123916627, Training Accuracy: 58.005\n",
            "Validation Loss: 3.303940733526922, Validation Accuracy: 29.53\n",
            "[58/150]: Training Loss: 1.4886947209358214, Training Accuracy: 59.32\n",
            "Validation Loss: 3.306815612088343, Validation Accuracy: 30.63\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 12.757227435992782, Test Accuracy: 12.53\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>12.53</td></tr><tr><td>Test Loss</td><td>12.75723</td></tr><tr><td>Train Accuracy</td><td>59.32</td></tr><tr><td>Train Loss</td><td>1.48869</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/gw8sn909</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240611_235508-gw8sn909/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240611_235923-9i8aijvl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl' target=\"_blank\">learning_rate=0.001 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605354942321777, Training Accuracy: 1.0075\n",
            "Validation Loss: 4.604883777108162, Validation Accuracy: 0.9\n",
            "[2/150]: Training Loss: 4.602973918151855, Training Accuracy: 1.0475\n",
            "Validation Loss: 4.6016397293965525, Validation Accuracy: 0.9\n",
            "[3/150]: Training Loss: 4.596875128936768, Training Accuracy: 1.3425\n",
            "Validation Loss: 4.590152442834939, Validation Accuracy: 2.52\n",
            "[4/150]: Training Loss: 4.556022624969483, Training Accuracy: 2.9175\n",
            "Validation Loss: 4.472016604842653, Validation Accuracy: 3.55\n",
            "[5/150]: Training Loss: 4.280242394256592, Training Accuracy: 4.6625\n",
            "Validation Loss: 4.20257385673037, Validation Accuracy: 5.95\n",
            "[6/150]: Training Loss: 4.122820203781128, Training Accuracy: 6.5925\n",
            "Validation Loss: 4.098185727550725, Validation Accuracy: 6.72\n",
            "[7/150]: Training Loss: 4.0448949375152585, Training Accuracy: 7.655\n",
            "Validation Loss: 4.033748802865387, Validation Accuracy: 8.54\n",
            "[8/150]: Training Loss: 3.977578921508789, Training Accuracy: 9.06\n",
            "Validation Loss: 3.970679567118359, Validation Accuracy: 8.73\n",
            "[9/150]: Training Loss: 3.9144488010406495, Training Accuracy: 9.9875\n",
            "Validation Loss: 3.902484763200116, Validation Accuracy: 10.84\n",
            "[10/150]: Training Loss: 3.8504066452026366, Training Accuracy: 11.2025\n",
            "Validation Loss: 3.853308222096437, Validation Accuracy: 11.05\n",
            "[11/150]: Training Loss: 3.776761824798584, Training Accuracy: 12.605\n",
            "Validation Loss: 3.7683646602995076, Validation Accuracy: 13.16\n",
            "[12/150]: Training Loss: 3.6920017036437986, Training Accuracy: 14.09\n",
            "Validation Loss: 3.712872941023225, Validation Accuracy: 13.54\n",
            "[13/150]: Training Loss: 3.617317741394043, Training Accuracy: 15.2475\n",
            "Validation Loss: 3.619570322097487, Validation Accuracy: 14.95\n",
            "[14/150]: Training Loss: 3.5433271072387695, Training Accuracy: 16.7275\n",
            "Validation Loss: 3.5456171734317854, Validation Accuracy: 16.81\n",
            "[15/150]: Training Loss: 3.47977327041626, Training Accuracy: 17.6525\n",
            "Validation Loss: 3.503120666856219, Validation Accuracy: 17.12\n",
            "[16/150]: Training Loss: 3.4149503967285155, Training Accuracy: 18.85\n",
            "Validation Loss: 3.449874220380358, Validation Accuracy: 17.98\n",
            "[17/150]: Training Loss: 3.357447999191284, Training Accuracy: 20.3025\n",
            "Validation Loss: 3.3906334479143667, Validation Accuracy: 19.6\n",
            "[18/150]: Training Loss: 3.3055746353149416, Training Accuracy: 20.985\n",
            "Validation Loss: 3.34384480859064, Validation Accuracy: 20.15\n",
            "[19/150]: Training Loss: 3.249339769363403, Training Accuracy: 21.955\n",
            "Validation Loss: 3.3408105813773576, Validation Accuracy: 20.75\n",
            "[20/150]: Training Loss: 3.2052097175598147, Training Accuracy: 22.84\n",
            "Validation Loss: 3.3127595497544404, Validation Accuracy: 21.29\n",
            "[21/150]: Training Loss: 3.150541979598999, Training Accuracy: 23.8475\n",
            "Validation Loss: 3.267914258750381, Validation Accuracy: 22.13\n",
            "[22/150]: Training Loss: 3.1069109004974367, Training Accuracy: 24.64\n",
            "Validation Loss: 3.2093709350391557, Validation Accuracy: 23.03\n",
            "[23/150]: Training Loss: 3.0725105766296386, Training Accuracy: 25.155\n",
            "Validation Loss: 3.2081045861456805, Validation Accuracy: 23.23\n",
            "[24/150]: Training Loss: 3.0264828220367432, Training Accuracy: 26.165\n",
            "Validation Loss: 3.153175331225061, Validation Accuracy: 23.9\n",
            "[25/150]: Training Loss: 2.9784529972076417, Training Accuracy: 27.0575\n",
            "Validation Loss: 3.120233716478773, Validation Accuracy: 24.62\n",
            "[26/150]: Training Loss: 2.9407661106109617, Training Accuracy: 27.7025\n",
            "Validation Loss: 3.122152067293787, Validation Accuracy: 25.03\n",
            "[27/150]: Training Loss: 2.9034343135833742, Training Accuracy: 28.395\n",
            "Validation Loss: 3.062598638473802, Validation Accuracy: 25.75\n",
            "[28/150]: Training Loss: 2.859911437988281, Training Accuracy: 29.1425\n",
            "Validation Loss: 3.0851597072212558, Validation Accuracy: 26.24\n",
            "[29/150]: Training Loss: 2.825396254348755, Training Accuracy: 29.9575\n",
            "Validation Loss: 3.0798455939930713, Validation Accuracy: 25.39\n",
            "[30/150]: Training Loss: 2.783854722213745, Training Accuracy: 30.46\n",
            "Validation Loss: 2.994018077850342, Validation Accuracy: 27.4\n",
            "[31/150]: Training Loss: 2.7530001544952394, Training Accuracy: 31.445\n",
            "Validation Loss: 3.023551480785297, Validation Accuracy: 26.3\n",
            "[32/150]: Training Loss: 2.715887685775757, Training Accuracy: 31.8725\n",
            "Validation Loss: 2.966492583037941, Validation Accuracy: 27.56\n",
            "[33/150]: Training Loss: 2.673046026611328, Training Accuracy: 32.72\n",
            "Validation Loss: 2.9745204934648646, Validation Accuracy: 27.99\n",
            "[34/150]: Training Loss: 2.6407437208175657, Training Accuracy: 33.34\n",
            "Validation Loss: 2.942354439170497, Validation Accuracy: 29.0\n",
            "[35/150]: Training Loss: 2.5946556980133058, Training Accuracy: 34.7025\n",
            "Validation Loss: 2.930390673837844, Validation Accuracy: 28.88\n",
            "[36/150]: Training Loss: 2.560257712173462, Training Accuracy: 35.2025\n",
            "Validation Loss: 2.9051667292406607, Validation Accuracy: 29.88\n",
            "[37/150]: Training Loss: 2.5238379081726072, Training Accuracy: 35.76\n",
            "Validation Loss: 2.890098687190159, Validation Accuracy: 29.33\n",
            "[38/150]: Training Loss: 2.486947275352478, Training Accuracy: 36.69\n",
            "Validation Loss: 2.9050150206134577, Validation Accuracy: 29.6\n",
            "[39/150]: Training Loss: 2.452709559249878, Training Accuracy: 37.475\n",
            "Validation Loss: 2.8899250182376544, Validation Accuracy: 29.65\n",
            "[40/150]: Training Loss: 2.4208646841049193, Training Accuracy: 37.9575\n",
            "Validation Loss: 2.9285842643421924, Validation Accuracy: 29.49\n",
            "[41/150]: Training Loss: 2.3780534410476686, Training Accuracy: 38.925\n",
            "Validation Loss: 2.896095122501349, Validation Accuracy: 30.27\n",
            "[42/150]: Training Loss: 2.3445595056533812, Training Accuracy: 39.4975\n",
            "Validation Loss: 2.903249968389037, Validation Accuracy: 29.49\n",
            "[43/150]: Training Loss: 2.3128848968505857, Training Accuracy: 40.2375\n",
            "Validation Loss: 2.8930906854617366, Validation Accuracy: 30.18\n",
            "[44/150]: Training Loss: 2.275460436248779, Training Accuracy: 40.965\n",
            "Validation Loss: 2.847154028096776, Validation Accuracy: 30.73\n",
            "[45/150]: Training Loss: 2.2384806400299073, Training Accuracy: 41.705\n",
            "Validation Loss: 2.898577917912963, Validation Accuracy: 30.18\n",
            "[46/150]: Training Loss: 2.214237283706665, Training Accuracy: 42.415\n",
            "Validation Loss: 2.8367908441337053, Validation Accuracy: 31.94\n",
            "[47/150]: Training Loss: 2.174029080581665, Training Accuracy: 43.2825\n",
            "Validation Loss: 2.8599718254842577, Validation Accuracy: 31.4\n",
            "[48/150]: Training Loss: 2.134048504257202, Training Accuracy: 44.0225\n",
            "Validation Loss: 2.8570211207031444, Validation Accuracy: 31.32\n",
            "[49/150]: Training Loss: 2.1011022747039796, Training Accuracy: 44.7675\n",
            "Validation Loss: 2.876888126324696, Validation Accuracy: 31.24\n",
            "[50/150]: Training Loss: 2.066174629211426, Training Accuracy: 45.84\n",
            "Validation Loss: 2.873898560833779, Validation Accuracy: 31.01\n",
            "[51/150]: Training Loss: 2.0310755935668947, Training Accuracy: 46.2775\n",
            "Validation Loss: 2.8654664003165666, Validation Accuracy: 31.98\n",
            "[52/150]: Training Loss: 1.9909400806427002, Training Accuracy: 47.45\n",
            "Validation Loss: 2.8746210936528103, Validation Accuracy: 31.69\n",
            "[53/150]: Training Loss: 1.9579231163024902, Training Accuracy: 48.0525\n",
            "Validation Loss: 2.8748430902031576, Validation Accuracy: 31.57\n",
            "[54/150]: Training Loss: 1.9230639179229736, Training Accuracy: 48.785\n",
            "Validation Loss: 2.910501542364716, Validation Accuracy: 31.56\n",
            "[55/150]: Training Loss: 1.8876561931610107, Training Accuracy: 49.645\n",
            "Validation Loss: 2.8805068404811203, Validation Accuracy: 32.02\n",
            "[56/150]: Training Loss: 1.8495587772369384, Training Accuracy: 50.57\n",
            "Validation Loss: 2.9215301510634695, Validation Accuracy: 31.68\n",
            "[57/150]: Training Loss: 1.8102108228683471, Training Accuracy: 51.2975\n",
            "Validation Loss: 2.926473318391545, Validation Accuracy: 32.35\n",
            "[58/150]: Training Loss: 1.7818908670425415, Training Accuracy: 52.1325\n",
            "Validation Loss: 2.9246792929947, Validation Accuracy: 32.57\n",
            "[59/150]: Training Loss: 1.7456034435272216, Training Accuracy: 52.775\n",
            "Validation Loss: 2.984062427168439, Validation Accuracy: 31.53\n",
            "[60/150]: Training Loss: 1.706142727279663, Training Accuracy: 53.955\n",
            "Validation Loss: 2.9781496843714623, Validation Accuracy: 32.57\n",
            "[61/150]: Training Loss: 1.6744635740280152, Training Accuracy: 54.59\n",
            "Validation Loss: 3.0356672569444982, Validation Accuracy: 31.58\n",
            "[62/150]: Training Loss: 1.6349090488433837, Training Accuracy: 55.5975\n",
            "Validation Loss: 3.0081513763233354, Validation Accuracy: 32.09\n",
            "[63/150]: Training Loss: 1.6007863130569457, Training Accuracy: 56.41\n",
            "Validation Loss: 3.032350327558578, Validation Accuracy: 32.3\n",
            "[64/150]: Training Loss: 1.5609112140655517, Training Accuracy: 57.235\n",
            "Validation Loss: 3.077729872077893, Validation Accuracy: 31.42\n",
            "[65/150]: Training Loss: 1.5253083936691285, Training Accuracy: 58.1475\n",
            "Validation Loss: 3.0732312840261278, Validation Accuracy: 32.72\n",
            "[66/150]: Training Loss: 1.4928287380218506, Training Accuracy: 59.1925\n",
            "Validation Loss: 3.0616249688871346, Validation Accuracy: 31.97\n",
            "[67/150]: Training Loss: 1.4535140877723693, Training Accuracy: 60.1225\n",
            "Validation Loss: 3.124178699627044, Validation Accuracy: 32.15\n",
            "[68/150]: Training Loss: 1.4196137544631957, Training Accuracy: 60.7675\n",
            "Validation Loss: 3.2008153435530935, Validation Accuracy: 31.3\n",
            "[69/150]: Training Loss: 1.3877691086769104, Training Accuracy: 61.4575\n",
            "Validation Loss: 3.1977471834535054, Validation Accuracy: 31.22\n",
            "[70/150]: Training Loss: 1.3564362874031066, Training Accuracy: 62.2125\n",
            "Validation Loss: 3.2483551836317512, Validation Accuracy: 31.49\n",
            "[71/150]: Training Loss: 1.3180213891029358, Training Accuracy: 63.6275\n",
            "Validation Loss: 3.3253093616218323, Validation Accuracy: 31.54\n",
            "[72/150]: Training Loss: 1.2782609523773194, Training Accuracy: 64.6325\n",
            "Validation Loss: 3.300645703722717, Validation Accuracy: 32.03\n",
            "[73/150]: Training Loss: 1.2435202094078064, Training Accuracy: 65.5975\n",
            "Validation Loss: 3.340803805430224, Validation Accuracy: 31.19\n",
            "[74/150]: Training Loss: 1.2111891516685487, Training Accuracy: 66.105\n",
            "Validation Loss: 3.3482626076716526, Validation Accuracy: 31.98\n",
            "[75/150]: Training Loss: 1.1782402634620666, Training Accuracy: 67.3425\n",
            "Validation Loss: 3.442917116128715, Validation Accuracy: 31.68\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 15.762403002210483, Test Accuracy: 12.36\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>12.36</td></tr><tr><td>Test Loss</td><td>15.7624</td></tr><tr><td>Train Accuracy</td><td>67.3425</td></tr><tr><td>Train Loss</td><td>1.17824</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/9i8aijvl</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240611_235923-9i8aijvl/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_002202-8lcpcbs2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2' target=\"_blank\">learning_rate=0.001 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605505518341064, Training Accuracy: 1.0675\n",
            "Validation Loss: 4.60469646514601, Validation Accuracy: 0.9\n",
            "[2/150]: Training Loss: 4.603742761993408, Training Accuracy: 1.0775\n",
            "Validation Loss: 4.602513228252435, Validation Accuracy: 0.97\n",
            "[3/150]: Training Loss: 4.60020831451416, Training Accuracy: 1.42\n",
            "Validation Loss: 4.596951505940432, Validation Accuracy: 1.39\n",
            "[4/150]: Training Loss: 4.590583048248291, Training Accuracy: 1.345\n",
            "Validation Loss: 4.579934718502555, Validation Accuracy: 1.81\n",
            "[5/150]: Training Loss: 4.522595316314697, Training Accuracy: 2.9\n",
            "Validation Loss: 4.390095902096694, Validation Accuracy: 3.43\n",
            "[6/150]: Training Loss: 4.228593465805054, Training Accuracy: 5.2625\n",
            "Validation Loss: 4.173950313762495, Validation Accuracy: 5.48\n",
            "[7/150]: Training Loss: 4.1090137573242185, Training Accuracy: 6.88\n",
            "Validation Loss: 4.087003700292794, Validation Accuracy: 7.44\n",
            "[8/150]: Training Loss: 4.035317427825928, Training Accuracy: 8.06\n",
            "Validation Loss: 4.015521998618059, Validation Accuracy: 8.52\n",
            "[9/150]: Training Loss: 3.960421589279175, Training Accuracy: 9.445\n",
            "Validation Loss: 3.9580009849208175, Validation Accuracy: 9.3\n",
            "[10/150]: Training Loss: 3.8591443000793455, Training Accuracy: 11.34\n",
            "Validation Loss: 3.84393062105604, Validation Accuracy: 11.61\n",
            "[11/150]: Training Loss: 3.7639447425842287, Training Accuracy: 12.9725\n",
            "Validation Loss: 3.7741037219952625, Validation Accuracy: 12.76\n",
            "[12/150]: Training Loss: 3.6964004222869873, Training Accuracy: 14.225\n",
            "Validation Loss: 3.6935869903321477, Validation Accuracy: 14.02\n",
            "[13/150]: Training Loss: 3.6246248401641847, Training Accuracy: 15.4325\n",
            "Validation Loss: 3.6313245053503924, Validation Accuracy: 15.35\n",
            "[14/150]: Training Loss: 3.5586987380981445, Training Accuracy: 16.58\n",
            "Validation Loss: 3.570704687932494, Validation Accuracy: 16.63\n",
            "[15/150]: Training Loss: 3.492145662689209, Training Accuracy: 17.56\n",
            "Validation Loss: 3.5174384117126465, Validation Accuracy: 17.05\n",
            "[16/150]: Training Loss: 3.4223767150878905, Training Accuracy: 18.875\n",
            "Validation Loss: 3.4360159157188077, Validation Accuracy: 18.52\n",
            "[17/150]: Training Loss: 3.3623727890014647, Training Accuracy: 19.63\n",
            "Validation Loss: 3.3825773327213944, Validation Accuracy: 19.78\n",
            "[18/150]: Training Loss: 3.3074194034576414, Training Accuracy: 20.8975\n",
            "Validation Loss: 3.3404667316728336, Validation Accuracy: 20.19\n",
            "[19/150]: Training Loss: 3.251063732147217, Training Accuracy: 21.8975\n",
            "Validation Loss: 3.312105529627223, Validation Accuracy: 20.9\n",
            "[20/150]: Training Loss: 3.204967420578003, Training Accuracy: 22.7225\n",
            "Validation Loss: 3.2530917027953326, Validation Accuracy: 22.31\n",
            "[21/150]: Training Loss: 3.15532322807312, Training Accuracy: 23.64\n",
            "Validation Loss: 3.229210142876692, Validation Accuracy: 22.45\n",
            "[22/150]: Training Loss: 3.1103267768859864, Training Accuracy: 24.38\n",
            "Validation Loss: 3.213347544336015, Validation Accuracy: 23.38\n",
            "[23/150]: Training Loss: 3.0626929641723635, Training Accuracy: 25.55\n",
            "Validation Loss: 3.181753307391124, Validation Accuracy: 24.04\n",
            "[24/150]: Training Loss: 3.01508925819397, Training Accuracy: 26.1675\n",
            "Validation Loss: 3.159736381214895, Validation Accuracy: 24.07\n",
            "[25/150]: Training Loss: 2.977297193527222, Training Accuracy: 26.95\n",
            "Validation Loss: 3.139521881273598, Validation Accuracy: 24.65\n",
            "[26/150]: Training Loss: 2.9348321487426756, Training Accuracy: 27.76\n",
            "Validation Loss: 3.129030877617514, Validation Accuracy: 25.02\n",
            "[27/150]: Training Loss: 2.892203674316406, Training Accuracy: 28.71\n",
            "Validation Loss: 3.1089744279339055, Validation Accuracy: 24.88\n",
            "[28/150]: Training Loss: 2.8514965145111084, Training Accuracy: 29.41\n",
            "Validation Loss: 3.0390360476864373, Validation Accuracy: 26.37\n",
            "[29/150]: Training Loss: 2.8064930957794187, Training Accuracy: 30.3275\n",
            "Validation Loss: 3.0866621254356046, Validation Accuracy: 26.25\n",
            "[30/150]: Training Loss: 2.771508701324463, Training Accuracy: 30.8325\n",
            "Validation Loss: 3.034489528388734, Validation Accuracy: 26.79\n",
            "[31/150]: Training Loss: 2.726985428237915, Training Accuracy: 32.06\n",
            "Validation Loss: 2.994590279403006, Validation Accuracy: 27.49\n",
            "[32/150]: Training Loss: 2.693068378448486, Training Accuracy: 32.6375\n",
            "Validation Loss: 2.991791993949064, Validation Accuracy: 27.57\n",
            "[33/150]: Training Loss: 2.6538521224975584, Training Accuracy: 33.305\n",
            "Validation Loss: 2.9930253712234984, Validation Accuracy: 27.59\n",
            "[34/150]: Training Loss: 2.6168819108963013, Training Accuracy: 33.9675\n",
            "Validation Loss: 2.9458029012011875, Validation Accuracy: 28.63\n",
            "[35/150]: Training Loss: 2.5724296060562133, Training Accuracy: 34.885\n",
            "Validation Loss: 2.9706625437280936, Validation Accuracy: 28.37\n",
            "[36/150]: Training Loss: 2.532205513381958, Training Accuracy: 35.7275\n",
            "Validation Loss: 3.0205048026552626, Validation Accuracy: 27.62\n",
            "[37/150]: Training Loss: 2.501378486442566, Training Accuracy: 36.655\n",
            "Validation Loss: 2.9459367466580337, Validation Accuracy: 29.16\n",
            "[38/150]: Training Loss: 2.4599771045684813, Training Accuracy: 37.265\n",
            "Validation Loss: 2.9162613555883907, Validation Accuracy: 29.42\n",
            "[39/150]: Training Loss: 2.416532469558716, Training Accuracy: 38.265\n",
            "Validation Loss: 2.9226647676176327, Validation Accuracy: 29.71\n",
            "[40/150]: Training Loss: 2.377360425758362, Training Accuracy: 39.1\n",
            "Validation Loss: 2.912419487716286, Validation Accuracy: 30.24\n",
            "[41/150]: Training Loss: 2.3416698053359983, Training Accuracy: 39.7725\n",
            "Validation Loss: 2.9336505536061184, Validation Accuracy: 29.72\n",
            "[42/150]: Training Loss: 2.302257305908203, Training Accuracy: 40.6025\n",
            "Validation Loss: 2.9164519552971906, Validation Accuracy: 30.29\n",
            "[43/150]: Training Loss: 2.2648201442718507, Training Accuracy: 41.4425\n",
            "Validation Loss: 2.917926338068239, Validation Accuracy: 30.33\n",
            "[44/150]: Training Loss: 2.2238695373535156, Training Accuracy: 42.1875\n",
            "Validation Loss: 2.920909085091512, Validation Accuracy: 30.59\n",
            "[45/150]: Training Loss: 2.186175981903076, Training Accuracy: 43.1875\n",
            "Validation Loss: 2.979639395027404, Validation Accuracy: 29.65\n",
            "[46/150]: Training Loss: 2.1421424005508425, Training Accuracy: 44.26\n",
            "Validation Loss: 2.955421671745883, Validation Accuracy: 29.75\n",
            "[47/150]: Training Loss: 2.1072659519195556, Training Accuracy: 44.7325\n",
            "Validation Loss: 2.9603501687383957, Validation Accuracy: 30.38\n",
            "[48/150]: Training Loss: 2.0704134420394897, Training Accuracy: 45.5625\n",
            "Validation Loss: 2.9289260366160397, Validation Accuracy: 30.82\n",
            "[49/150]: Training Loss: 2.0253924531936645, Training Accuracy: 46.8725\n",
            "Validation Loss: 2.9537550051500845, Validation Accuracy: 30.64\n",
            "[50/150]: Training Loss: 1.9894214233398437, Training Accuracy: 47.5425\n",
            "Validation Loss: 2.992744015280608, Validation Accuracy: 30.54\n",
            "[51/150]: Training Loss: 1.9478775398254395, Training Accuracy: 48.1625\n",
            "Validation Loss: 3.0440484505550116, Validation Accuracy: 30.87\n",
            "[52/150]: Training Loss: 1.9146845523834228, Training Accuracy: 48.9825\n",
            "Validation Loss: 2.9890038989911414, Validation Accuracy: 31.24\n",
            "[53/150]: Training Loss: 1.8723485668182374, Training Accuracy: 50.01\n",
            "Validation Loss: 2.993290390937951, Validation Accuracy: 31.22\n",
            "[54/150]: Training Loss: 1.8348793195724487, Training Accuracy: 50.52\n",
            "Validation Loss: 3.0035920021640266, Validation Accuracy: 31.61\n",
            "[55/150]: Training Loss: 1.7904879375457763, Training Accuracy: 52.175\n",
            "Validation Loss: 3.0537699574877504, Validation Accuracy: 30.68\n",
            "[56/150]: Training Loss: 1.7518022777557374, Training Accuracy: 52.58\n",
            "Validation Loss: 3.0826165934277188, Validation Accuracy: 30.36\n",
            "[57/150]: Training Loss: 1.723713356781006, Training Accuracy: 53.2725\n",
            "Validation Loss: 3.0921939679771473, Validation Accuracy: 30.79\n",
            "[58/150]: Training Loss: 1.673542138671875, Training Accuracy: 54.7525\n",
            "Validation Loss: 3.114953031965122, Validation Accuracy: 30.97\n",
            "[59/150]: Training Loss: 1.6370127866744995, Training Accuracy: 55.3475\n",
            "Validation Loss: 3.1347598710637183, Validation Accuracy: 31.13\n",
            "[60/150]: Training Loss: 1.595728307723999, Training Accuracy: 56.3775\n",
            "Validation Loss: 3.211446317138186, Validation Accuracy: 30.86\n",
            "[61/150]: Training Loss: 1.5595929719924926, Training Accuracy: 57.445\n",
            "Validation Loss: 3.1916901260424573, Validation Accuracy: 31.69\n",
            "[62/150]: Training Loss: 1.5153404804229735, Training Accuracy: 58.605\n",
            "Validation Loss: 3.2764444821959087, Validation Accuracy: 30.21\n",
            "[63/150]: Training Loss: 1.4795546808242799, Training Accuracy: 59.165\n",
            "Validation Loss: 3.2651574581292024, Validation Accuracy: 31.32\n",
            "[64/150]: Training Loss: 1.443065357875824, Training Accuracy: 60.3525\n",
            "Validation Loss: 3.2850031807164477, Validation Accuracy: 30.88\n",
            "[65/150]: Training Loss: 1.3952457666397096, Training Accuracy: 61.3125\n",
            "Validation Loss: 3.334014499263399, Validation Accuracy: 30.51\n",
            "[66/150]: Training Loss: 1.3616252402305602, Training Accuracy: 62.2875\n",
            "Validation Loss: 3.3781587804199025, Validation Accuracy: 30.85\n",
            "[67/150]: Training Loss: 1.3209700021743775, Training Accuracy: 63.1475\n",
            "Validation Loss: 3.4392695958447304, Validation Accuracy: 30.92\n",
            "[68/150]: Training Loss: 1.2754105567932128, Training Accuracy: 64.4\n",
            "Validation Loss: 3.487312679837464, Validation Accuracy: 30.89\n",
            "[69/150]: Training Loss: 1.2461662047386168, Training Accuracy: 65.2125\n",
            "Validation Loss: 3.470762025019166, Validation Accuracy: 30.79\n",
            "[70/150]: Training Loss: 1.2068843828201294, Training Accuracy: 66.345\n",
            "Validation Loss: 3.5463279022532666, Validation Accuracy: 30.57\n",
            "[71/150]: Training Loss: 1.174416847038269, Training Accuracy: 66.87\n",
            "Validation Loss: 3.5970414428953914, Validation Accuracy: 30.5\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 17.28561769473325, Test Accuracy: 11.15\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>11.15</td></tr><tr><td>Test Loss</td><td>17.28562</td></tr><tr><td>Train Accuracy</td><td>66.87</td></tr><tr><td>Train Loss</td><td>1.17442</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/8lcpcbs2</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_002202-8lcpcbs2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.0001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_002809-yufpefeq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq' target=\"_blank\">learning_rate=0.01 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.394053651046753, Training Accuracy: 3.235\n",
            "Validation Loss: 4.1179144488778086, Validation Accuracy: 6.51\n",
            "[2/150]: Training Loss: 3.940752759170532, Training Accuracy: 9.1825\n",
            "Validation Loss: 3.793641860318032, Validation Accuracy: 10.96\n",
            "[3/150]: Training Loss: 3.607163610458374, Training Accuracy: 14.85\n",
            "Validation Loss: 3.445422892357893, Validation Accuracy: 18.4\n",
            "[4/150]: Training Loss: 3.32619926071167, Training Accuracy: 19.315\n",
            "Validation Loss: 3.202780070578217, Validation Accuracy: 21.55\n",
            "[5/150]: Training Loss: 3.0974891372680666, Training Accuracy: 23.9725\n",
            "Validation Loss: 3.0388081802684033, Validation Accuracy: 24.97\n",
            "[6/150]: Training Loss: 2.901975968170166, Training Accuracy: 27.42\n",
            "Validation Loss: 3.0498315498327755, Validation Accuracy: 25.89\n",
            "[7/150]: Training Loss: 2.72861491394043, Training Accuracy: 31.1425\n",
            "Validation Loss: 2.817266490049423, Validation Accuracy: 30.26\n",
            "[8/150]: Training Loss: 2.565713974761963, Training Accuracy: 33.8575\n",
            "Validation Loss: 2.7873742094465124, Validation Accuracy: 30.1\n",
            "[9/150]: Training Loss: 2.394580401802063, Training Accuracy: 37.745\n",
            "Validation Loss: 2.714815883879449, Validation Accuracy: 32.43\n",
            "[10/150]: Training Loss: 2.2325665201187133, Training Accuracy: 41.275\n",
            "Validation Loss: 2.6796328748107716, Validation Accuracy: 33.37\n",
            "[11/150]: Training Loss: 2.0732334920883178, Training Accuracy: 44.825\n",
            "Validation Loss: 2.743416508291937, Validation Accuracy: 33.16\n",
            "[12/150]: Training Loss: 1.9020709432601928, Training Accuracy: 48.52\n",
            "Validation Loss: 2.761919692823082, Validation Accuracy: 34.09\n",
            "[13/150]: Training Loss: 1.723027162361145, Training Accuracy: 52.47\n",
            "Validation Loss: 2.781366317894808, Validation Accuracy: 33.87\n",
            "[14/150]: Training Loss: 1.5519161633491516, Training Accuracy: 56.5925\n",
            "Validation Loss: 2.8901124114443544, Validation Accuracy: 32.73\n",
            "[15/150]: Training Loss: 1.3848727501869202, Training Accuracy: 60.6225\n",
            "Validation Loss: 3.061296709024223, Validation Accuracy: 32.5\n",
            "[16/150]: Training Loss: 1.2385452840805053, Training Accuracy: 64.0375\n",
            "Validation Loss: 3.1685607524434474, Validation Accuracy: 33.0\n",
            "[17/150]: Training Loss: 1.0766600145339966, Training Accuracy: 68.1675\n",
            "Validation Loss: 3.470290554556877, Validation Accuracy: 32.52\n",
            "[18/150]: Training Loss: 0.953560617685318, Training Accuracy: 71.2375\n",
            "Validation Loss: 3.737919813508441, Validation Accuracy: 32.63\n",
            "[19/150]: Training Loss: 0.8452390188217163, Training Accuracy: 74.1975\n",
            "Validation Loss: 3.9338995074010956, Validation Accuracy: 32.13\n",
            "[20/150]: Training Loss: 0.7454726006031036, Training Accuracy: 76.935\n",
            "Validation Loss: 4.16999766325495, Validation Accuracy: 32.08\n",
            "[21/150]: Training Loss: 0.6607641342639923, Training Accuracy: 79.455\n",
            "Validation Loss: 4.340858131457287, Validation Accuracy: 31.19\n",
            "[22/150]: Training Loss: 0.5830736963987351, Training Accuracy: 81.7975\n",
            "Validation Loss: 4.7213960696177875, Validation Accuracy: 32.35\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 31.791706814128123, Test Accuracy: 14.4\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>14.4</td></tr><tr><td>Test Loss</td><td>31.79171</td></tr><tr><td>Train Accuracy</td><td>81.7975</td></tr><tr><td>Train Loss</td><td>0.58307</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/yufpefeq</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_002809-yufpefeq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_003027-5ijvpyn3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3' target=\"_blank\">learning_rate=0.01 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.452998904800415, Training Accuracy: 2.615\n",
            "Validation Loss: 4.1346032179085315, Validation Accuracy: 5.28\n",
            "[2/150]: Training Loss: 3.9527258255004885, Training Accuracy: 8.88\n",
            "Validation Loss: 3.7435999432946465, Validation Accuracy: 11.55\n",
            "[3/150]: Training Loss: 3.6381911922454835, Training Accuracy: 14.3725\n",
            "Validation Loss: 3.50478922181828, Validation Accuracy: 15.58\n",
            "[4/150]: Training Loss: 3.3785652015686036, Training Accuracy: 18.865\n",
            "Validation Loss: 3.3451961089091697, Validation Accuracy: 19.0\n",
            "[5/150]: Training Loss: 3.17810930519104, Training Accuracy: 22.375\n",
            "Validation Loss: 3.1223374703887163, Validation Accuracy: 23.6\n",
            "[6/150]: Training Loss: 3.0013810291290284, Training Accuracy: 25.565\n",
            "Validation Loss: 3.015145599462424, Validation Accuracy: 25.56\n",
            "[7/150]: Training Loss: 2.8544215950012206, Training Accuracy: 28.7175\n",
            "Validation Loss: 2.917889818264421, Validation Accuracy: 28.04\n",
            "[8/150]: Training Loss: 2.6971351528167724, Training Accuracy: 31.3825\n",
            "Validation Loss: 2.8436176351680875, Validation Accuracy: 29.6\n",
            "[9/150]: Training Loss: 2.567851602554321, Training Accuracy: 34.1025\n",
            "Validation Loss: 2.748332465530201, Validation Accuracy: 31.45\n",
            "[10/150]: Training Loss: 2.422238304901123, Training Accuracy: 37.085\n",
            "Validation Loss: 2.7193879473740887, Validation Accuracy: 32.08\n",
            "[11/150]: Training Loss: 2.3000989530563354, Training Accuracy: 39.695\n",
            "Validation Loss: 2.736810536141608, Validation Accuracy: 32.35\n",
            "[12/150]: Training Loss: 2.170335920906067, Training Accuracy: 42.5\n",
            "Validation Loss: 2.668315727239961, Validation Accuracy: 34.0\n",
            "[13/150]: Training Loss: 2.037977534675598, Training Accuracy: 45.47\n",
            "Validation Loss: 2.6606684786498924, Validation Accuracy: 34.72\n",
            "[14/150]: Training Loss: 1.9109795570373536, Training Accuracy: 48.245\n",
            "Validation Loss: 2.677434118690005, Validation Accuracy: 34.85\n",
            "[15/150]: Training Loss: 1.7840767404556275, Training Accuracy: 50.9475\n",
            "Validation Loss: 2.6786925458604363, Validation Accuracy: 35.46\n",
            "[16/150]: Training Loss: 1.6501886499404907, Training Accuracy: 54.325\n",
            "Validation Loss: 2.749260063383989, Validation Accuracy: 34.73\n",
            "[17/150]: Training Loss: 1.5220398645401, Training Accuracy: 57.045\n",
            "Validation Loss: 2.9064030396710536, Validation Accuracy: 34.13\n",
            "[18/150]: Training Loss: 1.3921052120208741, Training Accuracy: 60.4275\n",
            "Validation Loss: 2.978949681968446, Validation Accuracy: 33.67\n",
            "[19/150]: Training Loss: 1.2853214281082153, Training Accuracy: 62.9225\n",
            "Validation Loss: 3.0623086941470006, Validation Accuracy: 33.54\n",
            "[20/150]: Training Loss: 1.1686985822677611, Training Accuracy: 65.5875\n",
            "Validation Loss: 3.099342126755198, Validation Accuracy: 34.34\n",
            "[21/150]: Training Loss: 1.0535273086547852, Training Accuracy: 68.77\n",
            "Validation Loss: 3.2730220791640554, Validation Accuracy: 32.71\n",
            "[22/150]: Training Loss: 0.9473227613449097, Training Accuracy: 71.6425\n",
            "Validation Loss: 3.5567993069909942, Validation Accuracy: 33.18\n",
            "[23/150]: Training Loss: 0.8736372189521789, Training Accuracy: 73.5275\n",
            "Validation Loss: 3.4931609501504592, Validation Accuracy: 33.71\n",
            "[24/150]: Training Loss: 0.7797131684780121, Training Accuracy: 76.18\n",
            "Validation Loss: 3.6084122536288703, Validation Accuracy: 31.79\n",
            "[25/150]: Training Loss: 0.72144877743721, Training Accuracy: 77.925\n",
            "Validation Loss: 3.6706639156220064, Validation Accuracy: 33.34\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 17.253872677019448, Test Accuracy: 18.45\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>18.45</td></tr><tr><td>Test Loss</td><td>17.25387</td></tr><tr><td>Train Accuracy</td><td>77.925</td></tr><tr><td>Train Loss</td><td>0.72145</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/5ijvpyn3</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_003027-5ijvpyn3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_003346-pvvd2my8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8' target=\"_blank\">learning_rate=0.01 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.436210793685913, Training Accuracy: 2.8025\n",
            "Validation Loss: 4.1512272403498365, Validation Accuracy: 5.38\n",
            "[2/150]: Training Loss: 3.943741687011719, Training Accuracy: 9.175\n",
            "Validation Loss: 3.791590502307673, Validation Accuracy: 11.54\n",
            "[3/150]: Training Loss: 3.6135140613555907, Training Accuracy: 14.57\n",
            "Validation Loss: 3.473675949558331, Validation Accuracy: 17.5\n",
            "[4/150]: Training Loss: 3.3465395004272462, Training Accuracy: 19.22\n",
            "Validation Loss: 3.3301091406755385, Validation Accuracy: 19.19\n",
            "[5/150]: Training Loss: 3.1310076709747316, Training Accuracy: 22.94\n",
            "Validation Loss: 3.0678081117617855, Validation Accuracy: 24.65\n",
            "[6/150]: Training Loss: 2.9402838905334474, Training Accuracy: 26.4875\n",
            "Validation Loss: 2.9766739310732313, Validation Accuracy: 25.68\n",
            "[7/150]: Training Loss: 2.7704892574310302, Training Accuracy: 29.855\n",
            "Validation Loss: 2.8771827342403924, Validation Accuracy: 28.79\n",
            "[8/150]: Training Loss: 2.6034393648147582, Training Accuracy: 33.84\n",
            "Validation Loss: 2.7646109807263515, Validation Accuracy: 31.02\n",
            "[9/150]: Training Loss: 2.436837389945984, Training Accuracy: 37.1175\n",
            "Validation Loss: 2.731329696193622, Validation Accuracy: 32.37\n",
            "[10/150]: Training Loss: 2.273189482879639, Training Accuracy: 40.3275\n",
            "Validation Loss: 2.663352646645467, Validation Accuracy: 33.62\n",
            "[11/150]: Training Loss: 2.1113974294662476, Training Accuracy: 44.0125\n",
            "Validation Loss: 2.70243866884025, Validation Accuracy: 33.48\n",
            "[12/150]: Training Loss: 1.9719651878356934, Training Accuracy: 46.755\n",
            "Validation Loss: 2.7540456124931385, Validation Accuracy: 34.46\n",
            "[13/150]: Training Loss: 1.8135078485488891, Training Accuracy: 50.6725\n",
            "Validation Loss: 2.7649777801173507, Validation Accuracy: 34.67\n",
            "[14/150]: Training Loss: 1.663066688156128, Training Accuracy: 53.855\n",
            "Validation Loss: 2.8507347205641924, Validation Accuracy: 34.61\n",
            "[15/150]: Training Loss: 1.5029290641784667, Training Accuracy: 57.7575\n",
            "Validation Loss: 2.961932021341506, Validation Accuracy: 33.61\n",
            "[16/150]: Training Loss: 1.3492597907066346, Training Accuracy: 61.345\n",
            "Validation Loss: 3.0608204777833, Validation Accuracy: 33.71\n",
            "[17/150]: Training Loss: 1.2147304633140563, Training Accuracy: 64.7325\n",
            "Validation Loss: 3.1523648325804694, Validation Accuracy: 33.57\n",
            "[18/150]: Training Loss: 1.0741954045295716, Training Accuracy: 68.3375\n",
            "Validation Loss: 3.4972750517972715, Validation Accuracy: 32.75\n",
            "[19/150]: Training Loss: 0.9752286433696746, Training Accuracy: 70.7775\n",
            "Validation Loss: 3.703600254787761, Validation Accuracy: 32.29\n",
            "[20/150]: Training Loss: 0.8703240002632141, Training Accuracy: 73.66\n",
            "Validation Loss: 3.6860399884023485, Validation Accuracy: 32.6\n",
            "[21/150]: Training Loss: 0.7578078193187714, Training Accuracy: 76.6025\n",
            "Validation Loss: 4.062916931832672, Validation Accuracy: 31.29\n",
            "[22/150]: Training Loss: 0.673084812450409, Training Accuracy: 79.0375\n",
            "Validation Loss: 4.114260547480006, Validation Accuracy: 31.62\n",
            "[23/150]: Training Loss: 0.6178978152275085, Training Accuracy: 80.77\n",
            "Validation Loss: 4.332119194565306, Validation Accuracy: 32.24\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 24.731220123874156, Test Accuracy: 17.1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>17.1</td></tr><tr><td>Test Loss</td><td>24.73122</td></tr><tr><td>Train Accuracy</td><td>80.77</td></tr><tr><td>Train Loss</td><td>0.6179</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning/runs/pvvd2my8</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_003346-pvvd2my8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "\n",
        "for lr in learning_rates:\n",
        "  for wd in weight_decays:\n",
        "\n",
        "    print('='*50)\n",
        "    print(f'Hyperparameter with lr:{lr} and wd:{wd}')\n",
        "    print('='*50)\n",
        "\n",
        "    hyperparameters = {'learning_rate': lr,\n",
        "                       'weight_decay' : wd\n",
        "                       }\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(num_epochs, model, train_loader, validation_loader, test_loader, optimizer, scheduler, criterion, device, 'SGDM-HyperParameterTuning', hyperparameters=hyperparameters, is_wandb = True, n_epochs_stop = 10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training Using Best Hyperparameters Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f9dca98d1b8c402db8fa03f1354a051b",
            "3445c5fe76514de0b6b9cc31200c8544",
            "eafc1e77174c4758a3780b481d694ae0",
            "e6a773393e084de9a117fbbf43b7a59e",
            "b0748d32c99741d39b18512400e07f30",
            "dc43eb867d6b446cb0cb8e5debae57e7",
            "6c30f9a9e6c44d78ad39f43a5f11a6d0",
            "81500a36ef5e4a9e9b36aef01bee3697"
          ]
        },
        "id": "i9Q1MpZmVAUp",
        "outputId": "00a406f8-47bc-405c-d5e8-e10713b7f5fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_013938-4i9h8t3n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n' target=\"_blank\">learning_rate=0.01 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.089274020146226, Training Accuracy: 7.336\n",
            "Validation Loss: 3.675565988394865, Validation Accuracy: 11.94\n",
            "[2/150]: Training Loss: 3.4894355224526445, Training Accuracy: 16.328\n",
            "Validation Loss: 3.239326788361665, Validation Accuracy: 21.43\n",
            "[3/150]: Training Loss: 3.1558719864281852, Training Accuracy: 21.82\n",
            "Validation Loss: 2.9393957374961515, Validation Accuracy: 26.07\n",
            "[4/150]: Training Loss: 2.9186559430778485, Training Accuracy: 26.722\n",
            "Validation Loss: 2.7213429053118277, Validation Accuracy: 30.34\n",
            "[5/150]: Training Loss: 2.7546427048685604, Training Accuracy: 29.83\n",
            "Validation Loss: 2.6445619574018346, Validation Accuracy: 32.87\n",
            "[6/150]: Training Loss: 2.6314107673552334, Training Accuracy: 32.316\n",
            "Validation Loss: 2.463979342940507, Validation Accuracy: 35.97\n",
            "[7/150]: Training Loss: 2.525907842399519, Training Accuracy: 34.748\n",
            "Validation Loss: 2.402360181899587, Validation Accuracy: 37.43\n",
            "[8/150]: Training Loss: 2.4479937202790203, Training Accuracy: 36.512\n",
            "Validation Loss: 2.4091407804732112, Validation Accuracy: 36.77\n",
            "[9/150]: Training Loss: 2.3761757938453303, Training Accuracy: 37.836\n",
            "Validation Loss: 2.266651909062817, Validation Accuracy: 40.12\n",
            "[10/150]: Training Loss: 2.3189178927780114, Training Accuracy: 39.182\n",
            "Validation Loss: 2.248874021943208, Validation Accuracy: 40.42\n",
            "[11/150]: Training Loss: 2.2658593403104015, Training Accuracy: 40.134\n",
            "Validation Loss: 2.259856101054295, Validation Accuracy: 40.67\n",
            "[12/150]: Training Loss: 2.2226978584628583, Training Accuracy: 41.142\n",
            "Validation Loss: 2.1725807235499097, Validation Accuracy: 42.45\n",
            "[13/150]: Training Loss: 2.1647636545893483, Training Accuracy: 42.432\n",
            "Validation Loss: 2.1585155209158637, Validation Accuracy: 43.43\n",
            "[14/150]: Training Loss: 2.130936350206585, Training Accuracy: 43.382\n",
            "Validation Loss: 2.099344393250289, Validation Accuracy: 44.47\n",
            "[15/150]: Training Loss: 2.100127648514555, Training Accuracy: 44.078\n",
            "Validation Loss: 2.1766397307632834, Validation Accuracy: 43.16\n",
            "[16/150]: Training Loss: 2.0619241555633447, Training Accuracy: 44.864\n",
            "Validation Loss: 2.086525283042033, Validation Accuracy: 45.45\n",
            "[17/150]: Training Loss: 2.030578180042374, Training Accuracy: 45.488\n",
            "Validation Loss: 2.056034764666466, Validation Accuracy: 45.56\n",
            "[18/150]: Training Loss: 2.0153322762540538, Training Accuracy: 45.894\n",
            "Validation Loss: 2.0836284441553103, Validation Accuracy: 45.49\n",
            "[19/150]: Training Loss: 1.9813076870520707, Training Accuracy: 46.512\n",
            "Validation Loss: 2.123422172418825, Validation Accuracy: 44.8\n",
            "[20/150]: Training Loss: 1.9613309851692766, Training Accuracy: 47.156\n",
            "Validation Loss: 2.0666351356324117, Validation Accuracy: 45.67\n",
            "[21/150]: Training Loss: 1.9383105931379605, Training Accuracy: 47.68\n",
            "Validation Loss: 2.002842481728572, Validation Accuracy: 47.53\n",
            "[22/150]: Training Loss: 1.9150681045963942, Training Accuracy: 48.088\n",
            "Validation Loss: 2.0622900238462316, Validation Accuracy: 47.17\n",
            "[23/150]: Training Loss: 1.893286463854563, Training Accuracy: 48.534\n",
            "Validation Loss: 2.00721144600279, Validation Accuracy: 46.89\n",
            "[24/150]: Training Loss: 1.890704987908873, Training Accuracy: 48.7\n",
            "Validation Loss: 2.038584551993449, Validation Accuracy: 46.81\n",
            "[25/150]: Training Loss: 1.864747319989802, Training Accuracy: 49.254\n",
            "Validation Loss: 2.0571977568280166, Validation Accuracy: 46.35\n",
            "[26/150]: Training Loss: 1.851232278225062, Training Accuracy: 49.578\n",
            "Validation Loss: 2.067047737206623, Validation Accuracy: 45.89\n",
            "[27/150]: Training Loss: 1.8424793141882132, Training Accuracy: 49.854\n",
            "Validation Loss: 1.9625371656600077, Validation Accuracy: 48.1\n",
            "[28/150]: Training Loss: 1.8046449944186393, Training Accuracy: 50.832\n",
            "Validation Loss: 2.0106217762467207, Validation Accuracy: 47.7\n",
            "[29/150]: Training Loss: 1.8060023488900852, Training Accuracy: 50.666\n",
            "Validation Loss: 1.972709655002424, Validation Accuracy: 48.73\n",
            "[30/150]: Training Loss: 1.794560846770206, Training Accuracy: 50.516\n",
            "Validation Loss: 1.9357355255989512, Validation Accuracy: 49.77\n",
            "[31/150]: Training Loss: 1.7700339468848674, Training Accuracy: 51.572\n",
            "Validation Loss: 2.0069477284789845, Validation Accuracy: 47.09\n",
            "[32/150]: Training Loss: 1.760896964146353, Training Accuracy: 51.7\n",
            "Validation Loss: 1.9989952729765776, Validation Accuracy: 48.32\n",
            "[33/150]: Training Loss: 1.7410227035927346, Training Accuracy: 52.406\n",
            "Validation Loss: 1.912184556578375, Validation Accuracy: 50.02\n",
            "[34/150]: Training Loss: 1.7288355792269987, Training Accuracy: 52.308\n",
            "Validation Loss: 1.960264290973639, Validation Accuracy: 49.04\n",
            "[35/150]: Training Loss: 1.726897613929056, Training Accuracy: 52.332\n",
            "Validation Loss: 1.9354495113822305, Validation Accuracy: 49.72\n",
            "[36/150]: Training Loss: 1.7120373965529225, Training Accuracy: 52.802\n",
            "Validation Loss: 1.9664037690800467, Validation Accuracy: 48.71\n",
            "[37/150]: Training Loss: 1.693096386502161, Training Accuracy: 53.136\n",
            "Validation Loss: 2.0150347925295495, Validation Accuracy: 47.45\n",
            "[38/150]: Training Loss: 1.6896419387949093, Training Accuracy: 53.464\n",
            "Validation Loss: 1.8958269729735746, Validation Accuracy: 50.19\n",
            "[39/150]: Training Loss: 1.6746401542897724, Training Accuracy: 53.896\n",
            "Validation Loss: 1.8924665306783786, Validation Accuracy: 50.21\n",
            "[40/150]: Training Loss: 1.658155962481828, Training Accuracy: 54.248\n",
            "Validation Loss: 2.013761671485415, Validation Accuracy: 47.68\n",
            "[41/150]: Training Loss: 1.6497482628468663, Training Accuracy: 54.48\n",
            "Validation Loss: 1.9099182338471625, Validation Accuracy: 49.93\n",
            "[42/150]: Training Loss: 1.640857491423102, Training Accuracy: 54.522\n",
            "Validation Loss: 1.9500497716247656, Validation Accuracy: 48.99\n",
            "[43/150]: Training Loss: 1.627800954272375, Training Accuracy: 54.828\n",
            "Validation Loss: 1.8859608940258148, Validation Accuracy: 50.08\n",
            "[44/150]: Training Loss: 1.6115469687125261, Training Accuracy: 55.468\n",
            "Validation Loss: 1.8382089783431619, Validation Accuracy: 51.49\n",
            "[45/150]: Training Loss: 1.5990354193141088, Training Accuracy: 55.408\n",
            "Validation Loss: 1.9672614282863155, Validation Accuracy: 49.18\n",
            "[46/150]: Training Loss: 1.597626144090272, Training Accuracy: 55.548\n",
            "Validation Loss: 1.8989077024399095, Validation Accuracy: 50.63\n",
            "[47/150]: Training Loss: 1.583931565284729, Training Accuracy: 55.81\n",
            "Validation Loss: 1.9013854675232225, Validation Accuracy: 50.7\n",
            "[48/150]: Training Loss: 1.5654289053224237, Training Accuracy: 56.032\n",
            "Validation Loss: 1.9721670173535681, Validation Accuracy: 49.36\n",
            "[49/150]: Training Loss: 1.5542657918789808, Training Accuracy: 56.446\n",
            "Validation Loss: 1.9395824830243542, Validation Accuracy: 49.82\n",
            "[50/150]: Training Loss: 1.5429580295482255, Training Accuracy: 56.698\n",
            "Validation Loss: 1.9366691735140078, Validation Accuracy: 50.0\n",
            "[51/150]: Training Loss: 1.539840473086023, Training Accuracy: 56.68\n",
            "Validation Loss: 1.93095024452088, Validation Accuracy: 49.97\n",
            "[52/150]: Training Loss: 1.522039294928846, Training Accuracy: 57.282\n",
            "Validation Loss: 1.901054722488306, Validation Accuracy: 50.91\n",
            "[53/150]: Training Loss: 1.505999029597358, Training Accuracy: 57.376\n",
            "Validation Loss: 1.945236231870712, Validation Accuracy: 49.27\n",
            "[54/150]: Training Loss: 1.507185840469492, Training Accuracy: 57.802\n",
            "Validation Loss: 1.8845598515431592, Validation Accuracy: 50.54\n",
            "[55/150]: Training Loss: 1.4943511856486424, Training Accuracy: 57.754\n",
            "Validation Loss: 1.908029711170561, Validation Accuracy: 50.57\n",
            "[56/150]: Training Loss: 1.4784885358322613, Training Accuracy: 58.39\n",
            "Validation Loss: 1.9093056567914926, Validation Accuracy: 50.32\n",
            "[57/150]: Training Loss: 1.4689392635736929, Training Accuracy: 58.69\n",
            "Validation Loss: 1.9369963088612647, Validation Accuracy: 50.18\n",
            "[58/150]: Training Loss: 1.4634843205704409, Training Accuracy: 58.688\n",
            "Validation Loss: 1.9295313540537646, Validation Accuracy: 50.43\n",
            "[59/150]: Training Loss: 1.4444945978996393, Training Accuracy: 59.07\n",
            "Validation Loss: 1.8427119877687685, Validation Accuracy: 52.33\n",
            "[60/150]: Training Loss: 1.4354293743515258, Training Accuracy: 59.374\n",
            "Validation Loss: 1.910806885950125, Validation Accuracy: 50.94\n",
            "[61/150]: Training Loss: 1.4113788747269174, Training Accuracy: 59.82\n",
            "Validation Loss: 1.8786808958478793, Validation Accuracy: 51.43\n",
            "[62/150]: Training Loss: 1.4087011503898883, Training Accuracy: 59.934\n",
            "Validation Loss: 1.93228034608683, Validation Accuracy: 50.3\n",
            "[63/150]: Training Loss: 1.4004798267046203, Training Accuracy: 60.114\n",
            "Validation Loss: 1.862087261145282, Validation Accuracy: 51.42\n",
            "[64/150]: Training Loss: 1.3885550134627105, Training Accuracy: 60.078\n",
            "Validation Loss: 1.839821860289118, Validation Accuracy: 52.23\n",
            "[65/150]: Training Loss: 1.3659701371741721, Training Accuracy: 61.178\n",
            "Validation Loss: 1.8993338536305033, Validation Accuracy: 51.3\n",
            "[66/150]: Training Loss: 1.359891347720495, Training Accuracy: 61.062\n",
            "Validation Loss: 1.808910168659915, Validation Accuracy: 52.34\n",
            "[67/150]: Training Loss: 1.350066394376023, Training Accuracy: 61.206\n",
            "Validation Loss: 1.842593222666698, Validation Accuracy: 52.23\n",
            "[68/150]: Training Loss: 1.3413548037371672, Training Accuracy: 61.606\n",
            "Validation Loss: 1.8450721175807296, Validation Accuracy: 52.19\n",
            "[69/150]: Training Loss: 1.328629597907176, Training Accuracy: 61.842\n",
            "Validation Loss: 1.9130320769206735, Validation Accuracy: 51.75\n",
            "[70/150]: Training Loss: 1.308861137579774, Training Accuracy: 62.402\n",
            "Validation Loss: 1.8209870947394402, Validation Accuracy: 52.76\n",
            "[71/150]: Training Loss: 1.3000667644736101, Training Accuracy: 62.528\n",
            "Validation Loss: 1.8656981424161583, Validation Accuracy: 52.29\n",
            "[72/150]: Training Loss: 1.2866845129395994, Training Accuracy: 62.902\n",
            "Validation Loss: 1.8237224574301654, Validation Accuracy: 53.36\n",
            "[73/150]: Training Loss: 1.2716818537248675, Training Accuracy: 63.328\n",
            "Validation Loss: 1.872310409879988, Validation Accuracy: 52.51\n",
            "[74/150]: Training Loss: 1.2592318182253777, Training Accuracy: 63.678\n",
            "Validation Loss: 1.8924590835146085, Validation Accuracy: 52.18\n",
            "[75/150]: Training Loss: 1.2489444612694518, Training Accuracy: 63.924\n",
            "Validation Loss: 1.8441433739510311, Validation Accuracy: 53.18\n",
            "[76/150]: Training Loss: 1.2271763168637404, Training Accuracy: 64.204\n",
            "Validation Loss: 1.826472075881472, Validation Accuracy: 53.48\n",
            "[77/150]: Training Loss: 1.2162439644031817, Training Accuracy: 64.628\n",
            "Validation Loss: 1.7922283798266367, Validation Accuracy: 53.91\n",
            "[78/150]: Training Loss: 1.1974031528091187, Training Accuracy: 65.212\n",
            "Validation Loss: 1.854033857394176, Validation Accuracy: 52.77\n",
            "[79/150]: Training Loss: 1.1963484688945438, Training Accuracy: 65.298\n",
            "Validation Loss: 1.8417060686524507, Validation Accuracy: 53.83\n",
            "[80/150]: Training Loss: 1.170646650559457, Training Accuracy: 65.906\n",
            "Validation Loss: 1.8188444110238629, Validation Accuracy: 54.0\n",
            "[81/150]: Training Loss: 1.1599712444998114, Training Accuracy: 66.228\n",
            "Validation Loss: 1.832872725596094, Validation Accuracy: 53.93\n",
            "[82/150]: Training Loss: 1.1514770396987495, Training Accuracy: 66.602\n",
            "Validation Loss: 1.8412558204808813, Validation Accuracy: 53.79\n",
            "[83/150]: Training Loss: 1.1428028439621791, Training Accuracy: 66.712\n",
            "Validation Loss: 1.8457995410178119, Validation Accuracy: 53.19\n",
            "[84/150]: Training Loss: 1.1221959683901208, Training Accuracy: 67.186\n",
            "Validation Loss: 1.8420210499672374, Validation Accuracy: 53.69\n",
            "[85/150]: Training Loss: 1.1132261551859435, Training Accuracy: 67.302\n",
            "Validation Loss: 1.8613816719905587, Validation Accuracy: 53.54\n",
            "[86/150]: Training Loss: 1.095082609473592, Training Accuracy: 68.072\n",
            "Validation Loss: 1.8083385081048224, Validation Accuracy: 53.78\n",
            "[87/150]: Training Loss: 1.0820598827908412, Training Accuracy: 68.22\n",
            "Validation Loss: 1.8537752772592435, Validation Accuracy: 54.26\n",
            "[88/150]: Training Loss: 1.0588698522818973, Training Accuracy: 68.838\n",
            "Validation Loss: 1.8358791383208743, Validation Accuracy: 54.18\n",
            "[89/150]: Training Loss: 1.0537138239806876, Training Accuracy: 68.858\n",
            "Validation Loss: 1.9032178441430354, Validation Accuracy: 53.39\n",
            "[90/150]: Training Loss: 1.0420529545115693, Training Accuracy: 69.19\n",
            "Validation Loss: 1.8331271857972358, Validation Accuracy: 54.72\n",
            "[91/150]: Training Loss: 1.0190018734053883, Training Accuracy: 69.946\n",
            "Validation Loss: 1.81914554811587, Validation Accuracy: 54.61\n",
            "[92/150]: Training Loss: 1.0052901713744453, Training Accuracy: 70.376\n",
            "Validation Loss: 1.8224602520086204, Validation Accuracy: 54.54\n",
            "[93/150]: Training Loss: 0.9959899578100581, Training Accuracy: 70.576\n",
            "Validation Loss: 1.8335816890570769, Validation Accuracy: 54.66\n",
            "[94/150]: Training Loss: 0.9820676271034323, Training Accuracy: 70.882\n",
            "Validation Loss: 1.8163665267312603, Validation Accuracy: 55.18\n",
            "[95/150]: Training Loss: 0.9656730821675352, Training Accuracy: 71.258\n",
            "Validation Loss: 1.8308387478445745, Validation Accuracy: 54.6\n",
            "[96/150]: Training Loss: 0.9520570190666277, Training Accuracy: 71.578\n",
            "Validation Loss: 1.8336243507968393, Validation Accuracy: 55.03\n",
            "[97/150]: Training Loss: 0.938745556661235, Training Accuracy: 71.902\n",
            "Validation Loss: 1.8366246033625997, Validation Accuracy: 54.91\n",
            "[98/150]: Training Loss: 0.9180464198827134, Training Accuracy: 72.722\n",
            "Validation Loss: 1.8760136365890503, Validation Accuracy: 54.5\n",
            "[99/150]: Training Loss: 0.9068292014281768, Training Accuracy: 73.062\n",
            "Validation Loss: 1.7993891132864983, Validation Accuracy: 55.5\n",
            "[100/150]: Training Loss: 0.8922816610625942, Training Accuracy: 73.34\n",
            "Validation Loss: 1.8412930228907591, Validation Accuracy: 54.68\n",
            "[101/150]: Training Loss: 0.8739150777421034, Training Accuracy: 73.706\n",
            "Validation Loss: 1.8339118517128525, Validation Accuracy: 55.67\n",
            "[102/150]: Training Loss: 0.8564372022667199, Training Accuracy: 74.472\n",
            "Validation Loss: 1.831563648904205, Validation Accuracy: 54.7\n",
            "[103/150]: Training Loss: 0.8455693187463619, Training Accuracy: 74.832\n",
            "Validation Loss: 1.8586692977103458, Validation Accuracy: 55.09\n",
            "[104/150]: Training Loss: 0.8356970895815383, Training Accuracy: 74.91\n",
            "Validation Loss: 1.891693490326025, Validation Accuracy: 54.66\n",
            "[105/150]: Training Loss: 0.8192636847038708, Training Accuracy: 75.622\n",
            "Validation Loss: 1.836436649037015, Validation Accuracy: 55.18\n",
            "[106/150]: Training Loss: 0.8021497989783202, Training Accuracy: 75.904\n",
            "Validation Loss: 1.8664625199737064, Validation Accuracy: 55.06\n",
            "[107/150]: Training Loss: 0.7924835441438743, Training Accuracy: 76.238\n",
            "Validation Loss: 1.8400274932764138, Validation Accuracy: 56.23\n",
            "[108/150]: Training Loss: 0.7697334062031773, Training Accuracy: 76.922\n",
            "Validation Loss: 1.852326028665919, Validation Accuracy: 55.58\n",
            "[109/150]: Training Loss: 0.7663251587268337, Training Accuracy: 77.026\n",
            "Validation Loss: 1.851918256206877, Validation Accuracy: 55.79\n",
            "[110/150]: Training Loss: 0.7508895213113111, Training Accuracy: 77.64\n",
            "Validation Loss: 1.8553174369654077, Validation Accuracy: 55.56\n",
            "[111/150]: Training Loss: 0.7446020825973252, Training Accuracy: 77.658\n",
            "Validation Loss: 1.8480755607033992, Validation Accuracy: 55.4\n",
            "[112/150]: Training Loss: 0.7242154034278582, Training Accuracy: 78.302\n",
            "Validation Loss: 1.8599452805367245, Validation Accuracy: 55.63\n",
            "[113/150]: Training Loss: 0.710430224609497, Training Accuracy: 78.58\n",
            "Validation Loss: 1.853841427405169, Validation Accuracy: 55.72\n",
            "[114/150]: Training Loss: 0.6952832066418265, Training Accuracy: 79.15\n",
            "Validation Loss: 1.8361253859890494, Validation Accuracy: 56.53\n",
            "[115/150]: Training Loss: 0.6855566057631427, Training Accuracy: 79.384\n",
            "Validation Loss: 1.8501070935255404, Validation Accuracy: 55.8\n",
            "[116/150]: Training Loss: 0.670332043791366, Training Accuracy: 79.848\n",
            "Validation Loss: 1.871697852565984, Validation Accuracy: 55.87\n",
            "[117/150]: Training Loss: 0.659062210518076, Training Accuracy: 80.28\n",
            "Validation Loss: 1.8932185765284641, Validation Accuracy: 56.33\n",
            "[118/150]: Training Loss: 0.6481513134049027, Training Accuracy: 80.47\n",
            "Validation Loss: 1.8721413916083658, Validation Accuracy: 56.69\n",
            "[119/150]: Training Loss: 0.6378583989256178, Training Accuracy: 80.804\n",
            "Validation Loss: 1.8575806663294507, Validation Accuracy: 56.3\n",
            "[120/150]: Training Loss: 0.6237786744561646, Training Accuracy: 81.468\n",
            "Validation Loss: 1.8885631751103007, Validation Accuracy: 56.27\n",
            "[121/150]: Training Loss: 0.6136706139311157, Training Accuracy: 81.672\n",
            "Validation Loss: 1.8672110297877318, Validation Accuracy: 56.64\n",
            "[122/150]: Training Loss: 0.598588163380885, Training Accuracy: 82.084\n",
            "Validation Loss: 1.8859197356898314, Validation Accuracy: 56.85\n",
            "[123/150]: Training Loss: 0.5942039575494463, Training Accuracy: 82.472\n",
            "Validation Loss: 1.8791207416801696, Validation Accuracy: 56.65\n",
            "[124/150]: Training Loss: 0.5779005947625241, Training Accuracy: 82.892\n",
            "Validation Loss: 1.8723569349118858, Validation Accuracy: 56.69\n",
            "[125/150]: Training Loss: 0.5656420003689463, Training Accuracy: 83.386\n",
            "Validation Loss: 1.8776653800041052, Validation Accuracy: 56.51\n",
            "[126/150]: Training Loss: 0.5588299318423966, Training Accuracy: 83.53\n",
            "Validation Loss: 1.8926861441818772, Validation Accuracy: 56.77\n",
            "[127/150]: Training Loss: 0.5484652115065424, Training Accuracy: 83.916\n",
            "Validation Loss: 1.8761122143192657, Validation Accuracy: 56.82\n",
            "[128/150]: Training Loss: 0.5397156641230254, Training Accuracy: 84.144\n",
            "Validation Loss: 1.883816226272826, Validation Accuracy: 56.91\n",
            "[129/150]: Training Loss: 0.5299459375307688, Training Accuracy: 84.524\n",
            "Validation Loss: 1.885110402942463, Validation Accuracy: 56.93\n",
            "[130/150]: Training Loss: 0.5226550506203985, Training Accuracy: 84.69\n",
            "Validation Loss: 1.8858621705109906, Validation Accuracy: 57.03\n",
            "[131/150]: Training Loss: 0.5145904917622466, Training Accuracy: 85.08\n",
            "Validation Loss: 1.898423175902883, Validation Accuracy: 56.86\n",
            "[132/150]: Training Loss: 0.5080187612444239, Training Accuracy: 85.244\n",
            "Validation Loss: 1.8937967233597093, Validation Accuracy: 57.24\n",
            "[133/150]: Training Loss: 0.501667047438719, Training Accuracy: 85.274\n",
            "Validation Loss: 1.8932398701928983, Validation Accuracy: 56.88\n",
            "[134/150]: Training Loss: 0.49395688687977585, Training Accuracy: 85.592\n",
            "Validation Loss: 1.8888862345628679, Validation Accuracy: 57.11\n",
            "[135/150]: Training Loss: 0.48916787244474796, Training Accuracy: 85.884\n",
            "Validation Loss: 1.8910012594453849, Validation Accuracy: 57.08\n",
            "[136/150]: Training Loss: 0.48000375615894947, Training Accuracy: 86.098\n",
            "Validation Loss: 1.9055567767210067, Validation Accuracy: 56.92\n",
            "[137/150]: Training Loss: 0.48404536522029307, Training Accuracy: 86.152\n",
            "Validation Loss: 1.8957096717919513, Validation Accuracy: 57.23\n",
            "[138/150]: Training Loss: 0.4733923572637236, Training Accuracy: 86.464\n",
            "Validation Loss: 1.8970948966445438, Validation Accuracy: 57.23\n",
            "[139/150]: Training Loss: 0.4687954626424843, Training Accuracy: 86.406\n",
            "Validation Loss: 1.8926855910355878, Validation Accuracy: 57.49\n",
            "[140/150]: Training Loss: 0.4656825878126237, Training Accuracy: 86.552\n",
            "Validation Loss: 1.8944045935466791, Validation Accuracy: 57.25\n",
            "[141/150]: Training Loss: 0.46502622329365567, Training Accuracy: 86.75\n",
            "Validation Loss: 1.892073856797188, Validation Accuracy: 57.21\n",
            "[142/150]: Training Loss: 0.45881695900579245, Training Accuracy: 86.948\n",
            "Validation Loss: 1.8972379872753362, Validation Accuracy: 57.33\n",
            "[143/150]: Training Loss: 0.4567206385152419, Training Accuracy: 86.978\n",
            "Validation Loss: 1.8969501234163904, Validation Accuracy: 57.41\n",
            "[144/150]: Training Loss: 0.45361602742729895, Training Accuracy: 87.108\n",
            "Validation Loss: 1.8974497530870378, Validation Accuracy: 57.36\n",
            "[145/150]: Training Loss: 0.45476321409196807, Training Accuracy: 87.206\n",
            "Validation Loss: 1.9007372218332472, Validation Accuracy: 57.39\n",
            "[146/150]: Training Loss: 0.45061131850685304, Training Accuracy: 87.172\n",
            "Validation Loss: 1.8978507139120893, Validation Accuracy: 57.36\n",
            "[147/150]: Training Loss: 0.4471297430832063, Training Accuracy: 87.32\n",
            "Validation Loss: 1.8995944998066896, Validation Accuracy: 57.45\n",
            "[148/150]: Training Loss: 0.44544086216584494, Training Accuracy: 87.358\n",
            "Validation Loss: 1.8984677867524942, Validation Accuracy: 57.57\n",
            "[149/150]: Training Loss: 0.45087983754589733, Training Accuracy: 87.206\n",
            "Validation Loss: 1.8988231648305418, Validation Accuracy: 57.57\n",
            "[150/150]: Training Loss: 0.445312842879149, Training Accuracy: 87.314\n",
            "Validation Loss: 1.898690512985181, Validation Accuracy: 57.55\n",
            "**********************************************************************\n",
            "Test Loss: 1.898690512985181, Test Accuracy: 57.55\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>57.55</td></tr><tr><td>Test Loss</td><td>1.89869</td></tr><tr><td>Train Accuracy</td><td>87.314</td></tr><tr><td>Train Loss</td><td>0.44531</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM/runs/4i9h8t3n</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-SGDM</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_013938-4i9h8t3n/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "\n",
        "hyperparameters = {'learning_rate': lr,\n",
        "                  'weight_decay' : wd}\n",
        "\n",
        "# Load the model\n",
        "model_0 = LeNet5().to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer_0 = torch.optim.SGD(model_0.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_0, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# Training\n",
        "run_training(num_epochs, model_0, original_train_loader, original_test_loader, original_test_loader, optimizer_0, scheduler, criterion, device, optimizer_name='SGDM', hyperparameters=hyperparameters, is_wandb=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### AdamW (Adam with Weight Decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ba6f9560801c461a90a0fd2f8c20d918",
            "5055c91b4ed34c1baf249fa25948e84f",
            "1feabb0772134ace893d71cb905e9b12",
            "fb5f2e328dff44018f41180c2a2ec871",
            "33050a2d80c24235aecc36cb34e79684",
            "2bc1423c729a4faa9a184ae092eda8ad",
            "1b1acf7a1f5f43739e1fe0894ba48950",
            "0d710da02bfe4c8e80d205158d9d64c7",
            "d770fa4de3bd479886dbf8e1f6369543",
            "c20d73b37f6a497a8847e2fd20a98d16",
            "edd61f4569b54b869669aebf91420508",
            "a6ba8ab59be54b8ca096631627ee9713",
            "15a0bbd1e2b14cef8a2f9accf120c1ad",
            "e181391aa4424c04804ac665abd6f594",
            "f8ce66183b2543e7bf19b71d90d1c53e",
            "904f899f441149e9b707699a0ebf31b5",
            "f76ed5acf91a4edf92950dcb88356f23",
            "9387a738135842cf965db860499510f5",
            "33a438b911c04f1f96c67b4f7ba7477d",
            "35d90a90d5854bcaa2bc5f385cdad931",
            "5630d5fcb50a46f2887e0cce74a005a6",
            "fed90f96881140119ee97a0d2120b615",
            "ff7b6956e7e34db68cd38dee19c798f1",
            "d71c8b5d30df474ca6f30976d0a33c71",
            "cbe10db0bb8d45609a0f3d59a30c8f61",
            "07ff6351429c48c2b835305d3111af40",
            "ab580ef13b18428c994fc4f8b80885b6",
            "497de066b1964cd4b931476e0bd50c66",
            "c5de35a9063345b1a12e212718a02575",
            "e86e517239b54ca4a6767a300aa7e01d",
            "eb268a732bc74b2d81ec3c3a6ecd0362",
            "f2915a147a4246dba7984a90f23c34ae",
            "2f39d5f73c7647f1804e4212cb39924d",
            "6e7ede02663f4eb0927872bf17858f18",
            "a562d76741f74b10b8095be14da779d1",
            "f4e7759ba0f544d8a28a9922e06e890b",
            "0a9ee7dc8817456aab4af03cbfa4c6ed",
            "567cc1542f09433f8cc8b3a39237f198",
            "8dc245e02cac40f1b601fa42a0e6e77a",
            "1b093d744b374154afe2058e4a6de822",
            "4302f47b1ff043aca2523212b015e080",
            "371471be80ca47dbb005c853b7832f04",
            "e75a0d8ae24e462aa3075a813aad302d",
            "fd2cd8045a5c4387b98d080782289c48",
            "1574e69b88de479da2aadc2dfdd43261",
            "f523ae2aca8c46878f0a6ddc1f798ef5",
            "0b42ea1995bb410b99e653780dad3916",
            "f3ebd9b7bf7a4d24b54ab6673322f74b"
          ]
        },
        "id": "Toi1eWRqJuhK",
        "outputId": "bd028d6d-1e8d-4fff-bccf-88ed9aa1c4ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:ymdv8k6d) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>7.7625</td></tr><tr><td>Train Loss</td><td>4.01706</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.00095 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/ymdv8k6d' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/ymdv8k6d</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_034825-ymdv8k6d/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:ymdv8k6d). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_034959-j16vayol</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol' target=\"_blank\">learning_rate=0.001 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.032126424407959, Training Accuracy: 7.645\n",
            "Validation Loss: 3.633875617555752, Validation Accuracy: 13.81\n",
            "[2/150]: Training Loss: 3.4117616390228274, Training Accuracy: 17.9375\n",
            "Validation Loss: 3.291252329091358, Validation Accuracy: 19.57\n",
            "[3/150]: Training Loss: 3.0980306770324706, Training Accuracy: 23.7325\n",
            "Validation Loss: 3.0133402590539045, Validation Accuracy: 25.49\n",
            "[4/150]: Training Loss: 2.875119793319702, Training Accuracy: 27.905\n",
            "Validation Loss: 2.9483105938905365, Validation Accuracy: 26.77\n",
            "[5/150]: Training Loss: 2.6911334384918213, Training Accuracy: 31.615\n",
            "Validation Loss: 2.851942961383018, Validation Accuracy: 28.77\n",
            "[6/150]: Training Loss: 2.5383362628936768, Training Accuracy: 34.6425\n",
            "Validation Loss: 2.8010447116414454, Validation Accuracy: 30.38\n",
            "[7/150]: Training Loss: 2.3951899276733397, Training Accuracy: 37.7375\n",
            "Validation Loss: 2.772052654035532, Validation Accuracy: 31.08\n",
            "[8/150]: Training Loss: 2.2677825828552245, Training Accuracy: 40.2175\n",
            "Validation Loss: 2.740394227823634, Validation Accuracy: 32.51\n",
            "[9/150]: Training Loss: 2.1415996942520144, Training Accuracy: 43.1725\n",
            "Validation Loss: 2.7748732961666813, Validation Accuracy: 31.82\n",
            "[10/150]: Training Loss: 2.016268748664856, Training Accuracy: 45.7725\n",
            "Validation Loss: 2.766294874203433, Validation Accuracy: 33.51\n",
            "[11/150]: Training Loss: 1.8994886245727538, Training Accuracy: 48.375\n",
            "Validation Loss: 2.840820978401573, Validation Accuracy: 32.8\n",
            "[12/150]: Training Loss: 1.7866277021408081, Training Accuracy: 51.045\n",
            "Validation Loss: 2.8579562796149283, Validation Accuracy: 32.9\n",
            "[13/150]: Training Loss: 1.6882859018325806, Training Accuracy: 53.12\n",
            "Validation Loss: 2.923183766899595, Validation Accuracy: 32.48\n",
            "[14/150]: Training Loss: 1.5779105269432068, Training Accuracy: 55.655\n",
            "Validation Loss: 3.072260461795102, Validation Accuracy: 32.29\n",
            "[15/150]: Training Loss: 1.478017513847351, Training Accuracy: 58.1575\n",
            "Validation Loss: 3.1435716137005265, Validation Accuracy: 32.08\n",
            "[16/150]: Training Loss: 1.3899440537452699, Training Accuracy: 59.9975\n",
            "Validation Loss: 3.176926696376436, Validation Accuracy: 31.97\n",
            "[17/150]: Training Loss: 1.3004323136329652, Training Accuracy: 62.3725\n",
            "Validation Loss: 3.3300343015391354, Validation Accuracy: 32.02\n",
            "[18/150]: Training Loss: 1.2033100715637206, Training Accuracy: 64.74\n",
            "Validation Loss: 3.478040464364799, Validation Accuracy: 31.7\n",
            "[19/150]: Training Loss: 1.1342673721313477, Training Accuracy: 66.51\n",
            "Validation Loss: 3.621978481863714, Validation Accuracy: 31.07\n",
            "[20/150]: Training Loss: 1.057302718448639, Training Accuracy: 68.56\n",
            "Validation Loss: 3.8245642261140667, Validation Accuracy: 31.16\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 198.79228443704594, Test Accuracy: 3.39\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>3.39</td></tr><tr><td>Test Loss</td><td>198.79228</td></tr><tr><td>Train Accuracy</td><td>68.56</td></tr><tr><td>Train Loss</td><td>1.0573</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/j16vayol</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_034959-j16vayol/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_035204-l4zoq5dd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd' target=\"_blank\">learning_rate=0.001 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.108658624267578, Training Accuracy: 6.6075\n",
            "Validation Loss: 3.7040131304674087, Validation Accuracy: 12.1\n",
            "[2/150]: Training Loss: 3.5060665573120118, Training Accuracy: 16.015\n",
            "Validation Loss: 3.3598362321306947, Validation Accuracy: 18.42\n",
            "[3/150]: Training Loss: 3.1865245040893555, Training Accuracy: 21.7275\n",
            "Validation Loss: 3.128495198146553, Validation Accuracy: 22.84\n",
            "[4/150]: Training Loss: 2.9679495239257814, Training Accuracy: 25.8525\n",
            "Validation Loss: 2.9615708293428846, Validation Accuracy: 26.83\n",
            "[5/150]: Training Loss: 2.797335900115967, Training Accuracy: 29.515\n",
            "Validation Loss: 2.9264626108157406, Validation Accuracy: 26.96\n",
            "[6/150]: Training Loss: 2.6617756172180176, Training Accuracy: 32.13\n",
            "Validation Loss: 2.8755724490827816, Validation Accuracy: 28.35\n",
            "[7/150]: Training Loss: 2.54268462638855, Training Accuracy: 34.435\n",
            "Validation Loss: 2.7718013547788, Validation Accuracy: 30.25\n",
            "[8/150]: Training Loss: 2.424749851608276, Training Accuracy: 37.1025\n",
            "Validation Loss: 2.7567353567500024, Validation Accuracy: 31.11\n",
            "[9/150]: Training Loss: 2.3229350467681886, Training Accuracy: 39.08\n",
            "Validation Loss: 2.7088757032042095, Validation Accuracy: 32.08\n",
            "[10/150]: Training Loss: 2.2215481660842897, Training Accuracy: 41.4275\n",
            "Validation Loss: 2.75471066821153, Validation Accuracy: 31.83\n",
            "[11/150]: Training Loss: 2.134213170814514, Training Accuracy: 43.25\n",
            "Validation Loss: 2.8348169676057853, Validation Accuracy: 31.64\n",
            "[12/150]: Training Loss: 2.051020913696289, Training Accuracy: 44.9475\n",
            "Validation Loss: 2.7630925406316282, Validation Accuracy: 32.98\n",
            "[13/150]: Training Loss: 1.960823282814026, Training Accuracy: 46.7775\n",
            "Validation Loss: 2.7420302629470825, Validation Accuracy: 33.4\n",
            "[14/150]: Training Loss: 1.8790180908203125, Training Accuracy: 48.855\n",
            "Validation Loss: 2.7926843044864142, Validation Accuracy: 33.79\n",
            "[15/150]: Training Loss: 1.80182436504364, Training Accuracy: 50.6875\n",
            "Validation Loss: 2.8919099911003356, Validation Accuracy: 33.19\n",
            "[16/150]: Training Loss: 1.727850655937195, Training Accuracy: 52.3025\n",
            "Validation Loss: 2.8964282950018623, Validation Accuracy: 33.5\n",
            "[17/150]: Training Loss: 1.6616035480499267, Training Accuracy: 53.85\n",
            "Validation Loss: 3.033423226350432, Validation Accuracy: 32.53\n",
            "[18/150]: Training Loss: 1.5868734314918518, Training Accuracy: 55.5\n",
            "Validation Loss: 3.032342692089688, Validation Accuracy: 33.04\n",
            "[19/150]: Training Loss: 1.5333876008987426, Training Accuracy: 56.7525\n",
            "Validation Loss: 3.079486154446936, Validation Accuracy: 32.97\n",
            "[20/150]: Training Loss: 1.4575365795135498, Training Accuracy: 58.77\n",
            "Validation Loss: 3.181258452166418, Validation Accuracy: 32.82\n",
            "[21/150]: Training Loss: 1.39640816450119, Training Accuracy: 60.14\n",
            "Validation Loss: 3.216413262543405, Validation Accuracy: 32.76\n",
            "[22/150]: Training Loss: 1.337559293270111, Training Accuracy: 61.5275\n",
            "Validation Loss: 3.3633130872325534, Validation Accuracy: 31.73\n",
            "[23/150]: Training Loss: 1.2836213255882263, Training Accuracy: 62.6875\n",
            "Validation Loss: 3.4558656868661286, Validation Accuracy: 32.33\n",
            "[24/150]: Training Loss: 1.2338986722946168, Training Accuracy: 64.2825\n",
            "Validation Loss: 3.4873077155678134, Validation Accuracy: 32.02\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 156.72609671817463, Test Accuracy: 3.33\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>3.33</td></tr><tr><td>Test Loss</td><td>156.7261</td></tr><tr><td>Train Accuracy</td><td>64.2825</td></tr><tr><td>Train Loss</td><td>1.2339</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/l4zoq5dd</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_035204-l4zoq5dd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_035441-y4ask5ys</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys' target=\"_blank\">learning_rate=0.001 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.053023485183716, Training Accuracy: 7.3\n",
            "Validation Loss: 3.71551227873298, Validation Accuracy: 12.6\n",
            "[2/150]: Training Loss: 3.4803551902770997, Training Accuracy: 16.4825\n",
            "Validation Loss: 3.3104240119836894, Validation Accuracy: 19.6\n",
            "[3/150]: Training Loss: 3.1457452793121337, Training Accuracy: 22.71\n",
            "Validation Loss: 3.1647931253834134, Validation Accuracy: 22.36\n",
            "[4/150]: Training Loss: 2.9213612785339356, Training Accuracy: 26.8375\n",
            "Validation Loss: 2.967867093481076, Validation Accuracy: 26.13\n",
            "[5/150]: Training Loss: 2.7346919048309326, Training Accuracy: 30.745\n",
            "Validation Loss: 2.870804744161618, Validation Accuracy: 28.43\n",
            "[6/150]: Training Loss: 2.580346668434143, Training Accuracy: 33.8975\n",
            "Validation Loss: 2.813369401700937, Validation Accuracy: 29.97\n",
            "[7/150]: Training Loss: 2.4525743492126466, Training Accuracy: 36.445\n",
            "Validation Loss: 2.8111886962963517, Validation Accuracy: 31.76\n",
            "[8/150]: Training Loss: 2.3342738655090334, Training Accuracy: 38.8\n",
            "Validation Loss: 2.7751463211266096, Validation Accuracy: 31.29\n",
            "[9/150]: Training Loss: 2.230911629486084, Training Accuracy: 40.89\n",
            "Validation Loss: 2.733239137443008, Validation Accuracy: 32.32\n",
            "[10/150]: Training Loss: 2.127202660560608, Training Accuracy: 43.0575\n",
            "Validation Loss: 2.740311449500406, Validation Accuracy: 32.67\n",
            "[11/150]: Training Loss: 2.0320646129608155, Training Accuracy: 45.3025\n",
            "Validation Loss: 2.7493710031934606, Validation Accuracy: 33.28\n",
            "[12/150]: Training Loss: 1.9452043891906738, Training Accuracy: 47.2675\n",
            "Validation Loss: 2.776471818328663, Validation Accuracy: 33.2\n",
            "[13/150]: Training Loss: 1.8665077058792114, Training Accuracy: 49.035\n",
            "Validation Loss: 2.801089420440091, Validation Accuracy: 32.56\n",
            "[14/150]: Training Loss: 1.782764630126953, Training Accuracy: 50.66\n",
            "Validation Loss: 2.869801163673401, Validation Accuracy: 33.19\n",
            "[15/150]: Training Loss: 1.6995231729507447, Training Accuracy: 52.77\n",
            "Validation Loss: 3.0037558374890856, Validation Accuracy: 32.92\n",
            "[16/150]: Training Loss: 1.6300209772109986, Training Accuracy: 54.165\n",
            "Validation Loss: 2.9989138818850183, Validation Accuracy: 32.86\n",
            "[17/150]: Training Loss: 1.5460361848831177, Training Accuracy: 56.605\n",
            "Validation Loss: 3.0711293569795646, Validation Accuracy: 32.69\n",
            "[18/150]: Training Loss: 1.48129998254776, Training Accuracy: 57.9875\n",
            "Validation Loss: 3.1773584519222284, Validation Accuracy: 31.98\n",
            "[19/150]: Training Loss: 1.4184319323539734, Training Accuracy: 59.43\n",
            "Validation Loss: 3.278900881481778, Validation Accuracy: 31.73\n",
            "[20/150]: Training Loss: 1.3525760270118714, Training Accuracy: 60.9775\n",
            "Validation Loss: 3.3168472605905714, Validation Accuracy: 31.51\n",
            "[21/150]: Training Loss: 1.275265542125702, Training Accuracy: 63.035\n",
            "Validation Loss: 3.5034019506660994, Validation Accuracy: 31.12\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 67.07529235645464, Test Accuracy: 5.39\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>5.39</td></tr><tr><td>Test Loss</td><td>67.07529</td></tr><tr><td>Train Accuracy</td><td>63.035</td></tr><tr><td>Train Loss</td><td>1.27527</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/y4ask5ys</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_035441-y4ask5ys/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.0001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_035656-za8h22vt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt' target=\"_blank\">learning_rate=0.01 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.622148120880127, Training Accuracy: 0.935\n",
            "Validation Loss: 4.609954472560032, Validation Accuracy: 0.99\n",
            "[2/150]: Training Loss: 4.609106838226318, Training Accuracy: 0.9475\n",
            "Validation Loss: 4.611106529357327, Validation Accuracy: 0.9\n",
            "[3/150]: Training Loss: 4.608888108062744, Training Accuracy: 0.9925\n",
            "Validation Loss: 4.608767968074531, Validation Accuracy: 0.84\n",
            "[4/150]: Training Loss: 4.608837638854981, Training Accuracy: 0.935\n",
            "Validation Loss: 4.610516976399027, Validation Accuracy: 0.89\n",
            "[5/150]: Training Loss: 4.608827792358398, Training Accuracy: 0.9775\n",
            "Validation Loss: 4.610510003035236, Validation Accuracy: 0.9\n",
            "[6/150]: Training Loss: 4.608845700073243, Training Accuracy: 0.935\n",
            "Validation Loss: 4.609803257474474, Validation Accuracy: 0.83\n",
            "[7/150]: Training Loss: 4.608741146087646, Training Accuracy: 0.9625\n",
            "Validation Loss: 4.60897787665106, Validation Accuracy: 0.89\n",
            "[8/150]: Training Loss: 4.608720026397705, Training Accuracy: 0.9775\n",
            "Validation Loss: 4.610442146374162, Validation Accuracy: 0.89\n",
            "[9/150]: Training Loss: 4.60900821762085, Training Accuracy: 1.005\n",
            "Validation Loss: 4.609640285467646, Validation Accuracy: 0.9\n",
            "[10/150]: Training Loss: 4.609175831604004, Training Accuracy: 0.9175\n",
            "Validation Loss: 4.609878120908312, Validation Accuracy: 0.95\n",
            "[11/150]: Training Loss: 4.609065142822265, Training Accuracy: 0.8625\n",
            "Validation Loss: 4.610616553361249, Validation Accuracy: 0.9\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 19.343990089027745, Test Accuracy: 1.01\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.01</td></tr><tr><td>Test Loss</td><td>19.34399</td></tr><tr><td>Train Accuracy</td><td>0.8625</td></tr><tr><td>Train Loss</td><td>4.60907</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/za8h22vt</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_035656-za8h22vt/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_035822-1oq8f1cm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm' target=\"_blank\">learning_rate=0.01 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.625637398529053, Training Accuracy: 0.885\n",
            "Validation Loss: 4.60909015509733, Validation Accuracy: 1.06\n",
            "[2/150]: Training Loss: 4.608977200317383, Training Accuracy: 0.94\n",
            "Validation Loss: 4.609363568056921, Validation Accuracy: 0.92\n",
            "[3/150]: Training Loss: 4.608765270996094, Training Accuracy: 0.9275\n",
            "Validation Loss: 4.610460181145152, Validation Accuracy: 0.9\n",
            "[4/150]: Training Loss: 4.609055269622803, Training Accuracy: 0.855\n",
            "Validation Loss: 4.609649321076217, Validation Accuracy: 0.91\n",
            "[5/150]: Training Loss: 4.608893507385254, Training Accuracy: 0.965\n",
            "Validation Loss: 4.610334041012321, Validation Accuracy: 1.15\n",
            "[6/150]: Training Loss: 4.609225297546387, Training Accuracy: 1.0125\n",
            "Validation Loss: 4.608022522774472, Validation Accuracy: 1.16\n",
            "[7/150]: Training Loss: 4.609016616821289, Training Accuracy: 0.9875\n",
            "Validation Loss: 4.609603547746209, Validation Accuracy: 0.82\n",
            "[8/150]: Training Loss: 4.609039859008789, Training Accuracy: 1.0025\n",
            "Validation Loss: 4.608962860836345, Validation Accuracy: 0.88\n",
            "[9/150]: Training Loss: 4.608584417724609, Training Accuracy: 0.98\n",
            "Validation Loss: 4.609565239803047, Validation Accuracy: 0.92\n",
            "[10/150]: Training Loss: 4.609195093536377, Training Accuracy: 0.9675\n",
            "Validation Loss: 4.609652103132503, Validation Accuracy: 0.9\n",
            "[11/150]: Training Loss: 4.608826132202148, Training Accuracy: 0.975\n",
            "Validation Loss: 4.610933561993253, Validation Accuracy: 0.95\n",
            "[12/150]: Training Loss: 4.609026128387451, Training Accuracy: 1.0425\n",
            "Validation Loss: 4.609673776444356, Validation Accuracy: 0.81\n",
            "[13/150]: Training Loss: 4.60898267364502, Training Accuracy: 0.9275\n",
            "Validation Loss: 4.6093105510541585, Validation Accuracy: 0.89\n",
            "[14/150]: Training Loss: 4.608909271240234, Training Accuracy: 0.885\n",
            "Validation Loss: 4.610904125650977, Validation Accuracy: 0.82\n",
            "[15/150]: Training Loss: 4.609115404510498, Training Accuracy: 0.925\n",
            "Validation Loss: 4.609269661508548, Validation Accuracy: 0.9\n",
            "[16/150]: Training Loss: 4.608642578887939, Training Accuracy: 0.9875\n",
            "Validation Loss: 4.610215545459917, Validation Accuracy: 0.81\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 51.00807974748551, Test Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.0</td></tr><tr><td>Test Loss</td><td>51.00808</td></tr><tr><td>Train Accuracy</td><td>0.9875</td></tr><tr><td>Train Loss</td><td>4.60864</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/1oq8f1cm</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_035822-1oq8f1cm/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_040031-bgr70v9g</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g' target=\"_blank\">learning_rate=0.01 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.626314550018311, Training Accuracy: 0.9725\n",
            "Validation Loss: 4.610879454643103, Validation Accuracy: 0.88\n",
            "[2/150]: Training Loss: 4.609021481323242, Training Accuracy: 0.9225\n",
            "Validation Loss: 4.6096283493527945, Validation Accuracy: 1.0\n",
            "[3/150]: Training Loss: 4.608779692077637, Training Accuracy: 0.9575\n",
            "Validation Loss: 4.610723838684665, Validation Accuracy: 1.07\n",
            "[4/150]: Training Loss: 4.609119167327881, Training Accuracy: 0.9125\n",
            "Validation Loss: 4.60863508236636, Validation Accuracy: 0.89\n",
            "[5/150]: Training Loss: 4.608814185333252, Training Accuracy: 0.95\n",
            "Validation Loss: 4.611634175488903, Validation Accuracy: 0.93\n",
            "[6/150]: Training Loss: 4.6092648048400875, Training Accuracy: 0.8575\n",
            "Validation Loss: 4.610189516832874, Validation Accuracy: 0.92\n",
            "[7/150]: Training Loss: 4.6090187705993655, Training Accuracy: 1.005\n",
            "Validation Loss: 4.610130288798338, Validation Accuracy: 0.88\n",
            "[8/150]: Training Loss: 4.608886881256104, Training Accuracy: 1.01\n",
            "Validation Loss: 4.610127786162552, Validation Accuracy: 0.84\n",
            "[9/150]: Training Loss: 4.608884384155274, Training Accuracy: 1.0175\n",
            "Validation Loss: 4.610361232879056, Validation Accuracy: 0.95\n",
            "[10/150]: Training Loss: 4.60871681137085, Training Accuracy: 1.0025\n",
            "Validation Loss: 4.6113608263100785, Validation Accuracy: 1.03\n",
            "[11/150]: Training Loss: 4.608806131744385, Training Accuracy: 1.035\n",
            "Validation Loss: 4.609321922253651, Validation Accuracy: 0.94\n",
            "[12/150]: Training Loss: 4.6090567108154294, Training Accuracy: 0.9075\n",
            "Validation Loss: 4.6090145414801915, Validation Accuracy: 0.9\n",
            "[13/150]: Training Loss: 4.608706290435791, Training Accuracy: 0.9225\n",
            "Validation Loss: 4.609259438362851, Validation Accuracy: 0.82\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 7.142075587230123, Test Accuracy: 1.08\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.08</td></tr><tr><td>Test Loss</td><td>7.14208</td></tr><tr><td>Train Accuracy</td><td>0.9225</td></tr><tr><td>Train Loss</td><td>4.60871</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning/runs/bgr70v9g</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_040031-bgr70v9g/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "\n",
        "for lr in learning_rates:\n",
        "  for wd in weight_decays:\n",
        "\n",
        "    print('='*50)\n",
        "    print(f'Hyperparameter with lr:{lr} and wd:{wd}')\n",
        "    print('='*50)\n",
        "\n",
        "    hyperparameters = {'learning_rate': lr,\n",
        "                       'weight_decay' : wd\n",
        "                       }\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(num_epochs, model, train_loader, validation_loader, test_loader, optimizer, scheduler, criterion, device, 'AdamW-HyperParameterTuning', hyperparameters=hyperparameters, is_wandb = True, n_epochs_stop = 10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training Using Best Hyperparameters Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "63dd47b50bdc44e88edd97d14585f7ea",
            "5e0116dc735e4ad995a5b1a039c6a55a",
            "8cf1e952ed294ccdbc3bb2275b5d3efd",
            "7421027bfcb34262a1b99d297e49bf8e",
            "76dfea14213f4e66b269b901150c6cba",
            "d1e4a03094b94c34ac235ffbd0c7fa06",
            "c8e410ff55fc448bbe87c8975a552e88",
            "f49251ce876f4d9292a4179b48ebb22a"
          ]
        },
        "id": "dC9sTmuvEymk",
        "outputId": "66657750-ed1c-435f-c9e5-a2cc4271560e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:z7dx25wy) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/z7dx25wy' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/z7dx25wy</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_002941-z7dx25wy/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:z7dx25wy). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_003350-ulxra0pg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg' target=\"_blank\">learning_rate=0.001 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 3.863059723773576, Training Accuracy: 10.28\n",
            "Validation Loss: 3.371582930255088, Validation Accuracy: 17.92\n",
            "[2/150]: Training Loss: 3.2885083447941734, Training Accuracy: 19.766\n",
            "Validation Loss: 3.0681341438536434, Validation Accuracy: 23.43\n",
            "[3/150]: Training Loss: 3.0068617961595736, Training Accuracy: 25.216\n",
            "Validation Loss: 2.8509516169311135, Validation Accuracy: 28.09\n",
            "[4/150]: Training Loss: 2.8243453850221756, Training Accuracy: 28.812\n",
            "Validation Loss: 2.6608721253218923, Validation Accuracy: 31.85\n",
            "[5/150]: Training Loss: 2.7198512532826884, Training Accuracy: 30.66\n",
            "Validation Loss: 2.614684703243766, Validation Accuracy: 32.43\n",
            "[6/150]: Training Loss: 2.6302451995937415, Training Accuracy: 32.706\n",
            "Validation Loss: 2.523006957807359, Validation Accuracy: 35.09\n",
            "[7/150]: Training Loss: 2.5562818654053046, Training Accuracy: 34.01\n",
            "Validation Loss: 2.4693225895523265, Validation Accuracy: 36.29\n",
            "[8/150]: Training Loss: 2.503678919409242, Training Accuracy: 35.054\n",
            "Validation Loss: 2.4447873132244036, Validation Accuracy: 36.7\n",
            "[9/150]: Training Loss: 2.4561512488538346, Training Accuracy: 36.162\n",
            "Validation Loss: 2.41026672663962, Validation Accuracy: 37.97\n",
            "[10/150]: Training Loss: 2.4110919961234187, Training Accuracy: 37.32\n",
            "Validation Loss: 2.3787831933635055, Validation Accuracy: 37.75\n",
            "[11/150]: Training Loss: 2.374972592686753, Training Accuracy: 37.942\n",
            "Validation Loss: 2.419552489450783, Validation Accuracy: 38.02\n",
            "[12/150]: Training Loss: 2.341363592220999, Training Accuracy: 38.71\n",
            "Validation Loss: 2.353040744544594, Validation Accuracy: 39.18\n",
            "[13/150]: Training Loss: 2.3113136654314785, Training Accuracy: 39.284\n",
            "Validation Loss: 2.329031374044479, Validation Accuracy: 40.04\n",
            "[14/150]: Training Loss: 2.286358056928191, Training Accuracy: 39.702\n",
            "Validation Loss: 2.353606297711658, Validation Accuracy: 38.89\n",
            "[15/150]: Training Loss: 2.255754057251279, Training Accuracy: 40.382\n",
            "Validation Loss: 2.313938088477797, Validation Accuracy: 40.46\n",
            "[16/150]: Training Loss: 2.2417570858660256, Training Accuracy: 40.766\n",
            "Validation Loss: 2.245134223798278, Validation Accuracy: 41.86\n",
            "[17/150]: Training Loss: 2.229791578886759, Training Accuracy: 40.774\n",
            "Validation Loss: 2.2972328845103074, Validation Accuracy: 40.73\n",
            "[18/150]: Training Loss: 2.198040244981761, Training Accuracy: 41.792\n",
            "Validation Loss: 2.284319670337021, Validation Accuracy: 40.6\n",
            "[19/150]: Training Loss: 2.1809934542307157, Training Accuracy: 41.802\n",
            "Validation Loss: 2.2219127932931206, Validation Accuracy: 41.97\n",
            "[20/150]: Training Loss: 2.172169676827043, Training Accuracy: 41.862\n",
            "Validation Loss: 2.3093747029638596, Validation Accuracy: 40.82\n",
            "[21/150]: Training Loss: 2.1469304264353974, Training Accuracy: 42.644\n",
            "Validation Loss: 2.2553204001894422, Validation Accuracy: 41.67\n",
            "[22/150]: Training Loss: 2.1338748418156754, Training Accuracy: 43.018\n",
            "Validation Loss: 2.300890388002821, Validation Accuracy: 40.66\n",
            "[23/150]: Training Loss: 2.120842968259016, Training Accuracy: 43.254\n",
            "Validation Loss: 2.2334311372914892, Validation Accuracy: 41.97\n",
            "[24/150]: Training Loss: 2.0970673977261614, Training Accuracy: 43.616\n",
            "Validation Loss: 2.2538574728996132, Validation Accuracy: 42.09\n",
            "[25/150]: Training Loss: 2.0897668813500565, Training Accuracy: 43.946\n",
            "Validation Loss: 2.2414238969231866, Validation Accuracy: 41.72\n",
            "[26/150]: Training Loss: 2.0756560088423512, Training Accuracy: 44.038\n",
            "Validation Loss: 2.1973593523547907, Validation Accuracy: 43.01\n",
            "[27/150]: Training Loss: 2.057910572537376, Training Accuracy: 44.59\n",
            "Validation Loss: 2.2248903679999574, Validation Accuracy: 42.7\n",
            "[28/150]: Training Loss: 2.043146767274803, Training Accuracy: 45.032\n",
            "Validation Loss: 2.210173088274184, Validation Accuracy: 42.56\n",
            "[29/150]: Training Loss: 2.03314662345535, Training Accuracy: 45.244\n",
            "Validation Loss: 2.2085875697955966, Validation Accuracy: 42.59\n",
            "[30/150]: Training Loss: 2.028535710576245, Training Accuracy: 45.074\n",
            "Validation Loss: 2.2007097385491536, Validation Accuracy: 43.4\n",
            "[31/150]: Training Loss: 2.009151526576723, Training Accuracy: 45.862\n",
            "Validation Loss: 2.201018148926413, Validation Accuracy: 43.24\n",
            "[32/150]: Training Loss: 1.9960764486466527, Training Accuracy: 46.118\n",
            "Validation Loss: 2.207929955925911, Validation Accuracy: 43.19\n",
            "[33/150]: Training Loss: 1.988575564930811, Training Accuracy: 46.282\n",
            "Validation Loss: 2.2023386712286883, Validation Accuracy: 43.24\n",
            "[34/150]: Training Loss: 1.9779068563905213, Training Accuracy: 46.246\n",
            "Validation Loss: 2.1736220698447744, Validation Accuracy: 43.85\n",
            "[35/150]: Training Loss: 1.9696562569159681, Training Accuracy: 46.612\n",
            "Validation Loss: 2.2412819467532406, Validation Accuracy: 42.97\n",
            "[36/150]: Training Loss: 1.9518849866469499, Training Accuracy: 47.002\n",
            "Validation Loss: 2.142467891334728, Validation Accuracy: 44.48\n",
            "[37/150]: Training Loss: 1.942737179491526, Training Accuracy: 47.244\n",
            "Validation Loss: 2.1808249912444193, Validation Accuracy: 43.96\n",
            "[38/150]: Training Loss: 1.934893620441027, Training Accuracy: 47.278\n",
            "Validation Loss: 2.194337487220764, Validation Accuracy: 44.03\n",
            "[39/150]: Training Loss: 1.9254444508296449, Training Accuracy: 47.672\n",
            "Validation Loss: 2.1627620777506738, Validation Accuracy: 44.64\n",
            "[40/150]: Training Loss: 1.9116085487253525, Training Accuracy: 47.786\n",
            "Validation Loss: 2.143426476770146, Validation Accuracy: 44.69\n",
            "[41/150]: Training Loss: 1.8990542265155432, Training Accuracy: 48.106\n",
            "Validation Loss: 2.1570872994744854, Validation Accuracy: 44.18\n",
            "[42/150]: Training Loss: 1.8952381147448059, Training Accuracy: 48.148\n",
            "Validation Loss: 2.179479962701251, Validation Accuracy: 44.42\n",
            "[43/150]: Training Loss: 1.8831077039699116, Training Accuracy: 48.608\n",
            "Validation Loss: 2.14893990337469, Validation Accuracy: 44.65\n",
            "[44/150]: Training Loss: 1.8653588525169646, Training Accuracy: 49.024\n",
            "Validation Loss: 2.139679863954046, Validation Accuracy: 44.98\n",
            "[45/150]: Training Loss: 1.8571323093855778, Training Accuracy: 49.132\n",
            "Validation Loss: 2.1511670715489966, Validation Accuracy: 45.05\n",
            "[46/150]: Training Loss: 1.851196085248152, Training Accuracy: 49.268\n",
            "Validation Loss: 2.1228003342440176, Validation Accuracy: 45.15\n",
            "[47/150]: Training Loss: 1.8476994010188696, Training Accuracy: 49.152\n",
            "Validation Loss: 2.2052687976011045, Validation Accuracy: 44.33\n",
            "[48/150]: Training Loss: 1.834682262919443, Training Accuracy: 49.48\n",
            "Validation Loss: 2.167970584456328, Validation Accuracy: 44.5\n",
            "[49/150]: Training Loss: 1.8160154127403902, Training Accuracy: 50.016\n",
            "Validation Loss: 2.1706748312445963, Validation Accuracy: 44.33\n",
            "[50/150]: Training Loss: 1.8177930782823002, Training Accuracy: 49.864\n",
            "Validation Loss: 2.1794446357496224, Validation Accuracy: 44.96\n",
            "[51/150]: Training Loss: 1.8067406710913725, Training Accuracy: 50.086\n",
            "Validation Loss: 2.1342575109688338, Validation Accuracy: 45.96\n",
            "[52/150]: Training Loss: 1.7917756205019744, Training Accuracy: 50.474\n",
            "Validation Loss: 2.1779661793617686, Validation Accuracy: 45.2\n",
            "[53/150]: Training Loss: 1.7835827201528622, Training Accuracy: 50.574\n",
            "Validation Loss: 2.105961331136667, Validation Accuracy: 46.26\n",
            "[54/150]: Training Loss: 1.775593501527596, Training Accuracy: 50.768\n",
            "Validation Loss: 2.135809161860472, Validation Accuracy: 45.59\n",
            "[55/150]: Training Loss: 1.774858244087385, Training Accuracy: 50.898\n",
            "Validation Loss: 2.129430113324694, Validation Accuracy: 45.8\n",
            "[56/150]: Training Loss: 1.7574954881997364, Training Accuracy: 51.258\n",
            "Validation Loss: 2.1614714724243065, Validation Accuracy: 45.34\n",
            "[57/150]: Training Loss: 1.7551402684367832, Training Accuracy: 51.536\n",
            "Validation Loss: 2.116780410906312, Validation Accuracy: 45.57\n",
            "[58/150]: Training Loss: 1.7323481986284865, Training Accuracy: 51.87\n",
            "Validation Loss: 2.1469044146264435, Validation Accuracy: 46.29\n",
            "[59/150]: Training Loss: 1.731923724684264, Training Accuracy: 52.004\n",
            "Validation Loss: 2.103028847913074, Validation Accuracy: 46.36\n",
            "[60/150]: Training Loss: 1.7310465676400362, Training Accuracy: 52.0\n",
            "Validation Loss: 2.1536702441561753, Validation Accuracy: 45.7\n",
            "[61/150]: Training Loss: 1.7244189166656845, Training Accuracy: 51.944\n",
            "Validation Loss: 2.140022467655741, Validation Accuracy: 45.48\n",
            "[62/150]: Training Loss: 1.7049367572645397, Training Accuracy: 52.6\n",
            "Validation Loss: 2.1410929146845628, Validation Accuracy: 46.43\n",
            "[63/150]: Training Loss: 1.6979511242998226, Training Accuracy: 52.706\n",
            "Validation Loss: 2.159957699714952, Validation Accuracy: 45.77\n",
            "[64/150]: Training Loss: 1.6934019074110729, Training Accuracy: 52.83\n",
            "Validation Loss: 2.1361425302590535, Validation Accuracy: 46.59\n",
            "[65/150]: Training Loss: 1.6794907693058023, Training Accuracy: 53.024\n",
            "Validation Loss: 2.1693425915043827, Validation Accuracy: 46.15\n",
            "[66/150]: Training Loss: 1.6717879327056964, Training Accuracy: 53.454\n",
            "Validation Loss: 2.1304182694975737, Validation Accuracy: 46.71\n",
            "[67/150]: Training Loss: 1.6595926739828055, Training Accuracy: 53.614\n",
            "Validation Loss: 2.120200866346906, Validation Accuracy: 46.39\n",
            "[68/150]: Training Loss: 1.648870440395287, Training Accuracy: 54.184\n",
            "Validation Loss: 2.098656458459842, Validation Accuracy: 46.87\n",
            "[69/150]: Training Loss: 1.63317440499735, Training Accuracy: 53.856\n",
            "Validation Loss: 2.10397704163934, Validation Accuracy: 47.28\n",
            "[70/150]: Training Loss: 1.6387051284465644, Training Accuracy: 54.282\n",
            "Validation Loss: 2.0964533455052954, Validation Accuracy: 47.14\n",
            "[71/150]: Training Loss: 1.6188073368633495, Training Accuracy: 54.606\n",
            "Validation Loss: 2.12077300032233, Validation Accuracy: 46.67\n",
            "[72/150]: Training Loss: 1.6113011437608762, Training Accuracy: 54.874\n",
            "Validation Loss: 2.1084914108750166, Validation Accuracy: 47.47\n",
            "[73/150]: Training Loss: 1.6181168059253936, Training Accuracy: 54.584\n",
            "Validation Loss: 2.0926199581972353, Validation Accuracy: 47.8\n",
            "[74/150]: Training Loss: 1.6009672808525202, Training Accuracy: 55.14\n",
            "Validation Loss: 2.1137902524061265, Validation Accuracy: 47.2\n",
            "[75/150]: Training Loss: 1.5877168545942477, Training Accuracy: 55.52\n",
            "Validation Loss: 2.0988229885222807, Validation Accuracy: 47.29\n",
            "[76/150]: Training Loss: 1.5841481110933797, Training Accuracy: 55.536\n",
            "Validation Loss: 2.130301916675203, Validation Accuracy: 47.01\n",
            "[77/150]: Training Loss: 1.572215504810938, Training Accuracy: 55.894\n",
            "Validation Loss: 2.149995141727909, Validation Accuracy: 47.05\n",
            "[78/150]: Training Loss: 1.5624580438179738, Training Accuracy: 55.868\n",
            "Validation Loss: 2.111784276688934, Validation Accuracy: 47.42\n",
            "[79/150]: Training Loss: 1.5614525537051813, Training Accuracy: 55.984\n",
            "Validation Loss: 2.114105687019931, Validation Accuracy: 47.26\n",
            "[80/150]: Training Loss: 1.539920823577115, Training Accuracy: 56.65\n",
            "Validation Loss: 2.113597672456389, Validation Accuracy: 48.04\n",
            "[81/150]: Training Loss: 1.5321300439822398, Training Accuracy: 56.72\n",
            "Validation Loss: 2.115057561048277, Validation Accuracy: 47.52\n",
            "[82/150]: Training Loss: 1.5347046963394146, Training Accuracy: 56.472\n",
            "Validation Loss: 2.120380314292422, Validation Accuracy: 47.69\n",
            "[83/150]: Training Loss: 1.5234107173922118, Training Accuracy: 56.854\n",
            "Validation Loss: 2.117662426772391, Validation Accuracy: 47.66\n",
            "[84/150]: Training Loss: 1.5176254797469624, Training Accuracy: 57.066\n",
            "Validation Loss: 2.1148886255397916, Validation Accuracy: 47.15\n",
            "[85/150]: Training Loss: 1.5007459478610008, Training Accuracy: 57.452\n",
            "Validation Loss: 2.114618365931663, Validation Accuracy: 47.39\n",
            "[86/150]: Training Loss: 1.4968964526872806, Training Accuracy: 57.466\n",
            "Validation Loss: 2.1021699586491676, Validation Accuracy: 47.78\n",
            "[87/150]: Training Loss: 1.4844805052518235, Training Accuracy: 57.862\n",
            "Validation Loss: 2.1039314968570784, Validation Accuracy: 47.72\n",
            "[88/150]: Training Loss: 1.4738873809652255, Training Accuracy: 58.114\n",
            "Validation Loss: 2.097645890181232, Validation Accuracy: 48.24\n",
            "[89/150]: Training Loss: 1.483021430271056, Training Accuracy: 57.876\n",
            "Validation Loss: 2.108822873443555, Validation Accuracy: 47.97\n",
            "[90/150]: Training Loss: 1.4626602787343437, Training Accuracy: 58.702\n",
            "Validation Loss: 2.0972186729406856, Validation Accuracy: 47.88\n",
            "[91/150]: Training Loss: 1.4539960583152673, Training Accuracy: 58.564\n",
            "Validation Loss: 2.1077453277672933, Validation Accuracy: 48.02\n",
            "[92/150]: Training Loss: 1.4446301146236527, Training Accuracy: 58.7\n",
            "Validation Loss: 2.0844076651676446, Validation Accuracy: 48.44\n",
            "[93/150]: Training Loss: 1.4391108949471008, Training Accuracy: 59.026\n",
            "Validation Loss: 2.093005408147338, Validation Accuracy: 48.37\n",
            "[94/150]: Training Loss: 1.423930914581889, Training Accuracy: 59.616\n",
            "Validation Loss: 2.1331047563795833, Validation Accuracy: 47.89\n",
            "[95/150]: Training Loss: 1.41947230582347, Training Accuracy: 59.502\n",
            "Validation Loss: 2.1126899483856882, Validation Accuracy: 48.0\n",
            "[96/150]: Training Loss: 1.4149078359384366, Training Accuracy: 59.56\n",
            "Validation Loss: 2.0845493116196554, Validation Accuracy: 48.7\n",
            "[97/150]: Training Loss: 1.3990580768841308, Training Accuracy: 60.132\n",
            "Validation Loss: 2.1009285654991294, Validation Accuracy: 48.44\n",
            "[98/150]: Training Loss: 1.4016986463380896, Training Accuracy: 60.082\n",
            "Validation Loss: 2.1022102696121117, Validation Accuracy: 48.23\n",
            "[99/150]: Training Loss: 1.391786497572194, Training Accuracy: 60.44\n",
            "Validation Loss: 2.087019986407772, Validation Accuracy: 48.33\n",
            "[100/150]: Training Loss: 1.3808096985682807, Training Accuracy: 60.598\n",
            "Validation Loss: 2.1079373640619266, Validation Accuracy: 48.46\n",
            "[101/150]: Training Loss: 1.375501683377244, Training Accuracy: 60.572\n",
            "Validation Loss: 2.1078098131592866, Validation Accuracy: 48.82\n",
            "[102/150]: Training Loss: 1.3738657930470488, Training Accuracy: 60.484\n",
            "Validation Loss: 2.118699251466496, Validation Accuracy: 47.93\n",
            "[103/150]: Training Loss: 1.366849816973557, Training Accuracy: 60.848\n",
            "Validation Loss: 2.07916833564734, Validation Accuracy: 49.02\n",
            "[104/150]: Training Loss: 1.3610880574606874, Training Accuracy: 60.932\n",
            "Validation Loss: 2.087844172860407, Validation Accuracy: 48.91\n",
            "[105/150]: Training Loss: 1.34294292833799, Training Accuracy: 61.462\n",
            "Validation Loss: 2.1156119442289802, Validation Accuracy: 49.17\n",
            "[106/150]: Training Loss: 1.3525332610320557, Training Accuracy: 61.288\n",
            "Validation Loss: 2.0910707043994003, Validation Accuracy: 48.55\n",
            "[107/150]: Training Loss: 1.331598934538834, Training Accuracy: 61.91\n",
            "Validation Loss: 2.1001909211942347, Validation Accuracy: 49.0\n",
            "[108/150]: Training Loss: 1.3267096242179042, Training Accuracy: 61.816\n",
            "Validation Loss: 2.1083290622492505, Validation Accuracy: 48.74\n",
            "[109/150]: Training Loss: 1.3154139440230397, Training Accuracy: 62.34\n",
            "Validation Loss: 2.1171723535865734, Validation Accuracy: 49.04\n",
            "[110/150]: Training Loss: 1.3305041518662593, Training Accuracy: 61.83\n",
            "Validation Loss: 2.0921388515241586, Validation Accuracy: 49.21\n",
            "[111/150]: Training Loss: 1.3154786750483696, Training Accuracy: 61.994\n",
            "Validation Loss: 2.1219316781706112, Validation Accuracy: 48.77\n",
            "[112/150]: Training Loss: 1.3147668346876988, Training Accuracy: 61.946\n",
            "Validation Loss: 2.0896254671607046, Validation Accuracy: 48.89\n",
            "[113/150]: Training Loss: 1.3001932930915863, Training Accuracy: 62.724\n",
            "Validation Loss: 2.103232970662937, Validation Accuracy: 49.02\n",
            "[114/150]: Training Loss: 1.2976091802120209, Training Accuracy: 62.64\n",
            "Validation Loss: 2.089232644457726, Validation Accuracy: 48.88\n",
            "[115/150]: Training Loss: 1.2918279953015126, Training Accuracy: 62.894\n",
            "Validation Loss: 2.098377283971021, Validation Accuracy: 49.17\n",
            "[116/150]: Training Loss: 1.2901173764482483, Training Accuracy: 63.008\n",
            "Validation Loss: 2.096894028080497, Validation Accuracy: 49.39\n",
            "[117/150]: Training Loss: 1.275385139619603, Training Accuracy: 63.266\n",
            "Validation Loss: 2.1110763952230953, Validation Accuracy: 49.35\n",
            "[118/150]: Training Loss: 1.2755424442803462, Training Accuracy: 63.438\n",
            "Validation Loss: 2.1062107443050215, Validation Accuracy: 49.34\n",
            "[119/150]: Training Loss: 1.2707537008673333, Training Accuracy: 63.144\n",
            "Validation Loss: 2.094398064977804, Validation Accuracy: 49.44\n",
            "[120/150]: Training Loss: 1.2619870495613275, Training Accuracy: 63.662\n",
            "Validation Loss: 2.109912445590754, Validation Accuracy: 49.31\n",
            "[121/150]: Training Loss: 1.2651302716921053, Training Accuracy: 63.564\n",
            "Validation Loss: 2.1148870439286442, Validation Accuracy: 48.87\n",
            "[122/150]: Training Loss: 1.2517427335614744, Training Accuracy: 63.992\n",
            "Validation Loss: 2.111772822726304, Validation Accuracy: 49.14\n",
            "[123/150]: Training Loss: 1.2525572213522917, Training Accuracy: 63.8\n",
            "Validation Loss: 2.097744130784539, Validation Accuracy: 49.4\n",
            "[124/150]: Training Loss: 1.2481763019891041, Training Accuracy: 63.982\n",
            "Validation Loss: 2.110331610509544, Validation Accuracy: 49.5\n",
            "[125/150]: Training Loss: 1.242631159322646, Training Accuracy: 64.258\n",
            "Validation Loss: 2.113812481521801, Validation Accuracy: 49.25\n",
            "[126/150]: Training Loss: 1.2439294564144692, Training Accuracy: 64.134\n",
            "Validation Loss: 2.106003999710083, Validation Accuracy: 49.44\n",
            "[127/150]: Training Loss: 1.2376054860746768, Training Accuracy: 64.1\n",
            "Validation Loss: 2.1138451304405357, Validation Accuracy: 49.45\n",
            "[128/150]: Training Loss: 1.2359750416425184, Training Accuracy: 64.344\n",
            "Validation Loss: 2.0956578482488157, Validation Accuracy: 49.5\n",
            "[129/150]: Training Loss: 1.2426931114910205, Training Accuracy: 64.084\n",
            "Validation Loss: 2.102140469915548, Validation Accuracy: 49.39\n",
            "[130/150]: Training Loss: 1.2271092739099128, Training Accuracy: 64.588\n",
            "Validation Loss: 2.1065317156967844, Validation Accuracy: 49.55\n",
            "[131/150]: Training Loss: 1.2293265251552357, Training Accuracy: 64.606\n",
            "Validation Loss: 2.1087451816364458, Validation Accuracy: 49.71\n",
            "[132/150]: Training Loss: 1.2165279118606196, Training Accuracy: 64.626\n",
            "Validation Loss: 2.1100895678161815, Validation Accuracy: 49.58\n",
            "[133/150]: Training Loss: 1.2260109276112998, Training Accuracy: 64.716\n",
            "Validation Loss: 2.107479982315355, Validation Accuracy: 49.48\n",
            "[134/150]: Training Loss: 1.2170597407823938, Training Accuracy: 64.91\n",
            "Validation Loss: 2.109668661075033, Validation Accuracy: 49.21\n",
            "[135/150]: Training Loss: 1.2119625063655932, Training Accuracy: 64.864\n",
            "Validation Loss: 2.1121888259413897, Validation Accuracy: 49.6\n",
            "[136/150]: Training Loss: 1.2143927402508534, Training Accuracy: 64.752\n",
            "Validation Loss: 2.1146486792594765, Validation Accuracy: 49.42\n",
            "[137/150]: Training Loss: 1.20208662275768, Training Accuracy: 65.196\n",
            "Validation Loss: 2.1086552408850117, Validation Accuracy: 49.53\n",
            "[138/150]: Training Loss: 1.2066055967679719, Training Accuracy: 65.132\n",
            "Validation Loss: 2.106570492884156, Validation Accuracy: 49.36\n",
            "[139/150]: Training Loss: 1.2045007688767464, Training Accuracy: 65.206\n",
            "Validation Loss: 2.1134595893750525, Validation Accuracy: 49.61\n",
            "[140/150]: Training Loss: 1.209533206413469, Training Accuracy: 65.066\n",
            "Validation Loss: 2.111029122285782, Validation Accuracy: 49.59\n",
            "[141/150]: Training Loss: 1.2018247985321542, Training Accuracy: 64.974\n",
            "Validation Loss: 2.112992768834351, Validation Accuracy: 49.59\n",
            "[142/150]: Training Loss: 1.2081030414384955, Training Accuracy: 65.38\n",
            "Validation Loss: 2.1114872906618056, Validation Accuracy: 49.54\n",
            "[143/150]: Training Loss: 1.1987551257128606, Training Accuracy: 65.484\n",
            "Validation Loss: 2.11025989283422, Validation Accuracy: 49.53\n",
            "[144/150]: Training Loss: 1.204268764961711, Training Accuracy: 65.084\n",
            "Validation Loss: 2.1116166456489807, Validation Accuracy: 49.52\n",
            "[145/150]: Training Loss: 1.2022794322741917, Training Accuracy: 65.088\n",
            "Validation Loss: 2.1134905511406576, Validation Accuracy: 49.47\n",
            "[146/150]: Training Loss: 1.1945363251144623, Training Accuracy: 65.452\n",
            "Validation Loss: 2.1135922700736174, Validation Accuracy: 49.52\n",
            "[147/150]: Training Loss: 1.1900141044803287, Training Accuracy: 65.494\n",
            "Validation Loss: 2.113608984430884, Validation Accuracy: 49.49\n",
            "[148/150]: Training Loss: 1.2009751156467916, Training Accuracy: 65.298\n",
            "Validation Loss: 2.1133065656491907, Validation Accuracy: 49.47\n",
            "[149/150]: Training Loss: 1.2031257037464005, Training Accuracy: 65.434\n",
            "Validation Loss: 2.113412342253764, Validation Accuracy: 49.47\n",
            "[150/150]: Training Loss: 1.1963014528726983, Training Accuracy: 65.458\n",
            "Validation Loss: 2.113395432757724, Validation Accuracy: 49.47\n",
            "**********************************************************************\n",
            "Test Loss: 2.113395432757724, Test Accuracy: 49.47\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>49.47</td></tr><tr><td>Test Loss</td><td>2.1134</td></tr><tr><td>Train Accuracy</td><td>65.458</td></tr><tr><td>Train Loss</td><td>1.1963</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW/runs/ulxra0pg</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-AdamW</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_003350-ulxra0pg/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 1e-03\n",
        "wd = 4e-04\n",
        "\n",
        "hyperparameters = {'learning_rate': lr,\n",
        "                    'weight_decay' : wd\n",
        "                  }\n",
        "\n",
        "# Load the model\n",
        "model_1 = LeNet5().to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer_1 = torch.optim.AdamW(model_1.parameters(), lr=lr, weight_decay=wd)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_1, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# Training\n",
        "run_training(num_epochs, model_1, original_train_loader, original_test_loader, original_test_loader, optimizer_1, scheduler, criterion, device, optimizer_name='AdamW', hyperparameters=hyperparameters, is_wandb = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeMWj_f0sbQE"
      },
      "source": [
        "### **Large Batch Optimizers**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WkZmVFG0q90m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class LARS(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements LARS (Layer-wise Adaptive Rate Scaling).\n",
        "\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "        lr (float): learning rate (default: 1e-3)\n",
        "        momentum (float, optional): momentum factor (default: 0)\n",
        "        trust_coef (float, optional): LARS coefficient as used in the paper (default: 1e-3)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        dampening (float, optional): dampening for momentum (default: 0)\n",
        "        nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
        "        epsilon (float, optional): epsilon to prevent zero division (default: 0)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            params,\n",
        "            lr: float = 1e-3,\n",
        "            momentum: float = 0,\n",
        "            trust_coef: float = 1e-3,\n",
        "            dampening: float = 0,\n",
        "            weight_decay: float = 0,\n",
        "            nesterov=False,\n",
        "            epsilon: float = 1e-9\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a new instance of the LARS optimizer.\n",
        "\n",
        "        Args:\n",
        "            params: iterable of parameters to optimize or dicts defining\n",
        "            lr: learning rate\n",
        "            momentum: momentum factor\n",
        "            trust_coef: LARS coefficient as used in the paper\n",
        "            weight_decay: weight decay (L2 penalty)\n",
        "            dampening: dampening for momentum\n",
        "            nesterov: enables Nesterov momentum\n",
        "            epsilon: epsilon to prevent zero division\n",
        "        \"\"\"\n",
        "\n",
        "        if lr <= 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            trust_coef=trust_coef,\n",
        "            dampening=dampening,\n",
        "            weight_decay=weight_decay,\n",
        "            nesterov=nesterov,\n",
        "            epsilon=epsilon)\n",
        "\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(LARS, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        \"\"\"\n",
        "        Sets the state of the optimizer.\n",
        "\n",
        "        Args:\n",
        "            state: The state to set the optimizer to.\n",
        "        \"\"\"\n",
        "        super(LARS, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def _compute_local_lr(self, p, weight_decay, trust_coef, epsilon):\n",
        "        \"\"\"\n",
        "        Computes the local learning rate for a given parameter.\n",
        "\n",
        "        Args:\n",
        "            p: The parameter to compute the local learning rate for.\n",
        "            weight_decay: The weight decay factor.\n",
        "            trust_coef: The trust coefficient.\n",
        "            epsilon: A small constant for numerical stability.\n",
        "\n",
        "        Returns:\n",
        "            float: The computed local learning rate.\n",
        "        \"\"\"\n",
        "        w_norm = torch.norm(p.data)\n",
        "        g_norm = torch.norm(p.grad.data)\n",
        "        if w_norm * g_norm > 0:\n",
        "            return trust_coef * w_norm / (g_norm + weight_decay * w_norm + epsilon)\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    def _update_params(self, p, d_p, local_lr, lr, momentum, buf,\n",
        "                       dampening, nesterov, weight_decay):\n",
        "        \"\"\"\n",
        "        Updates the parameters with the computed update.\n",
        "\n",
        "        Args:\n",
        "            p: The parameter to be updated.\n",
        "            d_p: The computed update for the parameter.\n",
        "            local_lr: The local learning rate.\n",
        "            lr: The global learning rate.\n",
        "            momentum: The momentum factor.\n",
        "            buf: The buffer for the momentum.\n",
        "            dampening: The dampening for the momentum.\n",
        "            nesterov: A flag indicating whether to use Nesterov momentum.\n",
        "            weight_decay: The weight decay factor.\n",
        "        \"\"\"\n",
        "        if weight_decay != 0:\n",
        "            d_p.add_(weight_decay, p.data)\n",
        "        if momentum != 0:\n",
        "            param_state = self.state[p]\n",
        "            if 'momentum_buffer' not in param_state:\n",
        "                buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "            else:\n",
        "                buf = param_state['momentum_buffer']\n",
        "            buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "            if nesterov:\n",
        "                d_p = d_p.add(momentum, buf)\n",
        "            else:\n",
        "                d_p = buf\n",
        "\n",
        "        p.data.add_(-local_lr * lr, d_p)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            trust_coef = group['trust_coef']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "            epsilon = group['epsilon']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                local_lr = self._compute_local_lr(p, weight_decay, trust_coef, epsilon)\n",
        "                self._update_params(p, p.grad.data, local_lr, group['lr'], momentum, None, dampening, nesterov, weight_decay)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "k3ow7VNfs377"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from typing import Optional, Union, Callable, List\n",
        "\n",
        "class LAMB(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements LAMB (Layer-wise Adaptive Moments) optimizer.\n",
        "\n",
        "    Args:\n",
        "        params (iterable): Iterable of parameters to optimize or dicts defining parameter groups.\n",
        "        learning_rate (Union[float, Callable], optional): The learning rate. Default is 0.001.\n",
        "        beta_1 (float, optional): The exponential decay rate for the 1st moment estimates. Default is 0.9.\n",
        "        beta_2 (float, optional): The exponential decay rate for the 2nd moment estimates. Default is 0.999.\n",
        "        epsilon (float, optional): A small constant for numerical stability. Default is 1e-6.\n",
        "        weight_decay (float, optional): Weight decay. Default is 0.0.\n",
        "        exclude_from_weight_decay (Optional[List[str]], optional): List of regex patterns of variables excluded from weight decay. Variables whose name contain a substring matching the pattern will be excluded. Default is None.\n",
        "        exclude_from_layer_adaptation (Optional[List[str]], optional): List of regex patterns of variables excluded from layer adaptation. Variables whose name contain a substring matching the pattern will be excluded. Default is None.\n",
        "        name (str, optional): Optional name for the operations created when applying gradients. Defaults to \"LAMB\".\n",
        "        **kwargs: Keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip gradients by value, `decay` is included for backward compatibility to allow time inverse decay of learning rate. `lr` is included for backward compatibility, recommended to use `learning_rate` instead.\n",
        "\n",
        "    Note:\n",
        "        - If \"weight_decay_rate\" is found in kwargs, it will be renamed to \"weight_decay\", and will be deprecated in Addons 0.18.\n",
        "        - If exclude_from_layer_adaptation is None, it will be set to exclude_from_weight_decay.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr: Union[float, Callable] = 0.001,\n",
        "        beta_1: float = 0.9,\n",
        "        beta_2: float = 0.999,\n",
        "        epsilon: float = 1e-6,\n",
        "        weight_decay: float = 0.0,\n",
        "        exclude_from_weight_decay: Optional[List[str]] = None,\n",
        "        exclude_from_layer_adaptation: Optional[List[str]] = None,\n",
        "        name: str = \"LAMB\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a new instance of the LAMB optimizer.\n",
        "        \"\"\"\n",
        "\n",
        "        if \"weight_decay_rate\" in kwargs:\n",
        "            warnings.warn(\n",
        "                \"weight_decay_rate has been renamed to weight_decay,\"\n",
        "                \"and will be deprecated in Addons 0.18.\",\n",
        "                DeprecationWarning,\n",
        "            )\n",
        "            weight_decay = kwargs[\"weight_decay_rate\"]\n",
        "            del kwargs[\"weight_decay_rate\"]\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            betas=(beta_1, beta_2),\n",
        "            eps=epsilon,\n",
        "            weight_decay=weight_decay,\n",
        "            **kwargs)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if\n",
        "        # the arg is None.\n",
        "        if exclude_from_layer_adaptation:\n",
        "            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
        "        else:\n",
        "            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
        "\n",
        "    def _compute_update(self, p, grad, state, group):\n",
        "        \"\"\"\n",
        "        Computes the update for a given parameter.\n",
        "\n",
        "        Args:\n",
        "            p (Tensor): The parameter to be updated.\n",
        "            grad (Tensor): The gradient of the parameter.\n",
        "            state (dict): A dictionary containing information about the optimization state.\n",
        "            group (dict): A dictionary containing the optimization parameters.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The computed update for the parameter.\n",
        "        \"\"\"\n",
        "        # State initialization\n",
        "        if len(state) == 0:\n",
        "            state['step'] = 0\n",
        "            # Exponential moving average of gradient values\n",
        "            state['exp_avg'] = torch.zeros_like(p.data)\n",
        "            # Exponential moving average of squared gradient values\n",
        "            state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "        exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "        beta1, beta2 = group['betas']\n",
        "\n",
        "        state['step'] += 1\n",
        "\n",
        "        # Decay the first and second moment running average coefficient\n",
        "        exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "        exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "        denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "        update = exp_avg / denom\n",
        "\n",
        "        # LAMB layer-wise adaptation\n",
        "        r1 = p.data.pow(2).sum().sqrt()\n",
        "        r2 = update.pow(2).sum().sqrt()\n",
        "        r = torch.where(r1 == 0, torch.zeros_like(r1), r1 / r2)\n",
        "\n",
        "        return r * update\n",
        "\n",
        "    def _update_params(self, p, update, step_size, weight_decay):\n",
        "        \"\"\"\n",
        "        Updates the parameters with the computed update.\n",
        "\n",
        "        Args:\n",
        "            p (Tensor): The parameter to be updated.\n",
        "            update (Tensor): The computed update for the parameter.\n",
        "            step_size (float): The step size for the update.\n",
        "            weight_decay (float): The weight decay factor.\n",
        "        \"\"\"\n",
        "        if weight_decay != 0:\n",
        "            p.data.add_(-weight_decay * step_size, p.data)\n",
        "\n",
        "        p.data.add_(-step_size, update)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model and returns the loss. Default is None.\n",
        "        \"\"\"\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('LAMB does not support sparse gradients.')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                update = self._compute_update(p, grad, state, group)\n",
        "                self._update_params(p, update, group['lr'], group['weight_decay'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "dh421LdmM9XP"
      },
      "outputs": [],
      "source": [
        "learning_rates = [1e-02, 5e-02, 1e-01, 5e-01, 1, 1.5, 2]\n",
        "wd = 1e-03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "30d70d57987c4349b55560e5d9626201",
            "36ac25551e574241bed4d7e5a4301d95",
            "6898349b9c754ca7be91bea29cabbba5",
            "623f947ef06c41569be7fcdda4141174",
            "d268f74266044451b230f20756bab2f6",
            "6896790e80d1450c821918c7ef41e42f",
            "a30700c8bfb449cda98a40d75828f3d3",
            "77beee57b67d4bad826f616e5483b87a",
            "c59ee0d8b9fb4694ada362115bf965a9",
            "ab62d64100a84b1faae744ee0f99501a",
            "6abbcbe71a314356ba04f8349d1fc127",
            "97c3921b8e454ccdb7b4db95d4440c64",
            "50b4ba3147b540abad020ae780353f27",
            "584e5a6aada04535b64c40c3ece566e8",
            "d3326a99729a416abca13d3122196e64",
            "1d534596e41340438fc8d083fff6aa8b",
            "19ed5f5d4f714246bbdf44513ecc50f0",
            "1f9d7ac8fa8a4bcdb76fe2b2cb443f66",
            "71c0cc303cfc44ba8d989d5f8feca119",
            "07dc9b3c563e4615bec4dbd3233bf4ba",
            "a3ccc0b32e9a4ceebd6045dff63e6621",
            "104402d4cba5477499593d972bc48e9d",
            "80fe0a51b14b4e548454d4f83215336c",
            "8e13a8bef6e14973912966984e83cd4c"
          ]
        },
        "id": "dzsCB_Q9NnCD",
        "outputId": "56d144bd-faa8-4df2-a19c-7cc15394622d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.01 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_022555-nh2g7isx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx' target=\"_blank\">learning_rate=0.01 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605613048553467, Training Accuracy: 1.0325\n",
            "Validation Loss: 4.605481047539195, Validation Accuracy: 1.01\n",
            "[2/150]: Training Loss: 4.603893094635009, Training Accuracy: 1.2825\n",
            "Validation Loss: 4.603514659176966, Validation Accuracy: 1.37\n",
            "[3/150]: Training Loss: 4.601452378845215, Training Accuracy: 1.7625\n",
            "Validation Loss: 4.600479241389378, Validation Accuracy: 1.96\n",
            "[4/150]: Training Loss: 4.597398109436035, Training Accuracy: 1.815\n",
            "Validation Loss: 4.595512341541849, Validation Accuracy: 1.41\n",
            "[5/150]: Training Loss: 4.590549831390381, Training Accuracy: 1.5175\n",
            "Validation Loss: 4.587287993947411, Validation Accuracy: 1.68\n",
            "[6/150]: Training Loss: 4.579249038696289, Training Accuracy: 1.93\n",
            "Validation Loss: 4.573939116897097, Validation Accuracy: 2.37\n",
            "[7/150]: Training Loss: 4.560530513763427, Training Accuracy: 2.69\n",
            "Validation Loss: 4.5522112117451465, Validation Accuracy: 2.64\n",
            "[8/150]: Training Loss: 4.5309873321533205, Training Accuracy: 2.92\n",
            "Validation Loss: 4.519049043108703, Validation Accuracy: 2.85\n",
            "[9/150]: Training Loss: 4.488997451782226, Training Accuracy: 2.9475\n",
            "Validation Loss: 4.475813519423175, Validation Accuracy: 3.12\n",
            "[10/150]: Training Loss: 4.439574425506592, Training Accuracy: 3.3375\n",
            "Validation Loss: 4.429785430811013, Validation Accuracy: 3.32\n",
            "[11/150]: Training Loss: 4.391272030639648, Training Accuracy: 3.61\n",
            "Validation Loss: 4.386288050633327, Validation Accuracy: 3.95\n",
            "[12/150]: Training Loss: 4.346567308807373, Training Accuracy: 4.215\n",
            "Validation Loss: 4.348948326839763, Validation Accuracy: 4.02\n",
            "[13/150]: Training Loss: 4.3075446601867675, Training Accuracy: 4.48\n",
            "Validation Loss: 4.312188755934406, Validation Accuracy: 4.38\n",
            "[14/150]: Training Loss: 4.274280471801758, Training Accuracy: 4.9575\n",
            "Validation Loss: 4.284171402074729, Validation Accuracy: 4.91\n",
            "[15/150]: Training Loss: 4.24782478981018, Training Accuracy: 5.3275\n",
            "Validation Loss: 4.261963683328811, Validation Accuracy: 5.05\n",
            "[16/150]: Training Loss: 4.226929823303223, Training Accuracy: 5.7425\n",
            "Validation Loss: 4.2434815540435205, Validation Accuracy: 5.46\n",
            "[17/150]: Training Loss: 4.210706592941285, Training Accuracy: 5.9375\n",
            "Validation Loss: 4.230875344792748, Validation Accuracy: 5.78\n",
            "[18/150]: Training Loss: 4.197167670059204, Training Accuracy: 6.23\n",
            "Validation Loss: 4.218225898256727, Validation Accuracy: 5.69\n",
            "[19/150]: Training Loss: 4.1860190193176265, Training Accuracy: 6.505\n",
            "Validation Loss: 4.208940963076937, Validation Accuracy: 5.86\n",
            "[20/150]: Training Loss: 4.175828193664551, Training Accuracy: 6.6025\n",
            "Validation Loss: 4.201019479970264, Validation Accuracy: 6.0\n",
            "[21/150]: Training Loss: 4.167100285339355, Training Accuracy: 6.8025\n",
            "Validation Loss: 4.1936384735593375, Validation Accuracy: 5.97\n",
            "[22/150]: Training Loss: 4.158846283340454, Training Accuracy: 6.92\n",
            "Validation Loss: 4.185141735016161, Validation Accuracy: 6.28\n",
            "[23/150]: Training Loss: 4.151738377380371, Training Accuracy: 7.0375\n",
            "Validation Loss: 4.178911400448745, Validation Accuracy: 6.83\n",
            "[24/150]: Training Loss: 4.144738430023193, Training Accuracy: 7.23\n",
            "Validation Loss: 4.172337419667821, Validation Accuracy: 6.52\n",
            "[25/150]: Training Loss: 4.1379581127166745, Training Accuracy: 7.265\n",
            "Validation Loss: 4.165943450988478, Validation Accuracy: 6.63\n",
            "[26/150]: Training Loss: 4.132068264770508, Training Accuracy: 7.4525\n",
            "Validation Loss: 4.160335552920202, Validation Accuracy: 7.0\n",
            "[27/150]: Training Loss: 4.126062253570557, Training Accuracy: 7.4725\n",
            "Validation Loss: 4.157630066962758, Validation Accuracy: 6.66\n",
            "[28/150]: Training Loss: 4.120529958724975, Training Accuracy: 7.52\n",
            "Validation Loss: 4.150649539983956, Validation Accuracy: 7.07\n",
            "[29/150]: Training Loss: 4.115550531768799, Training Accuracy: 7.6175\n",
            "Validation Loss: 4.145238416210102, Validation Accuracy: 7.01\n",
            "[30/150]: Training Loss: 4.109972083282471, Training Accuracy: 7.74\n",
            "Validation Loss: 4.140822541182208, Validation Accuracy: 6.87\n",
            "[31/150]: Training Loss: 4.1054605201721195, Training Accuracy: 7.7775\n",
            "Validation Loss: 4.135137576206475, Validation Accuracy: 7.26\n",
            "[32/150]: Training Loss: 4.100883611297608, Training Accuracy: 7.9175\n",
            "Validation Loss: 4.13245949623691, Validation Accuracy: 7.37\n",
            "[33/150]: Training Loss: 4.096106577682495, Training Accuracy: 7.965\n",
            "Validation Loss: 4.1272796597450405, Validation Accuracy: 7.21\n",
            "[34/150]: Training Loss: 4.092031303024292, Training Accuracy: 8.075\n",
            "Validation Loss: 4.1231977316983945, Validation Accuracy: 7.19\n",
            "[35/150]: Training Loss: 4.087661064910889, Training Accuracy: 8.0775\n",
            "Validation Loss: 4.119425037104612, Validation Accuracy: 7.42\n",
            "[36/150]: Training Loss: 4.083959820175171, Training Accuracy: 8.14\n",
            "Validation Loss: 4.117144297642313, Validation Accuracy: 7.43\n",
            "[37/150]: Training Loss: 4.079987169265747, Training Accuracy: 8.2575\n",
            "Validation Loss: 4.112940530108798, Validation Accuracy: 7.33\n",
            "[38/150]: Training Loss: 4.076256622695923, Training Accuracy: 8.275\n",
            "Validation Loss: 4.10887487071335, Validation Accuracy: 7.72\n",
            "[39/150]: Training Loss: 4.072722610473633, Training Accuracy: 8.2625\n",
            "Validation Loss: 4.1064621779569395, Validation Accuracy: 7.62\n",
            "[40/150]: Training Loss: 4.06919368019104, Training Accuracy: 8.4025\n",
            "Validation Loss: 4.102874081605559, Validation Accuracy: 7.77\n",
            "[41/150]: Training Loss: 4.065893580627441, Training Accuracy: 8.425\n",
            "Validation Loss: 4.101947505003328, Validation Accuracy: 7.99\n",
            "[42/150]: Training Loss: 4.062419548797608, Training Accuracy: 8.64\n",
            "Validation Loss: 4.096677272942416, Validation Accuracy: 7.69\n",
            "[43/150]: Training Loss: 4.059447618484497, Training Accuracy: 8.6025\n",
            "Validation Loss: 4.096899234565201, Validation Accuracy: 7.72\n",
            "[44/150]: Training Loss: 4.056013444137573, Training Accuracy: 8.5725\n",
            "Validation Loss: 4.091373089772121, Validation Accuracy: 7.88\n",
            "[45/150]: Training Loss: 4.053135622024536, Training Accuracy: 8.6275\n",
            "Validation Loss: 4.088051226488345, Validation Accuracy: 8.07\n",
            "[46/150]: Training Loss: 4.049987061309815, Training Accuracy: 8.63\n",
            "Validation Loss: 4.085329578180981, Validation Accuracy: 8.03\n",
            "[47/150]: Training Loss: 4.047172613143921, Training Accuracy: 8.7125\n",
            "Validation Loss: 4.082841596785625, Validation Accuracy: 7.78\n",
            "[48/150]: Training Loss: 4.0441587448120115, Training Accuracy: 8.745\n",
            "Validation Loss: 4.081506820241357, Validation Accuracy: 8.21\n",
            "[49/150]: Training Loss: 4.04135802230835, Training Accuracy: 8.82\n",
            "Validation Loss: 4.079341964357218, Validation Accuracy: 8.03\n",
            "[50/150]: Training Loss: 4.0387205871582035, Training Accuracy: 8.7775\n",
            "Validation Loss: 4.075337004509701, Validation Accuracy: 8.12\n",
            "[51/150]: Training Loss: 4.036031353759766, Training Accuracy: 8.9025\n",
            "Validation Loss: 4.074757421092623, Validation Accuracy: 8.17\n",
            "[52/150]: Training Loss: 4.033370276260376, Training Accuracy: 8.91\n",
            "Validation Loss: 4.071030196110914, Validation Accuracy: 8.29\n",
            "[53/150]: Training Loss: 4.031033205032348, Training Accuracy: 8.965\n",
            "Validation Loss: 4.069437462812776, Validation Accuracy: 8.36\n",
            "[54/150]: Training Loss: 4.028417000579834, Training Accuracy: 9.095\n",
            "Validation Loss: 4.067116659917649, Validation Accuracy: 8.11\n",
            "[55/150]: Training Loss: 4.0260539081573485, Training Accuracy: 9.045\n",
            "Validation Loss: 4.065322780305413, Validation Accuracy: 8.45\n",
            "[56/150]: Training Loss: 4.0235095344543454, Training Accuracy: 9.085\n",
            "Validation Loss: 4.064038521165301, Validation Accuracy: 8.38\n",
            "[57/150]: Training Loss: 4.02125683631897, Training Accuracy: 9.18\n",
            "Validation Loss: 4.0599597381178745, Validation Accuracy: 8.62\n",
            "[58/150]: Training Loss: 4.019049766159058, Training Accuracy: 9.2225\n",
            "Validation Loss: 4.058757024206174, Validation Accuracy: 8.53\n",
            "[59/150]: Training Loss: 4.016744113540649, Training Accuracy: 9.3325\n",
            "Validation Loss: 4.056453751910264, Validation Accuracy: 8.65\n",
            "[60/150]: Training Loss: 4.014383445358276, Training Accuracy: 9.33\n",
            "Validation Loss: 4.054738797959248, Validation Accuracy: 8.49\n",
            "[61/150]: Training Loss: 4.012673494720459, Training Accuracy: 9.4\n",
            "Validation Loss: 4.05312654015365, Validation Accuracy: 8.45\n",
            "[62/150]: Training Loss: 4.0104282833099365, Training Accuracy: 9.3375\n",
            "Validation Loss: 4.050798089640915, Validation Accuracy: 8.61\n",
            "[63/150]: Training Loss: 4.0085177280426025, Training Accuracy: 9.345\n",
            "Validation Loss: 4.047672698452215, Validation Accuracy: 9.08\n",
            "[64/150]: Training Loss: 4.007076532363891, Training Accuracy: 9.44\n",
            "Validation Loss: 4.047075613289122, Validation Accuracy: 8.55\n",
            "[65/150]: Training Loss: 4.004718017959595, Training Accuracy: 9.47\n",
            "Validation Loss: 4.044435259642874, Validation Accuracy: 8.66\n",
            "[66/150]: Training Loss: 4.002854734802246, Training Accuracy: 9.58\n",
            "Validation Loss: 4.043384468479521, Validation Accuracy: 8.68\n",
            "[67/150]: Training Loss: 4.000918030929565, Training Accuracy: 9.59\n",
            "Validation Loss: 4.042230905241268, Validation Accuracy: 8.87\n",
            "[68/150]: Training Loss: 3.9992192554473878, Training Accuracy: 9.5775\n",
            "Validation Loss: 4.040329585409468, Validation Accuracy: 8.83\n",
            "[69/150]: Training Loss: 3.9973626461029053, Training Accuracy: 9.615\n",
            "Validation Loss: 4.037929029221747, Validation Accuracy: 9.01\n",
            "[70/150]: Training Loss: 3.9957307056427003, Training Accuracy: 9.69\n",
            "Validation Loss: 4.037050957892351, Validation Accuracy: 8.67\n",
            "[71/150]: Training Loss: 3.9943285522460936, Training Accuracy: 9.7375\n",
            "Validation Loss: 4.0364253475407885, Validation Accuracy: 8.88\n",
            "[72/150]: Training Loss: 3.9925530368804933, Training Accuracy: 9.7025\n",
            "Validation Loss: 4.033815119676529, Validation Accuracy: 8.93\n",
            "[73/150]: Training Loss: 3.9908373458862303, Training Accuracy: 9.7625\n",
            "Validation Loss: 4.0321422671056855, Validation Accuracy: 8.97\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 4.837270994854581, Test Accuracy: 5.7\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>5.7</td></tr><tr><td>Test Loss</td><td>4.83727</td></tr><tr><td>Train Accuracy</td><td>9.7625</td></tr><tr><td>Train Loss</td><td>3.99084</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.01 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/nh2g7isx</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_022555-nh2g7isx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.05 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_023355-cthymqs8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8' target=\"_blank\">learning_rate=0.05 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.600790646362305, Training Accuracy: 1.1675\n",
            "Validation Loss: 4.590667490746565, Validation Accuracy: 1.26\n",
            "[2/150]: Training Loss: 4.559383809661865, Training Accuracy: 2.1575\n",
            "Validation Loss: 4.506907958133965, Validation Accuracy: 2.54\n",
            "[3/150]: Training Loss: 4.4036731437683105, Training Accuracy: 3.8525\n",
            "Validation Loss: 4.321598778864381, Validation Accuracy: 4.84\n",
            "[4/150]: Training Loss: 4.246265041351318, Training Accuracy: 5.2175\n",
            "Validation Loss: 4.227905522486207, Validation Accuracy: 6.02\n",
            "[5/150]: Training Loss: 4.176178216934204, Training Accuracy: 6.1675\n",
            "Validation Loss: 4.179626170237353, Validation Accuracy: 6.38\n",
            "[6/150]: Training Loss: 4.135982835769654, Training Accuracy: 6.6675\n",
            "Validation Loss: 4.148335417364813, Validation Accuracy: 6.64\n",
            "[7/150]: Training Loss: 4.108450821304321, Training Accuracy: 7.2275\n",
            "Validation Loss: 4.130975275282648, Validation Accuracy: 6.89\n",
            "[8/150]: Training Loss: 4.0844139430999755, Training Accuracy: 7.6\n",
            "Validation Loss: 4.101699337078507, Validation Accuracy: 7.5\n",
            "[9/150]: Training Loss: 4.065156806564331, Training Accuracy: 7.93\n",
            "Validation Loss: 4.086441223788413, Validation Accuracy: 7.13\n",
            "[10/150]: Training Loss: 4.045691847991943, Training Accuracy: 8.4825\n",
            "Validation Loss: 4.07577043733779, Validation Accuracy: 8.03\n",
            "[11/150]: Training Loss: 4.029001393127442, Training Accuracy: 8.6075\n",
            "Validation Loss: 4.055504823186595, Validation Accuracy: 8.33\n",
            "[12/150]: Training Loss: 4.011618738937378, Training Accuracy: 9.0575\n",
            "Validation Loss: 4.039901239856793, Validation Accuracy: 8.48\n",
            "[13/150]: Training Loss: 3.9964084663391115, Training Accuracy: 9.2925\n",
            "Validation Loss: 4.02936831097694, Validation Accuracy: 8.91\n",
            "[14/150]: Training Loss: 3.9814702980041505, Training Accuracy: 9.4925\n",
            "Validation Loss: 4.011358877655807, Validation Accuracy: 9.08\n",
            "[15/150]: Training Loss: 3.967844100570679, Training Accuracy: 9.735\n",
            "Validation Loss: 4.00843292133064, Validation Accuracy: 9.01\n",
            "[16/150]: Training Loss: 3.9526515048980713, Training Accuracy: 10.11\n",
            "Validation Loss: 3.983490101091421, Validation Accuracy: 9.73\n",
            "[17/150]: Training Loss: 3.9376510147094725, Training Accuracy: 10.2675\n",
            "Validation Loss: 3.9689508137429597, Validation Accuracy: 9.61\n",
            "[18/150]: Training Loss: 3.9245626121520996, Training Accuracy: 10.52\n",
            "Validation Loss: 3.9620496361118973, Validation Accuracy: 9.98\n",
            "[19/150]: Training Loss: 3.910297552108765, Training Accuracy: 10.8175\n",
            "Validation Loss: 3.952874835129756, Validation Accuracy: 10.09\n",
            "[20/150]: Training Loss: 3.8983414665222167, Training Accuracy: 11.1225\n",
            "Validation Loss: 3.930051308528633, Validation Accuracy: 10.44\n",
            "[21/150]: Training Loss: 3.8838251056671145, Training Accuracy: 11.2975\n",
            "Validation Loss: 3.9188870700301637, Validation Accuracy: 10.53\n",
            "[22/150]: Training Loss: 3.870637998199463, Training Accuracy: 11.44\n",
            "Validation Loss: 3.903455512538837, Validation Accuracy: 10.81\n",
            "[23/150]: Training Loss: 3.857129457092285, Training Accuracy: 11.84\n",
            "Validation Loss: 3.8918681099156665, Validation Accuracy: 11.32\n",
            "[24/150]: Training Loss: 3.8432881324768067, Training Accuracy: 12.0075\n",
            "Validation Loss: 3.8916967568124177, Validation Accuracy: 11.36\n",
            "[25/150]: Training Loss: 3.829104020690918, Training Accuracy: 12.395\n",
            "Validation Loss: 3.874841636912838, Validation Accuracy: 11.71\n",
            "[26/150]: Training Loss: 3.814932382965088, Training Accuracy: 12.6375\n",
            "Validation Loss: 3.862990617752075, Validation Accuracy: 11.7\n",
            "[27/150]: Training Loss: 3.801380111312866, Training Accuracy: 12.8\n",
            "Validation Loss: 3.850984105638638, Validation Accuracy: 12.17\n",
            "[28/150]: Training Loss: 3.7865677406311034, Training Accuracy: 13.18\n",
            "Validation Loss: 3.830880017796899, Validation Accuracy: 12.18\n",
            "[29/150]: Training Loss: 3.7712997707366944, Training Accuracy: 13.4425\n",
            "Validation Loss: 3.816112823547072, Validation Accuracy: 12.61\n",
            "[30/150]: Training Loss: 3.7579914638519285, Training Accuracy: 13.83\n",
            "Validation Loss: 3.817626942494872, Validation Accuracy: 12.9\n",
            "[31/150]: Training Loss: 3.7429426765441893, Training Accuracy: 14.14\n",
            "Validation Loss: 3.795013807381794, Validation Accuracy: 12.82\n",
            "[32/150]: Training Loss: 3.7270426654815676, Training Accuracy: 14.46\n",
            "Validation Loss: 3.774455428882769, Validation Accuracy: 13.53\n",
            "[33/150]: Training Loss: 3.7124608081817625, Training Accuracy: 14.53\n",
            "Validation Loss: 3.7734299799439253, Validation Accuracy: 13.65\n",
            "[34/150]: Training Loss: 3.695247130584717, Training Accuracy: 15.0575\n",
            "Validation Loss: 3.749705171888801, Validation Accuracy: 14.01\n",
            "[35/150]: Training Loss: 3.679954329299927, Training Accuracy: 15.12\n",
            "Validation Loss: 3.7417220279669308, Validation Accuracy: 13.98\n",
            "[36/150]: Training Loss: 3.6635797924041746, Training Accuracy: 15.48\n",
            "Validation Loss: 3.7247597214522634, Validation Accuracy: 14.42\n",
            "[37/150]: Training Loss: 3.6480526805877687, Training Accuracy: 15.7375\n",
            "Validation Loss: 3.7059438805671254, Validation Accuracy: 14.5\n",
            "[38/150]: Training Loss: 3.6313486305236817, Training Accuracy: 16.3075\n",
            "Validation Loss: 3.6903846977622647, Validation Accuracy: 14.92\n",
            "[39/150]: Training Loss: 3.6153011852264405, Training Accuracy: 16.3475\n",
            "Validation Loss: 3.6778608826315327, Validation Accuracy: 14.79\n",
            "[40/150]: Training Loss: 3.60027672958374, Training Accuracy: 16.67\n",
            "Validation Loss: 3.6614270756958396, Validation Accuracy: 15.31\n",
            "[41/150]: Training Loss: 3.585800159072876, Training Accuracy: 16.7725\n",
            "Validation Loss: 3.655038889805982, Validation Accuracy: 15.05\n",
            "[42/150]: Training Loss: 3.5694461936950685, Training Accuracy: 17.0375\n",
            "Validation Loss: 3.636945672855256, Validation Accuracy: 15.93\n",
            "[43/150]: Training Loss: 3.5568551288604735, Training Accuracy: 17.3975\n",
            "Validation Loss: 3.627324414101376, Validation Accuracy: 16.21\n",
            "[44/150]: Training Loss: 3.5419315227508545, Training Accuracy: 17.7\n",
            "Validation Loss: 3.6199980754001886, Validation Accuracy: 16.01\n",
            "[45/150]: Training Loss: 3.529905016708374, Training Accuracy: 17.7675\n",
            "Validation Loss: 3.6078376861134913, Validation Accuracy: 16.36\n",
            "[46/150]: Training Loss: 3.5179548221588135, Training Accuracy: 18.195\n",
            "Validation Loss: 3.591066009679418, Validation Accuracy: 16.38\n",
            "[47/150]: Training Loss: 3.5055387844085693, Training Accuracy: 18.2875\n",
            "Validation Loss: 3.5880215836178726, Validation Accuracy: 16.91\n",
            "[48/150]: Training Loss: 3.4944853992462157, Training Accuracy: 18.445\n",
            "Validation Loss: 3.573359035382605, Validation Accuracy: 17.28\n",
            "[49/150]: Training Loss: 3.4836856788635253, Training Accuracy: 18.615\n",
            "Validation Loss: 3.5583009431316595, Validation Accuracy: 17.54\n",
            "[50/150]: Training Loss: 3.4725227031707764, Training Accuracy: 18.9225\n",
            "Validation Loss: 3.5552009609854145, Validation Accuracy: 17.27\n",
            "[51/150]: Training Loss: 3.461646089553833, Training Accuracy: 19.095\n",
            "Validation Loss: 3.5449378551191586, Validation Accuracy: 17.49\n",
            "[52/150]: Training Loss: 3.451679636383057, Training Accuracy: 19.2575\n",
            "Validation Loss: 3.5295108943987805, Validation Accuracy: 17.59\n",
            "[53/150]: Training Loss: 3.44256662979126, Training Accuracy: 19.475\n",
            "Validation Loss: 3.5264732868048796, Validation Accuracy: 17.73\n",
            "[54/150]: Training Loss: 3.4335047622680666, Training Accuracy: 19.5175\n",
            "Validation Loss: 3.521607499213735, Validation Accuracy: 17.96\n",
            "[55/150]: Training Loss: 3.425080271530151, Training Accuracy: 19.8125\n",
            "Validation Loss: 3.513597962203299, Validation Accuracy: 18.01\n",
            "[56/150]: Training Loss: 3.4155618144989015, Training Accuracy: 19.8425\n",
            "Validation Loss: 3.5052308413633115, Validation Accuracy: 18.2\n",
            "[57/150]: Training Loss: 3.4078545150756834, Training Accuracy: 20.0575\n",
            "Validation Loss: 3.498734108202017, Validation Accuracy: 18.36\n",
            "[58/150]: Training Loss: 3.399192015457153, Training Accuracy: 20.1425\n",
            "Validation Loss: 3.4935538814326, Validation Accuracy: 18.5\n",
            "[59/150]: Training Loss: 3.3918968856811524, Training Accuracy: 20.275\n",
            "Validation Loss: 3.4842984858591843, Validation Accuracy: 18.83\n",
            "[60/150]: Training Loss: 3.3846318199157714, Training Accuracy: 20.45\n",
            "Validation Loss: 3.475666542721402, Validation Accuracy: 18.75\n",
            "[61/150]: Training Loss: 3.378842526626587, Training Accuracy: 20.395\n",
            "Validation Loss: 3.47740289360095, Validation Accuracy: 18.53\n",
            "[62/150]: Training Loss: 3.369043716430664, Training Accuracy: 20.85\n",
            "Validation Loss: 3.469373791081131, Validation Accuracy: 18.88\n",
            "[63/150]: Training Loss: 3.363127135467529, Training Accuracy: 20.8125\n",
            "Validation Loss: 3.453160047531128, Validation Accuracy: 19.11\n",
            "[64/150]: Training Loss: 3.355681411361694, Training Accuracy: 21.0925\n",
            "Validation Loss: 3.466009464992839, Validation Accuracy: 19.12\n",
            "[65/150]: Training Loss: 3.3498121349334715, Training Accuracy: 21.1625\n",
            "Validation Loss: 3.4505164547331013, Validation Accuracy: 19.11\n",
            "[66/150]: Training Loss: 3.343057912063599, Training Accuracy: 21.225\n",
            "Validation Loss: 3.4455762957311737, Validation Accuracy: 19.05\n",
            "[67/150]: Training Loss: 3.3380538593292237, Training Accuracy: 21.2275\n",
            "Validation Loss: 3.448872314137258, Validation Accuracy: 18.99\n",
            "[68/150]: Training Loss: 3.3309446399688722, Training Accuracy: 21.4425\n",
            "Validation Loss: 3.434380317189891, Validation Accuracy: 19.65\n",
            "[69/150]: Training Loss: 3.325505153656006, Training Accuracy: 21.5125\n",
            "Validation Loss: 3.428423096419899, Validation Accuracy: 19.77\n",
            "[70/150]: Training Loss: 3.319794240951538, Training Accuracy: 21.7375\n",
            "Validation Loss: 3.4293241318623733, Validation Accuracy: 19.58\n",
            "[71/150]: Training Loss: 3.3128502590179445, Training Accuracy: 21.7775\n",
            "Validation Loss: 3.422350405128139, Validation Accuracy: 19.89\n",
            "[72/150]: Training Loss: 3.308202914047241, Training Accuracy: 21.9175\n",
            "Validation Loss: 3.4211007197191763, Validation Accuracy: 20.03\n",
            "[73/150]: Training Loss: 3.3036910511016844, Training Accuracy: 21.85\n",
            "Validation Loss: 3.412458062931231, Validation Accuracy: 19.99\n",
            "[74/150]: Training Loss: 3.2999632190704347, Training Accuracy: 22.05\n",
            "Validation Loss: 3.410503987294094, Validation Accuracy: 19.96\n",
            "[75/150]: Training Loss: 3.2934399642944334, Training Accuracy: 21.9825\n",
            "Validation Loss: 3.4070342604521735, Validation Accuracy: 20.1\n",
            "[76/150]: Training Loss: 3.288022666168213, Training Accuracy: 22.4075\n",
            "Validation Loss: 3.3981059220186465, Validation Accuracy: 20.33\n",
            "[77/150]: Training Loss: 3.283489876937866, Training Accuracy: 22.37\n",
            "Validation Loss: 3.403488172846995, Validation Accuracy: 20.14\n",
            "[78/150]: Training Loss: 3.279419002151489, Training Accuracy: 22.4725\n",
            "Validation Loss: 3.39810815434547, Validation Accuracy: 20.23\n",
            "[79/150]: Training Loss: 3.275748169708252, Training Accuracy: 22.3675\n",
            "Validation Loss: 3.3924497191313727, Validation Accuracy: 20.38\n",
            "[80/150]: Training Loss: 3.2700943855285645, Training Accuracy: 22.625\n",
            "Validation Loss: 3.394149239655513, Validation Accuracy: 20.3\n",
            "[81/150]: Training Loss: 3.266976011657715, Training Accuracy: 22.6325\n",
            "Validation Loss: 3.389360663237845, Validation Accuracy: 20.25\n",
            "[82/150]: Training Loss: 3.2631465213775637, Training Accuracy: 22.7725\n",
            "Validation Loss: 3.3781265529098023, Validation Accuracy: 20.61\n",
            "[83/150]: Training Loss: 3.2582495727539063, Training Accuracy: 22.885\n",
            "Validation Loss: 3.380337715148926, Validation Accuracy: 20.88\n",
            "[84/150]: Training Loss: 3.2537473976135254, Training Accuracy: 23.075\n",
            "Validation Loss: 3.3771434042863784, Validation Accuracy: 20.89\n",
            "[85/150]: Training Loss: 3.2507594604492187, Training Accuracy: 22.9875\n",
            "Validation Loss: 3.3780695936482426, Validation Accuracy: 20.92\n",
            "[86/150]: Training Loss: 3.246923331832886, Training Accuracy: 23.055\n",
            "Validation Loss: 3.3708357325025426, Validation Accuracy: 20.88\n",
            "[87/150]: Training Loss: 3.2429458431243896, Training Accuracy: 23.1625\n",
            "Validation Loss: 3.36568513493629, Validation Accuracy: 21.0\n",
            "[88/150]: Training Loss: 3.239496036148071, Training Accuracy: 23.09\n",
            "Validation Loss: 3.369001481183775, Validation Accuracy: 20.84\n",
            "[89/150]: Training Loss: 3.235803797531128, Training Accuracy: 23.2875\n",
            "Validation Loss: 3.362776779065466, Validation Accuracy: 21.19\n",
            "[90/150]: Training Loss: 3.2327476997375486, Training Accuracy: 23.39\n",
            "Validation Loss: 3.3616829556264696, Validation Accuracy: 21.16\n",
            "[91/150]: Training Loss: 3.229936269760132, Training Accuracy: 23.3875\n",
            "Validation Loss: 3.360888686149743, Validation Accuracy: 21.41\n",
            "[92/150]: Training Loss: 3.2263083686828615, Training Accuracy: 23.4225\n",
            "Validation Loss: 3.359251875786265, Validation Accuracy: 21.08\n",
            "[93/150]: Training Loss: 3.224231428909302, Training Accuracy: 23.5575\n",
            "Validation Loss: 3.3559617753241473, Validation Accuracy: 21.38\n",
            "[94/150]: Training Loss: 3.2209563293457033, Training Accuracy: 23.52\n",
            "Validation Loss: 3.3566071850479027, Validation Accuracy: 21.38\n",
            "[95/150]: Training Loss: 3.2181186485290527, Training Accuracy: 23.5925\n",
            "Validation Loss: 3.3525171613996956, Validation Accuracy: 21.11\n",
            "[96/150]: Training Loss: 3.2156093044281007, Training Accuracy: 23.6025\n",
            "Validation Loss: 3.3487717983828986, Validation Accuracy: 21.48\n",
            "[97/150]: Training Loss: 3.211993044281006, Training Accuracy: 23.67\n",
            "Validation Loss: 3.3509595333390934, Validation Accuracy: 21.34\n",
            "[98/150]: Training Loss: 3.2086189796447755, Training Accuracy: 23.7725\n",
            "Validation Loss: 3.35127718585312, Validation Accuracy: 21.49\n",
            "[99/150]: Training Loss: 3.2077421699523927, Training Accuracy: 23.7925\n",
            "Validation Loss: 3.345605060553095, Validation Accuracy: 21.5\n",
            "[100/150]: Training Loss: 3.2047137928009035, Training Accuracy: 23.7025\n",
            "Validation Loss: 3.3447778088271996, Validation Accuracy: 21.56\n",
            "[101/150]: Training Loss: 3.202946053314209, Training Accuracy: 23.8775\n",
            "Validation Loss: 3.342098644584607, Validation Accuracy: 21.49\n",
            "[102/150]: Training Loss: 3.2004322288513185, Training Accuracy: 23.8375\n",
            "Validation Loss: 3.343631648713616, Validation Accuracy: 21.61\n",
            "[103/150]: Training Loss: 3.1983728965759277, Training Accuracy: 23.9425\n",
            "Validation Loss: 3.3422155471364405, Validation Accuracy: 21.24\n",
            "[104/150]: Training Loss: 3.196205286026001, Training Accuracy: 23.9975\n",
            "Validation Loss: 3.3361381269564294, Validation Accuracy: 21.8\n",
            "[105/150]: Training Loss: 3.194296379852295, Training Accuracy: 23.9125\n",
            "Validation Loss: 3.3376316750884816, Validation Accuracy: 21.59\n",
            "[106/150]: Training Loss: 3.1917529289245605, Training Accuracy: 23.9425\n",
            "Validation Loss: 3.3328404289901634, Validation Accuracy: 21.83\n",
            "[107/150]: Training Loss: 3.1901755290985108, Training Accuracy: 24.06\n",
            "Validation Loss: 3.335774242498313, Validation Accuracy: 21.85\n",
            "[108/150]: Training Loss: 3.188256001663208, Training Accuracy: 24.01\n",
            "Validation Loss: 3.335664199416045, Validation Accuracy: 21.84\n",
            "[109/150]: Training Loss: 3.186541044998169, Training Accuracy: 24.1\n",
            "Validation Loss: 3.331627411447513, Validation Accuracy: 21.98\n",
            "[110/150]: Training Loss: 3.184773151397705, Training Accuracy: 24.0825\n",
            "Validation Loss: 3.3308697946512016, Validation Accuracy: 21.79\n",
            "[111/150]: Training Loss: 3.1835228912353517, Training Accuracy: 24.1275\n",
            "Validation Loss: 3.3312593797209917, Validation Accuracy: 21.68\n",
            "[112/150]: Training Loss: 3.181557007980347, Training Accuracy: 24.2275\n",
            "Validation Loss: 3.3289258191539983, Validation Accuracy: 21.87\n",
            "[113/150]: Training Loss: 3.180008267211914, Training Accuracy: 24.3675\n",
            "Validation Loss: 3.3277820234845397, Validation Accuracy: 21.73\n",
            "[114/150]: Training Loss: 3.1787550163269045, Training Accuracy: 24.2975\n",
            "Validation Loss: 3.326631183077575, Validation Accuracy: 21.95\n",
            "[115/150]: Training Loss: 3.177647034072876, Training Accuracy: 24.285\n",
            "Validation Loss: 3.3255794716488785, Validation Accuracy: 22.03\n",
            "[116/150]: Training Loss: 3.176307448196411, Training Accuracy: 24.385\n",
            "Validation Loss: 3.3254039242009448, Validation Accuracy: 21.9\n",
            "[117/150]: Training Loss: 3.175141114425659, Training Accuracy: 24.3075\n",
            "Validation Loss: 3.3223649164673628, Validation Accuracy: 22.03\n",
            "[118/150]: Training Loss: 3.173803305053711, Training Accuracy: 24.4125\n",
            "Validation Loss: 3.323437298938727, Validation Accuracy: 21.99\n",
            "[119/150]: Training Loss: 3.1727485507965087, Training Accuracy: 24.345\n",
            "Validation Loss: 3.3220133781433105, Validation Accuracy: 22.09\n",
            "[120/150]: Training Loss: 3.171308903121948, Training Accuracy: 24.475\n",
            "Validation Loss: 3.320924275999616, Validation Accuracy: 22.15\n",
            "[121/150]: Training Loss: 3.1707864654541016, Training Accuracy: 24.485\n",
            "Validation Loss: 3.3217681638754097, Validation Accuracy: 21.98\n",
            "[122/150]: Training Loss: 3.169487755203247, Training Accuracy: 24.5\n",
            "Validation Loss: 3.3205978354071357, Validation Accuracy: 22.2\n",
            "[123/150]: Training Loss: 3.1687754665374754, Training Accuracy: 24.4775\n",
            "Validation Loss: 3.3199286217902118, Validation Accuracy: 22.03\n",
            "[124/150]: Training Loss: 3.1677217502593993, Training Accuracy: 24.5225\n",
            "Validation Loss: 3.3194276329818044, Validation Accuracy: 22.2\n",
            "[125/150]: Training Loss: 3.1672953506469725, Training Accuracy: 24.53\n",
            "Validation Loss: 3.3200536381666828, Validation Accuracy: 22.05\n",
            "[126/150]: Training Loss: 3.166370540237427, Training Accuracy: 24.53\n",
            "Validation Loss: 3.3180840486174175, Validation Accuracy: 22.01\n",
            "[127/150]: Training Loss: 3.165336986541748, Training Accuracy: 24.565\n",
            "Validation Loss: 3.318746434655159, Validation Accuracy: 22.24\n",
            "[128/150]: Training Loss: 3.164968849182129, Training Accuracy: 24.535\n",
            "Validation Loss: 3.3186944214401732, Validation Accuracy: 22.05\n",
            "[129/150]: Training Loss: 3.1642740074157714, Training Accuracy: 24.5175\n",
            "Validation Loss: 3.3177084133123897, Validation Accuracy: 22.21\n",
            "[130/150]: Training Loss: 3.16352066116333, Training Accuracy: 24.655\n",
            "Validation Loss: 3.318562794642843, Validation Accuracy: 22.01\n",
            "[131/150]: Training Loss: 3.1630500473022463, Training Accuracy: 24.5975\n",
            "Validation Loss: 3.317300656798539, Validation Accuracy: 22.06\n",
            "[132/150]: Training Loss: 3.1626014945983885, Training Accuracy: 24.66\n",
            "Validation Loss: 3.317058933768303, Validation Accuracy: 22.16\n",
            "[133/150]: Training Loss: 3.161920825958252, Training Accuracy: 24.64\n",
            "Validation Loss: 3.3171579837799072, Validation Accuracy: 22.02\n",
            "[134/150]: Training Loss: 3.161545040512085, Training Accuracy: 24.6375\n",
            "Validation Loss: 3.316502076045723, Validation Accuracy: 22.18\n",
            "[135/150]: Training Loss: 3.1611102500915527, Training Accuracy: 24.585\n",
            "Validation Loss: 3.3166633107859615, Validation Accuracy: 22.06\n",
            "[136/150]: Training Loss: 3.160765283203125, Training Accuracy: 24.6325\n",
            "Validation Loss: 3.3164391669498126, Validation Accuracy: 22.14\n",
            "[137/150]: Training Loss: 3.160325936508179, Training Accuracy: 24.6675\n",
            "Validation Loss: 3.315987286294342, Validation Accuracy: 22.11\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 5.462109195198982, Test Accuracy: 10.88\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>10.88</td></tr><tr><td>Test Loss</td><td>5.46211</td></tr><tr><td>Train Accuracy</td><td>24.6675</td></tr><tr><td>Train Loss</td><td>3.16033</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.05 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/cthymqs8</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_023355-cthymqs8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.1 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_024913-26wygp07</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07' target=\"_blank\">learning_rate=0.1 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.554393922424317, Training Accuracy: 2.2\n",
            "Validation Loss: 4.421840394378468, Validation Accuracy: 3.74\n",
            "[2/150]: Training Loss: 4.284681567382813, Training Accuracy: 4.52\n",
            "Validation Loss: 4.239770348664302, Validation Accuracy: 4.95\n",
            "[3/150]: Training Loss: 4.181965134429932, Training Accuracy: 5.585\n",
            "Validation Loss: 4.184003517126581, Validation Accuracy: 5.53\n",
            "[4/150]: Training Loss: 4.125413941192627, Training Accuracy: 6.6775\n",
            "Validation Loss: 4.123335443484556, Validation Accuracy: 6.52\n",
            "[5/150]: Training Loss: 4.084817845535278, Training Accuracy: 7.4525\n",
            "Validation Loss: 4.096477279237881, Validation Accuracy: 6.87\n",
            "[6/150]: Training Loss: 4.048351853179931, Training Accuracy: 8.1125\n",
            "Validation Loss: 4.069233142646255, Validation Accuracy: 7.8\n",
            "[7/150]: Training Loss: 4.018422792816162, Training Accuracy: 8.56\n",
            "Validation Loss: 4.0364193445558, Validation Accuracy: 8.44\n",
            "[8/150]: Training Loss: 3.989247378540039, Training Accuracy: 8.9625\n",
            "Validation Loss: 4.010960247865908, Validation Accuracy: 8.51\n",
            "[9/150]: Training Loss: 3.9593318000793456, Training Accuracy: 9.5725\n",
            "Validation Loss: 3.9864684988738626, Validation Accuracy: 8.73\n",
            "[10/150]: Training Loss: 3.935969552612305, Training Accuracy: 9.97\n",
            "Validation Loss: 3.959391053315181, Validation Accuracy: 9.43\n",
            "[11/150]: Training Loss: 3.910384812927246, Training Accuracy: 10.53\n",
            "Validation Loss: 3.9335520723063473, Validation Accuracy: 10.05\n",
            "[12/150]: Training Loss: 3.8837633472442628, Training Accuracy: 11.015\n",
            "Validation Loss: 3.9218593843423637, Validation Accuracy: 9.91\n",
            "[13/150]: Training Loss: 3.85968777885437, Training Accuracy: 11.265\n",
            "Validation Loss: 3.884523991566555, Validation Accuracy: 10.94\n",
            "[14/150]: Training Loss: 3.83401321182251, Training Accuracy: 12.19\n",
            "Validation Loss: 3.8702170302154153, Validation Accuracy: 11.29\n",
            "[15/150]: Training Loss: 3.8098957332611083, Training Accuracy: 12.6325\n",
            "Validation Loss: 3.840197701363047, Validation Accuracy: 11.62\n",
            "[16/150]: Training Loss: 3.7832756935119627, Training Accuracy: 13.2\n",
            "Validation Loss: 3.8233511766810326, Validation Accuracy: 12.45\n",
            "[17/150]: Training Loss: 3.755026846694946, Training Accuracy: 13.5\n",
            "Validation Loss: 3.7900661996975065, Validation Accuracy: 12.91\n",
            "[18/150]: Training Loss: 3.7291974296569825, Training Accuracy: 14.06\n",
            "Validation Loss: 3.7731176181963293, Validation Accuracy: 13.22\n",
            "[19/150]: Training Loss: 3.703822989273071, Training Accuracy: 14.6625\n",
            "Validation Loss: 3.742501725057128, Validation Accuracy: 13.6\n",
            "[20/150]: Training Loss: 3.676954047012329, Training Accuracy: 15.14\n",
            "Validation Loss: 3.718170521365609, Validation Accuracy: 14.24\n",
            "[21/150]: Training Loss: 3.6502103706359863, Training Accuracy: 15.59\n",
            "Validation Loss: 3.6871351967951296, Validation Accuracy: 14.81\n",
            "[22/150]: Training Loss: 3.622364068222046, Training Accuracy: 16.105\n",
            "Validation Loss: 3.67376760464565, Validation Accuracy: 14.79\n",
            "[23/150]: Training Loss: 3.5969187736511232, Training Accuracy: 16.585\n",
            "Validation Loss: 3.6408486214413007, Validation Accuracy: 15.59\n",
            "[24/150]: Training Loss: 3.568799534988403, Training Accuracy: 17.215\n",
            "Validation Loss: 3.6294803862359113, Validation Accuracy: 15.79\n",
            "[25/150]: Training Loss: 3.5421108798980714, Training Accuracy: 17.57\n",
            "Validation Loss: 3.598143538092352, Validation Accuracy: 16.37\n",
            "[26/150]: Training Loss: 3.5124191093444823, Training Accuracy: 18.205\n",
            "Validation Loss: 3.5703049495721317, Validation Accuracy: 16.98\n",
            "[27/150]: Training Loss: 3.484529112625122, Training Accuracy: 18.7275\n",
            "Validation Loss: 3.5376776236637384, Validation Accuracy: 17.48\n",
            "[28/150]: Training Loss: 3.455819899749756, Training Accuracy: 19.0425\n",
            "Validation Loss: 3.5291991643844898, Validation Accuracy: 17.68\n",
            "[29/150]: Training Loss: 3.4284223934173585, Training Accuracy: 19.5625\n",
            "Validation Loss: 3.4991380257211673, Validation Accuracy: 18.43\n",
            "[30/150]: Training Loss: 3.4028552402496337, Training Accuracy: 20.0525\n",
            "Validation Loss: 3.4806957472661497, Validation Accuracy: 18.35\n",
            "[31/150]: Training Loss: 3.376880758666992, Training Accuracy: 20.5925\n",
            "Validation Loss: 3.450026891793415, Validation Accuracy: 19.53\n",
            "[32/150]: Training Loss: 3.354429434585571, Training Accuracy: 21.165\n",
            "Validation Loss: 3.435450357995975, Validation Accuracy: 19.72\n",
            "[33/150]: Training Loss: 3.3340842437744143, Training Accuracy: 21.175\n",
            "Validation Loss: 3.418211444927629, Validation Accuracy: 19.67\n",
            "[34/150]: Training Loss: 3.314141171646118, Training Accuracy: 21.5925\n",
            "Validation Loss: 3.4085868847597935, Validation Accuracy: 20.29\n",
            "[35/150]: Training Loss: 3.2981444416046144, Training Accuracy: 21.85\n",
            "Validation Loss: 3.3930132495369882, Validation Accuracy: 20.07\n",
            "[36/150]: Training Loss: 3.280233634567261, Training Accuracy: 22.4375\n",
            "Validation Loss: 3.3805624132703063, Validation Accuracy: 20.71\n",
            "[37/150]: Training Loss: 3.2627798179626466, Training Accuracy: 22.555\n",
            "Validation Loss: 3.365730774630407, Validation Accuracy: 21.1\n",
            "[38/150]: Training Loss: 3.246723106765747, Training Accuracy: 22.8925\n",
            "Validation Loss: 3.3649645671722994, Validation Accuracy: 21.17\n",
            "[39/150]: Training Loss: 3.23333472366333, Training Accuracy: 23.1725\n",
            "Validation Loss: 3.340107489543356, Validation Accuracy: 21.48\n",
            "[40/150]: Training Loss: 3.2185105766296385, Training Accuracy: 23.425\n",
            "Validation Loss: 3.3351985269291387, Validation Accuracy: 21.9\n",
            "[41/150]: Training Loss: 3.2041201099395753, Training Accuracy: 23.65\n",
            "Validation Loss: 3.320353848159693, Validation Accuracy: 22.37\n",
            "[42/150]: Training Loss: 3.1920853351593017, Training Accuracy: 23.97\n",
            "Validation Loss: 3.3151075627393785, Validation Accuracy: 21.98\n",
            "[43/150]: Training Loss: 3.178889651107788, Training Accuracy: 24.015\n",
            "Validation Loss: 3.3039617690311114, Validation Accuracy: 22.11\n",
            "[44/150]: Training Loss: 3.164986518096924, Training Accuracy: 24.4275\n",
            "Validation Loss: 3.2982496850809473, Validation Accuracy: 22.16\n",
            "[45/150]: Training Loss: 3.1540319355010986, Training Accuracy: 24.705\n",
            "Validation Loss: 3.2881052099215755, Validation Accuracy: 22.49\n",
            "[46/150]: Training Loss: 3.1423869262695314, Training Accuracy: 24.745\n",
            "Validation Loss: 3.285053641932785, Validation Accuracy: 22.32\n",
            "[47/150]: Training Loss: 3.1313994647979735, Training Accuracy: 25.0425\n",
            "Validation Loss: 3.2773830009873506, Validation Accuracy: 22.64\n",
            "[48/150]: Training Loss: 3.1199681632995606, Training Accuracy: 25.2175\n",
            "Validation Loss: 3.2647413463349553, Validation Accuracy: 22.66\n",
            "[49/150]: Training Loss: 3.107323546600342, Training Accuracy: 25.5575\n",
            "Validation Loss: 3.277184949559011, Validation Accuracy: 22.72\n",
            "[50/150]: Training Loss: 3.0971016899108887, Training Accuracy: 25.77\n",
            "Validation Loss: 3.2499756160055755, Validation Accuracy: 22.69\n",
            "[51/150]: Training Loss: 3.0865638957977293, Training Accuracy: 25.915\n",
            "Validation Loss: 3.28796663102071, Validation Accuracy: 22.45\n",
            "[52/150]: Training Loss: 3.075482699203491, Training Accuracy: 26.0775\n",
            "Validation Loss: 3.256385016593204, Validation Accuracy: 22.97\n",
            "[53/150]: Training Loss: 3.0676188301086427, Training Accuracy: 26.045\n",
            "Validation Loss: 3.2297865843317313, Validation Accuracy: 23.53\n",
            "[54/150]: Training Loss: 3.0562880493164064, Training Accuracy: 26.5925\n",
            "Validation Loss: 3.2346752266974965, Validation Accuracy: 23.32\n",
            "[55/150]: Training Loss: 3.046115529251099, Training Accuracy: 26.525\n",
            "Validation Loss: 3.229158471344383, Validation Accuracy: 23.27\n",
            "[56/150]: Training Loss: 3.0363171295166014, Training Accuracy: 26.5875\n",
            "Validation Loss: 3.2251109287237667, Validation Accuracy: 23.85\n",
            "[57/150]: Training Loss: 3.0262594100952147, Training Accuracy: 26.96\n",
            "Validation Loss: 3.20999311793382, Validation Accuracy: 23.73\n",
            "[58/150]: Training Loss: 3.018730586242676, Training Accuracy: 26.925\n",
            "Validation Loss: 3.206504849111958, Validation Accuracy: 23.66\n",
            "[59/150]: Training Loss: 3.007856759262085, Training Accuracy: 27.315\n",
            "Validation Loss: 3.2090084021258507, Validation Accuracy: 24.02\n",
            "[60/150]: Training Loss: 3.0000602096557616, Training Accuracy: 27.39\n",
            "Validation Loss: 3.2029919001706846, Validation Accuracy: 23.96\n",
            "[61/150]: Training Loss: 2.9893322017669677, Training Accuracy: 27.6525\n",
            "Validation Loss: 3.207233451733923, Validation Accuracy: 23.61\n",
            "[62/150]: Training Loss: 2.9812552192687987, Training Accuracy: 27.855\n",
            "Validation Loss: 3.191280894978031, Validation Accuracy: 23.71\n",
            "[63/150]: Training Loss: 2.972245023727417, Training Accuracy: 27.83\n",
            "Validation Loss: 3.191427630224046, Validation Accuracy: 24.21\n",
            "[64/150]: Training Loss: 2.9653648654937745, Training Accuracy: 28.0175\n",
            "Validation Loss: 3.189020594214178, Validation Accuracy: 24.07\n",
            "[65/150]: Training Loss: 2.957701969909668, Training Accuracy: 28.245\n",
            "Validation Loss: 3.183226536793314, Validation Accuracy: 23.98\n",
            "[66/150]: Training Loss: 2.9480487686157226, Training Accuracy: 28.3725\n",
            "Validation Loss: 3.1869888670125586, Validation Accuracy: 24.08\n",
            "[67/150]: Training Loss: 2.9404784973144533, Training Accuracy: 28.5625\n",
            "Validation Loss: 3.1826100653144205, Validation Accuracy: 24.61\n",
            "[68/150]: Training Loss: 2.9308408599853517, Training Accuracy: 28.73\n",
            "Validation Loss: 3.1697433404861743, Validation Accuracy: 24.11\n",
            "[69/150]: Training Loss: 2.924504146194458, Training Accuracy: 28.735\n",
            "Validation Loss: 3.164285852650928, Validation Accuracy: 24.76\n",
            "[70/150]: Training Loss: 2.9181696743011476, Training Accuracy: 29.045\n",
            "Validation Loss: 3.1617295559804153, Validation Accuracy: 24.5\n",
            "[71/150]: Training Loss: 2.909669787979126, Training Accuracy: 28.97\n",
            "Validation Loss: 3.16842480222131, Validation Accuracy: 24.55\n",
            "[72/150]: Training Loss: 2.901284480667114, Training Accuracy: 29.165\n",
            "Validation Loss: 3.16541398710506, Validation Accuracy: 24.39\n",
            "[73/150]: Training Loss: 2.8958903297424317, Training Accuracy: 29.41\n",
            "Validation Loss: 3.156350550378204, Validation Accuracy: 24.81\n",
            "[74/150]: Training Loss: 2.8880927780151366, Training Accuracy: 29.51\n",
            "Validation Loss: 3.1484538536922186, Validation Accuracy: 24.79\n",
            "[75/150]: Training Loss: 2.8816709129333495, Training Accuracy: 29.6575\n",
            "Validation Loss: 3.1521760521421007, Validation Accuracy: 24.75\n",
            "[76/150]: Training Loss: 2.874811888885498, Training Accuracy: 29.6025\n",
            "Validation Loss: 3.1412497159022434, Validation Accuracy: 24.86\n",
            "[77/150]: Training Loss: 2.868447174453735, Training Accuracy: 29.75\n",
            "Validation Loss: 3.148860450003557, Validation Accuracy: 25.12\n",
            "[78/150]: Training Loss: 2.8620141311645506, Training Accuracy: 30.02\n",
            "Validation Loss: 3.1491585096735863, Validation Accuracy: 25.07\n",
            "[79/150]: Training Loss: 2.8542796688079832, Training Accuracy: 30.2175\n",
            "Validation Loss: 3.1315311428847585, Validation Accuracy: 25.26\n",
            "[80/150]: Training Loss: 2.8495312446594236, Training Accuracy: 30.2625\n",
            "Validation Loss: 3.1400332511610287, Validation Accuracy: 25.12\n",
            "[81/150]: Training Loss: 2.8422299156188964, Training Accuracy: 30.4475\n",
            "Validation Loss: 3.129277274866772, Validation Accuracy: 25.51\n",
            "[82/150]: Training Loss: 2.8352813999176028, Training Accuracy: 30.645\n",
            "Validation Loss: 3.1319479532302563, Validation Accuracy: 25.37\n",
            "[83/150]: Training Loss: 2.830805637359619, Training Accuracy: 30.595\n",
            "Validation Loss: 3.1387126217981813, Validation Accuracy: 25.34\n",
            "[84/150]: Training Loss: 2.824944213485718, Training Accuracy: 30.7875\n",
            "Validation Loss: 3.1277991054923673, Validation Accuracy: 25.58\n",
            "[85/150]: Training Loss: 2.8196789070129396, Training Accuracy: 31.02\n",
            "Validation Loss: 3.1215793433462737, Validation Accuracy: 25.59\n",
            "[86/150]: Training Loss: 2.814448028564453, Training Accuracy: 31.1025\n",
            "Validation Loss: 3.1227446027622103, Validation Accuracy: 25.41\n",
            "[87/150]: Training Loss: 2.8087932884216307, Training Accuracy: 31.3075\n",
            "Validation Loss: 3.123080830665151, Validation Accuracy: 25.3\n",
            "[88/150]: Training Loss: 2.8027573429107666, Training Accuracy: 31.35\n",
            "Validation Loss: 3.1130096623851995, Validation Accuracy: 25.94\n",
            "[89/150]: Training Loss: 2.7979101371765136, Training Accuracy: 31.195\n",
            "Validation Loss: 3.1245606795997376, Validation Accuracy: 25.59\n",
            "[90/150]: Training Loss: 2.79326137008667, Training Accuracy: 31.37\n",
            "Validation Loss: 3.1252101788854905, Validation Accuracy: 25.47\n",
            "[91/150]: Training Loss: 2.7884928009033203, Training Accuracy: 31.3925\n",
            "Validation Loss: 3.115745410797702, Validation Accuracy: 25.52\n",
            "[92/150]: Training Loss: 2.7844152694702147, Training Accuracy: 31.5825\n",
            "Validation Loss: 3.1132962870749696, Validation Accuracy: 25.67\n",
            "[93/150]: Training Loss: 2.7792159679412842, Training Accuracy: 31.665\n",
            "Validation Loss: 3.111494478906036, Validation Accuracy: 25.84\n",
            "[94/150]: Training Loss: 2.773549281311035, Training Accuracy: 31.6525\n",
            "Validation Loss: 3.1067730256706287, Validation Accuracy: 25.89\n",
            "[95/150]: Training Loss: 2.76964068031311, Training Accuracy: 31.895\n",
            "Validation Loss: 3.1083280220153227, Validation Accuracy: 25.78\n",
            "[96/150]: Training Loss: 2.7651722465515136, Training Accuracy: 31.8825\n",
            "Validation Loss: 3.1057740730844485, Validation Accuracy: 25.67\n",
            "[97/150]: Training Loss: 2.761124114608765, Training Accuracy: 32.1025\n",
            "Validation Loss: 3.1073889534944183, Validation Accuracy: 25.55\n",
            "[98/150]: Training Loss: 2.756427172470093, Training Accuracy: 32.1475\n",
            "Validation Loss: 3.1095221422280477, Validation Accuracy: 25.75\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 6.147166403995198, Test Accuracy: 12.53\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>12.53</td></tr><tr><td>Test Loss</td><td>6.14717</td></tr><tr><td>Train Accuracy</td><td>32.1475</td></tr><tr><td>Train Loss</td><td>2.75643</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.1 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/26wygp07</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_024913-26wygp07/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.5 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_030015-a8qnvlbp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp' target=\"_blank\">learning_rate=0.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.377298123168945, Training Accuracy: 3.2725\n",
            "Validation Loss: 4.2101050941807445, Validation Accuracy: 5.2\n",
            "[2/150]: Training Loss: 4.07323479423523, Training Accuracy: 7.05\n",
            "Validation Loss: 3.997956647994412, Validation Accuracy: 8.69\n",
            "[3/150]: Training Loss: 3.9166299026489257, Training Accuracy: 9.98\n",
            "Validation Loss: 3.8700176090191882, Validation Accuracy: 10.85\n",
            "[4/150]: Training Loss: 3.76255333404541, Training Accuracy: 12.6475\n",
            "Validation Loss: 3.7763638131937403, Validation Accuracy: 12.03\n",
            "[5/150]: Training Loss: 3.6385873168945313, Training Accuracy: 14.8\n",
            "Validation Loss: 3.5995885247637514, Validation Accuracy: 15.3\n",
            "[6/150]: Training Loss: 3.5313883621215822, Training Accuracy: 16.6625\n",
            "Validation Loss: 3.547201700271315, Validation Accuracy: 16.33\n",
            "[7/150]: Training Loss: 3.4311797870635985, Training Accuracy: 18.0425\n",
            "Validation Loss: 3.447270431336324, Validation Accuracy: 18.49\n",
            "[8/150]: Training Loss: 3.3406018447875976, Training Accuracy: 20.2325\n",
            "Validation Loss: 3.3745645914867426, Validation Accuracy: 19.81\n",
            "[9/150]: Training Loss: 3.2601512760162352, Training Accuracy: 21.73\n",
            "Validation Loss: 3.3083346163391307, Validation Accuracy: 21.21\n",
            "[10/150]: Training Loss: 3.190315942764282, Training Accuracy: 23.0925\n",
            "Validation Loss: 3.2459553244766917, Validation Accuracy: 22.4\n",
            "[11/150]: Training Loss: 3.120881378555298, Training Accuracy: 24.2625\n",
            "Validation Loss: 3.2441095774340782, Validation Accuracy: 22.07\n",
            "[12/150]: Training Loss: 3.065099639892578, Training Accuracy: 25.1825\n",
            "Validation Loss: 3.205546066259882, Validation Accuracy: 22.44\n",
            "[13/150]: Training Loss: 3.002047216796875, Training Accuracy: 26.4075\n",
            "Validation Loss: 3.1507282090035216, Validation Accuracy: 24.42\n",
            "[14/150]: Training Loss: 2.947496301269531, Training Accuracy: 27.6075\n",
            "Validation Loss: 3.1272822519776167, Validation Accuracy: 24.2\n",
            "[15/150]: Training Loss: 2.8929669227600097, Training Accuracy: 28.1625\n",
            "Validation Loss: 3.08642372052381, Validation Accuracy: 25.55\n",
            "[16/150]: Training Loss: 2.8405557304382323, Training Accuracy: 29.4725\n",
            "Validation Loss: 3.021532262206837, Validation Accuracy: 26.7\n",
            "[17/150]: Training Loss: 2.7890852066040037, Training Accuracy: 30.505\n",
            "Validation Loss: 3.042905480998337, Validation Accuracy: 26.24\n",
            "[18/150]: Training Loss: 2.74124369392395, Training Accuracy: 31.39\n",
            "Validation Loss: 2.9991517309929914, Validation Accuracy: 26.63\n",
            "[19/150]: Training Loss: 2.692549164581299, Training Accuracy: 32.36\n",
            "Validation Loss: 2.9798509998685994, Validation Accuracy: 27.88\n",
            "[20/150]: Training Loss: 2.6449439025878907, Training Accuracy: 33.1025\n",
            "Validation Loss: 3.0654538998937912, Validation Accuracy: 26.5\n",
            "[21/150]: Training Loss: 2.6013897836685183, Training Accuracy: 34.135\n",
            "Validation Loss: 2.937944315041706, Validation Accuracy: 29.27\n",
            "[22/150]: Training Loss: 2.5583987575531006, Training Accuracy: 35.0375\n",
            "Validation Loss: 2.965156334980278, Validation Accuracy: 28.48\n",
            "[23/150]: Training Loss: 2.5067957414627076, Training Accuracy: 36.145\n",
            "Validation Loss: 2.940655703757219, Validation Accuracy: 28.99\n",
            "[24/150]: Training Loss: 2.4618710725784303, Training Accuracy: 37.005\n",
            "Validation Loss: 2.9297352444594074, Validation Accuracy: 28.88\n",
            "[25/150]: Training Loss: 2.4159774208068847, Training Accuracy: 37.9375\n",
            "Validation Loss: 2.9579435579336373, Validation Accuracy: 28.85\n",
            "[26/150]: Training Loss: 2.3772740001678465, Training Accuracy: 38.8375\n",
            "Validation Loss: 2.892203370476984, Validation Accuracy: 29.97\n",
            "[27/150]: Training Loss: 2.331508861351013, Training Accuracy: 39.695\n",
            "Validation Loss: 2.902201515853785, Validation Accuracy: 29.95\n",
            "[28/150]: Training Loss: 2.292740362548828, Training Accuracy: 40.25\n",
            "Validation Loss: 2.884886794788822, Validation Accuracy: 30.54\n",
            "[29/150]: Training Loss: 2.246493480873108, Training Accuracy: 41.2375\n",
            "Validation Loss: 2.9104482884619647, Validation Accuracy: 30.14\n",
            "[30/150]: Training Loss: 2.203635430717468, Training Accuracy: 42.4475\n",
            "Validation Loss: 2.9059300164508213, Validation Accuracy: 31.3\n",
            "[31/150]: Training Loss: 2.1624910346984865, Training Accuracy: 43.2625\n",
            "Validation Loss: 2.884638991325524, Validation Accuracy: 31.26\n",
            "[32/150]: Training Loss: 2.122718844985962, Training Accuracy: 44.17\n",
            "Validation Loss: 2.9034518907024602, Validation Accuracy: 30.71\n",
            "[33/150]: Training Loss: 2.0803083070755006, Training Accuracy: 45.055\n",
            "Validation Loss: 2.8911248453103813, Validation Accuracy: 31.08\n",
            "[34/150]: Training Loss: 2.0393711433410644, Training Accuracy: 45.9575\n",
            "Validation Loss: 2.9181400818429934, Validation Accuracy: 31.06\n",
            "[35/150]: Training Loss: 2.001698533630371, Training Accuracy: 46.8225\n",
            "Validation Loss: 2.9216579084943053, Validation Accuracy: 30.6\n",
            "[36/150]: Training Loss: 1.9612565605163574, Training Accuracy: 47.845\n",
            "Validation Loss: 2.95554546945414, Validation Accuracy: 31.47\n",
            "[37/150]: Training Loss: 1.9143695009231567, Training Accuracy: 48.7825\n",
            "Validation Loss: 2.9769522748934993, Validation Accuracy: 30.95\n",
            "[38/150]: Training Loss: 1.8757219652175903, Training Accuracy: 49.6925\n",
            "Validation Loss: 2.954901927595685, Validation Accuracy: 31.79\n",
            "[39/150]: Training Loss: 1.8348310264587402, Training Accuracy: 50.46\n",
            "Validation Loss: 2.97764066374226, Validation Accuracy: 31.41\n",
            "[40/150]: Training Loss: 1.7973475383758546, Training Accuracy: 51.6725\n",
            "Validation Loss: 3.0162993069666966, Validation Accuracy: 31.12\n",
            "[41/150]: Training Loss: 1.7525395877838135, Training Accuracy: 52.525\n",
            "Validation Loss: 3.0236745138836514, Validation Accuracy: 30.91\n",
            "[42/150]: Training Loss: 1.7114470052719115, Training Accuracy: 53.885\n",
            "Validation Loss: 3.051269774224348, Validation Accuracy: 31.17\n",
            "[43/150]: Training Loss: 1.6800853149414063, Training Accuracy: 54.0875\n",
            "Validation Loss: 3.0829398237216243, Validation Accuracy: 31.28\n",
            "[44/150]: Training Loss: 1.636157625389099, Training Accuracy: 55.3375\n",
            "Validation Loss: 3.110968621673098, Validation Accuracy: 31.14\n",
            "[45/150]: Training Loss: 1.589929871749878, Training Accuracy: 56.7225\n",
            "Validation Loss: 3.142918248085459, Validation Accuracy: 31.26\n",
            "[46/150]: Training Loss: 1.5445106566429139, Training Accuracy: 57.9225\n",
            "Validation Loss: 3.1339099574240907, Validation Accuracy: 31.8\n",
            "[47/150]: Training Loss: 1.5067133940696715, Training Accuracy: 58.8375\n",
            "Validation Loss: 3.1682807533604325, Validation Accuracy: 31.84\n",
            "[48/150]: Training Loss: 1.4696476486206054, Training Accuracy: 59.655\n",
            "Validation Loss: 3.2345377184023523, Validation Accuracy: 30.89\n",
            "[49/150]: Training Loss: 1.4380368849754332, Training Accuracy: 60.32\n",
            "Validation Loss: 3.2836993484740047, Validation Accuracy: 31.58\n",
            "[50/150]: Training Loss: 1.3966082113265992, Training Accuracy: 61.33\n",
            "Validation Loss: 3.369933787424853, Validation Accuracy: 30.61\n",
            "[51/150]: Training Loss: 1.3530357138633728, Training Accuracy: 62.43\n",
            "Validation Loss: 3.357933571384211, Validation Accuracy: 31.19\n",
            "[52/150]: Training Loss: 1.31355241689682, Training Accuracy: 63.4325\n",
            "Validation Loss: 3.41107276138986, Validation Accuracy: 30.17\n",
            "[53/150]: Training Loss: 1.2827554839134216, Training Accuracy: 64.0425\n",
            "Validation Loss: 3.376580183673057, Validation Accuracy: 30.85\n",
            "[54/150]: Training Loss: 1.2412437489509582, Training Accuracy: 65.2825\n",
            "Validation Loss: 3.48424918332677, Validation Accuracy: 30.12\n",
            "[55/150]: Training Loss: 1.1999455925941467, Training Accuracy: 66.39\n",
            "Validation Loss: 3.4844332743602195, Validation Accuracy: 30.82\n",
            "[56/150]: Training Loss: 1.1624719073295593, Training Accuracy: 67.315\n",
            "Validation Loss: 3.535801676428242, Validation Accuracy: 30.86\n",
            "[57/150]: Training Loss: 1.1305819693565369, Training Accuracy: 67.9275\n",
            "Validation Loss: 3.6139490315868597, Validation Accuracy: 30.44\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 16.0335865749675, Test Accuracy: 13.22\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>13.22</td></tr><tr><td>Test Loss</td><td>16.03359</td></tr><tr><td>Train Accuracy</td><td>67.9275</td></tr><tr><td>Train Loss</td><td>1.13058</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/a8qnvlbp</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_030015-a8qnvlbp/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:1 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_030647-hgowk59s</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s' target=\"_blank\">learning_rate=1 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.390103232955933, Training Accuracy: 2.935\n",
            "Validation Loss: 4.160131826522244, Validation Accuracy: 5.54\n",
            "[2/150]: Training Loss: 4.031327721786499, Training Accuracy: 7.5025\n",
            "Validation Loss: 3.9958771960750505, Validation Accuracy: 8.6\n",
            "[3/150]: Training Loss: 3.800543330383301, Training Accuracy: 11.665\n",
            "Validation Loss: 3.7379826664165328, Validation Accuracy: 12.06\n",
            "[4/150]: Training Loss: 3.6242603660583494, Training Accuracy: 14.345\n",
            "Validation Loss: 3.559469526740396, Validation Accuracy: 15.28\n",
            "[5/150]: Training Loss: 3.48455090675354, Training Accuracy: 16.9325\n",
            "Validation Loss: 3.491581111956554, Validation Accuracy: 16.56\n",
            "[6/150]: Training Loss: 3.3448009601593016, Training Accuracy: 19.435\n",
            "Validation Loss: 3.3586603653658726, Validation Accuracy: 19.41\n",
            "[7/150]: Training Loss: 3.229361641693115, Training Accuracy: 21.4375\n",
            "Validation Loss: 3.2139996130754995, Validation Accuracy: 21.78\n",
            "[8/150]: Training Loss: 3.1254781677246095, Training Accuracy: 23.4825\n",
            "Validation Loss: 3.168266389020689, Validation Accuracy: 22.86\n",
            "[9/150]: Training Loss: 3.0173661666870117, Training Accuracy: 25.6925\n",
            "Validation Loss: 3.1283769850518293, Validation Accuracy: 24.17\n",
            "[10/150]: Training Loss: 2.931245880126953, Training Accuracy: 26.99\n",
            "Validation Loss: 3.016676799506898, Validation Accuracy: 25.79\n",
            "[11/150]: Training Loss: 2.8445968715667727, Training Accuracy: 28.6675\n",
            "Validation Loss: 2.968725839238258, Validation Accuracy: 26.94\n",
            "[12/150]: Training Loss: 2.769311902618408, Training Accuracy: 30.345\n",
            "Validation Loss: 2.935349525160091, Validation Accuracy: 27.73\n",
            "[13/150]: Training Loss: 2.698523984146118, Training Accuracy: 31.47\n",
            "Validation Loss: 2.9356894462731233, Validation Accuracy: 28.11\n",
            "[14/150]: Training Loss: 2.6179113666534426, Training Accuracy: 33.2625\n",
            "Validation Loss: 2.8293940914664297, Validation Accuracy: 30.0\n",
            "[15/150]: Training Loss: 2.5539515073776244, Training Accuracy: 34.565\n",
            "Validation Loss: 2.8150154101620815, Validation Accuracy: 30.54\n",
            "[16/150]: Training Loss: 2.482784992599487, Training Accuracy: 36.2825\n",
            "Validation Loss: 2.801185814438352, Validation Accuracy: 30.73\n",
            "[17/150]: Training Loss: 2.416481968307495, Training Accuracy: 37.2175\n",
            "Validation Loss: 2.778297721200688, Validation Accuracy: 31.36\n",
            "[18/150]: Training Loss: 2.3516669242858885, Training Accuracy: 38.785\n",
            "Validation Loss: 2.803610542017943, Validation Accuracy: 31.44\n",
            "[19/150]: Training Loss: 2.2785828254699707, Training Accuracy: 40.36\n",
            "Validation Loss: 2.770737362515395, Validation Accuracy: 31.54\n",
            "[20/150]: Training Loss: 2.2117202407836913, Training Accuracy: 42.02\n",
            "Validation Loss: 2.751694551698721, Validation Accuracy: 32.2\n",
            "[21/150]: Training Loss: 2.1421232624053954, Training Accuracy: 43.1125\n",
            "Validation Loss: 2.7352929327897963, Validation Accuracy: 32.64\n",
            "[22/150]: Training Loss: 2.0760102186203, Training Accuracy: 44.805\n",
            "Validation Loss: 2.7648525397489023, Validation Accuracy: 32.85\n",
            "[23/150]: Training Loss: 2.0119320999145507, Training Accuracy: 46.1375\n",
            "Validation Loss: 2.7834541448362313, Validation Accuracy: 33.19\n",
            "[24/150]: Training Loss: 1.946039645767212, Training Accuracy: 47.555\n",
            "Validation Loss: 2.805889447023914, Validation Accuracy: 33.52\n",
            "[25/150]: Training Loss: 1.8788161462783814, Training Accuracy: 49.055\n",
            "Validation Loss: 2.7862223508251702, Validation Accuracy: 34.25\n",
            "[26/150]: Training Loss: 1.799899059486389, Training Accuracy: 50.915\n",
            "Validation Loss: 2.804932374863108, Validation Accuracy: 33.77\n",
            "[27/150]: Training Loss: 1.7454933450698853, Training Accuracy: 52.16\n",
            "Validation Loss: 2.861256376193587, Validation Accuracy: 33.83\n",
            "[28/150]: Training Loss: 1.6650921211242675, Training Accuracy: 53.9225\n",
            "Validation Loss: 2.877333830116661, Validation Accuracy: 34.25\n",
            "[29/150]: Training Loss: 1.6064131809234619, Training Accuracy: 55.555\n",
            "Validation Loss: 2.973996728089205, Validation Accuracy: 33.39\n",
            "[30/150]: Training Loss: 1.543812055683136, Training Accuracy: 56.7775\n",
            "Validation Loss: 2.943861024394916, Validation Accuracy: 34.15\n",
            "[31/150]: Training Loss: 1.4620859157562256, Training Accuracy: 58.88\n",
            "Validation Loss: 2.9692332015675342, Validation Accuracy: 33.88\n",
            "[32/150]: Training Loss: 1.4009520672798157, Training Accuracy: 60.3925\n",
            "Validation Loss: 3.024310210707841, Validation Accuracy: 33.42\n",
            "[33/150]: Training Loss: 1.3313846939086913, Training Accuracy: 62.155\n",
            "Validation Loss: 3.1067024993289047, Validation Accuracy: 33.18\n",
            "[34/150]: Training Loss: 1.2518226915359496, Training Accuracy: 64.225\n",
            "Validation Loss: 3.307321997964458, Validation Accuracy: 33.31\n",
            "[35/150]: Training Loss: 1.1938729809761048, Training Accuracy: 65.5075\n",
            "Validation Loss: 3.3076086089869214, Validation Accuracy: 32.76\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 18.36642926210051, Test Accuracy: 13.77\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>13.77</td></tr><tr><td>Test Loss</td><td>18.36643</td></tr><tr><td>Train Accuracy</td><td>65.5075</td></tr><tr><td>Train Loss</td><td>1.19387</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=1 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hgowk59s</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_030647-hgowk59s/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:1.5 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_031055-hfm8dgbz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz' target=\"_blank\">learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.426865048217773, Training Accuracy: 2.45\n",
            "Validation Loss: 4.219578746018136, Validation Accuracy: 4.19\n",
            "[2/150]: Training Loss: 4.060279036712647, Training Accuracy: 6.7225\n",
            "Validation Loss: 3.8940811293899635, Validation Accuracy: 9.43\n",
            "[3/150]: Training Loss: 3.7707011901855467, Training Accuracy: 11.6925\n",
            "Validation Loss: 3.8030508642743346, Validation Accuracy: 11.0\n",
            "[4/150]: Training Loss: 3.58028080368042, Training Accuracy: 15.05\n",
            "Validation Loss: 3.516739834645751, Validation Accuracy: 16.1\n",
            "[5/150]: Training Loss: 3.4152388591766356, Training Accuracy: 17.82\n",
            "Validation Loss: 3.411277610025588, Validation Accuracy: 17.62\n",
            "[6/150]: Training Loss: 3.2448327434539794, Training Accuracy: 20.7725\n",
            "Validation Loss: 3.2892891753251385, Validation Accuracy: 19.86\n",
            "[7/150]: Training Loss: 3.1129616333007815, Training Accuracy: 23.41\n",
            "Validation Loss: 3.155244686041668, Validation Accuracy: 22.75\n",
            "[8/150]: Training Loss: 2.9936462223052978, Training Accuracy: 25.73\n",
            "Validation Loss: 3.020396041262681, Validation Accuracy: 25.53\n",
            "[9/150]: Training Loss: 2.8851455352783204, Training Accuracy: 28.0475\n",
            "Validation Loss: 2.9794276216227535, Validation Accuracy: 26.31\n",
            "[10/150]: Training Loss: 2.7872766895294188, Training Accuracy: 29.955\n",
            "Validation Loss: 2.873067584007409, Validation Accuracy: 28.57\n",
            "[11/150]: Training Loss: 2.6929493949890135, Training Accuracy: 31.84\n",
            "Validation Loss: 2.8678578267431565, Validation Accuracy: 28.72\n",
            "[12/150]: Training Loss: 2.60749265499115, Training Accuracy: 33.46\n",
            "Validation Loss: 2.831394789325204, Validation Accuracy: 29.21\n",
            "[13/150]: Training Loss: 2.5269983169555665, Training Accuracy: 35.2325\n",
            "Validation Loss: 2.8061546094857963, Validation Accuracy: 29.73\n",
            "[14/150]: Training Loss: 2.430880810165405, Training Accuracy: 37.0475\n",
            "Validation Loss: 2.777110814288923, Validation Accuracy: 31.06\n",
            "[15/150]: Training Loss: 2.357077407836914, Training Accuracy: 38.8175\n",
            "Validation Loss: 2.7613005273661035, Validation Accuracy: 32.32\n",
            "[16/150]: Training Loss: 2.2730004482269286, Training Accuracy: 40.5\n",
            "Validation Loss: 2.7338021667140304, Validation Accuracy: 32.77\n",
            "[17/150]: Training Loss: 2.183533114242554, Training Accuracy: 42.27\n",
            "Validation Loss: 2.764839885341134, Validation Accuracy: 32.81\n",
            "[18/150]: Training Loss: 2.1031003602981566, Training Accuracy: 43.9825\n",
            "Validation Loss: 2.7428993381512394, Validation Accuracy: 32.82\n",
            "[19/150]: Training Loss: 2.0166405237197877, Training Accuracy: 46.195\n",
            "Validation Loss: 2.792896580544247, Validation Accuracy: 32.59\n",
            "[20/150]: Training Loss: 1.9404266941070556, Training Accuracy: 47.745\n",
            "Validation Loss: 2.742409686374057, Validation Accuracy: 34.58\n",
            "[21/150]: Training Loss: 1.8538161834716798, Training Accuracy: 49.4875\n",
            "Validation Loss: 2.809636064395783, Validation Accuracy: 33.93\n",
            "[22/150]: Training Loss: 1.768288578414917, Training Accuracy: 51.4775\n",
            "Validation Loss: 2.7697540058451855, Validation Accuracy: 34.68\n",
            "[23/150]: Training Loss: 1.684787001991272, Training Accuracy: 53.0825\n",
            "Validation Loss: 2.8915080067458425, Validation Accuracy: 33.94\n",
            "[24/150]: Training Loss: 1.6049392827987672, Training Accuracy: 55.145\n",
            "Validation Loss: 2.8930992533446878, Validation Accuracy: 33.94\n",
            "[25/150]: Training Loss: 1.511135014438629, Training Accuracy: 57.31\n",
            "Validation Loss: 2.9712191205115834, Validation Accuracy: 33.37\n",
            "[26/150]: Training Loss: 1.431588893699646, Training Accuracy: 59.12\n",
            "Validation Loss: 3.04860136311525, Validation Accuracy: 34.26\n",
            "[27/150]: Training Loss: 1.359563471508026, Training Accuracy: 60.8075\n",
            "Validation Loss: 3.205587582983029, Validation Accuracy: 33.21\n",
            "[28/150]: Training Loss: 1.2744348917961121, Training Accuracy: 63.0075\n",
            "Validation Loss: 3.1817259317750386, Validation Accuracy: 32.84\n",
            "[29/150]: Training Loss: 1.2077363389968871, Training Accuracy: 64.38\n",
            "Validation Loss: 3.2625666909916387, Validation Accuracy: 33.74\n",
            "[30/150]: Training Loss: 1.146615783405304, Training Accuracy: 66.3775\n",
            "Validation Loss: 3.439337396317986, Validation Accuracy: 32.76\n",
            "[31/150]: Training Loss: 1.0637209503173828, Training Accuracy: 68.4275\n",
            "Validation Loss: 3.464182601612844, Validation Accuracy: 32.3\n",
            "[32/150]: Training Loss: 1.0153738078117371, Training Accuracy: 69.42\n",
            "Validation Loss: 3.585658008125937, Validation Accuracy: 32.99\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 20.296875631733304, Test Accuracy: 15.72\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>15.72</td></tr><tr><td>Test Loss</td><td>20.29688</td></tr><tr><td>Train Accuracy</td><td>69.42</td></tr><tr><td>Train Loss</td><td>1.01537</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/hfm8dgbz</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_031055-hfm8dgbz/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:2 and wd:0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240612_031444-ws7fqwm1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1' target=\"_blank\">learning_rate=2 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.6014143745422365, Training Accuracy: 0.98\n",
            "Validation Loss: 4.606205284215842, Validation Accuracy: 0.91\n",
            "[2/150]: Training Loss: 4.605726162719726, Training Accuracy: 0.93\n",
            "Validation Loss: 4.606422876856129, Validation Accuracy: 0.91\n",
            "[3/150]: Training Loss: 4.605443458557129, Training Accuracy: 0.9375\n",
            "Validation Loss: 4.606579792727331, Validation Accuracy: 0.82\n",
            "[4/150]: Training Loss: 4.6053140991210935, Training Accuracy: 1.015\n",
            "Validation Loss: 4.606754381945179, Validation Accuracy: 0.82\n",
            "[5/150]: Training Loss: 4.6052259376525875, Training Accuracy: 1.0275\n",
            "Validation Loss: 4.60688726765335, Validation Accuracy: 0.82\n",
            "[6/150]: Training Loss: 4.605191477966309, Training Accuracy: 1.0175\n",
            "Validation Loss: 4.607019655264107, Validation Accuracy: 0.82\n",
            "[7/150]: Training Loss: 4.6051658882141115, Training Accuracy: 1.015\n",
            "Validation Loss: 4.607097844409335, Validation Accuracy: 0.82\n",
            "[8/150]: Training Loss: 4.605149390411377, Training Accuracy: 0.9775\n",
            "Validation Loss: 4.607179025176224, Validation Accuracy: 0.82\n",
            "[9/150]: Training Loss: 4.605146067810058, Training Accuracy: 0.9425\n",
            "Validation Loss: 4.6072455181437695, Validation Accuracy: 0.82\n",
            "[10/150]: Training Loss: 4.605130741882324, Training Accuracy: 1.02\n",
            "Validation Loss: 4.607305599625703, Validation Accuracy: 0.82\n",
            "[11/150]: Training Loss: 4.605136211395264, Training Accuracy: 0.9625\n",
            "Validation Loss: 4.607342504392005, Validation Accuracy: 0.82\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 4.605385412835771, Test Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.0</td></tr><tr><td>Test Loss</td><td>4.60539</td></tr><tr><td>Train Accuracy</td><td>0.9625</td></tr><tr><td>Train Loss</td><td>4.60514</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=2 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning/runs/ws7fqwm1</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240612_031444-ws7fqwm1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "\n",
        "for lr in learning_rates:\n",
        "\n",
        "  print('='*50)\n",
        "  print(f'Hyperparameter with lr:{lr} and wd:{wd}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {'learning_rate': lr,\n",
        "                      'weight_decay' : wd\n",
        "                      }\n",
        "  # Load the model\n",
        "  model = LeNet5().to(device)\n",
        "\n",
        "  # Optimizer and scheduler setup\n",
        "  optimizer = LARS(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "  # Training\n",
        "  run_training(num_epochs,\n",
        "                model,\n",
        "                train_loader,\n",
        "                validation_loader,\n",
        "                test_loader,\n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                criterion,\n",
        "                device,\n",
        "                'LARS-HyperParameterTuning',\n",
        "                hyperparameters=hyperparameters,\n",
        "                is_wandb = True,\n",
        "                n_epochs_stop = 10\n",
        "  )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2bd5fda76227433aa9332e6e48cd415b",
            "fd6480fe59104c7aa15d39bf6ea4a63b",
            "3cb7b5c42e844747ae133750b7d42882",
            "b81cc89707e44dedb51081d13a3ba424",
            "1d74e9350c2c4c61b2e95924ec133012",
            "a5758bfac16a4388a036005a15812d1d",
            "0fdaf43c468043139e5d72397b118371",
            "f63fffe56ff64f1eb2b6e8f14974f011"
          ]
        },
        "id": "czSHzFs7SQug",
        "outputId": "fc513f95-8814-4002-e0d6-2a2eedad7dd7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_005802-t2cjgem5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5' target=\"_blank\">learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1584951/4263216451.py:113: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1578.)\n",
            "  d_p.add_(weight_decay, p.data)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.004822687724667, Training Accuracy: 8.156\n",
            "Validation Loss: 3.6169370329304105, Validation Accuracy: 15.1\n",
            "[2/150]: Training Loss: 3.489779775099986, Training Accuracy: 16.288\n",
            "Validation Loss: 3.195649910884298, Validation Accuracy: 21.71\n",
            "[3/150]: Training Loss: 3.188335987003258, Training Accuracy: 21.346\n",
            "Validation Loss: 2.9737777011409685, Validation Accuracy: 26.15\n",
            "[4/150]: Training Loss: 3.0118914528576006, Training Accuracy: 24.982\n",
            "Validation Loss: 2.7995891707717995, Validation Accuracy: 29.11\n",
            "[5/150]: Training Loss: 2.8688232484071152, Training Accuracy: 28.08\n",
            "Validation Loss: 2.7174989600090464, Validation Accuracy: 30.23\n",
            "[6/150]: Training Loss: 2.7214920926276984, Training Accuracy: 30.662\n",
            "Validation Loss: 2.5305145515757763, Validation Accuracy: 34.82\n",
            "[7/150]: Training Loss: 2.6200312084858983, Training Accuracy: 32.576\n",
            "Validation Loss: 2.474041129373441, Validation Accuracy: 35.97\n",
            "[8/150]: Training Loss: 2.53857664821093, Training Accuracy: 34.358\n",
            "Validation Loss: 2.3864964094890913, Validation Accuracy: 37.73\n",
            "[9/150]: Training Loss: 2.460603771764604, Training Accuracy: 36.126\n",
            "Validation Loss: 2.3855689604570913, Validation Accuracy: 38.2\n",
            "[10/150]: Training Loss: 2.3902963816052507, Training Accuracy: 37.29\n",
            "Validation Loss: 2.301407885399594, Validation Accuracy: 40.45\n",
            "[11/150]: Training Loss: 2.3503475908733087, Training Accuracy: 38.624\n",
            "Validation Loss: 2.3037940666174435, Validation Accuracy: 39.96\n",
            "[12/150]: Training Loss: 2.303130599696313, Training Accuracy: 39.49\n",
            "Validation Loss: 2.2026211046109534, Validation Accuracy: 42.0\n",
            "[13/150]: Training Loss: 2.2486851939459895, Training Accuracy: 40.568\n",
            "Validation Loss: 2.212206517055536, Validation Accuracy: 42.21\n",
            "[14/150]: Training Loss: 2.203081512085312, Training Accuracy: 41.506\n",
            "Validation Loss: 2.1992271386893694, Validation Accuracy: 43.09\n",
            "[15/150]: Training Loss: 2.159660982963679, Training Accuracy: 42.428\n",
            "Validation Loss: 2.1746684677281958, Validation Accuracy: 43.09\n",
            "[16/150]: Training Loss: 2.1303664485511877, Training Accuracy: 43.166\n",
            "Validation Loss: 2.146143364298875, Validation Accuracy: 44.21\n",
            "[17/150]: Training Loss: 2.1123076631589925, Training Accuracy: 43.67\n",
            "Validation Loss: 2.1201481705258605, Validation Accuracy: 44.27\n",
            "[18/150]: Training Loss: 2.0810553551939748, Training Accuracy: 44.61\n",
            "Validation Loss: 2.0423277001472036, Validation Accuracy: 46.15\n",
            "[19/150]: Training Loss: 2.0513661890993338, Training Accuracy: 44.866\n",
            "Validation Loss: 2.091191877225402, Validation Accuracy: 44.81\n",
            "[20/150]: Training Loss: 2.018301057541157, Training Accuracy: 45.826\n",
            "Validation Loss: 2.1546491832490178, Validation Accuracy: 43.92\n",
            "[21/150]: Training Loss: 1.99891439834824, Training Accuracy: 46.082\n",
            "Validation Loss: 2.149173748720983, Validation Accuracy: 44.11\n",
            "[22/150]: Training Loss: 1.9834696277023276, Training Accuracy: 46.51\n",
            "Validation Loss: 2.0435469044241934, Validation Accuracy: 46.15\n",
            "[23/150]: Training Loss: 1.954583641970554, Training Accuracy: 46.984\n",
            "Validation Loss: 2.0593765806999937, Validation Accuracy: 46.4\n",
            "[24/150]: Training Loss: 1.9357828209772134, Training Accuracy: 47.47\n",
            "Validation Loss: 1.9959042148225625, Validation Accuracy: 47.77\n",
            "[25/150]: Training Loss: 1.9152823200311198, Training Accuracy: 48.11\n",
            "Validation Loss: 2.007097185037698, Validation Accuracy: 46.74\n",
            "[26/150]: Training Loss: 1.889640983870572, Training Accuracy: 48.486\n",
            "Validation Loss: 2.0237637371014636, Validation Accuracy: 46.53\n",
            "[27/150]: Training Loss: 1.8898505502954468, Training Accuracy: 48.75\n",
            "Validation Loss: 1.9383376062295998, Validation Accuracy: 48.79\n",
            "[28/150]: Training Loss: 1.8655293333865797, Training Accuracy: 49.182\n",
            "Validation Loss: 2.000895071181522, Validation Accuracy: 47.12\n",
            "[29/150]: Training Loss: 1.843536861564802, Training Accuracy: 49.576\n",
            "Validation Loss: 2.0557514102595627, Validation Accuracy: 46.42\n",
            "[30/150]: Training Loss: 1.8280195388037834, Training Accuracy: 49.938\n",
            "Validation Loss: 1.9708276941518115, Validation Accuracy: 48.17\n",
            "[31/150]: Training Loss: 1.8121484304632982, Training Accuracy: 50.256\n",
            "Validation Loss: 2.0148930929269, Validation Accuracy: 48.07\n",
            "[32/150]: Training Loss: 1.7905213011195287, Training Accuracy: 51.016\n",
            "Validation Loss: 1.9494955805456562, Validation Accuracy: 48.66\n",
            "[33/150]: Training Loss: 1.7819690168513667, Training Accuracy: 51.152\n",
            "Validation Loss: 1.9355155472542829, Validation Accuracy: 49.85\n",
            "[34/150]: Training Loss: 1.7735850943628784, Training Accuracy: 51.054\n",
            "Validation Loss: 1.9679190664534356, Validation Accuracy: 48.93\n",
            "[35/150]: Training Loss: 1.7615117981000934, Training Accuracy: 51.618\n",
            "Validation Loss: 2.019153520559809, Validation Accuracy: 47.96\n",
            "[36/150]: Training Loss: 1.7415476721875809, Training Accuracy: 51.984\n",
            "Validation Loss: 1.9617887530357214, Validation Accuracy: 48.72\n",
            "[37/150]: Training Loss: 1.7273670095007132, Training Accuracy: 52.228\n",
            "Validation Loss: 1.9595575993228111, Validation Accuracy: 49.25\n",
            "[38/150]: Training Loss: 1.7174837630423134, Training Accuracy: 52.45\n",
            "Validation Loss: 1.9162586555359469, Validation Accuracy: 50.15\n",
            "[39/150]: Training Loss: 1.6948718257877222, Training Accuracy: 53.104\n",
            "Validation Loss: 1.9148104600845628, Validation Accuracy: 50.11\n",
            "[40/150]: Training Loss: 1.6948116973537923, Training Accuracy: 53.164\n",
            "Validation Loss: 1.9015048544877653, Validation Accuracy: 50.46\n",
            "[41/150]: Training Loss: 1.669160524292675, Training Accuracy: 53.832\n",
            "Validation Loss: 1.9202428220943282, Validation Accuracy: 49.95\n",
            "[42/150]: Training Loss: 1.6587599679027372, Training Accuracy: 53.99\n",
            "Validation Loss: 1.9989394374713776, Validation Accuracy: 48.53\n",
            "[43/150]: Training Loss: 1.6486307676033596, Training Accuracy: 54.324\n",
            "Validation Loss: 1.9677664428759531, Validation Accuracy: 49.26\n",
            "[44/150]: Training Loss: 1.6284820756034168, Training Accuracy: 54.798\n",
            "Validation Loss: 1.917847274215358, Validation Accuracy: 50.53\n",
            "[45/150]: Training Loss: 1.6195188324774623, Training Accuracy: 54.902\n",
            "Validation Loss: 1.9071525859225327, Validation Accuracy: 50.67\n",
            "[46/150]: Training Loss: 1.6109208017206558, Training Accuracy: 54.96\n",
            "Validation Loss: 1.8967621220145257, Validation Accuracy: 51.23\n",
            "[47/150]: Training Loss: 1.602793920375502, Training Accuracy: 55.188\n",
            "Validation Loss: 1.8559222494720653, Validation Accuracy: 51.43\n",
            "[48/150]: Training Loss: 1.5858894056066528, Training Accuracy: 55.652\n",
            "Validation Loss: 1.9233948349193404, Validation Accuracy: 50.32\n",
            "[49/150]: Training Loss: 1.5650417865694637, Training Accuracy: 56.188\n",
            "Validation Loss: 1.9384330412384811, Validation Accuracy: 49.91\n",
            "[50/150]: Training Loss: 1.5663130182744291, Training Accuracy: 56.228\n",
            "Validation Loss: 1.8889641389725313, Validation Accuracy: 51.35\n",
            "[51/150]: Training Loss: 1.542809939445437, Training Accuracy: 56.846\n",
            "Validation Loss: 1.8398898855136459, Validation Accuracy: 51.84\n",
            "[52/150]: Training Loss: 1.5279571914002108, Training Accuracy: 57.064\n",
            "Validation Loss: 1.9208611204366015, Validation Accuracy: 50.34\n",
            "[53/150]: Training Loss: 1.510901224735143, Training Accuracy: 57.122\n",
            "Validation Loss: 1.9133090471765797, Validation Accuracy: 50.8\n",
            "[54/150]: Training Loss: 1.511483409596831, Training Accuracy: 57.642\n",
            "Validation Loss: 1.8751734176259132, Validation Accuracy: 51.71\n",
            "[55/150]: Training Loss: 1.4843521323960152, Training Accuracy: 58.136\n",
            "Validation Loss: 1.8774340836105832, Validation Accuracy: 51.56\n",
            "[56/150]: Training Loss: 1.4896384542403014, Training Accuracy: 57.742\n",
            "Validation Loss: 1.8484339972210537, Validation Accuracy: 52.7\n",
            "[57/150]: Training Loss: 1.4539345915207778, Training Accuracy: 58.774\n",
            "Validation Loss: 1.8711960125880636, Validation Accuracy: 50.9\n",
            "[58/150]: Training Loss: 1.4413522976591153, Training Accuracy: 59.254\n",
            "Validation Loss: 1.8433482806394055, Validation Accuracy: 52.14\n",
            "[59/150]: Training Loss: 1.435598407407551, Training Accuracy: 59.348\n",
            "Validation Loss: 1.842708926291982, Validation Accuracy: 52.32\n",
            "[60/150]: Training Loss: 1.415708273572995, Training Accuracy: 59.856\n",
            "Validation Loss: 1.8708495941891032, Validation Accuracy: 51.34\n",
            "[61/150]: Training Loss: 1.4131718484489508, Training Accuracy: 59.934\n",
            "Validation Loss: 1.8397565495436359, Validation Accuracy: 52.21\n",
            "[62/150]: Training Loss: 1.3917764421466672, Training Accuracy: 60.466\n",
            "Validation Loss: 1.8553483759521678, Validation Accuracy: 52.56\n",
            "[63/150]: Training Loss: 1.3845181973541485, Training Accuracy: 60.468\n",
            "Validation Loss: 1.8767407699755043, Validation Accuracy: 51.4\n",
            "[64/150]: Training Loss: 1.3718093946156904, Training Accuracy: 60.9\n",
            "Validation Loss: 1.833029270931414, Validation Accuracy: 53.34\n",
            "[65/150]: Training Loss: 1.3485638827771482, Training Accuracy: 61.372\n",
            "Validation Loss: 1.8435825352456159, Validation Accuracy: 52.78\n",
            "[66/150]: Training Loss: 1.3356790865778618, Training Accuracy: 61.914\n",
            "Validation Loss: 1.8920623299422537, Validation Accuracy: 52.03\n",
            "[67/150]: Training Loss: 1.3243823959242047, Training Accuracy: 61.984\n",
            "Validation Loss: 1.8541991725848739, Validation Accuracy: 53.25\n",
            "[68/150]: Training Loss: 1.3088703974128684, Training Accuracy: 62.324\n",
            "Validation Loss: 1.8384279855497323, Validation Accuracy: 52.97\n",
            "[69/150]: Training Loss: 1.2935843141487493, Training Accuracy: 62.726\n",
            "Validation Loss: 1.8648313618010017, Validation Accuracy: 52.35\n",
            "[70/150]: Training Loss: 1.2788333388240747, Training Accuracy: 63.162\n",
            "Validation Loss: 1.8200989673092107, Validation Accuracy: 53.57\n",
            "[71/150]: Training Loss: 1.275286832810058, Training Accuracy: 63.326\n",
            "Validation Loss: 1.8271277934122996, Validation Accuracy: 52.6\n",
            "[72/150]: Training Loss: 1.2548354720063222, Training Accuracy: 63.724\n",
            "Validation Loss: 1.832458599357848, Validation Accuracy: 53.43\n",
            "[73/150]: Training Loss: 1.2409771790589823, Training Accuracy: 64.15\n",
            "Validation Loss: 1.8489187993821066, Validation Accuracy: 53.54\n",
            "[74/150]: Training Loss: 1.2177538118703897, Training Accuracy: 64.682\n",
            "Validation Loss: 1.8377945020699957, Validation Accuracy: 53.86\n",
            "[75/150]: Training Loss: 1.2167601452764039, Training Accuracy: 64.92\n",
            "Validation Loss: 1.856788229031168, Validation Accuracy: 53.15\n",
            "[76/150]: Training Loss: 1.1967681403964987, Training Accuracy: 65.394\n",
            "Validation Loss: 1.8202834759548212, Validation Accuracy: 54.44\n",
            "[77/150]: Training Loss: 1.1781836492021371, Training Accuracy: 65.698\n",
            "Validation Loss: 1.8182067392738002, Validation Accuracy: 54.42\n",
            "[78/150]: Training Loss: 1.1718934528967913, Training Accuracy: 65.94\n",
            "Validation Loss: 1.8361693392893312, Validation Accuracy: 53.34\n",
            "[79/150]: Training Loss: 1.1540917455387847, Training Accuracy: 66.372\n",
            "Validation Loss: 1.8517911365837048, Validation Accuracy: 53.86\n",
            "[80/150]: Training Loss: 1.1260635425215182, Training Accuracy: 67.01\n",
            "Validation Loss: 1.8486420083197819, Validation Accuracy: 53.93\n",
            "[81/150]: Training Loss: 1.1187372058248886, Training Accuracy: 67.426\n",
            "Validation Loss: 1.822678742894701, Validation Accuracy: 54.82\n",
            "[82/150]: Training Loss: 1.1072917601184162, Training Accuracy: 67.646\n",
            "Validation Loss: 1.8640035948935587, Validation Accuracy: 53.57\n",
            "[83/150]: Training Loss: 1.082973983510376, Training Accuracy: 68.116\n",
            "Validation Loss: 1.867675439567323, Validation Accuracy: 53.9\n",
            "[84/150]: Training Loss: 1.0828096682915602, Training Accuracy: 68.366\n",
            "Validation Loss: 1.826197045244229, Validation Accuracy: 54.16\n",
            "[85/150]: Training Loss: 1.0598852286100997, Training Accuracy: 68.856\n",
            "Validation Loss: 1.8198747148938998, Validation Accuracy: 54.4\n",
            "[86/150]: Training Loss: 1.0407975543185572, Training Accuracy: 69.42\n",
            "Validation Loss: 1.8279076829837386, Validation Accuracy: 54.55\n",
            "[87/150]: Training Loss: 1.030418163827618, Training Accuracy: 69.804\n",
            "Validation Loss: 1.854318435784358, Validation Accuracy: 54.88\n",
            "[88/150]: Training Loss: 1.0299987217120807, Training Accuracy: 69.594\n",
            "Validation Loss: 1.8434015595988862, Validation Accuracy: 54.47\n",
            "[89/150]: Training Loss: 1.0026646399741892, Training Accuracy: 70.49\n",
            "Validation Loss: 1.8365007965428055, Validation Accuracy: 54.24\n",
            "[90/150]: Training Loss: 0.9944430979164055, Training Accuracy: 70.764\n",
            "Validation Loss: 1.845389035097353, Validation Accuracy: 54.42\n",
            "[91/150]: Training Loss: 0.9791042178945468, Training Accuracy: 70.958\n",
            "Validation Loss: 1.8592563897940764, Validation Accuracy: 54.71\n",
            "[92/150]: Training Loss: 0.9622275731371491, Training Accuracy: 71.348\n",
            "Validation Loss: 1.8250258181505143, Validation Accuracy: 55.29\n",
            "[93/150]: Training Loss: 0.9501752741349018, Training Accuracy: 71.708\n",
            "Validation Loss: 1.849954966526882, Validation Accuracy: 55.02\n",
            "[94/150]: Training Loss: 0.9297837285358278, Training Accuracy: 72.38\n",
            "Validation Loss: 1.8463909307103248, Validation Accuracy: 55.77\n",
            "[95/150]: Training Loss: 0.9163948694991944, Training Accuracy: 72.898\n",
            "Validation Loss: 1.8669461308011583, Validation Accuracy: 55.06\n",
            "[96/150]: Training Loss: 0.9114100606468938, Training Accuracy: 72.956\n",
            "Validation Loss: 1.8656909784693627, Validation Accuracy: 54.75\n",
            "[97/150]: Training Loss: 0.8974789249165284, Training Accuracy: 73.276\n",
            "Validation Loss: 1.834348332350421, Validation Accuracy: 55.01\n",
            "[98/150]: Training Loss: 0.8797091352360328, Training Accuracy: 73.696\n",
            "Validation Loss: 1.8680614483584264, Validation Accuracy: 55.11\n",
            "[99/150]: Training Loss: 0.8685366097085007, Training Accuracy: 74.026\n",
            "Validation Loss: 1.8696094318559975, Validation Accuracy: 55.43\n",
            "[100/150]: Training Loss: 0.8596895751745804, Training Accuracy: 74.396\n",
            "Validation Loss: 1.8580050965782944, Validation Accuracy: 55.31\n",
            "[101/150]: Training Loss: 0.8450928368531835, Training Accuracy: 74.934\n",
            "Validation Loss: 1.8560257132645626, Validation Accuracy: 55.71\n",
            "[102/150]: Training Loss: 0.834670692567935, Training Accuracy: 75.278\n",
            "Validation Loss: 1.8615986776959366, Validation Accuracy: 55.84\n",
            "[103/150]: Training Loss: 0.8168128573757303, Training Accuracy: 75.56\n",
            "Validation Loss: 1.8920900590100866, Validation Accuracy: 55.38\n",
            "[104/150]: Training Loss: 0.8022570627577165, Training Accuracy: 76.022\n",
            "Validation Loss: 1.9013077168707635, Validation Accuracy: 55.71\n",
            "[105/150]: Training Loss: 0.7898137537414766, Training Accuracy: 76.588\n",
            "Validation Loss: 1.8971044345266501, Validation Accuracy: 55.23\n",
            "[106/150]: Training Loss: 0.7799203321528252, Training Accuracy: 76.854\n",
            "Validation Loss: 1.8587115873956377, Validation Accuracy: 55.33\n",
            "[107/150]: Training Loss: 0.7687876791600377, Training Accuracy: 76.832\n",
            "Validation Loss: 1.9066706402286602, Validation Accuracy: 55.7\n",
            "[108/150]: Training Loss: 0.7502601898234823, Training Accuracy: 77.674\n",
            "Validation Loss: 1.891866291784177, Validation Accuracy: 56.06\n",
            "[109/150]: Training Loss: 0.7453718431236799, Training Accuracy: 77.84\n",
            "Validation Loss: 1.9089836011267012, Validation Accuracy: 55.88\n",
            "[110/150]: Training Loss: 0.7324799866322667, Training Accuracy: 78.2\n",
            "Validation Loss: 1.8889893972949616, Validation Accuracy: 55.7\n",
            "[111/150]: Training Loss: 0.716928729620736, Training Accuracy: 78.53\n",
            "Validation Loss: 1.9158697025791096, Validation Accuracy: 56.07\n",
            "[112/150]: Training Loss: 0.7084723916809882, Training Accuracy: 78.768\n",
            "Validation Loss: 1.9187719480247254, Validation Accuracy: 55.71\n",
            "[113/150]: Training Loss: 0.6928996008146754, Training Accuracy: 79.508\n",
            "Validation Loss: 1.9122050970223299, Validation Accuracy: 56.4\n",
            "[114/150]: Training Loss: 0.686598046775669, Training Accuracy: 79.428\n",
            "Validation Loss: 1.895180571990408, Validation Accuracy: 56.18\n",
            "[115/150]: Training Loss: 0.6837174600881079, Training Accuracy: 79.66\n",
            "Validation Loss: 1.9321321894408792, Validation Accuracy: 55.4\n",
            "[116/150]: Training Loss: 0.6646802525233735, Training Accuracy: 80.106\n",
            "Validation Loss: 1.9081798047776435, Validation Accuracy: 56.21\n",
            "[117/150]: Training Loss: 0.6522155370172638, Training Accuracy: 80.528\n",
            "Validation Loss: 1.9046855428416258, Validation Accuracy: 56.69\n",
            "[118/150]: Training Loss: 0.6456939719064766, Training Accuracy: 80.752\n",
            "Validation Loss: 1.9132253895899294, Validation Accuracy: 56.47\n",
            "[119/150]: Training Loss: 0.6339363064378729, Training Accuracy: 81.286\n",
            "Validation Loss: 1.914469665782467, Validation Accuracy: 56.51\n",
            "[120/150]: Training Loss: 0.6339401570351227, Training Accuracy: 81.134\n",
            "Validation Loss: 1.913045568830648, Validation Accuracy: 55.93\n",
            "[121/150]: Training Loss: 0.6202226441610804, Training Accuracy: 81.422\n",
            "Validation Loss: 1.9112963069016766, Validation Accuracy: 56.58\n",
            "[122/150]: Training Loss: 0.6146376765216403, Training Accuracy: 81.858\n",
            "Validation Loss: 1.916867652516456, Validation Accuracy: 56.28\n",
            "[123/150]: Training Loss: 0.6090771163363591, Training Accuracy: 81.96\n",
            "Validation Loss: 1.9238637059357515, Validation Accuracy: 56.33\n",
            "[124/150]: Training Loss: 0.5977434807497523, Training Accuracy: 82.352\n",
            "Validation Loss: 1.9187599549627607, Validation Accuracy: 56.09\n",
            "[125/150]: Training Loss: 0.5865007763933343, Training Accuracy: 82.728\n",
            "Validation Loss: 1.9290249192031326, Validation Accuracy: 55.85\n",
            "[126/150]: Training Loss: 0.5832711922390686, Training Accuracy: 82.85\n",
            "Validation Loss: 1.925058329560954, Validation Accuracy: 56.54\n",
            "[127/150]: Training Loss: 0.5710540865845692, Training Accuracy: 83.058\n",
            "Validation Loss: 1.9433572846613112, Validation Accuracy: 56.46\n",
            "[128/150]: Training Loss: 0.566251437987208, Training Accuracy: 83.328\n",
            "Validation Loss: 1.9474792632327718, Validation Accuracy: 56.24\n",
            "[129/150]: Training Loss: 0.5592512233787791, Training Accuracy: 83.606\n",
            "Validation Loss: 1.9336790825910628, Validation Accuracy: 56.49\n",
            "[130/150]: Training Loss: 0.5560948127675849, Training Accuracy: 83.726\n",
            "Validation Loss: 1.9358682472994373, Validation Accuracy: 56.75\n",
            "[131/150]: Training Loss: 0.5547005703191623, Training Accuracy: 83.68\n",
            "Validation Loss: 1.941912688647106, Validation Accuracy: 56.32\n",
            "[132/150]: Training Loss: 0.5438491536299591, Training Accuracy: 84.188\n",
            "Validation Loss: 1.9333096700868788, Validation Accuracy: 56.52\n",
            "[133/150]: Training Loss: 0.5359534242421465, Training Accuracy: 84.398\n",
            "Validation Loss: 1.9515662831106004, Validation Accuracy: 56.36\n",
            "[134/150]: Training Loss: 0.5345852662763937, Training Accuracy: 84.37\n",
            "Validation Loss: 1.939547040280263, Validation Accuracy: 56.31\n",
            "[135/150]: Training Loss: 0.5338244077266024, Training Accuracy: 84.354\n",
            "Validation Loss: 1.9529236646214867, Validation Accuracy: 56.4\n",
            "[136/150]: Training Loss: 0.5233329969751256, Training Accuracy: 84.792\n",
            "Validation Loss: 1.9439837556735726, Validation Accuracy: 56.43\n",
            "[137/150]: Training Loss: 0.519332280549247, Training Accuracy: 84.868\n",
            "Validation Loss: 1.9427648835880742, Validation Accuracy: 56.25\n",
            "[138/150]: Training Loss: 0.513342816468395, Training Accuracy: 85.202\n",
            "Validation Loss: 1.9522086123751987, Validation Accuracy: 56.69\n",
            "[139/150]: Training Loss: 0.5097889236515135, Training Accuracy: 85.132\n",
            "Validation Loss: 1.950890618904381, Validation Accuracy: 56.41\n",
            "[140/150]: Training Loss: 0.5080071812319329, Training Accuracy: 85.386\n",
            "Validation Loss: 1.9511753282729227, Validation Accuracy: 56.69\n",
            "[141/150]: Training Loss: 0.5083310039299528, Training Accuracy: 85.398\n",
            "Validation Loss: 1.9493300193434309, Validation Accuracy: 56.5\n",
            "[142/150]: Training Loss: 0.5110094372726157, Training Accuracy: 85.232\n",
            "Validation Loss: 1.9510933920076698, Validation Accuracy: 56.46\n",
            "[143/150]: Training Loss: 0.5043484553161179, Training Accuracy: 85.402\n",
            "Validation Loss: 1.9473720959797027, Validation Accuracy: 56.53\n",
            "[144/150]: Training Loss: 0.5001817758926346, Training Accuracy: 85.59\n",
            "Validation Loss: 1.9499852835752403, Validation Accuracy: 56.64\n",
            "[145/150]: Training Loss: 0.5050505888088584, Training Accuracy: 85.566\n",
            "Validation Loss: 1.9509570135432444, Validation Accuracy: 56.58\n",
            "[146/150]: Training Loss: 0.4944563165230824, Training Accuracy: 85.7\n",
            "Validation Loss: 1.9510046653686814, Validation Accuracy: 56.56\n",
            "[147/150]: Training Loss: 0.4947198845655717, Training Accuracy: 85.914\n",
            "Validation Loss: 1.9500796012817674, Validation Accuracy: 56.62\n",
            "[148/150]: Training Loss: 0.4965038236487857, Training Accuracy: 85.548\n",
            "Validation Loss: 1.9505202922092122, Validation Accuracy: 56.65\n",
            "[149/150]: Training Loss: 0.4984786443964905, Training Accuracy: 85.54\n",
            "Validation Loss: 1.94986033325742, Validation Accuracy: 56.62\n",
            "[150/150]: Training Loss: 0.498654707256333, Training Accuracy: 85.464\n",
            "Validation Loss: 1.9498052581860001, Validation Accuracy: 56.63\n",
            "**********************************************************************\n",
            "Test Loss: 1.9498052581860001, Test Accuracy: 56.63\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>56.63</td></tr><tr><td>Test Loss</td><td>1.94981</td></tr><tr><td>Train Accuracy</td><td>85.464</td></tr><tr><td>Train Loss</td><td>0.49865</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS/runs/t2cjgem5</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_005802-t2cjgem5/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 1.5\n",
        "wd = 1e-03\n",
        "\n",
        "hyperparameters = {'learning_rate': lr,\n",
        "                    'weight_decay' : wd\n",
        "                  }\n",
        "\n",
        "# Load the model\n",
        "model = LeNet5().to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer = LARS(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# Training\n",
        "run_training(\n",
        "    num_epochs,\n",
        "    model,\n",
        "    original_train_loader,\n",
        "    original_test_loader,\n",
        "    original_test_loader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    criterion,\n",
        "    device,\n",
        "    optimizer_name='LARS',\n",
        "    hyperparameters=hyperparameters,\n",
        "    is_wandb = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "81408f178c46447f90d36d1d18cdad82",
            "06d3de2006b14793bd57a5ca89d44e4a",
            "804c12a3f13840c7bdb2e6df69e62c10",
            "4a4cfb14c56e477cbfef5a652c05fabc",
            "68d86018628f420e8d7e516ba5827e35",
            "8877fd11846246f989e31835e5d3e7ae",
            "68ff4108835c462b929d0b5c78497555",
            "a63e1d987556448280ca217f17b60b49"
          ]
        },
        "id": "RFWHh4OL50WX",
        "outputId": "099a0039-9168-4d4e-84ec-4d6300dd3498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 512, Learning rate: 4.242640687119286, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:9hoyxk41) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=512 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/9hoyxk41' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/9hoyxk41</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_012745-9hoyxk41/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:9hoyxk41). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_012749-momu4e5u</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u' target=\"_blank\">batch_size=512 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.194531238808924, Training Accuracy: 5.802\n",
            "Validation Loss: 3.8219263911247254, Validation Accuracy: 11.06\n",
            "[2/150]: Training Loss: 3.737521487839368, Training Accuracy: 12.162\n",
            "Validation Loss: 3.5326468467712404, Validation Accuracy: 15.5\n",
            "[3/150]: Training Loss: 3.469568571265863, Training Accuracy: 16.654\n",
            "Validation Loss: 3.2325081706047056, Validation Accuracy: 21.14\n",
            "[4/150]: Training Loss: 3.2623822543085836, Training Accuracy: 20.338\n",
            "Validation Loss: 3.041664385795593, Validation Accuracy: 24.34\n",
            "[5/150]: Training Loss: 3.1202199824002324, Training Accuracy: 23.134\n",
            "Validation Loss: 2.9670259952545166, Validation Accuracy: 26.23\n",
            "[6/150]: Training Loss: 2.973516508024566, Training Accuracy: 25.864\n",
            "Validation Loss: 2.835892844200134, Validation Accuracy: 28.97\n",
            "[7/150]: Training Loss: 2.8618772808386357, Training Accuracy: 27.73\n",
            "Validation Loss: 2.731530475616455, Validation Accuracy: 31.08\n",
            "[8/150]: Training Loss: 2.7622045083921782, Training Accuracy: 29.856\n",
            "Validation Loss: 2.6019123077392576, Validation Accuracy: 33.01\n",
            "[9/150]: Training Loss: 2.719049726213728, Training Accuracy: 30.804\n",
            "Validation Loss: 2.6247976064682006, Validation Accuracy: 33.43\n",
            "[10/150]: Training Loss: 2.6084506000791277, Training Accuracy: 33.166\n",
            "Validation Loss: 2.4777934432029722, Validation Accuracy: 36.28\n",
            "[11/150]: Training Loss: 2.5361861233808556, Training Accuracy: 34.45\n",
            "Validation Loss: 2.3982665181159972, Validation Accuracy: 37.66\n",
            "[12/150]: Training Loss: 2.486152622164512, Training Accuracy: 35.61\n",
            "Validation Loss: 2.4073413372039796, Validation Accuracy: 37.69\n",
            "[13/150]: Training Loss: 2.419785971544227, Training Accuracy: 36.994\n",
            "Validation Loss: 2.3586077094078064, Validation Accuracy: 39.62\n",
            "[14/150]: Training Loss: 2.378681479668131, Training Accuracy: 37.686\n",
            "Validation Loss: 2.2874060750007628, Validation Accuracy: 40.44\n",
            "[15/150]: Training Loss: 2.3229291341742693, Training Accuracy: 39.04\n",
            "Validation Loss: 2.391434836387634, Validation Accuracy: 38.28\n",
            "[16/150]: Training Loss: 2.3006048445798912, Training Accuracy: 39.566\n",
            "Validation Loss: 2.233357620239258, Validation Accuracy: 40.92\n",
            "[17/150]: Training Loss: 2.270021514016755, Training Accuracy: 40.304\n",
            "Validation Loss: 2.2809773087501526, Validation Accuracy: 40.35\n",
            "[18/150]: Training Loss: 2.219994238444737, Training Accuracy: 41.128\n",
            "Validation Loss: 2.208327281475067, Validation Accuracy: 42.26\n",
            "[19/150]: Training Loss: 2.1639412665853697, Training Accuracy: 42.49\n",
            "Validation Loss: 2.1477415204048156, Validation Accuracy: 43.47\n",
            "[20/150]: Training Loss: 2.1379866052647025, Training Accuracy: 43.012\n",
            "Validation Loss: 2.202856254577637, Validation Accuracy: 42.27\n",
            "[21/150]: Training Loss: 2.139409989726787, Training Accuracy: 43.086\n",
            "Validation Loss: 2.1385614931583405, Validation Accuracy: 43.93\n",
            "[22/150]: Training Loss: 2.07097029564332, Training Accuracy: 44.974\n",
            "Validation Loss: 2.128925609588623, Validation Accuracy: 44.07\n",
            "[23/150]: Training Loss: 2.0605698106240253, Training Accuracy: 44.808\n",
            "Validation Loss: 2.15288764834404, Validation Accuracy: 44.03\n",
            "[24/150]: Training Loss: 2.0457090200210106, Training Accuracy: 45.174\n",
            "Validation Loss: 2.1201067209243774, Validation Accuracy: 44.63\n",
            "[25/150]: Training Loss: 2.0101604510326774, Training Accuracy: 45.978\n",
            "Validation Loss: 2.0930452048778534, Validation Accuracy: 45.37\n",
            "[26/150]: Training Loss: 1.9699756795046282, Training Accuracy: 46.9\n",
            "Validation Loss: 2.089737904071808, Validation Accuracy: 45.61\n",
            "[27/150]: Training Loss: 1.967079225851565, Training Accuracy: 47.01\n",
            "Validation Loss: 2.1089664578437803, Validation Accuracy: 44.55\n",
            "[28/150]: Training Loss: 1.9404766535272404, Training Accuracy: 47.568\n",
            "Validation Loss: 2.0624644994735717, Validation Accuracy: 45.57\n",
            "[29/150]: Training Loss: 1.928514483023663, Training Accuracy: 47.712\n",
            "Validation Loss: 2.0849966049194335, Validation Accuracy: 45.11\n",
            "[30/150]: Training Loss: 1.9005685375661265, Training Accuracy: 48.44\n",
            "Validation Loss: 2.015596163272858, Validation Accuracy: 46.79\n",
            "[31/150]: Training Loss: 1.868617719533492, Training Accuracy: 49.064\n",
            "Validation Loss: 2.044619733095169, Validation Accuracy: 45.67\n",
            "[32/150]: Training Loss: 1.8784857465296376, Training Accuracy: 48.794\n",
            "Validation Loss: 2.029283958673477, Validation Accuracy: 46.27\n",
            "[33/150]: Training Loss: 1.8382798968529215, Training Accuracy: 49.868\n",
            "Validation Loss: 1.9727658331394196, Validation Accuracy: 48.14\n",
            "[34/150]: Training Loss: 1.8194199094966965, Training Accuracy: 50.486\n",
            "Validation Loss: 1.9934200942516327, Validation Accuracy: 47.5\n",
            "[35/150]: Training Loss: 1.7988280094399745, Training Accuracy: 50.79\n",
            "Validation Loss: 2.0087626039981843, Validation Accuracy: 46.76\n",
            "[36/150]: Training Loss: 1.8009161900500863, Training Accuracy: 50.804\n",
            "Validation Loss: 1.9823269903659821, Validation Accuracy: 48.14\n",
            "[37/150]: Training Loss: 1.767955198579905, Training Accuracy: 51.498\n",
            "Validation Loss: 1.973670369386673, Validation Accuracy: 48.18\n",
            "[38/150]: Training Loss: 1.761318182458683, Training Accuracy: 51.526\n",
            "Validation Loss: 1.9882086098194123, Validation Accuracy: 47.85\n",
            "[39/150]: Training Loss: 1.7494291188765545, Training Accuracy: 52.066\n",
            "Validation Loss: 1.945462554693222, Validation Accuracy: 48.63\n",
            "[40/150]: Training Loss: 1.7158451688532927, Training Accuracy: 52.788\n",
            "Validation Loss: 1.9138611912727357, Validation Accuracy: 49.24\n",
            "[41/150]: Training Loss: 1.7024361783144426, Training Accuracy: 52.972\n",
            "Validation Loss: 1.9541066110134124, Validation Accuracy: 49.38\n",
            "[42/150]: Training Loss: 1.6934348527266054, Training Accuracy: 53.184\n",
            "Validation Loss: 1.9292299151420593, Validation Accuracy: 49.48\n",
            "[43/150]: Training Loss: 1.6734726489806662, Training Accuracy: 53.69\n",
            "Validation Loss: 1.9281952559947968, Validation Accuracy: 49.57\n",
            "[44/150]: Training Loss: 1.6466177933070125, Training Accuracy: 54.438\n",
            "Validation Loss: 1.92038534283638, Validation Accuracy: 49.7\n",
            "[45/150]: Training Loss: 1.6545339092916371, Training Accuracy: 54.198\n",
            "Validation Loss: 1.9556318461894988, Validation Accuracy: 49.26\n",
            "[46/150]: Training Loss: 1.619915745696243, Training Accuracy: 54.986\n",
            "Validation Loss: 1.899458384513855, Validation Accuracy: 50.32\n",
            "[47/150]: Training Loss: 1.6211930987786274, Training Accuracy: 54.738\n",
            "Validation Loss: 1.9299038767814636, Validation Accuracy: 49.14\n",
            "[48/150]: Training Loss: 1.5995109361045214, Training Accuracy: 55.258\n",
            "Validation Loss: 1.9302596151828766, Validation Accuracy: 49.32\n",
            "[49/150]: Training Loss: 1.5750943154704815, Training Accuracy: 55.898\n",
            "Validation Loss: 1.8993667602539062, Validation Accuracy: 49.91\n",
            "[50/150]: Training Loss: 1.5744633698950008, Training Accuracy: 55.76\n",
            "Validation Loss: 1.9295048713684082, Validation Accuracy: 50.3\n",
            "[51/150]: Training Loss: 1.5583884862004493, Training Accuracy: 56.348\n",
            "Validation Loss: 1.925916600227356, Validation Accuracy: 50.14\n",
            "[52/150]: Training Loss: 1.5439508910081825, Training Accuracy: 56.592\n",
            "Validation Loss: 1.8903591930866241, Validation Accuracy: 50.65\n",
            "[53/150]: Training Loss: 1.5187961057740815, Training Accuracy: 57.12\n",
            "Validation Loss: 1.893898105621338, Validation Accuracy: 51.28\n",
            "[54/150]: Training Loss: 1.500082329827912, Training Accuracy: 57.754\n",
            "Validation Loss: 1.8901280999183654, Validation Accuracy: 50.42\n",
            "[55/150]: Training Loss: 1.5054621915428006, Training Accuracy: 57.632\n",
            "Validation Loss: 1.890578955411911, Validation Accuracy: 51.07\n",
            "[56/150]: Training Loss: 1.4812841756003243, Training Accuracy: 57.986\n",
            "Validation Loss: 1.9068008363246918, Validation Accuracy: 51.44\n",
            "[57/150]: Training Loss: 1.4542514341218131, Training Accuracy: 58.654\n",
            "Validation Loss: 1.8665942788124084, Validation Accuracy: 51.26\n",
            "[58/150]: Training Loss: 1.4304890097404013, Training Accuracy: 59.206\n",
            "Validation Loss: 1.872557681798935, Validation Accuracy: 51.35\n",
            "[59/150]: Training Loss: 1.4232833567930727, Training Accuracy: 59.584\n",
            "Validation Loss: 1.9096780002117157, Validation Accuracy: 50.55\n",
            "[60/150]: Training Loss: 1.4309481491847915, Training Accuracy: 59.184\n",
            "Validation Loss: 1.8791188061237336, Validation Accuracy: 51.2\n",
            "[61/150]: Training Loss: 1.40531452334657, Training Accuracy: 59.96\n",
            "Validation Loss: 1.8862035810947417, Validation Accuracy: 51.85\n",
            "[62/150]: Training Loss: 1.3938273799662688, Training Accuracy: 60.274\n",
            "Validation Loss: 1.8729142725467682, Validation Accuracy: 51.37\n",
            "[63/150]: Training Loss: 1.3863425692733453, Training Accuracy: 60.462\n",
            "Validation Loss: 1.8725131154060364, Validation Accuracy: 52.01\n",
            "[64/150]: Training Loss: 1.366844469187211, Training Accuracy: 60.966\n",
            "Validation Loss: 1.8498412251472474, Validation Accuracy: 51.87\n",
            "[65/150]: Training Loss: 1.3531476259231567, Training Accuracy: 61.048\n",
            "Validation Loss: 1.8571744859218597, Validation Accuracy: 52.36\n",
            "[66/150]: Training Loss: 1.3398733954040372, Training Accuracy: 61.472\n",
            "Validation Loss: 1.869799542427063, Validation Accuracy: 52.02\n",
            "[67/150]: Training Loss: 1.3146201080205488, Training Accuracy: 62.184\n",
            "Validation Loss: 1.85557941198349, Validation Accuracy: 52.16\n",
            "[68/150]: Training Loss: 1.3349930096645743, Training Accuracy: 61.624\n",
            "Validation Loss: 1.899474561214447, Validation Accuracy: 51.8\n",
            "[69/150]: Training Loss: 1.2890492300597989, Training Accuracy: 63.07\n",
            "Validation Loss: 1.8615913569927216, Validation Accuracy: 52.55\n",
            "[70/150]: Training Loss: 1.281143541238746, Training Accuracy: 63.172\n",
            "Validation Loss: 1.8803191304206848, Validation Accuracy: 51.97\n",
            "[71/150]: Training Loss: 1.2725608847579177, Training Accuracy: 63.378\n",
            "Validation Loss: 1.859445983171463, Validation Accuracy: 52.42\n",
            "[72/150]: Training Loss: 1.2559378949963316, Training Accuracy: 63.878\n",
            "Validation Loss: 1.8568916201591492, Validation Accuracy: 52.05\n",
            "[73/150]: Training Loss: 1.236931935865052, Training Accuracy: 64.244\n",
            "Validation Loss: 1.8739505887031556, Validation Accuracy: 52.77\n",
            "[74/150]: Training Loss: 1.2173650824293798, Training Accuracy: 64.762\n",
            "Validation Loss: 1.8348273098468781, Validation Accuracy: 53.4\n",
            "[75/150]: Training Loss: 1.2067504488691991, Training Accuracy: 65.066\n",
            "Validation Loss: 1.8692607581615448, Validation Accuracy: 53.1\n",
            "[76/150]: Training Loss: 1.1903777292796545, Training Accuracy: 65.334\n",
            "Validation Loss: 1.8275393545627594, Validation Accuracy: 54.25\n",
            "[77/150]: Training Loss: 1.1792806843105628, Training Accuracy: 65.694\n",
            "Validation Loss: 1.8515967845916748, Validation Accuracy: 53.52\n",
            "[78/150]: Training Loss: 1.1699374263383904, Training Accuracy: 65.942\n",
            "Validation Loss: 1.8811356365680694, Validation Accuracy: 53.09\n",
            "[79/150]: Training Loss: 1.1490371531369734, Training Accuracy: 66.792\n",
            "Validation Loss: 1.8353333652019501, Validation Accuracy: 53.82\n",
            "[80/150]: Training Loss: 1.13668700383634, Training Accuracy: 66.562\n",
            "Validation Loss: 1.8432445168495177, Validation Accuracy: 53.57\n",
            "[81/150]: Training Loss: 1.1307378818794174, Training Accuracy: 67.21\n",
            "Validation Loss: 1.8481176435947417, Validation Accuracy: 53.14\n",
            "[82/150]: Training Loss: 1.113474543605532, Training Accuracy: 67.312\n",
            "Validation Loss: 1.8410939693450927, Validation Accuracy: 53.83\n",
            "[83/150]: Training Loss: 1.087531100122296, Training Accuracy: 68.23\n",
            "Validation Loss: 1.8605490565299987, Validation Accuracy: 54.45\n",
            "[84/150]: Training Loss: 1.0892518618885352, Training Accuracy: 68.016\n",
            "Validation Loss: 1.8244962751865388, Validation Accuracy: 54.28\n",
            "[85/150]: Training Loss: 1.0597951071602958, Training Accuracy: 68.916\n",
            "Validation Loss: 1.848558533191681, Validation Accuracy: 54.32\n",
            "[86/150]: Training Loss: 1.0610668713949165, Training Accuracy: 69.08\n",
            "Validation Loss: 1.834687203168869, Validation Accuracy: 54.36\n",
            "[87/150]: Training Loss: 1.029247879373784, Training Accuracy: 69.662\n",
            "Validation Loss: 1.8577094554901123, Validation Accuracy: 54.1\n",
            "[88/150]: Training Loss: 1.0305843438420976, Training Accuracy: 69.63\n",
            "Validation Loss: 1.88287433385849, Validation Accuracy: 53.74\n",
            "[89/150]: Training Loss: 1.0086117690923262, Training Accuracy: 70.346\n",
            "Validation Loss: 1.8663456857204437, Validation Accuracy: 53.54\n",
            "[90/150]: Training Loss: 0.9981355539390019, Training Accuracy: 70.802\n",
            "Validation Loss: 1.8623527228832244, Validation Accuracy: 53.93\n",
            "[91/150]: Training Loss: 0.986907957159743, Training Accuracy: 70.956\n",
            "Validation Loss: 1.8775681614875794, Validation Accuracy: 54.56\n",
            "[92/150]: Training Loss: 0.9677880217834395, Training Accuracy: 71.402\n",
            "Validation Loss: 1.886433583498001, Validation Accuracy: 54.06\n",
            "[93/150]: Training Loss: 0.9601449692735866, Training Accuracy: 71.714\n",
            "Validation Loss: 1.848057508468628, Validation Accuracy: 55.12\n",
            "[94/150]: Training Loss: 0.9483164567120221, Training Accuracy: 71.99\n",
            "Validation Loss: 1.8797329545021058, Validation Accuracy: 54.52\n",
            "[95/150]: Training Loss: 0.9349474110165421, Training Accuracy: 72.398\n",
            "Validation Loss: 1.8802269518375396, Validation Accuracy: 54.13\n",
            "[96/150]: Training Loss: 0.9153923319310558, Training Accuracy: 72.582\n",
            "Validation Loss: 1.8897387385368347, Validation Accuracy: 54.28\n",
            "[97/150]: Training Loss: 0.9160392132340646, Training Accuracy: 72.794\n",
            "Validation Loss: 1.9283715963363648, Validation Accuracy: 53.5\n",
            "[98/150]: Training Loss: 0.9007182382807439, Training Accuracy: 73.294\n",
            "Validation Loss: 1.8609421133995057, Validation Accuracy: 54.96\n",
            "[99/150]: Training Loss: 0.8783578246223683, Training Accuracy: 73.912\n",
            "Validation Loss: 1.8637104988098145, Validation Accuracy: 54.56\n",
            "[100/150]: Training Loss: 0.8765367439814976, Training Accuracy: 74.022\n",
            "Validation Loss: 1.8750475943088531, Validation Accuracy: 54.59\n",
            "[101/150]: Training Loss: 0.8546222393610039, Training Accuracy: 74.918\n",
            "Validation Loss: 1.8807278335094453, Validation Accuracy: 55.13\n",
            "[102/150]: Training Loss: 0.8389398102857628, Training Accuracy: 74.972\n",
            "Validation Loss: 1.8944664776325226, Validation Accuracy: 54.78\n",
            "[103/150]: Training Loss: 0.8364474201688961, Training Accuracy: 75.32\n",
            "Validation Loss: 1.8999429523944855, Validation Accuracy: 54.68\n",
            "[104/150]: Training Loss: 0.8233104719191181, Training Accuracy: 75.606\n",
            "Validation Loss: 1.9150757372379303, Validation Accuracy: 54.49\n",
            "[105/150]: Training Loss: 0.8071710479502775, Training Accuracy: 76.094\n",
            "Validation Loss: 1.8923523843288421, Validation Accuracy: 55.54\n",
            "[106/150]: Training Loss: 0.7909371567015745, Training Accuracy: 76.448\n",
            "Validation Loss: 1.883741980791092, Validation Accuracy: 54.92\n",
            "[107/150]: Training Loss: 0.7867257595062256, Training Accuracy: 76.748\n",
            "Validation Loss: 1.8950099110603333, Validation Accuracy: 55.47\n",
            "[108/150]: Training Loss: 0.7650761269793218, Training Accuracy: 77.31\n",
            "Validation Loss: 1.9024061024188996, Validation Accuracy: 54.91\n",
            "[109/150]: Training Loss: 0.7652728691393015, Training Accuracy: 77.178\n",
            "Validation Loss: 1.9312968671321868, Validation Accuracy: 54.57\n",
            "[110/150]: Training Loss: 0.7572217492424712, Training Accuracy: 77.548\n",
            "Validation Loss: 1.8906243860721588, Validation Accuracy: 55.71\n",
            "[111/150]: Training Loss: 0.738684381149253, Training Accuracy: 77.932\n",
            "Validation Loss: 1.945603609085083, Validation Accuracy: 55.18\n",
            "[112/150]: Training Loss: 0.7319968044757843, Training Accuracy: 78.382\n",
            "Validation Loss: 1.9286673545837403, Validation Accuracy: 55.27\n",
            "[113/150]: Training Loss: 0.7233879006638819, Training Accuracy: 78.482\n",
            "Validation Loss: 1.9232488691806793, Validation Accuracy: 55.58\n",
            "[114/150]: Training Loss: 0.7083694521261721, Training Accuracy: 78.814\n",
            "Validation Loss: 1.907062864303589, Validation Accuracy: 55.51\n",
            "[115/150]: Training Loss: 0.7031112666032753, Training Accuracy: 79.204\n",
            "Validation Loss: 1.9117585182189942, Validation Accuracy: 56.02\n",
            "[116/150]: Training Loss: 0.6852655745282465, Training Accuracy: 79.458\n",
            "Validation Loss: 1.9387612223625184, Validation Accuracy: 55.18\n",
            "[117/150]: Training Loss: 0.6826438423322172, Training Accuracy: 79.792\n",
            "Validation Loss: 1.923887985944748, Validation Accuracy: 55.49\n",
            "[118/150]: Training Loss: 0.6681522337757811, Training Accuracy: 80.242\n",
            "Validation Loss: 1.940861666202545, Validation Accuracy: 55.59\n",
            "[119/150]: Training Loss: 0.6691557758924912, Training Accuracy: 80.146\n",
            "Validation Loss: 1.953080016374588, Validation Accuracy: 55.29\n",
            "[120/150]: Training Loss: 0.6531465272514188, Training Accuracy: 80.714\n",
            "Validation Loss: 1.9404853343963624, Validation Accuracy: 55.54\n",
            "[121/150]: Training Loss: 0.6419856098233437, Training Accuracy: 81.21\n",
            "Validation Loss: 1.9526274442672729, Validation Accuracy: 56.17\n",
            "[122/150]: Training Loss: 0.6383003540793244, Training Accuracy: 81.27\n",
            "Validation Loss: 1.9685742020606996, Validation Accuracy: 55.67\n",
            "[123/150]: Training Loss: 0.6263646182357049, Training Accuracy: 81.396\n",
            "Validation Loss: 1.9449464201927185, Validation Accuracy: 56.05\n",
            "[124/150]: Training Loss: 0.62737323982375, Training Accuracy: 81.438\n",
            "Validation Loss: 1.9453241765499114, Validation Accuracy: 56.28\n",
            "[125/150]: Training Loss: 0.6171576502371807, Training Accuracy: 81.866\n",
            "Validation Loss: 1.9553956925868987, Validation Accuracy: 55.63\n",
            "[126/150]: Training Loss: 0.6041063827519514, Training Accuracy: 82.16\n",
            "Validation Loss: 1.9582968175411224, Validation Accuracy: 56.15\n",
            "[127/150]: Training Loss: 0.6006453934372687, Training Accuracy: 82.38\n",
            "Validation Loss: 1.9529106080532075, Validation Accuracy: 56.03\n",
            "[128/150]: Training Loss: 0.6008156434613832, Training Accuracy: 82.348\n",
            "Validation Loss: 1.9448323190212249, Validation Accuracy: 56.3\n",
            "[129/150]: Training Loss: 0.5887871582289131, Training Accuracy: 82.762\n",
            "Validation Loss: 1.9513534665107728, Validation Accuracy: 56.25\n",
            "[130/150]: Training Loss: 0.5888028677020755, Training Accuracy: 82.86\n",
            "Validation Loss: 1.9529175937175751, Validation Accuracy: 56.18\n",
            "[131/150]: Training Loss: 0.5761450562550097, Training Accuracy: 83.106\n",
            "Validation Loss: 1.9645096361637115, Validation Accuracy: 56.02\n",
            "[132/150]: Training Loss: 0.5727416809116092, Training Accuracy: 83.094\n",
            "Validation Loss: 1.963749361038208, Validation Accuracy: 56.38\n",
            "[133/150]: Training Loss: 0.5652356436666177, Training Accuracy: 83.672\n",
            "Validation Loss: 1.9716902256011963, Validation Accuracy: 56.28\n",
            "[134/150]: Training Loss: 0.5695076165150623, Training Accuracy: 83.448\n",
            "Validation Loss: 1.964329320192337, Validation Accuracy: 55.99\n",
            "[135/150]: Training Loss: 0.5608592775403237, Training Accuracy: 83.678\n",
            "Validation Loss: 1.9603359639644622, Validation Accuracy: 56.25\n",
            "[136/150]: Training Loss: 0.5542723040799705, Training Accuracy: 83.854\n",
            "Validation Loss: 1.9673468470573425, Validation Accuracy: 56.04\n",
            "[137/150]: Training Loss: 0.5495593383604166, Training Accuracy: 84.128\n",
            "Validation Loss: 1.9731853008270264, Validation Accuracy: 56.19\n",
            "[138/150]: Training Loss: 0.5444910410715609, Training Accuracy: 84.204\n",
            "Validation Loss: 1.9700454473495483, Validation Accuracy: 56.12\n",
            "[139/150]: Training Loss: 0.5433347140039716, Training Accuracy: 84.306\n",
            "Validation Loss: 1.9688788115978242, Validation Accuracy: 56.13\n",
            "[140/150]: Training Loss: 0.5362506448006144, Training Accuracy: 84.586\n",
            "Validation Loss: 1.966701751947403, Validation Accuracy: 56.24\n",
            "[141/150]: Training Loss: 0.5390761190531205, Training Accuracy: 84.338\n",
            "Validation Loss: 1.9651995241641997, Validation Accuracy: 56.21\n",
            "[142/150]: Training Loss: 0.5341207445884237, Training Accuracy: 84.41\n",
            "Validation Loss: 1.9675312757492065, Validation Accuracy: 56.34\n",
            "[143/150]: Training Loss: 0.5282508870776819, Training Accuracy: 84.872\n",
            "Validation Loss: 1.967752468585968, Validation Accuracy: 56.25\n",
            "[144/150]: Training Loss: 0.5355107656547001, Training Accuracy: 84.532\n",
            "Validation Loss: 1.967381328344345, Validation Accuracy: 56.19\n",
            "[145/150]: Training Loss: 0.5333734960580359, Training Accuracy: 84.51\n",
            "Validation Loss: 1.969217723608017, Validation Accuracy: 56.19\n",
            "[146/150]: Training Loss: 0.5298109002867524, Training Accuracy: 84.75\n",
            "Validation Loss: 1.9694741308689117, Validation Accuracy: 56.12\n",
            "[147/150]: Training Loss: 0.529324583253082, Training Accuracy: 84.722\n",
            "Validation Loss: 1.9707287430763245, Validation Accuracy: 56.17\n",
            "[148/150]: Training Loss: 0.5251490242627203, Training Accuracy: 84.936\n",
            "Validation Loss: 1.9699740409851074, Validation Accuracy: 56.26\n",
            "[149/150]: Training Loss: 0.5242341507454308, Training Accuracy: 84.942\n",
            "Validation Loss: 1.97024707198143, Validation Accuracy: 56.16\n",
            "[150/150]: Training Loss: 0.5341996495821038, Training Accuracy: 84.57\n",
            "Validation Loss: 1.9700913786888123, Validation Accuracy: 56.18\n",
            "**********************************************************************\n",
            "Test Loss: 1.9700913786888123, Test Accuracy: 56.18\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>56.18</td></tr><tr><td>Test Loss</td><td>1.97009</td></tr><tr><td>Train Accuracy</td><td>84.57</td></tr><tr><td>Train Loss</td><td>0.5342</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=512 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/momu4e5u</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_012749-momu4e5u/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 1024, Learning rate: 6.0, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_014707-w80ujlhs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs' target=\"_blank\">batch_size=1024 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.271560571631607, Training Accuracy: 4.982\n",
            "Validation Loss: 3.9391962051391602, Validation Accuracy: 8.84\n",
            "[2/150]: Training Loss: 3.82728639914065, Training Accuracy: 10.948\n",
            "Validation Loss: 3.6827336311340333, Validation Accuracy: 13.37\n",
            "[3/150]: Training Loss: 3.6269987845907408, Training Accuracy: 14.39\n",
            "Validation Loss: 3.4175909042358397, Validation Accuracy: 18.27\n",
            "[4/150]: Training Loss: 3.4358463238696664, Training Accuracy: 17.696\n",
            "Validation Loss: 3.2815504550933836, Validation Accuracy: 20.38\n",
            "[5/150]: Training Loss: 3.3039202203555984, Training Accuracy: 19.798\n",
            "Validation Loss: 3.171510195732117, Validation Accuracy: 23.09\n",
            "[6/150]: Training Loss: 3.1788000671231016, Training Accuracy: 22.106\n",
            "Validation Loss: 3.083156633377075, Validation Accuracy: 24.25\n",
            "[7/150]: Training Loss: 3.0781905553778826, Training Accuracy: 24.04\n",
            "Validation Loss: 2.9470993995666506, Validation Accuracy: 26.99\n",
            "[8/150]: Training Loss: 2.9905799846259917, Training Accuracy: 25.462\n",
            "Validation Loss: 2.8303237915039063, Validation Accuracy: 29.04\n",
            "[9/150]: Training Loss: 2.8789747199233697, Training Accuracy: 28.024\n",
            "Validation Loss: 2.7251497507095337, Validation Accuracy: 31.63\n",
            "[10/150]: Training Loss: 2.752841701312941, Training Accuracy: 30.378\n",
            "Validation Loss: 2.6787135124206545, Validation Accuracy: 32.86\n",
            "[11/150]: Training Loss: 2.664300650966411, Training Accuracy: 32.018\n",
            "Validation Loss: 2.5810091733932494, Validation Accuracy: 34.34\n",
            "[12/150]: Training Loss: 2.6055227445096385, Training Accuracy: 33.34\n",
            "Validation Loss: 2.511804437637329, Validation Accuracy: 35.24\n",
            "[13/150]: Training Loss: 2.569062106463374, Training Accuracy: 34.12\n",
            "Validation Loss: 2.452015995979309, Validation Accuracy: 36.63\n",
            "[14/150]: Training Loss: 2.4965496793085213, Training Accuracy: 35.586\n",
            "Validation Loss: 2.353745174407959, Validation Accuracy: 38.35\n",
            "[15/150]: Training Loss: 2.4792360870205625, Training Accuracy: 35.788\n",
            "Validation Loss: 2.362164545059204, Validation Accuracy: 38.51\n",
            "[16/150]: Training Loss: 2.406777240792099, Training Accuracy: 37.498\n",
            "Validation Loss: 2.322283720970154, Validation Accuracy: 39.52\n",
            "[17/150]: Training Loss: 2.3524391651153564, Training Accuracy: 38.372\n",
            "Validation Loss: 2.2932183027267454, Validation Accuracy: 40.0\n",
            "[18/150]: Training Loss: 2.3162564647441006, Training Accuracy: 39.304\n",
            "Validation Loss: 2.390507221221924, Validation Accuracy: 38.54\n",
            "[19/150]: Training Loss: 2.3010421821049283, Training Accuracy: 39.48\n",
            "Validation Loss: 2.2039053201675416, Validation Accuracy: 42.83\n",
            "[20/150]: Training Loss: 2.2905601968570632, Training Accuracy: 40.014\n",
            "Validation Loss: 2.327000045776367, Validation Accuracy: 39.01\n",
            "[21/150]: Training Loss: 2.2232315151058897, Training Accuracy: 41.472\n",
            "Validation Loss: 2.1848330736160277, Validation Accuracy: 42.37\n",
            "[22/150]: Training Loss: 2.175964046497734, Training Accuracy: 42.316\n",
            "Validation Loss: 2.192854475975037, Validation Accuracy: 42.34\n",
            "[23/150]: Training Loss: 2.12663603315548, Training Accuracy: 43.39\n",
            "Validation Loss: 2.184653973579407, Validation Accuracy: 42.91\n",
            "[24/150]: Training Loss: 2.1151150294712613, Training Accuracy: 43.664\n",
            "Validation Loss: 2.1352186679840086, Validation Accuracy: 43.8\n",
            "[25/150]: Training Loss: 2.0990794507824644, Training Accuracy: 44.256\n",
            "Validation Loss: 2.088001215457916, Validation Accuracy: 44.67\n",
            "[26/150]: Training Loss: 2.0608093349301084, Training Accuracy: 44.852\n",
            "Validation Loss: 2.1832612276077272, Validation Accuracy: 42.92\n",
            "[27/150]: Training Loss: 2.0558675892499028, Training Accuracy: 45.23\n",
            "Validation Loss: 2.176005721092224, Validation Accuracy: 43.08\n",
            "[28/150]: Training Loss: 2.0222370770512796, Training Accuracy: 45.868\n",
            "Validation Loss: 2.1084380388259887, Validation Accuracy: 44.53\n",
            "[29/150]: Training Loss: 1.9681626996215509, Training Accuracy: 47.006\n",
            "Validation Loss: 2.07066353559494, Validation Accuracy: 45.38\n",
            "[30/150]: Training Loss: 1.9436734793137531, Training Accuracy: 47.268\n",
            "Validation Loss: 2.0732940912246702, Validation Accuracy: 45.56\n",
            "[31/150]: Training Loss: 1.9339864083698817, Training Accuracy: 48.042\n",
            "Validation Loss: 2.095534420013428, Validation Accuracy: 45.23\n",
            "[32/150]: Training Loss: 1.9033171717001467, Training Accuracy: 48.32\n",
            "Validation Loss: 2.111021065711975, Validation Accuracy: 45.35\n",
            "[33/150]: Training Loss: 1.904011852887212, Training Accuracy: 48.414\n",
            "Validation Loss: 2.050978398323059, Validation Accuracy: 46.52\n",
            "[34/150]: Training Loss: 1.8750450124545974, Training Accuracy: 48.976\n",
            "Validation Loss: 2.080773401260376, Validation Accuracy: 45.2\n",
            "[35/150]: Training Loss: 1.8805813740710824, Training Accuracy: 48.968\n",
            "Validation Loss: 2.036470913887024, Validation Accuracy: 46.34\n",
            "[36/150]: Training Loss: 1.851276424466347, Training Accuracy: 49.646\n",
            "Validation Loss: 2.1005828619003295, Validation Accuracy: 45.2\n",
            "[37/150]: Training Loss: 1.8493371909978438, Training Accuracy: 49.736\n",
            "Validation Loss: 2.003628730773926, Validation Accuracy: 46.62\n",
            "[38/150]: Training Loss: 1.7895233460835047, Training Accuracy: 50.876\n",
            "Validation Loss: 1.9916603326797486, Validation Accuracy: 47.91\n",
            "[39/150]: Training Loss: 1.783764887829216, Training Accuracy: 51.294\n",
            "Validation Loss: 2.066832160949707, Validation Accuracy: 45.61\n",
            "[40/150]: Training Loss: 1.7760933297021049, Training Accuracy: 51.504\n",
            "Validation Loss: 1.9901643991470337, Validation Accuracy: 47.5\n",
            "[41/150]: Training Loss: 1.7614454274274864, Training Accuracy: 51.798\n",
            "Validation Loss: 2.0182290196418764, Validation Accuracy: 47.1\n",
            "[42/150]: Training Loss: 1.7892101711156416, Training Accuracy: 51.024\n",
            "Validation Loss: 1.9716030478477478, Validation Accuracy: 48.76\n",
            "[43/150]: Training Loss: 1.7074257597631337, Training Accuracy: 52.916\n",
            "Validation Loss: 1.9718806385993957, Validation Accuracy: 48.61\n",
            "[44/150]: Training Loss: 1.714418238523055, Training Accuracy: 52.714\n",
            "Validation Loss: 2.0062466263771057, Validation Accuracy: 48.27\n",
            "[45/150]: Training Loss: 1.6869578483153362, Training Accuracy: 53.334\n",
            "Validation Loss: 1.95201518535614, Validation Accuracy: 49.06\n",
            "[46/150]: Training Loss: 1.6628945092765652, Training Accuracy: 53.922\n",
            "Validation Loss: 2.012867844104767, Validation Accuracy: 47.15\n",
            "[47/150]: Training Loss: 1.6412470778640436, Training Accuracy: 54.254\n",
            "Validation Loss: 1.923640739917755, Validation Accuracy: 49.27\n",
            "[48/150]: Training Loss: 1.6261077705694704, Training Accuracy: 55.032\n",
            "Validation Loss: 1.9601864099502564, Validation Accuracy: 48.98\n",
            "[49/150]: Training Loss: 1.6493362480280351, Training Accuracy: 54.152\n",
            "Validation Loss: 1.9593440890312195, Validation Accuracy: 48.65\n",
            "[50/150]: Training Loss: 1.612999371119908, Training Accuracy: 55.134\n",
            "Validation Loss: 1.9344399094581604, Validation Accuracy: 49.38\n",
            "[51/150]: Training Loss: 1.5939155281806479, Training Accuracy: 55.666\n",
            "Validation Loss: 1.9557547926902772, Validation Accuracy: 48.7\n",
            "[52/150]: Training Loss: 1.6124721181635955, Training Accuracy: 55.024\n",
            "Validation Loss: 1.9627613067626952, Validation Accuracy: 48.75\n",
            "[53/150]: Training Loss: 1.5704505808499394, Training Accuracy: 56.09\n",
            "Validation Loss: 1.9339674592018128, Validation Accuracy: 49.77\n",
            "[54/150]: Training Loss: 1.556705151285444, Training Accuracy: 56.304\n",
            "Validation Loss: 1.910308563709259, Validation Accuracy: 50.74\n",
            "[55/150]: Training Loss: 1.5271518887305746, Training Accuracy: 57.15\n",
            "Validation Loss: 1.8914035081863403, Validation Accuracy: 50.77\n",
            "[56/150]: Training Loss: 1.5157563102488616, Training Accuracy: 57.53\n",
            "Validation Loss: 1.8953699111938476, Validation Accuracy: 50.82\n",
            "[57/150]: Training Loss: 1.4868425179500968, Training Accuracy: 58.284\n",
            "Validation Loss: 1.960522997379303, Validation Accuracy: 49.2\n",
            "[58/150]: Training Loss: 1.5009924586938352, Training Accuracy: 57.544\n",
            "Validation Loss: 1.9257111549377441, Validation Accuracy: 50.88\n",
            "[59/150]: Training Loss: 1.4776659741693614, Training Accuracy: 58.214\n",
            "Validation Loss: 1.9358840227127074, Validation Accuracy: 50.04\n",
            "[60/150]: Training Loss: 1.4657867806298392, Training Accuracy: 58.49\n",
            "Validation Loss: 2.0199026823043824, Validation Accuracy: 48.59\n",
            "[61/150]: Training Loss: 1.4666189709488227, Training Accuracy: 58.868\n",
            "Validation Loss: 1.9441120982170106, Validation Accuracy: 49.79\n",
            "[62/150]: Training Loss: 1.4370595435706937, Training Accuracy: 59.142\n",
            "Validation Loss: 1.8950949430465698, Validation Accuracy: 51.17\n",
            "[63/150]: Training Loss: 1.4119965835493438, Training Accuracy: 59.966\n",
            "Validation Loss: 1.946419107913971, Validation Accuracy: 49.96\n",
            "[64/150]: Training Loss: 1.3992952692265412, Training Accuracy: 60.37\n",
            "Validation Loss: 1.929420042037964, Validation Accuracy: 50.74\n",
            "[65/150]: Training Loss: 1.3843509275086072, Training Accuracy: 60.93\n",
            "Validation Loss: 1.8808708190917969, Validation Accuracy: 51.52\n",
            "[66/150]: Training Loss: 1.3892741276293386, Training Accuracy: 60.538\n",
            "Validation Loss: 1.9178170323371888, Validation Accuracy: 51.05\n",
            "[67/150]: Training Loss: 1.3926233807388617, Training Accuracy: 60.324\n",
            "Validation Loss: 1.879079854488373, Validation Accuracy: 52.14\n",
            "[68/150]: Training Loss: 1.3284603332986638, Training Accuracy: 62.108\n",
            "Validation Loss: 1.8741150736808776, Validation Accuracy: 51.87\n",
            "[69/150]: Training Loss: 1.3297611377677139, Training Accuracy: 62.144\n",
            "Validation Loss: 1.8850895166397095, Validation Accuracy: 51.78\n",
            "[70/150]: Training Loss: 1.3190836857776254, Training Accuracy: 62.086\n",
            "Validation Loss: 1.8964969992637635, Validation Accuracy: 51.72\n",
            "[71/150]: Training Loss: 1.297834805079869, Training Accuracy: 63.002\n",
            "Validation Loss: 1.8922056078910827, Validation Accuracy: 51.94\n",
            "[72/150]: Training Loss: 1.3022558105235198, Training Accuracy: 62.866\n",
            "Validation Loss: 1.8891796469688416, Validation Accuracy: 52.07\n",
            "[73/150]: Training Loss: 1.2681196271156778, Training Accuracy: 63.622\n",
            "Validation Loss: 1.9051270723342895, Validation Accuracy: 52.41\n",
            "[74/150]: Training Loss: 1.2562547800492267, Training Accuracy: 64.092\n",
            "Validation Loss: 1.8912514567375183, Validation Accuracy: 52.28\n",
            "[75/150]: Training Loss: 1.2469108761573324, Training Accuracy: 64.088\n",
            "Validation Loss: 1.9140023827552795, Validation Accuracy: 52.11\n",
            "[76/150]: Training Loss: 1.2424099080416622, Training Accuracy: 64.206\n",
            "Validation Loss: 1.896690595149994, Validation Accuracy: 52.53\n",
            "[77/150]: Training Loss: 1.2172678052162638, Training Accuracy: 64.804\n",
            "Validation Loss: 1.8944836139678956, Validation Accuracy: 52.54\n",
            "[78/150]: Training Loss: 1.198543380717842, Training Accuracy: 65.358\n",
            "Validation Loss: 1.8942208647727967, Validation Accuracy: 52.32\n",
            "[79/150]: Training Loss: 1.198562699921277, Training Accuracy: 65.602\n",
            "Validation Loss: 1.8823209404945374, Validation Accuracy: 52.42\n",
            "[80/150]: Training Loss: 1.1809428224758225, Training Accuracy: 66.04\n",
            "Validation Loss: 1.8809239506721496, Validation Accuracy: 52.57\n",
            "[81/150]: Training Loss: 1.1573098119424314, Training Accuracy: 66.476\n",
            "Validation Loss: 1.904803991317749, Validation Accuracy: 52.64\n",
            "[82/150]: Training Loss: 1.1748385405053898, Training Accuracy: 65.876\n",
            "Validation Loss: 1.8975732803344727, Validation Accuracy: 52.68\n",
            "[83/150]: Training Loss: 1.1381618465696062, Training Accuracy: 67.06\n",
            "Validation Loss: 1.87950758934021, Validation Accuracy: 53.8\n",
            "[84/150]: Training Loss: 1.1169312851769584, Training Accuracy: 67.614\n",
            "Validation Loss: 1.9594017744064331, Validation Accuracy: 51.52\n",
            "[85/150]: Training Loss: 1.1326112601221825, Training Accuracy: 67.212\n",
            "Validation Loss: 1.9392709732055664, Validation Accuracy: 52.51\n",
            "[86/150]: Training Loss: 1.1153452299079116, Training Accuracy: 67.558\n",
            "Validation Loss: 1.8824020862579345, Validation Accuracy: 53.05\n",
            "[87/150]: Training Loss: 1.075961629955136, Training Accuracy: 68.556\n",
            "Validation Loss: 1.8995132923126221, Validation Accuracy: 53.22\n",
            "[88/150]: Training Loss: 1.0561599804430593, Training Accuracy: 69.222\n",
            "Validation Loss: 1.8934579133987426, Validation Accuracy: 53.25\n",
            "[89/150]: Training Loss: 1.0379744488365796, Training Accuracy: 69.548\n",
            "Validation Loss: 1.8907739281654359, Validation Accuracy: 54.05\n",
            "[90/150]: Training Loss: 1.0645462481343015, Training Accuracy: 69.002\n",
            "Validation Loss: 1.9286641478538513, Validation Accuracy: 52.38\n",
            "[91/150]: Training Loss: 1.0510404596523362, Training Accuracy: 69.392\n",
            "Validation Loss: 1.897424077987671, Validation Accuracy: 53.61\n",
            "[92/150]: Training Loss: 1.017185703832276, Training Accuracy: 70.506\n",
            "Validation Loss: 1.9066467523574828, Validation Accuracy: 53.56\n",
            "[93/150]: Training Loss: 0.9912244945156331, Training Accuracy: 70.89\n",
            "Validation Loss: 1.9277787804603577, Validation Accuracy: 53.53\n",
            "[94/150]: Training Loss: 1.0054571652898983, Training Accuracy: 70.53\n",
            "Validation Loss: 1.8809196829795838, Validation Accuracy: 54.1\n",
            "[95/150]: Training Loss: 0.9765667416611497, Training Accuracy: 71.32\n",
            "Validation Loss: 1.8944197297096252, Validation Accuracy: 53.48\n",
            "[96/150]: Training Loss: 0.9578249174721387, Training Accuracy: 71.926\n",
            "Validation Loss: 1.942376160621643, Validation Accuracy: 53.53\n",
            "[97/150]: Training Loss: 0.9575933753227701, Training Accuracy: 71.676\n",
            "Validation Loss: 1.8917027354240417, Validation Accuracy: 53.65\n",
            "[98/150]: Training Loss: 0.944321874453097, Training Accuracy: 72.258\n",
            "Validation Loss: 1.8984474778175353, Validation Accuracy: 54.22\n",
            "[99/150]: Training Loss: 0.9205025398001379, Training Accuracy: 72.942\n",
            "Validation Loss: 1.9325331091880797, Validation Accuracy: 54.27\n",
            "[100/150]: Training Loss: 0.9185542439927861, Training Accuracy: 72.924\n",
            "Validation Loss: 1.9163445711135865, Validation Accuracy: 54.26\n",
            "[101/150]: Training Loss: 0.8990846799344433, Training Accuracy: 73.314\n",
            "Validation Loss: 1.9365061402320862, Validation Accuracy: 54.02\n",
            "[102/150]: Training Loss: 0.8797871519108208, Training Accuracy: 74.004\n",
            "Validation Loss: 1.937075173854828, Validation Accuracy: 54.45\n",
            "[103/150]: Training Loss: 0.8798652291297913, Training Accuracy: 74.044\n",
            "Validation Loss: 1.9165674328804017, Validation Accuracy: 54.24\n",
            "[104/150]: Training Loss: 0.8599669252123151, Training Accuracy: 74.54\n",
            "Validation Loss: 1.9492801547050476, Validation Accuracy: 53.91\n",
            "[105/150]: Training Loss: 0.8473539729507602, Training Accuracy: 74.856\n",
            "Validation Loss: 1.9327858686447144, Validation Accuracy: 53.8\n",
            "[106/150]: Training Loss: 0.8454914895855651, Training Accuracy: 74.948\n",
            "Validation Loss: 1.918694531917572, Validation Accuracy: 54.58\n",
            "[107/150]: Training Loss: 0.8351617601453042, Training Accuracy: 75.38\n",
            "Validation Loss: 1.925265645980835, Validation Accuracy: 54.65\n",
            "[108/150]: Training Loss: 0.8257343720416633, Training Accuracy: 75.846\n",
            "Validation Loss: 1.9380712270736695, Validation Accuracy: 54.61\n",
            "[109/150]: Training Loss: 0.803754199524315, Training Accuracy: 76.216\n",
            "Validation Loss: 1.952760434150696, Validation Accuracy: 54.15\n",
            "[110/150]: Training Loss: 0.8017950021490758, Training Accuracy: 76.334\n",
            "Validation Loss: 1.938999843597412, Validation Accuracy: 54.87\n",
            "[111/150]: Training Loss: 0.7898118264821111, Training Accuracy: 76.776\n",
            "Validation Loss: 1.9549705505371093, Validation Accuracy: 54.67\n",
            "[112/150]: Training Loss: 0.7774463660862981, Training Accuracy: 76.952\n",
            "Validation Loss: 1.9725535273551942, Validation Accuracy: 54.17\n",
            "[113/150]: Training Loss: 0.7767588861134588, Training Accuracy: 76.824\n",
            "Validation Loss: 1.9652800679206848, Validation Accuracy: 54.49\n",
            "[114/150]: Training Loss: 0.7543022997525274, Training Accuracy: 77.61\n",
            "Validation Loss: 1.9479739904403686, Validation Accuracy: 54.85\n",
            "[115/150]: Training Loss: 0.7407014296979321, Training Accuracy: 78.064\n",
            "Validation Loss: 1.974105668067932, Validation Accuracy: 54.73\n",
            "[116/150]: Training Loss: 0.7379214763641357, Training Accuracy: 78.122\n",
            "Validation Loss: 1.9709696292877197, Validation Accuracy: 54.92\n",
            "[117/150]: Training Loss: 0.7280089295640284, Training Accuracy: 78.332\n",
            "Validation Loss: 1.9550812244415283, Validation Accuracy: 54.87\n",
            "[118/150]: Training Loss: 0.719780373330019, Training Accuracy: 78.916\n",
            "Validation Loss: 1.9715718150138855, Validation Accuracy: 54.88\n",
            "[119/150]: Training Loss: 0.7140980338563725, Training Accuracy: 78.938\n",
            "Validation Loss: 1.9744232773780823, Validation Accuracy: 54.88\n",
            "[120/150]: Training Loss: 0.7100381559255172, Training Accuracy: 79.112\n",
            "Validation Loss: 1.9600295305252076, Validation Accuracy: 55.01\n",
            "[121/150]: Training Loss: 0.695380872609664, Training Accuracy: 79.644\n",
            "Validation Loss: 1.9685936331748963, Validation Accuracy: 55.25\n",
            "[122/150]: Training Loss: 0.693044870483632, Training Accuracy: 79.566\n",
            "Validation Loss: 1.973974621295929, Validation Accuracy: 54.99\n",
            "[123/150]: Training Loss: 0.685412128360904, Training Accuracy: 79.762\n",
            "Validation Loss: 1.981511080265045, Validation Accuracy: 55.03\n",
            "[124/150]: Training Loss: 0.6729757055944326, Training Accuracy: 80.388\n",
            "Validation Loss: 1.9723920106887818, Validation Accuracy: 54.88\n",
            "[125/150]: Training Loss: 0.6728029567368177, Training Accuracy: 80.238\n",
            "Validation Loss: 1.972815704345703, Validation Accuracy: 55.06\n",
            "[126/150]: Training Loss: 0.6577607624384821, Training Accuracy: 80.906\n",
            "Validation Loss: 1.9941662311553956, Validation Accuracy: 55.01\n",
            "[127/150]: Training Loss: 0.6531734880135984, Training Accuracy: 80.798\n",
            "Validation Loss: 1.9830293536186219, Validation Accuracy: 55.29\n",
            "[128/150]: Training Loss: 0.6451807484334829, Training Accuracy: 80.998\n",
            "Validation Loss: 1.98456552028656, Validation Accuracy: 55.25\n",
            "[129/150]: Training Loss: 0.6361871045462939, Training Accuracy: 81.438\n",
            "Validation Loss: 1.991374099254608, Validation Accuracy: 55.33\n",
            "[130/150]: Training Loss: 0.6329771852006718, Training Accuracy: 81.326\n",
            "Validation Loss: 1.982979428768158, Validation Accuracy: 55.15\n",
            "[131/150]: Training Loss: 0.6337368731596031, Training Accuracy: 81.498\n",
            "Validation Loss: 1.9814332127571106, Validation Accuracy: 55.25\n",
            "[132/150]: Training Loss: 0.6237865613431347, Training Accuracy: 81.828\n",
            "Validation Loss: 1.9956311464309693, Validation Accuracy: 55.37\n",
            "[133/150]: Training Loss: 0.6184223087466493, Training Accuracy: 81.804\n",
            "Validation Loss: 1.997809612751007, Validation Accuracy: 55.31\n",
            "[134/150]: Training Loss: 0.6162658759525844, Training Accuracy: 82.05\n",
            "Validation Loss: 1.9944164514541627, Validation Accuracy: 55.37\n",
            "[135/150]: Training Loss: 0.6049274242654139, Training Accuracy: 82.308\n",
            "Validation Loss: 2.0007421493530275, Validation Accuracy: 55.18\n",
            "[136/150]: Training Loss: 0.6065428415123297, Training Accuracy: 82.346\n",
            "Validation Loss: 2.002026319503784, Validation Accuracy: 55.35\n",
            "[137/150]: Training Loss: 0.6054561515243686, Training Accuracy: 82.446\n",
            "Validation Loss: 2.0026141166687013, Validation Accuracy: 55.51\n",
            "[138/150]: Training Loss: 0.6016628170499996, Training Accuracy: 82.674\n",
            "Validation Loss: 1.9972286820411682, Validation Accuracy: 55.54\n",
            "[139/150]: Training Loss: 0.599141927397981, Training Accuracy: 82.822\n",
            "Validation Loss: 1.9984585762023925, Validation Accuracy: 55.21\n",
            "[140/150]: Training Loss: 0.5853999877462581, Training Accuracy: 82.924\n",
            "Validation Loss: 2.0003657698631288, Validation Accuracy: 55.39\n",
            "[141/150]: Training Loss: 0.5930902678139356, Training Accuracy: 82.696\n",
            "Validation Loss: 1.9994045495986938, Validation Accuracy: 55.54\n",
            "[142/150]: Training Loss: 0.5897825044028613, Training Accuracy: 83.098\n",
            "Validation Loss: 2.0004444122314453, Validation Accuracy: 55.4\n",
            "[143/150]: Training Loss: 0.5907509253949536, Training Accuracy: 82.836\n",
            "Validation Loss: 1.9981922507286072, Validation Accuracy: 55.3\n",
            "[144/150]: Training Loss: 0.5838782507546094, Training Accuracy: 83.046\n",
            "Validation Loss: 1.9986275434494019, Validation Accuracy: 55.59\n",
            "[145/150]: Training Loss: 0.5819402069461589, Training Accuracy: 83.02\n",
            "Validation Loss: 2.0001697182655334, Validation Accuracy: 55.45\n",
            "[146/150]: Training Loss: 0.5872244786243049, Training Accuracy: 82.898\n",
            "Validation Loss: 2.000009226799011, Validation Accuracy: 55.41\n",
            "[147/150]: Training Loss: 0.5821643848808444, Training Accuracy: 83.186\n",
            "Validation Loss: 2.0006944298744203, Validation Accuracy: 55.35\n",
            "[148/150]: Training Loss: 0.5849178761852031, Training Accuracy: 83.14\n",
            "Validation Loss: 2.000812566280365, Validation Accuracy: 55.33\n",
            "[149/150]: Training Loss: 0.5800890168365167, Training Accuracy: 83.304\n",
            "Validation Loss: 2.000700604915619, Validation Accuracy: 55.4\n",
            "[150/150]: Training Loss: 0.5826626444349483, Training Accuracy: 83.206\n",
            "Validation Loss: 2.000621163845062, Validation Accuracy: 55.35\n",
            "**********************************************************************\n",
            "Test Loss: 2.000621163845062, Test Accuracy: 55.35\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>55.35</td></tr><tr><td>Test Loss</td><td>2.00062</td></tr><tr><td>Train Accuracy</td><td>83.206</td></tr><tr><td>Train Loss</td><td>0.58266</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=1024 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/w80ujlhs</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_014707-w80ujlhs/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 2048, Learning rate: 8.485281374238571, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_022321-j8jszxxs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs' target=\"_blank\">batch_size=2048 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.36701738357544, Training Accuracy: 3.886\n",
            "Validation Loss: 4.163058471679688, Validation Accuracy: 5.67\n",
            "[2/150]: Training Loss: 4.113679485321045, Training Accuracy: 6.492\n",
            "Validation Loss: 3.9081267356872558, Validation Accuracy: 9.58\n",
            "[3/150]: Training Loss: 3.8604202461242676, Training Accuracy: 10.016\n",
            "Validation Loss: 3.6436347484588625, Validation Accuracy: 14.06\n",
            "[4/150]: Training Loss: 3.7227595615386964, Training Accuracy: 12.524\n",
            "Validation Loss: 3.58444561958313, Validation Accuracy: 15.49\n",
            "[5/150]: Training Loss: 3.5834127140045164, Training Accuracy: 14.702\n",
            "Validation Loss: 3.4575596332550047, Validation Accuracy: 17.38\n",
            "[6/150]: Training Loss: 3.4569161987304686, Training Accuracy: 16.904\n",
            "Validation Loss: 3.262946367263794, Validation Accuracy: 20.16\n",
            "[7/150]: Training Loss: 3.3158952045440673, Training Accuracy: 19.406\n",
            "Validation Loss: 3.2002731800079345, Validation Accuracy: 22.26\n",
            "[8/150]: Training Loss: 3.1832161426544188, Training Accuracy: 22.068\n",
            "Validation Loss: 3.0290011405944823, Validation Accuracy: 24.81\n",
            "[9/150]: Training Loss: 3.185646390914917, Training Accuracy: 21.936\n",
            "Validation Loss: 3.1018139362335204, Validation Accuracy: 24.11\n",
            "[10/150]: Training Loss: 3.0637048053741456, Training Accuracy: 23.926\n",
            "Validation Loss: 2.9448835372924806, Validation Accuracy: 26.91\n",
            "[11/150]: Training Loss: 2.951322078704834, Training Accuracy: 26.03\n",
            "Validation Loss: 2.8756459236145018, Validation Accuracy: 27.94\n",
            "[12/150]: Training Loss: 2.9279331874847414, Training Accuracy: 26.79\n",
            "Validation Loss: 2.9318973541259767, Validation Accuracy: 27.51\n",
            "[13/150]: Training Loss: 2.828905839920044, Training Accuracy: 28.722\n",
            "Validation Loss: 2.634994125366211, Validation Accuracy: 33.35\n",
            "[14/150]: Training Loss: 2.8078646850585938, Training Accuracy: 29.266\n",
            "Validation Loss: 2.7456551074981688, Validation Accuracy: 30.56\n",
            "[15/150]: Training Loss: 2.712659044265747, Training Accuracy: 30.836\n",
            "Validation Loss: 2.5297746658325195, Validation Accuracy: 34.77\n",
            "[16/150]: Training Loss: 2.651611213684082, Training Accuracy: 32.136\n",
            "Validation Loss: 2.547566270828247, Validation Accuracy: 34.69\n",
            "[17/150]: Training Loss: 2.599648675918579, Training Accuracy: 33.174\n",
            "Validation Loss: 2.5292351245880127, Validation Accuracy: 34.72\n",
            "[18/150]: Training Loss: 2.577428216934204, Training Accuracy: 33.64\n",
            "Validation Loss: 2.5313771247863768, Validation Accuracy: 35.11\n",
            "[19/150]: Training Loss: 2.575561113357544, Training Accuracy: 33.86\n",
            "Validation Loss: 2.50340313911438, Validation Accuracy: 36.18\n",
            "[20/150]: Training Loss: 2.617272434234619, Training Accuracy: 32.958\n",
            "Validation Loss: 2.601680040359497, Validation Accuracy: 34.74\n",
            "[21/150]: Training Loss: 2.503620433807373, Training Accuracy: 35.3\n",
            "Validation Loss: 2.3930795192718506, Validation Accuracy: 38.69\n",
            "[22/150]: Training Loss: 2.4627874088287354, Training Accuracy: 36.662\n",
            "Validation Loss: 2.669213056564331, Validation Accuracy: 33.56\n",
            "[23/150]: Training Loss: 2.548478717803955, Training Accuracy: 34.642\n",
            "Validation Loss: 2.3424915313720702, Validation Accuracy: 39.2\n",
            "[24/150]: Training Loss: 2.3940266799926757, Training Accuracy: 37.736\n",
            "Validation Loss: 2.3067100048065186, Validation Accuracy: 39.53\n",
            "[25/150]: Training Loss: 2.3650291538238526, Training Accuracy: 38.102\n",
            "Validation Loss: 2.4807125091552735, Validation Accuracy: 36.63\n",
            "[26/150]: Training Loss: 2.3525609493255617, Training Accuracy: 38.382\n",
            "Validation Loss: 2.3692517280578613, Validation Accuracy: 39.11\n",
            "[27/150]: Training Loss: 2.2789812183380125, Training Accuracy: 40.074\n",
            "Validation Loss: 2.2349360942840577, Validation Accuracy: 41.75\n",
            "[28/150]: Training Loss: 2.2222459888458252, Training Accuracy: 40.97\n",
            "Validation Loss: 2.19801549911499, Validation Accuracy: 42.61\n",
            "[29/150]: Training Loss: 2.214301881790161, Training Accuracy: 41.694\n",
            "Validation Loss: 2.605668783187866, Validation Accuracy: 35.32\n",
            "[30/150]: Training Loss: 2.338454341888428, Training Accuracy: 38.786\n",
            "Validation Loss: 2.288765287399292, Validation Accuracy: 40.36\n",
            "[31/150]: Training Loss: 2.201904296875, Training Accuracy: 41.99\n",
            "Validation Loss: 2.205613946914673, Validation Accuracy: 42.56\n",
            "[32/150]: Training Loss: 2.1113436079025267, Training Accuracy: 43.934\n",
            "Validation Loss: 2.321266937255859, Validation Accuracy: 41.03\n",
            "[33/150]: Training Loss: 2.1466791677474975, Training Accuracy: 43.006\n",
            "Validation Loss: 2.1349957466125487, Validation Accuracy: 43.8\n",
            "[34/150]: Training Loss: 2.1290991640090944, Training Accuracy: 43.29\n",
            "Validation Loss: 2.217708683013916, Validation Accuracy: 42.79\n",
            "[35/150]: Training Loss: 2.0663461446762086, Training Accuracy: 44.68\n",
            "Validation Loss: 2.3302943229675295, Validation Accuracy: 40.26\n",
            "[36/150]: Training Loss: 2.0297911977767944, Training Accuracy: 45.408\n",
            "Validation Loss: 2.21708025932312, Validation Accuracy: 43.36\n",
            "[37/150]: Training Loss: 2.0129797410964967, Training Accuracy: 45.814\n",
            "Validation Loss: 2.0887409687042235, Validation Accuracy: 45.25\n",
            "[38/150]: Training Loss: 1.937804069519043, Training Accuracy: 47.634\n",
            "Validation Loss: 2.0723501682281493, Validation Accuracy: 45.71\n",
            "[39/150]: Training Loss: 1.943020739555359, Training Accuracy: 47.708\n",
            "Validation Loss: 2.0943463325500487, Validation Accuracy: 44.8\n",
            "[40/150]: Training Loss: 1.9138334608078003, Training Accuracy: 48.346\n",
            "Validation Loss: 2.2366223335266113, Validation Accuracy: 42.21\n",
            "[41/150]: Training Loss: 1.9987104320526123, Training Accuracy: 46.376\n",
            "Validation Loss: 2.12234206199646, Validation Accuracy: 45.35\n",
            "[42/150]: Training Loss: 1.9329777145385743, Training Accuracy: 47.608\n",
            "Validation Loss: 2.097510814666748, Validation Accuracy: 45.23\n",
            "[43/150]: Training Loss: 1.9182713079452514, Training Accuracy: 47.922\n",
            "Validation Loss: 2.085854196548462, Validation Accuracy: 45.54\n",
            "[44/150]: Training Loss: 1.900009379386902, Training Accuracy: 48.732\n",
            "Validation Loss: 2.051387619972229, Validation Accuracy: 46.24\n",
            "[45/150]: Training Loss: 1.8408135080337524, Training Accuracy: 49.714\n",
            "Validation Loss: 2.0023281574249268, Validation Accuracy: 47.57\n",
            "[46/150]: Training Loss: 1.785364203453064, Training Accuracy: 51.326\n",
            "Validation Loss: 2.049905252456665, Validation Accuracy: 46.49\n",
            "[47/150]: Training Loss: 1.8102792501449585, Training Accuracy: 50.858\n",
            "Validation Loss: 2.0309868812561036, Validation Accuracy: 46.64\n",
            "[48/150]: Training Loss: 1.7956236791610718, Training Accuracy: 50.794\n",
            "Validation Loss: 2.0011850118637087, Validation Accuracy: 47.81\n",
            "[49/150]: Training Loss: 1.7439770412445068, Training Accuracy: 52.0\n",
            "Validation Loss: 1.9770531415939332, Validation Accuracy: 48.07\n",
            "[50/150]: Training Loss: 1.7183953332901, Training Accuracy: 52.358\n",
            "Validation Loss: 2.0561673641204834, Validation Accuracy: 46.4\n",
            "[51/150]: Training Loss: 1.717338604927063, Training Accuracy: 53.112\n",
            "Validation Loss: 1.9954840183258056, Validation Accuracy: 47.55\n",
            "[52/150]: Training Loss: 1.7013448238372804, Training Accuracy: 53.098\n",
            "Validation Loss: 2.0024028778076173, Validation Accuracy: 47.72\n",
            "[53/150]: Training Loss: 1.7069044065475465, Training Accuracy: 53.0\n",
            "Validation Loss: 2.08711462020874, Validation Accuracy: 45.49\n",
            "[54/150]: Training Loss: 1.6761716079711915, Training Accuracy: 53.252\n",
            "Validation Loss: 1.9557607173919678, Validation Accuracy: 49.03\n",
            "[55/150]: Training Loss: 1.6440558958053588, Training Accuracy: 54.564\n",
            "Validation Loss: 1.9891844987869263, Validation Accuracy: 48.39\n",
            "[56/150]: Training Loss: 1.6161942625045775, Training Accuracy: 54.926\n",
            "Validation Loss: 2.029493975639343, Validation Accuracy: 47.58\n",
            "[57/150]: Training Loss: 1.6047872447967528, Training Accuracy: 55.192\n",
            "Validation Loss: 1.967372179031372, Validation Accuracy: 48.91\n",
            "[58/150]: Training Loss: 1.713731393814087, Training Accuracy: 52.926\n",
            "Validation Loss: 2.0652761220932008, Validation Accuracy: 46.68\n",
            "[59/150]: Training Loss: 1.6507894086837769, Training Accuracy: 54.226\n",
            "Validation Loss: 1.9929080486297608, Validation Accuracy: 48.66\n",
            "[60/150]: Training Loss: 1.6021452569961547, Training Accuracy: 55.432\n",
            "Validation Loss: 1.9558722257614136, Validation Accuracy: 49.28\n",
            "[61/150]: Training Loss: 1.543088173866272, Training Accuracy: 56.656\n",
            "Validation Loss: 1.9447553634643555, Validation Accuracy: 49.2\n",
            "[62/150]: Training Loss: 1.5494691371917724, Training Accuracy: 56.864\n",
            "Validation Loss: 1.885341501235962, Validation Accuracy: 50.76\n",
            "[63/150]: Training Loss: 1.4937063312530519, Training Accuracy: 58.158\n",
            "Validation Loss: 1.9500130414962769, Validation Accuracy: 49.94\n",
            "[64/150]: Training Loss: 1.4539641427993775, Training Accuracy: 59.004\n",
            "Validation Loss: 1.9285942554473876, Validation Accuracy: 49.98\n",
            "[65/150]: Training Loss: 1.4972302389144898, Training Accuracy: 57.904\n",
            "Validation Loss: 1.9630061149597169, Validation Accuracy: 49.26\n",
            "[66/150]: Training Loss: 1.518665623664856, Training Accuracy: 57.152\n",
            "Validation Loss: 1.9333840608596802, Validation Accuracy: 50.24\n",
            "[67/150]: Training Loss: 1.5036478281021117, Training Accuracy: 57.512\n",
            "Validation Loss: 1.961331272125244, Validation Accuracy: 50.09\n",
            "[68/150]: Training Loss: 1.4539181280136109, Training Accuracy: 59.142\n",
            "Validation Loss: 1.9611144304275512, Validation Accuracy: 49.25\n",
            "[69/150]: Training Loss: 1.46078604221344, Training Accuracy: 58.794\n",
            "Validation Loss: 1.916247057914734, Validation Accuracy: 50.02\n",
            "[70/150]: Training Loss: 1.4112844705581664, Training Accuracy: 59.934\n",
            "Validation Loss: 1.8900265216827392, Validation Accuracy: 51.2\n",
            "[71/150]: Training Loss: 1.3915088891983032, Training Accuracy: 60.376\n",
            "Validation Loss: 1.9043931245803833, Validation Accuracy: 50.87\n",
            "[72/150]: Training Loss: 1.3921622323989868, Training Accuracy: 60.564\n",
            "Validation Loss: 1.9693795204162599, Validation Accuracy: 50.0\n",
            "[73/150]: Training Loss: 1.3862884998321534, Training Accuracy: 60.752\n",
            "Validation Loss: 1.9339972734451294, Validation Accuracy: 50.94\n",
            "[74/150]: Training Loss: 1.358318910598755, Training Accuracy: 61.528\n",
            "Validation Loss: 1.9029748439788818, Validation Accuracy: 51.01\n",
            "[75/150]: Training Loss: 1.3378327751159669, Training Accuracy: 61.9\n",
            "Validation Loss: 1.9114508390426637, Validation Accuracy: 51.04\n",
            "[76/150]: Training Loss: 1.307257251739502, Training Accuracy: 62.684\n",
            "Validation Loss: 1.897865653038025, Validation Accuracy: 51.33\n",
            "[77/150]: Training Loss: 1.307571873664856, Training Accuracy: 62.566\n",
            "Validation Loss: 1.9430776834487915, Validation Accuracy: 50.79\n",
            "[78/150]: Training Loss: 1.2927326250076294, Training Accuracy: 62.904\n",
            "Validation Loss: 1.910500168800354, Validation Accuracy: 52.01\n",
            "[79/150]: Training Loss: 1.2737730932235718, Training Accuracy: 63.4\n",
            "Validation Loss: 1.8831387996673583, Validation Accuracy: 52.08\n",
            "[80/150]: Training Loss: 1.254057068824768, Training Accuracy: 64.394\n",
            "Validation Loss: 1.8864023208618164, Validation Accuracy: 51.51\n",
            "[81/150]: Training Loss: 1.2196114826202393, Training Accuracy: 64.948\n",
            "Validation Loss: 1.9068121194839478, Validation Accuracy: 51.97\n",
            "[82/150]: Training Loss: 1.2375918960571288, Training Accuracy: 64.48\n",
            "Validation Loss: 1.9304296493530273, Validation Accuracy: 51.61\n",
            "[83/150]: Training Loss: 1.2156933164596557, Training Accuracy: 64.806\n",
            "Validation Loss: 1.9103889226913453, Validation Accuracy: 51.82\n",
            "[84/150]: Training Loss: 1.1918401718139648, Training Accuracy: 65.572\n",
            "Validation Loss: 1.9341503858566285, Validation Accuracy: 51.6\n",
            "[85/150]: Training Loss: 1.2064945554733277, Training Accuracy: 65.146\n",
            "Validation Loss: 1.9983864784240724, Validation Accuracy: 50.34\n",
            "[86/150]: Training Loss: 1.201401376724243, Training Accuracy: 65.426\n",
            "Validation Loss: 1.9176611185073853, Validation Accuracy: 51.45\n",
            "[87/150]: Training Loss: 1.1660136127471923, Training Accuracy: 66.176\n",
            "Validation Loss: 1.943800139427185, Validation Accuracy: 51.49\n",
            "[88/150]: Training Loss: 1.1312262058258056, Training Accuracy: 67.438\n",
            "Validation Loss: 1.9152551412582397, Validation Accuracy: 51.68\n",
            "[89/150]: Training Loss: 1.1454276323318482, Training Accuracy: 66.958\n",
            "Validation Loss: 1.9066155195236205, Validation Accuracy: 52.43\n",
            "[90/150]: Training Loss: 1.1536525392532349, Training Accuracy: 66.622\n",
            "Validation Loss: 1.912238073348999, Validation Accuracy: 51.99\n",
            "[91/150]: Training Loss: 1.1152591037750244, Training Accuracy: 67.662\n",
            "Validation Loss: 1.9254616022109985, Validation Accuracy: 52.11\n",
            "[92/150]: Training Loss: 1.0796469306945802, Training Accuracy: 68.538\n",
            "Validation Loss: 1.918501377105713, Validation Accuracy: 52.25\n",
            "[93/150]: Training Loss: 1.0836839008331298, Training Accuracy: 68.43\n",
            "Validation Loss: 1.9036231279373168, Validation Accuracy: 52.52\n",
            "[94/150]: Training Loss: 1.111613211631775, Training Accuracy: 67.66\n",
            "Validation Loss: 1.9749577283859252, Validation Accuracy: 51.0\n",
            "[95/150]: Training Loss: 1.1183206748962402, Training Accuracy: 67.53\n",
            "Validation Loss: 1.9319661378860473, Validation Accuracy: 52.53\n",
            "[96/150]: Training Loss: 1.0428566884994508, Training Accuracy: 69.502\n",
            "Validation Loss: 1.9246442794799805, Validation Accuracy: 52.17\n",
            "[97/150]: Training Loss: 1.029050705432892, Training Accuracy: 70.002\n",
            "Validation Loss: 1.9244839429855347, Validation Accuracy: 52.21\n",
            "[98/150]: Training Loss: 1.0212417674064636, Training Accuracy: 70.272\n",
            "Validation Loss: 1.9145327806472778, Validation Accuracy: 52.78\n",
            "[99/150]: Training Loss: 1.0124672603607179, Training Accuracy: 70.44\n",
            "Validation Loss: 1.9107223749160767, Validation Accuracy: 53.04\n",
            "[100/150]: Training Loss: 0.9880559778213501, Training Accuracy: 71.17\n",
            "Validation Loss: 1.9347419500350953, Validation Accuracy: 52.57\n",
            "[101/150]: Training Loss: 0.9812371230125427, Training Accuracy: 71.144\n",
            "Validation Loss: 1.9186458826065063, Validation Accuracy: 53.22\n",
            "[102/150]: Training Loss: 0.9656218528747559, Training Accuracy: 72.014\n",
            "Validation Loss: 1.9120693922042846, Validation Accuracy: 53.07\n",
            "[103/150]: Training Loss: 0.9567889213562012, Training Accuracy: 71.89\n",
            "Validation Loss: 1.9197413206100464, Validation Accuracy: 52.84\n",
            "[104/150]: Training Loss: 0.9529655456542969, Training Accuracy: 72.086\n",
            "Validation Loss: 1.9144711256027223, Validation Accuracy: 53.0\n",
            "[105/150]: Training Loss: 0.9852134394645691, Training Accuracy: 71.252\n",
            "Validation Loss: 1.9319961071014404, Validation Accuracy: 53.06\n",
            "[106/150]: Training Loss: 0.9273789191246032, Training Accuracy: 72.91\n",
            "Validation Loss: 1.9424444437026978, Validation Accuracy: 52.84\n",
            "[107/150]: Training Loss: 0.9234069514274598, Training Accuracy: 73.084\n",
            "Validation Loss: 1.9234145641326905, Validation Accuracy: 52.94\n",
            "[108/150]: Training Loss: 0.9229924130439758, Training Accuracy: 73.21\n",
            "Validation Loss: 1.9324826002120972, Validation Accuracy: 52.73\n",
            "[109/150]: Training Loss: 0.9058748340606689, Training Accuracy: 73.67\n",
            "Validation Loss: 1.9500421285629272, Validation Accuracy: 53.29\n",
            "[110/150]: Training Loss: 0.8911954641342164, Training Accuracy: 73.872\n",
            "Validation Loss: 1.941013789176941, Validation Accuracy: 53.28\n",
            "[111/150]: Training Loss: 0.8825567650794983, Training Accuracy: 74.012\n",
            "Validation Loss: 1.9361898183822632, Validation Accuracy: 53.09\n",
            "[112/150]: Training Loss: 0.8620765686035157, Training Accuracy: 74.656\n",
            "Validation Loss: 1.9341821908950805, Validation Accuracy: 53.36\n",
            "[113/150]: Training Loss: 0.8601221704483032, Training Accuracy: 74.8\n",
            "Validation Loss: 1.9615466117858886, Validation Accuracy: 53.34\n",
            "[114/150]: Training Loss: 0.8472113418579101, Training Accuracy: 75.216\n",
            "Validation Loss: 1.950052309036255, Validation Accuracy: 53.42\n",
            "[115/150]: Training Loss: 0.84150705575943, Training Accuracy: 75.34\n",
            "Validation Loss: 1.9719900608062744, Validation Accuracy: 53.33\n",
            "[116/150]: Training Loss: 0.8295826292037964, Training Accuracy: 75.764\n",
            "Validation Loss: 1.9504849910736084, Validation Accuracy: 53.65\n",
            "[117/150]: Training Loss: 0.8204774522781372, Training Accuracy: 76.024\n",
            "Validation Loss: 1.9509764194488526, Validation Accuracy: 54.02\n",
            "[118/150]: Training Loss: 0.8103749394416809, Training Accuracy: 76.418\n",
            "Validation Loss: 1.9664921760559082, Validation Accuracy: 53.3\n",
            "[119/150]: Training Loss: 0.8074698400497436, Training Accuracy: 76.44\n",
            "Validation Loss: 1.955758023262024, Validation Accuracy: 53.71\n",
            "[120/150]: Training Loss: 0.8068988251686097, Training Accuracy: 76.406\n",
            "Validation Loss: 1.9637468814849854, Validation Accuracy: 53.55\n",
            "[121/150]: Training Loss: 0.7903911852836609, Training Accuracy: 76.678\n",
            "Validation Loss: 1.9706329584121705, Validation Accuracy: 53.57\n",
            "[122/150]: Training Loss: 0.7799955105781555, Training Accuracy: 77.276\n",
            "Validation Loss: 1.976036286354065, Validation Accuracy: 53.7\n",
            "[123/150]: Training Loss: 0.7765986251831055, Training Accuracy: 77.324\n",
            "Validation Loss: 1.9730368375778198, Validation Accuracy: 53.3\n",
            "[124/150]: Training Loss: 0.7681253123283386, Training Accuracy: 77.54\n",
            "Validation Loss: 1.9741400957107544, Validation Accuracy: 53.63\n",
            "[125/150]: Training Loss: 0.7622663331031799, Training Accuracy: 77.518\n",
            "Validation Loss: 1.9705329179763793, Validation Accuracy: 54.09\n",
            "[126/150]: Training Loss: 0.7574337005615235, Training Accuracy: 78.034\n",
            "Validation Loss: 1.9730172395706176, Validation Accuracy: 53.71\n",
            "[127/150]: Training Loss: 0.7504327082633973, Training Accuracy: 78.258\n",
            "Validation Loss: 1.9689469575881957, Validation Accuracy: 53.87\n",
            "[128/150]: Training Loss: 0.7432142400741577, Training Accuracy: 78.082\n",
            "Validation Loss: 1.9787657976150512, Validation Accuracy: 53.54\n",
            "[129/150]: Training Loss: 0.7408276867866516, Training Accuracy: 78.396\n",
            "Validation Loss: 1.9802783012390137, Validation Accuracy: 53.89\n",
            "[130/150]: Training Loss: 0.7272537612915039, Training Accuracy: 78.856\n",
            "Validation Loss: 1.9677628517150878, Validation Accuracy: 53.76\n",
            "[131/150]: Training Loss: 0.7158604049682618, Training Accuracy: 78.946\n",
            "Validation Loss: 1.97785542011261, Validation Accuracy: 53.9\n",
            "[132/150]: Training Loss: 0.7163805866241455, Training Accuracy: 79.138\n",
            "Validation Loss: 1.976898455619812, Validation Accuracy: 53.7\n",
            "[133/150]: Training Loss: 0.721850438117981, Training Accuracy: 78.816\n",
            "Validation Loss: 1.9874832153320312, Validation Accuracy: 53.66\n",
            "[134/150]: Training Loss: 0.719142906665802, Training Accuracy: 79.046\n",
            "Validation Loss: 1.9858264207839966, Validation Accuracy: 53.92\n",
            "[135/150]: Training Loss: 0.7152685523033142, Training Accuracy: 79.11\n",
            "Validation Loss: 1.9745909452438355, Validation Accuracy: 53.99\n",
            "[136/150]: Training Loss: 0.7077970743179322, Training Accuracy: 79.564\n",
            "Validation Loss: 1.9791152715682983, Validation Accuracy: 53.54\n",
            "[137/150]: Training Loss: 0.7019647455215454, Training Accuracy: 79.626\n",
            "Validation Loss: 1.9777050018310547, Validation Accuracy: 54.06\n",
            "[138/150]: Training Loss: 0.6962298607826233, Training Accuracy: 79.744\n",
            "Validation Loss: 1.9783851623535156, Validation Accuracy: 53.9\n",
            "[139/150]: Training Loss: 0.6945947551727295, Training Accuracy: 79.802\n",
            "Validation Loss: 1.9834458827972412, Validation Accuracy: 53.84\n",
            "[140/150]: Training Loss: 0.6916244769096375, Training Accuracy: 79.876\n",
            "Validation Loss: 1.9782796621322631, Validation Accuracy: 54.08\n",
            "[141/150]: Training Loss: 0.686495201587677, Training Accuracy: 80.022\n",
            "Validation Loss: 1.9823485612869263, Validation Accuracy: 54.07\n",
            "[142/150]: Training Loss: 0.6907814073562623, Training Accuracy: 80.056\n",
            "Validation Loss: 1.9827837467193603, Validation Accuracy: 53.98\n",
            "[143/150]: Training Loss: 0.6871487998962402, Training Accuracy: 80.084\n",
            "Validation Loss: 1.9844423055648803, Validation Accuracy: 54.1\n",
            "[144/150]: Training Loss: 0.6854921150207519, Training Accuracy: 80.314\n",
            "Validation Loss: 1.9852968454360962, Validation Accuracy: 53.98\n",
            "[145/150]: Training Loss: 0.6804221367835999, Training Accuracy: 80.244\n",
            "Validation Loss: 1.9844240427017212, Validation Accuracy: 54.1\n",
            "[146/150]: Training Loss: 0.682576208114624, Training Accuracy: 80.136\n",
            "Validation Loss: 1.9841588735580444, Validation Accuracy: 54.07\n",
            "[147/150]: Training Loss: 0.6786355781555176, Training Accuracy: 80.254\n",
            "Validation Loss: 1.9839539051055908, Validation Accuracy: 54.12\n",
            "[148/150]: Training Loss: 0.6753697633743286, Training Accuracy: 80.27\n",
            "Validation Loss: 1.9842885971069335, Validation Accuracy: 54.16\n",
            "[149/150]: Training Loss: 0.6753653740882873, Training Accuracy: 80.488\n",
            "Validation Loss: 1.9842547655105591, Validation Accuracy: 54.12\n",
            "[150/150]: Training Loss: 0.6740122199058532, Training Accuracy: 80.532\n",
            "Validation Loss: 1.984296441078186, Validation Accuracy: 54.15\n",
            "**********************************************************************\n",
            "Test Loss: 1.984296441078186, Test Accuracy: 54.15\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>54.15</td></tr><tr><td>Test Loss</td><td>1.9843</td></tr><tr><td>Train Accuracy</td><td>80.532</td></tr><tr><td>Train Loss</td><td>0.67401</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=2048 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/j8jszxxs</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_022321-j8jszxxs/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 4096, Learning rate: 12.0, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_024410-fcrci1zx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx' target=\"_blank\">batch_size=4096 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.448218419001653, Training Accuracy: 3.104\n",
            "Validation Loss: 4.19553820292155, Validation Accuracy: 5.83\n",
            "[2/150]: Training Loss: 4.162900081047645, Training Accuracy: 5.992\n",
            "Validation Loss: 4.001244783401489, Validation Accuracy: 8.19\n",
            "[3/150]: Training Loss: 3.9526728116548977, Training Accuracy: 9.02\n",
            "Validation Loss: 3.849735975265503, Validation Accuracy: 10.99\n",
            "[4/150]: Training Loss: 3.9088880098783054, Training Accuracy: 9.748\n",
            "Validation Loss: 3.7720150152842202, Validation Accuracy: 11.82\n",
            "[5/150]: Training Loss: 3.7579474999354434, Training Accuracy: 11.674\n",
            "Validation Loss: 3.6346964836120605, Validation Accuracy: 13.58\n",
            "[6/150]: Training Loss: 3.8317384169651914, Training Accuracy: 10.848\n",
            "Validation Loss: 3.6928911209106445, Validation Accuracy: 13.8\n",
            "[7/150]: Training Loss: 3.6279670825371375, Training Accuracy: 13.764\n",
            "Validation Loss: 3.426232655843099, Validation Accuracy: 17.9\n",
            "[8/150]: Training Loss: 3.559702047934899, Training Accuracy: 15.198\n",
            "Validation Loss: 3.5184563795725503, Validation Accuracy: 15.74\n",
            "[9/150]: Training Loss: 3.619346306874202, Training Accuracy: 14.12\n",
            "Validation Loss: 3.4724766413370767, Validation Accuracy: 16.53\n",
            "[10/150]: Training Loss: 3.4825954070458045, Training Accuracy: 16.518\n",
            "Validation Loss: 3.491668939590454, Validation Accuracy: 17.05\n",
            "[11/150]: Training Loss: 3.5322888447688174, Training Accuracy: 15.738\n",
            "Validation Loss: 3.3563063939412436, Validation Accuracy: 18.53\n",
            "[12/150]: Training Loss: 3.381067386040321, Training Accuracy: 18.274\n",
            "Validation Loss: 3.371330420176188, Validation Accuracy: 18.51\n",
            "[13/150]: Training Loss: 3.3759262011601376, Training Accuracy: 18.364\n",
            "Validation Loss: 3.1718979676564536, Validation Accuracy: 22.38\n",
            "[14/150]: Training Loss: 3.24786586027879, Training Accuracy: 20.88\n",
            "Validation Loss: 3.149214585622152, Validation Accuracy: 22.67\n",
            "[15/150]: Training Loss: 3.155352940926185, Training Accuracy: 22.24\n",
            "Validation Loss: 2.999330917994181, Validation Accuracy: 25.44\n",
            "[16/150]: Training Loss: 2.9997272124657264, Training Accuracy: 25.252\n",
            "Validation Loss: 2.8626222610473633, Validation Accuracy: 28.18\n",
            "[17/150]: Training Loss: 2.952247326190655, Training Accuracy: 26.39\n",
            "Validation Loss: 3.0151708920796714, Validation Accuracy: 25.48\n",
            "[18/150]: Training Loss: 3.035908350577721, Training Accuracy: 24.716\n",
            "Validation Loss: 2.84928830464681, Validation Accuracy: 28.54\n",
            "[19/150]: Training Loss: 2.8744187355041504, Training Accuracy: 27.56\n",
            "Validation Loss: 2.732924222946167, Validation Accuracy: 30.87\n",
            "[20/150]: Training Loss: 2.7774770443256083, Training Accuracy: 29.672\n",
            "Validation Loss: 2.800260861714681, Validation Accuracy: 29.69\n",
            "[21/150]: Training Loss: 2.9696661142202525, Training Accuracy: 26.582\n",
            "Validation Loss: 3.213308095932007, Validation Accuracy: 22.23\n",
            "[22/150]: Training Loss: 3.2055968137887807, Training Accuracy: 22.87\n",
            "Validation Loss: 3.0360422134399414, Validation Accuracy: 25.56\n",
            "[23/150]: Training Loss: 3.0269867456876316, Training Accuracy: 25.102\n",
            "Validation Loss: 2.861661911010742, Validation Accuracy: 29.24\n",
            "[24/150]: Training Loss: 2.8519647121429443, Training Accuracy: 28.248\n",
            "Validation Loss: 2.682717482248942, Validation Accuracy: 31.6\n",
            "[25/150]: Training Loss: 2.705435276031494, Training Accuracy: 31.156\n",
            "Validation Loss: 2.509896198908488, Validation Accuracy: 35.11\n",
            "[26/150]: Training Loss: 2.613956708174485, Training Accuracy: 33.41\n",
            "Validation Loss: 2.505950371424357, Validation Accuracy: 34.77\n",
            "[27/150]: Training Loss: 3.002190589904785, Training Accuracy: 26.466\n",
            "Validation Loss: 2.775864839553833, Validation Accuracy: 29.88\n",
            "[28/150]: Training Loss: 2.742469017322247, Training Accuracy: 30.316\n",
            "Validation Loss: 2.589009443918864, Validation Accuracy: 33.99\n",
            "[29/150]: Training Loss: 2.8341226394359884, Training Accuracy: 29.282\n",
            "Validation Loss: 2.8079089323679605, Validation Accuracy: 29.56\n",
            "[30/150]: Training Loss: 2.745966214400071, Training Accuracy: 30.384\n",
            "Validation Loss: 2.5663379033406577, Validation Accuracy: 34.18\n",
            "[31/150]: Training Loss: 2.5754335476801944, Training Accuracy: 33.944\n",
            "Validation Loss: 2.4243160088857016, Validation Accuracy: 37.33\n",
            "[32/150]: Training Loss: 2.4728290117703953, Training Accuracy: 35.998\n",
            "Validation Loss: 2.4164366722106934, Validation Accuracy: 37.02\n",
            "[33/150]: Training Loss: 2.550288127018855, Training Accuracy: 34.558\n",
            "Validation Loss: 2.4929703871409097, Validation Accuracy: 35.7\n",
            "[34/150]: Training Loss: 2.5172606064723086, Training Accuracy: 35.468\n",
            "Validation Loss: 2.5022354125976562, Validation Accuracy: 35.92\n",
            "[35/150]: Training Loss: 2.5095452528733473, Training Accuracy: 35.052\n",
            "Validation Loss: 2.3538503646850586, Validation Accuracy: 38.43\n",
            "[36/150]: Training Loss: 2.595434335561899, Training Accuracy: 34.342\n",
            "Validation Loss: 2.7459415594736734, Validation Accuracy: 30.95\n",
            "[37/150]: Training Loss: 2.5996641562535214, Training Accuracy: 33.51\n",
            "Validation Loss: 2.4413878122965493, Validation Accuracy: 37.55\n",
            "[38/150]: Training Loss: 2.4308731005742, Training Accuracy: 36.788\n",
            "Validation Loss: 2.3517263730367026, Validation Accuracy: 38.8\n",
            "[39/150]: Training Loss: 2.3385920707996073, Training Accuracy: 39.318\n",
            "Validation Loss: 2.4507851600646973, Validation Accuracy: 37.58\n",
            "[40/150]: Training Loss: 2.4006691529200626, Training Accuracy: 37.842\n",
            "Validation Loss: 2.3381757736206055, Validation Accuracy: 39.0\n",
            "[41/150]: Training Loss: 2.4301048792325535, Training Accuracy: 36.898\n",
            "Validation Loss: 2.3649702866872153, Validation Accuracy: 38.64\n",
            "[42/150]: Training Loss: 2.282798070173997, Training Accuracy: 39.856\n",
            "Validation Loss: 2.2728551228841147, Validation Accuracy: 40.36\n",
            "[43/150]: Training Loss: 2.235896715751061, Training Accuracy: 40.962\n",
            "Validation Loss: 2.2269629637400308, Validation Accuracy: 41.47\n",
            "[44/150]: Training Loss: 2.277051045344426, Training Accuracy: 40.666\n",
            "Validation Loss: 2.393709977467855, Validation Accuracy: 38.68\n",
            "[45/150]: Training Loss: 2.266980061164269, Training Accuracy: 40.186\n",
            "Validation Loss: 2.260339101155599, Validation Accuracy: 41.24\n",
            "[46/150]: Training Loss: 2.2269545518434963, Training Accuracy: 41.5\n",
            "Validation Loss: 2.184856653213501, Validation Accuracy: 42.59\n",
            "[47/150]: Training Loss: 2.1506412762861986, Training Accuracy: 42.82\n",
            "Validation Loss: 2.151843468348185, Validation Accuracy: 43.6\n",
            "[48/150]: Training Loss: 2.0945023756760817, Training Accuracy: 45.142\n",
            "Validation Loss: 2.1851927439371743, Validation Accuracy: 42.94\n",
            "[49/150]: Training Loss: 2.368942279082078, Training Accuracy: 38.482\n",
            "Validation Loss: 2.311251401901245, Validation Accuracy: 39.8\n",
            "[50/150]: Training Loss: 2.1795968275803785, Training Accuracy: 42.274\n",
            "Validation Loss: 2.1368969281514487, Validation Accuracy: 43.53\n",
            "[51/150]: Training Loss: 2.287435614145719, Training Accuracy: 40.578\n",
            "Validation Loss: 2.3101561864217124, Validation Accuracy: 40.31\n",
            "[52/150]: Training Loss: 2.133154667340792, Training Accuracy: 43.412\n",
            "Validation Loss: 2.101039091746012, Validation Accuracy: 44.48\n",
            "[53/150]: Training Loss: 2.009332299232483, Training Accuracy: 46.076\n",
            "Validation Loss: 2.053215821584066, Validation Accuracy: 46.02\n",
            "[54/150]: Training Loss: 2.0092475414276123, Training Accuracy: 46.312\n",
            "Validation Loss: 2.068614880243937, Validation Accuracy: 45.15\n",
            "[55/150]: Training Loss: 2.0747447105554433, Training Accuracy: 44.708\n",
            "Validation Loss: 2.0757156213124595, Validation Accuracy: 45.38\n",
            "[56/150]: Training Loss: 2.0832339341823873, Training Accuracy: 44.52\n",
            "Validation Loss: 2.2822861671447754, Validation Accuracy: 40.59\n",
            "[57/150]: Training Loss: 2.035889350450956, Training Accuracy: 45.234\n",
            "Validation Loss: 2.077279488245646, Validation Accuracy: 45.32\n",
            "[58/150]: Training Loss: 1.9101605140245879, Training Accuracy: 48.098\n",
            "Validation Loss: 2.075277328491211, Validation Accuracy: 45.52\n",
            "[59/150]: Training Loss: 1.930518700526311, Training Accuracy: 47.694\n",
            "Validation Loss: 2.0554560820261636, Validation Accuracy: 45.94\n",
            "[60/150]: Training Loss: 1.9262570784642146, Training Accuracy: 47.71\n",
            "Validation Loss: 1.9856750170389812, Validation Accuracy: 46.93\n",
            "[61/150]: Training Loss: 1.8526782714403593, Training Accuracy: 49.478\n",
            "Validation Loss: 1.9915226300557454, Validation Accuracy: 46.83\n",
            "[62/150]: Training Loss: 1.8636801151128917, Training Accuracy: 49.23\n",
            "Validation Loss: 2.094877322514852, Validation Accuracy: 45.09\n",
            "[63/150]: Training Loss: 1.8440089867665217, Training Accuracy: 49.998\n",
            "Validation Loss: 1.9725687503814697, Validation Accuracy: 47.58\n",
            "[64/150]: Training Loss: 1.8951456546783447, Training Accuracy: 48.798\n",
            "Validation Loss: 2.079833189646403, Validation Accuracy: 45.33\n",
            "[65/150]: Training Loss: 1.9134588700074415, Training Accuracy: 48.274\n",
            "Validation Loss: 2.0578166246414185, Validation Accuracy: 45.69\n",
            "[66/150]: Training Loss: 1.850765347480774, Training Accuracy: 49.748\n",
            "Validation Loss: 2.0475390752156577, Validation Accuracy: 47.14\n",
            "[67/150]: Training Loss: 1.7821540557421172, Training Accuracy: 51.288\n",
            "Validation Loss: 1.9497700134913127, Validation Accuracy: 48.19\n",
            "[68/150]: Training Loss: 1.7319823136696448, Training Accuracy: 52.606\n",
            "Validation Loss: 2.018282691637675, Validation Accuracy: 47.2\n",
            "[69/150]: Training Loss: 1.7186179894667406, Training Accuracy: 52.626\n",
            "Validation Loss: 1.9749605258305867, Validation Accuracy: 48.19\n",
            "[70/150]: Training Loss: 1.6911903069569514, Training Accuracy: 53.452\n",
            "Validation Loss: 1.9826482931772869, Validation Accuracy: 47.74\n",
            "[71/150]: Training Loss: 1.660032529097337, Training Accuracy: 54.11\n",
            "Validation Loss: 1.908194661140442, Validation Accuracy: 49.51\n",
            "[72/150]: Training Loss: 1.6769285110326915, Training Accuracy: 53.518\n",
            "Validation Loss: 1.954678734143575, Validation Accuracy: 48.03\n",
            "[73/150]: Training Loss: 1.6593820590239305, Training Accuracy: 53.96\n",
            "Validation Loss: 1.9272022247314453, Validation Accuracy: 48.63\n",
            "[74/150]: Training Loss: 1.6419056562276988, Training Accuracy: 54.992\n",
            "Validation Loss: 1.992493987083435, Validation Accuracy: 47.79\n",
            "[75/150]: Training Loss: 1.6689472198486328, Training Accuracy: 53.936\n",
            "Validation Loss: 1.9214499394098918, Validation Accuracy: 49.0\n",
            "[76/150]: Training Loss: 1.6087861702992365, Training Accuracy: 55.276\n",
            "Validation Loss: 1.896010160446167, Validation Accuracy: 49.84\n",
            "[77/150]: Training Loss: 1.5652516438410833, Training Accuracy: 56.72\n",
            "Validation Loss: 1.873304804166158, Validation Accuracy: 51.03\n",
            "[78/150]: Training Loss: 1.6746907876088069, Training Accuracy: 54.032\n",
            "Validation Loss: 1.954778750737508, Validation Accuracy: 49.08\n",
            "[79/150]: Training Loss: 1.5788640517454882, Training Accuracy: 55.932\n",
            "Validation Loss: 1.9272086222966511, Validation Accuracy: 50.3\n",
            "[80/150]: Training Loss: 1.6021267634171705, Training Accuracy: 55.644\n",
            "Validation Loss: 2.1279262701670327, Validation Accuracy: 45.34\n",
            "[81/150]: Training Loss: 1.7035195552385771, Training Accuracy: 53.114\n",
            "Validation Loss: 1.9278326431910198, Validation Accuracy: 49.04\n",
            "[82/150]: Training Loss: 1.559159095470722, Training Accuracy: 56.4\n",
            "Validation Loss: 1.9029817183812459, Validation Accuracy: 49.79\n",
            "[83/150]: Training Loss: 1.5267010835500865, Training Accuracy: 57.53\n",
            "Validation Loss: 1.9380817810694377, Validation Accuracy: 48.88\n",
            "[84/150]: Training Loss: 1.5102592706680298, Training Accuracy: 57.916\n",
            "Validation Loss: 1.8722127676010132, Validation Accuracy: 51.08\n",
            "[85/150]: Training Loss: 1.4807829398375292, Training Accuracy: 58.592\n",
            "Validation Loss: 1.9032895962397258, Validation Accuracy: 49.92\n",
            "[86/150]: Training Loss: 1.4740843681188731, Training Accuracy: 58.914\n",
            "Validation Loss: 1.8672935565312703, Validation Accuracy: 50.45\n",
            "[87/150]: Training Loss: 1.515398117212149, Training Accuracy: 57.934\n",
            "Validation Loss: 1.9412325620651245, Validation Accuracy: 49.15\n",
            "[88/150]: Training Loss: 1.576969366807204, Training Accuracy: 55.868\n",
            "Validation Loss: 1.935785969098409, Validation Accuracy: 49.25\n",
            "[89/150]: Training Loss: 1.4918665610826933, Training Accuracy: 58.11\n",
            "Validation Loss: 1.8986582358678181, Validation Accuracy: 51.05\n",
            "[90/150]: Training Loss: 1.4492073059082031, Training Accuracy: 59.446\n",
            "Validation Loss: 1.8367600440979004, Validation Accuracy: 51.57\n",
            "[91/150]: Training Loss: 1.4048503912412202, Training Accuracy: 60.418\n",
            "Validation Loss: 1.837977687517802, Validation Accuracy: 51.68\n",
            "[92/150]: Training Loss: 1.3744991559248705, Training Accuracy: 61.112\n",
            "Validation Loss: 1.8420219818751018, Validation Accuracy: 51.77\n",
            "[93/150]: Training Loss: 1.420577076765207, Training Accuracy: 60.392\n",
            "Validation Loss: 1.8663844267527263, Validation Accuracy: 51.21\n",
            "[94/150]: Training Loss: 1.4505155269916241, Training Accuracy: 59.012\n",
            "Validation Loss: 1.8934478759765625, Validation Accuracy: 50.84\n",
            "[95/150]: Training Loss: 1.4003274349065928, Training Accuracy: 60.672\n",
            "Validation Loss: 1.8551263411839802, Validation Accuracy: 51.69\n",
            "[96/150]: Training Loss: 1.3484498354104848, Training Accuracy: 61.974\n",
            "Validation Loss: 1.8562318483988445, Validation Accuracy: 51.96\n",
            "[97/150]: Training Loss: 1.314302426118117, Training Accuracy: 62.828\n",
            "Validation Loss: 1.8313268423080444, Validation Accuracy: 52.72\n",
            "[98/150]: Training Loss: 1.2976581683525672, Training Accuracy: 63.268\n",
            "Validation Loss: 1.8362118005752563, Validation Accuracy: 52.11\n",
            "[99/150]: Training Loss: 1.317265354670011, Training Accuracy: 62.994\n",
            "Validation Loss: 1.8350664774576824, Validation Accuracy: 52.02\n",
            "[100/150]: Training Loss: 1.2960394254097571, Training Accuracy: 63.262\n",
            "Validation Loss: 1.800865610440572, Validation Accuracy: 53.11\n",
            "[101/150]: Training Loss: 1.298880622937129, Training Accuracy: 62.896\n",
            "Validation Loss: 1.883919596672058, Validation Accuracy: 51.53\n",
            "[102/150]: Training Loss: 1.2974212628144484, Training Accuracy: 62.852\n",
            "Validation Loss: 1.8424270550409954, Validation Accuracy: 52.02\n",
            "[103/150]: Training Loss: 1.256823787322411, Training Accuracy: 63.952\n",
            "Validation Loss: 1.8408455053965251, Validation Accuracy: 52.54\n",
            "[104/150]: Training Loss: 1.252828497153062, Training Accuracy: 64.42\n",
            "Validation Loss: 1.8253253698349, Validation Accuracy: 52.37\n",
            "[105/150]: Training Loss: 1.2491283691846407, Training Accuracy: 64.358\n",
            "Validation Loss: 1.8558521668116252, Validation Accuracy: 52.17\n",
            "[106/150]: Training Loss: 1.229967685846182, Training Accuracy: 64.958\n",
            "Validation Loss: 1.8309193849563599, Validation Accuracy: 52.71\n",
            "[107/150]: Training Loss: 1.2172498794702382, Training Accuracy: 65.282\n",
            "Validation Loss: 1.8394190073013306, Validation Accuracy: 52.48\n",
            "[108/150]: Training Loss: 1.207243589254526, Training Accuracy: 65.544\n",
            "Validation Loss: 1.82985524336497, Validation Accuracy: 52.3\n",
            "[109/150]: Training Loss: 1.1976150732774, Training Accuracy: 65.54\n",
            "Validation Loss: 1.8410567442576091, Validation Accuracy: 52.6\n",
            "[110/150]: Training Loss: 1.1797684522775502, Training Accuracy: 66.22\n",
            "Validation Loss: 1.828009049097697, Validation Accuracy: 53.27\n",
            "[111/150]: Training Loss: 1.1586786141762366, Training Accuracy: 66.72\n",
            "Validation Loss: 1.8170452117919922, Validation Accuracy: 53.66\n",
            "[112/150]: Training Loss: 1.171253332724938, Training Accuracy: 66.59\n",
            "Validation Loss: 1.8729171355565388, Validation Accuracy: 52.55\n",
            "[113/150]: Training Loss: 1.2010775346022387, Training Accuracy: 65.75\n",
            "Validation Loss: 1.8387389580408733, Validation Accuracy: 53.17\n",
            "[114/150]: Training Loss: 1.1887562825129583, Training Accuracy: 66.114\n",
            "Validation Loss: 1.8573131561279297, Validation Accuracy: 52.59\n",
            "[115/150]: Training Loss: 1.1683265062478871, Training Accuracy: 66.588\n",
            "Validation Loss: 1.8474773168563843, Validation Accuracy: 53.17\n",
            "[116/150]: Training Loss: 1.1644465373112605, Training Accuracy: 66.768\n",
            "Validation Loss: 1.8591439326604207, Validation Accuracy: 52.41\n",
            "[117/150]: Training Loss: 1.1479460184390728, Training Accuracy: 67.308\n",
            "Validation Loss: 1.8460925817489624, Validation Accuracy: 53.29\n",
            "[118/150]: Training Loss: 1.1312938928604126, Training Accuracy: 67.496\n",
            "Validation Loss: 1.8282337188720703, Validation Accuracy: 53.62\n",
            "[119/150]: Training Loss: 1.1176557632593007, Training Accuracy: 68.078\n",
            "Validation Loss: 1.8184215625127156, Validation Accuracy: 53.58\n",
            "[120/150]: Training Loss: 1.1109853432728694, Training Accuracy: 68.212\n",
            "Validation Loss: 1.8269612789154053, Validation Accuracy: 53.4\n",
            "[121/150]: Training Loss: 1.092060116621164, Training Accuracy: 68.912\n",
            "Validation Loss: 1.8181384801864624, Validation Accuracy: 53.59\n",
            "[122/150]: Training Loss: 1.0812291502952576, Training Accuracy: 69.18\n",
            "Validation Loss: 1.8252775271733601, Validation Accuracy: 53.6\n",
            "[123/150]: Training Loss: 1.0699465458209698, Training Accuracy: 69.43\n",
            "Validation Loss: 1.834611177444458, Validation Accuracy: 54.01\n",
            "[124/150]: Training Loss: 1.0735741762014537, Training Accuracy: 69.112\n",
            "Validation Loss: 1.8203496138254802, Validation Accuracy: 53.93\n",
            "[125/150]: Training Loss: 1.0661220917334924, Training Accuracy: 69.538\n",
            "Validation Loss: 1.828150749206543, Validation Accuracy: 53.85\n",
            "[126/150]: Training Loss: 1.059658169746399, Training Accuracy: 69.666\n",
            "Validation Loss: 1.8271387418111165, Validation Accuracy: 53.8\n",
            "[127/150]: Training Loss: 1.0502580862778883, Training Accuracy: 70.108\n",
            "Validation Loss: 1.824654181798299, Validation Accuracy: 53.91\n",
            "[128/150]: Training Loss: 1.045825692323538, Training Accuracy: 69.762\n",
            "Validation Loss: 1.828839858373006, Validation Accuracy: 54.18\n",
            "[129/150]: Training Loss: 1.041814657358023, Training Accuracy: 70.04\n",
            "Validation Loss: 1.8183866739273071, Validation Accuracy: 53.89\n",
            "[130/150]: Training Loss: 1.0349373588195214, Training Accuracy: 70.334\n",
            "Validation Loss: 1.817612926165263, Validation Accuracy: 54.08\n",
            "[131/150]: Training Loss: 1.0304582439936125, Training Accuracy: 70.738\n",
            "Validation Loss: 1.8261488676071167, Validation Accuracy: 54.44\n",
            "[132/150]: Training Loss: 1.0328317513832679, Training Accuracy: 70.262\n",
            "Validation Loss: 1.8170503377914429, Validation Accuracy: 54.11\n",
            "[133/150]: Training Loss: 1.0127528584920442, Training Accuracy: 71.006\n",
            "Validation Loss: 1.823221206665039, Validation Accuracy: 54.24\n",
            "[134/150]: Training Loss: 1.0145881175994873, Training Accuracy: 70.876\n",
            "Validation Loss: 1.8177992900212605, Validation Accuracy: 54.42\n",
            "[135/150]: Training Loss: 1.0154722057856047, Training Accuracy: 70.856\n",
            "Validation Loss: 1.8151982227961223, Validation Accuracy: 54.36\n",
            "[136/150]: Training Loss: 1.0053791770568261, Training Accuracy: 70.962\n",
            "Validation Loss: 1.815675139427185, Validation Accuracy: 54.52\n",
            "[137/150]: Training Loss: 1.0002271212064302, Training Accuracy: 71.094\n",
            "Validation Loss: 1.8183471361796062, Validation Accuracy: 54.62\n",
            "[138/150]: Training Loss: 1.003823459148407, Training Accuracy: 71.112\n",
            "Validation Loss: 1.8167388836542766, Validation Accuracy: 54.32\n",
            "[139/150]: Training Loss: 1.0023488723314726, Training Accuracy: 71.28\n",
            "Validation Loss: 1.8161654869715373, Validation Accuracy: 54.5\n",
            "[140/150]: Training Loss: 0.996692391542288, Training Accuracy: 71.326\n",
            "Validation Loss: 1.8168489535649617, Validation Accuracy: 54.36\n",
            "[141/150]: Training Loss: 0.9932475502674396, Training Accuracy: 71.382\n",
            "Validation Loss: 1.8148254950841267, Validation Accuracy: 54.41\n",
            "[142/150]: Training Loss: 0.9950209443385785, Training Accuracy: 71.754\n",
            "Validation Loss: 1.8171526590983074, Validation Accuracy: 54.22\n",
            "[143/150]: Training Loss: 0.9949191717001108, Training Accuracy: 71.676\n",
            "Validation Loss: 1.8166076342264812, Validation Accuracy: 54.39\n",
            "[144/150]: Training Loss: 0.9898046300961421, Training Accuracy: 71.61\n",
            "Validation Loss: 1.8168938557306926, Validation Accuracy: 54.34\n",
            "[145/150]: Training Loss: 0.9879593665783222, Training Accuracy: 71.72\n",
            "Validation Loss: 1.8167452017466228, Validation Accuracy: 54.36\n",
            "[146/150]: Training Loss: 0.9881623249787551, Training Accuracy: 71.81\n",
            "Validation Loss: 1.8165217638015747, Validation Accuracy: 54.38\n",
            "[147/150]: Training Loss: 0.9901151886353126, Training Accuracy: 71.69\n",
            "Validation Loss: 1.81660795211792, Validation Accuracy: 54.48\n",
            "[148/150]: Training Loss: 0.9835631480583777, Training Accuracy: 71.944\n",
            "Validation Loss: 1.8168546358744304, Validation Accuracy: 54.43\n",
            "[149/150]: Training Loss: 0.9820910050318792, Training Accuracy: 72.082\n",
            "Validation Loss: 1.8165034850438435, Validation Accuracy: 54.45\n",
            "[150/150]: Training Loss: 0.9884678217080923, Training Accuracy: 71.694\n",
            "Validation Loss: 1.8165420691172283, Validation Accuracy: 54.48\n",
            "**********************************************************************\n",
            "Test Loss: 1.8165420691172283, Test Accuracy: 54.48\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>54.48</td></tr><tr><td>Test Loss</td><td>1.81654</td></tr><tr><td>Train Accuracy</td><td>71.694</td></tr><tr><td>Train Loss</td><td>0.98847</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=4096 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/fcrci1zx</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_024410-fcrci1zx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 8192, Learning rate: 16.970562748477143, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_030841-573n6uz6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6' target=\"_blank\">batch_size=8192 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.543815356034499, Training Accuracy: 1.934\n",
            "Validation Loss: 4.557299613952637, Validation Accuracy: 4.11\n",
            "[2/150]: Training Loss: 4.4831575613755446, Training Accuracy: 3.478\n",
            "Validation Loss: 4.407679557800293, Validation Accuracy: 3.73\n",
            "[3/150]: Training Loss: 4.383443318880522, Training Accuracy: 4.272\n",
            "Validation Loss: 4.315660317738851, Validation Accuracy: 4.56\n",
            "[4/150]: Training Loss: 4.753879473759578, Training Accuracy: 2.854\n",
            "Validation Loss: 4.617130438486735, Validation Accuracy: 1.55\n",
            "[5/150]: Training Loss: 4.834007776700533, Training Accuracy: 1.152\n",
            "Validation Loss: 4.623517354329427, Validation Accuracy: 1.21\n",
            "[6/150]: Training Loss: 4.748236729548528, Training Accuracy: 1.046\n",
            "Validation Loss: 4.616909344991048, Validation Accuracy: 1.0\n",
            "[7/150]: Training Loss: 4.6143631201524, Training Accuracy: 1.022\n",
            "Validation Loss: 4.6133114496866865, Validation Accuracy: 0.95\n",
            "[8/150]: Training Loss: 86586261001215.23, Training Accuracy: 1.076\n",
            "Validation Loss: 199153685277354.66, Validation Accuracy: 1.08\n",
            "[9/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[10/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[11/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[12/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[13/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[14/150]: Training Loss: inf, Training Accuracy: 1.0\n",
            "Validation Loss: inf, Validation Accuracy: 1.0\n",
            "[15/150]: Training Loss: inf, Training Accuracy: 1.006\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[16/150]: Training Loss: inf, Training Accuracy: 1.004\n",
            "Validation Loss: inf, Validation Accuracy: 0.99\n",
            "[17/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[18/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[19/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[20/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[21/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[22/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[23/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[24/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[25/150]: Training Loss: inf, Training Accuracy: 1.014\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[26/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[27/150]: Training Loss: inf, Training Accuracy: 0.998\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[28/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[29/150]: Training Loss: inf, Training Accuracy: 1.004\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[30/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[31/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[32/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: inf, Validation Accuracy: 1.02\n",
            "[33/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3707833002946235e+34, Validation Accuracy: 1.04\n",
            "[34/150]: Training Loss: 7.178876870446214e+34, Training Accuracy: 1.03\n",
            "Validation Loss: 6.908918765509594e+34, Validation Accuracy: 1.04\n",
            "[35/150]: Training Loss: 7.0300555279696e+34, Training Accuracy: 1.006\n",
            "Validation Loss: 6.850623178707614e+34, Validation Accuracy: 1.04\n",
            "[36/150]: Training Loss: 7.161850738321898e+34, Training Accuracy: 1.024\n",
            "Validation Loss: 6.997600003222513e+34, Validation Accuracy: 1.04\n",
            "[37/150]: Training Loss: 7.295138713909261e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.183176128279674e+34, Validation Accuracy: 1.04\n",
            "[38/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.320277327395843e+34, Validation Accuracy: 1.04\n",
            "[39/150]: Training Loss: 7.652232993118385e+34, Training Accuracy: 1.024\n",
            "Validation Loss: 7.385754947129741e+34, Validation Accuracy: 1.04\n",
            "[40/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.395794805907017e+34, Validation Accuracy: 1.04\n",
            "[41/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.389329127611166e+34, Validation Accuracy: 1.0\n",
            "[42/150]: Training Loss: 7.64618030431646e+34, Training Accuracy: 1.012\n",
            "Validation Loss: 7.36814170130946e+34, Validation Accuracy: 1.04\n",
            "[43/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.351131744934992e+34, Validation Accuracy: 1.04\n",
            "[44/150]: Training Loss: inf, Training Accuracy: 1.026\n",
            "Validation Loss: 7.339262375838323e+34, Validation Accuracy: 1.04\n",
            "[45/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.334463460011365e+34, Validation Accuracy: 1.04\n",
            "[46/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.33473861281743e+34, Validation Accuracy: 1.04\n",
            "[47/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.342335768309189e+34, Validation Accuracy: 1.04\n",
            "[48/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.343183509648092e+34, Validation Accuracy: 1.04\n",
            "[49/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.353242350172637e+34, Validation Accuracy: 1.04\n",
            "[50/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.35132750451987e+34, Validation Accuracy: 1.04\n",
            "[51/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.338507727590375e+34, Validation Accuracy: 1.04\n",
            "[52/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.345233868470492e+34, Validation Accuracy: 1.04\n",
            "[53/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.35075210998961e+34, Validation Accuracy: 1.0\n",
            "[54/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.35572186154199e+34, Validation Accuracy: 1.04\n",
            "[55/150]: Training Loss: 7.620612957278016e+34, Training Accuracy: 1.026\n",
            "Validation Loss: 7.362447672304763e+34, Validation Accuracy: 1.04\n",
            "[56/150]: Training Loss: 7.605070410587243e+34, Training Accuracy: 1.012\n",
            "Validation Loss: 7.363398575313606e+34, Validation Accuracy: 1.04\n",
            "[57/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3600082701926835e+34, Validation Accuracy: 1.04\n",
            "[58/150]: Training Loss: 7.453089464225746e+34, Training Accuracy: 1.02\n",
            "Validation Loss: 7.363201660318024e+34, Validation Accuracy: 1.04\n",
            "[59/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.36800305202506e+34, Validation Accuracy: 1.04\n",
            "[60/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.36741461785972e+34, Validation Accuracy: 1.04\n",
            "[61/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.367097540150991e+34, Validation Accuracy: 1.04\n",
            "[62/150]: Training Loss: inf, Training Accuracy: 1.024\n",
            "Validation Loss: 7.360266091838198e+34, Validation Accuracy: 1.04\n",
            "[63/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.357579927011621e+34, Validation Accuracy: 1.04\n",
            "[64/150]: Training Loss: inf, Training Accuracy: 1.002\n",
            "Validation Loss: 7.3600130568941685e+34, Validation Accuracy: 1.04\n",
            "[65/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.361191245694225e+34, Validation Accuracy: 1.04\n",
            "[66/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.358085336664993e+34, Validation Accuracy: 1.04\n",
            "[67/150]: Training Loss: 7.58769795997563e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.343106922424328e+34, Validation Accuracy: 1.04\n",
            "[68/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.340167062419033e+34, Validation Accuracy: 1.04\n",
            "[69/150]: Training Loss: 7.539564832453689e+34, Training Accuracy: 1.01\n",
            "Validation Loss: 7.3399107263015645e+34, Validation Accuracy: 1.04\n",
            "[70/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.343239299479195e+34, Validation Accuracy: 1.04\n",
            "[71/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.344283460637665e+34, Validation Accuracy: 1.04\n",
            "[72/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3470638739659e+34, Validation Accuracy: 1.04\n",
            "[73/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.337876213111667e+34, Validation Accuracy: 1.04\n",
            "[74/150]: Training Loss: 7.520041527682127e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.339984672586578e+34, Validation Accuracy: 1.04\n",
            "[75/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.341706399593216e+34, Validation Accuracy: 1.04\n",
            "[76/150]: Training Loss: 7.50774223153243e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.344978357646383e+34, Validation Accuracy: 1.04\n",
            "[77/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.34611412636776e+34, Validation Accuracy: 1.04\n",
            "[78/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.34531260145699e+34, Validation Accuracy: 1.04\n",
            "[79/150]: Training Loss: 7.570326728258824e+34, Training Accuracy: 1.008\n",
            "Validation Loss: 7.35118538900336e+34, Validation Accuracy: 1.04\n",
            "[80/150]: Training Loss: 7.392040813047783e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.351906530340912e+34, Validation Accuracy: 1.04\n",
            "[81/150]: Training Loss: 7.55008671334021e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.357009484241519e+34, Validation Accuracy: 1.04\n",
            "[82/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.355688354631593e+34, Validation Accuracy: 1.04\n",
            "[83/150]: Training Loss: inf, Training Accuracy: 1.024\n",
            "Validation Loss: 7.351709120169314e+34, Validation Accuracy: 1.04\n",
            "[84/150]: Training Loss: 7.518704819072938e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.352724726177544e+34, Validation Accuracy: 1.04\n",
            "[85/150]: Training Loss: 7.480363492538042e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.353025628136426e+34, Validation Accuracy: 1.04\n",
            "[86/150]: Training Loss: 7.609761708160119e+34, Training Accuracy: 1.02\n",
            "Validation Loss: 7.355622331162831e+34, Validation Accuracy: 1.04\n",
            "[87/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.351417956672074e+34, Validation Accuracy: 1.04\n",
            "[88/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.3432506885275565e+34, Validation Accuracy: 1.04\n",
            "[89/150]: Training Loss: 7.571858879032369e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.342947970923284e+34, Validation Accuracy: 1.04\n",
            "[90/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.341380243657533e+34, Validation Accuracy: 1.04\n",
            "[91/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3435405315554215e+34, Validation Accuracy: 1.04\n",
            "[92/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.347343978532122e+34, Validation Accuracy: 1.04\n",
            "[93/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.349835869301868e+34, Validation Accuracy: 1.04\n",
            "[94/150]: Training Loss: 7.553363102583877e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.350179851574117e+34, Validation Accuracy: 1.04\n",
            "[95/150]: Training Loss: 7.518843468357338e+34, Training Accuracy: 1.006\n",
            "Validation Loss: 7.349381297719442e+34, Validation Accuracy: 1.04\n",
            "[96/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.350629966572401e+34, Validation Accuracy: 1.04\n",
            "[97/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.350986328245043e+34, Validation Accuracy: 1.04\n",
            "[98/150]: Training Loss: inf, Training Accuracy: 1.02\n",
            "Validation Loss: 7.349232414797383e+34, Validation Accuracy: 1.04\n",
            "[99/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.3498634341000755e+34, Validation Accuracy: 1.04\n",
            "[100/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.346068900291658e+34, Validation Accuracy: 1.04\n",
            "[101/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.343747350071318e+34, Validation Accuracy: 1.04\n",
            "[102/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.343838462458209e+34, Validation Accuracy: 1.04\n",
            "[103/150]: Training Loss: 7.587162712793105e+34, Training Accuracy: 1.036\n",
            "Validation Loss: 7.34444604342949e+34, Validation Accuracy: 1.04\n",
            "[104/150]: Training Loss: 7.563224723389138e+34, Training Accuracy: 1.024\n",
            "Validation Loss: 7.3440896817568485e+34, Validation Accuracy: 1.04\n",
            "[105/150]: Training Loss: 7.502001541710868e+34, Training Accuracy: 1.012\n",
            "Validation Loss: 7.343770788402729e+34, Validation Accuracy: 1.04\n",
            "[106/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.344186901314601e+34, Validation Accuracy: 1.04\n",
            "[107/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.347785180362123e+34, Validation Accuracy: 1.04\n",
            "[108/150]: Training Loss: 7.541743759284682e+34, Training Accuracy: 1.008\n",
            "Validation Loss: 7.349218714927616e+34, Validation Accuracy: 1.04\n",
            "[109/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.350196357441308e+34, Validation Accuracy: 1.04\n",
            "[110/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.35085609695291e+34, Validation Accuracy: 1.04\n",
            "[111/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.3504860354105e+34, Validation Accuracy: 1.04\n",
            "[112/150]: Training Loss: inf, Training Accuracy: 1.006\n",
            "Validation Loss: 7.350065630973159e+34, Validation Accuracy: 1.04\n",
            "[113/150]: Training Loss: 7.572177010577234e+34, Training Accuracy: 1.014\n",
            "Validation Loss: 7.34878494073785e+34, Validation Accuracy: 1.04\n",
            "[114/150]: Training Loss: 7.602740036076367e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.345721781904642e+34, Validation Accuracy: 1.04\n",
            "[115/150]: Training Loss: 7.637903627666111e+34, Training Accuracy: 1.014\n",
            "Validation Loss: 7.342419287997172e+34, Validation Accuracy: 1.04\n",
            "[116/150]: Training Loss: 7.578869428701e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.340205025913571e+34, Validation Accuracy: 1.04\n",
            "[117/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.33890419852029e+34, Validation Accuracy: 1.04\n",
            "[118/150]: Training Loss: 7.53878203535377e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.3400515213487e+34, Validation Accuracy: 1.04\n",
            "[119/150]: Training Loss: 7.55180291722975e+34, Training Accuracy: 1.006\n",
            "Validation Loss: 7.341673717976179e+34, Validation Accuracy: 1.04\n",
            "[120/150]: Training Loss: 7.52984426063198e+34, Training Accuracy: 1.014\n",
            "Validation Loss: 7.341424314322931e+34, Validation Accuracy: 1.04\n",
            "[121/150]: Training Loss: inf, Training Accuracy: 1.008\n",
            "Validation Loss: 7.342221382649559e+34, Validation Accuracy: 1.04\n",
            "[122/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.341639550831095e+34, Validation Accuracy: 1.04\n",
            "[123/150]: Training Loss: 7.564221817432473e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.34171366217478e+34, Validation Accuracy: 1.04\n",
            "[124/150]: Training Loss: inf, Training Accuracy: 1.012\n",
            "Validation Loss: 7.341365883553077e+34, Validation Accuracy: 1.04\n",
            "[125/150]: Training Loss: 7.467476193914916e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.341843068173553e+34, Validation Accuracy: 1.04\n",
            "[126/150]: Training Loss: 7.557965611288089e+34, Training Accuracy: 1.018\n",
            "Validation Loss: 7.34209494770688e+34, Validation Accuracy: 1.04\n",
            "[127/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.342893501561555e+34, Validation Accuracy: 1.04\n",
            "[128/150]: Training Loss: inf, Training Accuracy: 1.014\n",
            "Validation Loss: 7.343795217086171e+34, Validation Accuracy: 1.04\n",
            "[129/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.3440480869715285e+34, Validation Accuracy: 1.04\n",
            "[130/150]: Training Loss: inf, Training Accuracy: 1.026\n",
            "Validation Loss: 7.343806276017188e+34, Validation Accuracy: 1.04\n",
            "[131/150]: Training Loss: 7.511219776509865e+34, Training Accuracy: 1.006\n",
            "Validation Loss: 7.343705425168654e+34, Validation Accuracy: 1.04\n",
            "[132/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.343869988664544e+34, Validation Accuracy: 1.04\n",
            "[133/150]: Training Loss: 7.572846983726496e+34, Training Accuracy: 1.004\n",
            "Validation Loss: 7.34368512295201e+34, Validation Accuracy: 1.04\n",
            "[134/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.3437027842299035e+34, Validation Accuracy: 1.04\n",
            "[135/150]: Training Loss: inf, Training Accuracy: 1.006\n",
            "Validation Loss: 7.34337316206211e+34, Validation Accuracy: 1.04\n",
            "[136/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.343067143284399e+34, Validation Accuracy: 1.04\n",
            "[137/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.343213385267707e+34, Validation Accuracy: 1.04\n",
            "[138/150]: Training Loss: 7.591718865404154e+34, Training Accuracy: 1.022\n",
            "Validation Loss: 7.343415086964774e+34, Validation Accuracy: 1.04\n",
            "[139/150]: Training Loss: inf, Training Accuracy: 1.024\n",
            "Validation Loss: 7.343418718255556e+34, Validation Accuracy: 1.04\n",
            "[140/150]: Training Loss: 7.5901811534230485e+34, Training Accuracy: 1.016\n",
            "Validation Loss: 7.343325625164602e+34, Validation Accuracy: 1.04\n",
            "[141/150]: Training Loss: inf, Training Accuracy: 1.022\n",
            "Validation Loss: 7.343275777445687e+34, Validation Accuracy: 1.04\n",
            "[142/150]: Training Loss: inf, Training Accuracy: 1.014\n",
            "Validation Loss: 7.3432812243818595e+34, Validation Accuracy: 1.04\n",
            "[143/150]: Training Loss: 7.521443789977704e+34, Training Accuracy: 1.008\n",
            "Validation Loss: 7.343387192049222e+34, Validation Accuracy: 1.04\n",
            "[144/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.343500752415492e+34, Validation Accuracy: 1.04\n",
            "[145/150]: Training Loss: 7.489635587251364e+34, Training Accuracy: 1.014\n",
            "Validation Loss: 7.343563144593472e+34, Validation Accuracy: 1.04\n",
            "[146/150]: Training Loss: inf, Training Accuracy: 1.018\n",
            "Validation Loss: 7.343603253850745e+34, Validation Accuracy: 1.04\n",
            "[147/150]: Training Loss: 7.537443193678667e+34, Training Accuracy: 1.026\n",
            "Validation Loss: 7.343630983707625e+34, Validation Accuracy: 1.04\n",
            "[148/150]: Training Loss: 7.421953062990879e+34, Training Accuracy: 1.024\n",
            "Validation Loss: 7.343639566758564e+34, Validation Accuracy: 1.04\n",
            "[149/150]: Training Loss: inf, Training Accuracy: 1.016\n",
            "Validation Loss: 7.343644683577394e+34, Validation Accuracy: 1.04\n",
            "[150/150]: Training Loss: inf, Training Accuracy: 1.01\n",
            "Validation Loss: 7.343644848636065e+34, Validation Accuracy: 1.04\n",
            "**********************************************************************\n",
            "Test Loss: 7.343644848636065e+34, Test Accuracy: 1.04\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td>                        </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.04</td></tr><tr><td>Test Loss</td><td>7.343644848636065e+34</td></tr><tr><td>Train Accuracy</td><td>1.01</td></tr><tr><td>Train Loss</td><td>inf</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=8192 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/573n6uz6</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_030841-573n6uz6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 16384, Learning rate: 24.0, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_033254-cneyt8y0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0' target=\"_blank\">batch_size=16384 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.5802256144010105, Training Accuracy: 1.524\n",
            "Validation Loss: 4.505903720855713, Validation Accuracy: 3.24\n",
            "[2/150]: Training Loss: 4.487952305720403, Training Accuracy: 2.782\n",
            "Validation Loss: 4.534458001454671, Validation Accuracy: 1.82\n",
            "[3/150]: Training Loss: 4.498784615443303, Training Accuracy: 2.232\n",
            "Validation Loss: 4.477282365163167, Validation Accuracy: 2.08\n",
            "[4/150]: Training Loss: 4.4941191306481, Training Accuracy: 2.178\n",
            "Validation Loss: 4.612959861755371, Validation Accuracy: 1.29\n",
            "[5/150]: Training Loss: 4.5912819642287035, Training Accuracy: 1.504\n",
            "Validation Loss: 5.598563989003499, Validation Accuracy: 2.19\n",
            "[6/150]: Training Loss: 5.028410141284649, Training Accuracy: 1.632\n",
            "Validation Loss: 7.806542873382568, Validation Accuracy: 1.07\n",
            "[7/150]: Training Loss: 14089.29747926272, Training Accuracy: 1.044\n",
            "Validation Loss: 7.282219409942627, Validation Accuracy: 0.93\n",
            "[8/150]: Training Loss: 461533512.8043683, Training Accuracy: 0.978\n",
            "Validation Loss: 145186464.0, Validation Accuracy: 0.98\n",
            "[9/150]: Training Loss: 1.1580541048823784e+29, Training Accuracy: 0.952\n",
            "Validation Loss: 2.748474842936338e+29, Validation Accuracy: 1.0\n",
            "[10/150]: Training Loss: 2.8978003478935013e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7484746540416787e+29, Validation Accuracy: 1.0\n",
            "[11/150]: Training Loss: 2.828666588105865e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.748474402182133e+29, Validation Accuracy: 1.0\n",
            "[12/150]: Training Loss: 2.8943495766660634e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7484738354981552e+29, Validation Accuracy: 1.0\n",
            "[13/150]: Training Loss: 2.848512485818888e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.748472828059972e+29, Validation Accuracy: 1.0\n",
            "[14/150]: Training Loss: 2.8171856724252214e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.748468672377467e+29, Validation Accuracy: 1.0\n",
            "[15/150]: Training Loss: 2.9237725789099303e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7484595424689335e+29, Validation Accuracy: 1.0\n",
            "[16/150]: Training Loss: 2.8024521069540443e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.748430578621172e+29, Validation Accuracy: 1.0\n",
            "[17/150]: Training Loss: 2.818569224092193e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7483436870778873e+29, Validation Accuracy: 1.0\n",
            "[18/150]: Training Loss: 2.8163175416317275e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7480828865182598e+29, Validation Accuracy: 1.0\n",
            "[19/150]: Training Loss: 2.818046581630586e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7474053213752977e+29, Validation Accuracy: 1.0\n",
            "[20/150]: Training Loss: 2.8566013910414994e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7454263979594298e+29, Validation Accuracy: 1.0\n",
            "[21/150]: Training Loss: 2.8411686828451232e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.7395975495271803e+29, Validation Accuracy: 1.0\n",
            "[22/150]: Training Loss: 2.8607326044228643e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.722675988156443e+29, Validation Accuracy: 1.0\n",
            "[23/150]: Training Loss: 2.7987878847615806e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.6741031117411477e+29, Validation Accuracy: 1.0\n",
            "[24/150]: Training Loss: 2.723279453314973e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.535316349808136e+29, Validation Accuracy: 1.0\n",
            "[25/150]: Training Loss: 2.573586814635347e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 2.1297902431238725e+29, Validation Accuracy: 1.0\n",
            "[26/150]: Training Loss: 2.051805262233517e+29, Training Accuracy: 1.0\n",
            "Validation Loss: 1.0476803956799425e+29, Validation Accuracy: 1.0\n",
            "[27/150]: Training Loss: 8.450376531000046e+28, Training Accuracy: 1.0\n",
            "Validation Loss: 9.264404581011073e+18, Validation Accuracy: 1.0\n",
            "[28/150]: Training Loss: nan, Training Accuracy: 1.044\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[29/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[30/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[31/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[32/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[33/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[34/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[35/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[36/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[37/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[38/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[39/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[40/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[41/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[42/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[43/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[44/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[45/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[46/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[47/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[48/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[49/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[50/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[51/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[52/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[53/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[54/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[55/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[56/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[57/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[58/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[59/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[60/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[61/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[62/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[63/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[64/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[65/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[66/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[67/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[68/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[69/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[70/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[71/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[72/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[73/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[74/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[75/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[76/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[77/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[78/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[79/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[80/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[81/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[82/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[83/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[84/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[85/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[86/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[87/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[88/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[89/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[90/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[91/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[92/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[93/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[94/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[95/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[96/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[97/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[98/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[99/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[100/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[101/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[102/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[103/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[104/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[105/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[106/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[107/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[108/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[109/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[110/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[111/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[112/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[113/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[114/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[115/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[116/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[117/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[118/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[119/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[120/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[121/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[122/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[123/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[124/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[125/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[126/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[127/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[128/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[129/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[130/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[131/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[132/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[133/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[134/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[135/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[136/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[137/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[138/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[139/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[140/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[141/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[142/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[143/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[144/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[145/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[146/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[147/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[148/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[149/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[150/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "**********************************************************************\n",
            "Test Loss: nan, Test Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td>                                 </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.0</td></tr><tr><td>Test Loss</td><td>nan</td></tr><tr><td>Train Accuracy</td><td>1.0</td></tr><tr><td>Train Loss</td><td>nan</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=16384 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/cneyt8y0</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_033254-cneyt8y0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 32768, Learning rate: 33.941125496954285, Weight decay: 0.001\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_035739-zqj7yms0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0' target=\"_blank\">batch_size=32768 learning_rate=1.5 weight_decay=0.001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.600700745215783, Training Accuracy: 1.18\n",
            "Validation Loss: 4.5345486005147295, Validation Accuracy: 2.25\n",
            "[2/150]: Training Loss: 4.525542259216309, Training Accuracy: 2.194\n",
            "Validation Loss: 4.480770270029704, Validation Accuracy: 3.63\n",
            "[3/150]: Training Loss: 4.467245468726525, Training Accuracy: 3.348\n",
            "Validation Loss: 4.682004292805989, Validation Accuracy: 2.02\n",
            "[4/150]: Training Loss: 4.584587500645564, Training Accuracy: 2.316\n",
            "Validation Loss: 4.589324951171875, Validation Accuracy: 1.07\n",
            "[5/150]: Training Loss: 4.593533919407771, Training Accuracy: 1.128\n",
            "Validation Loss: 4.565906047821045, Validation Accuracy: 2.24\n",
            "[6/150]: Training Loss: 4.610266098609338, Training Accuracy: 1.65\n",
            "Validation Loss: 5.121236960093181, Validation Accuracy: 1.01\n",
            "[7/150]: Training Loss: 5.261244590465839, Training Accuracy: 1.226\n",
            "Validation Loss: 7.05544392267863, Validation Accuracy: 1.19\n",
            "[8/150]: Training Loss: 6.352289456587571, Training Accuracy: 1.176\n",
            "Validation Loss: 22.820095698038738, Validation Accuracy: 0.83\n",
            "[9/150]: Training Loss: 33.29087433448205, Training Accuracy: 0.906\n",
            "Validation Loss: 54.22376505533854, Validation Accuracy: 1.0\n",
            "[10/150]: Training Loss: 35.99252113929162, Training Accuracy: 0.988\n",
            "Validation Loss: 69.11057027180989, Validation Accuracy: 0.95\n",
            "[11/150]: Training Loss: 76.55324378380409, Training Accuracy: 1.196\n",
            "Validation Loss: 7.983708381652832, Validation Accuracy: 1.06\n",
            "[12/150]: Training Loss: 58.39160236945519, Training Accuracy: 1.032\n",
            "Validation Loss: 164.86418660481772, Validation Accuracy: 0.92\n",
            "[13/150]: Training Loss: 131.58557803814227, Training Accuracy: 0.95\n",
            "Validation Loss: 281.80524190266925, Validation Accuracy: 1.0\n",
            "[14/150]: Training Loss: 178.63248865421002, Training Accuracy: 0.962\n",
            "Validation Loss: 7817948401325397.0, Validation Accuracy: 1.39\n",
            "[15/150]: Training Loss: 4967409836682319.0, Training Accuracy: 1.34\n",
            "Validation Loss: 2.8247563537114726e+17, Validation Accuracy: 1.0\n",
            "[16/150]: Training Loss: 1.7259852552660077e+17, Training Accuracy: 0.982\n",
            "Validation Loss: 7061819945735509.0, Validation Accuracy: 0.96\n",
            "[17/150]: Training Loss: 7.113184439761936e+16, Training Accuracy: 0.924\n",
            "Validation Loss: 64496.6328125, Validation Accuracy: 1.0\n",
            "[18/150]: Training Loss: nan, Training Accuracy: 1.014\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[19/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[20/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[21/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[22/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[23/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[24/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[25/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[26/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[27/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[28/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[29/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[30/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[31/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[32/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[33/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[34/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[35/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[36/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[37/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[38/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[39/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[40/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[41/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[42/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[43/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[44/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[45/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[46/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[47/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[48/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[49/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[50/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[51/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[52/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[53/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[54/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[55/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[56/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[57/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[58/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[59/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[60/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[61/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[62/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[63/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[64/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[65/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[66/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[67/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[68/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[69/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[70/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[71/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[72/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[73/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[74/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[75/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[76/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[77/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[78/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[79/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[80/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[81/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[82/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[83/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[84/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[85/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[86/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[87/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[88/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[89/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[90/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[91/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[92/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[93/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[94/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[95/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[96/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[97/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[98/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[99/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[100/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[101/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[102/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[103/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[104/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[105/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[106/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[107/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[108/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[109/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[110/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[111/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[112/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[113/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[114/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[115/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[116/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[117/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[118/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[119/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[120/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[121/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[122/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[123/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[124/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[125/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[126/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[127/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[128/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[129/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[130/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[131/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[132/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[133/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[134/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[135/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[136/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[137/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[138/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[139/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[140/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[141/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[142/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[143/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[144/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[145/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[146/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[147/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[148/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[149/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "[150/150]: Training Loss: nan, Training Accuracy: 1.0\n",
            "Validation Loss: nan, Validation Accuracy: 1.0\n",
            "**********************************************************************\n",
            "Test Loss: nan, Test Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td>                                   </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>1.0</td></tr><tr><td>Test Loss</td><td>nan</td></tr><tr><td>Train Accuracy</td><td>1.0</td></tr><tr><td>Train Loss</td><td>nan</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=32768 learning_rate=1.5 weight_decay=0.001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches/runs/zqj7yms0</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LARS_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_035739-zqj7yms0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# check the lr updates from paper 18\n",
        "\n",
        "num_epochs = 150\n",
        "lr = 1.5\n",
        "wd = 1e-03\n",
        "batch_sizes = [512, 1024, 2048, 4096, 8192, 16384 , 32768]\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "  # Root square scale-up of learning rate\n",
        "  k = (batch_size / 64.0) ** 0.5\n",
        "  print('='*50)\n",
        "  print(f'Batch size: {batch_size}, Learning rate: {lr*k}, Weight decay: {wd}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {\n",
        "    'batch_size': batch_size,\n",
        "    'learning_rate': lr*k,\n",
        "    'weight_decay' : wd\n",
        "  }\n",
        "  if batch_size <= 4096:\n",
        "  \n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= batch_size)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LARS(model.parameters(), lr= lr*k, weight_decay=wd, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LARS_Large_Batches',\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True\n",
        "    )\n",
        "  else:\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= 4096)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LARS(model.parameters(), lr= lr*k, weight_decay=wd, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    accumulation_steps = batch_size // 4096\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LARS_Large_Batches',\n",
        "        accumulation_steps=accumulation_steps,\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ftxikW3Lr2ya"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.0009 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_045009-kw5xm0f9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9' target=\"_blank\">learning_rate=0.0009 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.366185345458985, Training Accuracy: 3.775\n",
            "Validation Loss: 4.140061999582182, Validation Accuracy: 6.19\n",
            "[2/150]: Training Loss: 3.9872212955474855, Training Accuracy: 9.2825\n",
            "Validation Loss: 3.9175391182018693, Validation Accuracy: 10.13\n",
            "[3/150]: Training Loss: 3.7751855819702147, Training Accuracy: 12.73\n",
            "Validation Loss: 3.754828929901123, Validation Accuracy: 13.44\n",
            "[4/150]: Training Loss: 3.6031822479248046, Training Accuracy: 15.7675\n",
            "Validation Loss: 3.5609892857302525, Validation Accuracy: 16.33\n",
            "[5/150]: Training Loss: 3.4478089206695555, Training Accuracy: 18.66\n",
            "Validation Loss: 3.4119288389849816, Validation Accuracy: 19.45\n",
            "[6/150]: Training Loss: 3.3203197284698485, Training Accuracy: 20.7075\n",
            "Validation Loss: 3.349654642639646, Validation Accuracy: 20.47\n",
            "[7/150]: Training Loss: 3.20732957611084, Training Accuracy: 22.7475\n",
            "Validation Loss: 3.2386614349996967, Validation Accuracy: 22.79\n",
            "[8/150]: Training Loss: 3.115479539489746, Training Accuracy: 24.5875\n",
            "Validation Loss: 3.1584747019846726, Validation Accuracy: 24.53\n",
            "[9/150]: Training Loss: 3.0234272315979003, Training Accuracy: 26.28\n",
            "Validation Loss: 3.0975672788680737, Validation Accuracy: 24.97\n",
            "[10/150]: Training Loss: 2.9408509815216064, Training Accuracy: 27.665\n",
            "Validation Loss: 3.0618672537955507, Validation Accuracy: 25.33\n",
            "[11/150]: Training Loss: 2.862503829956055, Training Accuracy: 29.3325\n",
            "Validation Loss: 2.9638838965422027, Validation Accuracy: 27.41\n",
            "[12/150]: Training Loss: 2.790407749938965, Training Accuracy: 30.48\n",
            "Validation Loss: 2.937404105617742, Validation Accuracy: 28.32\n",
            "[13/150]: Training Loss: 2.724927402114868, Training Accuracy: 31.9425\n",
            "Validation Loss: 2.9041963838467932, Validation Accuracy: 28.5\n",
            "[14/150]: Training Loss: 2.659171875, Training Accuracy: 33.2275\n",
            "Validation Loss: 2.8527053769227044, Validation Accuracy: 29.6\n",
            "[15/150]: Training Loss: 2.604087335395813, Training Accuracy: 34.1075\n",
            "Validation Loss: 2.8513991179739593, Validation Accuracy: 30.15\n",
            "[16/150]: Training Loss: 2.5427039728164673, Training Accuracy: 35.57\n",
            "Validation Loss: 2.8172519510718668, Validation Accuracy: 30.73\n",
            "[17/150]: Training Loss: 2.4821195234298705, Training Accuracy: 36.73\n",
            "Validation Loss: 2.826693563704278, Validation Accuracy: 31.28\n",
            "[18/150]: Training Loss: 2.4249122804641723, Training Accuracy: 37.97\n",
            "Validation Loss: 2.78142261353268, Validation Accuracy: 32.01\n",
            "[19/150]: Training Loss: 2.366035011482239, Training Accuracy: 39.02\n",
            "Validation Loss: 2.7907780021618884, Validation Accuracy: 31.84\n",
            "[20/150]: Training Loss: 2.3027314464569093, Training Accuracy: 40.4925\n",
            "Validation Loss: 2.740103484718663, Validation Accuracy: 32.92\n",
            "[21/150]: Training Loss: 2.24520486240387, Training Accuracy: 41.56\n",
            "Validation Loss: 2.720843028111063, Validation Accuracy: 33.71\n",
            "[22/150]: Training Loss: 2.194983146095276, Training Accuracy: 42.9375\n",
            "Validation Loss: 2.7611854592705987, Validation Accuracy: 32.67\n",
            "[23/150]: Training Loss: 2.138135533905029, Training Accuracy: 44.1975\n",
            "Validation Loss: 2.7068749218230037, Validation Accuracy: 33.35\n",
            "[24/150]: Training Loss: 2.078735979270935, Training Accuracy: 45.3275\n",
            "Validation Loss: 2.6949741673317686, Validation Accuracy: 34.58\n",
            "[25/150]: Training Loss: 2.0237128454208375, Training Accuracy: 46.48\n",
            "Validation Loss: 2.700145792809262, Validation Accuracy: 33.95\n",
            "[26/150]: Training Loss: 1.9667144546508788, Training Accuracy: 47.8375\n",
            "Validation Loss: 2.749180986623096, Validation Accuracy: 34.02\n",
            "[27/150]: Training Loss: 1.9086554452896118, Training Accuracy: 49.35\n",
            "Validation Loss: 2.719582610828861, Validation Accuracy: 34.45\n",
            "[28/150]: Training Loss: 1.845149391746521, Training Accuracy: 50.64\n",
            "Validation Loss: 2.735584658422288, Validation Accuracy: 34.52\n",
            "[29/150]: Training Loss: 1.7947315074920653, Training Accuracy: 51.665\n",
            "Validation Loss: 2.7848551835224127, Validation Accuracy: 33.98\n",
            "[30/150]: Training Loss: 1.7361212677001954, Training Accuracy: 53.24\n",
            "Validation Loss: 2.7989868426778513, Validation Accuracy: 34.65\n",
            "[31/150]: Training Loss: 1.675447853088379, Training Accuracy: 54.63\n",
            "Validation Loss: 2.805454480420252, Validation Accuracy: 35.16\n",
            "[32/150]: Training Loss: 1.6181456073760987, Training Accuracy: 55.7275\n",
            "Validation Loss: 2.825571666097945, Validation Accuracy: 35.24\n",
            "[33/150]: Training Loss: 1.5572484771728516, Training Accuracy: 57.33\n",
            "Validation Loss: 2.895953078178843, Validation Accuracy: 34.87\n",
            "[34/150]: Training Loss: 1.493730352783203, Training Accuracy: 58.9\n",
            "Validation Loss: 2.8819164500874317, Validation Accuracy: 34.54\n",
            "[35/150]: Training Loss: 1.4407835181236268, Training Accuracy: 60.1325\n",
            "Validation Loss: 2.9298907556351583, Validation Accuracy: 35.13\n",
            "[36/150]: Training Loss: 1.378046295261383, Training Accuracy: 61.6725\n",
            "Validation Loss: 2.9877944615236514, Validation Accuracy: 34.88\n",
            "[37/150]: Training Loss: 1.3214307213783265, Training Accuracy: 63.3825\n",
            "Validation Loss: 3.01296287281498, Validation Accuracy: 34.97\n",
            "[38/150]: Training Loss: 1.2566315541267394, Training Accuracy: 64.46\n",
            "Validation Loss: 3.1075331208052908, Validation Accuracy: 34.96\n",
            "[39/150]: Training Loss: 1.2033088095664979, Training Accuracy: 66.1\n",
            "Validation Loss: 3.1721465040923684, Validation Accuracy: 34.01\n",
            "[40/150]: Training Loss: 1.1419879460334779, Training Accuracy: 67.7475\n",
            "Validation Loss: 3.260179937265481, Validation Accuracy: 34.51\n",
            "[41/150]: Training Loss: 1.0848600325584412, Training Accuracy: 69.2325\n",
            "Validation Loss: 3.281414782165722, Validation Accuracy: 34.07\n",
            "[42/150]: Training Loss: 1.0264957049369812, Training Accuracy: 70.73\n",
            "Validation Loss: 3.4151626255861514, Validation Accuracy: 34.15\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 17.67243060336751, Test Accuracy: 14.49\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>14.49</td></tr><tr><td>Test Loss</td><td>17.67243</td></tr><tr><td>Train Accuracy</td><td>70.73</td></tr><tr><td>Train Loss</td><td>1.0265</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.0009 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/kw5xm0f9</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_045009-kw5xm0f9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.00095 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_045608-cwudz2wj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj' target=\"_blank\">learning_rate=0.00095 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.36243955039978, Training Accuracy: 3.8475\n",
            "Validation Loss: 4.152171432592307, Validation Accuracy: 6.23\n",
            "[2/150]: Training Loss: 3.988539717102051, Training Accuracy: 9.035\n",
            "Validation Loss: 3.8899211443153914, Validation Accuracy: 10.42\n",
            "[3/150]: Training Loss: 3.784552033996582, Training Accuracy: 12.44\n",
            "Validation Loss: 3.716481655266634, Validation Accuracy: 13.63\n",
            "[4/150]: Training Loss: 3.6115343948364256, Training Accuracy: 15.38\n",
            "Validation Loss: 3.575136796684022, Validation Accuracy: 15.25\n",
            "[5/150]: Training Loss: 3.464410584640503, Training Accuracy: 18.1625\n",
            "Validation Loss: 3.448981456695848, Validation Accuracy: 17.92\n",
            "[6/150]: Training Loss: 3.340861389923096, Training Accuracy: 20.2875\n",
            "Validation Loss: 3.3443257702384024, Validation Accuracy: 19.79\n",
            "[7/150]: Training Loss: 3.240240854263306, Training Accuracy: 22.0975\n",
            "Validation Loss: 3.2861741910314866, Validation Accuracy: 21.07\n",
            "[8/150]: Training Loss: 3.1493673683166503, Training Accuracy: 23.8725\n",
            "Validation Loss: 3.1970221844448408, Validation Accuracy: 22.88\n",
            "[9/150]: Training Loss: 3.058048994064331, Training Accuracy: 25.4\n",
            "Validation Loss: 3.149918096080707, Validation Accuracy: 23.89\n",
            "[10/150]: Training Loss: 2.980970076370239, Training Accuracy: 26.93\n",
            "Validation Loss: 3.05195216008812, Validation Accuracy: 25.52\n",
            "[11/150]: Training Loss: 2.8978964962005613, Training Accuracy: 28.4275\n",
            "Validation Loss: 3.035321120243923, Validation Accuracy: 26.01\n",
            "[12/150]: Training Loss: 2.828321962738037, Training Accuracy: 29.7825\n",
            "Validation Loss: 3.033455335410537, Validation Accuracy: 26.14\n",
            "[13/150]: Training Loss: 2.764588646316528, Training Accuracy: 31.0525\n",
            "Validation Loss: 2.9178414527018357, Validation Accuracy: 28.5\n",
            "[14/150]: Training Loss: 2.6963481563568115, Training Accuracy: 32.6275\n",
            "Validation Loss: 2.8597835826266342, Validation Accuracy: 29.48\n",
            "[15/150]: Training Loss: 2.6279782932281495, Training Accuracy: 33.5125\n",
            "Validation Loss: 2.820923639710542, Validation Accuracy: 30.76\n",
            "[16/150]: Training Loss: 2.567752400970459, Training Accuracy: 35.0025\n",
            "Validation Loss: 2.8166574232137886, Validation Accuracy: 30.67\n",
            "[17/150]: Training Loss: 2.5122242719650267, Training Accuracy: 36.11\n",
            "Validation Loss: 2.77275866611748, Validation Accuracy: 31.51\n",
            "[18/150]: Training Loss: 2.448093600654602, Training Accuracy: 37.37\n",
            "Validation Loss: 2.7573567454222663, Validation Accuracy: 31.87\n",
            "[19/150]: Training Loss: 2.390553472328186, Training Accuracy: 38.6925\n",
            "Validation Loss: 2.779167422823086, Validation Accuracy: 31.58\n",
            "[20/150]: Training Loss: 2.329650112724304, Training Accuracy: 39.8\n",
            "Validation Loss: 2.7296412940237933, Validation Accuracy: 32.77\n",
            "[21/150]: Training Loss: 2.2680850820541383, Training Accuracy: 41.2075\n",
            "Validation Loss: 2.7711537537301423, Validation Accuracy: 32.31\n",
            "[22/150]: Training Loss: 2.205855764579773, Training Accuracy: 42.4775\n",
            "Validation Loss: 2.710599136959975, Validation Accuracy: 33.08\n",
            "[23/150]: Training Loss: 2.153862490653992, Training Accuracy: 43.845\n",
            "Validation Loss: 2.6846724543601845, Validation Accuracy: 34.22\n",
            "[24/150]: Training Loss: 2.0879069725036623, Training Accuracy: 45.07\n",
            "Validation Loss: 2.6652191763470885, Validation Accuracy: 34.62\n",
            "[25/150]: Training Loss: 2.027932558250427, Training Accuracy: 46.0825\n",
            "Validation Loss: 2.7143203483265674, Validation Accuracy: 34.04\n",
            "[26/150]: Training Loss: 1.9607762811660767, Training Accuracy: 47.975\n",
            "Validation Loss: 2.6897484147624606, Validation Accuracy: 34.96\n",
            "[27/150]: Training Loss: 1.9115407361984253, Training Accuracy: 48.8575\n",
            "Validation Loss: 2.7138490722437574, Validation Accuracy: 34.41\n",
            "[28/150]: Training Loss: 1.8502281684875488, Training Accuracy: 50.4525\n",
            "Validation Loss: 2.699836605673383, Validation Accuracy: 35.02\n",
            "[29/150]: Training Loss: 1.790220089149475, Training Accuracy: 51.585\n",
            "Validation Loss: 2.7410409693505353, Validation Accuracy: 34.77\n",
            "[30/150]: Training Loss: 1.7332203540802003, Training Accuracy: 53.0625\n",
            "Validation Loss: 2.6978999216845083, Validation Accuracy: 35.51\n",
            "[31/150]: Training Loss: 1.6659312999725342, Training Accuracy: 54.48\n",
            "Validation Loss: 2.7799882926758688, Validation Accuracy: 35.39\n",
            "[32/150]: Training Loss: 1.6054936313629151, Training Accuracy: 55.9275\n",
            "Validation Loss: 2.783534232218554, Validation Accuracy: 35.38\n",
            "[33/150]: Training Loss: 1.5524930331230165, Training Accuracy: 57.2425\n",
            "Validation Loss: 2.7876193948612094, Validation Accuracy: 35.43\n",
            "[34/150]: Training Loss: 1.4864114666938781, Training Accuracy: 58.62\n",
            "Validation Loss: 2.8515792378954066, Validation Accuracy: 34.97\n",
            "[35/150]: Training Loss: 1.4250763600349425, Training Accuracy: 60.31\n",
            "Validation Loss: 2.89135210453325, Validation Accuracy: 34.83\n",
            "[36/150]: Training Loss: 1.3677435276031493, Training Accuracy: 61.51\n",
            "Validation Loss: 2.919705820691054, Validation Accuracy: 36.01\n",
            "[37/150]: Training Loss: 1.3075296778678893, Training Accuracy: 63.435\n",
            "Validation Loss: 3.000214212259669, Validation Accuracy: 35.23\n",
            "[38/150]: Training Loss: 1.2455093726158142, Training Accuracy: 64.7925\n",
            "Validation Loss: 3.0685715766469386, Validation Accuracy: 34.47\n",
            "[39/150]: Training Loss: 1.1875538174629212, Training Accuracy: 66.195\n",
            "Validation Loss: 3.0860758040361342, Validation Accuracy: 35.17\n",
            "[40/150]: Training Loss: 1.1235356809616088, Training Accuracy: 67.92\n",
            "Validation Loss: 3.142808488979461, Validation Accuracy: 35.12\n",
            "[41/150]: Training Loss: 1.0638824738502501, Training Accuracy: 69.31\n",
            "Validation Loss: 3.2775286580346954, Validation Accuracy: 35.6\n",
            "[42/150]: Training Loss: 1.0043726112365723, Training Accuracy: 71.1\n",
            "Validation Loss: 3.332795017084498, Validation Accuracy: 34.53\n",
            "[43/150]: Training Loss: 0.9537097842216492, Training Accuracy: 72.4\n",
            "Validation Loss: 3.416984627960594, Validation Accuracy: 34.84\n",
            "[44/150]: Training Loss: 0.8938642192840576, Training Accuracy: 74.145\n",
            "Validation Loss: 3.5402958104564886, Validation Accuracy: 34.94\n",
            "[45/150]: Training Loss: 0.8376890470981598, Training Accuracy: 75.4875\n",
            "Validation Loss: 3.6570517561238285, Validation Accuracy: 34.43\n",
            "[46/150]: Training Loss: 0.7843643071174622, Training Accuracy: 76.875\n",
            "Validation Loss: 3.745219696858886, Validation Accuracy: 34.67\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 24.405659256467395, Test Accuracy: 13.44\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>13.44</td></tr><tr><td>Test Loss</td><td>24.40566</td></tr><tr><td>Train Accuracy</td><td>76.875</td></tr><tr><td>Train Loss</td><td>0.78436</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.00095 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/cwudz2wj</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_045608-cwudz2wj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.001 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_050220-wgvfp3rx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx' target=\"_blank\">learning_rate=0.001 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.351185768127442, Training Accuracy: 3.94\n",
            "Validation Loss: 4.125218055810139, Validation Accuracy: 6.94\n",
            "[2/150]: Training Loss: 3.934326037979126, Training Accuracy: 9.675\n",
            "Validation Loss: 3.8352680054439863, Validation Accuracy: 11.53\n",
            "[3/150]: Training Loss: 3.6920593730926514, Training Accuracy: 14.25\n",
            "Validation Loss: 3.6194094821905636, Validation Accuracy: 15.04\n",
            "[4/150]: Training Loss: 3.5119912174224854, Training Accuracy: 17.235\n",
            "Validation Loss: 3.498675950773203, Validation Accuracy: 17.26\n",
            "[5/150]: Training Loss: 3.3595370391845703, Training Accuracy: 19.9025\n",
            "Validation Loss: 3.3320715184424334, Validation Accuracy: 20.65\n",
            "[6/150]: Training Loss: 3.2359037544250486, Training Accuracy: 22.135\n",
            "Validation Loss: 3.2282794827868226, Validation Accuracy: 21.83\n",
            "[7/150]: Training Loss: 3.1236792266845703, Training Accuracy: 24.1875\n",
            "Validation Loss: 3.1484436852157494, Validation Accuracy: 23.69\n",
            "[8/150]: Training Loss: 3.0264555549621583, Training Accuracy: 25.99\n",
            "Validation Loss: 3.0823915505864816, Validation Accuracy: 24.76\n",
            "[9/150]: Training Loss: 2.9406655250549316, Training Accuracy: 27.4325\n",
            "Validation Loss: 3.018990922126041, Validation Accuracy: 25.75\n",
            "[10/150]: Training Loss: 2.8602982753753663, Training Accuracy: 28.8825\n",
            "Validation Loss: 2.982261516486004, Validation Accuracy: 26.83\n",
            "[11/150]: Training Loss: 2.7865394050598145, Training Accuracy: 30.62\n",
            "Validation Loss: 2.881638531472273, Validation Accuracy: 28.86\n",
            "[12/150]: Training Loss: 2.713952407836914, Training Accuracy: 32.0525\n",
            "Validation Loss: 2.8773576226204063, Validation Accuracy: 28.51\n",
            "[13/150]: Training Loss: 2.6428218954086304, Training Accuracy: 33.4175\n",
            "Validation Loss: 2.852285245421586, Validation Accuracy: 29.44\n",
            "[14/150]: Training Loss: 2.5819866678237915, Training Accuracy: 34.2325\n",
            "Validation Loss: 2.8152025581165483, Validation Accuracy: 29.99\n",
            "[15/150]: Training Loss: 2.5168129930496215, Training Accuracy: 36.01\n",
            "Validation Loss: 2.7263220571408606, Validation Accuracy: 32.4\n",
            "[16/150]: Training Loss: 2.456671890640259, Training Accuracy: 37.025\n",
            "Validation Loss: 2.7360152697107596, Validation Accuracy: 31.89\n",
            "[17/150]: Training Loss: 2.394518960952759, Training Accuracy: 38.405\n",
            "Validation Loss: 2.693733840231683, Validation Accuracy: 33.43\n",
            "[18/150]: Training Loss: 2.329904039955139, Training Accuracy: 40.0125\n",
            "Validation Loss: 2.679514690569252, Validation Accuracy: 33.49\n",
            "[19/150]: Training Loss: 2.271217462730408, Training Accuracy: 41.0875\n",
            "Validation Loss: 2.7137394892941615, Validation Accuracy: 32.98\n",
            "[20/150]: Training Loss: 2.2047285720825194, Training Accuracy: 42.575\n",
            "Validation Loss: 2.6841572416815787, Validation Accuracy: 32.96\n",
            "[21/150]: Training Loss: 2.1471473873138427, Training Accuracy: 43.83\n",
            "Validation Loss: 2.6751675947456603, Validation Accuracy: 33.61\n",
            "[22/150]: Training Loss: 2.085570357322693, Training Accuracy: 45.055\n",
            "Validation Loss: 2.6481618797703153, Validation Accuracy: 34.55\n",
            "[23/150]: Training Loss: 2.0214510499954224, Training Accuracy: 46.6625\n",
            "Validation Loss: 2.638173457163914, Validation Accuracy: 35.26\n",
            "[24/150]: Training Loss: 1.9607698831558227, Training Accuracy: 48.02\n",
            "Validation Loss: 2.621039168849872, Validation Accuracy: 35.84\n",
            "[25/150]: Training Loss: 1.8981156831741333, Training Accuracy: 49.45\n",
            "Validation Loss: 2.62651793896013, Validation Accuracy: 36.22\n",
            "[26/150]: Training Loss: 1.8291910402297973, Training Accuracy: 51.235\n",
            "Validation Loss: 2.6563384396255394, Validation Accuracy: 35.62\n",
            "[27/150]: Training Loss: 1.7651165187835693, Training Accuracy: 52.31\n",
            "Validation Loss: 2.639584611935221, Validation Accuracy: 36.55\n",
            "[28/150]: Training Loss: 1.7005936141967772, Training Accuracy: 54.275\n",
            "Validation Loss: 2.7095768307424652, Validation Accuracy: 36.06\n",
            "[29/150]: Training Loss: 1.6383595853805542, Training Accuracy: 55.6225\n",
            "Validation Loss: 2.7259993150735355, Validation Accuracy: 35.64\n",
            "[30/150]: Training Loss: 1.5726151014328003, Training Accuracy: 57.1225\n",
            "Validation Loss: 2.7611397246646274, Validation Accuracy: 36.11\n",
            "[31/150]: Training Loss: 1.508023483467102, Training Accuracy: 58.3025\n",
            "Validation Loss: 2.7436208489594187, Validation Accuracy: 35.66\n",
            "[32/150]: Training Loss: 1.4352122190475465, Training Accuracy: 60.295\n",
            "Validation Loss: 2.8204059456564057, Validation Accuracy: 36.43\n",
            "[33/150]: Training Loss: 1.3806078368186951, Training Accuracy: 61.585\n",
            "Validation Loss: 2.876485233853577, Validation Accuracy: 34.85\n",
            "[34/150]: Training Loss: 1.3117412397384645, Training Accuracy: 63.325\n",
            "Validation Loss: 2.8764415660481544, Validation Accuracy: 36.16\n",
            "[35/150]: Training Loss: 1.2477456188201905, Training Accuracy: 64.815\n",
            "Validation Loss: 2.959255384032134, Validation Accuracy: 35.83\n",
            "[36/150]: Training Loss: 1.178989047718048, Training Accuracy: 66.845\n",
            "Validation Loss: 3.055402055667464, Validation Accuracy: 35.68\n",
            "[37/150]: Training Loss: 1.1195641567230226, Training Accuracy: 68.255\n",
            "Validation Loss: 3.092683092044417, Validation Accuracy: 35.74\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 19.867750665944094, Test Accuracy: 12.24\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>12.24</td></tr><tr><td>Test Loss</td><td>19.86775</td></tr><tr><td>Train Accuracy</td><td>68.255</td></tr><tr><td>Train Loss</td><td>1.11956</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.001 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/wgvfp3rx</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_050220-wgvfp3rx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.0015 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_050736-gnfp4sxh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh' target=\"_blank\">learning_rate=0.0015 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.262849729156494, Training Accuracy: 4.9725\n",
            "Validation Loss: 3.9988777105975304, Validation Accuracy: 9.32\n",
            "[2/150]: Training Loss: 3.787733755874634, Training Accuracy: 12.5275\n",
            "Validation Loss: 3.666901140455987, Validation Accuracy: 14.13\n",
            "[3/150]: Training Loss: 3.5222632232666014, Training Accuracy: 16.975\n",
            "Validation Loss: 3.430799526773441, Validation Accuracy: 18.28\n",
            "[4/150]: Training Loss: 3.3261954177856445, Training Accuracy: 20.0275\n",
            "Validation Loss: 3.268884590476941, Validation Accuracy: 20.25\n",
            "[5/150]: Training Loss: 3.1651099575042725, Training Accuracy: 22.825\n",
            "Validation Loss: 3.140599861266507, Validation Accuracy: 23.63\n",
            "[6/150]: Training Loss: 3.0220409996032713, Training Accuracy: 25.57\n",
            "Validation Loss: 3.062152488975768, Validation Accuracy: 24.92\n",
            "[7/150]: Training Loss: 2.9046311878204345, Training Accuracy: 27.855\n",
            "Validation Loss: 2.9628464735237654, Validation Accuracy: 26.95\n",
            "[8/150]: Training Loss: 2.7910710647583006, Training Accuracy: 30.235\n",
            "Validation Loss: 2.8832454301749064, Validation Accuracy: 28.14\n",
            "[9/150]: Training Loss: 2.687530333709717, Training Accuracy: 32.2625\n",
            "Validation Loss: 2.8348786056421367, Validation Accuracy: 30.07\n",
            "[10/150]: Training Loss: 2.590290707015991, Training Accuracy: 34.1175\n",
            "Validation Loss: 2.7776184780582502, Validation Accuracy: 30.52\n",
            "[11/150]: Training Loss: 2.499223603057861, Training Accuracy: 36.12\n",
            "Validation Loss: 2.726046642680077, Validation Accuracy: 31.91\n",
            "[12/150]: Training Loss: 2.4020754039764403, Training Accuracy: 37.9725\n",
            "Validation Loss: 2.7060916757887337, Validation Accuracy: 32.51\n",
            "[13/150]: Training Loss: 2.302425242996216, Training Accuracy: 40.3075\n",
            "Validation Loss: 2.660059646436363, Validation Accuracy: 33.06\n",
            "[14/150]: Training Loss: 2.2182941759109496, Training Accuracy: 42.03\n",
            "Validation Loss: 2.647583461870813, Validation Accuracy: 34.15\n",
            "[15/150]: Training Loss: 2.1231776348114013, Training Accuracy: 43.9025\n",
            "Validation Loss: 2.6469775620539475, Validation Accuracy: 34.32\n",
            "[16/150]: Training Loss: 2.030972394180298, Training Accuracy: 45.9525\n",
            "Validation Loss: 2.7037209963342947, Validation Accuracy: 32.86\n",
            "[17/150]: Training Loss: 1.9431676984786987, Training Accuracy: 47.91\n",
            "Validation Loss: 2.5850457507333937, Validation Accuracy: 36.27\n",
            "[18/150]: Training Loss: 1.8313071418762208, Training Accuracy: 50.255\n",
            "Validation Loss: 2.699979861071155, Validation Accuracy: 35.5\n",
            "[19/150]: Training Loss: 1.7440621503829956, Training Accuracy: 52.4725\n",
            "Validation Loss: 2.661453550028953, Validation Accuracy: 35.96\n",
            "[20/150]: Training Loss: 1.637422308921814, Training Accuracy: 54.8325\n",
            "Validation Loss: 2.7208387426509977, Validation Accuracy: 35.5\n",
            "[21/150]: Training Loss: 1.5425473594665526, Training Accuracy: 57.03\n",
            "Validation Loss: 2.766743406368669, Validation Accuracy: 35.3\n",
            "[22/150]: Training Loss: 1.4425379981994628, Training Accuracy: 59.42\n",
            "Validation Loss: 2.7848968460301684, Validation Accuracy: 36.36\n",
            "[23/150]: Training Loss: 1.346246865272522, Training Accuracy: 61.82\n",
            "Validation Loss: 2.918702983552483, Validation Accuracy: 35.2\n",
            "[24/150]: Training Loss: 1.2415513612747193, Training Accuracy: 64.48\n",
            "Validation Loss: 3.0769596373199657, Validation Accuracy: 34.71\n",
            "[25/150]: Training Loss: 1.1484121152877809, Training Accuracy: 66.9425\n",
            "Validation Loss: 3.111480196570135, Validation Accuracy: 35.56\n",
            "[26/150]: Training Loss: 1.049374456501007, Training Accuracy: 69.2\n",
            "Validation Loss: 3.2441035076311437, Validation Accuracy: 35.12\n",
            "[27/150]: Training Loss: 0.9539197593688965, Training Accuracy: 71.64\n",
            "Validation Loss: 3.3398004808243673, Validation Accuracy: 34.83\n",
            "[28/150]: Training Loss: 0.8686957097053528, Training Accuracy: 73.885\n",
            "Validation Loss: 3.526861637261263, Validation Accuracy: 35.38\n",
            "[29/150]: Training Loss: 0.7777377708911896, Training Accuracy: 76.4\n",
            "Validation Loss: 3.74925019938475, Validation Accuracy: 34.95\n",
            "[30/150]: Training Loss: 0.6971015272140503, Training Accuracy: 78.4225\n",
            "Validation Loss: 3.921683767039305, Validation Accuracy: 34.49\n",
            "[31/150]: Training Loss: 0.6319020359992981, Training Accuracy: 80.37\n",
            "Validation Loss: 4.265976667404175, Validation Accuracy: 33.7\n",
            "[32/150]: Training Loss: 0.5592198015213012, Training Accuracy: 82.41\n",
            "Validation Loss: 4.3743809985507065, Validation Accuracy: 34.21\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 32.802743437943185, Test Accuracy: 15.7\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>15.7</td></tr><tr><td>Test Loss</td><td>32.80274</td></tr><tr><td>Train Accuracy</td><td>82.41</td></tr><tr><td>Train Loss</td><td>0.55922</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.0015 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/gnfp4sxh</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_050736-gnfp4sxh/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Hyperparameter with lr:0.002 and wd:0.0004\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_051217-y92gs5m0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0' target=\"_blank\">learning_rate=0.002 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.222224911499024, Training Accuracy: 5.3975\n",
            "Validation Loss: 3.937301506662065, Validation Accuracy: 9.22\n",
            "[2/150]: Training Loss: 3.7261308227539063, Training Accuracy: 13.09\n",
            "Validation Loss: 3.593827167134376, Validation Accuracy: 15.34\n",
            "[3/150]: Training Loss: 3.4419474758148194, Training Accuracy: 18.2775\n",
            "Validation Loss: 3.3379262875599465, Validation Accuracy: 19.89\n",
            "[4/150]: Training Loss: 3.242010182952881, Training Accuracy: 21.865\n",
            "Validation Loss: 3.2397833553848754, Validation Accuracy: 21.54\n",
            "[5/150]: Training Loss: 3.074250465774536, Training Accuracy: 24.8525\n",
            "Validation Loss: 3.2218855049959414, Validation Accuracy: 22.29\n",
            "[6/150]: Training Loss: 2.9360343181610107, Training Accuracy: 27.3175\n",
            "Validation Loss: 2.9865669110778033, Validation Accuracy: 26.78\n",
            "[7/150]: Training Loss: 2.801424619293213, Training Accuracy: 30.0175\n",
            "Validation Loss: 2.881270451150882, Validation Accuracy: 28.82\n",
            "[8/150]: Training Loss: 2.688298546409607, Training Accuracy: 32.05\n",
            "Validation Loss: 2.8182313017025113, Validation Accuracy: 30.07\n",
            "[9/150]: Training Loss: 2.5692677375793456, Training Accuracy: 34.84\n",
            "Validation Loss: 2.7818179950592623, Validation Accuracy: 30.94\n",
            "[10/150]: Training Loss: 2.454072025489807, Training Accuracy: 37.1425\n",
            "Validation Loss: 2.741547807766374, Validation Accuracy: 32.13\n",
            "[11/150]: Training Loss: 2.341491235733032, Training Accuracy: 39.3875\n",
            "Validation Loss: 2.735630521349087, Validation Accuracy: 32.51\n",
            "[12/150]: Training Loss: 2.2293454483032225, Training Accuracy: 41.4725\n",
            "Validation Loss: 2.690371492106444, Validation Accuracy: 33.37\n",
            "[13/150]: Training Loss: 2.1135470266342162, Training Accuracy: 43.935\n",
            "Validation Loss: 2.7198572918108312, Validation Accuracy: 34.01\n",
            "[14/150]: Training Loss: 1.9979984771728516, Training Accuracy: 46.1675\n",
            "Validation Loss: 2.7182075719165195, Validation Accuracy: 34.3\n",
            "[15/150]: Training Loss: 1.8890921760559083, Training Accuracy: 49.265\n",
            "Validation Loss: 2.7245476724235873, Validation Accuracy: 34.8\n",
            "[16/150]: Training Loss: 1.7773400522232055, Training Accuracy: 51.53\n",
            "Validation Loss: 2.749075835677469, Validation Accuracy: 34.12\n",
            "[17/150]: Training Loss: 1.676476739501953, Training Accuracy: 53.69\n",
            "Validation Loss: 2.8863297662917216, Validation Accuracy: 34.72\n",
            "[18/150]: Training Loss: 1.5655937635421753, Training Accuracy: 56.19\n",
            "Validation Loss: 2.8975180911410385, Validation Accuracy: 35.07\n",
            "[19/150]: Training Loss: 1.4489710484504699, Training Accuracy: 59.165\n",
            "Validation Loss: 2.9350054689273715, Validation Accuracy: 34.48\n",
            "[20/150]: Training Loss: 1.3497021337509156, Training Accuracy: 61.3125\n",
            "Validation Loss: 3.0767477864672426, Validation Accuracy: 34.78\n",
            "[21/150]: Training Loss: 1.2329449357032776, Training Accuracy: 64.19\n",
            "Validation Loss: 3.22720639113408, Validation Accuracy: 33.72\n",
            "[22/150]: Training Loss: 1.1406824831962585, Training Accuracy: 66.7375\n",
            "Validation Loss: 3.406786183642734, Validation Accuracy: 33.27\n",
            "[23/150]: Training Loss: 1.044043209552765, Training Accuracy: 69.1625\n",
            "Validation Loss: 3.664752894905722, Validation Accuracy: 32.75\n",
            "[24/150]: Training Loss: 0.9549756371498108, Training Accuracy: 71.1525\n",
            "Validation Loss: 3.7399451838936777, Validation Accuracy: 32.73\n",
            "[25/150]: Training Loss: 0.8772646800518036, Training Accuracy: 73.25\n",
            "Validation Loss: 3.9628275534149946, Validation Accuracy: 33.22\n",
            "[26/150]: Training Loss: 0.7976884460449218, Training Accuracy: 75.4625\n",
            "Validation Loss: 4.143224073823091, Validation Accuracy: 32.26\n",
            "[27/150]: Training Loss: 0.7417024869441986, Training Accuracy: 76.86\n",
            "Validation Loss: 4.497031521645321, Validation Accuracy: 32.38\n",
            "[28/150]: Training Loss: 0.682380113697052, Training Accuracy: 78.7575\n",
            "Validation Loss: 4.677568254956774, Validation Accuracy: 32.16\n",
            "Early stopping!\n",
            "**********************************************************************\n",
            "Test Loss: 35.528370875461846, Test Accuracy: 11.85\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>11.85</td></tr><tr><td>Test Loss</td><td>35.52837</td></tr><tr><td>Train Accuracy</td><td>78.7575</td></tr><tr><td>Train Loss</td><td>0.68238</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.002 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning/runs/y92gs5m0</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB-HyperParameterTuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_051217-y92gs5m0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "learning_rates = [9e-04, 95e-05, 1e-03, 15e-04, 2e-03]\n",
        "wd = 4e-04\n",
        "for lr in learning_rates:\n",
        "\n",
        "  print('='*50)\n",
        "  print(f'Hyperparameter with lr:{lr} and wd:{wd}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {'learning_rate': lr,\n",
        "                      'weight_decay' : wd\n",
        "                      }\n",
        "  # Load the model\n",
        "  model = LeNet5().to(device)\n",
        "\n",
        "  # Optimizer and scheduler setup\n",
        "  optimizer = LAMB(model.parameters(), lr=lr, weight_decay=wd)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "  # Training\n",
        "  run_training(num_epochs,\n",
        "                model,\n",
        "                train_loader,\n",
        "                validation_loader,\n",
        "                test_loader,\n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                criterion,\n",
        "                device,\n",
        "                'LAMB-HyperParameterTuning',\n",
        "                hyperparameters=hyperparameters,\n",
        "                is_wandb = True,\n",
        "                n_epochs_stop = 10\n",
        "  )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "90b585318084475b9543b3226230d373",
            "5f3a33bc6a334c9e8c5886caa5f015ca",
            "46c9d83bf8af443bb376ed4858ce35f7",
            "2402e482be7e484fa98ae3f60f68d95a",
            "e8040d3a83de41319a2a836def914b1b",
            "0b4b10bb8bff4d40afa8df981440fe43",
            "3b0b3657d30547d7abe702d6c1e7b7c4",
            "17ac4efa9cf148e5be69edbc5b0bdecf",
            "f5fd8db807e144578a2e20762d7369d0",
            "32c759d7fff64701a6ad61b38ac63187",
            "557cf2de74314915b7204d9055fa6987",
            "c5bb24dc0f56483b88a0efbfa2a1c8ee",
            "67aa8f8c2b244c9a9a3bb11caf491247",
            "09b762f5ba784066a9408ffcfdea5b2e",
            "6a345ff7386643eeb7a5b190d69d2e0e",
            "8f4783e2060a4295bb68036a4a7310d9",
            "aa6aa819c1fd4776ac47af588aaaaacd",
            "64224c777f01418e904dcd30ecd7c5ef",
            "c1dbd12595384bc6a0240aaeb16014dc",
            "6aa92cf922724ac08c45385ff8a58447",
            "99929754d3094d03a5f311177ac26cb3",
            "9ea517158b974c5da17899e3fea4fc3b",
            "bb2ee36dd3c04926b10b832928d7d0a0",
            "9d88657b39fd4b169d61b1b8d240e6ff"
          ]
        },
        "id": "7sUsHkiHgURG",
        "outputId": "3829f79a-3d06-48ee-fea5-a00c18f8744b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:uqgfhn52) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>22.668</td></tr><tr><td>Train Loss</td><td>3.18342</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.0015026019100214134 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/uqgfhn52' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/uqgfhn52</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_051725-uqgfhn52/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:uqgfhn52). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_052010-62vz5e00</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00' target=\"_blank\">learning_rate=0.0015 weight_decay=0.0004</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.170614353226274, Training Accuracy: 6.554\n",
            "Validation Loss: 3.8419871087286883, Validation Accuracy: 11.55\n",
            "[2/150]: Training Loss: 3.73113738697813, Training Accuracy: 13.326\n",
            "Validation Loss: 3.5141605601948536, Validation Accuracy: 17.24\n",
            "[3/150]: Training Loss: 3.4963422099037853, Training Accuracy: 17.148\n",
            "Validation Loss: 3.329459586720558, Validation Accuracy: 21.14\n",
            "[4/150]: Training Loss: 3.32537763838268, Training Accuracy: 20.17\n",
            "Validation Loss: 3.1594036445496188, Validation Accuracy: 23.38\n",
            "[5/150]: Training Loss: 3.1821046982274948, Training Accuracy: 22.724\n",
            "Validation Loss: 3.024734082495331, Validation Accuracy: 25.42\n",
            "[6/150]: Training Loss: 3.0576654736648132, Training Accuracy: 24.838\n",
            "Validation Loss: 2.917888655024729, Validation Accuracy: 27.48\n",
            "[7/150]: Training Loss: 2.9461568444586166, Training Accuracy: 26.996\n",
            "Validation Loss: 2.8047209317517128, Validation Accuracy: 29.77\n",
            "[8/150]: Training Loss: 2.860208951298843, Training Accuracy: 28.704\n",
            "Validation Loss: 2.7236505541831826, Validation Accuracy: 30.98\n",
            "[9/150]: Training Loss: 2.7667418916512023, Training Accuracy: 30.53\n",
            "Validation Loss: 2.6368031137308496, Validation Accuracy: 33.13\n",
            "[10/150]: Training Loss: 2.6879202682343895, Training Accuracy: 31.998\n",
            "Validation Loss: 2.536134794259527, Validation Accuracy: 35.32\n",
            "[11/150]: Training Loss: 2.61040174458033, Training Accuracy: 33.4\n",
            "Validation Loss: 2.4932953764678567, Validation Accuracy: 36.12\n",
            "[12/150]: Training Loss: 2.546117087306879, Training Accuracy: 34.894\n",
            "Validation Loss: 2.4478625689342524, Validation Accuracy: 37.12\n",
            "[13/150]: Training Loss: 2.4885423736803975, Training Accuracy: 35.89\n",
            "Validation Loss: 2.4252931342762745, Validation Accuracy: 37.55\n",
            "[14/150]: Training Loss: 2.4414472836057852, Training Accuracy: 37.026\n",
            "Validation Loss: 2.3621389410298343, Validation Accuracy: 39.2\n",
            "[15/150]: Training Loss: 2.3877986550636, Training Accuracy: 37.84\n",
            "Validation Loss: 2.3034424402151896, Validation Accuracy: 40.21\n",
            "[16/150]: Training Loss: 2.3379326612138382, Training Accuracy: 39.018\n",
            "Validation Loss: 2.272260831419829, Validation Accuracy: 41.24\n",
            "[17/150]: Training Loss: 2.300395258247395, Training Accuracy: 39.968\n",
            "Validation Loss: 2.2585707120834644, Validation Accuracy: 40.6\n",
            "[18/150]: Training Loss: 2.254751528315532, Training Accuracy: 40.976\n",
            "Validation Loss: 2.2186143770339384, Validation Accuracy: 42.07\n",
            "[19/150]: Training Loss: 2.2090765830805843, Training Accuracy: 42.026\n",
            "Validation Loss: 2.1868586600965756, Validation Accuracy: 42.61\n",
            "[20/150]: Training Loss: 2.183374183562101, Training Accuracy: 42.532\n",
            "Validation Loss: 2.184412774007032, Validation Accuracy: 42.58\n",
            "[21/150]: Training Loss: 2.1446489868566507, Training Accuracy: 43.298\n",
            "Validation Loss: 2.144960983543639, Validation Accuracy: 43.98\n",
            "[22/150]: Training Loss: 2.1038296910198144, Training Accuracy: 44.234\n",
            "Validation Loss: 2.14452546493263, Validation Accuracy: 44.01\n",
            "[23/150]: Training Loss: 2.0707620507311026, Training Accuracy: 44.894\n",
            "Validation Loss: 2.098228795513226, Validation Accuracy: 45.23\n",
            "[24/150]: Training Loss: 2.043932166093451, Training Accuracy: 45.518\n",
            "Validation Loss: 2.0813077680624215, Validation Accuracy: 45.38\n",
            "[25/150]: Training Loss: 2.0066164360021994, Training Accuracy: 46.274\n",
            "Validation Loss: 2.075542102953431, Validation Accuracy: 45.85\n",
            "[26/150]: Training Loss: 1.9791695739302184, Training Accuracy: 46.988\n",
            "Validation Loss: 2.0774263060016995, Validation Accuracy: 45.28\n",
            "[27/150]: Training Loss: 1.948626570232079, Training Accuracy: 47.584\n",
            "Validation Loss: 2.0519798697939344, Validation Accuracy: 46.31\n",
            "[28/150]: Training Loss: 1.9167836584398508, Training Accuracy: 48.236\n",
            "Validation Loss: 2.0201504807563344, Validation Accuracy: 46.88\n",
            "[29/150]: Training Loss: 1.8909891124271676, Training Accuracy: 48.752\n",
            "Validation Loss: 2.040525470569635, Validation Accuracy: 46.57\n",
            "[30/150]: Training Loss: 1.8726914470153087, Training Accuracy: 49.618\n",
            "Validation Loss: 2.0113253760489687, Validation Accuracy: 47.44\n",
            "[31/150]: Training Loss: 1.8444365990131408, Training Accuracy: 50.096\n",
            "Validation Loss: 2.0178289990516225, Validation Accuracy: 47.23\n",
            "[32/150]: Training Loss: 1.8157384923047117, Training Accuracy: 50.462\n",
            "Validation Loss: 1.9997166759648901, Validation Accuracy: 47.55\n",
            "[33/150]: Training Loss: 1.7931588938474046, Training Accuracy: 51.29\n",
            "Validation Loss: 1.9950028824958073, Validation Accuracy: 47.93\n",
            "[34/150]: Training Loss: 1.770590110355631, Training Accuracy: 51.824\n",
            "Validation Loss: 1.993164770162789, Validation Accuracy: 48.12\n",
            "[35/150]: Training Loss: 1.7467985225607976, Training Accuracy: 52.03\n",
            "Validation Loss: 1.9869080242837311, Validation Accuracy: 47.9\n",
            "[36/150]: Training Loss: 1.7260596163742377, Training Accuracy: 52.556\n",
            "Validation Loss: 1.9724081488931255, Validation Accuracy: 48.61\n",
            "[37/150]: Training Loss: 1.7053224419998696, Training Accuracy: 53.142\n",
            "Validation Loss: 1.9826935643603087, Validation Accuracy: 48.29\n",
            "[38/150]: Training Loss: 1.67785135681367, Training Accuracy: 53.82\n",
            "Validation Loss: 1.9470138079041888, Validation Accuracy: 49.88\n",
            "[39/150]: Training Loss: 1.6623152171254463, Training Accuracy: 54.074\n",
            "Validation Loss: 1.9769204634769706, Validation Accuracy: 48.87\n",
            "[40/150]: Training Loss: 1.6364040380853522, Training Accuracy: 54.652\n",
            "Validation Loss: 1.9554897744184847, Validation Accuracy: 49.57\n",
            "[41/150]: Training Loss: 1.6296177715291758, Training Accuracy: 54.87\n",
            "Validation Loss: 1.9755018660976629, Validation Accuracy: 49.42\n",
            "[42/150]: Training Loss: 1.6172699540319955, Training Accuracy: 55.052\n",
            "Validation Loss: 1.9689236112460968, Validation Accuracy: 49.6\n",
            "[43/150]: Training Loss: 1.5901159763793506, Training Accuracy: 55.706\n",
            "Validation Loss: 1.9709602108426914, Validation Accuracy: 49.39\n",
            "[44/150]: Training Loss: 1.5688080241917954, Training Accuracy: 56.494\n",
            "Validation Loss: 1.944002225140857, Validation Accuracy: 50.24\n",
            "[45/150]: Training Loss: 1.5468424783490808, Training Accuracy: 57.02\n",
            "Validation Loss: 1.9809717410688947, Validation Accuracy: 49.68\n",
            "[46/150]: Training Loss: 1.5288271441331605, Training Accuracy: 57.372\n",
            "Validation Loss: 1.9420389149599016, Validation Accuracy: 50.41\n",
            "[47/150]: Training Loss: 1.527083154453341, Training Accuracy: 57.472\n",
            "Validation Loss: 1.9570772792123685, Validation Accuracy: 49.71\n",
            "[48/150]: Training Loss: 1.5049020071773578, Training Accuracy: 57.818\n",
            "Validation Loss: 1.954159548328181, Validation Accuracy: 50.38\n",
            "[49/150]: Training Loss: 1.48557850992893, Training Accuracy: 58.67\n",
            "Validation Loss: 1.934345255232161, Validation Accuracy: 50.33\n",
            "[50/150]: Training Loss: 1.461377340052134, Training Accuracy: 58.866\n",
            "Validation Loss: 1.9670750221629052, Validation Accuracy: 50.09\n",
            "[51/150]: Training Loss: 1.4531892761397545, Training Accuracy: 59.148\n",
            "Validation Loss: 1.9439144878630426, Validation Accuracy: 50.57\n",
            "[52/150]: Training Loss: 1.4365854823528348, Training Accuracy: 59.4\n",
            "Validation Loss: 1.9623823484797387, Validation Accuracy: 50.49\n",
            "[53/150]: Training Loss: 1.4231898410393453, Training Accuracy: 59.886\n",
            "Validation Loss: 1.9759472274476555, Validation Accuracy: 50.14\n",
            "[54/150]: Training Loss: 1.408926703664653, Training Accuracy: 60.114\n",
            "Validation Loss: 1.924273559242297, Validation Accuracy: 51.21\n",
            "[55/150]: Training Loss: 1.3876726690613095, Training Accuracy: 60.616\n",
            "Validation Loss: 1.9458329692767684, Validation Accuracy: 50.63\n",
            "[56/150]: Training Loss: 1.375196378478004, Training Accuracy: 61.12\n",
            "Validation Loss: 1.9410056468028172, Validation Accuracy: 51.4\n",
            "[57/150]: Training Loss: 1.3586041162081082, Training Accuracy: 61.634\n",
            "Validation Loss: 1.9649552509283563, Validation Accuracy: 50.61\n",
            "[58/150]: Training Loss: 1.3581253530271828, Training Accuracy: 61.412\n",
            "Validation Loss: 1.9388997919240576, Validation Accuracy: 51.27\n",
            "[59/150]: Training Loss: 1.3380669855400729, Training Accuracy: 61.932\n",
            "Validation Loss: 1.9778220661126884, Validation Accuracy: 50.91\n",
            "[60/150]: Training Loss: 1.3257979125622898, Training Accuracy: 62.056\n",
            "Validation Loss: 1.9304483665782175, Validation Accuracy: 51.44\n",
            "[61/150]: Training Loss: 1.316665162896866, Training Accuracy: 62.458\n",
            "Validation Loss: 1.9412351808730204, Validation Accuracy: 51.61\n",
            "[62/150]: Training Loss: 1.2979835793947625, Training Accuracy: 62.832\n",
            "Validation Loss: 1.9522758213577756, Validation Accuracy: 51.6\n",
            "[63/150]: Training Loss: 1.2900130991130838, Training Accuracy: 63.092\n",
            "Validation Loss: 1.9639577113898696, Validation Accuracy: 51.23\n",
            "[64/150]: Training Loss: 1.2804708784955847, Training Accuracy: 63.436\n",
            "Validation Loss: 1.9787959422275518, Validation Accuracy: 51.42\n",
            "[65/150]: Training Loss: 1.255796139959789, Training Accuracy: 63.948\n",
            "Validation Loss: 1.9797948439409778, Validation Accuracy: 51.63\n",
            "[66/150]: Training Loss: 1.25389591042343, Training Accuracy: 64.23\n",
            "Validation Loss: 1.97708031554131, Validation Accuracy: 52.19\n",
            "[67/150]: Training Loss: 1.2357215599330795, Training Accuracy: 64.366\n",
            "Validation Loss: 1.9641305322100402, Validation Accuracy: 52.22\n",
            "[68/150]: Training Loss: 1.2240538022402303, Training Accuracy: 64.862\n",
            "Validation Loss: 1.9815536661512534, Validation Accuracy: 51.47\n",
            "[69/150]: Training Loss: 1.2087419308969736, Training Accuracy: 65.136\n",
            "Validation Loss: 1.979879951021474, Validation Accuracy: 51.77\n",
            "[70/150]: Training Loss: 1.1984577734604516, Training Accuracy: 65.552\n",
            "Validation Loss: 2.005889118856685, Validation Accuracy: 51.82\n",
            "[71/150]: Training Loss: 1.184933677506264, Training Accuracy: 66.026\n",
            "Validation Loss: 1.9805337274150483, Validation Accuracy: 51.89\n",
            "[72/150]: Training Loss: 1.1769923466398282, Training Accuracy: 65.964\n",
            "Validation Loss: 2.0164508561419834, Validation Accuracy: 51.97\n",
            "[73/150]: Training Loss: 1.1639809905720488, Training Accuracy: 66.116\n",
            "Validation Loss: 2.0093113024523306, Validation Accuracy: 51.75\n",
            "[74/150]: Training Loss: 1.1529584921077085, Training Accuracy: 66.55\n",
            "Validation Loss: 2.017468781987573, Validation Accuracy: 51.53\n",
            "[75/150]: Training Loss: 1.1453676276347216, Training Accuracy: 66.758\n",
            "Validation Loss: 2.011715882902692, Validation Accuracy: 51.48\n",
            "[76/150]: Training Loss: 1.1254654065574832, Training Accuracy: 67.396\n",
            "Validation Loss: 2.0016076625532406, Validation Accuracy: 51.93\n",
            "[77/150]: Training Loss: 1.1255306048161537, Training Accuracy: 67.168\n",
            "Validation Loss: 2.037281950567938, Validation Accuracy: 51.54\n",
            "[78/150]: Training Loss: 1.107588133391212, Training Accuracy: 67.898\n",
            "Validation Loss: 2.0207215995545598, Validation Accuracy: 52.25\n",
            "[79/150]: Training Loss: 1.0974091778478354, Training Accuracy: 67.988\n",
            "Validation Loss: 2.0399385941256383, Validation Accuracy: 52.27\n",
            "[80/150]: Training Loss: 1.0841643994726489, Training Accuracy: 68.482\n",
            "Validation Loss: 2.0396120669735467, Validation Accuracy: 51.88\n",
            "[81/150]: Training Loss: 1.086065025128367, Training Accuracy: 68.472\n",
            "Validation Loss: 2.0602660406926634, Validation Accuracy: 52.23\n",
            "[82/150]: Training Loss: 1.068733287741766, Training Accuracy: 68.792\n",
            "Validation Loss: 2.038099098357425, Validation Accuracy: 52.32\n",
            "[83/150]: Training Loss: 1.0567113834116466, Training Accuracy: 69.072\n",
            "Validation Loss: 2.0566319333519907, Validation Accuracy: 52.0\n",
            "[84/150]: Training Loss: 1.0568410671123154, Training Accuracy: 69.164\n",
            "Validation Loss: 2.031417142054078, Validation Accuracy: 52.23\n",
            "[85/150]: Training Loss: 1.0462324290019471, Training Accuracy: 69.518\n",
            "Validation Loss: 2.039376434247205, Validation Accuracy: 52.34\n",
            "[86/150]: Training Loss: 1.0314530159353905, Training Accuracy: 69.866\n",
            "Validation Loss: 2.0698205103540115, Validation Accuracy: 52.02\n",
            "[87/150]: Training Loss: 1.0182559825956363, Training Accuracy: 70.136\n",
            "Validation Loss: 2.063336102825821, Validation Accuracy: 52.83\n",
            "[88/150]: Training Loss: 1.010449574350396, Training Accuracy: 70.25\n",
            "Validation Loss: 2.077486888618226, Validation Accuracy: 52.3\n",
            "[89/150]: Training Loss: 1.0035706778316547, Training Accuracy: 70.786\n",
            "Validation Loss: 2.095621291998845, Validation Accuracy: 52.44\n",
            "[90/150]: Training Loss: 0.9991849294251494, Training Accuracy: 70.812\n",
            "Validation Loss: 2.069785719464539, Validation Accuracy: 52.4\n",
            "[91/150]: Training Loss: 0.993351522995078, Training Accuracy: 70.858\n",
            "Validation Loss: 2.1024031069627993, Validation Accuracy: 52.29\n",
            "[92/150]: Training Loss: 0.9726321048215222, Training Accuracy: 71.618\n",
            "Validation Loss: 2.084312925672835, Validation Accuracy: 52.53\n",
            "[93/150]: Training Loss: 0.97342309279515, Training Accuracy: 71.502\n",
            "Validation Loss: 2.0946866744642803, Validation Accuracy: 52.66\n",
            "[94/150]: Training Loss: 0.963803626920866, Training Accuracy: 71.718\n",
            "Validation Loss: 2.110511908105984, Validation Accuracy: 52.0\n",
            "[95/150]: Training Loss: 0.9580129440635672, Training Accuracy: 71.984\n",
            "Validation Loss: 2.111149025570815, Validation Accuracy: 52.13\n",
            "[96/150]: Training Loss: 0.9513878270869365, Training Accuracy: 72.204\n",
            "Validation Loss: 2.1115560136782894, Validation Accuracy: 52.52\n",
            "[97/150]: Training Loss: 0.938226883673607, Training Accuracy: 72.542\n",
            "Validation Loss: 2.1242388023692333, Validation Accuracy: 52.65\n",
            "[98/150]: Training Loss: 0.938509788811969, Training Accuracy: 72.476\n",
            "Validation Loss: 2.1135271635784467, Validation Accuracy: 52.82\n",
            "[99/150]: Training Loss: 0.9283110121326983, Training Accuracy: 72.674\n",
            "Validation Loss: 2.125285923860635, Validation Accuracy: 52.06\n",
            "[100/150]: Training Loss: 0.9165082442790956, Training Accuracy: 73.184\n",
            "Validation Loss: 2.1202975777304096, Validation Accuracy: 52.38\n",
            "[101/150]: Training Loss: 0.9109463012584335, Training Accuracy: 72.998\n",
            "Validation Loss: 2.136450658937928, Validation Accuracy: 52.42\n",
            "[102/150]: Training Loss: 0.9098046744418571, Training Accuracy: 73.26\n",
            "Validation Loss: 2.1267288725846893, Validation Accuracy: 52.86\n",
            "[103/150]: Training Loss: 0.8908002294237961, Training Accuracy: 73.838\n",
            "Validation Loss: 2.125086438883642, Validation Accuracy: 52.34\n",
            "[104/150]: Training Loss: 0.8947355315432219, Training Accuracy: 73.496\n",
            "Validation Loss: 2.13778414771815, Validation Accuracy: 52.62\n",
            "[105/150]: Training Loss: 0.8788276913830692, Training Accuracy: 74.106\n",
            "Validation Loss: 2.153025217876313, Validation Accuracy: 52.42\n",
            "[106/150]: Training Loss: 0.8798901442524112, Training Accuracy: 74.082\n",
            "Validation Loss: 2.1569059441803367, Validation Accuracy: 52.12\n",
            "[107/150]: Training Loss: 0.8666944346388282, Training Accuracy: 74.51\n",
            "Validation Loss: 2.1608687851839004, Validation Accuracy: 52.57\n",
            "[108/150]: Training Loss: 0.863055142485882, Training Accuracy: 74.528\n",
            "Validation Loss: 2.152739950805713, Validation Accuracy: 52.64\n",
            "[109/150]: Training Loss: 0.8662346141874943, Training Accuracy: 74.474\n",
            "Validation Loss: 2.14941197823567, Validation Accuracy: 52.71\n",
            "[110/150]: Training Loss: 0.8472307874342365, Training Accuracy: 75.102\n",
            "Validation Loss: 2.1724644390640746, Validation Accuracy: 52.69\n",
            "[111/150]: Training Loss: 0.8462071400469221, Training Accuracy: 75.26\n",
            "Validation Loss: 2.166171987345264, Validation Accuracy: 52.5\n",
            "[112/150]: Training Loss: 0.8393054546221442, Training Accuracy: 75.156\n",
            "Validation Loss: 2.16822886315121, Validation Accuracy: 52.79\n",
            "[113/150]: Training Loss: 0.835232794132379, Training Accuracy: 75.426\n",
            "Validation Loss: 2.1755759412316, Validation Accuracy: 52.8\n",
            "[114/150]: Training Loss: 0.8176318519484357, Training Accuracy: 75.594\n",
            "Validation Loss: 2.1797922325741714, Validation Accuracy: 52.63\n",
            "[115/150]: Training Loss: 0.8166115129618998, Training Accuracy: 75.654\n",
            "Validation Loss: 2.1856938577761316, Validation Accuracy: 52.44\n",
            "[116/150]: Training Loss: 0.8185412460733252, Training Accuracy: 75.768\n",
            "Validation Loss: 2.178807595732865, Validation Accuracy: 52.51\n",
            "[117/150]: Training Loss: 0.819738648203023, Training Accuracy: 75.68\n",
            "Validation Loss: 2.196637267519714, Validation Accuracy: 52.53\n",
            "[118/150]: Training Loss: 0.8139445048463924, Training Accuracy: 76.114\n",
            "Validation Loss: 2.1727140124436395, Validation Accuracy: 52.14\n",
            "[119/150]: Training Loss: 0.804041659359432, Training Accuracy: 76.238\n",
            "Validation Loss: 2.189874280789855, Validation Accuracy: 52.54\n",
            "[120/150]: Training Loss: 0.8070289515473349, Training Accuracy: 75.962\n",
            "Validation Loss: 2.1908302922157725, Validation Accuracy: 52.74\n",
            "[121/150]: Training Loss: 0.7972686002626443, Training Accuracy: 76.462\n",
            "Validation Loss: 2.183206063167305, Validation Accuracy: 52.9\n",
            "[122/150]: Training Loss: 0.7931420375090426, Training Accuracy: 76.7\n",
            "Validation Loss: 2.1991693654637428, Validation Accuracy: 52.88\n",
            "[123/150]: Training Loss: 0.7942516611284002, Training Accuracy: 76.382\n",
            "Validation Loss: 2.1899024916302627, Validation Accuracy: 52.96\n",
            "[124/150]: Training Loss: 0.7901588624243236, Training Accuracy: 76.912\n",
            "Validation Loss: 2.1976229795225106, Validation Accuracy: 52.76\n",
            "[125/150]: Training Loss: 0.784064800881059, Training Accuracy: 76.852\n",
            "Validation Loss: 2.197301295152895, Validation Accuracy: 52.94\n",
            "[126/150]: Training Loss: 0.7810554346236427, Training Accuracy: 77.018\n",
            "Validation Loss: 2.1978831883448704, Validation Accuracy: 53.05\n",
            "[127/150]: Training Loss: 0.7694210947855659, Training Accuracy: 77.242\n",
            "Validation Loss: 2.2037578973041216, Validation Accuracy: 52.8\n",
            "[128/150]: Training Loss: 0.771505500425768, Training Accuracy: 77.276\n",
            "Validation Loss: 2.204603326548437, Validation Accuracy: 52.61\n",
            "[129/150]: Training Loss: 0.7755528422039183, Training Accuracy: 77.05\n",
            "Validation Loss: 2.2066474340523885, Validation Accuracy: 52.48\n",
            "[130/150]: Training Loss: 0.7633897851190299, Training Accuracy: 77.462\n",
            "Validation Loss: 2.20657627294018, Validation Accuracy: 52.87\n",
            "[131/150]: Training Loss: 0.7619557766734487, Training Accuracy: 77.74\n",
            "Validation Loss: 2.2119038287241746, Validation Accuracy: 52.87\n",
            "[132/150]: Training Loss: 0.7639070795015301, Training Accuracy: 77.628\n",
            "Validation Loss: 2.214540675946861, Validation Accuracy: 52.93\n",
            "[133/150]: Training Loss: 0.7627752876800039, Training Accuracy: 77.624\n",
            "Validation Loss: 2.2126305232382126, Validation Accuracy: 52.92\n",
            "[134/150]: Training Loss: 0.762916150681503, Training Accuracy: 77.468\n",
            "Validation Loss: 2.213455740813237, Validation Accuracy: 52.87\n",
            "[135/150]: Training Loss: 0.7556408758434798, Training Accuracy: 77.858\n",
            "Validation Loss: 2.2093218823147427, Validation Accuracy: 52.78\n",
            "[136/150]: Training Loss: 0.7546808309567249, Training Accuracy: 77.614\n",
            "Validation Loss: 2.215512813276546, Validation Accuracy: 52.82\n",
            "[137/150]: Training Loss: 0.7569507613130237, Training Accuracy: 77.856\n",
            "Validation Loss: 2.214977786799145, Validation Accuracy: 52.94\n",
            "[138/150]: Training Loss: 0.7538443226414873, Training Accuracy: 77.938\n",
            "Validation Loss: 2.215168529255375, Validation Accuracy: 52.96\n",
            "[139/150]: Training Loss: 0.7538767649084711, Training Accuracy: 77.666\n",
            "Validation Loss: 2.215025094664021, Validation Accuracy: 52.95\n",
            "[140/150]: Training Loss: 0.7476015420216123, Training Accuracy: 77.98\n",
            "Validation Loss: 2.2151690637989407, Validation Accuracy: 53.11\n",
            "[141/150]: Training Loss: 0.7429782649135346, Training Accuracy: 78.256\n",
            "Validation Loss: 2.214695118794775, Validation Accuracy: 52.91\n",
            "[142/150]: Training Loss: 0.7340887488077974, Training Accuracy: 78.538\n",
            "Validation Loss: 2.216404147968171, Validation Accuracy: 53.08\n",
            "[143/150]: Training Loss: 0.7499228092029576, Training Accuracy: 78.16\n",
            "Validation Loss: 2.216663356799229, Validation Accuracy: 52.99\n",
            "[144/150]: Training Loss: 0.739901978646398, Training Accuracy: 78.208\n",
            "Validation Loss: 2.216373413231722, Validation Accuracy: 53.03\n",
            "[145/150]: Training Loss: 0.7356343204560487, Training Accuracy: 78.332\n",
            "Validation Loss: 2.21599667087482, Validation Accuracy: 52.82\n",
            "[146/150]: Training Loss: 0.7417128108575216, Training Accuracy: 78.1\n",
            "Validation Loss: 2.216853774277268, Validation Accuracy: 53.06\n",
            "[147/150]: Training Loss: 0.7406589664385447, Training Accuracy: 78.444\n",
            "Validation Loss: 2.2168655562552675, Validation Accuracy: 52.95\n",
            "[148/150]: Training Loss: 0.7310213162694745, Training Accuracy: 78.62\n",
            "Validation Loss: 2.216719139913085, Validation Accuracy: 52.99\n",
            "[149/150]: Training Loss: 0.7304428781923431, Training Accuracy: 78.478\n",
            "Validation Loss: 2.216782635943905, Validation Accuracy: 53.01\n",
            "[150/150]: Training Loss: 0.7461780087493569, Training Accuracy: 78.148\n",
            "Validation Loss: 2.2168066478838586, Validation Accuracy: 53.01\n",
            "**********************************************************************\n",
            "Test Loss: 2.2168066478838586, Test Accuracy: 53.01\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>53.01</td></tr><tr><td>Test Loss</td><td>2.21681</td></tr><tr><td>Train Accuracy</td><td>78.148</td></tr><tr><td>Train Loss</td><td>0.74618</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate=0.0015 weight_decay=0.0004</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB/runs/62vz5e00</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_052010-62vz5e00/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "lr = 4.8/(2**5 *1e02) # approximately 15e-04\n",
        "wd = 4e-04\n",
        "\n",
        "hyperparameters = {'learning_rate': lr,\n",
        "                    'weight_decay' : wd\n",
        "                  }\n",
        "\n",
        "# Load the model\n",
        "model = LeNet5().to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer = LAMB(model.parameters(), learning_rate=lr, weight_decay=wd)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# Training\n",
        "run_training(num_epochs, model, original_train_loader, original_test_loader, original_test_loader, optimizer, scheduler, criterion, device, optimizer_name='LAMB', hyperparameters=hyperparameters, is_wandb = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 512\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_055120-629ppi9i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/629ppi9i' target=\"_blank\">batch_size=512 learning_rate=0.0004242640687119285 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/629ppi9i' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/629ppi9i</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.548603617415136, Training Accuracy: 2.498\n",
            "Validation Loss: 4.416820383071899, Validation Accuracy: 4.51\n",
            "[2/150]: Training Loss: 4.243599555930313, Training Accuracy: 5.9\n",
            "Validation Loss: 4.063300430774689, Validation Accuracy: 8.11\n",
            "[3/150]: Training Loss: 4.025548063978857, Training Accuracy: 8.684\n",
            "Validation Loss: 3.9094903230667115, Validation Accuracy: 10.6\n",
            "[4/150]: Training Loss: 3.9005869870283165, Training Accuracy: 10.854\n",
            "Validation Loss: 3.7815954089164734, Validation Accuracy: 13.47\n",
            "[5/150]: Training Loss: 3.7978148922628288, Training Accuracy: 12.58\n",
            "Validation Loss: 3.688620662689209, Validation Accuracy: 14.4\n",
            "[6/150]: Training Loss: 3.708504837386462, Training Accuracy: 14.016\n",
            "Validation Loss: 3.5979228973388673, Validation Accuracy: 15.97\n",
            "[7/150]: Training Loss: 3.6287196242079442, Training Accuracy: 15.174\n",
            "Validation Loss: 3.5210944056510924, Validation Accuracy: 17.53\n",
            "[8/150]: Training Loss: 3.5536105267855587, Training Accuracy: 16.508\n",
            "Validation Loss: 3.4404045224189757, Validation Accuracy: 19.09\n",
            "[9/150]: Training Loss: 3.4805082763944353, Training Accuracy: 17.65\n",
            "Validation Loss: 3.376018834114075, Validation Accuracy: 19.85\n",
            "[10/150]: Training Loss: 3.4161258960256773, Training Accuracy: 18.91\n",
            "Validation Loss: 3.303411877155304, Validation Accuracy: 21.76\n",
            "[11/150]: Training Loss: 3.3596596231265945, Training Accuracy: 19.954\n",
            "Validation Loss: 3.2550312638282777, Validation Accuracy: 22.38\n",
            "[12/150]: Training Loss: 3.299455146400296, Training Accuracy: 20.792\n",
            "Validation Loss: 3.2068899393081667, Validation Accuracy: 23.12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "Exception ignored in:     Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>if w.is_alive():\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "        self._shutdown_workers()Traceback (most recent call last):\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "    \n",
            "self._shutdown_workers()AssertionError  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "if w.is_alive():    : \n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "can only test a child processif w.is_alive():    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "    AssertionError\n",
            ": assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process\n",
            "\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Exception ignored in: Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "self._shutdown_workers()\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "        if w.is_alive():\n",
            "self._shutdown_workers()  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
            "if w.is_alive():AssertionError: can only test a child process\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "AssertionErrorTraceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            ":     can only test a child processself._shutdown_workers()\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40><function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    Traceback (most recent call last):\n",
            "self._shutdown_workers()  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    \n",
            "self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():if w.is_alive():\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionErrorAssertionError: : can only test a child processcan only test a child process\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[13/150]: Training Loss: 3.2494236060551236, Training Accuracy: 21.83\n",
            "Validation Loss: 3.150032651424408, Validation Accuracy: 24.52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "Exception ignored in:     assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "AssertionError: can only test a child process\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>AssertionError\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    : can only test a child processself._shutdown_workers()\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>can only test a child process\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    Exception ignored in: Exception ignored in: self._shutdown_workers()\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40><function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "Traceback (most recent call last):\n",
            "    Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "if w.is_alive():  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "        \n",
            "self._shutdown_workers()self._shutdown_workers()\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "            assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():if w.is_alive():Exception ignored in: \n",
            "\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>        \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n",
            "\n",
            "AssertionErrorAssertionError: assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            ": can only test a child process    can only test a child process\n",
            "AssertionErrorself._shutdown_workers()\n",
            "\n",
            ": \n",
            "can only test a child process\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[14/150]: Training Loss: 3.2080983264105662, Training Accuracy: 22.638\n",
            "Validation Loss: 3.109050440788269, Validation Accuracy: 24.66\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "        self._shutdown_workers()self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "if w.is_alive():\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "AssertionError: can only test a child process\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40><function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>self._shutdown_workers()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "        self._shutdown_workers()  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "self._shutdown_workers()\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "if w.is_alive():        \n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "if w.is_alive():if w.is_alive():    \n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'        \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError: \n",
            "can only test a child process\n",
            "\n",
            "AssertionErrorAssertionError: : can only test a child process\n",
            "can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15/150]: Training Loss: 3.1625750454104677, Training Accuracy: 23.334\n",
            "Validation Loss: 3.048142147064209, Validation Accuracy: 26.48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "Exception ignored in:   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "AssertionError: can only test a child process\n",
            "    self._shutdown_workers()Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():Traceback (most recent call last):\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "Exception ignored in:         self._shutdown_workers()assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "AssertionError:   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "    can only test a child processTraceback (most recent call last):\n",
            "if w.is_alive():  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "self._shutdown_workers()    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "    if w.is_alive():AssertionError: \n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "can only test a child processException ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "AssertionErrorException ignored in: :   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "can only test a child process<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>    \n",
            "\n",
            "self._shutdown_workers()\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "        if w.is_alive():self._shutdown_workers()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "AssertionError    : if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "can only test a child process    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Exception ignored in: Traceback (most recent call last):\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "if w.is_alive():    \n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "self._shutdown_workers()    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "AssertionError    : can only test a child process\n",
            "if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[16/150]: Training Loss: 3.1151485175502542, Training Accuracy: 24.318\n",
            "Validation Loss: 3.019995415210724, Validation Accuracy: 26.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child processException ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>    \n",
            "self._shutdown_workers()Traceback (most recent call last):\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "        if w.is_alive():\n",
            "self._shutdown_workers()  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    \n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionErrorif w.is_alive():: \n",
            "can only test a child process  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionErrorException ignored in: Exception ignored in: : <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40><function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "can only test a child processTraceback (most recent call last):\n",
            "\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "        self._shutdown_workers()Exception ignored in: self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "Traceback (most recent call last):\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "if w.is_alive():\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "        self._shutdown_workers()if w.is_alive():\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():\n",
            "\n",
            "\n",
            "AssertionError: AssertionErrorcan only test a child process: \n",
            "can only test a child process  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[17/150]: Training Loss: 3.074753547201351, Training Accuracy: 25.172\n",
            "Validation Loss: 2.9703970074653627, Validation Accuracy: 27.48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[18/150]: Training Loss: 3.036428164462654, Training Accuracy: 25.832\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "    Traceback (most recent call last):\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "if w.is_alive():    Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()Exception ignored in: \n",
            "self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "    \n",
            "\n",
            "Traceback (most recent call last):\n",
            "Exception ignored in:   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "    Traceback (most recent call last):\n",
            "    if w.is_alive():self._shutdown_workers()assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    \n",
            "if w.is_alive():    \n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "self._shutdown_workers()  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    Exception ignored in: AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    \n",
            "\n",
            ":     if w.is_alive():can only test a child processAssertionError  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>: \n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "\n",
            "        AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child processTraceback (most recent call last):\n",
            ": if w.is_alive():Exception ignored in: \n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "can only test a child process\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>AssertionError      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "\n",
            "self._shutdown_workers()\n",
            "    : assert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n",
            "can only test a child process  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "    self._shutdown_workers()\n",
            "AssertionError: \n",
            "    can only test a child processif w.is_alive():\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    \n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: if w.is_alive():can only test a child process\n",
            "\n",
            "Exception ignored in:   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "    Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError:     can only test a child processself._shutdown_workers()\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 2.91401789188385, Validation Accuracy: 28.63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "        self._shutdown_workers()\n",
            "self._shutdown_workers()  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "        assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():\n",
            "\n",
            "AssertionError:   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>AssertionError\n",
            ": Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "can only test a child process    self._shutdown_workers()\n",
            "Exception ignored in: \n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>if w.is_alive():\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "        assert self._parent_pid == os.getpid(), 'can only test a child process'Exception ignored in: self._shutdown_workers()\n",
            "\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>AssertionError:   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "can only test a child process\n",
            "    \n",
            "Traceback (most recent call last):\n",
            "if w.is_alive():  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    Exception ignored in: self._shutdown_workers()assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "AssertionError    if w.is_alive():: \n",
            "\n",
            "can only test a child process  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'    self._shutdown_workers()\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>AssertionError\n",
            ": \n",
            "can only test a child process  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "Traceback (most recent call last):\n",
            "\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    if w.is_alive():self._shutdown_workers()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child processAssertionError: can only test a child process\n",
            "Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[19/150]: Training Loss: 2.9957546457952384, Training Accuracy: 26.22\n",
            "Validation Loss: 2.8862305998802187, Validation Accuracy: 29.03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Exception ignored in: Traceback (most recent call last):\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "        assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():\n",
            "AssertionError\n",
            ": can only test a child process  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    \n",
            "Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "AssertionError\n",
            ": Traceback (most recent call last):\n",
            "can only test a child process  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>can only test a child process\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "Exception ignored in: Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>if w.is_alive():<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "Traceback (most recent call last):\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "    Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "self._shutdown_workers()AssertionError\n",
            ":     can only test a child process  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "self._shutdown_workers()\n",
            "    \n",
            "if w.is_alive():  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    \n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "if w.is_alive():Exception ignored in: \n",
            "    <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n",
            "\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "AssertionError: AssertionError    can only test a child process: \n",
            "can only test a child process\n",
            "self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[20/150]: Training Loss: 2.9593982185636247, Training Accuracy: 27.09\n",
            "Validation Loss: 2.8584514379501345, Validation Accuracy: 29.45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>Exception ignored in: \n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "Traceback (most recent call last):\n",
            "    if w.is_alive():\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "self._shutdown_workers()  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "        \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            ":     can only test a child processif w.is_alive():\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "AssertionError:   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "can only test a child process    \n",
            "self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>    self._shutdown_workers()\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "if w.is_alive():\n",
            "    self._shutdown_workers()Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "        self._shutdown_workers()    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "if w.is_alive():    \n",
            "\n",
            "AssertionError  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "if w.is_alive():: can only test a child process\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "\n",
            "AssertionError    assert self._parent_pid == os.getpid(), 'can only test a child process': \n",
            "can only test a child processAssertionError\n",
            ": can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[21/150]: Training Loss: 2.9291597142511483, Training Accuracy: 27.756\n",
            "Validation Loss: 2.823378837108612, Validation Accuracy: 30.07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "\n",
            "AssertionError: Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "can only test a child process    self._shutdown_workers()\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "Exception ignored in:     Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "\n",
            "\n",
            "AssertionErrorTraceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "Traceback (most recent call last):\n",
            ":   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    can only test a child process    \n",
            "self._shutdown_workers()self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "        if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "AssertionError  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            ":     can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "AssertionErrorException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>: \n",
            "Traceback (most recent call last):\n",
            "can only test a child process  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "Exception ignored in: \n",
            "    <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>self._shutdown_workers()Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "        if w.is_alive():\n",
            "\n",
            "Traceback (most recent call last):\n",
            "self._shutdown_workers()  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'self._shutdown_workers()    \n",
            "\n",
            "if w.is_alive():AssertionError: \n",
            "can only test a child process  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "        if w.is_alive():\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "AssertionError  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            ":     assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process\n",
            "\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[22/150]: Training Loss: 2.900624696089297, Training Accuracy: 28.334\n",
            "Validation Loss: 2.791938865184784, Validation Accuracy: 31.08\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    if w.is_alive():self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "\n",
            "AssertionError\n",
            "Traceback (most recent call last):\n",
            ":   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "can only test a child process    self._shutdown_workers()\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "Traceback (most recent call last):\n",
            "AssertionError  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers(): \n",
            "can only test a child process  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "Exception ignored in:     if w.is_alive():<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "Traceback (most recent call last):\n",
            "Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>assert self._parent_pid == os.getpid(), 'can only test a child process'Exception ignored in:   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "    <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "self._shutdown_workers()Traceback (most recent call last):\n",
            "AssertionError\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "Traceback (most recent call last):\n",
            ":     can only test a child process  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "if w.is_alive():        \n",
            "self._shutdown_workers()  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "\n",
            "self._shutdown_workers()\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'        \n",
            "if w.is_alive():AssertionError: \n",
            "if w.is_alive():can only test a child processException ignored in: \n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    Traceback (most recent call last):\n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "    \n",
            "AssertionErrorself._shutdown_workers()\n",
            ":   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "AssertionErrorcan only test a child process    \n",
            "if w.is_alive():: can only test a child process\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[23/150]: Training Loss: 2.8752694835468215, Training Accuracy: 28.62\n",
            "Validation Loss: 2.7719114065170287, Validation Accuracy: 30.97\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40><function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "          File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "if w.is_alive():  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "self._shutdown_workers()\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "        self._shutdown_workers()assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "    \n",
            "Exception ignored in:   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "AssertionErrorException ignored in: : <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "can only test a child processTraceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>    \n",
            "if w.is_alive():Traceback (most recent call last):\n",
            "    self._shutdown_workers()\n",
            "\n",
            "if w.is_alive():  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "            if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'self._shutdown_workers()\n",
            "\n",
            "\n",
            "Exception ignored in: AssertionError\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>can only test a child process:     \n",
            "        Exception ignored in: Traceback (most recent call last):\n",
            "\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>if w.is_alive():\n",
            "\n",
            "Exception ignored in: \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "AssertionError\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    : can only test a child process    assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n",
            "self._shutdown_workers()    \n",
            "Traceback (most recent call last):\n",
            ": \n",
            "AssertionError  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "self._shutdown_workers()can only test a child process: \n",
            "      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "can only test a child process\n",
            "    if w.is_alive():\n",
            "    self._shutdown_workers()\n",
            "\n",
            "if w.is_alive():  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "        assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "    if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n",
            "AssertionError: : \n",
            "can only test a child processcan only test a child process\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[24/150]: Training Loss: 2.844754442876699, Training Accuracy: 29.358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()Exception ignored in: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "if w.is_alive():Exception ignored in:     \n",
            "    Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>self._shutdown_workers()  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "self._shutdown_workers()    \n",
            "\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>Traceback (most recent call last):\n",
            "\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "        Traceback (most recent call last):\n",
            "AssertionError  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "self._shutdown_workers()  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "if w.is_alive():    \n",
            "\n",
            ": Exception ignored in:   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "if w.is_alive():    can only test a child process  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "    self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>if w.is_alive():    \n",
            "\n",
            "\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "        assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n",
            "if w.is_alive():    \n",
            "AssertionError\n",
            "self._shutdown_workers(): : \n",
            "AssertionErrorcan only test a child process  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "Exception ignored in: \n",
            "    <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>    if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "Traceback (most recent call last):\n",
            "\n",
            ": can only test a child process  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "can only test a child process    \n",
            "\n",
            "self._shutdown_workers()AssertionError:     \n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "if w.is_alive():\n",
            "\n",
            "AssertionError  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            ":     can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>AssertionError\n",
            "Traceback (most recent call last):\n",
            ":   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    can only test a child processself._shutdown_workers()\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 2.754125702381134, Validation Accuracy: 31.37\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>Exception ignored in: Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>Traceback (most recent call last):\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>\n",
            "\n",
            "\n",
            "\n",
            "    Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "self._shutdown_workers()Traceback (most recent call last):\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "self._shutdown_workers()      File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "Exception ignored in:   File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "        \n",
            "    <function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>if w.is_alive():    \n",
            "\n",
            "if w.is_alive():Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "Exception ignored in:     \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x74777b725b40>assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    \n",
            "\n",
            "self._shutdown_workers()self._shutdown_workers()AssertionErrorTraceback (most recent call last):\n",
            "    \n",
            ": can only test a child processself._shutdown_workers()\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "self._shutdown_workers()\n",
            "\n",
            "\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "        self._shutdown_workers()        assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "if w.is_alive():\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "AssertionErrorif w.is_alive():        \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            ": if w.is_alive():  File \"/home/ali/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    \n",
            "can only test a child process\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "if w.is_alive():\n",
            "\n",
            "        if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            ":   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    \n",
            "can only test a child processAssertionErrorAssertionError    : assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "can only test a child process\n",
            ": \n",
            "AssertionErrorcan only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "AssertionError: : can only test a child process\n",
            "can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[25/150]: Training Loss: 2.8207734166359413, Training Accuracy: 29.844\n",
            "Validation Loss: 2.705601465702057, Validation Accuracy: 32.25\n",
            "[26/150]: Training Loss: 2.794313367532224, Training Accuracy: 30.218\n",
            "Validation Loss: 2.687318742275238, Validation Accuracy: 32.36\n",
            "[27/150]: Training Loss: 2.7672535643285636, Training Accuracy: 31.0\n",
            "Validation Loss: 2.6692835092544556, Validation Accuracy: 33.62\n",
            "[28/150]: Training Loss: 2.7328374750760136, Training Accuracy: 31.594\n",
            "Validation Loss: 2.644136917591095, Validation Accuracy: 33.92\n",
            "[29/150]: Training Loss: 2.712839340677067, Training Accuracy: 32.242\n",
            "Validation Loss: 2.61125727891922, Validation Accuracy: 34.29\n",
            "[30/150]: Training Loss: 2.689220988020605, Training Accuracy: 32.716\n",
            "Validation Loss: 2.597063958644867, Validation Accuracy: 34.51\n",
            "[31/150]: Training Loss: 2.6592776678046404, Training Accuracy: 33.256\n",
            "Validation Loss: 2.5820674300193787, Validation Accuracy: 35.0\n",
            "[32/150]: Training Loss: 2.640095056319723, Training Accuracy: 33.314\n",
            "Validation Loss: 2.54053658246994, Validation Accuracy: 35.24\n",
            "[33/150]: Training Loss: 2.6196285121294918, Training Accuracy: 33.868\n",
            "Validation Loss: 2.5317852973937987, Validation Accuracy: 36.02\n",
            "[34/150]: Training Loss: 2.5938801741113466, Training Accuracy: 34.496\n",
            "Validation Loss: 2.5169005155563355, Validation Accuracy: 36.18\n",
            "[35/150]: Training Loss: 2.5702837875911166, Training Accuracy: 34.798\n",
            "Validation Loss: 2.4821427941322325, Validation Accuracy: 36.45\n",
            "[36/150]: Training Loss: 2.5516227970317917, Training Accuracy: 35.478\n",
            "Validation Loss: 2.4792662143707274, Validation Accuracy: 36.78\n",
            "[37/150]: Training Loss: 2.536287232321136, Training Accuracy: 35.748\n",
            "Validation Loss: 2.4615839600563048, Validation Accuracy: 36.91\n",
            "[38/150]: Training Loss: 2.518194748430836, Training Accuracy: 36.15\n",
            "Validation Loss: 2.4392840027809144, Validation Accuracy: 37.61\n",
            "[39/150]: Training Loss: 2.4998596444421883, Training Accuracy: 36.174\n",
            "Validation Loss: 2.4394460558891295, Validation Accuracy: 37.83\n",
            "[40/150]: Training Loss: 2.478595862583238, Training Accuracy: 36.792\n",
            "Validation Loss: 2.4048784017562865, Validation Accuracy: 38.4\n",
            "[41/150]: Training Loss: 2.4725446628064525, Training Accuracy: 36.98\n",
            "Validation Loss: 2.419749367237091, Validation Accuracy: 37.61\n",
            "[42/150]: Training Loss: 2.447700734041175, Training Accuracy: 37.712\n",
            "Validation Loss: 2.397221338748932, Validation Accuracy: 38.39\n",
            "[43/150]: Training Loss: 2.437787973150915, Training Accuracy: 37.604\n",
            "Validation Loss: 2.376715588569641, Validation Accuracy: 39.06\n",
            "[44/150]: Training Loss: 2.41146419486221, Training Accuracy: 38.236\n",
            "Validation Loss: 2.383975315093994, Validation Accuracy: 38.93\n",
            "[45/150]: Training Loss: 2.400857081218642, Training Accuracy: 38.53\n",
            "Validation Loss: 2.3536798119544984, Validation Accuracy: 39.75\n",
            "[46/150]: Training Loss: 2.3903429873135624, Training Accuracy: 38.942\n",
            "Validation Loss: 2.3401002407073976, Validation Accuracy: 39.98\n",
            "[47/150]: Training Loss: 2.3778630105816587, Training Accuracy: 38.864\n",
            "Validation Loss: 2.3238447666168214, Validation Accuracy: 40.35\n",
            "[48/150]: Training Loss: 2.3627792669802297, Training Accuracy: 39.344\n",
            "Validation Loss: 2.3337461233139036, Validation Accuracy: 40.21\n",
            "[49/150]: Training Loss: 2.341205180907736, Training Accuracy: 39.73\n",
            "Validation Loss: 2.31098872423172, Validation Accuracy: 40.22\n",
            "[50/150]: Training Loss: 2.334307079412499, Training Accuracy: 40.016\n",
            "Validation Loss: 2.315126097202301, Validation Accuracy: 40.58\n",
            "[51/150]: Training Loss: 2.3188869831513386, Training Accuracy: 40.006\n",
            "Validation Loss: 2.295890510082245, Validation Accuracy: 40.87\n",
            "[52/150]: Training Loss: 2.305944788212679, Training Accuracy: 40.576\n",
            "Validation Loss: 2.2844882011413574, Validation Accuracy: 41.3\n",
            "[53/150]: Training Loss: 2.3018058027539934, Training Accuracy: 40.752\n",
            "Validation Loss: 2.2773577690124513, Validation Accuracy: 41.28\n",
            "[54/150]: Training Loss: 2.2820585503870126, Training Accuracy: 41.126\n",
            "Validation Loss: 2.265846014022827, Validation Accuracy: 41.54\n",
            "[55/150]: Training Loss: 2.2734045617434444, Training Accuracy: 41.172\n",
            "Validation Loss: 2.2698575258255005, Validation Accuracy: 41.55\n",
            "[56/150]: Training Loss: 2.268057511777294, Training Accuracy: 41.382\n",
            "Validation Loss: 2.248699498176575, Validation Accuracy: 42.0\n",
            "[57/150]: Training Loss: 2.2563396935560265, Training Accuracy: 41.77\n",
            "Validation Loss: 2.242726683616638, Validation Accuracy: 41.71\n",
            "[58/150]: Training Loss: 2.2405153902209536, Training Accuracy: 41.894\n",
            "Validation Loss: 2.2411133885383605, Validation Accuracy: 42.32\n",
            "[59/150]: Training Loss: 2.233138741279135, Training Accuracy: 41.998\n",
            "Validation Loss: 2.2358715295791627, Validation Accuracy: 42.37\n",
            "[60/150]: Training Loss: 2.2201552464037526, Training Accuracy: 42.37\n",
            "Validation Loss: 2.2211275577545164, Validation Accuracy: 42.25\n",
            "[61/150]: Training Loss: 2.2125545107588476, Training Accuracy: 42.374\n",
            "Validation Loss: 2.22549809217453, Validation Accuracy: 42.04\n",
            "[62/150]: Training Loss: 2.2010000238613205, Training Accuracy: 42.592\n",
            "Validation Loss: 2.209044873714447, Validation Accuracy: 42.73\n",
            "[63/150]: Training Loss: 2.189410664597336, Training Accuracy: 43.022\n",
            "Validation Loss: 2.2072029948234557, Validation Accuracy: 43.21\n",
            "[64/150]: Training Loss: 2.181582011738602, Training Accuracy: 43.196\n",
            "Validation Loss: 2.211189329624176, Validation Accuracy: 42.98\n",
            "[65/150]: Training Loss: 2.17209978006324, Training Accuracy: 43.494\n",
            "Validation Loss: 2.1931957960128785, Validation Accuracy: 43.09\n",
            "[66/150]: Training Loss: 2.159282922744751, Training Accuracy: 43.83\n",
            "Validation Loss: 2.185472071170807, Validation Accuracy: 43.41\n",
            "[67/150]: Training Loss: 2.1514484091680877, Training Accuracy: 43.87\n",
            "Validation Loss: 2.1950502753257752, Validation Accuracy: 43.24\n",
            "[68/150]: Training Loss: 2.146765425497172, Training Accuracy: 43.912\n",
            "Validation Loss: 2.1978687405586244, Validation Accuracy: 42.73\n",
            "[69/150]: Training Loss: 2.1396148119653975, Training Accuracy: 44.17\n",
            "Validation Loss: 2.175988519191742, Validation Accuracy: 43.64\n",
            "[70/150]: Training Loss: 2.124094669916192, Training Accuracy: 44.388\n",
            "Validation Loss: 2.1625670552253724, Validation Accuracy: 44.15\n",
            "[71/150]: Training Loss: 2.118907108598826, Training Accuracy: 44.714\n",
            "Validation Loss: 2.1682013154029844, Validation Accuracy: 44.16\n",
            "[72/150]: Training Loss: 2.1218399077045675, Training Accuracy: 44.516\n",
            "Validation Loss: 2.1531296133995057, Validation Accuracy: 44.24\n",
            "[73/150]: Training Loss: 2.103327241479134, Training Accuracy: 44.822\n",
            "Validation Loss: 2.149845826625824, Validation Accuracy: 44.42\n",
            "[74/150]: Training Loss: 2.1007259342135214, Training Accuracy: 45.092\n",
            "Validation Loss: 2.149185609817505, Validation Accuracy: 44.29\n",
            "[75/150]: Training Loss: 2.0860105752944946, Training Accuracy: 45.244\n",
            "Validation Loss: 2.153393119573593, Validation Accuracy: 44.03\n",
            "[76/150]: Training Loss: 2.0842377008224022, Training Accuracy: 45.118\n",
            "Validation Loss: 2.1478596448898317, Validation Accuracy: 44.41\n",
            "[77/150]: Training Loss: 2.073838080678667, Training Accuracy: 45.74\n",
            "Validation Loss: 2.1420213758945463, Validation Accuracy: 44.47\n",
            "[78/150]: Training Loss: 2.064723936878905, Training Accuracy: 45.914\n",
            "Validation Loss: 2.1414406061172486, Validation Accuracy: 44.65\n",
            "[79/150]: Training Loss: 2.068708761614196, Training Accuracy: 45.76\n",
            "Validation Loss: 2.129684728384018, Validation Accuracy: 44.69\n",
            "[80/150]: Training Loss: 2.05464696640871, Training Accuracy: 45.966\n",
            "Validation Loss: 2.1296287894248964, Validation Accuracy: 44.82\n",
            "[81/150]: Training Loss: 2.048685365793656, Training Accuracy: 45.972\n",
            "Validation Loss: 2.1220090091228485, Validation Accuracy: 45.16\n",
            "[82/150]: Training Loss: 2.043823238538236, Training Accuracy: 46.17\n",
            "Validation Loss: 2.121674305200577, Validation Accuracy: 44.82\n",
            "[83/150]: Training Loss: 2.0398333644380373, Training Accuracy: 46.582\n",
            "Validation Loss: 2.1206820785999296, Validation Accuracy: 45.25\n",
            "[84/150]: Training Loss: 2.0399894969803944, Training Accuracy: 46.332\n",
            "Validation Loss: 2.117530012130737, Validation Accuracy: 45.12\n",
            "[85/150]: Training Loss: 2.020768941665182, Training Accuracy: 46.63\n",
            "Validation Loss: 2.116836541891098, Validation Accuracy: 45.3\n",
            "[86/150]: Training Loss: 2.0279355182939645, Training Accuracy: 46.574\n",
            "Validation Loss: 2.107444965839386, Validation Accuracy: 45.62\n",
            "[87/150]: Training Loss: 2.020287135425879, Training Accuracy: 46.76\n",
            "Validation Loss: 2.10392941236496, Validation Accuracy: 45.62\n",
            "[88/150]: Training Loss: 2.0118079963995488, Training Accuracy: 46.982\n",
            "Validation Loss: 2.10864252448082, Validation Accuracy: 45.04\n",
            "[89/150]: Training Loss: 2.0041175241373024, Training Accuracy: 47.0\n",
            "Validation Loss: 2.1043585777282714, Validation Accuracy: 45.51\n",
            "[90/150]: Training Loss: 1.997280935851895, Training Accuracy: 47.278\n",
            "Validation Loss: 2.090637058019638, Validation Accuracy: 45.85\n",
            "[91/150]: Training Loss: 1.9916248126905791, Training Accuracy: 47.574\n",
            "Validation Loss: 2.0972966492176055, Validation Accuracy: 45.5\n",
            "[92/150]: Training Loss: 1.9858960862062416, Training Accuracy: 47.498\n",
            "Validation Loss: 2.089981472492218, Validation Accuracy: 45.74\n",
            "[93/150]: Training Loss: 1.981804991255001, Training Accuracy: 47.632\n",
            "Validation Loss: 2.0950992584228514, Validation Accuracy: 45.79\n",
            "[94/150]: Training Loss: 1.9809188794116586, Training Accuracy: 47.736\n",
            "Validation Loss: 2.0917014718055724, Validation Accuracy: 45.81\n",
            "[95/150]: Training Loss: 1.9745077296179168, Training Accuracy: 48.028\n",
            "Validation Loss: 2.0903584539890288, Validation Accuracy: 45.82\n",
            "[96/150]: Training Loss: 1.9678826964631373, Training Accuracy: 48.062\n",
            "Validation Loss: 2.085793948173523, Validation Accuracy: 45.88\n",
            "[97/150]: Training Loss: 1.969686961903864, Training Accuracy: 47.974\n",
            "Validation Loss: 2.0842667639255525, Validation Accuracy: 45.94\n",
            "[98/150]: Training Loss: 1.967830895161142, Training Accuracy: 48.11\n",
            "Validation Loss: 2.0819462299346925, Validation Accuracy: 45.77\n",
            "[99/150]: Training Loss: 1.9539092487218428, Training Accuracy: 48.09\n",
            "Validation Loss: 2.0695468485355377, Validation Accuracy: 46.38\n",
            "[100/150]: Training Loss: 1.9470370752470834, Training Accuracy: 48.734\n",
            "Validation Loss: 2.080887311697006, Validation Accuracy: 46.05\n",
            "[101/150]: Training Loss: 1.9548694807655957, Training Accuracy: 48.234\n",
            "Validation Loss: 2.071767729520798, Validation Accuracy: 46.49\n",
            "[102/150]: Training Loss: 1.9464308716812913, Training Accuracy: 48.376\n",
            "Validation Loss: 2.0714339315891266, Validation Accuracy: 46.27\n",
            "[103/150]: Training Loss: 1.9454800291937224, Training Accuracy: 48.468\n",
            "Validation Loss: 2.069366031885147, Validation Accuracy: 46.16\n",
            "[104/150]: Training Loss: 1.9380488541661476, Training Accuracy: 48.806\n",
            "Validation Loss: 2.073972535133362, Validation Accuracy: 46.41\n",
            "[105/150]: Training Loss: 1.9342538422467757, Training Accuracy: 48.66\n",
            "Validation Loss: 2.068258821964264, Validation Accuracy: 46.17\n",
            "[106/150]: Training Loss: 1.937512426960225, Training Accuracy: 48.806\n",
            "Validation Loss: 2.065845030546188, Validation Accuracy: 46.3\n",
            "[107/150]: Training Loss: 1.930478497427337, Training Accuracy: 48.82\n",
            "Validation Loss: 2.0694516718387606, Validation Accuracy: 46.33\n",
            "[108/150]: Training Loss: 1.9285986253193446, Training Accuracy: 48.574\n",
            "Validation Loss: 2.0660508811473846, Validation Accuracy: 46.56\n",
            "[109/150]: Training Loss: 1.9226828570268593, Training Accuracy: 48.984\n",
            "Validation Loss: 2.065355122089386, Validation Accuracy: 46.38\n",
            "[110/150]: Training Loss: 1.9163579600197929, Training Accuracy: 49.164\n",
            "Validation Loss: 2.0628578305244445, Validation Accuracy: 46.46\n",
            "[111/150]: Training Loss: 1.9168027821852236, Training Accuracy: 49.252\n",
            "Validation Loss: 2.0593102276325226, Validation Accuracy: 46.63\n",
            "[112/150]: Training Loss: 1.9165059583527702, Training Accuracy: 49.328\n",
            "Validation Loss: 2.0612095296382904, Validation Accuracy: 46.65\n",
            "[113/150]: Training Loss: 1.9075169830906147, Training Accuracy: 49.53\n",
            "Validation Loss: 2.0582003355026246, Validation Accuracy: 46.64\n",
            "[114/150]: Training Loss: 1.9039872094076506, Training Accuracy: 49.556\n",
            "Validation Loss: 2.057022488117218, Validation Accuracy: 46.46\n",
            "[115/150]: Training Loss: 1.9039045523624032, Training Accuracy: 49.45\n",
            "Validation Loss: 2.058062309026718, Validation Accuracy: 46.66\n",
            "[116/150]: Training Loss: 1.9059245598559478, Training Accuracy: 49.46\n",
            "Validation Loss: 2.0549796164035796, Validation Accuracy: 46.98\n",
            "[117/150]: Training Loss: 1.909138171040282, Training Accuracy: 49.394\n",
            "Validation Loss: 2.0545377254486086, Validation Accuracy: 46.76\n",
            "[118/150]: Training Loss: 1.9025853884463408, Training Accuracy: 49.46\n",
            "Validation Loss: 2.0527754604816435, Validation Accuracy: 46.83\n",
            "[119/150]: Training Loss: 1.8941444937063723, Training Accuracy: 49.826\n",
            "Validation Loss: 2.055216532945633, Validation Accuracy: 46.61\n",
            "[120/150]: Training Loss: 1.8918506807210493, Training Accuracy: 49.522\n",
            "Validation Loss: 2.0529846370220186, Validation Accuracy: 46.66\n",
            "[121/150]: Training Loss: 1.8913645014470937, Training Accuracy: 49.806\n",
            "Validation Loss: 2.053318554162979, Validation Accuracy: 46.87\n",
            "[122/150]: Training Loss: 1.8967320079706154, Training Accuracy: 49.708\n",
            "Validation Loss: 2.0482302486896513, Validation Accuracy: 46.95\n",
            "[123/150]: Training Loss: 1.8930062055587769, Training Accuracy: 49.736\n",
            "Validation Loss: 2.0486309468746184, Validation Accuracy: 46.98\n",
            "[124/150]: Training Loss: 1.8859221606838459, Training Accuracy: 49.948\n",
            "Validation Loss: 2.047211617231369, Validation Accuracy: 46.98\n",
            "[125/150]: Training Loss: 1.8830693084366468, Training Accuracy: 50.07\n",
            "Validation Loss: 2.0473871648311617, Validation Accuracy: 46.98\n",
            "[126/150]: Training Loss: 1.8846859773811029, Training Accuracy: 50.09\n",
            "Validation Loss: 2.04728844165802, Validation Accuracy: 47.16\n",
            "[127/150]: Training Loss: 1.8857315279999558, Training Accuracy: 49.906\n",
            "Validation Loss: 2.046469569206238, Validation Accuracy: 47.04\n",
            "[128/150]: Training Loss: 1.8859595972664502, Training Accuracy: 49.9\n",
            "Validation Loss: 2.046849066019058, Validation Accuracy: 46.92\n",
            "[129/150]: Training Loss: 1.8825436745371138, Training Accuracy: 49.864\n",
            "Validation Loss: 2.048234748840332, Validation Accuracy: 46.97\n",
            "[130/150]: Training Loss: 1.8798605483405444, Training Accuracy: 49.902\n",
            "Validation Loss: 2.045301067829132, Validation Accuracy: 47.01\n",
            "[131/150]: Training Loss: 1.8820846980931807, Training Accuracy: 49.88\n",
            "Validation Loss: 2.0458612203598023, Validation Accuracy: 47.06\n",
            "[132/150]: Training Loss: 1.8795445658722703, Training Accuracy: 50.058\n",
            "Validation Loss: 2.0437388598918913, Validation Accuracy: 47.08\n",
            "[133/150]: Training Loss: 1.876546278291819, Training Accuracy: 50.138\n",
            "Validation Loss: 2.04643372297287, Validation Accuracy: 46.93\n",
            "[134/150]: Training Loss: 1.874571469365334, Training Accuracy: 50.216\n",
            "Validation Loss: 2.0444207012653353, Validation Accuracy: 46.99\n",
            "[135/150]: Training Loss: 1.878583361907881, Training Accuracy: 50.102\n",
            "Validation Loss: 2.043903875350952, Validation Accuracy: 47.0\n",
            "[136/150]: Training Loss: 1.8731432812554496, Training Accuracy: 50.264\n",
            "Validation Loss: 2.043297988176346, Validation Accuracy: 47.04\n",
            "[137/150]: Training Loss: 1.8719240147240308, Training Accuracy: 50.256\n",
            "Validation Loss: 2.044253832101822, Validation Accuracy: 46.85\n",
            "[138/150]: Training Loss: 1.8687705020515286, Training Accuracy: 50.026\n",
            "Validation Loss: 2.0435604214668275, Validation Accuracy: 47.0\n",
            "[139/150]: Training Loss: 1.8780759548654362, Training Accuracy: 50.126\n",
            "Validation Loss: 2.0427211821079254, Validation Accuracy: 47.05\n",
            "[140/150]: Training Loss: 1.8736839489061006, Training Accuracy: 50.238\n",
            "Validation Loss: 2.0425303280353546, Validation Accuracy: 47.02\n",
            "[141/150]: Training Loss: 1.8668698303553524, Training Accuracy: 50.304\n",
            "Validation Loss: 2.042210203409195, Validation Accuracy: 47.06\n",
            "[142/150]: Training Loss: 1.872799194588953, Training Accuracy: 50.112\n",
            "Validation Loss: 2.0424644231796263, Validation Accuracy: 46.97\n",
            "[143/150]: Training Loss: 1.8772344613561824, Training Accuracy: 50.41\n",
            "Validation Loss: 2.0423132359981535, Validation Accuracy: 47.03\n",
            "[144/150]: Training Loss: 1.8697663484787455, Training Accuracy: 50.348\n",
            "Validation Loss: 2.042390114068985, Validation Accuracy: 46.96\n",
            "[145/150]: Training Loss: 1.8656348148170783, Training Accuracy: 50.32\n",
            "Validation Loss: 2.0424346685409547, Validation Accuracy: 47.0\n",
            "[146/150]: Training Loss: 1.8737119424099824, Training Accuracy: 50.538\n",
            "Validation Loss: 2.0425304055213926, Validation Accuracy: 47.04\n",
            "[147/150]: Training Loss: 1.8692006566086594, Training Accuracy: 50.398\n",
            "Validation Loss: 2.0425583481788636, Validation Accuracy: 46.95\n",
            "[148/150]: Training Loss: 1.8687953511062934, Training Accuracy: 50.346\n",
            "Validation Loss: 2.042505383491516, Validation Accuracy: 46.94\n",
            "[149/150]: Training Loss: 1.871606239250728, Training Accuracy: 50.198\n",
            "Validation Loss: 2.042434561252594, Validation Accuracy: 46.95\n",
            "[150/150]: Training Loss: 1.866833773194527, Training Accuracy: 50.392\n",
            "Validation Loss: 2.0424169659614564, Validation Accuracy: 46.95\n",
            "**********************************************************************\n",
            "Test Loss: 2.0424169659614564, Test Accuracy: 46.95\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>46.95</td></tr><tr><td>Test Loss</td><td>2.04242</td></tr><tr><td>Train Accuracy</td><td>50.392</td></tr><tr><td>Train Loss</td><td>1.86683</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=512 learning_rate=0.0004242640687119285 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/629ppi9i' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/629ppi9i</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_055120-629ppi9i/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 1024\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_062343-f6zj2dy6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/f6zj2dy6' target=\"_blank\">batch_size=1024 learning_rate=0.0006 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/f6zj2dy6' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/f6zj2dy6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.590637878495819, Training Accuracy: 1.992\n",
            "Validation Loss: 4.556918144226074, Validation Accuracy: 3.09\n",
            "[2/150]: Training Loss: 4.497767944725192, Training Accuracy: 3.494\n",
            "Validation Loss: 4.4074239253997805, Validation Accuracy: 4.76\n",
            "[3/150]: Training Loss: 4.326443146686165, Training Accuracy: 4.914\n",
            "Validation Loss: 4.206428670883179, Validation Accuracy: 6.39\n",
            "[4/150]: Training Loss: 4.1541692480749015, Training Accuracy: 7.096\n",
            "Validation Loss: 4.048169994354248, Validation Accuracy: 8.63\n",
            "[5/150]: Training Loss: 4.041794528766554, Training Accuracy: 8.766\n",
            "Validation Loss: 3.943867230415344, Validation Accuracy: 10.57\n",
            "[6/150]: Training Loss: 3.9511346038506954, Training Accuracy: 10.2\n",
            "Validation Loss: 3.8476075649261476, Validation Accuracy: 12.17\n",
            "[7/150]: Training Loss: 3.8759255409240723, Training Accuracy: 11.486\n",
            "Validation Loss: 3.772109270095825, Validation Accuracy: 13.26\n",
            "[8/150]: Training Loss: 3.8086607310236715, Training Accuracy: 12.306\n",
            "Validation Loss: 3.715940999984741, Validation Accuracy: 14.08\n",
            "[9/150]: Training Loss: 3.7540337260888546, Training Accuracy: 13.228\n",
            "Validation Loss: 3.6463849544525146, Validation Accuracy: 15.81\n",
            "[10/150]: Training Loss: 3.70257940097731, Training Accuracy: 14.254\n",
            "Validation Loss: 3.6008975744247436, Validation Accuracy: 16.58\n",
            "[11/150]: Training Loss: 3.6446368256393744, Training Accuracy: 15.076\n",
            "Validation Loss: 3.537614035606384, Validation Accuracy: 17.38\n",
            "[12/150]: Training Loss: 3.5920012045879752, Training Accuracy: 16.168\n",
            "Validation Loss: 3.498702359199524, Validation Accuracy: 17.75\n",
            "[13/150]: Training Loss: 3.5408709195195414, Training Accuracy: 16.708\n",
            "Validation Loss: 3.435236358642578, Validation Accuracy: 19.14\n",
            "[14/150]: Training Loss: 3.4937274747965286, Training Accuracy: 17.706\n",
            "Validation Loss: 3.388493609428406, Validation Accuracy: 19.67\n",
            "[15/150]: Training Loss: 3.452468677442901, Training Accuracy: 18.38\n",
            "Validation Loss: 3.3595022439956663, Validation Accuracy: 20.31\n",
            "[16/150]: Training Loss: 3.41387711252485, Training Accuracy: 19.108\n",
            "Validation Loss: 3.3140936374664305, Validation Accuracy: 21.56\n",
            "[17/150]: Training Loss: 3.3753939258808994, Training Accuracy: 19.74\n",
            "Validation Loss: 3.275451421737671, Validation Accuracy: 22.49\n",
            "[18/150]: Training Loss: 3.338460786002023, Training Accuracy: 20.488\n",
            "Validation Loss: 3.233375310897827, Validation Accuracy: 23.1\n",
            "[19/150]: Training Loss: 3.3114829841925175, Training Accuracy: 21.016\n",
            "Validation Loss: 3.2035989284515383, Validation Accuracy: 23.04\n",
            "[20/150]: Training Loss: 3.2761189499679877, Training Accuracy: 21.566\n",
            "Validation Loss: 3.177353930473328, Validation Accuracy: 24.2\n",
            "[21/150]: Training Loss: 3.239693442169501, Training Accuracy: 22.324\n",
            "Validation Loss: 3.143001365661621, Validation Accuracy: 25.15\n",
            "[22/150]: Training Loss: 3.2111696467107658, Training Accuracy: 22.926\n",
            "Validation Loss: 3.1071121215820314, Validation Accuracy: 25.41\n",
            "[23/150]: Training Loss: 3.1833992928874735, Training Accuracy: 23.484\n",
            "Validation Loss: 3.0789601802825928, Validation Accuracy: 26.25\n",
            "[24/150]: Training Loss: 3.1474135603223528, Training Accuracy: 23.748\n",
            "Validation Loss: 3.0431544542312623, Validation Accuracy: 26.72\n",
            "[25/150]: Training Loss: 3.126202072416033, Training Accuracy: 24.388\n",
            "Validation Loss: 3.025442361831665, Validation Accuracy: 26.62\n",
            "[26/150]: Training Loss: 3.0991044433749453, Training Accuracy: 24.718\n",
            "Validation Loss: 2.9974936485290526, Validation Accuracy: 27.13\n",
            "[27/150]: Training Loss: 3.073319736792117, Training Accuracy: 25.346\n",
            "Validation Loss: 2.9764548778533935, Validation Accuracy: 27.54\n",
            "[28/150]: Training Loss: 3.0423376754838594, Training Accuracy: 25.946\n",
            "Validation Loss: 2.9351656436920166, Validation Accuracy: 28.32\n",
            "[29/150]: Training Loss: 3.013648816517421, Training Accuracy: 26.418\n",
            "Validation Loss: 2.919317698478699, Validation Accuracy: 28.99\n",
            "[30/150]: Training Loss: 2.9954160281590054, Training Accuracy: 26.764\n",
            "Validation Loss: 2.892651009559631, Validation Accuracy: 29.48\n",
            "[31/150]: Training Loss: 2.967517526782289, Training Accuracy: 27.272\n",
            "Validation Loss: 2.863291382789612, Validation Accuracy: 29.98\n",
            "[32/150]: Training Loss: 2.9497327609938018, Training Accuracy: 27.71\n",
            "Validation Loss: 2.855112910270691, Validation Accuracy: 29.8\n",
            "[33/150]: Training Loss: 2.9298942429678783, Training Accuracy: 27.868\n",
            "Validation Loss: 2.8315301179885863, Validation Accuracy: 30.54\n",
            "[34/150]: Training Loss: 2.906733279325524, Training Accuracy: 28.366\n",
            "Validation Loss: 2.7987032413482664, Validation Accuracy: 31.06\n",
            "[35/150]: Training Loss: 2.891595748006081, Training Accuracy: 28.684\n",
            "Validation Loss: 2.7815486907958986, Validation Accuracy: 31.66\n",
            "[36/150]: Training Loss: 2.8660872663770403, Training Accuracy: 29.228\n",
            "Validation Loss: 2.76305890083313, Validation Accuracy: 31.86\n",
            "[37/150]: Training Loss: 2.8532854245633494, Training Accuracy: 29.276\n",
            "Validation Loss: 2.7485848903656005, Validation Accuracy: 32.31\n",
            "[38/150]: Training Loss: 2.8363813964688047, Training Accuracy: 29.626\n",
            "Validation Loss: 2.741732931137085, Validation Accuracy: 32.37\n",
            "[39/150]: Training Loss: 2.817852025129357, Training Accuracy: 30.356\n",
            "Validation Loss: 2.716148281097412, Validation Accuracy: 32.52\n",
            "[40/150]: Training Loss: 2.7965505123138428, Training Accuracy: 30.468\n",
            "Validation Loss: 2.6954785108566286, Validation Accuracy: 33.26\n",
            "[41/150]: Training Loss: 2.777193152174658, Training Accuracy: 30.91\n",
            "Validation Loss: 2.689286541938782, Validation Accuracy: 33.33\n",
            "[42/150]: Training Loss: 2.7633943460425554, Training Accuracy: 30.874\n",
            "Validation Loss: 2.6628729820251467, Validation Accuracy: 33.54\n",
            "[43/150]: Training Loss: 2.7435058331002993, Training Accuracy: 31.472\n",
            "Validation Loss: 2.6562008619308473, Validation Accuracy: 33.72\n",
            "[44/150]: Training Loss: 2.725325599008677, Training Accuracy: 31.994\n",
            "Validation Loss: 2.644929838180542, Validation Accuracy: 33.83\n",
            "[45/150]: Training Loss: 2.7060077579653994, Training Accuracy: 32.404\n",
            "Validation Loss: 2.6198429346084593, Validation Accuracy: 34.2\n",
            "[46/150]: Training Loss: 2.6917145592825755, Training Accuracy: 32.636\n",
            "Validation Loss: 2.60967857837677, Validation Accuracy: 34.5\n",
            "[47/150]: Training Loss: 2.683579221063731, Training Accuracy: 32.58\n",
            "Validation Loss: 2.5991793870925903, Validation Accuracy: 35.03\n",
            "[48/150]: Training Loss: 2.6728540683279234, Training Accuracy: 33.022\n",
            "Validation Loss: 2.59100227355957, Validation Accuracy: 35.07\n",
            "[49/150]: Training Loss: 2.6557982454494553, Training Accuracy: 33.11\n",
            "Validation Loss: 2.577105498313904, Validation Accuracy: 35.14\n",
            "[50/150]: Training Loss: 2.6382284748310947, Training Accuracy: 33.666\n",
            "Validation Loss: 2.55712034702301, Validation Accuracy: 35.52\n",
            "[51/150]: Training Loss: 2.6272740947956943, Training Accuracy: 33.998\n",
            "Validation Loss: 2.5471322298049928, Validation Accuracy: 35.88\n",
            "[52/150]: Training Loss: 2.618494077604644, Training Accuracy: 34.04\n",
            "Validation Loss: 2.5421458959579466, Validation Accuracy: 35.73\n",
            "[53/150]: Training Loss: 2.598925138006405, Training Accuracy: 34.508\n",
            "Validation Loss: 2.536887764930725, Validation Accuracy: 36.18\n",
            "[54/150]: Training Loss: 2.5985752806371574, Training Accuracy: 34.666\n",
            "Validation Loss: 2.516120505332947, Validation Accuracy: 36.52\n",
            "[55/150]: Training Loss: 2.580332104040652, Training Accuracy: 34.912\n",
            "Validation Loss: 2.510345458984375, Validation Accuracy: 36.59\n",
            "[56/150]: Training Loss: 2.569650562442079, Training Accuracy: 34.968\n",
            "Validation Loss: 2.498000979423523, Validation Accuracy: 37.16\n",
            "[57/150]: Training Loss: 2.5618990878669585, Training Accuracy: 35.18\n",
            "Validation Loss: 2.495281624794006, Validation Accuracy: 37.19\n",
            "[58/150]: Training Loss: 2.5537893236899865, Training Accuracy: 35.348\n",
            "Validation Loss: 2.484134078025818, Validation Accuracy: 36.73\n",
            "[59/150]: Training Loss: 2.54392485715905, Training Accuracy: 35.704\n",
            "Validation Loss: 2.4660497426986696, Validation Accuracy: 37.66\n",
            "[60/150]: Training Loss: 2.5297549744041596, Training Accuracy: 35.884\n",
            "Validation Loss: 2.457916569709778, Validation Accuracy: 37.36\n",
            "[61/150]: Training Loss: 2.515256760071735, Training Accuracy: 36.114\n",
            "Validation Loss: 2.45170681476593, Validation Accuracy: 37.98\n",
            "[62/150]: Training Loss: 2.511414012130426, Training Accuracy: 36.192\n",
            "Validation Loss: 2.4506330490112305, Validation Accuracy: 37.83\n",
            "[63/150]: Training Loss: 2.505029770792747, Training Accuracy: 36.422\n",
            "Validation Loss: 2.4473904848098753, Validation Accuracy: 37.55\n",
            "[64/150]: Training Loss: 2.4959285015962562, Training Accuracy: 36.62\n",
            "Validation Loss: 2.438623309135437, Validation Accuracy: 37.77\n",
            "[65/150]: Training Loss: 2.4828718146499322, Training Accuracy: 36.9\n",
            "Validation Loss: 2.4237023115158083, Validation Accuracy: 38.23\n",
            "[66/150]: Training Loss: 2.4754056638600876, Training Accuracy: 37.1\n",
            "Validation Loss: 2.425072741508484, Validation Accuracy: 38.56\n",
            "[67/150]: Training Loss: 2.4672083854675293, Training Accuracy: 37.082\n",
            "Validation Loss: 2.411469268798828, Validation Accuracy: 38.18\n",
            "[68/150]: Training Loss: 2.46198515502774, Training Accuracy: 37.322\n",
            "Validation Loss: 2.409951162338257, Validation Accuracy: 38.86\n",
            "[69/150]: Training Loss: 2.4538968436572017, Training Accuracy: 37.764\n",
            "Validation Loss: 2.391971492767334, Validation Accuracy: 39.04\n",
            "[70/150]: Training Loss: 2.4462242953631343, Training Accuracy: 37.57\n",
            "Validation Loss: 2.394611191749573, Validation Accuracy: 38.71\n",
            "[71/150]: Training Loss: 2.429029284691324, Training Accuracy: 38.192\n",
            "Validation Loss: 2.386702823638916, Validation Accuracy: 38.88\n",
            "[72/150]: Training Loss: 2.43276318238706, Training Accuracy: 38.076\n",
            "Validation Loss: 2.3796674728393556, Validation Accuracy: 39.05\n",
            "[73/150]: Training Loss: 2.4164321957802284, Training Accuracy: 38.248\n",
            "Validation Loss: 2.3743354082107544, Validation Accuracy: 39.5\n",
            "[74/150]: Training Loss: 2.4130708344128666, Training Accuracy: 38.542\n",
            "Validation Loss: 2.3728184700012207, Validation Accuracy: 39.15\n",
            "[75/150]: Training Loss: 2.40391899614918, Training Accuracy: 38.72\n",
            "Validation Loss: 2.368211817741394, Validation Accuracy: 39.47\n",
            "[76/150]: Training Loss: 2.403892604672179, Training Accuracy: 38.72\n",
            "Validation Loss: 2.36419997215271, Validation Accuracy: 39.67\n",
            "[77/150]: Training Loss: 2.3943562458972543, Training Accuracy: 38.7\n",
            "Validation Loss: 2.3529299974441527, Validation Accuracy: 39.95\n",
            "[78/150]: Training Loss: 2.3884038876514047, Training Accuracy: 38.924\n",
            "Validation Loss: 2.3554153442382812, Validation Accuracy: 39.74\n",
            "[79/150]: Training Loss: 2.3807718559187285, Training Accuracy: 39.234\n",
            "Validation Loss: 2.3478601932525636, Validation Accuracy: 40.07\n",
            "[80/150]: Training Loss: 2.3758340952347736, Training Accuracy: 39.184\n",
            "Validation Loss: 2.348959946632385, Validation Accuracy: 40.03\n",
            "[81/150]: Training Loss: 2.3717645577022006, Training Accuracy: 39.1\n",
            "Validation Loss: 2.3494511365890505, Validation Accuracy: 39.68\n",
            "[82/150]: Training Loss: 2.3643068829361273, Training Accuracy: 39.686\n",
            "Validation Loss: 2.3405634641647337, Validation Accuracy: 40.06\n",
            "[83/150]: Training Loss: 2.357988488917448, Training Accuracy: 39.656\n",
            "Validation Loss: 2.333068513870239, Validation Accuracy: 40.54\n",
            "[84/150]: Training Loss: 2.349589080226665, Training Accuracy: 39.642\n",
            "Validation Loss: 2.3267210960388183, Validation Accuracy: 40.56\n",
            "[85/150]: Training Loss: 2.352643222224956, Training Accuracy: 39.594\n",
            "Validation Loss: 2.3241426944732666, Validation Accuracy: 40.55\n",
            "[86/150]: Training Loss: 2.345696108681815, Training Accuracy: 39.898\n",
            "Validation Loss: 2.32454628944397, Validation Accuracy: 40.42\n",
            "[87/150]: Training Loss: 2.3424980786381937, Training Accuracy: 39.724\n",
            "Validation Loss: 2.3185175895690917, Validation Accuracy: 41.01\n",
            "[88/150]: Training Loss: 2.332790350427433, Training Accuracy: 40.048\n",
            "Validation Loss: 2.3108999013900755, Validation Accuracy: 40.55\n",
            "[89/150]: Training Loss: 2.330972437955895, Training Accuracy: 40.282\n",
            "Validation Loss: 2.3076040267944338, Validation Accuracy: 41.04\n",
            "[90/150]: Training Loss: 2.331948946933357, Training Accuracy: 40.192\n",
            "Validation Loss: 2.313597249984741, Validation Accuracy: 40.67\n",
            "[91/150]: Training Loss: 2.3177189340396804, Training Accuracy: 40.614\n",
            "Validation Loss: 2.299083137512207, Validation Accuracy: 41.06\n",
            "[92/150]: Training Loss: 2.3187544443169417, Training Accuracy: 40.588\n",
            "Validation Loss: 2.3093937158584597, Validation Accuracy: 40.62\n",
            "[93/150]: Training Loss: 2.315753240974582, Training Accuracy: 40.59\n",
            "Validation Loss: 2.301071882247925, Validation Accuracy: 40.89\n",
            "[94/150]: Training Loss: 2.3093836502153047, Training Accuracy: 40.56\n",
            "Validation Loss: 2.295303463935852, Validation Accuracy: 41.39\n",
            "[95/150]: Training Loss: 2.3023490857104867, Training Accuracy: 40.91\n",
            "Validation Loss: 2.2921106100082396, Validation Accuracy: 41.17\n",
            "[96/150]: Training Loss: 2.3010492422142805, Training Accuracy: 40.876\n",
            "Validation Loss: 2.288085961341858, Validation Accuracy: 41.53\n",
            "[97/150]: Training Loss: 2.2992364387122954, Training Accuracy: 40.828\n",
            "Validation Loss: 2.2836422443389894, Validation Accuracy: 41.39\n",
            "[98/150]: Training Loss: 2.2914001844367204, Training Accuracy: 40.912\n",
            "Validation Loss: 2.288723373413086, Validation Accuracy: 41.45\n",
            "[99/150]: Training Loss: 2.28923791282031, Training Accuracy: 40.894\n",
            "Validation Loss: 2.2835492134094237, Validation Accuracy: 41.2\n",
            "[100/150]: Training Loss: 2.2794596467699324, Training Accuracy: 41.38\n",
            "Validation Loss: 2.2830920219421387, Validation Accuracy: 41.37\n",
            "[101/150]: Training Loss: 2.2831326844740887, Training Accuracy: 41.274\n",
            "Validation Loss: 2.2774308919906616, Validation Accuracy: 41.62\n",
            "[102/150]: Training Loss: 2.2726087472876726, Training Accuracy: 41.388\n",
            "Validation Loss: 2.276098442077637, Validation Accuracy: 41.66\n",
            "[103/150]: Training Loss: 2.2764012667597555, Training Accuracy: 41.556\n",
            "Validation Loss: 2.273059129714966, Validation Accuracy: 41.71\n",
            "[104/150]: Training Loss: 2.279530860939804, Training Accuracy: 41.396\n",
            "Validation Loss: 2.278269386291504, Validation Accuracy: 41.41\n",
            "[105/150]: Training Loss: 2.2749376637595042, Training Accuracy: 41.318\n",
            "Validation Loss: 2.2714242458343508, Validation Accuracy: 41.7\n",
            "[106/150]: Training Loss: 2.272334692429523, Training Accuracy: 41.548\n",
            "Validation Loss: 2.2693742513656616, Validation Accuracy: 41.48\n",
            "[107/150]: Training Loss: 2.2710340947520975, Training Accuracy: 41.378\n",
            "Validation Loss: 2.2652126789093017, Validation Accuracy: 41.68\n",
            "[108/150]: Training Loss: 2.267277630007997, Training Accuracy: 41.48\n",
            "Validation Loss: 2.266073441505432, Validation Accuracy: 41.61\n",
            "[109/150]: Training Loss: 2.260799787482437, Training Accuracy: 41.708\n",
            "Validation Loss: 2.2654496908187864, Validation Accuracy: 41.56\n",
            "[110/150]: Training Loss: 2.252530964053407, Training Accuracy: 42.132\n",
            "Validation Loss: 2.262526035308838, Validation Accuracy: 41.9\n",
            "[111/150]: Training Loss: 2.247779924042371, Training Accuracy: 42.178\n",
            "Validation Loss: 2.2605021238327025, Validation Accuracy: 41.78\n",
            "[112/150]: Training Loss: 2.2522501166985958, Training Accuracy: 41.864\n",
            "Validation Loss: 2.2593064069747926, Validation Accuracy: 41.88\n",
            "[113/150]: Training Loss: 2.2499374710783666, Training Accuracy: 41.928\n",
            "Validation Loss: 2.258829665184021, Validation Accuracy: 41.93\n",
            "[114/150]: Training Loss: 2.248957716688818, Training Accuracy: 41.898\n",
            "Validation Loss: 2.2602357864379883, Validation Accuracy: 41.8\n",
            "[115/150]: Training Loss: 2.246997745669618, Training Accuracy: 41.86\n",
            "Validation Loss: 2.2563353538513184, Validation Accuracy: 42.0\n",
            "[116/150]: Training Loss: 2.2447169118998, Training Accuracy: 42.086\n",
            "Validation Loss: 2.2574153184890746, Validation Accuracy: 41.69\n",
            "[117/150]: Training Loss: 2.246363454935502, Training Accuracy: 42.22\n",
            "Validation Loss: 2.255693864822388, Validation Accuracy: 41.89\n",
            "[118/150]: Training Loss: 2.2424791735045764, Training Accuracy: 42.13\n",
            "Validation Loss: 2.252920937538147, Validation Accuracy: 42.03\n",
            "[119/150]: Training Loss: 2.2432011000964107, Training Accuracy: 42.372\n",
            "Validation Loss: 2.2506787538528443, Validation Accuracy: 42.28\n",
            "[120/150]: Training Loss: 2.234548028634519, Training Accuracy: 42.426\n",
            "Validation Loss: 2.2501566410064697, Validation Accuracy: 42.13\n",
            "[121/150]: Training Loss: 2.23581794330052, Training Accuracy: 42.216\n",
            "Validation Loss: 2.249624562263489, Validation Accuracy: 42.2\n",
            "[122/150]: Training Loss: 2.2365842984647166, Training Accuracy: 42.344\n",
            "Validation Loss: 2.247886800765991, Validation Accuracy: 42.05\n",
            "[123/150]: Training Loss: 2.231048199595237, Training Accuracy: 42.258\n",
            "Validation Loss: 2.2481226205825804, Validation Accuracy: 42.21\n",
            "[124/150]: Training Loss: 2.2342022195154305, Training Accuracy: 42.48\n",
            "Validation Loss: 2.2451326131820677, Validation Accuracy: 42.16\n",
            "[125/150]: Training Loss: 2.2329893014868913, Training Accuracy: 42.156\n",
            "Validation Loss: 2.245043420791626, Validation Accuracy: 41.96\n",
            "[126/150]: Training Loss: 2.2330193714219697, Training Accuracy: 42.418\n",
            "Validation Loss: 2.24723482131958, Validation Accuracy: 41.96\n",
            "[127/150]: Training Loss: 2.2315565907225317, Training Accuracy: 42.332\n",
            "Validation Loss: 2.243821716308594, Validation Accuracy: 42.12\n",
            "[128/150]: Training Loss: 2.2296993148570157, Training Accuracy: 42.408\n",
            "Validation Loss: 2.2438623905181885, Validation Accuracy: 42.41\n",
            "[129/150]: Training Loss: 2.2296536795947017, Training Accuracy: 42.246\n",
            "Validation Loss: 2.2415871381759644, Validation Accuracy: 42.21\n",
            "[130/150]: Training Loss: 2.2241466337320754, Training Accuracy: 42.568\n",
            "Validation Loss: 2.2422409534454344, Validation Accuracy: 42.26\n",
            "[131/150]: Training Loss: 2.224864049833648, Training Accuracy: 42.598\n",
            "Validation Loss: 2.242423963546753, Validation Accuracy: 42.3\n",
            "[132/150]: Training Loss: 2.2205117478662606, Training Accuracy: 42.862\n",
            "Validation Loss: 2.241534924507141, Validation Accuracy: 42.14\n",
            "[133/150]: Training Loss: 2.224643206109806, Training Accuracy: 42.348\n",
            "Validation Loss: 2.239711046218872, Validation Accuracy: 42.17\n",
            "[134/150]: Training Loss: 2.2237596852438792, Training Accuracy: 42.628\n",
            "Validation Loss: 2.2399126052856446, Validation Accuracy: 42.42\n",
            "[135/150]: Training Loss: 2.219805839110394, Training Accuracy: 42.732\n",
            "Validation Loss: 2.239424467086792, Validation Accuracy: 42.14\n",
            "[136/150]: Training Loss: 2.217083351952689, Training Accuracy: 42.914\n",
            "Validation Loss: 2.2403131246566774, Validation Accuracy: 42.3\n",
            "[137/150]: Training Loss: 2.2213853573312563, Training Accuracy: 42.754\n",
            "Validation Loss: 2.2398319244384766, Validation Accuracy: 42.29\n",
            "[138/150]: Training Loss: 2.221228784444381, Training Accuracy: 42.688\n",
            "Validation Loss: 2.2398473262786864, Validation Accuracy: 42.13\n",
            "[139/150]: Training Loss: 2.220663090141452, Training Accuracy: 42.662\n",
            "Validation Loss: 2.2387043714523314, Validation Accuracy: 42.44\n",
            "[140/150]: Training Loss: 2.2193756784711565, Training Accuracy: 42.598\n",
            "Validation Loss: 2.2392037868499757, Validation Accuracy: 42.31\n",
            "[141/150]: Training Loss: 2.221537142383809, Training Accuracy: 42.654\n",
            "Validation Loss: 2.239200401306152, Validation Accuracy: 42.3\n",
            "[142/150]: Training Loss: 2.218330042702811, Training Accuracy: 42.8\n",
            "Validation Loss: 2.238777184486389, Validation Accuracy: 42.33\n",
            "[143/150]: Training Loss: 2.2231140574630426, Training Accuracy: 42.71\n",
            "Validation Loss: 2.2391661167144776, Validation Accuracy: 42.33\n",
            "[144/150]: Training Loss: 2.2220610939726537, Training Accuracy: 42.734\n",
            "Validation Loss: 2.23895423412323, Validation Accuracy: 42.28\n",
            "[145/150]: Training Loss: 2.2202161526193422, Training Accuracy: 42.758\n",
            "Validation Loss: 2.2384191513061524, Validation Accuracy: 42.25\n",
            "[146/150]: Training Loss: 2.2149193578836868, Training Accuracy: 42.996\n",
            "Validation Loss: 2.2386470317840574, Validation Accuracy: 42.3\n",
            "[147/150]: Training Loss: 2.2156761033194408, Training Accuracy: 42.662\n",
            "Validation Loss: 2.238538146018982, Validation Accuracy: 42.33\n",
            "[148/150]: Training Loss: 2.2188694087826475, Training Accuracy: 42.722\n",
            "Validation Loss: 2.2385375022888185, Validation Accuracy: 42.29\n",
            "[149/150]: Training Loss: 2.2163160820396577, Training Accuracy: 42.762\n",
            "Validation Loss: 2.2385093688964846, Validation Accuracy: 42.29\n",
            "[150/150]: Training Loss: 2.2146115546323815, Training Accuracy: 42.764\n",
            "Validation Loss: 2.238506293296814, Validation Accuracy: 42.29\n",
            "**********************************************************************\n",
            "Test Loss: 2.238506293296814, Test Accuracy: 42.29\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>42.29</td></tr><tr><td>Test Loss</td><td>2.23851</td></tr><tr><td>Train Accuracy</td><td>42.764</td></tr><tr><td>Train Loss</td><td>2.21461</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=1024 learning_rate=0.0006 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/f6zj2dy6' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/f6zj2dy6</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_062343-f6zj2dy6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 2048\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_064235-a1i0qpqi</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/a1i0qpqi' target=\"_blank\">batch_size=2048 learning_rate=0.000848528137423857 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/a1i0qpqi' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/a1i0qpqi</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.600872554779053, Training Accuracy: 1.838\n",
            "Validation Loss: 4.590850925445556, Validation Accuracy: 2.55\n",
            "[2/150]: Training Loss: 4.576414985656738, Training Accuracy: 2.548\n",
            "Validation Loss: 4.55309419631958, Validation Accuracy: 3.25\n",
            "[3/150]: Training Loss: 4.526648464202881, Training Accuracy: 3.408\n",
            "Validation Loss: 4.485427379608154, Validation Accuracy: 3.88\n",
            "[4/150]: Training Loss: 4.446523704528809, Training Accuracy: 4.17\n",
            "Validation Loss: 4.387019348144531, Validation Accuracy: 4.51\n",
            "[5/150]: Training Loss: 4.341707801818847, Training Accuracy: 4.806\n",
            "Validation Loss: 4.268606185913086, Validation Accuracy: 5.42\n",
            "[6/150]: Training Loss: 4.231560764312744, Training Accuracy: 5.834\n",
            "Validation Loss: 4.15619592666626, Validation Accuracy: 7.32\n",
            "[7/150]: Training Loss: 4.137169361114502, Training Accuracy: 7.258\n",
            "Validation Loss: 4.07053050994873, Validation Accuracy: 8.52\n",
            "[8/150]: Training Loss: 4.06978271484375, Training Accuracy: 8.214\n",
            "Validation Loss: 4.007327079772949, Validation Accuracy: 9.55\n",
            "[9/150]: Training Loss: 4.020532312393189, Training Accuracy: 9.192\n",
            "Validation Loss: 3.956293058395386, Validation Accuracy: 10.21\n",
            "[10/150]: Training Loss: 3.9746204376220704, Training Accuracy: 9.782\n",
            "Validation Loss: 3.902535915374756, Validation Accuracy: 11.51\n",
            "[11/150]: Training Loss: 3.9225975704193115, Training Accuracy: 10.588\n",
            "Validation Loss: 3.855096530914307, Validation Accuracy: 12.19\n",
            "[12/150]: Training Loss: 3.8843927574157715, Training Accuracy: 11.236\n",
            "Validation Loss: 3.8128616333007814, Validation Accuracy: 12.95\n",
            "[13/150]: Training Loss: 3.844352321624756, Training Accuracy: 11.834\n",
            "Validation Loss: 3.772704267501831, Validation Accuracy: 13.43\n",
            "[14/150]: Training Loss: 3.8149378585815428, Training Accuracy: 12.34\n",
            "Validation Loss: 3.740799808502197, Validation Accuracy: 14.52\n",
            "[15/150]: Training Loss: 3.778950262069702, Training Accuracy: 13.05\n",
            "Validation Loss: 3.7088190078735352, Validation Accuracy: 14.52\n",
            "[16/150]: Training Loss: 3.7480258464813234, Training Accuracy: 13.598\n",
            "Validation Loss: 3.6764598369598387, Validation Accuracy: 15.3\n",
            "[17/150]: Training Loss: 3.715016832351685, Training Accuracy: 14.102\n",
            "Validation Loss: 3.638604497909546, Validation Accuracy: 15.81\n",
            "[18/150]: Training Loss: 3.6842565822601316, Training Accuracy: 14.456\n",
            "Validation Loss: 3.598477506637573, Validation Accuracy: 16.36\n",
            "[19/150]: Training Loss: 3.6509159278869627, Training Accuracy: 15.14\n",
            "Validation Loss: 3.5720494270324705, Validation Accuracy: 17.0\n",
            "[20/150]: Training Loss: 3.6177934551239015, Training Accuracy: 15.678\n",
            "Validation Loss: 3.5425681114196776, Validation Accuracy: 17.62\n",
            "[21/150]: Training Loss: 3.582935447692871, Training Accuracy: 16.268\n",
            "Validation Loss: 3.50300612449646, Validation Accuracy: 18.33\n",
            "[22/150]: Training Loss: 3.550090913772583, Training Accuracy: 16.796\n",
            "Validation Loss: 3.473466396331787, Validation Accuracy: 19.04\n",
            "[23/150]: Training Loss: 3.5188063907623293, Training Accuracy: 17.128\n",
            "Validation Loss: 3.4353472232818603, Validation Accuracy: 19.68\n",
            "[24/150]: Training Loss: 3.495486888885498, Training Accuracy: 17.544\n",
            "Validation Loss: 3.414610576629639, Validation Accuracy: 19.64\n",
            "[25/150]: Training Loss: 3.46710319519043, Training Accuracy: 18.09\n",
            "Validation Loss: 3.380483055114746, Validation Accuracy: 20.42\n",
            "[26/150]: Training Loss: 3.4414588356018068, Training Accuracy: 18.524\n",
            "Validation Loss: 3.3567835807800295, Validation Accuracy: 20.4\n",
            "[27/150]: Training Loss: 3.415497074127197, Training Accuracy: 19.006\n",
            "Validation Loss: 3.336991882324219, Validation Accuracy: 21.37\n",
            "[28/150]: Training Loss: 3.400643033981323, Training Accuracy: 19.35\n",
            "Validation Loss: 3.315301847457886, Validation Accuracy: 21.36\n",
            "[29/150]: Training Loss: 3.3807515335083007, Training Accuracy: 19.672\n",
            "Validation Loss: 3.294951343536377, Validation Accuracy: 22.07\n",
            "[30/150]: Training Loss: 3.3530473613739016, Training Accuracy: 20.312\n",
            "Validation Loss: 3.270126295089722, Validation Accuracy: 22.23\n",
            "[31/150]: Training Loss: 3.3403017330169678, Training Accuracy: 20.354\n",
            "Validation Loss: 3.258027935028076, Validation Accuracy: 22.94\n",
            "[32/150]: Training Loss: 3.3205006408691404, Training Accuracy: 20.864\n",
            "Validation Loss: 3.225876235961914, Validation Accuracy: 23.57\n",
            "[33/150]: Training Loss: 3.3009582996368407, Training Accuracy: 21.156\n",
            "Validation Loss: 3.214678192138672, Validation Accuracy: 23.22\n",
            "[34/150]: Training Loss: 3.2803742408752443, Training Accuracy: 21.664\n",
            "Validation Loss: 3.193393039703369, Validation Accuracy: 23.96\n",
            "[35/150]: Training Loss: 3.261015605926514, Training Accuracy: 21.874\n",
            "Validation Loss: 3.178829050064087, Validation Accuracy: 24.36\n",
            "[36/150]: Training Loss: 3.248182525634766, Training Accuracy: 22.066\n",
            "Validation Loss: 3.1585222244262696, Validation Accuracy: 24.66\n",
            "[37/150]: Training Loss: 3.228750743865967, Training Accuracy: 22.64\n",
            "Validation Loss: 3.1375529289245607, Validation Accuracy: 25.22\n",
            "[38/150]: Training Loss: 3.2122406196594238, Training Accuracy: 22.746\n",
            "Validation Loss: 3.130802774429321, Validation Accuracy: 25.21\n",
            "[39/150]: Training Loss: 3.200315999984741, Training Accuracy: 23.118\n",
            "Validation Loss: 3.1034634590148924, Validation Accuracy: 25.76\n",
            "[40/150]: Training Loss: 3.179281768798828, Training Accuracy: 23.47\n",
            "Validation Loss: 3.0890458106994627, Validation Accuracy: 25.99\n",
            "[41/150]: Training Loss: 3.158253240585327, Training Accuracy: 23.802\n",
            "Validation Loss: 3.0687211990356444, Validation Accuracy: 26.33\n",
            "[42/150]: Training Loss: 3.146801538467407, Training Accuracy: 23.958\n",
            "Validation Loss: 3.055481195449829, Validation Accuracy: 26.63\n",
            "[43/150]: Training Loss: 3.1272859001159667, Training Accuracy: 24.326\n",
            "Validation Loss: 3.0511748790740967, Validation Accuracy: 26.8\n",
            "[44/150]: Training Loss: 3.117414894104004, Training Accuracy: 24.57\n",
            "Validation Loss: 3.0245641231536866, Validation Accuracy: 27.07\n",
            "[45/150]: Training Loss: 3.1020038509368897, Training Accuracy: 24.842\n",
            "Validation Loss: 3.008786678314209, Validation Accuracy: 27.39\n",
            "[46/150]: Training Loss: 3.0855378246307374, Training Accuracy: 25.254\n",
            "Validation Loss: 2.991756868362427, Validation Accuracy: 27.8\n",
            "[47/150]: Training Loss: 3.070243549346924, Training Accuracy: 25.452\n",
            "Validation Loss: 2.9793407917022705, Validation Accuracy: 28.28\n",
            "[48/150]: Training Loss: 3.0512985134124757, Training Accuracy: 25.744\n",
            "Validation Loss: 2.9645009517669676, Validation Accuracy: 27.98\n",
            "[49/150]: Training Loss: 3.037804851531982, Training Accuracy: 26.15\n",
            "Validation Loss: 2.94492564201355, Validation Accuracy: 28.53\n",
            "[50/150]: Training Loss: 3.02706787109375, Training Accuracy: 26.23\n",
            "Validation Loss: 2.933021306991577, Validation Accuracy: 28.91\n",
            "[51/150]: Training Loss: 3.0107177543640136, Training Accuracy: 26.506\n",
            "Validation Loss: 2.9290098667144777, Validation Accuracy: 28.84\n",
            "[52/150]: Training Loss: 2.9924880027770997, Training Accuracy: 27.032\n",
            "Validation Loss: 2.895456838607788, Validation Accuracy: 29.47\n",
            "[53/150]: Training Loss: 2.975731897354126, Training Accuracy: 27.272\n",
            "Validation Loss: 2.8865129470825197, Validation Accuracy: 29.5\n",
            "[54/150]: Training Loss: 2.9595889949798586, Training Accuracy: 27.632\n",
            "Validation Loss: 2.8671710014343263, Validation Accuracy: 30.08\n",
            "[55/150]: Training Loss: 2.945874729156494, Training Accuracy: 27.87\n",
            "Validation Loss: 2.8572846412658692, Validation Accuracy: 29.75\n",
            "[56/150]: Training Loss: 2.9351212215423583, Training Accuracy: 28.054\n",
            "Validation Loss: 2.8465723991394043, Validation Accuracy: 30.33\n",
            "[57/150]: Training Loss: 2.924895992279053, Training Accuracy: 28.296\n",
            "Validation Loss: 2.8296555995941164, Validation Accuracy: 30.55\n",
            "[58/150]: Training Loss: 2.910297574996948, Training Accuracy: 28.604\n",
            "Validation Loss: 2.826309061050415, Validation Accuracy: 30.86\n",
            "[59/150]: Training Loss: 2.8968456172943116, Training Accuracy: 28.5\n",
            "Validation Loss: 2.8103396892547607, Validation Accuracy: 30.74\n",
            "[60/150]: Training Loss: 2.8862410736083985, Training Accuracy: 29.156\n",
            "Validation Loss: 2.7891706466674804, Validation Accuracy: 31.45\n",
            "[61/150]: Training Loss: 2.8698458766937254, Training Accuracy: 28.992\n",
            "Validation Loss: 2.7788822650909424, Validation Accuracy: 31.5\n",
            "[62/150]: Training Loss: 2.8539681434631348, Training Accuracy: 29.33\n",
            "Validation Loss: 2.7650059700012206, Validation Accuracy: 32.31\n",
            "[63/150]: Training Loss: 2.84574893951416, Training Accuracy: 29.738\n",
            "Validation Loss: 2.7536985874176025, Validation Accuracy: 31.89\n",
            "[64/150]: Training Loss: 2.83269907951355, Training Accuracy: 29.984\n",
            "Validation Loss: 2.7471179485321047, Validation Accuracy: 31.71\n",
            "[65/150]: Training Loss: 2.8287733936309816, Training Accuracy: 29.936\n",
            "Validation Loss: 2.7425480842590333, Validation Accuracy: 32.37\n",
            "[66/150]: Training Loss: 2.8126729583740233, Training Accuracy: 30.256\n",
            "Validation Loss: 2.7356860637664795, Validation Accuracy: 32.08\n",
            "[67/150]: Training Loss: 2.79921669960022, Training Accuracy: 30.562\n",
            "Validation Loss: 2.71913685798645, Validation Accuracy: 32.48\n",
            "[68/150]: Training Loss: 2.796040153503418, Training Accuracy: 30.588\n",
            "Validation Loss: 2.70328369140625, Validation Accuracy: 33.07\n",
            "[69/150]: Training Loss: 2.784684944152832, Training Accuracy: 30.966\n",
            "Validation Loss: 2.6998016357421877, Validation Accuracy: 33.13\n",
            "[70/150]: Training Loss: 2.771635856628418, Training Accuracy: 30.944\n",
            "Validation Loss: 2.6886127948760987, Validation Accuracy: 33.05\n",
            "[71/150]: Training Loss: 2.768005828857422, Training Accuracy: 31.358\n",
            "Validation Loss: 2.6832919120788574, Validation Accuracy: 33.2\n",
            "[72/150]: Training Loss: 2.765062837600708, Training Accuracy: 31.252\n",
            "Validation Loss: 2.680223321914673, Validation Accuracy: 33.55\n",
            "[73/150]: Training Loss: 2.7503418922424316, Training Accuracy: 31.848\n",
            "Validation Loss: 2.670897054672241, Validation Accuracy: 33.25\n",
            "[74/150]: Training Loss: 2.740241832733154, Training Accuracy: 31.89\n",
            "Validation Loss: 2.6575933933258056, Validation Accuracy: 34.25\n",
            "[75/150]: Training Loss: 2.734506778717041, Training Accuracy: 31.896\n",
            "Validation Loss: 2.6604311943054197, Validation Accuracy: 33.96\n",
            "[76/150]: Training Loss: 2.7318229484558105, Training Accuracy: 32.09\n",
            "Validation Loss: 2.6528788089752195, Validation Accuracy: 33.73\n",
            "[77/150]: Training Loss: 2.7181160259246826, Training Accuracy: 32.278\n",
            "Validation Loss: 2.636572074890137, Validation Accuracy: 34.29\n",
            "[78/150]: Training Loss: 2.712002992630005, Training Accuracy: 32.138\n",
            "Validation Loss: 2.6339709758758545, Validation Accuracy: 34.11\n",
            "[79/150]: Training Loss: 2.708227500915527, Training Accuracy: 32.512\n",
            "Validation Loss: 2.632725191116333, Validation Accuracy: 34.27\n",
            "[80/150]: Training Loss: 2.6961685180664063, Training Accuracy: 32.58\n",
            "Validation Loss: 2.620032548904419, Validation Accuracy: 34.63\n",
            "[81/150]: Training Loss: 2.6996922492980957, Training Accuracy: 32.57\n",
            "Validation Loss: 2.622188997268677, Validation Accuracy: 34.46\n",
            "[82/150]: Training Loss: 2.687695999145508, Training Accuracy: 32.668\n",
            "Validation Loss: 2.6097755432128906, Validation Accuracy: 34.63\n",
            "[83/150]: Training Loss: 2.686816120147705, Training Accuracy: 33.0\n",
            "Validation Loss: 2.604731035232544, Validation Accuracy: 34.86\n",
            "[84/150]: Training Loss: 2.6810986232757568, Training Accuracy: 33.088\n",
            "Validation Loss: 2.60227370262146, Validation Accuracy: 34.58\n",
            "[85/150]: Training Loss: 2.667298593521118, Training Accuracy: 33.026\n",
            "Validation Loss: 2.5953265190124513, Validation Accuracy: 35.33\n",
            "[86/150]: Training Loss: 2.6712794971466063, Training Accuracy: 33.128\n",
            "Validation Loss: 2.595105266571045, Validation Accuracy: 35.11\n",
            "[87/150]: Training Loss: 2.6611053371429443, Training Accuracy: 33.468\n",
            "Validation Loss: 2.585465097427368, Validation Accuracy: 35.32\n",
            "[88/150]: Training Loss: 2.663666458129883, Training Accuracy: 33.35\n",
            "Validation Loss: 2.583946704864502, Validation Accuracy: 35.27\n",
            "[89/150]: Training Loss: 2.65557580947876, Training Accuracy: 33.416\n",
            "Validation Loss: 2.5775884628295898, Validation Accuracy: 35.09\n",
            "[90/150]: Training Loss: 2.645936164855957, Training Accuracy: 33.65\n",
            "Validation Loss: 2.575248098373413, Validation Accuracy: 35.32\n",
            "[91/150]: Training Loss: 2.641429653167725, Training Accuracy: 33.728\n",
            "Validation Loss: 2.5739793300628664, Validation Accuracy: 35.32\n",
            "[92/150]: Training Loss: 2.6367671966552733, Training Accuracy: 34.026\n",
            "Validation Loss: 2.5660872936248778, Validation Accuracy: 35.58\n",
            "[93/150]: Training Loss: 2.633252477645874, Training Accuracy: 33.894\n",
            "Validation Loss: 2.5642282962799072, Validation Accuracy: 35.44\n",
            "[94/150]: Training Loss: 2.6317241191864014, Training Accuracy: 34.138\n",
            "Validation Loss: 2.5631762504577638, Validation Accuracy: 35.39\n",
            "[95/150]: Training Loss: 2.6283755111694336, Training Accuracy: 34.238\n",
            "Validation Loss: 2.5549296855926515, Validation Accuracy: 35.99\n",
            "[96/150]: Training Loss: 2.6233442878723143, Training Accuracy: 34.092\n",
            "Validation Loss: 2.553958368301392, Validation Accuracy: 35.83\n",
            "[97/150]: Training Loss: 2.6188401126861574, Training Accuracy: 34.434\n",
            "Validation Loss: 2.5522915363311767, Validation Accuracy: 35.83\n",
            "[98/150]: Training Loss: 2.615722646713257, Training Accuracy: 34.306\n",
            "Validation Loss: 2.545611763000488, Validation Accuracy: 35.99\n",
            "[99/150]: Training Loss: 2.6122173404693605, Training Accuracy: 34.594\n",
            "Validation Loss: 2.5498822689056397, Validation Accuracy: 35.71\n",
            "[100/150]: Training Loss: 2.610180253982544, Training Accuracy: 34.442\n",
            "Validation Loss: 2.5442173004150392, Validation Accuracy: 36.31\n",
            "[101/150]: Training Loss: 2.6088740062713622, Training Accuracy: 34.904\n",
            "Validation Loss: 2.5409452438354494, Validation Accuracy: 36.03\n",
            "[102/150]: Training Loss: 2.6049435424804686, Training Accuracy: 34.646\n",
            "Validation Loss: 2.535108757019043, Validation Accuracy: 36.2\n",
            "[103/150]: Training Loss: 2.6048165130615235, Training Accuracy: 34.616\n",
            "Validation Loss: 2.534243869781494, Validation Accuracy: 36.2\n",
            "[104/150]: Training Loss: 2.599171781539917, Training Accuracy: 34.738\n",
            "Validation Loss: 2.533633089065552, Validation Accuracy: 36.05\n",
            "[105/150]: Training Loss: 2.597423791885376, Training Accuracy: 34.898\n",
            "Validation Loss: 2.529845094680786, Validation Accuracy: 36.45\n",
            "[106/150]: Training Loss: 2.596056480407715, Training Accuracy: 34.702\n",
            "Validation Loss: 2.5281211376190185, Validation Accuracy: 36.5\n",
            "[107/150]: Training Loss: 2.5845062160491943, Training Accuracy: 34.948\n",
            "Validation Loss: 2.5248257160186767, Validation Accuracy: 36.49\n",
            "[108/150]: Training Loss: 2.585829973220825, Training Accuracy: 35.074\n",
            "Validation Loss: 2.5224894523620605, Validation Accuracy: 36.49\n",
            "[109/150]: Training Loss: 2.5839975357055662, Training Accuracy: 35.16\n",
            "Validation Loss: 2.521509790420532, Validation Accuracy: 36.46\n",
            "[110/150]: Training Loss: 2.5794923305511475, Training Accuracy: 35.162\n",
            "Validation Loss: 2.5190929412841796, Validation Accuracy: 36.59\n",
            "[111/150]: Training Loss: 2.5758457374572754, Training Accuracy: 35.06\n",
            "Validation Loss: 2.51663761138916, Validation Accuracy: 36.37\n",
            "[112/150]: Training Loss: 2.576828861236572, Training Accuracy: 35.066\n",
            "Validation Loss: 2.5163278102874758, Validation Accuracy: 36.8\n",
            "[113/150]: Training Loss: 2.577974252700806, Training Accuracy: 35.126\n",
            "Validation Loss: 2.5136294841766356, Validation Accuracy: 36.77\n",
            "[114/150]: Training Loss: 2.5744064903259276, Training Accuracy: 35.212\n",
            "Validation Loss: 2.5124191284179687, Validation Accuracy: 36.53\n",
            "[115/150]: Training Loss: 2.5719550609588624, Training Accuracy: 35.18\n",
            "Validation Loss: 2.5119277000427247, Validation Accuracy: 36.54\n",
            "[116/150]: Training Loss: 2.5702946758270264, Training Accuracy: 35.3\n",
            "Validation Loss: 2.5109957695007323, Validation Accuracy: 36.92\n",
            "[117/150]: Training Loss: 2.570598087310791, Training Accuracy: 35.416\n",
            "Validation Loss: 2.507699728012085, Validation Accuracy: 36.88\n",
            "[118/150]: Training Loss: 2.5662026691436766, Training Accuracy: 35.434\n",
            "Validation Loss: 2.5102144718170165, Validation Accuracy: 36.76\n",
            "[119/150]: Training Loss: 2.5664295768737793, Training Accuracy: 35.47\n",
            "Validation Loss: 2.505918836593628, Validation Accuracy: 37.02\n",
            "[120/150]: Training Loss: 2.5633824253082276, Training Accuracy: 35.706\n",
            "Validation Loss: 2.503660488128662, Validation Accuracy: 36.85\n",
            "[121/150]: Training Loss: 2.56352068901062, Training Accuracy: 35.61\n",
            "Validation Loss: 2.502376890182495, Validation Accuracy: 37.07\n",
            "[122/150]: Training Loss: 2.5649334526062013, Training Accuracy: 35.482\n",
            "Validation Loss: 2.501589870452881, Validation Accuracy: 37.12\n",
            "[123/150]: Training Loss: 2.5551751327514647, Training Accuracy: 35.73\n",
            "Validation Loss: 2.5013341903686523, Validation Accuracy: 37.0\n",
            "[124/150]: Training Loss: 2.556901903152466, Training Accuracy: 35.526\n",
            "Validation Loss: 2.498872900009155, Validation Accuracy: 37.13\n",
            "[125/150]: Training Loss: 2.56141544342041, Training Accuracy: 35.398\n",
            "Validation Loss: 2.499050569534302, Validation Accuracy: 37.08\n",
            "[126/150]: Training Loss: 2.559230995178223, Training Accuracy: 35.614\n",
            "Validation Loss: 2.4986857414245605, Validation Accuracy: 36.93\n",
            "[127/150]: Training Loss: 2.5585966968536376, Training Accuracy: 35.736\n",
            "Validation Loss: 2.497245264053345, Validation Accuracy: 37.23\n",
            "[128/150]: Training Loss: 2.5579049968719483, Training Accuracy: 35.618\n",
            "Validation Loss: 2.4982468605041506, Validation Accuracy: 37.21\n",
            "[129/150]: Training Loss: 2.555950794219971, Training Accuracy: 35.748\n",
            "Validation Loss: 2.4955115795135496, Validation Accuracy: 37.22\n",
            "[130/150]: Training Loss: 2.5558400917053223, Training Accuracy: 35.628\n",
            "Validation Loss: 2.4957788944244386, Validation Accuracy: 36.97\n",
            "[131/150]: Training Loss: 2.549605436325073, Training Accuracy: 35.862\n",
            "Validation Loss: 2.494734859466553, Validation Accuracy: 37.41\n",
            "[132/150]: Training Loss: 2.5500775146484376, Training Accuracy: 35.77\n",
            "Validation Loss: 2.4960716724395753, Validation Accuracy: 37.08\n",
            "[133/150]: Training Loss: 2.5564523792266844, Training Accuracy: 35.596\n",
            "Validation Loss: 2.4939414501190185, Validation Accuracy: 37.17\n",
            "[134/150]: Training Loss: 2.5449225997924803, Training Accuracy: 35.882\n",
            "Validation Loss: 2.494381046295166, Validation Accuracy: 37.25\n",
            "[135/150]: Training Loss: 2.5483498573303223, Training Accuracy: 35.88\n",
            "Validation Loss: 2.493652677536011, Validation Accuracy: 37.19\n",
            "[136/150]: Training Loss: 2.5493384170532227, Training Accuracy: 35.934\n",
            "Validation Loss: 2.4934218406677244, Validation Accuracy: 37.27\n",
            "[137/150]: Training Loss: 2.54675745010376, Training Accuracy: 35.884\n",
            "Validation Loss: 2.4932618618011473, Validation Accuracy: 37.3\n",
            "[138/150]: Training Loss: 2.551592617034912, Training Accuracy: 35.658\n",
            "Validation Loss: 2.4930295944213867, Validation Accuracy: 37.26\n",
            "[139/150]: Training Loss: 2.5556958293914795, Training Accuracy: 35.616\n",
            "Validation Loss: 2.4928393840789793, Validation Accuracy: 37.32\n",
            "[140/150]: Training Loss: 2.5497092247009276, Training Accuracy: 35.746\n",
            "Validation Loss: 2.4923996925354004, Validation Accuracy: 37.19\n",
            "[141/150]: Training Loss: 2.5460032653808593, Training Accuracy: 36.058\n",
            "Validation Loss: 2.4924858093261717, Validation Accuracy: 37.35\n",
            "[142/150]: Training Loss: 2.5488867378234863, Training Accuracy: 35.844\n",
            "Validation Loss: 2.4925445079803468, Validation Accuracy: 37.29\n",
            "[143/150]: Training Loss: 2.5511161518096923, Training Accuracy: 35.83\n",
            "Validation Loss: 2.492031478881836, Validation Accuracy: 37.22\n",
            "[144/150]: Training Loss: 2.5504028892517088, Training Accuracy: 35.788\n",
            "Validation Loss: 2.4918816089630127, Validation Accuracy: 37.21\n",
            "[145/150]: Training Loss: 2.547076606750488, Training Accuracy: 35.668\n",
            "Validation Loss: 2.4918155670166016, Validation Accuracy: 37.23\n",
            "[146/150]: Training Loss: 2.542300052642822, Training Accuracy: 36.08\n",
            "Validation Loss: 2.4920117378234865, Validation Accuracy: 37.23\n",
            "[147/150]: Training Loss: 2.5489714622497557, Training Accuracy: 35.678\n",
            "Validation Loss: 2.4920287132263184, Validation Accuracy: 37.23\n",
            "[148/150]: Training Loss: 2.5497193813323973, Training Accuracy: 35.596\n",
            "Validation Loss: 2.4920116424560548, Validation Accuracy: 37.23\n",
            "[149/150]: Training Loss: 2.5469696426391604, Training Accuracy: 35.88\n",
            "Validation Loss: 2.4920021057128907, Validation Accuracy: 37.23\n",
            "[150/150]: Training Loss: 2.5485261821746827, Training Accuracy: 35.498\n",
            "Validation Loss: 2.491995668411255, Validation Accuracy: 37.22\n",
            "**********************************************************************\n",
            "Test Loss: 2.491995668411255, Test Accuracy: 37.22\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>37.22</td></tr><tr><td>Test Loss</td><td>2.492</td></tr><tr><td>Train Accuracy</td><td>35.498</td></tr><tr><td>Train Loss</td><td>2.54853</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=2048 learning_rate=0.000848528137423857 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/a1i0qpqi' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/a1i0qpqi</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_064235-a1i0qpqi/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 4096\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_070246-j81b4d42</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/j81b4d42' target=\"_blank\">batch_size=4096 learning_rate=0.0012 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/j81b4d42' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/j81b4d42</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.603197281177227, Training Accuracy: 1.246\n",
            "Validation Loss: 4.599034945170085, Validation Accuracy: 2.16\n",
            "[2/150]: Training Loss: 4.594296822181115, Training Accuracy: 2.678\n",
            "Validation Loss: 4.58727502822876, Validation Accuracy: 3.46\n",
            "[3/150]: Training Loss: 4.580152915074275, Training Accuracy: 3.29\n",
            "Validation Loss: 4.5680915514628095, Validation Accuracy: 3.75\n",
            "[4/150]: Training Loss: 4.55770389850323, Training Accuracy: 3.57\n",
            "Validation Loss: 4.539859771728516, Validation Accuracy: 4.09\n",
            "[5/150]: Training Loss: 4.526640048393836, Training Accuracy: 3.846\n",
            "Validation Loss: 4.5020168622334795, Validation Accuracy: 4.4\n",
            "[6/150]: Training Loss: 4.4863302157475395, Training Accuracy: 4.116\n",
            "Validation Loss: 4.454424858093262, Validation Accuracy: 4.5\n",
            "[7/150]: Training Loss: 4.436847613408015, Training Accuracy: 4.154\n",
            "Validation Loss: 4.398792107899983, Validation Accuracy: 4.51\n",
            "[8/150]: Training Loss: 4.382756233215332, Training Accuracy: 4.368\n",
            "Validation Loss: 4.336817264556885, Validation Accuracy: 5.07\n",
            "[9/150]: Training Loss: 4.320116079770601, Training Accuracy: 5.092\n",
            "Validation Loss: 4.271851857503255, Validation Accuracy: 5.91\n",
            "[10/150]: Training Loss: 4.2579911672152, Training Accuracy: 5.896\n",
            "Validation Loss: 4.20835542678833, Validation Accuracy: 6.72\n",
            "[11/150]: Training Loss: 4.2017460236182576, Training Accuracy: 6.778\n",
            "Validation Loss: 4.149452845255534, Validation Accuracy: 7.6\n",
            "[12/150]: Training Loss: 4.147615909576416, Training Accuracy: 7.544\n",
            "Validation Loss: 4.097760200500488, Validation Accuracy: 8.45\n",
            "[13/150]: Training Loss: 4.109276514786941, Training Accuracy: 7.878\n",
            "Validation Loss: 4.055920441945394, Validation Accuracy: 8.81\n",
            "[14/150]: Training Loss: 4.069469965421236, Training Accuracy: 8.186\n",
            "Validation Loss: 4.02240784962972, Validation Accuracy: 9.43\n",
            "[15/150]: Training Loss: 4.0482294376079855, Training Accuracy: 8.564\n",
            "Validation Loss: 3.9910717010498047, Validation Accuracy: 9.75\n",
            "[16/150]: Training Loss: 4.014009714126587, Training Accuracy: 8.978\n",
            "Validation Loss: 3.961193005243937, Validation Accuracy: 10.36\n",
            "[17/150]: Training Loss: 3.9931728656475363, Training Accuracy: 9.682\n",
            "Validation Loss: 3.9339873790740967, Validation Accuracy: 10.79\n",
            "[18/150]: Training Loss: 3.967302780884963, Training Accuracy: 9.836\n",
            "Validation Loss: 3.9086116949717202, Validation Accuracy: 11.43\n",
            "[19/150]: Training Loss: 3.945032779987042, Training Accuracy: 10.19\n",
            "Validation Loss: 3.882365862528483, Validation Accuracy: 11.76\n",
            "[20/150]: Training Loss: 3.9207697831667385, Training Accuracy: 10.68\n",
            "Validation Loss: 3.8563054402669272, Validation Accuracy: 12.18\n",
            "[21/150]: Training Loss: 3.895809503702017, Training Accuracy: 11.034\n",
            "Validation Loss: 3.8310407797495523, Validation Accuracy: 12.46\n",
            "[22/150]: Training Loss: 3.87269454735976, Training Accuracy: 11.518\n",
            "Validation Loss: 3.808952808380127, Validation Accuracy: 12.48\n",
            "[23/150]: Training Loss: 3.8534568639901967, Training Accuracy: 11.846\n",
            "Validation Loss: 3.782241185506185, Validation Accuracy: 13.07\n",
            "[24/150]: Training Loss: 3.829915010012113, Training Accuracy: 12.048\n",
            "Validation Loss: 3.760414203008016, Validation Accuracy: 13.69\n",
            "[25/150]: Training Loss: 3.8085157687847433, Training Accuracy: 12.546\n",
            "Validation Loss: 3.7369075616200766, Validation Accuracy: 14.06\n",
            "[26/150]: Training Loss: 3.7978774034059963, Training Accuracy: 12.83\n",
            "Validation Loss: 3.7192533810933432, Validation Accuracy: 14.28\n",
            "[27/150]: Training Loss: 3.7712434438558726, Training Accuracy: 13.502\n",
            "Validation Loss: 3.7015438079833984, Validation Accuracy: 14.49\n",
            "[28/150]: Training Loss: 3.757168146280142, Training Accuracy: 13.28\n",
            "Validation Loss: 3.682270367940267, Validation Accuracy: 14.62\n",
            "[29/150]: Training Loss: 3.739195860349215, Training Accuracy: 13.572\n",
            "Validation Loss: 3.6604272524515786, Validation Accuracy: 15.51\n",
            "[30/150]: Training Loss: 3.7192102945767918, Training Accuracy: 14.206\n",
            "Validation Loss: 3.6448915004730225, Validation Accuracy: 15.47\n",
            "[31/150]: Training Loss: 3.7009674035585842, Training Accuracy: 14.666\n",
            "Validation Loss: 3.6245458920796714, Validation Accuracy: 16.05\n",
            "[32/150]: Training Loss: 3.685235757094163, Training Accuracy: 14.604\n",
            "Validation Loss: 3.604191780090332, Validation Accuracy: 16.37\n",
            "[33/150]: Training Loss: 3.6659323068765493, Training Accuracy: 15.21\n",
            "Validation Loss: 3.5856331984202066, Validation Accuracy: 16.73\n",
            "[34/150]: Training Loss: 3.650921583175659, Training Accuracy: 15.322\n",
            "Validation Loss: 3.5731921195983887, Validation Accuracy: 16.91\n",
            "[35/150]: Training Loss: 3.638433786538931, Training Accuracy: 15.394\n",
            "Validation Loss: 3.5550003051757812, Validation Accuracy: 17.2\n",
            "[36/150]: Training Loss: 3.615772980910081, Training Accuracy: 15.728\n",
            "Validation Loss: 3.5329747994740806, Validation Accuracy: 17.79\n",
            "[37/150]: Training Loss: 3.6038941786839414, Training Accuracy: 16.05\n",
            "Validation Loss: 3.516122579574585, Validation Accuracy: 17.6\n",
            "[38/150]: Training Loss: 3.584128581560575, Training Accuracy: 16.3\n",
            "Validation Loss: 3.501359542210897, Validation Accuracy: 17.71\n",
            "[39/150]: Training Loss: 3.5705398779649, Training Accuracy: 16.562\n",
            "Validation Loss: 3.4867658615112305, Validation Accuracy: 17.99\n",
            "[40/150]: Training Loss: 3.5499917543851414, Training Accuracy: 16.898\n",
            "Validation Loss: 3.4654527505238852, Validation Accuracy: 18.27\n",
            "[41/150]: Training Loss: 3.5377132709209738, Training Accuracy: 17.152\n",
            "Validation Loss: 3.448629140853882, Validation Accuracy: 18.51\n",
            "[42/150]: Training Loss: 3.520086728609525, Training Accuracy: 17.36\n",
            "Validation Loss: 3.4321948687235513, Validation Accuracy: 19.02\n",
            "[43/150]: Training Loss: 3.5058687833639293, Training Accuracy: 17.682\n",
            "Validation Loss: 3.4222156206766763, Validation Accuracy: 19.32\n",
            "[44/150]: Training Loss: 3.4854386219611535, Training Accuracy: 17.882\n",
            "Validation Loss: 3.4029709498087564, Validation Accuracy: 19.43\n",
            "[45/150]: Training Loss: 3.4765561543978176, Training Accuracy: 18.026\n",
            "Validation Loss: 3.391355276107788, Validation Accuracy: 19.92\n",
            "[46/150]: Training Loss: 3.459314914850088, Training Accuracy: 18.246\n",
            "Validation Loss: 3.373873710632324, Validation Accuracy: 20.06\n",
            "[47/150]: Training Loss: 3.445302651478694, Training Accuracy: 18.344\n",
            "Validation Loss: 3.358146905899048, Validation Accuracy: 20.38\n",
            "[48/150]: Training Loss: 3.437425136566162, Training Accuracy: 18.682\n",
            "Validation Loss: 3.348493973414103, Validation Accuracy: 20.42\n",
            "[49/150]: Training Loss: 3.428378838759202, Training Accuracy: 18.738\n",
            "Validation Loss: 3.337660074234009, Validation Accuracy: 20.87\n",
            "[50/150]: Training Loss: 3.4082561823037953, Training Accuracy: 19.196\n",
            "Validation Loss: 3.323828379313151, Validation Accuracy: 21.05\n",
            "[51/150]: Training Loss: 3.394208926420945, Training Accuracy: 19.342\n",
            "Validation Loss: 3.3132495880126953, Validation Accuracy: 21.3\n",
            "[52/150]: Training Loss: 3.3845279766963077, Training Accuracy: 19.668\n",
            "Validation Loss: 3.2969996134440103, Validation Accuracy: 21.48\n",
            "[53/150]: Training Loss: 3.3752853136796217, Training Accuracy: 19.65\n",
            "Validation Loss: 3.283722718556722, Validation Accuracy: 22.01\n",
            "[54/150]: Training Loss: 3.3559417174412656, Training Accuracy: 20.118\n",
            "Validation Loss: 3.273935556411743, Validation Accuracy: 22.21\n",
            "[55/150]: Training Loss: 3.3527236534998965, Training Accuracy: 20.182\n",
            "Validation Loss: 3.2600110371907554, Validation Accuracy: 22.21\n",
            "[56/150]: Training Loss: 3.33942435337947, Training Accuracy: 20.32\n",
            "Validation Loss: 3.2532562414805093, Validation Accuracy: 22.54\n",
            "[57/150]: Training Loss: 3.3295783813183126, Training Accuracy: 20.47\n",
            "Validation Loss: 3.241544167200724, Validation Accuracy: 22.6\n",
            "[58/150]: Training Loss: 3.3188119484828067, Training Accuracy: 20.912\n",
            "Validation Loss: 3.2292733987172446, Validation Accuracy: 22.89\n",
            "[59/150]: Training Loss: 3.3085557314065785, Training Accuracy: 20.83\n",
            "Validation Loss: 3.2252915700276694, Validation Accuracy: 23.05\n",
            "[60/150]: Training Loss: 3.301169615525466, Training Accuracy: 21.142\n",
            "Validation Loss: 3.211410919825236, Validation Accuracy: 23.25\n",
            "[61/150]: Training Loss: 3.2943123487325816, Training Accuracy: 21.44\n",
            "Validation Loss: 3.2039496898651123, Validation Accuracy: 23.12\n",
            "[62/150]: Training Loss: 3.2789928362919736, Training Accuracy: 21.606\n",
            "Validation Loss: 3.191706339518229, Validation Accuracy: 23.63\n",
            "[63/150]: Training Loss: 3.271783847075242, Training Accuracy: 21.548\n",
            "Validation Loss: 3.183237632115682, Validation Accuracy: 23.84\n",
            "[64/150]: Training Loss: 3.267558996494, Training Accuracy: 21.776\n",
            "Validation Loss: 3.1755964756011963, Validation Accuracy: 23.78\n",
            "[65/150]: Training Loss: 3.2593024327204776, Training Accuracy: 21.928\n",
            "Validation Loss: 3.1660290559132895, Validation Accuracy: 24.0\n",
            "[66/150]: Training Loss: 3.245825639137855, Training Accuracy: 22.07\n",
            "Validation Loss: 3.154296080271403, Validation Accuracy: 23.95\n",
            "[67/150]: Training Loss: 3.2345091563004713, Training Accuracy: 22.202\n",
            "Validation Loss: 3.1482552687327066, Validation Accuracy: 24.37\n",
            "[68/150]: Training Loss: 3.2364803827725925, Training Accuracy: 22.468\n",
            "Validation Loss: 3.1444711685180664, Validation Accuracy: 24.36\n",
            "[69/150]: Training Loss: 3.2277981501359205, Training Accuracy: 22.69\n",
            "Validation Loss: 3.13324769337972, Validation Accuracy: 24.42\n",
            "[70/150]: Training Loss: 3.2204189667334924, Training Accuracy: 22.912\n",
            "Validation Loss: 3.1285581588745117, Validation Accuracy: 24.63\n",
            "[71/150]: Training Loss: 3.212432549549983, Training Accuracy: 22.806\n",
            "Validation Loss: 3.1225887139638266, Validation Accuracy: 24.19\n",
            "[72/150]: Training Loss: 3.2032233018141527, Training Accuracy: 22.884\n",
            "Validation Loss: 3.1134212811787925, Validation Accuracy: 24.9\n",
            "[73/150]: Training Loss: 3.197613184268658, Training Accuracy: 23.006\n",
            "Validation Loss: 3.1070095698038735, Validation Accuracy: 24.96\n",
            "[74/150]: Training Loss: 3.1896161299485426, Training Accuracy: 23.14\n",
            "Validation Loss: 3.103363116582235, Validation Accuracy: 24.66\n",
            "[75/150]: Training Loss: 3.1869239256932187, Training Accuracy: 23.35\n",
            "Validation Loss: 3.0950690110524497, Validation Accuracy: 25.11\n",
            "[76/150]: Training Loss: 3.1853491159585805, Training Accuracy: 23.316\n",
            "Validation Loss: 3.083833853403727, Validation Accuracy: 25.17\n",
            "[77/150]: Training Loss: 3.174022527841421, Training Accuracy: 23.79\n",
            "Validation Loss: 3.077226718266805, Validation Accuracy: 25.58\n",
            "[78/150]: Training Loss: 3.168999726955707, Training Accuracy: 23.912\n",
            "Validation Loss: 3.0722909768422446, Validation Accuracy: 25.47\n",
            "[79/150]: Training Loss: 3.1615905394920936, Training Accuracy: 23.708\n",
            "Validation Loss: 3.069349686304728, Validation Accuracy: 25.58\n",
            "[80/150]: Training Loss: 3.151583139712994, Training Accuracy: 24.078\n",
            "Validation Loss: 3.0607732137044272, Validation Accuracy: 25.63\n",
            "[81/150]: Training Loss: 3.149446652485774, Training Accuracy: 23.998\n",
            "Validation Loss: 3.057941754659017, Validation Accuracy: 25.81\n",
            "[82/150]: Training Loss: 3.145682463279137, Training Accuracy: 24.14\n",
            "Validation Loss: 3.0518407026926675, Validation Accuracy: 25.83\n",
            "[83/150]: Training Loss: 3.143723029356736, Training Accuracy: 24.192\n",
            "Validation Loss: 3.0451351006825766, Validation Accuracy: 25.97\n",
            "[84/150]: Training Loss: 3.1309727888840895, Training Accuracy: 24.412\n",
            "Validation Loss: 3.0412493546803794, Validation Accuracy: 26.02\n",
            "[85/150]: Training Loss: 3.13380243228032, Training Accuracy: 24.424\n",
            "Validation Loss: 3.0333561102549234, Validation Accuracy: 26.26\n",
            "[86/150]: Training Loss: 3.122998604407677, Training Accuracy: 24.482\n",
            "Validation Loss: 3.027780214945475, Validation Accuracy: 26.34\n",
            "[87/150]: Training Loss: 3.1258247999044566, Training Accuracy: 24.628\n",
            "Validation Loss: 3.0237667560577393, Validation Accuracy: 26.32\n",
            "[88/150]: Training Loss: 3.111476237957294, Training Accuracy: 24.682\n",
            "Validation Loss: 3.0223042170206704, Validation Accuracy: 26.43\n",
            "[89/150]: Training Loss: 3.120484553850614, Training Accuracy: 24.518\n",
            "Validation Loss: 3.0167242685953775, Validation Accuracy: 26.77\n",
            "[90/150]: Training Loss: 3.10716673044058, Training Accuracy: 24.886\n",
            "Validation Loss: 3.0143421490987143, Validation Accuracy: 26.59\n",
            "[91/150]: Training Loss: 3.096709581521841, Training Accuracy: 25.004\n",
            "Validation Loss: 3.0087748368581138, Validation Accuracy: 26.56\n",
            "[92/150]: Training Loss: 3.1066546073326697, Training Accuracy: 24.99\n",
            "Validation Loss: 3.005091905593872, Validation Accuracy: 26.95\n",
            "[93/150]: Training Loss: 3.090373185964731, Training Accuracy: 25.092\n",
            "Validation Loss: 3.000840902328491, Validation Accuracy: 27.03\n",
            "[94/150]: Training Loss: 3.0964173537034254, Training Accuracy: 25.044\n",
            "Validation Loss: 2.997614860534668, Validation Accuracy: 26.97\n",
            "[95/150]: Training Loss: 3.084940341802744, Training Accuracy: 25.148\n",
            "Validation Loss: 2.9952355225880942, Validation Accuracy: 26.87\n",
            "[96/150]: Training Loss: 3.09445228943458, Training Accuracy: 25.232\n",
            "Validation Loss: 2.9892234007517495, Validation Accuracy: 26.98\n",
            "[97/150]: Training Loss: 3.0830492606529822, Training Accuracy: 25.54\n",
            "Validation Loss: 2.9863297939300537, Validation Accuracy: 27.35\n",
            "[98/150]: Training Loss: 3.069628440416776, Training Accuracy: 25.52\n",
            "Validation Loss: 2.9870328108469644, Validation Accuracy: 27.45\n",
            "[99/150]: Training Loss: 3.0752683602846584, Training Accuracy: 25.444\n",
            "Validation Loss: 2.9794111251831055, Validation Accuracy: 27.24\n",
            "[100/150]: Training Loss: 3.069608578315148, Training Accuracy: 25.602\n",
            "Validation Loss: 2.977992057800293, Validation Accuracy: 27.51\n",
            "[101/150]: Training Loss: 3.0671214507176328, Training Accuracy: 25.66\n",
            "Validation Loss: 2.9746710459391275, Validation Accuracy: 27.32\n",
            "[102/150]: Training Loss: 3.0730385780334473, Training Accuracy: 25.788\n",
            "Validation Loss: 2.9698847929636636, Validation Accuracy: 27.54\n",
            "[103/150]: Training Loss: 3.054584264755249, Training Accuracy: 25.852\n",
            "Validation Loss: 2.967825253804525, Validation Accuracy: 27.85\n",
            "[104/150]: Training Loss: 3.06634913958036, Training Accuracy: 25.634\n",
            "Validation Loss: 2.965172370274862, Validation Accuracy: 27.64\n",
            "[105/150]: Training Loss: 3.0517038382016697, Training Accuracy: 26.048\n",
            "Validation Loss: 2.9605621496836343, Validation Accuracy: 27.68\n",
            "[106/150]: Training Loss: 3.0615938810201793, Training Accuracy: 25.86\n",
            "Validation Loss: 2.9625186920166016, Validation Accuracy: 27.67\n",
            "[107/150]: Training Loss: 3.0499529104966383, Training Accuracy: 26.146\n",
            "Validation Loss: 2.9583574136098227, Validation Accuracy: 27.73\n",
            "[108/150]: Training Loss: 3.0448730908907375, Training Accuracy: 25.872\n",
            "Validation Loss: 2.95459246635437, Validation Accuracy: 27.94\n",
            "[109/150]: Training Loss: 3.0504057774176965, Training Accuracy: 25.964\n",
            "Validation Loss: 2.953615427017212, Validation Accuracy: 27.92\n",
            "[110/150]: Training Loss: 3.0516765851240892, Training Accuracy: 25.97\n",
            "Validation Loss: 2.9511011441548667, Validation Accuracy: 27.82\n",
            "[111/150]: Training Loss: 3.0474912019876332, Training Accuracy: 26.346\n",
            "Validation Loss: 2.9493021965026855, Validation Accuracy: 27.93\n",
            "[112/150]: Training Loss: 3.047640800476074, Training Accuracy: 26.166\n",
            "Validation Loss: 2.9490411281585693, Validation Accuracy: 28.1\n",
            "[113/150]: Training Loss: 3.0393428068894606, Training Accuracy: 26.168\n",
            "Validation Loss: 2.9449055989583335, Validation Accuracy: 28.15\n",
            "[114/150]: Training Loss: 3.0394653173593373, Training Accuracy: 26.22\n",
            "Validation Loss: 2.9426263173421225, Validation Accuracy: 28.19\n",
            "[115/150]: Training Loss: 3.0380828013786902, Training Accuracy: 26.418\n",
            "Validation Loss: 2.9426259199778237, Validation Accuracy: 28.35\n",
            "[116/150]: Training Loss: 3.028611715023334, Training Accuracy: 26.422\n",
            "Validation Loss: 2.9393649101257324, Validation Accuracy: 28.0\n",
            "[117/150]: Training Loss: 3.030808155353253, Training Accuracy: 26.422\n",
            "Validation Loss: 2.9383947054545083, Validation Accuracy: 28.26\n",
            "[118/150]: Training Loss: 3.0299956798553467, Training Accuracy: 26.366\n",
            "Validation Loss: 2.937206586201986, Validation Accuracy: 28.37\n",
            "[119/150]: Training Loss: 3.029345255631667, Training Accuracy: 26.258\n",
            "Validation Loss: 2.9358492692311606, Validation Accuracy: 28.28\n",
            "[120/150]: Training Loss: 3.027382172071017, Training Accuracy: 26.33\n",
            "Validation Loss: 2.934878349304199, Validation Accuracy: 28.35\n",
            "[121/150]: Training Loss: 3.0247208155118503, Training Accuracy: 26.736\n",
            "Validation Loss: 2.934181054433187, Validation Accuracy: 28.29\n",
            "[122/150]: Training Loss: 3.0200733771690955, Training Accuracy: 26.62\n",
            "Validation Loss: 2.932123899459839, Validation Accuracy: 28.54\n",
            "[123/150]: Training Loss: 3.0223946754749003, Training Accuracy: 26.398\n",
            "Validation Loss: 2.9316653410593667, Validation Accuracy: 28.46\n",
            "[124/150]: Training Loss: 3.0208843304560733, Training Accuracy: 26.57\n",
            "Validation Loss: 2.9322214126586914, Validation Accuracy: 28.39\n",
            "[125/150]: Training Loss: 3.020762535241934, Training Accuracy: 26.512\n",
            "Validation Loss: 2.929534435272217, Validation Accuracy: 28.53\n",
            "[126/150]: Training Loss: 3.010956544142503, Training Accuracy: 26.498\n",
            "Validation Loss: 2.9289840857187905, Validation Accuracy: 28.49\n",
            "[127/150]: Training Loss: 3.0190560817718506, Training Accuracy: 26.902\n",
            "Validation Loss: 2.9280758698781333, Validation Accuracy: 28.4\n",
            "[128/150]: Training Loss: 3.016689117138202, Training Accuracy: 26.766\n",
            "Validation Loss: 2.9267163276672363, Validation Accuracy: 28.58\n",
            "[129/150]: Training Loss: 3.013255009284386, Training Accuracy: 26.838\n",
            "Validation Loss: 2.926602840423584, Validation Accuracy: 28.63\n",
            "[130/150]: Training Loss: 3.008544298318716, Training Accuracy: 26.784\n",
            "Validation Loss: 2.925427039464315, Validation Accuracy: 28.64\n",
            "[131/150]: Training Loss: 3.0117177046262302, Training Accuracy: 26.518\n",
            "Validation Loss: 2.925293207168579, Validation Accuracy: 28.49\n",
            "[132/150]: Training Loss: 3.0137419517223654, Training Accuracy: 26.672\n",
            "Validation Loss: 2.9240074157714844, Validation Accuracy: 28.62\n",
            "[133/150]: Training Loss: 3.0168117743272047, Training Accuracy: 26.534\n",
            "Validation Loss: 2.9242477416992188, Validation Accuracy: 28.53\n",
            "[134/150]: Training Loss: 3.0148205206944394, Training Accuracy: 26.644\n",
            "Validation Loss: 2.9236361980438232, Validation Accuracy: 28.59\n",
            "[135/150]: Training Loss: 3.0078163146972656, Training Accuracy: 26.776\n",
            "Validation Loss: 2.922917683919271, Validation Accuracy: 28.69\n",
            "[136/150]: Training Loss: 3.0173892241257887, Training Accuracy: 26.794\n",
            "Validation Loss: 2.922706445058187, Validation Accuracy: 28.62\n",
            "[137/150]: Training Loss: 3.0146707388070912, Training Accuracy: 26.676\n",
            "Validation Loss: 2.922335624694824, Validation Accuracy: 28.61\n",
            "[138/150]: Training Loss: 3.010614743599525, Training Accuracy: 26.824\n",
            "Validation Loss: 2.921966473261515, Validation Accuracy: 28.58\n",
            "[139/150]: Training Loss: 3.014005239193256, Training Accuracy: 26.566\n",
            "Validation Loss: 2.921886603037516, Validation Accuracy: 28.76\n",
            "[140/150]: Training Loss: 3.0139314211331882, Training Accuracy: 26.826\n",
            "Validation Loss: 2.921703894933065, Validation Accuracy: 28.65\n",
            "[141/150]: Training Loss: 3.0081347135397105, Training Accuracy: 26.866\n",
            "Validation Loss: 2.921877384185791, Validation Accuracy: 28.66\n",
            "[142/150]: Training Loss: 3.010774318988507, Training Accuracy: 27.064\n",
            "Validation Loss: 2.921614408493042, Validation Accuracy: 28.72\n",
            "[143/150]: Training Loss: 3.0070647643162656, Training Accuracy: 26.66\n",
            "Validation Loss: 2.9214425086975098, Validation Accuracy: 28.62\n",
            "[144/150]: Training Loss: 3.010379406122061, Training Accuracy: 26.672\n",
            "Validation Loss: 2.9214176336924234, Validation Accuracy: 28.66\n",
            "[145/150]: Training Loss: 3.0113004721128025, Training Accuracy: 26.766\n",
            "Validation Loss: 2.9213058948516846, Validation Accuracy: 28.66\n",
            "[146/150]: Training Loss: 3.016514778137207, Training Accuracy: 26.868\n",
            "Validation Loss: 2.921256939570109, Validation Accuracy: 28.67\n",
            "[147/150]: Training Loss: 3.013971310395461, Training Accuracy: 26.528\n",
            "Validation Loss: 2.921257495880127, Validation Accuracy: 28.66\n",
            "[148/150]: Training Loss: 3.0143010432903585, Training Accuracy: 26.594\n",
            "Validation Loss: 2.9212076663970947, Validation Accuracy: 28.67\n",
            "[149/150]: Training Loss: 3.01210671204787, Training Accuracy: 26.668\n",
            "Validation Loss: 2.921198527018229, Validation Accuracy: 28.68\n",
            "[150/150]: Training Loss: 3.010485594089215, Training Accuracy: 27.018\n",
            "Validation Loss: 2.921196937561035, Validation Accuracy: 28.68\n",
            "**********************************************************************\n",
            "Test Loss: 2.921196937561035, Test Accuracy: 28.68\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td></td></tr><tr><td>Test Loss</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>28.68</td></tr><tr><td>Test Loss</td><td>2.9212</td></tr><tr><td>Train Accuracy</td><td>27.018</td></tr><tr><td>Train Loss</td><td>3.01049</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_size=4096 learning_rate=0.0012 weight_decay=0.0001</strong> at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/j81b4d42' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/j81b4d42</a><br/> View project at: <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240613_070246-j81b4d42/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Batch size: 8192\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ali/Repos/University/MLDL/Project/wandb/run-20240613_072517-j7f6j52c</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/j7f6j52c' target=\"_blank\">batch_size=8192 learning_rate=0.001697056274847714 weight_decay=0.0001</a></strong> to <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/j7f6j52c' target=\"_blank\">https://wandb.ai/mldlpolitofari733/cifar100-training-mldl2024-baseline-LAMB_Large_Batches/runs/j7f6j52c</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/150]: Training Loss: 4.605092797960554, Training Accuracy: 1.07\n",
            "Validation Loss: 4.604321002960205, Validation Accuracy: 1.5\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.53 GiB. GPU ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39mnum_epochs)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_train_loader_large_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_test_loader_large_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_test_loader_large_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLAMB_Large_Batches\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_wandb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[22], line 46\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(num_epochs, model, trainloader, validationloader, testloader, optimizer, scheduler, loss_fn, device, optimizer_name, accumulation_steps, hyperparameters, is_wandb, n_epochs_stop)\u001b[0m\n\u001b[1;32m     40\u001b[0m   wandb\u001b[38;5;241m.\u001b[39minit(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mrun_id, name\u001b[38;5;241m=\u001b[39mrun_name, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcifar100-training-mldl2024-baseline-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     41\u001b[0m                  config\u001b[38;5;241m=\u001b[39mhyperparameters \u001b[38;5;28;01mif\u001b[39;00m hyperparameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {},\n\u001b[1;32m     42\u001b[0m                  resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m, reinit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, num_epochs):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Call the training function for each epoch\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_wandb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Update learning rate based on scheduler\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn, accumulation_steps, device, is_wandb)\u001b[0m\n\u001b[1;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     16\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/Project/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.53 GiB. GPU "
          ]
        }
      ],
      "source": [
        "# check the lr updates from paper 18\n",
        "\n",
        "num_epochs = 150\n",
        "wd = 1e-04\n",
        "batch_sizes = [512, 1024, 2048, 4096, 8192, 16384 , 32768]\n",
        "learning_rates = [4.8/(2**i * 10**3) for i in reversed([x * 0.5 for x in range(0, 8)])]\n",
        "\n",
        "for worker, batch_size in enumerate(batch_sizes):\n",
        "  print('='*50)\n",
        "  print(f'Batch size: {batch_size}')\n",
        "  print('='*50)\n",
        "\n",
        "  hyperparameters = {\n",
        "    'batch_size': batch_size,\n",
        "    'learning_rate': learning_rates[worker],\n",
        "    'weight_decay' : wd\n",
        "  }\n",
        "\n",
        "  if batch_size <= 4096:\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= batch_size)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LAMB(model.parameters(), learning_rate=learning_rates[worker], weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LAMB_Large_Batches',\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True\n",
        "    )\n",
        "  else:\n",
        "    accumulation_steps = batch_size // 4096\n",
        "    # load data\n",
        "    data = CIFAR100Data(batch_size= batch_size)\n",
        "    original_train_loader_large_batch, original_test_loader_large_batch = data.train_test()\n",
        "    # Load the model\n",
        "    model = LeNet5().to(device)\n",
        "    # Optimizer and scheduler setup\n",
        "    optimizer = LAMB(model.parameters(), learning_rate=learning_rates[worker], weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    # Training\n",
        "    run_training(\n",
        "        num_epochs,\n",
        "        model,\n",
        "        original_train_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        original_test_loader_large_batch,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        device,\n",
        "        optimizer_name='LAMB_Large_Batches',\n",
        "        accumulation_steps=accumulation_steps,\n",
        "        hyperparameters=hyperparameters,\n",
        "        is_wandb = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Distributed Approaches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hRd9SL7iRiKl"
      },
      "outputs": [],
      "source": [
        "# Initialize a model and save its initial parameters\n",
        "initial_model = LeNet5()\n",
        "initial_state_dict = initial_model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GXZaFXruZI_"
      },
      "source": [
        "### Local SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I8hvzRbQW32s"
      },
      "outputs": [],
      "source": [
        "def synchronize(models):\n",
        "  for params in zip(*[model.parameters() for model in models]):\n",
        "    param_avg = torch.mean(torch.stack([param.data for param in params]), dim=0)\n",
        "    for param in params:\n",
        "      param.data = param_avg\n",
        "  \n",
        "  return models[0]\n",
        "\n",
        "# def synchronize(global_model, local_models):\n",
        "#         # Initialize a state dict with zeros, same shape as the model parameters\n",
        "#         delta = {key: torch.zeros_like(value) for key, value in local_models[0].state_dict().items()}\n",
        "        \n",
        "#         # Sum up all the model parameters\n",
        "#         for local_model in local_models:\n",
        "#             for key, value in local_model.state_dict().items():\n",
        "#                 delta[key] += (global_model.state_dict()[key] - value)\n",
        "        \n",
        "#         # Divide each parameter by the number of models to get the average\n",
        "#         for key in delta:\n",
        "#             delta[key] /= len(local_models)\n",
        "        \n",
        "#         new_weights = {}\n",
        "#         for key, value in global_model.state_dict().items():\n",
        "#             new_weights[key] = value -  delta [key] # TODO: az TA beporsim ke learning rate ro chetor hesab konim.\n",
        "        \n",
        "#         global_model.load_state_dict(new_weights)\n",
        "#         return global_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def local_SGD(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs):\n",
        "  total_start_time = time.time()\n",
        "\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "  # global_optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "  # Initialize a scheduler for each optimizer\n",
        "  # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for loca_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    # global_model = synchronize(global_model, local_models, 1)\n",
        "    global_model = synchronize(local_models)\n",
        "\n",
        "    # for local_model in local_models:\n",
        "    #   local_model.load_state_dict(global_model.state_dict())\n",
        "    # scheduler.step()\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Local Step {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "# TODO: Schedule ro check konim \n",
        "\n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for local_SGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:4\n",
            "==================================================\n",
            "Worker 1, [01/04]: Training Loss: 4.315399546, Training Accuracy: 4.296\n",
            "Worker 1, [02/04]: Training Loss: 3.846193394, Training Accuracy: 10.336\n",
            "Worker 1, [03/04]: Training Loss: 3.586764340, Training Accuracy: 14.848\n",
            "Worker 1, [04/04]: Training Loss: 3.376465985, Training Accuracy: 18.140\n",
            "Time taken for training worker 1: 0:00:51.008340\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.316840096, Training Accuracy: 4.236\n",
            "Worker 2, [02/04]: Training Loss: 3.856072393, Training Accuracy: 10.148\n",
            "Worker 2, [03/04]: Training Loss: 3.589844117, Training Accuracy: 14.496\n",
            "Worker 2, [04/04]: Training Loss: 3.388656141, Training Accuracy: 18.276\n",
            "Time taken for training worker 2: 0:00:44.085647\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.008224\n",
            "Local Step 01: Test Loss: 3.216368985, Test Accuracy: 23.090\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.239569374, Training Accuracy: 20.712\n",
            "Worker 1, [02/04]: Training Loss: 3.068697775, Training Accuracy: 23.880\n",
            "Worker 1, [03/04]: Training Loss: 2.935029562, Training Accuracy: 26.196\n",
            "Worker 1, [04/04]: Training Loss: 2.823666452, Training Accuracy: 28.444\n",
            "Time taken for training worker 1: 0:00:47.276171\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.863885114, Training Accuracy: 27.676\n",
            "Worker 2, [02/04]: Training Loss: 2.737123156, Training Accuracy: 30.560\n",
            "Worker 2, [03/04]: Training Loss: 2.632063059, Training Accuracy: 32.368\n",
            "Worker 2, [04/04]: Training Loss: 2.580919172, Training Accuracy: 33.488\n",
            "Time taken for training worker 2: 0:00:43.917796\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000531\n",
            "Local Step 02: Test Loss: 2.489029051, Test Accuracy: 35.590\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.598559349, Training Accuracy: 33.692\n",
            "Worker 1, [02/04]: Training Loss: 2.498673613, Training Accuracy: 35.516\n",
            "Worker 1, [03/04]: Training Loss: 2.427237312, Training Accuracy: 36.652\n",
            "Worker 1, [04/04]: Training Loss: 2.366972564, Training Accuracy: 37.908\n",
            "Time taken for training worker 1: 0:00:48.763055\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.480018179, Training Accuracy: 35.936\n",
            "Worker 2, [02/04]: Training Loss: 2.356112279, Training Accuracy: 38.428\n",
            "Worker 2, [03/04]: Training Loss: 2.294468072, Training Accuracy: 39.648\n",
            "Worker 2, [04/04]: Training Loss: 2.242942953, Training Accuracy: 40.448\n",
            "Time taken for training worker 2: 0:00:46.188388\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000551\n",
            "Local Step 03: Test Loss: 2.278792409, Test Accuracy: 40.530\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.349842589, Training Accuracy: 38.708\n",
            "Worker 1, [02/04]: Training Loss: 2.243927027, Training Accuracy: 41.176\n",
            "Worker 1, [03/04]: Training Loss: 2.175549427, Training Accuracy: 41.880\n",
            "Worker 1, [04/04]: Training Loss: 2.105719034, Training Accuracy: 43.500\n",
            "Time taken for training worker 1: 0:00:45.070772\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.276641228, Training Accuracy: 40.468\n",
            "Worker 2, [02/04]: Training Loss: 2.165776744, Training Accuracy: 42.424\n",
            "Worker 2, [03/04]: Training Loss: 2.111116706, Training Accuracy: 43.880\n",
            "Worker 2, [04/04]: Training Loss: 2.068090763, Training Accuracy: 44.452\n",
            "Time taken for training worker 2: 0:00:47.743246\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000760\n",
            "Local Step 04: Test Loss: 2.173862699, Test Accuracy: 43.650\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.184221792, Training Accuracy: 42.184\n",
            "Worker 1, [02/04]: Training Loss: 2.086563339, Training Accuracy: 44.208\n",
            "Worker 1, [03/04]: Training Loss: 2.010224591, Training Accuracy: 45.668\n",
            "Worker 1, [04/04]: Training Loss: 1.988282413, Training Accuracy: 46.220\n",
            "Time taken for training worker 1: 0:00:46.436739\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.160926102, Training Accuracy: 42.844\n",
            "Worker 2, [02/04]: Training Loss: 2.038424500, Training Accuracy: 45.292\n",
            "Worker 2, [03/04]: Training Loss: 1.974370769, Training Accuracy: 46.900\n",
            "Worker 2, [04/04]: Training Loss: 1.938126489, Training Accuracy: 47.504\n",
            "Time taken for training worker 2: 0:00:46.689149\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000574\n",
            "Local Step 05: Test Loss: 2.224828663, Test Accuracy: 43.450\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.077701572, Training Accuracy: 44.380\n",
            "Worker 1, [02/04]: Training Loss: 1.979205159, Training Accuracy: 46.812\n",
            "Worker 1, [03/04]: Training Loss: 1.915687450, Training Accuracy: 47.972\n",
            "Worker 1, [04/04]: Training Loss: 1.882580233, Training Accuracy: 48.912\n",
            "Time taken for training worker 1: 0:00:42.228748\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.047660645, Training Accuracy: 45.124\n",
            "Worker 2, [02/04]: Training Loss: 1.951561577, Training Accuracy: 47.564\n",
            "Worker 2, [03/04]: Training Loss: 1.878381612, Training Accuracy: 48.508\n",
            "Worker 2, [04/04]: Training Loss: 1.852291736, Training Accuracy: 49.488\n",
            "Time taken for training worker 2: 0:00:41.467570\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000540\n",
            "Local Step 06: Test Loss: 2.140733832, Test Accuracy: 45.020\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.036254698, Training Accuracy: 45.588\n",
            "Worker 1, [02/04]: Training Loss: 1.902555000, Training Accuracy: 48.772\n",
            "Worker 1, [03/04]: Training Loss: 1.856601261, Training Accuracy: 49.732\n",
            "Worker 1, [04/04]: Training Loss: 1.801501949, Training Accuracy: 50.656\n",
            "Time taken for training worker 1: 0:00:44.574137\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 2.007684349, Training Accuracy: 46.508\n",
            "Worker 2, [02/04]: Training Loss: 1.894694477, Training Accuracy: 48.492\n",
            "Worker 2, [03/04]: Training Loss: 1.822851547, Training Accuracy: 50.184\n",
            "Worker 2, [04/04]: Training Loss: 1.791383231, Training Accuracy: 51.032\n",
            "Time taken for training worker 2: 0:00:45.191746\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000562\n",
            "Local Step 07: Test Loss: 2.012759759, Test Accuracy: 47.120\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.951399381, Training Accuracy: 47.640\n",
            "Worker 1, [02/04]: Training Loss: 1.845973177, Training Accuracy: 49.860\n",
            "Worker 1, [03/04]: Training Loss: 1.790295301, Training Accuracy: 50.872\n",
            "Worker 1, [04/04]: Training Loss: 1.740407906, Training Accuracy: 52.168\n",
            "Time taken for training worker 1: 0:00:40.352975\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.956294446, Training Accuracy: 47.356\n",
            "Worker 2, [02/04]: Training Loss: 1.834964831, Training Accuracy: 49.824\n",
            "Worker 2, [03/04]: Training Loss: 1.779525697, Training Accuracy: 50.892\n",
            "Worker 2, [04/04]: Training Loss: 1.747238727, Training Accuracy: 52.328\n",
            "Time taken for training worker 2: 0:00:40.305939\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000567\n",
            "Local Step 08: Test Loss: 2.013314363, Test Accuracy: 47.050\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.933766736, Training Accuracy: 47.960\n",
            "Worker 1, [02/04]: Training Loss: 1.802756399, Training Accuracy: 50.756\n",
            "Worker 1, [03/04]: Training Loss: 1.734640786, Training Accuracy: 52.276\n",
            "Worker 1, [04/04]: Training Loss: 1.696844753, Training Accuracy: 53.124\n",
            "Time taken for training worker 1: 0:00:42.108522\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.936961787, Training Accuracy: 47.676\n",
            "Worker 2, [02/04]: Training Loss: 1.783605564, Training Accuracy: 51.156\n",
            "Worker 2, [03/04]: Training Loss: 1.735144533, Training Accuracy: 52.376\n",
            "Worker 2, [04/04]: Training Loss: 1.700380967, Training Accuracy: 53.140\n",
            "Time taken for training worker 2: 0:00:44.558463\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000559\n",
            "Local Step 09: Test Loss: 2.027832682, Test Accuracy: 47.640\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.900707688, Training Accuracy: 48.836\n",
            "Worker 1, [02/04]: Training Loss: 1.768305817, Training Accuracy: 51.744\n",
            "Worker 1, [03/04]: Training Loss: 1.716563207, Training Accuracy: 52.552\n",
            "Worker 1, [04/04]: Training Loss: 1.667550586, Training Accuracy: 53.792\n",
            "Time taken for training worker 1: 0:00:41.257722\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.888583995, Training Accuracy: 49.220\n",
            "Worker 2, [02/04]: Training Loss: 1.751488832, Training Accuracy: 51.816\n",
            "Worker 2, [03/04]: Training Loss: 1.712358657, Training Accuracy: 52.704\n",
            "Worker 2, [04/04]: Training Loss: 1.668679609, Training Accuracy: 53.680\n",
            "Time taken for training worker 2: 0:00:43.482212\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000510\n",
            "Local Step 10: Test Loss: 2.029718738, Test Accuracy: 47.810\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.871908717, Training Accuracy: 49.852\n",
            "Worker 1, [02/04]: Training Loss: 1.735116032, Training Accuracy: 52.208\n",
            "Worker 1, [03/04]: Training Loss: 1.697324191, Training Accuracy: 53.344\n",
            "Worker 1, [04/04]: Training Loss: 1.654917484, Training Accuracy: 54.260\n",
            "Time taken for training worker 1: 0:00:41.910995\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.872440737, Training Accuracy: 49.476\n",
            "Worker 2, [02/04]: Training Loss: 1.768136095, Training Accuracy: 51.136\n",
            "Worker 2, [03/04]: Training Loss: 1.686603725, Training Accuracy: 53.532\n",
            "Worker 2, [04/04]: Training Loss: 1.659475773, Training Accuracy: 53.920\n",
            "Time taken for training worker 2: 0:00:42.094746\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000604\n",
            "Local Step 11: Test Loss: 2.046678778, Test Accuracy: 47.080\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.868558591, Training Accuracy: 49.328\n",
            "Worker 1, [02/04]: Training Loss: 1.719616632, Training Accuracy: 52.784\n",
            "Worker 1, [03/04]: Training Loss: 1.669654434, Training Accuracy: 53.916\n",
            "Worker 1, [04/04]: Training Loss: 1.615242213, Training Accuracy: 55.272\n",
            "Time taken for training worker 1: 0:00:43.660864\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.859684991, Training Accuracy: 49.384\n",
            "Worker 2, [02/04]: Training Loss: 1.722841801, Training Accuracy: 52.584\n",
            "Worker 2, [03/04]: Training Loss: 1.651348845, Training Accuracy: 54.212\n",
            "Worker 2, [04/04]: Training Loss: 1.623807970, Training Accuracy: 55.212\n",
            "Time taken for training worker 2: 0:00:44.407965\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000550\n",
            "Local Step 12: Test Loss: 1.992342483, Test Accuracy: 48.260\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.824328659, Training Accuracy: 50.500\n",
            "Worker 1, [02/04]: Training Loss: 1.696616916, Training Accuracy: 53.256\n",
            "Worker 1, [03/04]: Training Loss: 1.650483797, Training Accuracy: 54.220\n",
            "Worker 1, [04/04]: Training Loss: 1.597594370, Training Accuracy: 55.560\n",
            "Time taken for training worker 1: 0:00:41.969716\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.821966047, Training Accuracy: 50.396\n",
            "Worker 2, [02/04]: Training Loss: 1.706782409, Training Accuracy: 52.924\n",
            "Worker 2, [03/04]: Training Loss: 1.654572781, Training Accuracy: 54.388\n",
            "Worker 2, [04/04]: Training Loss: 1.615614506, Training Accuracy: 54.848\n",
            "Time taken for training worker 2: 0:00:42.899880\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000600\n",
            "Local Step 13: Test Loss: 2.010459565, Test Accuracy: 48.380\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.802059744, Training Accuracy: 51.192\n",
            "Worker 1, [02/04]: Training Loss: 1.678227955, Training Accuracy: 53.704\n",
            "Worker 1, [03/04]: Training Loss: 1.629456942, Training Accuracy: 54.584\n",
            "Worker 1, [04/04]: Training Loss: 1.580255166, Training Accuracy: 56.048\n",
            "Time taken for training worker 1: 0:00:41.687592\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.813535644, Training Accuracy: 50.528\n",
            "Worker 2, [02/04]: Training Loss: 1.693421038, Training Accuracy: 53.196\n",
            "Worker 2, [03/04]: Training Loss: 1.633764052, Training Accuracy: 55.020\n",
            "Worker 2, [04/04]: Training Loss: 1.609233691, Training Accuracy: 55.432\n",
            "Time taken for training worker 2: 0:00:44.647972\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000488\n",
            "Local Step 14: Test Loss: 2.036164627, Test Accuracy: 47.480\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.788684089, Training Accuracy: 51.024\n",
            "Worker 1, [02/04]: Training Loss: 1.672925325, Training Accuracy: 53.648\n",
            "Worker 1, [03/04]: Training Loss: 1.622793295, Training Accuracy: 55.064\n",
            "Worker 1, [04/04]: Training Loss: 1.565317077, Training Accuracy: 55.892\n",
            "Time taken for training worker 1: 0:00:44.553220\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.809192822, Training Accuracy: 50.980\n",
            "Worker 2, [02/04]: Training Loss: 1.664203435, Training Accuracy: 53.668\n",
            "Worker 2, [03/04]: Training Loss: 1.619450357, Training Accuracy: 54.792\n",
            "Worker 2, [04/04]: Training Loss: 1.582554337, Training Accuracy: 55.532\n",
            "Time taken for training worker 2: 0:00:47.214269\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000812\n",
            "Local Step 15: Test Loss: 2.027577579, Test Accuracy: 48.110\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.794271789, Training Accuracy: 51.056\n",
            "Worker 1, [02/04]: Training Loss: 1.644876641, Training Accuracy: 54.540\n",
            "Worker 1, [03/04]: Training Loss: 1.598927473, Training Accuracy: 55.616\n",
            "Worker 1, [04/04]: Training Loss: 1.557330744, Training Accuracy: 56.156\n",
            "Time taken for training worker 1: 0:00:46.106251\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.794189628, Training Accuracy: 51.460\n",
            "Worker 2, [02/04]: Training Loss: 1.655723564, Training Accuracy: 54.300\n",
            "Worker 2, [03/04]: Training Loss: 1.605733752, Training Accuracy: 55.416\n",
            "Worker 2, [04/04]: Training Loss: 1.565331547, Training Accuracy: 56.452\n",
            "Time taken for training worker 2: 0:00:45.267761\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000628\n",
            "Local Step 16: Test Loss: 1.941947171, Test Accuracy: 49.470\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.771252206, Training Accuracy: 51.704\n",
            "Worker 1, [02/04]: Training Loss: 1.630267228, Training Accuracy: 54.836\n",
            "Worker 1, [03/04]: Training Loss: 1.575858370, Training Accuracy: 56.216\n",
            "Worker 1, [04/04]: Training Loss: 1.559422623, Training Accuracy: 56.044\n",
            "Time taken for training worker 1: 0:00:47.446962\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.789662509, Training Accuracy: 51.144\n",
            "Worker 2, [02/04]: Training Loss: 1.653985833, Training Accuracy: 53.720\n",
            "Worker 2, [03/04]: Training Loss: 1.590113463, Training Accuracy: 56.008\n",
            "Worker 2, [04/04]: Training Loss: 1.575292255, Training Accuracy: 56.136\n",
            "Time taken for training worker 2: 0:00:46.996509\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000637\n",
            "Local Step 17: Test Loss: 2.005510665, Test Accuracy: 48.990\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.766591015, Training Accuracy: 51.932\n",
            "Worker 1, [02/04]: Training Loss: 1.618415339, Training Accuracy: 55.380\n",
            "Worker 1, [03/04]: Training Loss: 1.577990571, Training Accuracy: 55.968\n",
            "Worker 1, [04/04]: Training Loss: 1.550442837, Training Accuracy: 56.536\n",
            "Time taken for training worker 1: 0:00:48.322819\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.792976115, Training Accuracy: 51.044\n",
            "Worker 2, [02/04]: Training Loss: 1.640227460, Training Accuracy: 54.448\n",
            "Worker 2, [03/04]: Training Loss: 1.581089593, Training Accuracy: 55.900\n",
            "Worker 2, [04/04]: Training Loss: 1.541541680, Training Accuracy: 56.844\n",
            "Time taken for training worker 2: 0:00:49.118160\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000967\n",
            "Local Step 18: Test Loss: 1.963047835, Test Accuracy: 49.460\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.757024595, Training Accuracy: 52.240\n",
            "Worker 1, [02/04]: Training Loss: 1.610147198, Training Accuracy: 55.392\n",
            "Worker 1, [03/04]: Training Loss: 1.567212435, Training Accuracy: 56.116\n",
            "Worker 1, [04/04]: Training Loss: 1.530296017, Training Accuracy: 57.076\n",
            "Time taken for training worker 1: 0:00:45.955582\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.773661904, Training Accuracy: 51.644\n",
            "Worker 2, [02/04]: Training Loss: 1.644880925, Training Accuracy: 54.212\n",
            "Worker 2, [03/04]: Training Loss: 1.571882886, Training Accuracy: 55.924\n",
            "Worker 2, [04/04]: Training Loss: 1.530633163, Training Accuracy: 56.816\n",
            "Time taken for training worker 2: 0:00:45.642728\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000585\n",
            "Local Step 19: Test Loss: 2.048034929, Test Accuracy: 48.190\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.743999614, Training Accuracy: 52.508\n",
            "Worker 1, [02/04]: Training Loss: 1.595612974, Training Accuracy: 55.640\n",
            "Worker 1, [03/04]: Training Loss: 1.564588780, Training Accuracy: 56.420\n",
            "Worker 1, [04/04]: Training Loss: 1.528170942, Training Accuracy: 57.136\n",
            "Time taken for training worker 1: 0:00:47.627416\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.782926637, Training Accuracy: 51.480\n",
            "Worker 2, [02/04]: Training Loss: 1.635596977, Training Accuracy: 54.716\n",
            "Worker 2, [03/04]: Training Loss: 1.562120108, Training Accuracy: 56.540\n",
            "Worker 2, [04/04]: Training Loss: 1.536822163, Training Accuracy: 57.252\n",
            "Time taken for training worker 2: 0:00:41.633992\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000525\n",
            "Local Step 20: Test Loss: 2.021172227, Test Accuracy: 48.320\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.748767454, Training Accuracy: 52.276\n",
            "Worker 1, [02/04]: Training Loss: 1.615762820, Training Accuracy: 54.908\n",
            "Worker 1, [03/04]: Training Loss: 1.557103302, Training Accuracy: 56.416\n",
            "Worker 1, [04/04]: Training Loss: 1.516860030, Training Accuracy: 57.480\n",
            "Time taken for training worker 1: 0:00:44.356742\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.758009334, Training Accuracy: 51.776\n",
            "Worker 2, [02/04]: Training Loss: 1.621550469, Training Accuracy: 55.064\n",
            "Worker 2, [03/04]: Training Loss: 1.574730024, Training Accuracy: 56.388\n",
            "Worker 2, [04/04]: Training Loss: 1.533429039, Training Accuracy: 57.080\n",
            "Time taken for training worker 2: 0:00:45.883793\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000551\n",
            "Local Step 21: Test Loss: 1.970814219, Test Accuracy: 49.170\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.743379794, Training Accuracy: 52.548\n",
            "Worker 1, [02/04]: Training Loss: 1.579996549, Training Accuracy: 56.152\n",
            "Worker 1, [03/04]: Training Loss: 1.551186547, Training Accuracy: 56.748\n",
            "Worker 1, [04/04]: Training Loss: 1.508216276, Training Accuracy: 57.352\n",
            "Time taken for training worker 1: 0:00:48.357259\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.740001288, Training Accuracy: 52.372\n",
            "Worker 2, [02/04]: Training Loss: 1.620134672, Training Accuracy: 54.968\n",
            "Worker 2, [03/04]: Training Loss: 1.569661969, Training Accuracy: 56.168\n",
            "Worker 2, [04/04]: Training Loss: 1.541917728, Training Accuracy: 56.952\n",
            "Time taken for training worker 2: 0:00:49.139157\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000663\n",
            "Local Step 22: Test Loss: 1.992985031, Test Accuracy: 48.570\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.726623797, Training Accuracy: 52.932\n",
            "Worker 1, [02/04]: Training Loss: 1.599821070, Training Accuracy: 55.672\n",
            "Worker 1, [03/04]: Training Loss: 1.543485169, Training Accuracy: 57.036\n",
            "Worker 1, [04/04]: Training Loss: 1.503557831, Training Accuracy: 57.984\n",
            "Time taken for training worker 1: 0:00:46.220511\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.743281325, Training Accuracy: 52.124\n",
            "Worker 2, [02/04]: Training Loss: 1.612168718, Training Accuracy: 54.932\n",
            "Worker 2, [03/04]: Training Loss: 1.546689066, Training Accuracy: 56.872\n",
            "Worker 2, [04/04]: Training Loss: 1.508324786, Training Accuracy: 58.084\n",
            "Time taken for training worker 2: 0:00:41.504932\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000508\n",
            "Local Step 23: Test Loss: 1.981476207, Test Accuracy: 49.120\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.727423599, Training Accuracy: 52.548\n",
            "Worker 1, [02/04]: Training Loss: 1.580154255, Training Accuracy: 56.264\n",
            "Worker 1, [03/04]: Training Loss: 1.532020950, Training Accuracy: 57.204\n",
            "Worker 1, [04/04]: Training Loss: 1.490645967, Training Accuracy: 58.124\n",
            "Time taken for training worker 1: 0:00:42.153967\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.741474700, Training Accuracy: 52.076\n",
            "Worker 2, [02/04]: Training Loss: 1.597600942, Training Accuracy: 55.640\n",
            "Worker 2, [03/04]: Training Loss: 1.548683579, Training Accuracy: 56.588\n",
            "Worker 2, [04/04]: Training Loss: 1.523999765, Training Accuracy: 56.940\n",
            "Time taken for training worker 2: 0:00:40.835441\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000514\n",
            "Local Step 24: Test Loss: 1.982676328, Test Accuracy: 49.470\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.719898771, Training Accuracy: 52.824\n",
            "Worker 1, [02/04]: Training Loss: 1.594948723, Training Accuracy: 55.784\n",
            "Worker 1, [03/04]: Training Loss: 1.523222827, Training Accuracy: 57.392\n",
            "Worker 1, [04/04]: Training Loss: 1.487878665, Training Accuracy: 58.244\n",
            "Time taken for training worker 1: 0:00:42.764746\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.738092542, Training Accuracy: 52.160\n",
            "Worker 2, [02/04]: Training Loss: 1.598754983, Training Accuracy: 55.608\n",
            "Worker 2, [03/04]: Training Loss: 1.530116068, Training Accuracy: 57.188\n",
            "Worker 2, [04/04]: Training Loss: 1.519349833, Training Accuracy: 56.996\n",
            "Time taken for training worker 2: 0:00:44.322057\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000553\n",
            "Local Step 25: Test Loss: 1.919254536, Test Accuracy: 51.030\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.712816289, Training Accuracy: 52.728\n",
            "Worker 1, [02/04]: Training Loss: 1.582842863, Training Accuracy: 55.656\n",
            "Worker 1, [03/04]: Training Loss: 1.521281064, Training Accuracy: 57.344\n",
            "Worker 1, [04/04]: Training Loss: 1.486079219, Training Accuracy: 58.112\n",
            "Time taken for training worker 1: 0:00:43.206727\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.726664661, Training Accuracy: 52.924\n",
            "Worker 2, [02/04]: Training Loss: 1.588075712, Training Accuracy: 55.668\n",
            "Worker 2, [03/04]: Training Loss: 1.534796082, Training Accuracy: 56.968\n",
            "Worker 2, [04/04]: Training Loss: 1.491198698, Training Accuracy: 58.248\n",
            "Time taken for training worker 2: 0:00:42.785968\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000586\n",
            "Local Step 26: Test Loss: 1.963979442, Test Accuracy: 50.190\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.709011492, Training Accuracy: 53.032\n",
            "Worker 1, [02/04]: Training Loss: 1.579458117, Training Accuracy: 56.084\n",
            "Worker 1, [03/04]: Training Loss: 1.510303451, Training Accuracy: 57.372\n",
            "Worker 1, [04/04]: Training Loss: 1.486636658, Training Accuracy: 58.300\n",
            "Time taken for training worker 1: 0:00:43.511458\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.732896953, Training Accuracy: 52.436\n",
            "Worker 2, [02/04]: Training Loss: 1.590800848, Training Accuracy: 55.776\n",
            "Worker 2, [03/04]: Training Loss: 1.537164994, Training Accuracy: 56.912\n",
            "Worker 2, [04/04]: Training Loss: 1.481606446, Training Accuracy: 58.140\n",
            "Time taken for training worker 2: 0:00:39.573422\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000484\n",
            "Local Step 27: Test Loss: 1.999518040, Test Accuracy: 48.750\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.715185007, Training Accuracy: 52.988\n",
            "Worker 1, [02/04]: Training Loss: 1.570526120, Training Accuracy: 56.344\n",
            "Worker 1, [03/04]: Training Loss: 1.520587592, Training Accuracy: 57.340\n",
            "Worker 1, [04/04]: Training Loss: 1.495980633, Training Accuracy: 58.100\n",
            "Time taken for training worker 1: 0:00:41.370723\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.719495018, Training Accuracy: 52.540\n",
            "Worker 2, [02/04]: Training Loss: 1.587088098, Training Accuracy: 55.924\n",
            "Worker 2, [03/04]: Training Loss: 1.515994093, Training Accuracy: 57.256\n",
            "Worker 2, [04/04]: Training Loss: 1.491553501, Training Accuracy: 58.128\n",
            "Time taken for training worker 2: 0:00:43.474498\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000570\n",
            "Local Step 28: Test Loss: 1.974840876, Test Accuracy: 49.100\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.715711333, Training Accuracy: 52.916\n",
            "Worker 1, [02/04]: Training Loss: 1.579076908, Training Accuracy: 55.680\n",
            "Worker 1, [03/04]: Training Loss: 1.513719943, Training Accuracy: 57.692\n",
            "Worker 1, [04/04]: Training Loss: 1.487948426, Training Accuracy: 57.964\n",
            "Time taken for training worker 1: 0:00:39.899505\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.728943132, Training Accuracy: 52.628\n",
            "Worker 2, [02/04]: Training Loss: 1.577243233, Training Accuracy: 56.044\n",
            "Worker 2, [03/04]: Training Loss: 1.521403481, Training Accuracy: 57.220\n",
            "Worker 2, [04/04]: Training Loss: 1.489284218, Training Accuracy: 57.888\n",
            "Time taken for training worker 2: 0:00:42.130999\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000569\n",
            "Local Step 29: Test Loss: 2.024720843, Test Accuracy: 49.170\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.713529104, Training Accuracy: 53.220\n",
            "Worker 1, [02/04]: Training Loss: 1.570606570, Training Accuracy: 56.216\n",
            "Worker 1, [03/04]: Training Loss: 1.497901896, Training Accuracy: 58.064\n",
            "Worker 1, [04/04]: Training Loss: 1.472462072, Training Accuracy: 58.488\n",
            "Time taken for training worker 1: 0:00:43.869654\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.718856157, Training Accuracy: 52.448\n",
            "Worker 2, [02/04]: Training Loss: 1.588774224, Training Accuracy: 55.448\n",
            "Worker 2, [03/04]: Training Loss: 1.522163501, Training Accuracy: 57.452\n",
            "Worker 2, [04/04]: Training Loss: 1.498459523, Training Accuracy: 57.856\n",
            "Time taken for training worker 2: 0:00:42.174261\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000586\n",
            "Local Step 30: Test Loss: 1.956817803, Test Accuracy: 49.550\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.702564659, Training Accuracy: 52.968\n",
            "Worker 1, [02/04]: Training Loss: 1.555814085, Training Accuracy: 56.616\n",
            "Worker 1, [03/04]: Training Loss: 1.512382391, Training Accuracy: 57.784\n",
            "Worker 1, [04/04]: Training Loss: 1.475338584, Training Accuracy: 58.632\n",
            "Time taken for training worker 1: 0:00:39.959042\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.712906181, Training Accuracy: 52.976\n",
            "Worker 2, [02/04]: Training Loss: 1.593835678, Training Accuracy: 55.432\n",
            "Worker 2, [03/04]: Training Loss: 1.519914049, Training Accuracy: 57.332\n",
            "Worker 2, [04/04]: Training Loss: 1.486486630, Training Accuracy: 57.884\n",
            "Time taken for training worker 2: 0:00:40.088982\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000866\n",
            "Local Step 31: Test Loss: 1.996534659, Test Accuracy: 49.460\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.688784342, Training Accuracy: 53.580\n",
            "Worker 1, [02/04]: Training Loss: 1.565793454, Training Accuracy: 56.352\n",
            "Worker 1, [03/04]: Training Loss: 1.515849933, Training Accuracy: 57.288\n",
            "Worker 1, [04/04]: Training Loss: 1.462389791, Training Accuracy: 58.888\n",
            "Time taken for training worker 1: 0:00:40.906892\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.715255366, Training Accuracy: 52.640\n",
            "Worker 2, [02/04]: Training Loss: 1.560227241, Training Accuracy: 56.540\n",
            "Worker 2, [03/04]: Training Loss: 1.517128204, Training Accuracy: 57.260\n",
            "Worker 2, [04/04]: Training Loss: 1.467623723, Training Accuracy: 58.800\n",
            "Time taken for training worker 2: 0:00:44.662482\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000606\n",
            "Local Step 32: Test Loss: 1.954564362, Test Accuracy: 49.900\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.697402941, Training Accuracy: 53.476\n",
            "Worker 1, [02/04]: Training Loss: 1.554231995, Training Accuracy: 56.856\n",
            "Worker 1, [03/04]: Training Loss: 1.508851070, Training Accuracy: 57.512\n",
            "Worker 1, [04/04]: Training Loss: 1.473295179, Training Accuracy: 58.768\n",
            "Time taken for training worker 1: 0:00:39.455234\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.707664149, Training Accuracy: 52.888\n",
            "Worker 2, [02/04]: Training Loss: 1.563340579, Training Accuracy: 56.008\n",
            "Worker 2, [03/04]: Training Loss: 1.507884768, Training Accuracy: 57.972\n",
            "Worker 2, [04/04]: Training Loss: 1.473218084, Training Accuracy: 58.416\n",
            "Time taken for training worker 2: 0:00:41.018215\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000538\n",
            "Local Step 33: Test Loss: 1.973563703, Test Accuracy: 49.980\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.700517370, Training Accuracy: 53.300\n",
            "Worker 1, [02/04]: Training Loss: 1.536708380, Training Accuracy: 56.944\n",
            "Worker 1, [03/04]: Training Loss: 1.486924499, Training Accuracy: 58.244\n",
            "Worker 1, [04/04]: Training Loss: 1.466963014, Training Accuracy: 58.732\n",
            "Time taken for training worker 1: 0:00:41.836282\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.702410944, Training Accuracy: 53.316\n",
            "Worker 2, [02/04]: Training Loss: 1.552379121, Training Accuracy: 56.868\n",
            "Worker 2, [03/04]: Training Loss: 1.509630171, Training Accuracy: 57.232\n",
            "Worker 2, [04/04]: Training Loss: 1.478025026, Training Accuracy: 58.088\n",
            "Time taken for training worker 2: 0:00:42.242762\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000536\n",
            "Local Step 34: Test Loss: 1.966397746, Test Accuracy: 49.590\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.705011807, Training Accuracy: 53.176\n",
            "Worker 1, [02/04]: Training Loss: 1.551082253, Training Accuracy: 56.612\n",
            "Worker 1, [03/04]: Training Loss: 1.492225199, Training Accuracy: 58.244\n",
            "Worker 1, [04/04]: Training Loss: 1.462436819, Training Accuracy: 59.140\n",
            "Time taken for training worker 1: 0:00:44.275564\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.697814673, Training Accuracy: 53.416\n",
            "Worker 2, [02/04]: Training Loss: 1.563227142, Training Accuracy: 56.292\n",
            "Worker 2, [03/04]: Training Loss: 1.503596348, Training Accuracy: 57.672\n",
            "Worker 2, [04/04]: Training Loss: 1.466805199, Training Accuracy: 58.696\n",
            "Time taken for training worker 2: 0:00:43.596817\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000547\n",
            "Local Step 35: Test Loss: 1.961126883, Test Accuracy: 49.930\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.677905060, Training Accuracy: 53.316\n",
            "Worker 1, [02/04]: Training Loss: 1.547149316, Training Accuracy: 57.176\n",
            "Worker 1, [03/04]: Training Loss: 1.484030803, Training Accuracy: 58.232\n",
            "Worker 1, [04/04]: Training Loss: 1.452396661, Training Accuracy: 59.024\n",
            "Time taken for training worker 1: 0:00:38.689153\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.686170356, Training Accuracy: 53.496\n",
            "Worker 2, [02/04]: Training Loss: 1.561072369, Training Accuracy: 56.484\n",
            "Worker 2, [03/04]: Training Loss: 1.485503653, Training Accuracy: 58.104\n",
            "Worker 2, [04/04]: Training Loss: 1.456303857, Training Accuracy: 59.164\n",
            "Time taken for training worker 2: 0:00:41.388572\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000553\n",
            "Local Step 36: Test Loss: 1.967229666, Test Accuracy: 50.700\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 1.673130086, Training Accuracy: 54.260\n",
            "Worker 1, [02/04]: Training Loss: 1.530446126, Training Accuracy: 57.332\n",
            "Worker 1, [03/04]: Training Loss: 1.482892808, Training Accuracy: 58.672\n",
            "Worker 1, [04/04]: Training Loss: 1.441414894, Training Accuracy: 59.476\n",
            "Time taken for training worker 1: 0:00:43.603182\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 1.701890183, Training Accuracy: 53.452\n",
            "Worker 2, [02/04]: Training Loss: 1.534902188, Training Accuracy: 56.948\n",
            "Worker 2, [03/04]: Training Loss: 1.510098217, Training Accuracy: 57.284\n",
            "Worker 2, [04/04]: Training Loss: 1.456605009, Training Accuracy: 59.220\n",
            "Time taken for training worker 2: 0:00:40.500530\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000511\n",
            "Local Step 37: Test Loss: 1.991730550, Test Accuracy: 49.990\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:54:37.593905\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:8\n",
            "==================================================\n",
            "Worker 1, [01/08]: Training Loss: 4.321150297, Training Accuracy: 4.216\n",
            "Worker 1, [02/08]: Training Loss: 3.850044866, Training Accuracy: 10.244\n",
            "Worker 1, [03/08]: Training Loss: 3.573158594, Training Accuracy: 14.724\n",
            "Worker 1, [04/08]: Training Loss: 3.353283227, Training Accuracy: 18.596\n",
            "Worker 1, [05/08]: Training Loss: 3.192374020, Training Accuracy: 21.764\n",
            "Worker 1, [06/08]: Training Loss: 3.050389561, Training Accuracy: 24.048\n",
            "Worker 1, [07/08]: Training Loss: 2.905654280, Training Accuracy: 26.896\n",
            "Worker 1, [08/08]: Training Loss: 2.816139055, Training Accuracy: 28.832\n",
            "Time taken for training worker 1: 0:01:28.869135\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 4.319813446, Training Accuracy: 4.248\n",
            "Worker 2, [02/08]: Training Loss: 3.848780628, Training Accuracy: 10.108\n",
            "Worker 2, [03/08]: Training Loss: 3.579368954, Training Accuracy: 14.776\n",
            "Worker 2, [04/08]: Training Loss: 3.369569044, Training Accuracy: 18.576\n",
            "Worker 2, [05/08]: Training Loss: 3.198268572, Training Accuracy: 21.500\n",
            "Worker 2, [06/08]: Training Loss: 3.046164822, Training Accuracy: 24.188\n",
            "Worker 2, [07/08]: Training Loss: 2.911638416, Training Accuracy: 26.876\n",
            "Worker 2, [08/08]: Training Loss: 2.816306855, Training Accuracy: 28.456\n",
            "Time taken for training worker 2: 0:01:26.419692\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000899\n",
            "Local Step 01: Test Loss: 2.909851094, Test Accuracy: 30.790\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.846319444, Training Accuracy: 27.952\n",
            "Worker 1, [02/08]: Training Loss: 2.719888255, Training Accuracy: 30.464\n",
            "Worker 1, [03/08]: Training Loss: 2.639038994, Training Accuracy: 32.012\n",
            "Worker 1, [04/08]: Training Loss: 2.543691524, Training Accuracy: 34.292\n",
            "Worker 1, [05/08]: Training Loss: 2.505005423, Training Accuracy: 35.092\n",
            "Worker 1, [06/08]: Training Loss: 2.441065327, Training Accuracy: 36.180\n",
            "Worker 1, [07/08]: Training Loss: 2.372511945, Training Accuracy: 37.556\n",
            "Worker 1, [08/08]: Training Loss: 2.313538586, Training Accuracy: 38.604\n",
            "Time taken for training worker 1: 0:01:20.015214\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.580487263, Training Accuracy: 34.276\n",
            "Worker 2, [02/08]: Training Loss: 2.454430810, Training Accuracy: 36.644\n",
            "Worker 2, [03/08]: Training Loss: 2.369897203, Training Accuracy: 38.252\n",
            "Worker 2, [04/08]: Training Loss: 2.301538235, Training Accuracy: 39.616\n",
            "Worker 2, [05/08]: Training Loss: 2.254355774, Training Accuracy: 40.980\n",
            "Worker 2, [06/08]: Training Loss: 2.184023615, Training Accuracy: 42.140\n",
            "Worker 2, [07/08]: Training Loss: 2.151667979, Training Accuracy: 42.684\n",
            "Worker 2, [08/08]: Training Loss: 2.104463479, Training Accuracy: 43.688\n",
            "Time taken for training worker 2: 0:01:21.498477\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000504\n",
            "Local Step 02: Test Loss: 2.228719641, Test Accuracy: 42.440\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.327804109, Training Accuracy: 39.568\n",
            "Worker 1, [02/08]: Training Loss: 2.188304919, Training Accuracy: 42.540\n",
            "Worker 1, [03/08]: Training Loss: 2.125002764, Training Accuracy: 43.588\n",
            "Worker 1, [04/08]: Training Loss: 2.067514190, Training Accuracy: 44.408\n",
            "Worker 1, [05/08]: Training Loss: 2.025741714, Training Accuracy: 45.216\n",
            "Worker 1, [06/08]: Training Loss: 1.988849011, Training Accuracy: 46.572\n",
            "Worker 1, [07/08]: Training Loss: 1.949311142, Training Accuracy: 46.932\n",
            "Worker 1, [08/08]: Training Loss: 1.924072688, Training Accuracy: 47.620\n",
            "Time taken for training worker 1: 0:01:23.277123\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.254932088, Training Accuracy: 41.120\n",
            "Worker 2, [02/08]: Training Loss: 2.103236800, Training Accuracy: 44.272\n",
            "Worker 2, [03/08]: Training Loss: 2.029530436, Training Accuracy: 45.616\n",
            "Worker 2, [04/08]: Training Loss: 1.991325212, Training Accuracy: 46.128\n",
            "Worker 2, [05/08]: Training Loss: 1.937670054, Training Accuracy: 47.184\n",
            "Worker 2, [06/08]: Training Loss: 1.916023005, Training Accuracy: 47.860\n",
            "Worker 2, [07/08]: Training Loss: 1.862608753, Training Accuracy: 49.248\n",
            "Worker 2, [08/08]: Training Loss: 1.838363775, Training Accuracy: 49.968\n",
            "Time taken for training worker 2: 0:01:20.376331\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000893\n",
            "Local Step 03: Test Loss: 2.096003185, Test Accuracy: 45.100\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.142708001, Training Accuracy: 43.576\n",
            "Worker 1, [02/08]: Training Loss: 1.984330830, Training Accuracy: 46.816\n",
            "Worker 1, [03/08]: Training Loss: 1.923356688, Training Accuracy: 47.856\n",
            "Worker 1, [04/08]: Training Loss: 1.875968012, Training Accuracy: 48.816\n",
            "Worker 1, [05/08]: Training Loss: 1.823139739, Training Accuracy: 50.288\n",
            "Worker 1, [06/08]: Training Loss: 1.785601687, Training Accuracy: 50.600\n",
            "Worker 1, [07/08]: Training Loss: 1.758618837, Training Accuracy: 51.372\n",
            "Worker 1, [08/08]: Training Loss: 1.734664262, Training Accuracy: 51.920\n",
            "Time taken for training worker 1: 0:01:22.556291\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.110991966, Training Accuracy: 44.412\n",
            "Worker 2, [02/08]: Training Loss: 1.942132971, Training Accuracy: 47.432\n",
            "Worker 2, [03/08]: Training Loss: 1.892815182, Training Accuracy: 48.724\n",
            "Worker 2, [04/08]: Training Loss: 1.830128381, Training Accuracy: 49.832\n",
            "Worker 2, [05/08]: Training Loss: 1.804870785, Training Accuracy: 50.692\n",
            "Worker 2, [06/08]: Training Loss: 1.754963385, Training Accuracy: 51.632\n",
            "Worker 2, [07/08]: Training Loss: 1.731434776, Training Accuracy: 52.408\n",
            "Worker 2, [08/08]: Training Loss: 1.720783464, Training Accuracy: 52.640\n",
            "Time taken for training worker 2: 0:01:23.709005\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000489\n",
            "Local Step 04: Test Loss: 2.096809702, Test Accuracy: 46.530\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 2.025253243, Training Accuracy: 45.976\n",
            "Worker 1, [02/08]: Training Loss: 1.888684245, Training Accuracy: 48.856\n",
            "Worker 1, [03/08]: Training Loss: 1.813773085, Training Accuracy: 50.732\n",
            "Worker 1, [04/08]: Training Loss: 1.762219967, Training Accuracy: 51.608\n",
            "Worker 1, [05/08]: Training Loss: 1.723288679, Training Accuracy: 52.372\n",
            "Worker 1, [06/08]: Training Loss: 1.687777030, Training Accuracy: 53.212\n",
            "Worker 1, [07/08]: Training Loss: 1.675188117, Training Accuracy: 53.856\n",
            "Worker 1, [08/08]: Training Loss: 1.637664790, Training Accuracy: 54.488\n",
            "Time taken for training worker 1: 0:01:21.291663\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 2.036825657, Training Accuracy: 45.872\n",
            "Worker 2, [02/08]: Training Loss: 1.867008803, Training Accuracy: 49.608\n",
            "Worker 2, [03/08]: Training Loss: 1.805782825, Training Accuracy: 50.948\n",
            "Worker 2, [04/08]: Training Loss: 1.734873579, Training Accuracy: 52.220\n",
            "Worker 2, [05/08]: Training Loss: 1.697818143, Training Accuracy: 53.600\n",
            "Worker 2, [06/08]: Training Loss: 1.695926247, Training Accuracy: 53.012\n",
            "Worker 2, [07/08]: Training Loss: 1.649920779, Training Accuracy: 53.972\n",
            "Worker 2, [08/08]: Training Loss: 1.626314458, Training Accuracy: 54.544\n",
            "Time taken for training worker 2: 0:01:24.062451\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000533\n",
            "Local Step 05: Test Loss: 2.083454275, Test Accuracy: 46.420\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.989084595, Training Accuracy: 47.016\n",
            "Worker 1, [02/08]: Training Loss: 1.806277899, Training Accuracy: 50.672\n",
            "Worker 1, [03/08]: Training Loss: 1.737331918, Training Accuracy: 52.140\n",
            "Worker 1, [04/08]: Training Loss: 1.697558570, Training Accuracy: 53.068\n",
            "Worker 1, [05/08]: Training Loss: 1.650656847, Training Accuracy: 54.076\n",
            "Worker 1, [06/08]: Training Loss: 1.635013480, Training Accuracy: 54.724\n",
            "Worker 1, [07/08]: Training Loss: 1.602095501, Training Accuracy: 55.176\n",
            "Worker 1, [08/08]: Training Loss: 1.589569250, Training Accuracy: 55.612\n",
            "Time taken for training worker 1: 0:01:24.296772\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.992031565, Training Accuracy: 47.036\n",
            "Worker 2, [02/08]: Training Loss: 1.813293331, Training Accuracy: 50.800\n",
            "Worker 2, [03/08]: Training Loss: 1.745384138, Training Accuracy: 52.068\n",
            "Worker 2, [04/08]: Training Loss: 1.689325746, Training Accuracy: 53.552\n",
            "Worker 2, [05/08]: Training Loss: 1.644120352, Training Accuracy: 54.708\n",
            "Worker 2, [06/08]: Training Loss: 1.609927672, Training Accuracy: 55.272\n",
            "Worker 2, [07/08]: Training Loss: 1.602919116, Training Accuracy: 55.652\n",
            "Worker 2, [08/08]: Training Loss: 1.582995919, Training Accuracy: 55.828\n",
            "Time taken for training worker 2: 0:01:25.054185\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000564\n",
            "Local Step 06: Test Loss: 2.049395816, Test Accuracy: 47.330\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.944545113, Training Accuracy: 48.036\n",
            "Worker 1, [02/08]: Training Loss: 1.787417168, Training Accuracy: 51.260\n",
            "Worker 1, [03/08]: Training Loss: 1.706206812, Training Accuracy: 52.872\n",
            "Worker 1, [04/08]: Training Loss: 1.651569158, Training Accuracy: 54.304\n",
            "Worker 1, [05/08]: Training Loss: 1.617148695, Training Accuracy: 54.736\n",
            "Worker 1, [06/08]: Training Loss: 1.601347211, Training Accuracy: 55.412\n",
            "Worker 1, [07/08]: Training Loss: 1.565675609, Training Accuracy: 55.856\n",
            "Worker 1, [08/08]: Training Loss: 1.547603409, Training Accuracy: 56.444\n",
            "Time taken for training worker 1: 0:01:26.793625\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.948580675, Training Accuracy: 47.804\n",
            "Worker 2, [02/08]: Training Loss: 1.781740261, Training Accuracy: 51.080\n",
            "Worker 2, [03/08]: Training Loss: 1.701338524, Training Accuracy: 53.548\n",
            "Worker 2, [04/08]: Training Loss: 1.645371829, Training Accuracy: 53.872\n",
            "Worker 2, [05/08]: Training Loss: 1.626802213, Training Accuracy: 55.036\n",
            "Worker 2, [06/08]: Training Loss: 1.595194956, Training Accuracy: 55.332\n",
            "Worker 2, [07/08]: Training Loss: 1.580255954, Training Accuracy: 55.956\n",
            "Worker 2, [08/08]: Training Loss: 1.547993363, Training Accuracy: 56.892\n",
            "Time taken for training worker 2: 0:01:20.310744\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000482\n",
            "Local Step 07: Test Loss: 2.083878100, Test Accuracy: 47.250\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.917016875, Training Accuracy: 48.724\n",
            "Worker 1, [02/08]: Training Loss: 1.742786307, Training Accuracy: 52.612\n",
            "Worker 1, [03/08]: Training Loss: 1.691905466, Training Accuracy: 53.324\n",
            "Worker 1, [04/08]: Training Loss: 1.603385475, Training Accuracy: 55.676\n",
            "Worker 1, [05/08]: Training Loss: 1.579215416, Training Accuracy: 55.888\n",
            "Worker 1, [06/08]: Training Loss: 1.535403627, Training Accuracy: 57.200\n",
            "Worker 1, [07/08]: Training Loss: 1.532468112, Training Accuracy: 56.856\n",
            "Worker 1, [08/08]: Training Loss: 1.512313288, Training Accuracy: 57.240\n",
            "Time taken for training worker 1: 0:01:25.505727\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.915187672, Training Accuracy: 48.444\n",
            "Worker 2, [02/08]: Training Loss: 1.749787218, Training Accuracy: 52.116\n",
            "Worker 2, [03/08]: Training Loss: 1.669506361, Training Accuracy: 53.884\n",
            "Worker 2, [04/08]: Training Loss: 1.627525313, Training Accuracy: 55.216\n",
            "Worker 2, [05/08]: Training Loss: 1.599059853, Training Accuracy: 55.344\n",
            "Worker 2, [06/08]: Training Loss: 1.572177624, Training Accuracy: 55.852\n",
            "Worker 2, [07/08]: Training Loss: 1.535429375, Training Accuracy: 56.964\n",
            "Worker 2, [08/08]: Training Loss: 1.516608693, Training Accuracy: 57.612\n",
            "Time taken for training worker 2: 0:01:19.640407\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000578\n",
            "Local Step 08: Test Loss: 2.057928858, Test Accuracy: 48.200\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.883557457, Training Accuracy: 49.384\n",
            "Worker 1, [02/08]: Training Loss: 1.725204826, Training Accuracy: 52.872\n",
            "Worker 1, [03/08]: Training Loss: 1.644303346, Training Accuracy: 54.708\n",
            "Worker 1, [04/08]: Training Loss: 1.593495669, Training Accuracy: 55.444\n",
            "Worker 1, [05/08]: Training Loss: 1.555458755, Training Accuracy: 56.648\n",
            "Worker 1, [06/08]: Training Loss: 1.511986384, Training Accuracy: 57.372\n",
            "Worker 1, [07/08]: Training Loss: 1.519168489, Training Accuracy: 57.368\n",
            "Worker 1, [08/08]: Training Loss: 1.510482092, Training Accuracy: 57.592\n",
            "Time taken for training worker 1: 0:01:24.386071\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.905661845, Training Accuracy: 48.488\n",
            "Worker 2, [02/08]: Training Loss: 1.717982753, Training Accuracy: 52.904\n",
            "Worker 2, [03/08]: Training Loss: 1.652690880, Training Accuracy: 54.412\n",
            "Worker 2, [04/08]: Training Loss: 1.610972536, Training Accuracy: 55.520\n",
            "Worker 2, [05/08]: Training Loss: 1.560279610, Training Accuracy: 56.336\n",
            "Worker 2, [06/08]: Training Loss: 1.564972938, Training Accuracy: 55.964\n",
            "Worker 2, [07/08]: Training Loss: 1.522612614, Training Accuracy: 57.304\n",
            "Worker 2, [08/08]: Training Loss: 1.519334161, Training Accuracy: 57.204\n",
            "Time taken for training worker 2: 0:01:20.728913\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000543\n",
            "Local Step 09: Test Loss: 2.075557754, Test Accuracy: 47.360\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.894539336, Training Accuracy: 49.760\n",
            "Worker 1, [02/08]: Training Loss: 1.709565251, Training Accuracy: 53.104\n",
            "Worker 1, [03/08]: Training Loss: 1.624132362, Training Accuracy: 54.880\n",
            "Worker 1, [04/08]: Training Loss: 1.578437892, Training Accuracy: 56.016\n",
            "Worker 1, [05/08]: Training Loss: 1.538831010, Training Accuracy: 56.708\n",
            "Worker 1, [06/08]: Training Loss: 1.508513566, Training Accuracy: 57.568\n",
            "Worker 1, [07/08]: Training Loss: 1.496546276, Training Accuracy: 57.664\n",
            "Worker 1, [08/08]: Training Loss: 1.469678724, Training Accuracy: 58.304\n",
            "Time taken for training worker 1: 0:01:24.003468\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.887078572, Training Accuracy: 49.324\n",
            "Worker 2, [02/08]: Training Loss: 1.711853847, Training Accuracy: 52.992\n",
            "Worker 2, [03/08]: Training Loss: 1.638359164, Training Accuracy: 54.892\n",
            "Worker 2, [04/08]: Training Loss: 1.590464657, Training Accuracy: 55.476\n",
            "Worker 2, [05/08]: Training Loss: 1.549777242, Training Accuracy: 56.344\n",
            "Worker 2, [06/08]: Training Loss: 1.518750393, Training Accuracy: 56.928\n",
            "Worker 2, [07/08]: Training Loss: 1.507625948, Training Accuracy: 57.896\n",
            "Worker 2, [08/08]: Training Loss: 1.491988146, Training Accuracy: 57.896\n",
            "Time taken for training worker 2: 0:01:22.844085\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000558\n",
            "Local Step 10: Test Loss: 2.037734294, Test Accuracy: 48.100\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.879024337, Training Accuracy: 49.328\n",
            "Worker 1, [02/08]: Training Loss: 1.684445340, Training Accuracy: 53.924\n",
            "Worker 1, [03/08]: Training Loss: 1.605654732, Training Accuracy: 55.620\n",
            "Worker 1, [04/08]: Training Loss: 1.551136971, Training Accuracy: 56.392\n",
            "Worker 1, [05/08]: Training Loss: 1.540953211, Training Accuracy: 56.708\n",
            "Worker 1, [06/08]: Training Loss: 1.489101835, Training Accuracy: 58.284\n",
            "Worker 1, [07/08]: Training Loss: 1.476679825, Training Accuracy: 58.420\n",
            "Worker 1, [08/08]: Training Loss: 1.457274907, Training Accuracy: 59.172\n",
            "Time taken for training worker 1: 0:01:40.173955\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.875142820, Training Accuracy: 49.792\n",
            "Worker 2, [02/08]: Training Loss: 1.703365873, Training Accuracy: 53.236\n",
            "Worker 2, [03/08]: Training Loss: 1.630375695, Training Accuracy: 54.912\n",
            "Worker 2, [04/08]: Training Loss: 1.569668641, Training Accuracy: 56.144\n",
            "Worker 2, [05/08]: Training Loss: 1.521832097, Training Accuracy: 57.080\n",
            "Worker 2, [06/08]: Training Loss: 1.518733955, Training Accuracy: 57.660\n",
            "Worker 2, [07/08]: Training Loss: 1.483537241, Training Accuracy: 58.048\n",
            "Worker 2, [08/08]: Training Loss: 1.465624095, Training Accuracy: 58.440\n",
            "Time taken for training worker 2: 0:01:53.969275\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000816\n",
            "Local Step 11: Test Loss: 2.008356640, Test Accuracy: 49.030\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.875055435, Training Accuracy: 49.380\n",
            "Worker 1, [02/08]: Training Loss: 1.686562620, Training Accuracy: 53.688\n",
            "Worker 1, [03/08]: Training Loss: 1.605143839, Training Accuracy: 55.108\n",
            "Worker 1, [04/08]: Training Loss: 1.548959836, Training Accuracy: 56.432\n",
            "Worker 1, [05/08]: Training Loss: 1.519055394, Training Accuracy: 57.408\n",
            "Worker 1, [06/08]: Training Loss: 1.502906315, Training Accuracy: 58.028\n",
            "Worker 1, [07/08]: Training Loss: 1.461385089, Training Accuracy: 58.924\n",
            "Worker 1, [08/08]: Training Loss: 1.463598505, Training Accuracy: 58.828\n",
            "Time taken for training worker 1: 0:01:58.652529\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.886566165, Training Accuracy: 49.300\n",
            "Worker 2, [02/08]: Training Loss: 1.696560706, Training Accuracy: 53.532\n",
            "Worker 2, [03/08]: Training Loss: 1.613955276, Training Accuracy: 55.320\n",
            "Worker 2, [04/08]: Training Loss: 1.552222206, Training Accuracy: 56.716\n",
            "Worker 2, [05/08]: Training Loss: 1.511698764, Training Accuracy: 57.732\n",
            "Worker 2, [06/08]: Training Loss: 1.493410635, Training Accuracy: 58.260\n",
            "Worker 2, [07/08]: Training Loss: 1.465616249, Training Accuracy: 58.624\n",
            "Worker 2, [08/08]: Training Loss: 1.461652217, Training Accuracy: 58.908\n",
            "Time taken for training worker 2: 0:01:50.901009\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000904\n",
            "Local Step 12: Test Loss: 2.051410501, Test Accuracy: 48.630\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.861721006, Training Accuracy: 49.644\n",
            "Worker 1, [02/08]: Training Loss: 1.666941744, Training Accuracy: 54.128\n",
            "Worker 1, [03/08]: Training Loss: 1.565212904, Training Accuracy: 56.476\n",
            "Worker 1, [04/08]: Training Loss: 1.525573341, Training Accuracy: 57.396\n",
            "Worker 1, [05/08]: Training Loss: 1.508686475, Training Accuracy: 57.692\n",
            "Worker 1, [06/08]: Training Loss: 1.464545901, Training Accuracy: 58.676\n",
            "Worker 1, [07/08]: Training Loss: 1.472469987, Training Accuracy: 58.220\n",
            "Worker 1, [08/08]: Training Loss: 1.452286923, Training Accuracy: 58.904\n",
            "Time taken for training worker 1: 0:01:49.554801\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.869266219, Training Accuracy: 50.052\n",
            "Worker 2, [02/08]: Training Loss: 1.667191889, Training Accuracy: 53.504\n",
            "Worker 2, [03/08]: Training Loss: 1.585137637, Training Accuracy: 56.104\n",
            "Worker 2, [04/08]: Training Loss: 1.551513800, Training Accuracy: 56.524\n",
            "Worker 2, [05/08]: Training Loss: 1.519992023, Training Accuracy: 57.828\n",
            "Worker 2, [06/08]: Training Loss: 1.495699620, Training Accuracy: 57.644\n",
            "Worker 2, [07/08]: Training Loss: 1.469495508, Training Accuracy: 58.580\n",
            "Worker 2, [08/08]: Training Loss: 1.458967990, Training Accuracy: 59.212\n",
            "Time taken for training worker 2: 0:01:42.401661\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000676\n",
            "Local Step 13: Test Loss: 2.046189543, Test Accuracy: 49.020\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.825284487, Training Accuracy: 50.568\n",
            "Worker 1, [02/08]: Training Loss: 1.660467162, Training Accuracy: 54.608\n",
            "Worker 1, [03/08]: Training Loss: 1.593337569, Training Accuracy: 55.952\n",
            "Worker 1, [04/08]: Training Loss: 1.533949996, Training Accuracy: 56.812\n",
            "Worker 1, [05/08]: Training Loss: 1.500765550, Training Accuracy: 57.760\n",
            "Worker 1, [06/08]: Training Loss: 1.461570846, Training Accuracy: 58.960\n",
            "Worker 1, [07/08]: Training Loss: 1.441341819, Training Accuracy: 58.900\n",
            "Worker 1, [08/08]: Training Loss: 1.428041271, Training Accuracy: 59.520\n",
            "Time taken for training worker 1: 0:01:39.829048\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.869399888, Training Accuracy: 49.512\n",
            "Worker 2, [02/08]: Training Loss: 1.684455684, Training Accuracy: 53.488\n",
            "Worker 2, [03/08]: Training Loss: 1.582089164, Training Accuracy: 55.916\n",
            "Worker 2, [04/08]: Training Loss: 1.538437411, Training Accuracy: 57.080\n",
            "Worker 2, [05/08]: Training Loss: 1.533330546, Training Accuracy: 56.972\n",
            "Worker 2, [06/08]: Training Loss: 1.467269732, Training Accuracy: 58.632\n",
            "Worker 2, [07/08]: Training Loss: 1.466268355, Training Accuracy: 58.348\n",
            "Worker 2, [08/08]: Training Loss: 1.451446360, Training Accuracy: 59.344\n",
            "Time taken for training worker 2: 0:01:39.423702\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000634\n",
            "Local Step 14: Test Loss: 2.058708252, Test Accuracy: 48.780\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.848876636, Training Accuracy: 50.272\n",
            "Worker 1, [02/08]: Training Loss: 1.651353283, Training Accuracy: 54.392\n",
            "Worker 1, [03/08]: Training Loss: 1.574775322, Training Accuracy: 56.624\n",
            "Worker 1, [04/08]: Training Loss: 1.525868881, Training Accuracy: 56.988\n",
            "Worker 1, [05/08]: Training Loss: 1.493107493, Training Accuracy: 58.080\n",
            "Worker 1, [06/08]: Training Loss: 1.473864214, Training Accuracy: 58.316\n",
            "Worker 1, [07/08]: Training Loss: 1.430954069, Training Accuracy: 59.700\n",
            "Worker 1, [08/08]: Training Loss: 1.414879476, Training Accuracy: 60.024\n",
            "Time taken for training worker 1: 0:01:45.524369\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.845399278, Training Accuracy: 50.284\n",
            "Worker 2, [02/08]: Training Loss: 1.654932079, Training Accuracy: 54.516\n",
            "Worker 2, [03/08]: Training Loss: 1.579230093, Training Accuracy: 56.392\n",
            "Worker 2, [04/08]: Training Loss: 1.543165727, Training Accuracy: 56.952\n",
            "Worker 2, [05/08]: Training Loss: 1.500873780, Training Accuracy: 57.860\n",
            "Worker 2, [06/08]: Training Loss: 1.461381082, Training Accuracy: 58.652\n",
            "Worker 2, [07/08]: Training Loss: 1.440216246, Training Accuracy: 59.088\n",
            "Worker 2, [08/08]: Training Loss: 1.430982347, Training Accuracy: 59.448\n",
            "Time taken for training worker 2: 0:01:45.257062\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000551\n",
            "Local Step 15: Test Loss: 2.090166428, Test Accuracy: 48.120\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.824515633, Training Accuracy: 51.144\n",
            "Worker 1, [02/08]: Training Loss: 1.641531917, Training Accuracy: 54.756\n",
            "Worker 1, [03/08]: Training Loss: 1.579323261, Training Accuracy: 55.912\n",
            "Worker 1, [04/08]: Training Loss: 1.505961003, Training Accuracy: 57.548\n",
            "Worker 1, [05/08]: Training Loss: 1.500617950, Training Accuracy: 57.644\n",
            "Worker 1, [06/08]: Training Loss: 1.462840907, Training Accuracy: 58.704\n",
            "Worker 1, [07/08]: Training Loss: 1.443359539, Training Accuracy: 59.268\n",
            "Worker 1, [08/08]: Training Loss: 1.412883865, Training Accuracy: 59.544\n",
            "Time taken for training worker 1: 0:01:55.113373\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.865655220, Training Accuracy: 49.692\n",
            "Worker 2, [02/08]: Training Loss: 1.655984717, Training Accuracy: 54.672\n",
            "Worker 2, [03/08]: Training Loss: 1.576270750, Training Accuracy: 56.076\n",
            "Worker 2, [04/08]: Training Loss: 1.528075276, Training Accuracy: 57.056\n",
            "Worker 2, [05/08]: Training Loss: 1.489732399, Training Accuracy: 58.032\n",
            "Worker 2, [06/08]: Training Loss: 1.450052310, Training Accuracy: 59.008\n",
            "Worker 2, [07/08]: Training Loss: 1.445388016, Training Accuracy: 59.148\n",
            "Worker 2, [08/08]: Training Loss: 1.417567103, Training Accuracy: 59.696\n",
            "Time taken for training worker 2: 0:01:49.231456\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000952\n",
            "Local Step 16: Test Loss: 2.060243610, Test Accuracy: 49.070\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.834065457, Training Accuracy: 50.544\n",
            "Worker 1, [02/08]: Training Loss: 1.647197476, Training Accuracy: 54.356\n",
            "Worker 1, [03/08]: Training Loss: 1.577335392, Training Accuracy: 56.292\n",
            "Worker 1, [04/08]: Training Loss: 1.523462290, Training Accuracy: 57.256\n",
            "Worker 1, [05/08]: Training Loss: 1.459778947, Training Accuracy: 58.904\n",
            "Worker 1, [06/08]: Training Loss: 1.444814271, Training Accuracy: 59.244\n",
            "Worker 1, [07/08]: Training Loss: 1.431927806, Training Accuracy: 59.524\n",
            "Worker 1, [08/08]: Training Loss: 1.415048398, Training Accuracy: 59.764\n",
            "Time taken for training worker 1: 0:01:38.967725\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.827801972, Training Accuracy: 50.292\n",
            "Worker 2, [02/08]: Training Loss: 1.651749629, Training Accuracy: 54.544\n",
            "Worker 2, [03/08]: Training Loss: 1.551704042, Training Accuracy: 56.964\n",
            "Worker 2, [04/08]: Training Loss: 1.528914867, Training Accuracy: 57.396\n",
            "Worker 2, [05/08]: Training Loss: 1.495034830, Training Accuracy: 58.120\n",
            "Worker 2, [06/08]: Training Loss: 1.475483684, Training Accuracy: 58.544\n",
            "Worker 2, [07/08]: Training Loss: 1.435152693, Training Accuracy: 59.588\n",
            "Worker 2, [08/08]: Training Loss: 1.407538785, Training Accuracy: 60.152\n",
            "Time taken for training worker 2: 0:01:34.678155\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000537\n",
            "Local Step 17: Test Loss: 2.054128560, Test Accuracy: 48.850\n",
            "**************************************************\n",
            "Worker 1, [01/08]: Training Loss: 1.832082416, Training Accuracy: 50.920\n",
            "Worker 1, [02/08]: Training Loss: 1.631622703, Training Accuracy: 55.004\n",
            "Worker 1, [03/08]: Training Loss: 1.538417065, Training Accuracy: 57.024\n",
            "Worker 1, [04/08]: Training Loss: 1.509830115, Training Accuracy: 57.440\n",
            "Worker 1, [05/08]: Training Loss: 1.468233539, Training Accuracy: 58.820\n",
            "Worker 1, [06/08]: Training Loss: 1.441840227, Training Accuracy: 59.188\n",
            "Worker 1, [07/08]: Training Loss: 1.418369117, Training Accuracy: 59.600\n",
            "Worker 1, [08/08]: Training Loss: 1.401039299, Training Accuracy: 60.100\n",
            "Time taken for training worker 1: 0:01:34.880921\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/08]: Training Loss: 1.841374192, Training Accuracy: 50.420\n",
            "Worker 2, [02/08]: Training Loss: 1.668516193, Training Accuracy: 54.060\n",
            "Worker 2, [03/08]: Training Loss: 1.556448835, Training Accuracy: 56.440\n",
            "Worker 2, [04/08]: Training Loss: 1.508319685, Training Accuracy: 57.876\n",
            "Worker 2, [05/08]: Training Loss: 1.487374592, Training Accuracy: 57.912\n",
            "Worker 2, [06/08]: Training Loss: 1.451399683, Training Accuracy: 58.832\n",
            "Worker 2, [07/08]: Training Loss: 1.436916910, Training Accuracy: 59.064\n",
            "Worker 2, [08/08]: Training Loss: 1.398900757, Training Accuracy: 60.268\n",
            "Time taken for training worker 2: 0:01:35.461007\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000779\n",
            "Local Step 18: Test Loss: 2.074545109, Test Accuracy: 47.750\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:55:56.953039\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:16\n",
            "==================================================\n",
            "Worker 1, [01/16]: Training Loss: 4.317408604, Training Accuracy: 4.440\n",
            "Worker 1, [02/16]: Training Loss: 3.841739101, Training Accuracy: 10.212\n",
            "Worker 1, [03/16]: Training Loss: 3.573045145, Training Accuracy: 14.520\n",
            "Worker 1, [04/16]: Training Loss: 3.359579337, Training Accuracy: 18.476\n",
            "Worker 1, [05/16]: Training Loss: 3.195699444, Training Accuracy: 21.180\n",
            "Worker 1, [06/16]: Training Loss: 3.053601340, Training Accuracy: 24.076\n",
            "Worker 1, [07/16]: Training Loss: 2.934877503, Training Accuracy: 26.284\n",
            "Worker 1, [08/16]: Training Loss: 2.833563518, Training Accuracy: 28.324\n",
            "Worker 1, [09/16]: Training Loss: 2.723183029, Training Accuracy: 30.452\n",
            "Worker 1, [10/16]: Training Loss: 2.643069934, Training Accuracy: 32.248\n",
            "Worker 1, [11/16]: Training Loss: 2.567105292, Training Accuracy: 33.612\n",
            "Worker 1, [12/16]: Training Loss: 2.499018694, Training Accuracy: 35.044\n",
            "Worker 1, [13/16]: Training Loss: 2.453096934, Training Accuracy: 35.896\n",
            "Worker 1, [14/16]: Training Loss: 2.390333468, Training Accuracy: 37.248\n",
            "Worker 1, [15/16]: Training Loss: 2.348592024, Training Accuracy: 38.208\n",
            "Worker 1, [16/16]: Training Loss: 2.287045447, Training Accuracy: 39.612\n",
            "Time taken for training worker 1: 0:03:15.308404\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 4.324255824, Training Accuracy: 4.360\n",
            "Worker 2, [02/16]: Training Loss: 3.845024888, Training Accuracy: 10.420\n",
            "Worker 2, [03/16]: Training Loss: 3.596502325, Training Accuracy: 14.404\n",
            "Worker 2, [04/16]: Training Loss: 3.383440673, Training Accuracy: 18.088\n",
            "Worker 2, [05/16]: Training Loss: 3.212736439, Training Accuracy: 21.184\n",
            "Worker 2, [06/16]: Training Loss: 3.057738129, Training Accuracy: 23.916\n",
            "Worker 2, [07/16]: Training Loss: 2.911557115, Training Accuracy: 26.808\n",
            "Worker 2, [08/16]: Training Loss: 2.804549696, Training Accuracy: 28.568\n",
            "Worker 2, [09/16]: Training Loss: 2.714062791, Training Accuracy: 30.548\n",
            "Worker 2, [10/16]: Training Loss: 2.636026777, Training Accuracy: 32.092\n",
            "Worker 2, [11/16]: Training Loss: 2.573759224, Training Accuracy: 33.076\n",
            "Worker 2, [12/16]: Training Loss: 2.523554782, Training Accuracy: 34.688\n",
            "Worker 2, [13/16]: Training Loss: 2.448013204, Training Accuracy: 36.072\n",
            "Worker 2, [14/16]: Training Loss: 2.400039372, Training Accuracy: 36.972\n",
            "Worker 2, [15/16]: Training Loss: 2.332153723, Training Accuracy: 38.768\n",
            "Worker 2, [16/16]: Training Loss: 2.302322486, Training Accuracy: 39.116\n",
            "Time taken for training worker 2: 0:03:04.546478\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000645\n",
            "Local Step 01: Test Loss: 2.848461150, Test Accuracy: 34.470\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.552158022, Training Accuracy: 34.076\n",
            "Worker 1, [02/16]: Training Loss: 2.414749447, Training Accuracy: 36.936\n",
            "Worker 1, [03/16]: Training Loss: 2.364194721, Training Accuracy: 37.844\n",
            "Worker 1, [04/16]: Training Loss: 2.289861210, Training Accuracy: 39.732\n",
            "Worker 1, [05/16]: Training Loss: 2.232034059, Training Accuracy: 41.112\n",
            "Worker 1, [06/16]: Training Loss: 2.172497948, Training Accuracy: 42.292\n",
            "Worker 1, [07/16]: Training Loss: 2.156350854, Training Accuracy: 42.372\n",
            "Worker 1, [08/16]: Training Loss: 2.093502334, Training Accuracy: 43.868\n",
            "Worker 1, [09/16]: Training Loss: 2.068145715, Training Accuracy: 44.404\n",
            "Worker 1, [10/16]: Training Loss: 2.032034321, Training Accuracy: 45.248\n",
            "Worker 1, [11/16]: Training Loss: 1.989835985, Training Accuracy: 46.220\n",
            "Worker 1, [12/16]: Training Loss: 1.956974583, Training Accuracy: 46.524\n",
            "Worker 1, [13/16]: Training Loss: 1.930772894, Training Accuracy: 47.380\n",
            "Worker 1, [14/16]: Training Loss: 1.905000120, Training Accuracy: 47.920\n",
            "Worker 1, [15/16]: Training Loss: 1.870121372, Training Accuracy: 48.824\n",
            "Worker 1, [16/16]: Training Loss: 1.871008075, Training Accuracy: 48.636\n",
            "Time taken for training worker 1: 0:03:05.902009\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.409209133, Training Accuracy: 38.168\n",
            "Worker 2, [02/16]: Training Loss: 2.252268446, Training Accuracy: 41.500\n",
            "Worker 2, [03/16]: Training Loss: 2.171810797, Training Accuracy: 42.748\n",
            "Worker 2, [04/16]: Training Loss: 2.093772527, Training Accuracy: 44.160\n",
            "Worker 2, [05/16]: Training Loss: 2.037144998, Training Accuracy: 45.460\n",
            "Worker 2, [06/16]: Training Loss: 1.977119161, Training Accuracy: 46.624\n",
            "Worker 2, [07/16]: Training Loss: 1.973853147, Training Accuracy: 46.848\n",
            "Worker 2, [08/16]: Training Loss: 1.925187764, Training Accuracy: 47.908\n",
            "Worker 2, [09/16]: Training Loss: 1.894372770, Training Accuracy: 48.496\n",
            "Worker 2, [10/16]: Training Loss: 1.861561962, Training Accuracy: 49.232\n",
            "Worker 2, [11/16]: Training Loss: 1.818652721, Training Accuracy: 49.996\n",
            "Worker 2, [12/16]: Training Loss: 1.802942160, Training Accuracy: 50.492\n",
            "Worker 2, [13/16]: Training Loss: 1.811159879, Training Accuracy: 49.964\n",
            "Worker 2, [14/16]: Training Loss: 1.769927046, Training Accuracy: 51.304\n",
            "Worker 2, [15/16]: Training Loss: 1.748002648, Training Accuracy: 52.184\n",
            "Worker 2, [16/16]: Training Loss: 1.726089021, Training Accuracy: 52.536\n",
            "Time taken for training worker 2: 0:03:06.781713\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000885\n",
            "Local Step 02: Test Loss: 2.140722233, Test Accuracy: 45.830\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.196534240, Training Accuracy: 42.808\n",
            "Worker 1, [02/16]: Training Loss: 2.016387049, Training Accuracy: 46.304\n",
            "Worker 1, [03/16]: Training Loss: 1.937147004, Training Accuracy: 47.748\n",
            "Worker 1, [04/16]: Training Loss: 1.875175741, Training Accuracy: 49.200\n",
            "Worker 1, [05/16]: Training Loss: 1.825824472, Training Accuracy: 50.552\n",
            "Worker 1, [06/16]: Training Loss: 1.780376347, Training Accuracy: 51.044\n",
            "Worker 1, [07/16]: Training Loss: 1.764647755, Training Accuracy: 51.560\n",
            "Worker 1, [08/16]: Training Loss: 1.739362237, Training Accuracy: 51.772\n",
            "Worker 1, [09/16]: Training Loss: 1.711585178, Training Accuracy: 52.900\n",
            "Worker 1, [10/16]: Training Loss: 1.682121340, Training Accuracy: 53.032\n",
            "Worker 1, [11/16]: Training Loss: 1.661100765, Training Accuracy: 53.632\n",
            "Worker 1, [12/16]: Training Loss: 1.647352595, Training Accuracy: 53.944\n",
            "Worker 1, [13/16]: Training Loss: 1.622048487, Training Accuracy: 54.460\n",
            "Worker 1, [14/16]: Training Loss: 1.599502227, Training Accuracy: 55.344\n",
            "Worker 1, [15/16]: Training Loss: 1.589631807, Training Accuracy: 55.588\n",
            "Worker 1, [16/16]: Training Loss: 1.592374084, Training Accuracy: 55.344\n",
            "Time taken for training worker 1: 0:03:06.610008\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.175030767, Training Accuracy: 43.220\n",
            "Worker 2, [02/16]: Training Loss: 1.980160204, Training Accuracy: 47.228\n",
            "Worker 2, [03/16]: Training Loss: 1.891883698, Training Accuracy: 48.796\n",
            "Worker 2, [04/16]: Training Loss: 1.832916036, Training Accuracy: 50.004\n",
            "Worker 2, [05/16]: Training Loss: 1.786589769, Training Accuracy: 51.364\n",
            "Worker 2, [06/16]: Training Loss: 1.745587355, Training Accuracy: 52.220\n",
            "Worker 2, [07/16]: Training Loss: 1.720767187, Training Accuracy: 52.676\n",
            "Worker 2, [08/16]: Training Loss: 1.697174145, Training Accuracy: 53.092\n",
            "Worker 2, [09/16]: Training Loss: 1.676349559, Training Accuracy: 53.412\n",
            "Worker 2, [10/16]: Training Loss: 1.656138015, Training Accuracy: 54.336\n",
            "Worker 2, [11/16]: Training Loss: 1.628936227, Training Accuracy: 54.832\n",
            "Worker 2, [12/16]: Training Loss: 1.604103921, Training Accuracy: 54.896\n",
            "Worker 2, [13/16]: Training Loss: 1.573067313, Training Accuracy: 56.140\n",
            "Worker 2, [14/16]: Training Loss: 1.573886591, Training Accuracy: 56.132\n",
            "Worker 2, [15/16]: Training Loss: 1.564841174, Training Accuracy: 56.260\n",
            "Worker 2, [16/16]: Training Loss: 1.552931347, Training Accuracy: 56.796\n",
            "Time taken for training worker 2: 0:03:08.377805\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000568\n",
            "Local Step 03: Test Loss: 2.178563132, Test Accuracy: 46.670\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.083750147, Training Accuracy: 45.212\n",
            "Worker 1, [02/16]: Training Loss: 1.893890099, Training Accuracy: 49.088\n",
            "Worker 1, [03/16]: Training Loss: 1.801780231, Training Accuracy: 51.084\n",
            "Worker 1, [04/16]: Training Loss: 1.730576803, Training Accuracy: 52.460\n",
            "Worker 1, [05/16]: Training Loss: 1.695633231, Training Accuracy: 52.980\n",
            "Worker 1, [06/16]: Training Loss: 1.659778636, Training Accuracy: 53.940\n",
            "Worker 1, [07/16]: Training Loss: 1.626493212, Training Accuracy: 54.832\n",
            "Worker 1, [08/16]: Training Loss: 1.592008003, Training Accuracy: 55.720\n",
            "Worker 1, [09/16]: Training Loss: 1.591579183, Training Accuracy: 55.216\n",
            "Worker 1, [10/16]: Training Loss: 1.563526900, Training Accuracy: 56.172\n",
            "Worker 1, [11/16]: Training Loss: 1.545633820, Training Accuracy: 56.684\n",
            "Worker 1, [12/16]: Training Loss: 1.537286890, Training Accuracy: 56.656\n",
            "Worker 1, [13/16]: Training Loss: 1.521343292, Training Accuracy: 57.080\n",
            "Worker 1, [14/16]: Training Loss: 1.515076644, Training Accuracy: 57.376\n",
            "Worker 1, [15/16]: Training Loss: 1.479926594, Training Accuracy: 58.200\n",
            "Worker 1, [16/16]: Training Loss: 1.471651662, Training Accuracy: 58.244\n",
            "Time taken for training worker 1: 0:03:06.294265\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.099579561, Training Accuracy: 44.368\n",
            "Worker 2, [02/16]: Training Loss: 1.890557253, Training Accuracy: 49.080\n",
            "Worker 2, [03/16]: Training Loss: 1.783026777, Training Accuracy: 51.228\n",
            "Worker 2, [04/16]: Training Loss: 1.711319183, Training Accuracy: 53.156\n",
            "Worker 2, [05/16]: Training Loss: 1.686561457, Training Accuracy: 53.768\n",
            "Worker 2, [06/16]: Training Loss: 1.652465487, Training Accuracy: 54.120\n",
            "Worker 2, [07/16]: Training Loss: 1.617991603, Training Accuracy: 54.876\n",
            "Worker 2, [08/16]: Training Loss: 1.591738340, Training Accuracy: 55.688\n",
            "Worker 2, [09/16]: Training Loss: 1.567220453, Training Accuracy: 56.120\n",
            "Worker 2, [10/16]: Training Loss: 1.558894711, Training Accuracy: 55.960\n",
            "Worker 2, [11/16]: Training Loss: 1.532953145, Training Accuracy: 56.732\n",
            "Worker 2, [12/16]: Training Loss: 1.514118834, Training Accuracy: 57.560\n",
            "Worker 2, [13/16]: Training Loss: 1.506887933, Training Accuracy: 57.824\n",
            "Worker 2, [14/16]: Training Loss: 1.483987455, Training Accuracy: 57.932\n",
            "Worker 2, [15/16]: Training Loss: 1.469335131, Training Accuracy: 58.704\n",
            "Worker 2, [16/16]: Training Loss: 1.486737280, Training Accuracy: 57.960\n",
            "Time taken for training worker 2: 0:03:07.821363\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000553\n",
            "Local Step 04: Test Loss: 2.200970882, Test Accuracy: 46.040\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.047482033, Training Accuracy: 46.412\n",
            "Worker 1, [02/16]: Training Loss: 1.840240085, Training Accuracy: 50.316\n",
            "Worker 1, [03/16]: Training Loss: 1.747405159, Training Accuracy: 52.536\n",
            "Worker 1, [04/16]: Training Loss: 1.685102729, Training Accuracy: 53.328\n",
            "Worker 1, [05/16]: Training Loss: 1.647698036, Training Accuracy: 54.380\n",
            "Worker 1, [06/16]: Training Loss: 1.590284064, Training Accuracy: 55.492\n",
            "Worker 1, [07/16]: Training Loss: 1.573024179, Training Accuracy: 55.700\n",
            "Worker 1, [08/16]: Training Loss: 1.529455924, Training Accuracy: 57.108\n",
            "Worker 1, [09/16]: Training Loss: 1.513511343, Training Accuracy: 57.244\n",
            "Worker 1, [10/16]: Training Loss: 1.513280870, Training Accuracy: 57.168\n",
            "Worker 1, [11/16]: Training Loss: 1.476394626, Training Accuracy: 58.264\n",
            "Worker 1, [12/16]: Training Loss: 1.464747713, Training Accuracy: 58.552\n",
            "Worker 1, [13/16]: Training Loss: 1.446523635, Training Accuracy: 58.652\n",
            "Worker 1, [14/16]: Training Loss: 1.452930240, Training Accuracy: 58.708\n",
            "Worker 1, [15/16]: Training Loss: 1.435546018, Training Accuracy: 59.640\n",
            "Worker 1, [16/16]: Training Loss: 1.415079891, Training Accuracy: 59.796\n",
            "Time taken for training worker 1: 0:03:42.759508\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.051167272, Training Accuracy: 45.680\n",
            "Worker 2, [02/16]: Training Loss: 1.853594668, Training Accuracy: 49.940\n",
            "Worker 2, [03/16]: Training Loss: 1.746860644, Training Accuracy: 52.356\n",
            "Worker 2, [04/16]: Training Loss: 1.694540793, Training Accuracy: 53.476\n",
            "Worker 2, [05/16]: Training Loss: 1.628297531, Training Accuracy: 55.052\n",
            "Worker 2, [06/16]: Training Loss: 1.597514382, Training Accuracy: 55.532\n",
            "Worker 2, [07/16]: Training Loss: 1.562251390, Training Accuracy: 56.272\n",
            "Worker 2, [08/16]: Training Loss: 1.534404641, Training Accuracy: 57.012\n",
            "Worker 2, [09/16]: Training Loss: 1.509836424, Training Accuracy: 57.816\n",
            "Worker 2, [10/16]: Training Loss: 1.508321208, Training Accuracy: 57.676\n",
            "Worker 2, [11/16]: Training Loss: 1.491873759, Training Accuracy: 58.264\n",
            "Worker 2, [12/16]: Training Loss: 1.477098900, Training Accuracy: 58.584\n",
            "Worker 2, [13/16]: Training Loss: 1.471698841, Training Accuracy: 58.472\n",
            "Worker 2, [14/16]: Training Loss: 1.456024543, Training Accuracy: 58.928\n",
            "Worker 2, [15/16]: Training Loss: 1.436582932, Training Accuracy: 59.276\n",
            "Worker 2, [16/16]: Training Loss: 1.417857815, Training Accuracy: 59.776\n",
            "Time taken for training worker 2: 0:04:08.608622\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001406\n",
            "Local Step 05: Test Loss: 2.147417557, Test Accuracy: 47.350\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 2.012255499, Training Accuracy: 47.076\n",
            "Worker 1, [02/16]: Training Loss: 1.817424296, Training Accuracy: 50.728\n",
            "Worker 1, [03/16]: Training Loss: 1.702908553, Training Accuracy: 53.260\n",
            "Worker 1, [04/16]: Training Loss: 1.636555992, Training Accuracy: 54.948\n",
            "Worker 1, [05/16]: Training Loss: 1.588698921, Training Accuracy: 56.096\n",
            "Worker 1, [06/16]: Training Loss: 1.569790223, Training Accuracy: 56.040\n",
            "Worker 1, [07/16]: Training Loss: 1.521617824, Training Accuracy: 57.516\n",
            "Worker 1, [08/16]: Training Loss: 1.510914721, Training Accuracy: 57.148\n",
            "Worker 1, [09/16]: Training Loss: 1.488113512, Training Accuracy: 57.760\n",
            "Worker 1, [10/16]: Training Loss: 1.463708276, Training Accuracy: 58.672\n",
            "Worker 1, [11/16]: Training Loss: 1.441970875, Training Accuracy: 59.456\n",
            "Worker 1, [12/16]: Training Loss: 1.436543226, Training Accuracy: 59.284\n",
            "Worker 1, [13/16]: Training Loss: 1.424077773, Training Accuracy: 59.424\n",
            "Worker 1, [14/16]: Training Loss: 1.409048777, Training Accuracy: 59.864\n",
            "Worker 1, [15/16]: Training Loss: 1.412545985, Training Accuracy: 59.692\n",
            "Worker 1, [16/16]: Training Loss: 1.378488297, Training Accuracy: 60.464\n",
            "Time taken for training worker 1: 0:04:05.176333\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.022429897, Training Accuracy: 46.856\n",
            "Worker 2, [02/16]: Training Loss: 1.816790586, Training Accuracy: 50.912\n",
            "Worker 2, [03/16]: Training Loss: 1.718662438, Training Accuracy: 53.064\n",
            "Worker 2, [04/16]: Training Loss: 1.627535883, Training Accuracy: 54.836\n",
            "Worker 2, [05/16]: Training Loss: 1.600269825, Training Accuracy: 55.244\n",
            "Worker 2, [06/16]: Training Loss: 1.572563189, Training Accuracy: 56.140\n",
            "Worker 2, [07/16]: Training Loss: 1.542009582, Training Accuracy: 56.704\n",
            "Worker 2, [08/16]: Training Loss: 1.508278128, Training Accuracy: 57.744\n",
            "Worker 2, [09/16]: Training Loss: 1.484581343, Training Accuracy: 58.020\n",
            "Worker 2, [10/16]: Training Loss: 1.480121584, Training Accuracy: 58.300\n",
            "Worker 2, [11/16]: Training Loss: 1.452462988, Training Accuracy: 58.952\n",
            "Worker 2, [12/16]: Training Loss: 1.454339312, Training Accuracy: 58.980\n",
            "Worker 2, [13/16]: Training Loss: 1.424006048, Training Accuracy: 59.548\n",
            "Worker 2, [14/16]: Training Loss: 1.409687495, Training Accuracy: 59.984\n",
            "Worker 2, [15/16]: Training Loss: 1.406023202, Training Accuracy: 59.688\n",
            "Worker 2, [16/16]: Training Loss: 1.410679646, Training Accuracy: 59.888\n",
            "Time taken for training worker 2: 0:03:05.071259\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000562\n",
            "Local Step 06: Test Loss: 2.140128873, Test Accuracy: 47.430\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.997895411, Training Accuracy: 46.920\n",
            "Worker 1, [02/16]: Training Loss: 1.781484076, Training Accuracy: 51.712\n",
            "Worker 1, [03/16]: Training Loss: 1.666191203, Training Accuracy: 53.940\n",
            "Worker 1, [04/16]: Training Loss: 1.623003067, Training Accuracy: 54.632\n",
            "Worker 1, [05/16]: Training Loss: 1.560472695, Training Accuracy: 56.940\n",
            "Worker 1, [06/16]: Training Loss: 1.526648181, Training Accuracy: 57.220\n",
            "Worker 1, [07/16]: Training Loss: 1.496225699, Training Accuracy: 58.068\n",
            "Worker 1, [08/16]: Training Loss: 1.481443540, Training Accuracy: 58.276\n",
            "Worker 1, [09/16]: Training Loss: 1.457689183, Training Accuracy: 59.132\n",
            "Worker 1, [10/16]: Training Loss: 1.424023127, Training Accuracy: 59.680\n",
            "Worker 1, [11/16]: Training Loss: 1.411746120, Training Accuracy: 60.100\n",
            "Worker 1, [12/16]: Training Loss: 1.388182729, Training Accuracy: 60.204\n",
            "Worker 1, [13/16]: Training Loss: 1.399929102, Training Accuracy: 59.840\n",
            "Worker 1, [14/16]: Training Loss: 1.356718105, Training Accuracy: 60.712\n",
            "Worker 1, [15/16]: Training Loss: 1.391871234, Training Accuracy: 60.192\n",
            "Worker 1, [16/16]: Training Loss: 1.361152201, Training Accuracy: 60.932\n",
            "Time taken for training worker 1: 0:03:19.021337\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.985782209, Training Accuracy: 47.352\n",
            "Worker 2, [02/16]: Training Loss: 1.765226019, Training Accuracy: 52.076\n",
            "Worker 2, [03/16]: Training Loss: 1.669855108, Training Accuracy: 53.500\n",
            "Worker 2, [04/16]: Training Loss: 1.626201929, Training Accuracy: 54.892\n",
            "Worker 2, [05/16]: Training Loss: 1.557422572, Training Accuracy: 56.516\n",
            "Worker 2, [06/16]: Training Loss: 1.528672375, Training Accuracy: 57.640\n",
            "Worker 2, [07/16]: Training Loss: 1.508132708, Training Accuracy: 57.916\n",
            "Worker 2, [08/16]: Training Loss: 1.474666299, Training Accuracy: 58.260\n",
            "Worker 2, [09/16]: Training Loss: 1.478961602, Training Accuracy: 58.464\n",
            "Worker 2, [10/16]: Training Loss: 1.450756891, Training Accuracy: 58.860\n",
            "Worker 2, [11/16]: Training Loss: 1.422575672, Training Accuracy: 59.936\n",
            "Worker 2, [12/16]: Training Loss: 1.429361910, Training Accuracy: 59.480\n",
            "Worker 2, [13/16]: Training Loss: 1.408889961, Training Accuracy: 59.844\n",
            "Worker 2, [14/16]: Training Loss: 1.402360122, Training Accuracy: 60.204\n",
            "Worker 2, [15/16]: Training Loss: 1.382788449, Training Accuracy: 60.928\n",
            "Worker 2, [16/16]: Training Loss: 1.384798174, Training Accuracy: 60.588\n",
            "Time taken for training worker 2: 0:03:08.845149\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000602\n",
            "Local Step 07: Test Loss: 2.115118567, Test Accuracy: 47.810\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.988307262, Training Accuracy: 47.412\n",
            "Worker 1, [02/16]: Training Loss: 1.768884536, Training Accuracy: 51.984\n",
            "Worker 1, [03/16]: Training Loss: 1.650886152, Training Accuracy: 54.240\n",
            "Worker 1, [04/16]: Training Loss: 1.605110668, Training Accuracy: 55.220\n",
            "Worker 1, [05/16]: Training Loss: 1.552516184, Training Accuracy: 56.624\n",
            "Worker 1, [06/16]: Training Loss: 1.501211780, Training Accuracy: 58.048\n",
            "Worker 1, [07/16]: Training Loss: 1.486368295, Training Accuracy: 58.360\n",
            "Worker 1, [08/16]: Training Loss: 1.465561478, Training Accuracy: 58.576\n",
            "Worker 1, [09/16]: Training Loss: 1.441357145, Training Accuracy: 59.156\n",
            "Worker 1, [10/16]: Training Loss: 1.414132863, Training Accuracy: 59.732\n",
            "Worker 1, [11/16]: Training Loss: 1.407278520, Training Accuracy: 59.752\n",
            "Worker 1, [12/16]: Training Loss: 1.364810864, Training Accuracy: 60.888\n",
            "Worker 1, [13/16]: Training Loss: 1.365645990, Training Accuracy: 61.060\n",
            "Worker 1, [14/16]: Training Loss: 1.356984445, Training Accuracy: 61.016\n",
            "Worker 1, [15/16]: Training Loss: 1.331912560, Training Accuracy: 61.700\n",
            "Worker 1, [16/16]: Training Loss: 1.362487222, Training Accuracy: 61.248\n",
            "Time taken for training worker 1: 0:03:08.925361\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 1.984517061, Training Accuracy: 46.932\n",
            "Worker 2, [02/16]: Training Loss: 1.758191820, Training Accuracy: 51.944\n",
            "Worker 2, [03/16]: Training Loss: 1.656721797, Training Accuracy: 54.232\n",
            "Worker 2, [04/16]: Training Loss: 1.599692766, Training Accuracy: 55.748\n",
            "Worker 2, [05/16]: Training Loss: 1.547601655, Training Accuracy: 56.932\n",
            "Worker 2, [06/16]: Training Loss: 1.517648566, Training Accuracy: 57.496\n",
            "Worker 2, [07/16]: Training Loss: 1.500257334, Training Accuracy: 57.796\n",
            "Worker 2, [08/16]: Training Loss: 1.458747387, Training Accuracy: 58.828\n",
            "Worker 2, [09/16]: Training Loss: 1.430766911, Training Accuracy: 59.028\n",
            "Worker 2, [10/16]: Training Loss: 1.450993057, Training Accuracy: 58.836\n",
            "Worker 2, [11/16]: Training Loss: 1.405871280, Training Accuracy: 59.940\n",
            "Worker 2, [12/16]: Training Loss: 1.395225260, Training Accuracy: 60.468\n",
            "Worker 2, [13/16]: Training Loss: 1.396510714, Training Accuracy: 60.332\n",
            "Worker 2, [14/16]: Training Loss: 1.369353846, Training Accuracy: 60.728\n",
            "Worker 2, [15/16]: Training Loss: 1.362219617, Training Accuracy: 61.352\n",
            "Worker 2, [16/16]: Training Loss: 1.353414338, Training Accuracy: 61.404\n",
            "Time taken for training worker 2: 0:03:17.257493\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000676\n",
            "Local Step 08: Test Loss: 2.180802419, Test Accuracy: 46.000\n",
            "**************************************************\n",
            "Worker 1, [01/16]: Training Loss: 1.980311825, Training Accuracy: 47.368\n",
            "Worker 1, [02/16]: Training Loss: 1.737614731, Training Accuracy: 52.824\n",
            "Worker 1, [03/16]: Training Loss: 1.646864757, Training Accuracy: 54.628\n",
            "Worker 1, [04/16]: Training Loss: 1.576139805, Training Accuracy: 56.200\n",
            "Worker 1, [05/16]: Training Loss: 1.528902338, Training Accuracy: 56.996\n",
            "Worker 1, [06/16]: Training Loss: 1.480495924, Training Accuracy: 58.000\n",
            "Worker 1, [07/16]: Training Loss: 1.475724454, Training Accuracy: 58.228\n",
            "Worker 1, [08/16]: Training Loss: 1.425706612, Training Accuracy: 59.344\n",
            "Worker 1, [09/16]: Training Loss: 1.410918251, Training Accuracy: 59.868\n",
            "Worker 1, [10/16]: Training Loss: 1.404881334, Training Accuracy: 60.100\n",
            "Worker 1, [11/16]: Training Loss: 1.387635651, Training Accuracy: 60.648\n",
            "Worker 1, [12/16]: Training Loss: 1.359731345, Training Accuracy: 61.372\n",
            "Worker 1, [13/16]: Training Loss: 1.367150962, Training Accuracy: 60.816\n",
            "Worker 1, [14/16]: Training Loss: 1.338267541, Training Accuracy: 61.880\n",
            "Worker 1, [15/16]: Training Loss: 1.344655812, Training Accuracy: 61.424\n",
            "Worker 1, [16/16]: Training Loss: 1.333860079, Training Accuracy: 61.604\n",
            "Time taken for training worker 1: 0:03:18.302757\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/16]: Training Loss: 2.000927745, Training Accuracy: 47.360\n",
            "Worker 2, [02/16]: Training Loss: 1.750082240, Training Accuracy: 52.452\n",
            "Worker 2, [03/16]: Training Loss: 1.661753008, Training Accuracy: 54.096\n",
            "Worker 2, [04/16]: Training Loss: 1.594000553, Training Accuracy: 55.864\n",
            "Worker 2, [05/16]: Training Loss: 1.529874970, Training Accuracy: 57.152\n",
            "Worker 2, [06/16]: Training Loss: 1.497638059, Training Accuracy: 58.276\n",
            "Worker 2, [07/16]: Training Loss: 1.481559638, Training Accuracy: 58.344\n",
            "Worker 2, [08/16]: Training Loss: 1.440754654, Training Accuracy: 58.812\n",
            "Worker 2, [09/16]: Training Loss: 1.427447355, Training Accuracy: 59.728\n",
            "Worker 2, [10/16]: Training Loss: 1.396986353, Training Accuracy: 60.316\n",
            "Worker 2, [11/16]: Training Loss: 1.415616844, Training Accuracy: 59.724\n",
            "Worker 2, [12/16]: Training Loss: 1.378680679, Training Accuracy: 60.556\n",
            "Worker 2, [13/16]: Training Loss: 1.363927367, Training Accuracy: 61.008\n",
            "Worker 2, [14/16]: Training Loss: 1.369786629, Training Accuracy: 61.040\n",
            "Worker 2, [15/16]: Training Loss: 1.342144792, Training Accuracy: 61.776\n",
            "Worker 2, [16/16]: Training Loss: 1.334853251, Training Accuracy: 61.804\n",
            "Time taken for training worker 2: 0:03:16.183698\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000923\n",
            "Local Step 09: Test Loss: 2.134792559, Test Accuracy: 47.910\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:59:41.053590\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:32\n",
            "==================================================\n",
            "Worker 1, [01/32]: Training Loss: 4.326595491, Training Accuracy: 4.136\n",
            "Worker 1, [02/32]: Training Loss: 3.857211848, Training Accuracy: 9.900\n",
            "Worker 1, [03/32]: Training Loss: 3.578809728, Training Accuracy: 14.708\n",
            "Worker 1, [04/32]: Training Loss: 3.357370711, Training Accuracy: 18.304\n",
            "Worker 1, [05/32]: Training Loss: 3.193870259, Training Accuracy: 21.676\n",
            "Worker 1, [06/32]: Training Loss: 3.045835865, Training Accuracy: 23.800\n",
            "Worker 1, [07/32]: Training Loss: 2.917001488, Training Accuracy: 26.224\n",
            "Worker 1, [08/32]: Training Loss: 2.813969535, Training Accuracy: 28.788\n",
            "Worker 1, [09/32]: Training Loss: 2.722368787, Training Accuracy: 30.480\n",
            "Worker 1, [10/32]: Training Loss: 2.620401071, Training Accuracy: 32.384\n",
            "Worker 1, [11/32]: Training Loss: 2.562909383, Training Accuracy: 33.828\n",
            "Worker 1, [12/32]: Training Loss: 2.498354135, Training Accuracy: 35.268\n",
            "Worker 1, [13/32]: Training Loss: 2.423077488, Training Accuracy: 36.608\n",
            "Worker 1, [14/32]: Training Loss: 2.379538949, Training Accuracy: 37.608\n",
            "Worker 1, [15/32]: Training Loss: 2.343279362, Training Accuracy: 38.684\n",
            "Worker 1, [16/32]: Training Loss: 2.299414836, Training Accuracy: 39.448\n",
            "Worker 1, [17/32]: Training Loss: 2.242188518, Training Accuracy: 40.388\n",
            "Worker 1, [18/32]: Training Loss: 2.218698319, Training Accuracy: 40.976\n",
            "Worker 1, [19/32]: Training Loss: 2.172943416, Training Accuracy: 41.928\n",
            "Worker 1, [20/32]: Training Loss: 2.125687176, Training Accuracy: 43.284\n",
            "Worker 1, [21/32]: Training Loss: 2.097349817, Training Accuracy: 43.828\n",
            "Worker 1, [22/32]: Training Loss: 2.071921137, Training Accuracy: 44.192\n",
            "Worker 1, [23/32]: Training Loss: 2.065055040, Training Accuracy: 44.432\n",
            "Worker 1, [24/32]: Training Loss: 2.025643311, Training Accuracy: 45.236\n",
            "Worker 1, [25/32]: Training Loss: 1.999815006, Training Accuracy: 45.600\n",
            "Worker 1, [26/32]: Training Loss: 1.962810387, Training Accuracy: 46.404\n",
            "Worker 1, [27/32]: Training Loss: 1.950725178, Training Accuracy: 46.752\n",
            "Worker 1, [28/32]: Training Loss: 1.902055917, Training Accuracy: 47.844\n",
            "Worker 1, [29/32]: Training Loss: 1.893311491, Training Accuracy: 48.400\n",
            "Worker 1, [30/32]: Training Loss: 1.875174749, Training Accuracy: 49.172\n",
            "Worker 1, [31/32]: Training Loss: 1.849432849, Training Accuracy: 49.292\n",
            "Worker 1, [32/32]: Training Loss: 1.845005088, Training Accuracy: 49.268\n",
            "Time taken for training worker 1: 0:06:40.423050\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 4.323658731, Training Accuracy: 4.288\n",
            "Worker 2, [02/32]: Training Loss: 3.842807523, Training Accuracy: 10.440\n",
            "Worker 2, [03/32]: Training Loss: 3.567322412, Training Accuracy: 14.876\n",
            "Worker 2, [04/32]: Training Loss: 3.360994090, Training Accuracy: 18.212\n",
            "Worker 2, [05/32]: Training Loss: 3.184694832, Training Accuracy: 21.492\n",
            "Worker 2, [06/32]: Training Loss: 3.038239172, Training Accuracy: 24.212\n",
            "Worker 2, [07/32]: Training Loss: 2.909491752, Training Accuracy: 26.732\n",
            "Worker 2, [08/32]: Training Loss: 2.799606598, Training Accuracy: 28.732\n",
            "Worker 2, [09/32]: Training Loss: 2.719796978, Training Accuracy: 30.620\n",
            "Worker 2, [10/32]: Training Loss: 2.637610517, Training Accuracy: 31.708\n",
            "Worker 2, [11/32]: Training Loss: 2.577202339, Training Accuracy: 33.464\n",
            "Worker 2, [12/32]: Training Loss: 2.507635834, Training Accuracy: 34.636\n",
            "Worker 2, [13/32]: Training Loss: 2.458879680, Training Accuracy: 35.604\n",
            "Worker 2, [14/32]: Training Loss: 2.392892112, Training Accuracy: 37.640\n",
            "Worker 2, [15/32]: Training Loss: 2.335667201, Training Accuracy: 38.720\n",
            "Worker 2, [16/32]: Training Loss: 2.316559551, Training Accuracy: 38.976\n",
            "Worker 2, [17/32]: Training Loss: 2.261469789, Training Accuracy: 40.124\n",
            "Worker 2, [18/32]: Training Loss: 2.222414630, Training Accuracy: 40.760\n",
            "Worker 2, [19/32]: Training Loss: 2.179752280, Training Accuracy: 42.316\n",
            "Worker 2, [20/32]: Training Loss: 2.147166057, Training Accuracy: 42.124\n",
            "Worker 2, [21/32]: Training Loss: 2.105689161, Training Accuracy: 43.592\n",
            "Worker 2, [22/32]: Training Loss: 2.084393928, Training Accuracy: 43.584\n",
            "Worker 2, [23/32]: Training Loss: 2.056109761, Training Accuracy: 44.544\n",
            "Worker 2, [24/32]: Training Loss: 2.018702772, Training Accuracy: 45.348\n",
            "Worker 2, [25/32]: Training Loss: 2.007528803, Training Accuracy: 45.644\n",
            "Worker 2, [26/32]: Training Loss: 1.965106932, Training Accuracy: 46.756\n",
            "Worker 2, [27/32]: Training Loss: 1.954759490, Training Accuracy: 46.888\n",
            "Worker 2, [28/32]: Training Loss: 1.936736859, Training Accuracy: 47.288\n",
            "Worker 2, [29/32]: Training Loss: 1.905215907, Training Accuracy: 47.888\n",
            "Worker 2, [30/32]: Training Loss: 1.870671251, Training Accuracy: 48.884\n",
            "Worker 2, [31/32]: Training Loss: 1.850312299, Training Accuracy: 49.420\n",
            "Worker 2, [32/32]: Training Loss: 1.845390655, Training Accuracy: 49.204\n",
            "Time taken for training worker 2: 0:06:24.998830\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000623\n",
            "Local Step 01: Test Loss: 2.970327766, Test Accuracy: 31.800\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.454594925, Training Accuracy: 36.116\n",
            "Worker 1, [02/32]: Training Loss: 2.241454410, Training Accuracy: 40.424\n",
            "Worker 1, [03/32]: Training Loss: 2.137934259, Training Accuracy: 42.968\n",
            "Worker 1, [04/32]: Training Loss: 2.081738261, Training Accuracy: 44.048\n",
            "Worker 1, [05/32]: Training Loss: 2.025910377, Training Accuracy: 45.300\n",
            "Worker 1, [06/32]: Training Loss: 1.993321797, Training Accuracy: 45.936\n",
            "Worker 1, [07/32]: Training Loss: 1.943890758, Training Accuracy: 47.192\n",
            "Worker 1, [08/32]: Training Loss: 1.890032538, Training Accuracy: 48.596\n",
            "Worker 1, [09/32]: Training Loss: 1.867052040, Training Accuracy: 48.788\n",
            "Worker 1, [10/32]: Training Loss: 1.845549109, Training Accuracy: 49.356\n",
            "Worker 1, [11/32]: Training Loss: 1.816238564, Training Accuracy: 49.956\n",
            "Worker 1, [12/32]: Training Loss: 1.775737100, Training Accuracy: 51.256\n",
            "Worker 1, [13/32]: Training Loss: 1.773019651, Training Accuracy: 51.128\n",
            "Worker 1, [14/32]: Training Loss: 1.735707179, Training Accuracy: 52.028\n",
            "Worker 1, [15/32]: Training Loss: 1.740487240, Training Accuracy: 51.600\n",
            "Worker 1, [16/32]: Training Loss: 1.707999528, Training Accuracy: 52.808\n",
            "Worker 1, [17/32]: Training Loss: 1.685376515, Training Accuracy: 52.976\n",
            "Worker 1, [18/32]: Training Loss: 1.670351123, Training Accuracy: 53.404\n",
            "Worker 1, [19/32]: Training Loss: 1.634220047, Training Accuracy: 54.056\n",
            "Worker 1, [20/32]: Training Loss: 1.623604942, Training Accuracy: 54.428\n",
            "Worker 1, [21/32]: Training Loss: 1.612728467, Training Accuracy: 55.028\n",
            "Worker 1, [22/32]: Training Loss: 1.608793702, Training Accuracy: 55.072\n",
            "Worker 1, [23/32]: Training Loss: 1.591768009, Training Accuracy: 54.984\n",
            "Worker 1, [24/32]: Training Loss: 1.597423747, Training Accuracy: 55.196\n",
            "Worker 1, [25/32]: Training Loss: 1.599294843, Training Accuracy: 55.184\n",
            "Worker 1, [26/32]: Training Loss: 1.575419090, Training Accuracy: 55.616\n",
            "Worker 1, [27/32]: Training Loss: 1.568323627, Training Accuracy: 55.764\n",
            "Worker 1, [28/32]: Training Loss: 1.541136061, Training Accuracy: 56.628\n",
            "Worker 1, [29/32]: Training Loss: 1.540439844, Training Accuracy: 56.132\n",
            "Worker 1, [30/32]: Training Loss: 1.524904580, Training Accuracy: 57.172\n",
            "Worker 1, [31/32]: Training Loss: 1.518533427, Training Accuracy: 57.184\n",
            "Worker 1, [32/32]: Training Loss: 1.514348419, Training Accuracy: 57.388\n",
            "Time taken for training worker 1: 0:05:41.907830\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.412944996, Training Accuracy: 38.584\n",
            "Worker 2, [02/32]: Training Loss: 2.191465720, Training Accuracy: 43.032\n",
            "Worker 2, [03/32]: Training Loss: 2.090813996, Training Accuracy: 44.892\n",
            "Worker 2, [04/32]: Training Loss: 2.007778953, Training Accuracy: 46.568\n",
            "Worker 2, [05/32]: Training Loss: 1.942884988, Training Accuracy: 47.532\n",
            "Worker 2, [06/32]: Training Loss: 1.903786194, Training Accuracy: 48.588\n",
            "Worker 2, [07/32]: Training Loss: 1.846668185, Training Accuracy: 49.584\n",
            "Worker 2, [08/32]: Training Loss: 1.811444099, Training Accuracy: 51.024\n",
            "Worker 2, [09/32]: Training Loss: 1.788709920, Training Accuracy: 51.040\n",
            "Worker 2, [10/32]: Training Loss: 1.772880734, Training Accuracy: 51.520\n",
            "Worker 2, [11/32]: Training Loss: 1.729964231, Training Accuracy: 52.208\n",
            "Worker 2, [12/32]: Training Loss: 1.708571077, Training Accuracy: 53.164\n",
            "Worker 2, [13/32]: Training Loss: 1.715299403, Training Accuracy: 53.132\n",
            "Worker 2, [14/32]: Training Loss: 1.680434888, Training Accuracy: 53.712\n",
            "Worker 2, [15/32]: Training Loss: 1.671824728, Training Accuracy: 53.548\n",
            "Worker 2, [16/32]: Training Loss: 1.629346803, Training Accuracy: 54.816\n",
            "Worker 2, [17/32]: Training Loss: 1.639991783, Training Accuracy: 54.436\n",
            "Worker 2, [18/32]: Training Loss: 1.622435124, Training Accuracy: 54.860\n",
            "Worker 2, [19/32]: Training Loss: 1.588901553, Training Accuracy: 55.628\n",
            "Worker 2, [20/32]: Training Loss: 1.589369397, Training Accuracy: 55.476\n",
            "Worker 2, [21/32]: Training Loss: 1.578376412, Training Accuracy: 55.944\n",
            "Worker 2, [22/32]: Training Loss: 1.557637467, Training Accuracy: 56.600\n",
            "Worker 2, [23/32]: Training Loss: 1.536736925, Training Accuracy: 57.092\n",
            "Worker 2, [24/32]: Training Loss: 1.529881431, Training Accuracy: 56.952\n",
            "Worker 2, [25/32]: Training Loss: 1.533767913, Training Accuracy: 57.032\n",
            "Worker 2, [26/32]: Training Loss: 1.515511798, Training Accuracy: 57.268\n",
            "Worker 2, [27/32]: Training Loss: 1.501630872, Training Accuracy: 57.384\n",
            "Worker 2, [28/32]: Training Loss: 1.506104593, Training Accuracy: 57.672\n",
            "Worker 2, [29/32]: Training Loss: 1.474617191, Training Accuracy: 58.068\n",
            "Worker 2, [30/32]: Training Loss: 1.481096511, Training Accuracy: 58.200\n",
            "Worker 2, [31/32]: Training Loss: 1.461362697, Training Accuracy: 58.692\n",
            "Worker 2, [32/32]: Training Loss: 1.459333561, Training Accuracy: 58.524\n",
            "Time taken for training worker 2: 0:05:42.220767\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000553\n",
            "Local Step 02: Test Loss: 2.209128845, Test Accuracy: 46.590\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.202183394, Training Accuracy: 43.436\n",
            "Worker 1, [02/32]: Training Loss: 1.990215132, Training Accuracy: 47.228\n",
            "Worker 1, [03/32]: Training Loss: 1.874483754, Training Accuracy: 49.548\n",
            "Worker 1, [04/32]: Training Loss: 1.802943653, Training Accuracy: 50.696\n",
            "Worker 1, [05/32]: Training Loss: 1.748572040, Training Accuracy: 51.840\n",
            "Worker 1, [06/32]: Training Loss: 1.712407778, Training Accuracy: 52.724\n",
            "Worker 1, [07/32]: Training Loss: 1.668065386, Training Accuracy: 53.872\n",
            "Worker 1, [08/32]: Training Loss: 1.641838833, Training Accuracy: 54.572\n",
            "Worker 1, [09/32]: Training Loss: 1.606288992, Training Accuracy: 55.444\n",
            "Worker 1, [10/32]: Training Loss: 1.606226169, Training Accuracy: 55.076\n",
            "Worker 1, [11/32]: Training Loss: 1.572671495, Training Accuracy: 55.740\n",
            "Worker 1, [12/32]: Training Loss: 1.531781178, Training Accuracy: 56.924\n",
            "Worker 1, [13/32]: Training Loss: 1.540239390, Training Accuracy: 56.824\n",
            "Worker 1, [14/32]: Training Loss: 1.512134818, Training Accuracy: 57.452\n",
            "Worker 1, [15/32]: Training Loss: 1.505200727, Training Accuracy: 57.644\n",
            "Worker 1, [16/32]: Training Loss: 1.473731097, Training Accuracy: 58.032\n",
            "Worker 1, [17/32]: Training Loss: 1.480759995, Training Accuracy: 58.304\n",
            "Worker 1, [18/32]: Training Loss: 1.450284305, Training Accuracy: 58.888\n",
            "Worker 1, [19/32]: Training Loss: 1.457876515, Training Accuracy: 58.412\n",
            "Worker 1, [20/32]: Training Loss: 1.447141170, Training Accuracy: 58.916\n",
            "Worker 1, [21/32]: Training Loss: 1.426425105, Training Accuracy: 59.312\n",
            "Worker 1, [22/32]: Training Loss: 1.430562708, Training Accuracy: 59.288\n",
            "Worker 1, [23/32]: Training Loss: 1.407885355, Training Accuracy: 59.924\n",
            "Worker 1, [24/32]: Training Loss: 1.410478351, Training Accuracy: 59.816\n",
            "Worker 1, [25/32]: Training Loss: 1.393245530, Training Accuracy: 60.408\n",
            "Worker 1, [26/32]: Training Loss: 1.387281049, Training Accuracy: 60.204\n",
            "Worker 1, [27/32]: Training Loss: 1.400314511, Training Accuracy: 60.548\n",
            "Worker 1, [28/32]: Training Loss: 1.387142021, Training Accuracy: 60.344\n",
            "Worker 1, [29/32]: Training Loss: 1.397292741, Training Accuracy: 60.340\n",
            "Worker 1, [30/32]: Training Loss: 1.372410225, Training Accuracy: 60.700\n",
            "Worker 1, [31/32]: Training Loss: 1.344901977, Training Accuracy: 61.268\n",
            "Worker 1, [32/32]: Training Loss: 1.338450405, Training Accuracy: 61.316\n",
            "Time taken for training worker 1: 0:05:39.201341\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.231915453, Training Accuracy: 42.836\n",
            "Worker 2, [02/32]: Training Loss: 1.963260092, Training Accuracy: 47.652\n",
            "Worker 2, [03/32]: Training Loss: 1.879538249, Training Accuracy: 49.636\n",
            "Worker 2, [04/32]: Training Loss: 1.786045385, Training Accuracy: 51.224\n",
            "Worker 2, [05/32]: Training Loss: 1.739055984, Training Accuracy: 52.556\n",
            "Worker 2, [06/32]: Training Loss: 1.676191864, Training Accuracy: 54.208\n",
            "Worker 2, [07/32]: Training Loss: 1.665559520, Training Accuracy: 54.052\n",
            "Worker 2, [08/32]: Training Loss: 1.623111341, Training Accuracy: 55.188\n",
            "Worker 2, [09/32]: Training Loss: 1.595575444, Training Accuracy: 55.728\n",
            "Worker 2, [10/32]: Training Loss: 1.583907724, Training Accuracy: 55.780\n",
            "Worker 2, [11/32]: Training Loss: 1.561542130, Training Accuracy: 56.268\n",
            "Worker 2, [12/32]: Training Loss: 1.546469202, Training Accuracy: 56.608\n",
            "Worker 2, [13/32]: Training Loss: 1.526137383, Training Accuracy: 57.004\n",
            "Worker 2, [14/32]: Training Loss: 1.511319046, Training Accuracy: 57.260\n",
            "Worker 2, [15/32]: Training Loss: 1.481650876, Training Accuracy: 58.176\n",
            "Worker 2, [16/32]: Training Loss: 1.479222064, Training Accuracy: 58.216\n",
            "Worker 2, [17/32]: Training Loss: 1.470243104, Training Accuracy: 58.812\n",
            "Worker 2, [18/32]: Training Loss: 1.438279944, Training Accuracy: 59.028\n",
            "Worker 2, [19/32]: Training Loss: 1.442145442, Training Accuracy: 59.280\n",
            "Worker 2, [20/32]: Training Loss: 1.447011063, Training Accuracy: 59.008\n",
            "Worker 2, [21/32]: Training Loss: 1.430274027, Training Accuracy: 59.624\n",
            "Worker 2, [22/32]: Training Loss: 1.405328838, Training Accuracy: 60.160\n",
            "Worker 2, [23/32]: Training Loss: 1.416536529, Training Accuracy: 59.576\n",
            "Worker 2, [24/32]: Training Loss: 1.397093517, Training Accuracy: 60.220\n",
            "Worker 2, [25/32]: Training Loss: 1.392252187, Training Accuracy: 60.412\n",
            "Worker 2, [26/32]: Training Loss: 1.394479060, Training Accuracy: 60.108\n",
            "Worker 2, [27/32]: Training Loss: 1.383593066, Training Accuracy: 60.592\n",
            "Worker 2, [28/32]: Training Loss: 1.369204654, Training Accuracy: 60.648\n",
            "Worker 2, [29/32]: Training Loss: 1.387845103, Training Accuracy: 60.772\n",
            "Worker 2, [30/32]: Training Loss: 1.358166943, Training Accuracy: 61.080\n",
            "Worker 2, [31/32]: Training Loss: 1.345109862, Training Accuracy: 61.272\n",
            "Worker 2, [32/32]: Training Loss: 1.351895948, Training Accuracy: 61.364\n",
            "Time taken for training worker 2: 0:05:47.816244\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000525\n",
            "Local Step 03: Test Loss: 2.201502364, Test Accuracy: 47.050\n",
            "**************************************************\n",
            "Worker 1, [01/32]: Training Loss: 2.163302338, Training Accuracy: 44.124\n",
            "Worker 1, [02/32]: Training Loss: 1.931012094, Training Accuracy: 48.496\n",
            "Worker 1, [03/32]: Training Loss: 1.793428006, Training Accuracy: 51.336\n",
            "Worker 1, [04/32]: Training Loss: 1.734733621, Training Accuracy: 52.492\n",
            "Worker 1, [05/32]: Training Loss: 1.660572690, Training Accuracy: 54.304\n",
            "Worker 1, [06/32]: Training Loss: 1.624660094, Training Accuracy: 54.740\n",
            "Worker 1, [07/32]: Training Loss: 1.591866104, Training Accuracy: 55.668\n",
            "Worker 1, [08/32]: Training Loss: 1.569823771, Training Accuracy: 55.984\n",
            "Worker 1, [09/32]: Training Loss: 1.523517099, Training Accuracy: 57.364\n",
            "Worker 1, [10/32]: Training Loss: 1.512086617, Training Accuracy: 57.552\n",
            "Worker 1, [11/32]: Training Loss: 1.482046651, Training Accuracy: 58.612\n",
            "Worker 1, [12/32]: Training Loss: 1.469520655, Training Accuracy: 58.784\n",
            "Worker 1, [13/32]: Training Loss: 1.456549231, Training Accuracy: 58.536\n",
            "Worker 1, [14/32]: Training Loss: 1.443518279, Training Accuracy: 59.204\n",
            "Worker 1, [15/32]: Training Loss: 1.435650008, Training Accuracy: 58.980\n",
            "Worker 1, [16/32]: Training Loss: 1.420545572, Training Accuracy: 59.484\n",
            "Worker 1, [17/32]: Training Loss: 1.403681631, Training Accuracy: 60.032\n",
            "Worker 1, [18/32]: Training Loss: 1.398528077, Training Accuracy: 60.292\n",
            "Worker 1, [19/32]: Training Loss: 1.389598322, Training Accuracy: 59.940\n",
            "Worker 1, [20/32]: Training Loss: 1.382203499, Training Accuracy: 60.404\n",
            "Worker 1, [21/32]: Training Loss: 1.369916125, Training Accuracy: 60.960\n",
            "Worker 1, [22/32]: Training Loss: 1.347638156, Training Accuracy: 61.356\n",
            "Worker 1, [23/32]: Training Loss: 1.348026865, Training Accuracy: 61.440\n",
            "Worker 1, [24/32]: Training Loss: 1.351023626, Training Accuracy: 61.548\n",
            "Worker 1, [25/32]: Training Loss: 1.329431750, Training Accuracy: 61.844\n",
            "Worker 1, [26/32]: Training Loss: 1.323178419, Training Accuracy: 61.916\n",
            "Worker 1, [27/32]: Training Loss: 1.311996878, Training Accuracy: 62.372\n",
            "Worker 1, [28/32]: Training Loss: 1.323935099, Training Accuracy: 61.848\n",
            "Worker 1, [29/32]: Training Loss: 1.308516033, Training Accuracy: 62.632\n",
            "Worker 1, [30/32]: Training Loss: 1.298144379, Training Accuracy: 62.688\n",
            "Worker 1, [31/32]: Training Loss: 1.292802024, Training Accuracy: 62.852\n",
            "Worker 1, [32/32]: Training Loss: 1.298687821, Training Accuracy: 62.400\n",
            "Time taken for training worker 1: 0:06:42.442711\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/32]: Training Loss: 2.188456465, Training Accuracy: 43.800\n",
            "Worker 2, [02/32]: Training Loss: 1.923897654, Training Accuracy: 48.516\n",
            "Worker 2, [03/32]: Training Loss: 1.817643954, Training Accuracy: 50.988\n",
            "Worker 2, [04/32]: Training Loss: 1.725569961, Training Accuracy: 52.724\n",
            "Worker 2, [05/32]: Training Loss: 1.680186117, Training Accuracy: 53.656\n",
            "Worker 2, [06/32]: Training Loss: 1.621563510, Training Accuracy: 55.292\n",
            "Worker 2, [07/32]: Training Loss: 1.588640634, Training Accuracy: 55.492\n",
            "Worker 2, [08/32]: Training Loss: 1.559977802, Training Accuracy: 56.276\n",
            "Worker 2, [09/32]: Training Loss: 1.521857525, Training Accuracy: 57.452\n",
            "Worker 2, [10/32]: Training Loss: 1.517370021, Training Accuracy: 57.164\n",
            "Worker 2, [11/32]: Training Loss: 1.510005865, Training Accuracy: 57.272\n",
            "Worker 2, [12/32]: Training Loss: 1.476315584, Training Accuracy: 58.436\n",
            "Worker 2, [13/32]: Training Loss: 1.462553188, Training Accuracy: 58.588\n",
            "Worker 2, [14/32]: Training Loss: 1.455269656, Training Accuracy: 58.640\n",
            "Worker 2, [15/32]: Training Loss: 1.437578740, Training Accuracy: 59.372\n",
            "Worker 2, [16/32]: Training Loss: 1.430602786, Training Accuracy: 59.216\n",
            "Worker 2, [17/32]: Training Loss: 1.413811982, Training Accuracy: 59.680\n",
            "Worker 2, [18/32]: Training Loss: 1.392331294, Training Accuracy: 59.988\n",
            "Worker 2, [19/32]: Training Loss: 1.390062272, Training Accuracy: 60.324\n",
            "Worker 2, [20/32]: Training Loss: 1.399783472, Training Accuracy: 60.548\n",
            "Worker 2, [21/32]: Training Loss: 1.346433185, Training Accuracy: 61.376\n",
            "Worker 2, [22/32]: Training Loss: 1.365541624, Training Accuracy: 60.856\n",
            "Worker 2, [23/32]: Training Loss: 1.347075353, Training Accuracy: 61.692\n",
            "Worker 2, [24/32]: Training Loss: 1.348723050, Training Accuracy: 61.476\n",
            "Worker 2, [25/32]: Training Loss: 1.353701759, Training Accuracy: 61.308\n",
            "Worker 2, [26/32]: Training Loss: 1.346712757, Training Accuracy: 61.424\n",
            "Worker 2, [27/32]: Training Loss: 1.348127433, Training Accuracy: 61.704\n",
            "Worker 2, [28/32]: Training Loss: 1.326479137, Training Accuracy: 62.092\n",
            "Worker 2, [29/32]: Training Loss: 1.340762070, Training Accuracy: 61.380\n",
            "Worker 2, [30/32]: Training Loss: 1.312821561, Training Accuracy: 62.332\n",
            "Worker 2, [31/32]: Training Loss: 1.308604234, Training Accuracy: 62.304\n",
            "Worker 2, [32/32]: Training Loss: 1.302038266, Training Accuracy: 62.668\n",
            "Time taken for training worker 2: 0:06:39.620931\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001100\n",
            "Local Step 04: Test Loss: 2.179688710, Test Accuracy: 47.730\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for local_SGD: 0:49:22.561657\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:64\n",
            "==================================================\n",
            "Worker 1, [01/64]: Training Loss: 4.324253809, Training Accuracy: 4.464\n",
            "Worker 1, [02/64]: Training Loss: 3.845344862, Training Accuracy: 10.216\n",
            "Worker 1, [03/64]: Training Loss: 3.573525734, Training Accuracy: 14.528\n",
            "Worker 1, [04/64]: Training Loss: 3.376839858, Training Accuracy: 18.156\n",
            "Worker 1, [05/64]: Training Loss: 3.198038457, Training Accuracy: 21.908\n",
            "Worker 1, [06/64]: Training Loss: 3.047479368, Training Accuracy: 23.924\n",
            "Worker 1, [07/64]: Training Loss: 2.919156421, Training Accuracy: 26.732\n",
            "Worker 1, [08/64]: Training Loss: 2.818673478, Training Accuracy: 28.884\n",
            "Worker 1, [09/64]: Training Loss: 2.718586193, Training Accuracy: 30.748\n",
            "Worker 1, [10/64]: Training Loss: 2.640912533, Training Accuracy: 32.404\n",
            "Worker 1, [11/64]: Training Loss: 2.570098261, Training Accuracy: 33.772\n",
            "Worker 1, [12/64]: Training Loss: 2.506997281, Training Accuracy: 35.108\n",
            "Worker 1, [13/64]: Training Loss: 2.446588988, Training Accuracy: 36.016\n",
            "Worker 1, [14/64]: Training Loss: 2.395771429, Training Accuracy: 37.144\n",
            "Worker 1, [15/64]: Training Loss: 2.340892524, Training Accuracy: 38.592\n",
            "Worker 1, [16/64]: Training Loss: 2.290878704, Training Accuracy: 39.404\n",
            "Worker 1, [17/64]: Training Loss: 2.260021929, Training Accuracy: 39.924\n",
            "Worker 1, [18/64]: Training Loss: 2.195266785, Training Accuracy: 41.752\n",
            "Worker 1, [19/64]: Training Loss: 2.172463727, Training Accuracy: 41.900\n",
            "Worker 1, [20/64]: Training Loss: 2.136260432, Training Accuracy: 42.928\n",
            "Worker 1, [21/64]: Training Loss: 2.105037801, Training Accuracy: 43.720\n",
            "Worker 1, [22/64]: Training Loss: 2.079444492, Training Accuracy: 44.028\n",
            "Worker 1, [23/64]: Training Loss: 2.049418741, Training Accuracy: 44.952\n",
            "Worker 1, [24/64]: Training Loss: 2.008074587, Training Accuracy: 45.632\n",
            "Worker 1, [25/64]: Training Loss: 1.984741619, Training Accuracy: 46.356\n",
            "Worker 1, [26/64]: Training Loss: 1.960837100, Training Accuracy: 46.740\n",
            "Worker 1, [27/64]: Training Loss: 1.958554051, Training Accuracy: 46.912\n",
            "Worker 1, [28/64]: Training Loss: 1.924841933, Training Accuracy: 47.748\n",
            "Worker 1, [29/64]: Training Loss: 1.892109267, Training Accuracy: 47.924\n",
            "Worker 1, [30/64]: Training Loss: 1.888233843, Training Accuracy: 48.668\n",
            "Worker 1, [31/64]: Training Loss: 1.864973990, Training Accuracy: 48.684\n",
            "Worker 1, [32/64]: Training Loss: 1.839978826, Training Accuracy: 49.416\n",
            "Worker 1, [33/64]: Training Loss: 1.814592938, Training Accuracy: 50.048\n",
            "Worker 1, [34/64]: Training Loss: 1.807436035, Training Accuracy: 49.976\n",
            "Worker 1, [35/64]: Training Loss: 1.811890316, Training Accuracy: 50.152\n",
            "Worker 1, [36/64]: Training Loss: 1.783616028, Training Accuracy: 50.576\n",
            "Worker 1, [37/64]: Training Loss: 1.760649864, Training Accuracy: 51.552\n",
            "Worker 1, [38/64]: Training Loss: 1.761299747, Training Accuracy: 50.988\n",
            "Worker 1, [39/64]: Training Loss: 1.728684006, Training Accuracy: 52.256\n",
            "Worker 1, [40/64]: Training Loss: 1.707293996, Training Accuracy: 52.620\n",
            "Worker 1, [41/64]: Training Loss: 1.710439770, Training Accuracy: 52.420\n",
            "Worker 1, [42/64]: Training Loss: 1.677023831, Training Accuracy: 53.604\n",
            "Worker 1, [43/64]: Training Loss: 1.679740591, Training Accuracy: 53.204\n",
            "Worker 1, [44/64]: Training Loss: 1.673363346, Training Accuracy: 53.468\n",
            "Worker 1, [45/64]: Training Loss: 1.658596065, Training Accuracy: 54.176\n",
            "Worker 1, [46/64]: Training Loss: 1.642583601, Training Accuracy: 53.912\n",
            "Worker 1, [47/64]: Training Loss: 1.633798059, Training Accuracy: 53.980\n",
            "Worker 1, [48/64]: Training Loss: 1.620333055, Training Accuracy: 55.032\n",
            "Worker 1, [49/64]: Training Loss: 1.605009969, Training Accuracy: 54.852\n",
            "Worker 1, [50/64]: Training Loss: 1.599526849, Training Accuracy: 55.028\n",
            "Worker 1, [51/64]: Training Loss: 1.598870240, Training Accuracy: 54.944\n",
            "Worker 1, [52/64]: Training Loss: 1.594474398, Training Accuracy: 55.240\n",
            "Worker 1, [53/64]: Training Loss: 1.561178365, Training Accuracy: 56.080\n",
            "Worker 1, [54/64]: Training Loss: 1.581964510, Training Accuracy: 55.536\n",
            "Worker 1, [55/64]: Training Loss: 1.587222352, Training Accuracy: 55.308\n",
            "Worker 1, [56/64]: Training Loss: 1.560059806, Training Accuracy: 55.908\n",
            "Worker 1, [57/64]: Training Loss: 1.554943112, Training Accuracy: 55.704\n",
            "Worker 1, [58/64]: Training Loss: 1.529637066, Training Accuracy: 56.760\n",
            "Worker 1, [59/64]: Training Loss: 1.529332703, Training Accuracy: 56.924\n",
            "Worker 1, [60/64]: Training Loss: 1.531229401, Training Accuracy: 57.092\n",
            "Worker 1, [61/64]: Training Loss: 1.511393231, Training Accuracy: 57.604\n",
            "Worker 1, [62/64]: Training Loss: 1.523225480, Training Accuracy: 56.972\n",
            "Worker 1, [63/64]: Training Loss: 1.517140092, Training Accuracy: 57.016\n",
            "Worker 1, [64/64]: Training Loss: 1.500875375, Training Accuracy: 57.388\n",
            "Time taken for training worker 1: 0:13:22.398057\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/64]: Training Loss: 4.334111247, Training Accuracy: 4.060\n",
            "Worker 2, [02/64]: Training Loss: 3.845385627, Training Accuracy: 10.532\n",
            "Worker 2, [03/64]: Training Loss: 3.589182828, Training Accuracy: 14.632\n",
            "Worker 2, [04/64]: Training Loss: 3.364630492, Training Accuracy: 18.260\n",
            "Worker 2, [05/64]: Training Loss: 3.192648520, Training Accuracy: 21.396\n",
            "Worker 2, [06/64]: Training Loss: 3.039845024, Training Accuracy: 24.196\n",
            "Worker 2, [07/64]: Training Loss: 2.921117562, Training Accuracy: 26.640\n",
            "Worker 2, [08/64]: Training Loss: 2.829687070, Training Accuracy: 28.428\n",
            "Worker 2, [09/64]: Training Loss: 2.733008550, Training Accuracy: 30.148\n",
            "Worker 2, [10/64]: Training Loss: 2.652501770, Training Accuracy: 31.904\n",
            "Worker 2, [11/64]: Training Loss: 2.574404008, Training Accuracy: 33.256\n",
            "Worker 2, [12/64]: Training Loss: 2.522266296, Training Accuracy: 34.520\n",
            "Worker 2, [13/64]: Training Loss: 2.471582425, Training Accuracy: 35.472\n",
            "Worker 2, [14/64]: Training Loss: 2.425731776, Training Accuracy: 36.732\n",
            "Worker 2, [15/64]: Training Loss: 2.373792065, Training Accuracy: 37.792\n",
            "Worker 2, [16/64]: Training Loss: 2.323768182, Training Accuracy: 38.600\n",
            "Worker 2, [17/64]: Training Loss: 2.278415485, Training Accuracy: 39.556\n",
            "Worker 2, [18/64]: Training Loss: 2.236336773, Training Accuracy: 40.612\n",
            "Worker 2, [19/64]: Training Loss: 2.197465722, Training Accuracy: 41.256\n",
            "Worker 2, [20/64]: Training Loss: 2.180510832, Training Accuracy: 41.884\n",
            "Worker 2, [21/64]: Training Loss: 2.140911747, Training Accuracy: 42.224\n",
            "Worker 2, [22/64]: Training Loss: 2.101410633, Training Accuracy: 43.728\n",
            "Worker 2, [23/64]: Training Loss: 2.070980836, Training Accuracy: 44.360\n",
            "Worker 2, [24/64]: Training Loss: 2.064189275, Training Accuracy: 44.524\n",
            "Worker 2, [25/64]: Training Loss: 2.017578334, Training Accuracy: 45.320\n",
            "Worker 2, [26/64]: Training Loss: 1.992489025, Training Accuracy: 46.012\n",
            "Worker 2, [27/64]: Training Loss: 1.986766118, Training Accuracy: 45.840\n",
            "Worker 2, [28/64]: Training Loss: 1.947795433, Training Accuracy: 46.824\n",
            "Worker 2, [29/64]: Training Loss: 1.931068445, Training Accuracy: 47.412\n",
            "Worker 2, [30/64]: Training Loss: 1.907247979, Training Accuracy: 47.776\n",
            "Worker 2, [31/64]: Training Loss: 1.873874406, Training Accuracy: 48.136\n",
            "Worker 2, [32/64]: Training Loss: 1.865556914, Training Accuracy: 49.000\n",
            "Worker 2, [33/64]: Training Loss: 1.856675136, Training Accuracy: 48.516\n",
            "Worker 2, [34/64]: Training Loss: 1.823060505, Training Accuracy: 49.676\n",
            "Worker 2, [35/64]: Training Loss: 1.816544514, Training Accuracy: 50.112\n",
            "Worker 2, [36/64]: Training Loss: 1.792880546, Training Accuracy: 50.528\n",
            "Worker 2, [37/64]: Training Loss: 1.774670236, Training Accuracy: 51.212\n",
            "Worker 2, [38/64]: Training Loss: 1.757593017, Training Accuracy: 51.220\n",
            "Worker 2, [39/64]: Training Loss: 1.749315222, Training Accuracy: 51.540\n",
            "Worker 2, [40/64]: Training Loss: 1.725572986, Training Accuracy: 52.036\n",
            "Worker 2, [41/64]: Training Loss: 1.721552822, Training Accuracy: 52.176\n",
            "Worker 2, [42/64]: Training Loss: 1.723937331, Training Accuracy: 52.352\n",
            "Worker 2, [43/64]: Training Loss: 1.693911662, Training Accuracy: 53.036\n",
            "Worker 2, [44/64]: Training Loss: 1.671535034, Training Accuracy: 53.384\n",
            "Worker 2, [45/64]: Training Loss: 1.675270486, Training Accuracy: 53.248\n",
            "Worker 2, [46/64]: Training Loss: 1.658637974, Training Accuracy: 53.440\n",
            "Worker 2, [47/64]: Training Loss: 1.639749294, Training Accuracy: 53.880\n",
            "Worker 2, [48/64]: Training Loss: 1.635112452, Training Accuracy: 54.208\n",
            "Worker 2, [49/64]: Training Loss: 1.646463143, Training Accuracy: 54.068\n",
            "Worker 2, [50/64]: Training Loss: 1.618159773, Training Accuracy: 54.508\n",
            "Worker 2, [51/64]: Training Loss: 1.601583910, Training Accuracy: 55.176\n",
            "Worker 2, [52/64]: Training Loss: 1.596556001, Training Accuracy: 54.796\n",
            "Worker 2, [53/64]: Training Loss: 1.607719674, Training Accuracy: 54.940\n",
            "Worker 2, [54/64]: Training Loss: 1.572383617, Training Accuracy: 55.496\n",
            "Worker 2, [55/64]: Training Loss: 1.587728067, Training Accuracy: 55.392\n",
            "Worker 2, [56/64]: Training Loss: 1.570593274, Training Accuracy: 56.036\n",
            "Worker 2, [57/64]: Training Loss: 1.562242665, Training Accuracy: 56.032\n",
            "Worker 2, [58/64]: Training Loss: 1.568143442, Training Accuracy: 56.112\n",
            "Worker 2, [59/64]: Training Loss: 1.538139662, Training Accuracy: 56.736\n",
            "Worker 2, [60/64]: Training Loss: 1.527876959, Training Accuracy: 56.632\n",
            "Worker 2, [61/64]: Training Loss: 1.522006614, Training Accuracy: 57.256\n",
            "Worker 2, [62/64]: Training Loss: 1.518447411, Training Accuracy: 57.120\n",
            "Worker 2, [63/64]: Training Loss: 1.504970172, Training Accuracy: 57.460\n",
            "Worker 2, [64/64]: Training Loss: 1.519245584, Training Accuracy: 57.208\n",
            "Time taken for training worker 2: 0:11:54.213710\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000548\n",
            "Local Step 01: Test Loss: 3.223165409, Test Accuracy: 28.600\n",
            "**************************************************\n",
            "Worker 1, [01/64]: Training Loss: 2.450173878, Training Accuracy: 36.336\n",
            "Worker 1, [02/64]: Training Loss: 2.169695994, Training Accuracy: 42.224\n",
            "Worker 1, [03/64]: Training Loss: 2.021809995, Training Accuracy: 45.420\n",
            "Worker 1, [04/64]: Training Loss: 1.950778157, Training Accuracy: 46.928\n",
            "Worker 1, [05/64]: Training Loss: 1.906656981, Training Accuracy: 47.724\n",
            "Worker 1, [06/64]: Training Loss: 1.835178337, Training Accuracy: 49.360\n",
            "Worker 1, [07/64]: Training Loss: 1.800000137, Training Accuracy: 50.492\n",
            "Worker 1, [08/64]: Training Loss: 1.752746938, Training Accuracy: 51.584\n",
            "Worker 1, [09/64]: Training Loss: 1.735161975, Training Accuracy: 51.912\n",
            "Worker 1, [10/64]: Training Loss: 1.685549559, Training Accuracy: 53.372\n",
            "Worker 1, [11/64]: Training Loss: 1.676590539, Training Accuracy: 53.620\n",
            "Worker 1, [12/64]: Training Loss: 1.665864030, Training Accuracy: 53.588\n",
            "Worker 1, [13/64]: Training Loss: 1.635807857, Training Accuracy: 54.072\n",
            "Worker 1, [14/64]: Training Loss: 1.612725423, Training Accuracy: 54.760\n",
            "Worker 1, [15/64]: Training Loss: 1.611832387, Training Accuracy: 55.004\n",
            "Worker 1, [16/64]: Training Loss: 1.594812653, Training Accuracy: 54.852\n",
            "Worker 1, [17/64]: Training Loss: 1.561500452, Training Accuracy: 56.304\n",
            "Worker 1, [18/64]: Training Loss: 1.562200349, Training Accuracy: 55.884\n",
            "Worker 1, [19/64]: Training Loss: 1.540527040, Training Accuracy: 56.712\n",
            "Worker 1, [20/64]: Training Loss: 1.527403335, Training Accuracy: 57.196\n",
            "Worker 1, [21/64]: Training Loss: 1.522601839, Training Accuracy: 56.940\n",
            "Worker 1, [22/64]: Training Loss: 1.502397668, Training Accuracy: 57.708\n",
            "Worker 1, [23/64]: Training Loss: 1.513301878, Training Accuracy: 57.096\n",
            "Worker 1, [24/64]: Training Loss: 1.472957246, Training Accuracy: 58.160\n",
            "Worker 1, [25/64]: Training Loss: 1.471377229, Training Accuracy: 58.172\n",
            "Worker 1, [26/64]: Training Loss: 1.468624069, Training Accuracy: 58.280\n",
            "Worker 1, [27/64]: Training Loss: 1.446439528, Training Accuracy: 58.964\n",
            "Worker 1, [28/64]: Training Loss: 1.453736947, Training Accuracy: 59.048\n",
            "Worker 1, [29/64]: Training Loss: 1.437483005, Training Accuracy: 58.964\n",
            "Worker 1, [30/64]: Training Loss: 1.429381466, Training Accuracy: 59.032\n",
            "Worker 1, [31/64]: Training Loss: 1.416794503, Training Accuracy: 59.420\n",
            "Worker 1, [32/64]: Training Loss: 1.438524527, Training Accuracy: 59.064\n",
            "Worker 1, [33/64]: Training Loss: 1.434996475, Training Accuracy: 59.556\n",
            "Worker 1, [34/64]: Training Loss: 1.418097563, Training Accuracy: 59.612\n",
            "Worker 1, [35/64]: Training Loss: 1.400228638, Training Accuracy: 60.156\n",
            "Worker 1, [36/64]: Training Loss: 1.400533337, Training Accuracy: 60.012\n",
            "Worker 1, [37/64]: Training Loss: 1.406766297, Training Accuracy: 59.924\n",
            "Worker 1, [38/64]: Training Loss: 1.393351950, Training Accuracy: 60.068\n",
            "Worker 1, [39/64]: Training Loss: 1.381654383, Training Accuracy: 60.376\n",
            "Worker 1, [40/64]: Training Loss: 1.390620821, Training Accuracy: 60.000\n",
            "Worker 1, [41/64]: Training Loss: 1.366678625, Training Accuracy: 60.808\n",
            "Worker 1, [42/64]: Training Loss: 1.365017614, Training Accuracy: 60.924\n",
            "Worker 1, [43/64]: Training Loss: 1.385180973, Training Accuracy: 60.316\n",
            "Worker 1, [44/64]: Training Loss: 1.364631040, Training Accuracy: 60.780\n",
            "Worker 1, [45/64]: Training Loss: 1.352484758, Training Accuracy: 61.220\n",
            "Worker 1, [46/64]: Training Loss: 1.338311093, Training Accuracy: 61.480\n",
            "Worker 1, [47/64]: Training Loss: 1.361017825, Training Accuracy: 61.044\n",
            "Worker 1, [48/64]: Training Loss: 1.345737675, Training Accuracy: 61.552\n",
            "Worker 1, [49/64]: Training Loss: 1.338390898, Training Accuracy: 61.500\n",
            "Worker 1, [50/64]: Training Loss: 1.342377576, Training Accuracy: 61.724\n",
            "Worker 1, [51/64]: Training Loss: 1.357128311, Training Accuracy: 61.728\n",
            "Worker 1, [52/64]: Training Loss: 1.328207776, Training Accuracy: 61.948\n",
            "Worker 1, [53/64]: Training Loss: 1.344043266, Training Accuracy: 61.144\n",
            "Worker 1, [54/64]: Training Loss: 1.335355670, Training Accuracy: 61.560\n",
            "Worker 1, [55/64]: Training Loss: 1.326483614, Training Accuracy: 61.824\n",
            "Worker 1, [56/64]: Training Loss: 1.325003130, Training Accuracy: 61.860\n",
            "Worker 1, [57/64]: Training Loss: 1.315390069, Training Accuracy: 62.292\n",
            "Worker 1, [58/64]: Training Loss: 1.320043594, Training Accuracy: 62.256\n",
            "Worker 1, [59/64]: Training Loss: 1.326339274, Training Accuracy: 61.440\n",
            "Worker 1, [60/64]: Training Loss: 1.306476754, Training Accuracy: 62.652\n",
            "Worker 1, [61/64]: Training Loss: 1.326131571, Training Accuracy: 61.992\n",
            "Worker 1, [62/64]: Training Loss: 1.309336792, Training Accuracy: 62.308\n",
            "Worker 1, [63/64]: Training Loss: 1.313523233, Training Accuracy: 62.020\n",
            "Worker 1, [64/64]: Training Loss: 1.297579033, Training Accuracy: 62.548\n",
            "Time taken for training worker 1: 0:11:09.331820\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/64]: Training Loss: 2.428313667, Training Accuracy: 39.016\n",
            "Worker 2, [02/64]: Training Loss: 2.177465572, Training Accuracy: 43.492\n",
            "Worker 2, [03/64]: Training Loss: 2.045320612, Training Accuracy: 45.956\n",
            "Worker 2, [04/64]: Training Loss: 1.970369696, Training Accuracy: 47.724\n",
            "Worker 2, [05/64]: Training Loss: 1.909122529, Training Accuracy: 48.532\n",
            "Worker 2, [06/64]: Training Loss: 1.839162702, Training Accuracy: 50.592\n",
            "Worker 2, [07/64]: Training Loss: 1.810778910, Training Accuracy: 50.568\n",
            "Worker 2, [08/64]: Training Loss: 1.789012428, Training Accuracy: 51.040\n",
            "Worker 2, [09/64]: Training Loss: 1.760650171, Training Accuracy: 51.896\n",
            "Worker 2, [10/64]: Training Loss: 1.712925634, Training Accuracy: 52.832\n",
            "Worker 2, [11/64]: Training Loss: 1.695922140, Training Accuracy: 53.132\n",
            "Worker 2, [12/64]: Training Loss: 1.668802027, Training Accuracy: 53.428\n",
            "Worker 2, [13/64]: Training Loss: 1.650751736, Training Accuracy: 54.012\n",
            "Worker 2, [14/64]: Training Loss: 1.620433523, Training Accuracy: 54.780\n",
            "Worker 2, [15/64]: Training Loss: 1.606075904, Training Accuracy: 55.292\n",
            "Worker 2, [16/64]: Training Loss: 1.606597814, Training Accuracy: 55.164\n",
            "Worker 2, [17/64]: Training Loss: 1.581783398, Training Accuracy: 55.452\n",
            "Worker 2, [18/64]: Training Loss: 1.585411676, Training Accuracy: 55.760\n",
            "Worker 2, [19/64]: Training Loss: 1.560217901, Training Accuracy: 56.200\n",
            "Worker 2, [20/64]: Training Loss: 1.538778310, Training Accuracy: 56.892\n",
            "Worker 2, [21/64]: Training Loss: 1.522506557, Training Accuracy: 57.300\n",
            "Worker 2, [22/64]: Training Loss: 1.524026444, Training Accuracy: 57.000\n",
            "Worker 2, [23/64]: Training Loss: 1.492128319, Training Accuracy: 58.148\n",
            "Worker 2, [24/64]: Training Loss: 1.491512565, Training Accuracy: 57.984\n",
            "Worker 2, [25/64]: Training Loss: 1.488275452, Training Accuracy: 58.240\n",
            "Worker 2, [26/64]: Training Loss: 1.484137058, Training Accuracy: 58.512\n",
            "Worker 2, [27/64]: Training Loss: 1.476468196, Training Accuracy: 58.480\n",
            "Worker 2, [28/64]: Training Loss: 1.461333859, Training Accuracy: 58.832\n",
            "Worker 2, [29/64]: Training Loss: 1.445791682, Training Accuracy: 58.848\n",
            "Worker 2, [30/64]: Training Loss: 1.430550731, Training Accuracy: 59.564\n",
            "Worker 2, [31/64]: Training Loss: 1.426675133, Training Accuracy: 59.448\n",
            "Worker 2, [32/64]: Training Loss: 1.412510748, Training Accuracy: 60.016\n",
            "Worker 2, [33/64]: Training Loss: 1.417022493, Training Accuracy: 59.764\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Workers:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Number of Local Steps:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mlocal_SGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[15], line 26\u001b[0m, in \u001b[0;36mlocal_SGD\u001b[0;34m(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m train_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loca_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(j):\n\u001b[0;32m---> 26\u001b[0m   train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mis_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorker \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworker\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloca_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.9f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m train_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn, accumulation_steps, device, is_wandb)\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Initialize gradients to zero at the start of each epoch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m---> 10\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [2, 4, 8]\n",
        "J = [4, 8, 16, 32, 64] \n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    local_SGD(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SlowMo Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def synchronize(models)\n",
        "\n",
        "  \n",
        "  \n",
        "def calculate_exact_average(models):\n",
        "    # Initialize a state dict with zeros, same shape as the model parameters\n",
        "    avg_state_dict = {key: torch.zeros_like(value) for key, value in models[0].state_dict().items()}\n",
        "    \n",
        "    # Sum up all the model parameters\n",
        "    for model in models:\n",
        "        for key, value in model.state_dict().items():\n",
        "            avg_state_dict[key] += value\n",
        "    \n",
        "    # Divide each parameter by the number of models to get the average\n",
        "    for key in avg_state_dict:\n",
        "        avg_state_dict[key] /= len(models)\n",
        "    \n",
        "    # Create a new model to store the average parameters\n",
        "    exact_average_model = LeNet5().to(device)  # Assuming LeNet5 is your model architecture\n",
        "    exact_average_model.load_state_dict(avg_state_dict)\n",
        "    \n",
        "    return exact_average_model\n",
        "\n",
        "# def update_slow_momentum(beta, u_old, lr, global_model, exact_average_model_new):\n",
        "#     u_new = {}\n",
        "#     for (key1,param), (key2,param_exact) in zip(global_model.parameters().items(), exact_average_model_new.parameters().items()):\n",
        "#         u_new[key1] = beta * u_old[key2] + lr * (param.data - param_exact.data)\n",
        "#     return u_new\n",
        "\n",
        "def update_slow_momentum(beta, u_old, lr, global_model, exact_average_model_new):\n",
        "    # Iterate through parameters of each model\n",
        "    u_new = {}\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking for this operation\n",
        "        for key, value in global_model.state_dict().items():\n",
        "            v1 = exact_average_model_new.state_dict()[key]\n",
        "            v2 = u_old[key]\n",
        "            \n",
        "            # Perform the update\n",
        "            u_new[key] = beta * v2 + (1.0 / lr) * (value - v1)\n",
        "    \n",
        "    return u_new\n",
        "\n",
        "def update_weights(global_model, alpha, lr, u_new):\n",
        "    new_weights = {}\n",
        "    for key, value in global_model.state_dict().items():\n",
        "        new_weights[key] = value - alpha * lr * u_new[key]\n",
        "    \n",
        "    return new_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def local_SGD_SlowMo(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, beta, alpha):\n",
        "  total_start_time = time.time()\n",
        "\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "  # global_optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "  # Initialize a scheduler for each optimizer\n",
        "  # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "  momentum = {key: torch.zeros_like(value) for key, value in global_model.state_dict().items()}\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for loca_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    # global_model = exact_average_model_new\n",
        "    exact_average_model_new = calculate_exact_average(local_models)\n",
        "    \n",
        "    momentum = update_slow_momentum(beta, momentum, lr, global_model, exact_average_model_new)\n",
        "\n",
        "    global_model.load_state_dict(update_weights(global_model, alpha, lr, momentum))\n",
        "    \n",
        "\n",
        "    for local_model in local_models:\n",
        "       local_model.load_state_dict(global_model.state_dict())\n",
        "    # TODO: Schedule ro check konim\n",
        "    # scheduler.step() \n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Local Step {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "# TODO: Schedule ro check konim \n",
        "\n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for local_SGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:4, Update Slow Model every 2 steps\n",
            "==================================================\n",
            "Worker 1, [01/04]: Training Loss: 4.326976429, Training Accuracy: 4.340\n",
            "Worker 1, [02/04]: Training Loss: 3.846710726, Training Accuracy: 10.348\n",
            "Worker 1, [03/04]: Training Loss: 3.589552763, Training Accuracy: 14.352\n",
            "Worker 1, [04/04]: Training Loss: 3.365852302, Training Accuracy: 18.340\n",
            "Time taken for training worker 1: 0:01:23.489779\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 4.336933194, Training Accuracy: 4.328\n",
            "Worker 2, [02/04]: Training Loss: 3.851574042, Training Accuracy: 10.624\n",
            "Worker 2, [03/04]: Training Loss: 3.582781175, Training Accuracy: 14.772\n",
            "Worker 2, [04/04]: Training Loss: 3.376521511, Training Accuracy: 18.180\n",
            "Time taken for training worker 2: 0:01:14.554320\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.010884\n",
            "OrderedDict([('conv1.weight', tensor([[[[ 0.0782, -0.0107,  0.0974, -0.0358,  0.0833],\n",
            "          [-0.0214, -0.0122, -0.0079, -0.0730, -0.0319],\n",
            "          [-0.1158, -0.1031,  0.0525, -0.0559, -0.0044],\n",
            "          [ 0.0280,  0.1024, -0.0632, -0.0132, -0.0018],\n",
            "          [-0.0035, -0.0387,  0.0303,  0.0712,  0.0503]],\n",
            "\n",
            "         [[-0.0129, -0.0205,  0.1132, -0.0469,  0.0536],\n",
            "          [ 0.0367, -0.0030, -0.0902, -0.0294, -0.1064],\n",
            "          [-0.0826, -0.1022,  0.0273, -0.0554,  0.0162],\n",
            "          [-0.0163, -0.0493,  0.0460,  0.0340, -0.0507],\n",
            "          [-0.0330,  0.0147,  0.0487, -0.0168, -0.0196]],\n",
            "\n",
            "         [[ 0.0124, -0.0291,  0.1099,  0.0396, -0.0106],\n",
            "          [ 0.0941,  0.1049,  0.0933,  0.0857,  0.0256],\n",
            "          [ 0.0982, -0.0160, -0.0526,  0.0853, -0.0064],\n",
            "          [ 0.0307, -0.0628, -0.0379,  0.0882, -0.0066],\n",
            "          [-0.0243, -0.0727, -0.0771, -0.0232, -0.0378]]],\n",
            "\n",
            "\n",
            "        [[[-0.0026, -0.0874,  0.0297, -0.0380, -0.0988],\n",
            "          [-0.0122, -0.0835, -0.0335, -0.0460, -0.0550],\n",
            "          [ 0.0788, -0.0587, -0.0394,  0.1231, -0.0495],\n",
            "          [ 0.0777,  0.0009, -0.0506,  0.1170,  0.0791],\n",
            "          [-0.0547,  0.1095, -0.0212,  0.0745,  0.0851]],\n",
            "\n",
            "         [[ 0.0808, -0.0362, -0.0010, -0.0315,  0.0119],\n",
            "          [ 0.1003, -0.1121, -0.1074,  0.1120, -0.1097],\n",
            "          [-0.0058, -0.0050,  0.0357,  0.1307,  0.0279],\n",
            "          [-0.0535,  0.1067, -0.0855,  0.0343, -0.0948],\n",
            "          [-0.0564,  0.0960, -0.1052, -0.0805,  0.0595]],\n",
            "\n",
            "         [[ 0.0535, -0.0540,  0.1073,  0.0438, -0.1170],\n",
            "          [ 0.0525, -0.0142, -0.0894,  0.0460,  0.0625],\n",
            "          [-0.0618,  0.0006, -0.0595, -0.0278,  0.0421],\n",
            "          [ 0.0678, -0.0507,  0.0585, -0.0184, -0.0623],\n",
            "          [-0.1284,  0.0859,  0.0406, -0.0651, -0.0690]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0338, -0.0440, -0.0639,  0.0078,  0.0103],\n",
            "          [-0.0096, -0.0160, -0.0257, -0.0975, -0.0076],\n",
            "          [-0.0853,  0.0729, -0.0237, -0.0726, -0.0891],\n",
            "          [-0.0034,  0.0364, -0.0450, -0.0727,  0.0530],\n",
            "          [-0.0248, -0.0765, -0.0787,  0.0289,  0.0528]],\n",
            "\n",
            "         [[ 0.0290, -0.0050, -0.0589, -0.1037, -0.0793],\n",
            "          [ 0.0010, -0.0261,  0.0123,  0.0123, -0.0437],\n",
            "          [ 0.0374,  0.0033, -0.0790, -0.0233, -0.0454],\n",
            "          [ 0.0809, -0.0516,  0.0591,  0.0438,  0.0341],\n",
            "          [ 0.0282,  0.0347,  0.0525,  0.0403, -0.0987]],\n",
            "\n",
            "         [[ 0.1108,  0.1522,  0.1223,  0.0590,  0.1105],\n",
            "          [ 0.1477,  0.0618,  0.0178,  0.1226,  0.0921],\n",
            "          [ 0.0265,  0.0359,  0.0875,  0.1232, -0.0269],\n",
            "          [ 0.0779, -0.0538, -0.0197,  0.0175, -0.0278],\n",
            "          [-0.0091,  0.0957,  0.0771, -0.0860, -0.0868]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0366,  0.0083, -0.1324, -0.0722, -0.0235],\n",
            "          [-0.0290, -0.0207, -0.0144, -0.1416, -0.0028],\n",
            "          [ 0.1506,  0.0882,  0.0407,  0.0104, -0.0344],\n",
            "          [ 0.0981,  0.1278,  0.1203, -0.1076, -0.0717],\n",
            "          [ 0.1466,  0.1440,  0.1169, -0.0643, -0.1305]],\n",
            "\n",
            "         [[ 0.0361,  0.0716,  0.0481,  0.0793, -0.0695],\n",
            "          [-0.0333,  0.0499, -0.0650, -0.0079, -0.0167],\n",
            "          [ 0.0043, -0.1029,  0.0058,  0.0666, -0.0911],\n",
            "          [-0.0062, -0.0388, -0.1146,  0.0662, -0.0714],\n",
            "          [-0.1073, -0.0220, -0.0353, -0.0271, -0.0697]],\n",
            "\n",
            "         [[-0.0857,  0.0694,  0.0523,  0.1038,  0.0050],\n",
            "          [-0.0209, -0.0674,  0.0757,  0.0823,  0.0874],\n",
            "          [-0.0693, -0.0007,  0.0110,  0.1062,  0.0497],\n",
            "          [-0.0602, -0.1590, -0.1021,  0.0931,  0.1229],\n",
            "          [-0.1589, -0.1395, -0.1024,  0.1099,  0.1679]]],\n",
            "\n",
            "\n",
            "        [[[-0.0927,  0.0027, -0.0911, -0.1553, -0.0178],\n",
            "          [-0.0352, -0.0803, -0.1272, -0.0887, -0.1369],\n",
            "          [-0.1191, -0.0722, -0.0860, -0.1698, -0.1404],\n",
            "          [-0.1225, -0.0575, -0.0281, -0.0413, -0.1263],\n",
            "          [-0.0725, -0.1238, -0.0910, -0.0342, -0.0386]],\n",
            "\n",
            "         [[ 0.0210,  0.0388, -0.0733, -0.0306, -0.0109],\n",
            "          [ 0.0355, -0.0393,  0.0047, -0.0347, -0.0957],\n",
            "          [ 0.0121,  0.0406,  0.0643,  0.0283,  0.0358],\n",
            "          [-0.0310,  0.0210, -0.0193, -0.0362,  0.0573],\n",
            "          [-0.0186,  0.0727,  0.0318,  0.0431,  0.0666]],\n",
            "\n",
            "         [[ 0.0790,  0.1253,  0.0707,  0.1630,  0.1546],\n",
            "          [ 0.1459,  0.0977,  0.0121,  0.0014,  0.1130],\n",
            "          [ 0.1355,  0.0265,  0.0962,  0.1090, -0.0221],\n",
            "          [ 0.0505, -0.0250,  0.0690,  0.1195,  0.0978],\n",
            "          [ 0.0854,  0.0277,  0.0652,  0.0823,  0.1066]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0426,  0.0554,  0.1358, -0.0925,  0.0250],\n",
            "          [ 0.0090,  0.0620, -0.0452,  0.0743, -0.0217],\n",
            "          [-0.0324, -0.0730, -0.1000, -0.0190,  0.0849],\n",
            "          [ 0.0203, -0.1157, -0.1200, -0.0529, -0.0129],\n",
            "          [-0.0341,  0.0209, -0.0593, -0.0504, -0.0072]],\n",
            "\n",
            "         [[ 0.0234,  0.0102,  0.1362, -0.0724, -0.0948],\n",
            "          [ 0.0970, -0.0461,  0.1387,  0.0930, -0.0290],\n",
            "          [-0.0456,  0.0167, -0.0700,  0.0996,  0.0552],\n",
            "          [ 0.0184, -0.0839, -0.0015,  0.0327,  0.0321],\n",
            "          [-0.0667, -0.0629, -0.0558,  0.0297, -0.0141]],\n",
            "\n",
            "         [[ 0.0215, -0.0220,  0.0764,  0.0336, -0.1213],\n",
            "          [ 0.0464,  0.0410, -0.0307,  0.0444,  0.0393],\n",
            "          [-0.0615,  0.0960, -0.0121,  0.0562,  0.0502],\n",
            "          [ 0.0244, -0.0565, -0.0800, -0.0235,  0.0627],\n",
            "          [ 0.0043,  0.0385, -0.0653, -0.0984, -0.0783]]]], device='cuda:0')), ('conv1.bias', tensor([-0.1000,  0.2927,  0.2468,  0.3323, -0.0785, -0.0094, -0.0900,  0.1281,\n",
            "         0.1701, -0.1033,  0.1523, -0.0886,  0.0942, -0.0377,  0.2373, -0.2261,\n",
            "        -0.0642, -0.1190, -0.1967, -0.1002,  0.0549, -0.2764, -0.0738,  0.2235,\n",
            "        -0.0755, -0.1012,  0.0591, -0.3821, -0.0499, -0.1248, -0.1979,  0.0719,\n",
            "        -0.1071, -0.0237, -0.1289, -0.0520, -0.1501, -0.1031, -0.1172, -0.1453,\n",
            "        -0.1183, -0.0472, -0.2534, -0.3570,  0.0824, -0.0231, -0.2118, -0.2009,\n",
            "        -0.0979,  0.0792,  0.2209, -0.2210,  0.1421,  0.0897, -0.0473, -0.0662,\n",
            "         0.0600, -0.0678, -0.0954, -0.0504, -0.2998, -0.2054,  0.0388,  0.1337],\n",
            "       device='cuda:0')), ('conv2.weight', tensor([[[[-1.8307e-02, -1.6823e-04,  7.8644e-03,  1.4605e-03,  7.2307e-04],\n",
            "          [-2.1222e-02,  1.6872e-02, -9.6830e-03,  3.7525e-03,  9.9842e-04],\n",
            "          [-2.2076e-02, -1.6772e-02, -1.8268e-02, -1.8195e-02,  1.1787e-02],\n",
            "          [ 7.5794e-03, -1.0532e-02, -9.1209e-04, -2.3997e-02,  7.6303e-03],\n",
            "          [-2.3867e-03, -8.4320e-03, -1.1849e-02, -8.9319e-03, -9.0924e-03]],\n",
            "\n",
            "         [[ 2.1522e-02,  2.1523e-02,  1.0923e-02, -1.4159e-02, -1.8419e-02],\n",
            "          [ 2.0879e-02,  2.4470e-02,  1.2073e-02,  9.1617e-03,  1.1226e-02],\n",
            "          [ 3.5807e-02,  2.1968e-02,  2.9681e-02,  2.2580e-02, -8.6436e-03],\n",
            "          [ 1.7970e-02,  1.9121e-02,  2.4065e-03, -5.7300e-03, -2.7408e-03],\n",
            "          [ 1.1721e-02,  1.9829e-02,  8.4884e-03,  4.6469e-03, -1.0424e-02]],\n",
            "\n",
            "         [[ 1.9774e-02,  1.3760e-02,  2.0377e-02,  2.8906e-02,  4.8215e-03],\n",
            "          [ 1.5631e-02,  1.6297e-02,  2.2983e-02, -1.0216e-02,  2.4762e-02],\n",
            "          [ 1.6789e-02,  1.2214e-02, -8.8060e-03, -1.7501e-02,  2.5481e-02],\n",
            "          [-4.6060e-03,  9.4188e-03, -5.4554e-03,  1.7377e-03,  8.0115e-03],\n",
            "          [ 8.1635e-03,  1.3288e-04,  1.2491e-02,  3.0760e-02,  2.2671e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.3946e-02,  4.8606e-03, -1.1255e-02,  1.0180e-03,  8.4747e-03],\n",
            "          [ 6.4002e-03, -2.1984e-02, -2.8241e-02,  3.3692e-03, -4.4848e-03],\n",
            "          [-1.1356e-04, -1.7070e-02, -1.7378e-02, -3.0012e-02, -2.0582e-02],\n",
            "          [-1.4102e-02, -1.9090e-02, -1.2570e-02, -1.4217e-02, -3.1243e-02],\n",
            "          [-6.6273e-03, -2.2402e-02, -3.4156e-02, -5.4905e-03, -7.3224e-03]],\n",
            "\n",
            "         [[ 2.8601e-02,  1.4739e-02,  1.0855e-02, -1.9449e-02,  9.7034e-03],\n",
            "          [-7.6987e-03, -1.4921e-02, -1.2435e-02, -1.7068e-02, -2.2416e-02],\n",
            "          [-1.1686e-02, -5.8165e-03, -1.2034e-02, -1.8483e-03, -2.9382e-02],\n",
            "          [-3.4953e-03,  2.1953e-03, -1.9341e-02, -2.2957e-02, -2.6178e-02],\n",
            "          [-1.9050e-03, -8.4054e-03, -9.2440e-04, -1.4291e-02, -1.8327e-02]],\n",
            "\n",
            "         [[ 2.5119e-02,  2.2717e-02,  1.7563e-02, -1.3484e-02,  3.4079e-03],\n",
            "          [-1.0718e-02,  1.4464e-02, -4.4546e-03,  1.8639e-02, -1.4210e-02],\n",
            "          [-1.4429e-02,  2.5660e-03, -4.1024e-03,  6.9420e-03, -4.3063e-03],\n",
            "          [-1.7125e-02,  2.9025e-03, -1.5240e-02, -6.8614e-03, -5.7211e-03],\n",
            "          [-1.4299e-02,  2.3499e-03, -2.6122e-02, -4.7172e-03, -3.4298e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.8179e-03,  1.4635e-02,  4.0472e-03,  6.1114e-03, -9.3242e-03],\n",
            "          [-3.7610e-03,  1.6235e-02,  1.8276e-02, -1.1112e-02,  1.5957e-02],\n",
            "          [-1.8669e-03, -1.2136e-02,  1.2968e-02, -9.1037e-03,  1.6709e-02],\n",
            "          [-1.1653e-02,  2.1609e-03, -6.8752e-03,  6.7006e-03, -4.1348e-03],\n",
            "          [ 3.6443e-03,  2.7454e-03,  9.9972e-03, -1.2206e-02, -1.9963e-03]],\n",
            "\n",
            "         [[-2.2619e-02,  5.0507e-03,  1.3109e-02, -1.3436e-02,  4.8276e-03],\n",
            "          [-8.4917e-03, -1.3516e-02,  1.8884e-02, -1.2635e-02,  3.0012e-03],\n",
            "          [ 1.2481e-02,  1.8055e-02,  9.6746e-04,  1.7647e-02,  1.4024e-02],\n",
            "          [ 6.7539e-03,  7.9888e-03, -1.3027e-02, -6.6298e-03, -4.0869e-03],\n",
            "          [-9.6755e-03,  9.3774e-04,  3.7039e-03, -8.6801e-03,  1.6935e-02]],\n",
            "\n",
            "         [[ 3.8896e-03,  1.3649e-02,  2.8708e-02,  2.0383e-02, -1.8534e-02],\n",
            "          [ 4.8035e-03,  2.9638e-02, -4.3274e-03, -2.4211e-02, -1.4499e-02],\n",
            "          [ 9.2670e-03,  2.6359e-03, -1.2161e-02, -1.5156e-02,  5.6878e-03],\n",
            "          [ 1.7775e-02, -9.5500e-03, -1.5188e-02, -1.2013e-02, -1.9037e-02],\n",
            "          [-7.6142e-03, -7.2561e-03, -2.5490e-02, -1.7663e-02, -3.1905e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.7015e-03, -1.4519e-02,  6.1004e-03, -1.1557e-02,  3.8301e-03],\n",
            "          [ 1.4494e-02, -8.1660e-03,  1.8944e-02,  8.3907e-03,  8.9140e-03],\n",
            "          [-1.0687e-02, -1.3461e-02, -2.0371e-03, -1.4742e-02,  5.8767e-03],\n",
            "          [ 2.6614e-03, -1.0580e-02, -1.4434e-02,  5.8188e-03,  9.7302e-03],\n",
            "          [-8.1596e-03,  5.4325e-03, -2.0252e-02, -1.2884e-02,  2.8181e-03]],\n",
            "\n",
            "         [[ 1.4103e-02,  1.5163e-02,  2.0628e-02, -8.5612e-03, -1.7696e-02],\n",
            "          [ 1.8509e-02,  2.5103e-02,  7.4827e-03,  1.0794e-02, -2.3268e-02],\n",
            "          [ 8.9445e-03,  9.1880e-03, -5.2668e-03, -1.3911e-02, -1.7659e-02],\n",
            "          [ 7.3323e-03, -1.8358e-02,  1.8552e-03, -2.2779e-02, -1.7706e-02],\n",
            "          [ 6.5819e-03, -1.2213e-03, -2.7337e-02, -2.5366e-02, -1.3982e-02]],\n",
            "\n",
            "         [[-7.0457e-03, -1.0501e-03,  9.3691e-03, -1.0694e-02, -3.5749e-03],\n",
            "          [-2.5042e-03,  1.4567e-02,  2.1129e-02, -1.8651e-02, -1.7299e-02],\n",
            "          [-1.1199e-02,  1.2840e-02, -2.0137e-03,  1.5246e-02,  4.5895e-03],\n",
            "          [-1.0371e-02,  1.3609e-02, -1.2563e-02,  1.2101e-02, -1.2708e-02],\n",
            "          [ 1.1218e-03,  6.2335e-03, -1.5414e-02, -1.7791e-02, -1.5477e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.5285e-02,  9.4080e-03, -2.9969e-03, -1.2854e-02,  9.3467e-03],\n",
            "          [-6.2074e-04,  1.1902e-02,  4.2273e-03,  1.0296e-02, -1.4985e-02],\n",
            "          [ 6.1335e-03,  6.8007e-03, -1.6556e-03,  7.2207e-03,  2.1651e-02],\n",
            "          [ 1.7689e-02,  2.4747e-02,  1.5823e-02, -4.7012e-03, -4.4543e-03],\n",
            "          [ 2.2519e-02,  1.8523e-02,  7.3233e-03,  1.5397e-03,  5.7043e-03]],\n",
            "\n",
            "         [[-2.4505e-02,  1.5829e-02, -1.7777e-02,  1.3812e-02,  6.6208e-04],\n",
            "          [ 1.9744e-02, -9.7078e-03, -1.8024e-02, -7.9211e-03, -1.7191e-02],\n",
            "          [-9.1897e-03,  2.0810e-02, -1.5862e-03, -1.8948e-02,  6.7375e-03],\n",
            "          [-8.1097e-03,  1.0405e-02,  2.2091e-02,  8.3450e-03,  1.0369e-02],\n",
            "          [ 1.1117e-03,  2.0754e-02, -2.0520e-02, -4.8730e-03, -1.3871e-02]],\n",
            "\n",
            "         [[ 5.1085e-02,  3.7367e-02,  1.1989e-02, -1.8960e-02, -3.0977e-02],\n",
            "          [ 1.2303e-02,  2.7913e-02,  1.1442e-02, -1.4535e-02, -1.1399e-02],\n",
            "          [-2.7948e-03,  3.5973e-03,  6.7188e-03, -2.5653e-02, -2.1721e-03],\n",
            "          [-7.8187e-03, -1.3023e-02, -2.5540e-03, -1.8896e-02, -1.8890e-02],\n",
            "          [ 4.5740e-03, -1.3030e-02, -6.9856e-03, -1.0838e-02, -2.8507e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.8097e-02,  1.7744e-02,  9.2122e-04, -5.5181e-03,  2.0584e-02],\n",
            "          [ 4.4451e-03,  1.8020e-03, -7.7556e-03,  2.4060e-02,  2.1169e-02],\n",
            "          [ 4.6685e-03, -5.4470e-04,  2.4200e-02,  2.6276e-02,  5.3745e-03],\n",
            "          [-1.1284e-02,  2.1083e-02,  1.1927e-02, -2.4201e-03,  8.4080e-03],\n",
            "          [ 3.2486e-02,  7.7161e-03,  2.9066e-02, -5.7273e-03, -9.6057e-03]],\n",
            "\n",
            "         [[ 5.4683e-02,  5.4020e-02,  2.9483e-02, -1.1350e-02, -3.4076e-02],\n",
            "          [ 5.5957e-02,  3.0613e-02,  5.7993e-03, -2.7659e-02, -3.6267e-02],\n",
            "          [ 3.7833e-02,  3.4327e-03,  7.0325e-03, -9.0899e-04, -4.0465e-02],\n",
            "          [-9.5708e-03, -2.2745e-02, -1.9448e-02, -5.1312e-03, -2.5929e-02],\n",
            "          [-1.4753e-02,  6.3325e-04, -9.8059e-03, -2.7061e-02, -3.6775e-02]],\n",
            "\n",
            "         [[-6.1148e-03, -2.2309e-02,  4.5482e-03, -1.3975e-02, -8.9333e-03],\n",
            "          [ 1.5027e-02,  1.4703e-02, -1.7891e-02, -6.5676e-03, -2.4190e-02],\n",
            "          [ 2.0298e-03, -1.9583e-02,  5.0675e-03, -1.7083e-02, -1.3207e-02],\n",
            "          [-1.5535e-02, -2.0056e-02, -2.3443e-02, -7.5428e-03, -7.5791e-03],\n",
            "          [-3.0017e-04,  7.6150e-03,  1.7799e-02, -3.0155e-03, -1.6957e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.8494e-02, -2.2559e-02,  1.5215e-02, -1.9211e-02, -4.4178e-03],\n",
            "          [-7.7238e-03,  1.8576e-02,  1.1168e-02, -1.7408e-02,  1.3157e-02],\n",
            "          [-6.3048e-03, -1.0101e-02, -3.6555e-03, -1.1377e-02, -1.5927e-02],\n",
            "          [ 8.1813e-04,  3.9476e-03, -1.3659e-03,  2.1440e-02, -1.1722e-02],\n",
            "          [-1.2090e-02, -6.5025e-03,  5.5825e-03,  2.0438e-02,  4.2516e-03]],\n",
            "\n",
            "         [[-2.2191e-02,  1.2349e-02, -2.5670e-02,  4.6509e-03, -2.5872e-02],\n",
            "          [-1.2948e-02, -1.0435e-02,  1.0143e-02, -8.4775e-03, -1.6526e-02],\n",
            "          [-9.8927e-04, -2.7280e-03, -1.3676e-03, -1.1394e-02, -4.7954e-03],\n",
            "          [ 1.2711e-02, -7.2398e-03, -1.6655e-02, -1.1095e-02,  1.0188e-02],\n",
            "          [-9.6363e-04, -7.6441e-03,  1.1444e-02, -1.6062e-02, -2.3390e-02]],\n",
            "\n",
            "         [[-3.8619e-02, -2.9366e-02, -2.3311e-02, -2.1810e-03, -3.1302e-03],\n",
            "          [-2.5125e-02, -2.9136e-02,  1.0714e-03, -8.2454e-03, -9.9448e-03],\n",
            "          [-2.6022e-02, -9.5028e-03, -9.9959e-03, -1.7444e-02,  1.4147e-02],\n",
            "          [-2.4372e-02, -1.5471e-02,  1.7572e-02, -1.1787e-02,  2.6379e-02],\n",
            "          [-1.9851e-02, -5.4913e-04,  1.1552e-02,  2.4591e-02,  1.3756e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.1533e-02,  2.4923e-02,  6.1978e-03,  1.6157e-02, -1.3137e-02],\n",
            "          [-9.8981e-03,  2.2720e-02, -1.0438e-02,  1.8065e-02,  1.1177e-02],\n",
            "          [ 7.7186e-03,  1.8408e-02,  1.6216e-02,  1.4357e-02,  9.2105e-03],\n",
            "          [ 2.3408e-02,  6.8229e-03,  1.3956e-03,  2.1383e-02,  2.2770e-02],\n",
            "          [ 2.8685e-02, -4.1410e-03,  5.5012e-03, -3.8138e-03,  1.6679e-03]],\n",
            "\n",
            "         [[-4.4611e-02, -3.4689e-02, -5.8834e-03, -2.1122e-02, -2.6595e-02],\n",
            "          [-1.9891e-02, -2.8817e-02, -1.8997e-02, -1.7317e-02, -1.2564e-02],\n",
            "          [-1.4116e-02, -6.3384e-03,  5.8027e-03,  3.1648e-02,  2.6978e-02],\n",
            "          [-2.2790e-02,  2.0071e-02,  3.0787e-02,  2.8919e-03,  3.2506e-02],\n",
            "          [ 2.1327e-02,  3.1178e-02,  3.0647e-02,  1.8006e-02,  2.9693e-02]],\n",
            "\n",
            "         [[ 1.3899e-02, -7.4885e-03,  1.2040e-02,  4.5118e-03, -3.7807e-03],\n",
            "          [ 9.9986e-03,  9.0188e-03, -1.2006e-02, -8.0633e-03,  5.7040e-03],\n",
            "          [ 2.3539e-04, -4.2677e-03, -1.5861e-02, -1.3256e-02, -2.5342e-02],\n",
            "          [ 7.3257e-03,  2.8581e-03, -1.0016e-02,  1.4203e-02,  1.6758e-02],\n",
            "          [-1.4074e-02,  1.6625e-02, -9.6905e-03,  1.0476e-02,  1.5323e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 7.0210e-03,  9.3959e-03, -1.5842e-02, -4.2837e-03,  1.4747e-02],\n",
            "          [-1.9748e-02, -2.0126e-02, -1.6741e-03,  1.6120e-02, -1.7281e-02],\n",
            "          [ 1.8030e-02,  1.4871e-02,  1.9353e-02, -7.5175e-03,  2.0509e-03],\n",
            "          [ 1.3244e-02,  2.9829e-03, -7.7015e-03,  1.6105e-02, -5.1158e-03],\n",
            "          [ 1.4328e-02,  2.0153e-02,  2.7088e-02,  1.5772e-02,  2.2177e-02]],\n",
            "\n",
            "         [[-1.0119e-02,  2.8227e-03,  2.3058e-02, -7.3688e-03,  1.3435e-02],\n",
            "          [-9.9203e-03,  5.7642e-03,  4.2076e-03,  5.6534e-03, -6.3397e-03],\n",
            "          [ 6.9439e-03,  1.4849e-02, -2.0082e-02, -1.9449e-02, -2.0849e-02],\n",
            "          [-6.2510e-04,  1.4208e-02, -4.7654e-03,  8.2866e-04, -3.1030e-03],\n",
            "          [ 9.6892e-03, -4.3031e-03, -6.7875e-03, -1.7477e-03,  1.3934e-02]],\n",
            "\n",
            "         [[ 1.1772e-02,  1.3799e-02, -2.1963e-02, -2.0474e-02, -2.5842e-02],\n",
            "          [ 1.3590e-02, -1.4155e-02, -2.2164e-02, -1.3812e-02, -1.5105e-02],\n",
            "          [ 7.9788e-03, -1.3152e-02,  3.2421e-04, -2.7210e-03, -2.0062e-02],\n",
            "          [-1.7966e-02,  1.1016e-02, -1.9433e-02,  1.4014e-02,  5.3730e-03],\n",
            "          [-7.3610e-03,  1.6781e-02, -5.9451e-04, -6.8166e-03,  1.3314e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0634e-02, -1.3702e-02,  8.5311e-04,  2.3714e-03, -2.5186e-02],\n",
            "          [-1.3422e-02,  1.6986e-02, -1.7536e-02, -1.8240e-02, -1.7565e-02],\n",
            "          [-3.1461e-03,  1.1766e-02, -6.8788e-03,  7.0996e-03, -1.2115e-02],\n",
            "          [ 6.0018e-03,  1.0954e-02,  2.1359e-02,  6.9634e-03,  8.5202e-03],\n",
            "          [-1.0470e-02, -9.7891e-03,  2.4214e-02,  1.2965e-02,  1.1453e-02]],\n",
            "\n",
            "         [[-2.1133e-02, -1.1875e-02, -2.1514e-02, -2.7998e-02, -2.0733e-02],\n",
            "          [ 1.7554e-02, -1.5461e-03, -1.2941e-02, -5.2769e-04, -3.1655e-02],\n",
            "          [-8.3784e-03,  1.1028e-02,  1.4427e-02,  1.5294e-02,  6.3284e-03],\n",
            "          [ 1.5802e-02,  1.0292e-03,  2.0024e-03, -7.6712e-03,  7.2092e-03],\n",
            "          [ 2.9579e-03,  2.6083e-02, -1.2227e-02,  1.3989e-02,  9.7608e-03]],\n",
            "\n",
            "         [[-2.1274e-02,  1.1468e-02, -8.0376e-03, -2.4169e-02, -2.4288e-02],\n",
            "          [ 1.2120e-02, -1.3626e-02, -2.0118e-02,  8.8144e-05,  6.5556e-03],\n",
            "          [ 1.3815e-02, -1.4573e-03,  9.0098e-03, -8.4147e-03,  2.4335e-02],\n",
            "          [ 7.9085e-03,  1.1863e-02,  8.5851e-03,  1.0141e-02,  9.7792e-03],\n",
            "          [-2.1930e-03,  2.8921e-02,  6.0052e-03,  7.0274e-03,  5.3789e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 3.9189e-02,  6.5782e-03,  1.4731e-02,  1.1663e-02,  1.6684e-02],\n",
            "          [ 2.2849e-03, -7.2015e-03,  1.5231e-02, -1.3647e-02, -3.8566e-03],\n",
            "          [ 4.1611e-03, -1.5612e-02,  5.3439e-03,  1.0274e-03,  1.8302e-02],\n",
            "          [ 6.0198e-03,  1.0387e-02, -4.9203e-03,  2.2728e-02, -4.8912e-03],\n",
            "          [-9.8519e-03,  2.1175e-02,  2.6044e-02, -1.0588e-02,  1.7982e-02]],\n",
            "\n",
            "         [[ 4.6773e-03,  8.6796e-03,  4.8926e-03, -1.1171e-02, -7.7565e-03],\n",
            "          [ 7.1906e-04, -6.8958e-03,  2.4152e-02, -7.1741e-03,  6.5577e-03],\n",
            "          [-3.5704e-03, -4.3085e-05, -8.9179e-03, -9.4227e-03,  2.4843e-02],\n",
            "          [ 2.4831e-02,  1.3891e-02, -1.0310e-02, -1.0020e-02, -7.8077e-03],\n",
            "          [-3.8993e-03,  1.9969e-02, -3.3246e-03,  1.9147e-02,  5.7952e-03]],\n",
            "\n",
            "         [[ 1.0288e-02,  2.7197e-02, -9.6701e-04, -1.2101e-02, -4.0336e-02],\n",
            "          [-1.7908e-02, -8.3874e-03, -7.3258e-03, -3.5213e-02, -3.2333e-02],\n",
            "          [-1.9924e-02, -2.6062e-02,  1.2497e-03, -1.4748e-02, -2.7147e-02],\n",
            "          [ 1.0900e-02, -2.9284e-02, -9.8701e-03,  1.9074e-03, -3.3825e-02],\n",
            "          [-2.9385e-02,  1.3154e-02, -6.5299e-03, -2.2979e-02, -1.7325e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.2708e-03,  2.7919e-02, -1.6712e-03, -8.8374e-03,  2.6029e-02],\n",
            "          [ 1.4923e-02,  8.9605e-03,  2.4353e-02,  1.0595e-02,  1.9605e-03],\n",
            "          [ 1.6710e-02,  7.3720e-03,  1.7633e-03, -6.8883e-03,  1.1522e-02],\n",
            "          [ 2.3274e-02,  2.8807e-02,  2.2497e-02,  1.0721e-02,  2.9971e-02],\n",
            "          [ 2.1569e-02,  3.6435e-03, -4.3760e-03, -9.5296e-03,  1.7992e-02]],\n",
            "\n",
            "         [[ 2.5681e-02,  3.3579e-03,  2.2167e-02, -1.4259e-02, -2.5027e-02],\n",
            "          [ 1.1488e-03,  1.4760e-02,  1.4876e-03,  1.7367e-03, -2.8329e-02],\n",
            "          [ 2.2842e-03, -1.4680e-02,  6.6127e-04,  6.6960e-03, -2.1434e-02],\n",
            "          [-1.0052e-03, -2.2273e-03, -2.2967e-02,  4.3940e-04,  9.6783e-03],\n",
            "          [ 6.7015e-04, -6.9267e-03, -3.9115e-03, -5.2163e-03, -1.3182e-02]],\n",
            "\n",
            "         [[ 2.4997e-02,  3.1290e-02,  2.5772e-02, -3.1427e-03,  1.2692e-02],\n",
            "          [-1.2287e-02,  6.5082e-03, -2.3038e-02,  9.3483e-03,  1.1228e-02],\n",
            "          [-1.6541e-02, -3.0477e-02,  8.6699e-03, -2.5977e-02, -9.3507e-03],\n",
            "          [ 2.4943e-02,  1.2407e-02, -1.2589e-02, -1.2848e-02, -9.9348e-03],\n",
            "          [ 6.6241e-03,  5.9394e-03, -1.2440e-02, -2.3264e-03, -1.7780e-02]]]],\n",
            "       device='cuda:0')), ('conv2.bias', tensor([ 0.1754, -0.0243, -0.0656,  0.0641, -0.0372, -0.0301,  0.0511, -0.0200,\n",
            "         0.0246, -0.0785, -0.0747,  0.0098,  0.0287, -0.0248, -0.0299,  0.1303,\n",
            "         0.0986,  0.0269, -0.0288,  0.0967, -0.0096,  0.0763,  0.1347, -0.0505,\n",
            "         0.0158,  0.0784, -0.0368,  0.0643,  0.0429, -0.0846, -0.0538,  0.0913,\n",
            "         0.1424,  0.0120,  0.0318, -0.1772,  0.0244,  0.0321,  0.0397, -0.0496,\n",
            "         0.0324, -0.0037,  0.0486,  0.0173,  0.0014,  0.0069, -0.0175,  0.0579,\n",
            "        -0.0191, -0.0242,  0.0073, -0.0214, -0.0527,  0.0184, -0.0368, -0.0025,\n",
            "        -0.0865, -0.0200, -0.1252, -0.0674,  0.0409, -0.0408,  0.0040, -0.0420],\n",
            "       device='cuda:0')), ('fc1.weight', tensor([[-0.0153, -0.0042,  0.0112,  ..., -0.0108, -0.0115,  0.0055],\n",
            "        [ 0.0049,  0.0142,  0.0100,  ..., -0.0144,  0.0053,  0.0038],\n",
            "        [ 0.0160,  0.0026,  0.0091,  ..., -0.0025, -0.0010, -0.0042],\n",
            "        ...,\n",
            "        [ 0.0101,  0.0117, -0.0077,  ..., -0.0116,  0.0133,  0.0040],\n",
            "        [ 0.0038,  0.0107,  0.0037,  ...,  0.0075, -0.0151, -0.0095],\n",
            "        [-0.0077,  0.0103,  0.0209,  ...,  0.0133, -0.0093,  0.0067]],\n",
            "       device='cuda:0')), ('fc1.bias', tensor([ 6.5225e-03,  1.9298e-02, -3.3420e-03,  1.4286e-02, -4.1474e-02,\n",
            "        -1.5707e-02,  1.4605e-02, -1.1222e-02,  5.3537e-03,  4.5909e-03,\n",
            "        -5.2510e-03,  2.6344e-02, -2.6708e-02, -1.5024e-03,  2.7616e-02,\n",
            "        -1.1499e-02, -2.0037e-02,  1.9303e-02, -1.7105e-03,  5.0997e-05,\n",
            "        -1.5052e-02, -2.1853e-02, -6.2469e-03, -2.0263e-02,  1.1101e-02,\n",
            "         6.0259e-03,  4.4091e-03,  1.5474e-02, -1.3835e-02, -1.5608e-02,\n",
            "         9.3835e-04,  3.2024e-03, -8.9944e-03, -1.1910e-02,  2.7123e-02,\n",
            "         4.9499e-03,  3.4554e-02, -1.2373e-02, -3.0001e-03, -6.0543e-03,\n",
            "         9.0954e-03, -2.8252e-04,  1.6239e-02,  2.1795e-02, -7.7038e-03,\n",
            "        -4.6044e-04,  1.1580e-02,  3.7312e-02,  2.7611e-02,  4.5144e-03,\n",
            "         1.3745e-02, -1.2362e-02, -2.2774e-03,  1.5051e-02,  3.0711e-03,\n",
            "        -1.1873e-02,  1.3826e-02,  4.3725e-02, -1.1237e-02, -2.6726e-02,\n",
            "         2.9289e-02,  2.9223e-02,  5.0761e-03,  3.8695e-03, -9.9560e-04,\n",
            "         1.1031e-02,  3.0449e-02,  5.5045e-03,  2.8366e-02, -1.4355e-02,\n",
            "         5.7224e-05, -3.6719e-03, -2.6305e-02, -8.7523e-03,  1.0372e-02,\n",
            "         1.2948e-02, -7.1746e-03, -8.7588e-03,  1.3140e-02,  2.4665e-02,\n",
            "         8.5405e-03,  8.0477e-03, -3.1290e-02, -8.6184e-03,  1.1317e-02,\n",
            "        -6.6683e-03,  7.3276e-04,  2.1141e-02, -6.1017e-03, -5.0380e-03,\n",
            "         1.4105e-02, -1.4570e-02, -1.4554e-02, -1.1836e-02,  3.0775e-02,\n",
            "         4.1350e-02,  8.3950e-03, -8.6947e-04,  1.7605e-02,  4.2910e-02,\n",
            "         3.2503e-04,  1.1712e-02, -1.0762e-03,  8.3293e-03, -7.4603e-03,\n",
            "        -8.2907e-03,  3.1935e-02,  1.8644e-02,  4.0787e-03, -1.9069e-02,\n",
            "        -1.4403e-02,  3.6785e-02,  4.6327e-02, -6.1943e-03, -1.1472e-02,\n",
            "         7.5167e-03, -2.2455e-03,  5.2118e-03,  4.3319e-03, -1.0541e-02,\n",
            "         1.6151e-02, -1.6262e-02,  7.9921e-03,  1.1858e-02,  1.9558e-02,\n",
            "        -1.8696e-02, -1.2684e-02, -2.0807e-02,  1.2710e-02,  1.1677e-02,\n",
            "        -3.2110e-03,  1.3009e-02,  4.2777e-02,  1.5677e-02, -1.7867e-03,\n",
            "         4.1981e-03,  1.6565e-02, -3.5541e-03, -2.4973e-02,  8.5929e-03,\n",
            "         2.0868e-02,  5.3836e-03, -2.5029e-03,  1.9197e-02,  1.0296e-02,\n",
            "         8.5525e-04,  5.7739e-03, -1.1320e-02, -2.3921e-04,  1.5934e-02,\n",
            "        -4.1711e-03, -1.1146e-02,  5.4453e-03, -1.2013e-02,  1.6806e-02,\n",
            "        -1.8901e-02,  8.5246e-03,  6.9358e-03,  1.1532e-03,  2.2637e-03,\n",
            "         8.6171e-03,  1.8138e-02, -1.5400e-02, -1.9417e-02,  5.1938e-02,\n",
            "        -9.1870e-03, -2.1887e-02,  7.2830e-03, -2.9672e-03, -2.0430e-02,\n",
            "         2.9923e-02, -1.1001e-02,  8.0458e-03, -2.8786e-02, -6.7604e-03,\n",
            "        -7.8050e-03,  9.4427e-03, -1.5607e-02, -2.1991e-02, -2.4529e-02,\n",
            "        -1.8792e-02, -3.8717e-03,  1.9955e-03,  2.2233e-02, -2.0248e-02,\n",
            "         1.9529e-02, -7.6045e-03,  1.8140e-02,  1.3076e-02,  2.4824e-02,\n",
            "        -4.8953e-03, -1.4480e-02, -1.6895e-02,  1.3309e-02,  9.3481e-03,\n",
            "        -9.2534e-03,  6.3500e-03, -5.2775e-03,  1.7509e-02,  1.3813e-02,\n",
            "         9.1437e-03,  1.3183e-02, -7.0669e-03, -1.8701e-02, -1.7036e-02,\n",
            "         6.0806e-03,  5.5520e-03, -2.0708e-02, -9.3873e-03, -1.5362e-02,\n",
            "         2.4025e-02, -2.7113e-02,  3.2397e-03, -3.6147e-02, -8.1672e-03,\n",
            "         1.1706e-02,  1.4182e-02, -1.5617e-02,  2.0865e-02,  8.8615e-04,\n",
            "         4.1637e-04,  1.4549e-02, -8.9194e-03,  1.8874e-02, -1.9890e-02,\n",
            "         7.4213e-03,  4.4932e-02, -2.6163e-03, -1.7291e-02,  1.1242e-02,\n",
            "         1.3832e-02, -2.4036e-02, -5.7003e-03, -6.1556e-03, -5.9108e-03,\n",
            "         8.8711e-03,  2.5941e-02, -2.8345e-03, -2.0392e-02,  2.4573e-02,\n",
            "        -4.1470e-02, -6.4836e-03, -1.8607e-03, -3.1877e-02,  1.5730e-03,\n",
            "        -1.7542e-02,  6.5249e-02, -1.6547e-02, -9.2672e-03,  6.3159e-03,\n",
            "        -1.1631e-03, -9.9174e-03, -9.8445e-03, -6.6836e-03, -1.3592e-02,\n",
            "         2.5968e-02,  2.1873e-02,  4.1792e-02,  3.0975e-02,  2.8806e-02,\n",
            "        -6.9366e-03,  1.9318e-03,  2.1572e-02,  2.6281e-02, -6.8936e-03,\n",
            "        -2.3312e-02, -8.5713e-03,  1.2117e-02,  1.1901e-02, -7.8626e-03,\n",
            "         7.0815e-03,  7.3729e-03,  1.5806e-02,  3.4919e-02, -3.0459e-02,\n",
            "         4.2296e-03, -2.0637e-02,  2.9445e-02,  7.5021e-03, -8.7952e-03,\n",
            "         1.8532e-02,  1.4658e-02,  2.9757e-02,  3.1014e-03,  7.0746e-03,\n",
            "        -1.3495e-02,  2.0393e-02, -1.3035e-02,  1.1485e-02, -9.8577e-03,\n",
            "        -2.2028e-02,  1.8523e-02,  2.1410e-02,  2.5792e-03,  2.0128e-03,\n",
            "         1.3291e-02,  5.3028e-03,  1.3173e-02, -2.0080e-03, -4.2391e-03,\n",
            "         1.0844e-02, -9.6505e-03,  1.9107e-02,  3.1009e-02,  1.8846e-02,\n",
            "         4.3837e-02,  1.7874e-02, -2.6395e-02, -3.2176e-02, -5.7275e-03,\n",
            "        -1.5190e-02,  8.1898e-03, -1.6132e-02,  9.4076e-03,  2.3977e-02,\n",
            "         3.8707e-02, -2.6995e-02,  7.0003e-04, -1.7721e-03, -5.3130e-03,\n",
            "         1.2144e-02, -2.7616e-02,  8.5506e-03,  1.6011e-02,  1.2731e-02,\n",
            "         5.0074e-02,  2.2234e-02,  2.6237e-02,  1.0814e-02,  2.7543e-02,\n",
            "         2.1343e-02, -1.1906e-02, -8.3440e-03, -1.1341e-02, -3.2172e-03,\n",
            "         9.2291e-03,  2.2828e-02,  1.5868e-02,  7.5840e-03,  2.4456e-02,\n",
            "        -3.7708e-02,  4.7447e-03,  3.7234e-02,  3.1426e-02,  3.6099e-03,\n",
            "         1.5518e-02,  2.3546e-02, -3.0261e-03, -4.3997e-03,  9.3651e-03,\n",
            "        -8.1191e-03,  2.0845e-02, -3.1629e-02, -2.0305e-02, -3.5176e-03,\n",
            "        -8.7374e-03, -1.7460e-02,  1.1077e-02, -2.7610e-04, -5.5433e-03,\n",
            "         2.3405e-02,  1.7319e-02, -1.6226e-02,  6.1647e-03,  2.5885e-02,\n",
            "         1.6554e-02, -8.1709e-03,  9.2109e-03, -1.2883e-02, -6.4251e-03,\n",
            "         9.4280e-03,  2.1171e-02,  8.9952e-03,  2.3709e-03,  2.0738e-02,\n",
            "        -3.5638e-02, -1.0046e-02,  1.4867e-02, -1.1552e-02, -5.6133e-03,\n",
            "        -3.9043e-02,  1.6805e-02, -2.1771e-03, -1.3609e-03], device='cuda:0')), ('fc2.weight', tensor([[-0.0182, -0.0156,  0.0035,  ..., -0.0338, -0.0101, -0.0101],\n",
            "        [ 0.0240,  0.0250, -0.0257,  ...,  0.0059,  0.0273,  0.0010],\n",
            "        [-0.0507,  0.0054,  0.0083,  ..., -0.0085,  0.0149, -0.0281],\n",
            "        ...,\n",
            "        [ 0.0095, -0.0362, -0.0353,  ..., -0.0426,  0.0044, -0.0340],\n",
            "        [-0.0242,  0.0314, -0.0151,  ..., -0.0457,  0.0121, -0.0122],\n",
            "        [-0.0461,  0.0392,  0.0128,  ...,  0.0253,  0.0160,  0.0211]],\n",
            "       device='cuda:0')), ('fc2.bias', tensor([-0.0113,  0.0593, -0.0344,  0.0208, -0.0025,  0.0179, -0.0147, -0.0029,\n",
            "        -0.0034,  0.0147,  0.0249,  0.0102, -0.0112,  0.0122,  0.0536,  0.0365,\n",
            "         0.0520,  0.0453, -0.0006,  0.0151, -0.0320,  0.1051,  0.0144,  0.0872,\n",
            "        -0.0016, -0.0348, -0.0616,  0.0580, -0.0182,  0.0167, -0.0278,  0.0739,\n",
            "         0.0109,  0.0068, -0.0363, -0.0307, -0.0689, -0.0002, -0.0249,  0.0667,\n",
            "        -0.0006,  0.0155, -0.0514,  0.1170,  0.1127,  0.0344,  0.0094,  0.0066,\n",
            "        -0.0369, -0.0347,  0.0109,  0.1022,  0.1075, -0.0072, -0.0406,  0.0159,\n",
            "         0.0112,  0.0562,  0.0856,  0.0665,  0.0549,  0.0518, -0.0117,  0.0658,\n",
            "         0.0273, -0.0229, -0.0204, -0.0082,  0.1319,  0.0225,  0.0444, -0.0268,\n",
            "         0.0239,  0.0451,  0.0010, -0.0571, -0.0067,  0.0642,  0.0370, -0.0454,\n",
            "        -0.0243,  0.0319,  0.0415, -0.1020, -0.0375, -0.0030,  0.0616, -0.0002,\n",
            "        -0.0250,  0.0340,  0.0215,  0.0411,  0.0004,  0.1099,  0.0265,  0.0343,\n",
            "        -0.0501, -0.0261,  0.0519, -0.0156, -0.0529, -0.0420,  0.0097, -0.0487,\n",
            "        -0.0004, -0.0453,  0.0212, -0.0415,  0.0106,  0.0335,  0.0114,  0.0619,\n",
            "        -0.0075, -0.0291, -0.0098,  0.0123,  0.0758,  0.0313,  0.0065,  0.0356,\n",
            "         0.0607, -0.0172, -0.0196,  0.0697,  0.0143, -0.0302, -0.0525,  0.0201,\n",
            "         0.0142,  0.0498,  0.0274, -0.0478, -0.0046,  0.0270,  0.0170,  0.0242,\n",
            "        -0.0386, -0.0191,  0.0218, -0.0449, -0.0230,  0.0624,  0.0782,  0.0074,\n",
            "        -0.0367, -0.0020,  0.0618,  0.0365, -0.0334, -0.0214,  0.0278, -0.0053,\n",
            "         0.0032, -0.0162, -0.0083,  0.0626,  0.0174, -0.0072,  0.0206,  0.0081,\n",
            "         0.0431,  0.0335,  0.0591, -0.0346,  0.0404, -0.0300, -0.0129, -0.0738,\n",
            "        -0.0464,  0.0214,  0.0566, -0.0076, -0.0308, -0.0118,  0.0286, -0.0509,\n",
            "        -0.0009,  0.0260,  0.0578, -0.0394,  0.0380, -0.0128,  0.0279, -0.0096,\n",
            "         0.0186, -0.0089, -0.0312,  0.0580,  0.0721, -0.0191,  0.0202,  0.0192],\n",
            "       device='cuda:0')), ('fc3.weight', tensor([[-0.1501, -0.0082,  0.0696,  ..., -0.0559,  0.0837,  0.0240],\n",
            "        [ 0.1139, -0.0872,  0.0252,  ...,  0.1145, -0.0348,  0.0706],\n",
            "        [ 0.0079, -0.0153,  0.1064,  ..., -0.0088, -0.0515,  0.0110],\n",
            "        ...,\n",
            "        [ 0.0237, -0.0274, -0.0407,  ...,  0.0426, -0.1009, -0.0460],\n",
            "        [ 0.0025,  0.0232,  0.0611,  ..., -0.0357,  0.0134,  0.0085],\n",
            "        [ 0.0426, -0.0144,  0.0189,  ...,  0.0515, -0.0446,  0.0338]],\n",
            "       device='cuda:0')), ('fc3.bias', tensor([-0.0462, -0.0544,  0.0743,  0.0190,  0.0850,  0.0773, -0.0871, -0.0265,\n",
            "         0.0696, -0.0421,  0.0142, -0.0719, -0.0376, -0.0306, -0.1152, -0.0265,\n",
            "         0.0415,  0.0203,  0.0703, -0.0152, -0.1320,  0.0028, -0.0103,  0.1964,\n",
            "        -0.1216,  0.0317,  0.0240,  0.1571,  0.0400, -0.1195,  0.0004,  0.0686,\n",
            "         0.0849,  0.0969,  0.1440, -0.0272,  0.1058, -0.0380,  0.1477, -0.0980,\n",
            "        -0.0243, -0.1613,  0.0315,  0.0773,  0.1189, -0.1118,  0.0198, -0.0837,\n",
            "        -0.1265, -0.0601,  0.0300, -0.0209, -0.1004, -0.1097, -0.0495,  0.0566,\n",
            "        -0.0565, -0.0013, -0.0975,  0.0063, -0.0034,  0.0950, -0.2001,  0.1371,\n",
            "        -0.0063,  0.0350, -0.0212,  0.0423,  0.0345,  0.0192, -0.1901,  0.0691,\n",
            "         0.0213, -0.0521,  0.1287, -0.1557,  0.0324,  0.0456,  0.0870,  0.0536,\n",
            "         0.1573, -0.0038, -0.1102, -0.2372, -0.0061, -0.0322, -0.0683, -0.0333,\n",
            "         0.0685,  0.0167, -0.0235,  0.0227, -0.1609, -0.0080,  0.2072, -0.1676,\n",
            "         0.0582, -0.0286, -0.1149, -0.0662], device='cuda:0'))])\n",
            "Local Step 01: Test Loss: 3.256975347, Test Accuracy: 22.730\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 3.252691123, Training Accuracy: 20.156\n",
            "Worker 1, [02/04]: Training Loss: 3.051511727, Training Accuracy: 23.800\n",
            "Worker 1, [03/04]: Training Loss: 2.928677890, Training Accuracy: 26.208\n",
            "Worker 1, [04/04]: Training Loss: 2.791217345, Training Accuracy: 29.160\n",
            "Time taken for training worker 1: 0:01:17.453837\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/04]: Training Loss: 3.244619403, Training Accuracy: 20.236\n",
            "Worker 2, [02/04]: Training Loss: 3.050638059, Training Accuracy: 24.156\n",
            "Worker 2, [03/04]: Training Loss: 2.932536809, Training Accuracy: 26.672\n",
            "Worker 2, [04/04]: Training Loss: 2.806246983, Training Accuracy: 28.512\n",
            "Time taken for training worker 2: 0:01:16.529471\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.011539\n",
            "OrderedDict([('conv1.weight', tensor([[[[ 5.7278e-02, -3.7433e-02,  8.0645e-02, -4.1070e-02,  6.4150e-02],\n",
            "          [-4.2163e-02, -5.9330e-02, -1.9989e-02, -1.0015e-01, -4.9669e-02],\n",
            "          [-1.0242e-01, -1.0595e-01,  3.7580e-02, -6.9968e-02, -2.1801e-02],\n",
            "          [ 6.9823e-02,  1.2153e-01,  1.2047e-02,  2.6402e-02,  2.6582e-02],\n",
            "          [ 4.2379e-02,  4.6248e-03,  6.9996e-02,  9.2506e-02,  6.1081e-02]],\n",
            "\n",
            "         [[ 7.9003e-03, -3.1954e-02,  1.1443e-01, -2.8099e-02,  4.8362e-02],\n",
            "          [ 3.7963e-02, -3.5100e-02, -6.6173e-02, -4.2828e-02, -1.0351e-01],\n",
            "          [-5.8685e-02, -1.1043e-01,  1.7435e-02, -5.7550e-02, -1.0358e-02],\n",
            "          [ 2.0779e-02, -3.4324e-02,  7.3689e-02,  4.9779e-02, -4.1217e-02],\n",
            "          [-6.7149e-03,  3.1711e-03,  4.4202e-02, -1.5516e-02, -4.0896e-02]],\n",
            "\n",
            "         [[ 5.1142e-02, -9.7707e-03,  1.3876e-01,  7.6418e-02,  2.3542e-02],\n",
            "          [ 1.1861e-01,  9.1441e-02,  1.1777e-01,  9.6716e-02,  3.5202e-02],\n",
            "          [ 1.0286e-01, -2.3466e-02, -3.3462e-02,  7.4429e-02, -1.6463e-02],\n",
            "          [ 2.8767e-02, -8.0954e-02, -3.8398e-02,  5.8982e-02, -3.9078e-02],\n",
            "          [-5.4873e-02, -1.3022e-01, -1.2835e-01, -8.3192e-02, -1.1378e-01]]],\n",
            "\n",
            "\n",
            "        [[[-5.2294e-02, -1.2844e-01,  3.0147e-02, -2.9648e-02, -1.3145e-01],\n",
            "          [-2.7091e-02, -9.0228e-02, -7.1325e-02, -8.4201e-04, -6.1799e-02],\n",
            "          [ 3.1192e-02, -2.5785e-02, -4.4252e-02,  1.5972e-01, -4.7364e-02],\n",
            "          [ 5.9796e-02,  3.2971e-02, -7.4030e-02,  1.4194e-01,  3.8231e-02],\n",
            "          [-4.3810e-02,  1.5577e-01, -3.8905e-02,  1.0445e-01,  9.4506e-02]],\n",
            "\n",
            "         [[ 4.9111e-02, -3.4960e-02,  5.1180e-02, -5.9593e-05, -1.3450e-02],\n",
            "          [ 7.8298e-02, -7.7079e-02, -1.0884e-01,  1.0585e-01, -1.1521e-01],\n",
            "          [-2.1460e-02,  2.2696e-02,  5.6096e-03,  1.3355e-01, -1.3896e-02],\n",
            "          [-4.7556e-02,  8.6025e-02, -1.2551e-01,  6.4792e-02, -1.1454e-01],\n",
            "          [-7.2197e-02,  9.1910e-02, -1.4063e-01, -3.3661e-02,  3.8400e-02]],\n",
            "\n",
            "         [[ 6.0982e-02, -2.1969e-02,  1.3784e-01,  6.3646e-02, -8.0708e-02],\n",
            "          [ 6.5723e-02,  2.6796e-02, -7.0217e-02,  5.9570e-02,  3.6215e-02],\n",
            "          [-5.1970e-02,  4.2655e-02, -4.7210e-02,  6.1841e-03,  1.3940e-03],\n",
            "          [ 4.7178e-02, -3.6151e-02, -5.2507e-03,  1.4020e-02, -7.5429e-02],\n",
            "          [-1.3120e-01,  7.0286e-02, -3.4710e-02, -3.6603e-02, -5.4951e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.6645e-02, -1.1606e-01, -1.0629e-01, -3.2111e-02, -1.4492e-02],\n",
            "          [-8.0682e-02, -8.8395e-02, -6.6797e-02, -1.1433e-01, -3.6113e-02],\n",
            "          [-9.8897e-02,  3.2108e-02, -2.9699e-02, -5.3351e-02, -6.0195e-02],\n",
            "          [-2.4985e-02,  3.4603e-03, -5.0037e-02, -6.3862e-02,  5.1471e-02],\n",
            "          [-2.2834e-02, -6.7946e-02, -6.3605e-02,  3.6244e-02,  7.1664e-02]],\n",
            "\n",
            "         [[-3.1830e-03, -3.1135e-02, -5.2039e-02, -8.3071e-02, -6.5253e-02],\n",
            "          [-2.5116e-02, -4.4416e-02,  4.4081e-03, -4.5744e-03, -5.6633e-02],\n",
            "          [ 3.9382e-02,  2.4267e-02, -4.1179e-02, -2.9561e-03, -2.8161e-02],\n",
            "          [ 7.2630e-02, -3.2141e-02,  5.0135e-02,  2.8620e-02,  2.5901e-02],\n",
            "          [ 4.7913e-02,  4.5051e-02,  5.1127e-02,  4.6395e-02, -5.8853e-02]],\n",
            "\n",
            "         [[ 1.5213e-01,  1.8114e-01,  1.6777e-01,  1.0611e-01,  1.2994e-01],\n",
            "          [ 1.5390e-01,  8.3919e-02,  5.5513e-02,  1.1507e-01,  6.9846e-02],\n",
            "          [ 6.3361e-02,  7.5853e-02,  1.0485e-01,  1.1242e-01, -2.4915e-02],\n",
            "          [ 7.7216e-02, -3.0885e-02, -2.1837e-02, -1.7027e-02, -6.1504e-02],\n",
            "          [ 5.2978e-03,  7.4130e-02,  3.9451e-02, -9.8561e-02, -1.0762e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-5.4366e-02, -1.2962e-01, -2.4502e-01, -2.0459e-01, -1.3075e-01],\n",
            "          [-2.8062e-02, -5.7688e-02, -1.1418e-01, -2.4797e-01, -1.4289e-01],\n",
            "          [ 2.1664e-01,  1.8749e-01,  3.4266e-02, -1.0621e-01, -1.2997e-01],\n",
            "          [ 2.8047e-01,  3.5498e-01,  2.1457e-01, -1.4909e-01, -1.5628e-01],\n",
            "          [ 3.1627e-01,  3.2976e-01,  1.8378e-01, -1.0008e-01, -1.9911e-01]],\n",
            "\n",
            "         [[ 8.3634e-02,  9.0537e-02,  1.0783e-01,  1.3446e-01,  1.5584e-02],\n",
            "          [ 1.2735e-02,  6.2860e-02, -1.2257e-02,  6.0246e-02,  2.8568e-02],\n",
            "          [-2.0001e-02, -1.0377e-01, -1.3699e-02,  5.8483e-02, -2.4228e-02],\n",
            "          [-6.4005e-02, -9.1409e-02, -1.3596e-01,  3.9102e-02, -2.3219e-02],\n",
            "          [-1.2745e-01, -8.6038e-02, -8.4982e-02, -2.4007e-02, -1.2533e-02]],\n",
            "\n",
            "         [[-7.7413e-02,  5.0266e-02,  1.1494e-01,  1.7358e-01,  8.0362e-02],\n",
            "          [-5.7832e-02, -7.5494e-02,  1.2045e-01,  1.8645e-01,  1.6127e-01],\n",
            "          [-2.0025e-01, -1.1541e-01, -1.9275e-02,  1.4760e-01,  1.5957e-01],\n",
            "          [-2.5604e-01, -3.2645e-01, -1.6954e-01,  1.2320e-01,  2.2497e-01],\n",
            "          [-2.7948e-01, -2.9626e-01, -1.7297e-01,  1.4823e-01,  2.6693e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.0632e-01, -6.0940e-02, -1.2654e-01, -2.0577e-01, -8.9827e-02],\n",
            "          [-7.0724e-02, -1.2331e-01, -1.7709e-01, -1.7029e-01, -1.9590e-01],\n",
            "          [-1.2986e-01, -9.4484e-02, -1.3331e-01, -2.1395e-01, -1.7924e-01],\n",
            "          [-1.3682e-01, -8.6063e-02, -6.8047e-02, -7.5278e-02, -1.3431e-01],\n",
            "          [-1.0394e-01, -1.4842e-01, -1.1349e-01, -4.1205e-02, -3.1809e-02]],\n",
            "\n",
            "         [[ 2.4015e-02,  2.6604e-02, -3.7368e-02, -3.2358e-02, -1.5238e-02],\n",
            "          [ 2.9525e-02, -2.8272e-02,  3.3348e-03, -5.0556e-02, -9.0754e-02],\n",
            "          [ 1.9471e-02,  5.5295e-02,  5.5332e-02,  1.0090e-02,  2.5997e-02],\n",
            "          [-1.5430e-02,  3.5800e-02,  4.1444e-03, -1.1194e-02,  6.7027e-02],\n",
            "          [-1.6317e-02,  5.8904e-02,  3.9783e-02,  7.1099e-02,  9.6013e-02]],\n",
            "\n",
            "         [[ 1.0295e-01,  1.3921e-01,  1.2964e-01,  1.7390e-01,  1.6730e-01],\n",
            "          [ 1.3690e-01,  1.1148e-01,  5.2103e-02,  2.1581e-02,  1.1576e-01],\n",
            "          [ 1.3462e-01,  7.1684e-02,  1.1241e-01,  1.0375e-01,  1.4389e-02],\n",
            "          [ 7.0833e-02,  2.8429e-02,  9.6626e-02,  1.2439e-01,  1.1750e-01],\n",
            "          [ 9.2163e-02,  5.3146e-02,  8.6403e-02,  1.0709e-01,  1.3196e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0708e-02,  4.6581e-02,  1.5904e-01, -9.7302e-02, -2.0377e-02],\n",
            "          [ 1.2052e-03,  6.5084e-02,  6.4451e-03,  9.7049e-02, -3.4194e-02],\n",
            "          [-5.0647e-02, -5.6015e-02, -8.9092e-02,  3.0393e-02,  1.0505e-01],\n",
            "          [ 1.4992e-02, -1.1393e-01, -1.2499e-01, -5.4986e-02,  2.8949e-04],\n",
            "          [-4.0417e-02,  3.0931e-02, -5.5326e-02, -9.3894e-02, -5.5912e-02]],\n",
            "\n",
            "         [[ 1.8419e-02,  2.8081e-02,  1.6781e-01, -6.5386e-02, -7.3804e-02],\n",
            "          [ 8.3528e-02, -7.5316e-04,  1.6003e-01,  1.2028e-01, -9.1558e-03],\n",
            "          [-4.9588e-02,  3.1722e-02, -5.6085e-02,  1.2640e-01,  1.0221e-01],\n",
            "          [ 2.5514e-02, -7.5025e-02, -2.5335e-02,  1.0489e-02,  5.1004e-02],\n",
            "          [-5.7681e-02, -2.3814e-02, -5.0400e-02, -3.7026e-02, -5.2994e-02]],\n",
            "\n",
            "         [[ 1.7046e-03, -1.1776e-02,  1.0037e-01, -1.5881e-03, -1.1768e-01],\n",
            "          [ 2.4729e-02,  4.8762e-02,  8.8922e-03,  6.0111e-02,  2.4810e-02],\n",
            "          [-7.9398e-02,  7.6485e-02, -1.7794e-02,  7.8211e-02,  8.0455e-02],\n",
            "          [ 9.9107e-03, -6.6134e-02, -8.5364e-02, -3.2186e-02,  6.8394e-02],\n",
            "          [-2.1949e-02,  3.4994e-02, -5.9326e-02, -1.2789e-01, -9.7115e-02]]]],\n",
            "       device='cuda:0')), ('conv1.bias', tensor([-0.1822,  0.5640,  0.3689,  0.5599, -0.2501, -0.1048, -0.1917, -0.1070,\n",
            "         0.2311, -0.2171,  0.1876, -0.1229, -0.0274, -0.1695,  0.5428, -0.4250,\n",
            "        -0.3072, -0.2510, -0.3343, -0.1986, -0.0218, -0.5021, -0.2607,  0.2768,\n",
            "        -0.2702, -0.2524, -0.1052, -0.6983, -0.2456, -0.2283, -0.4248, -0.0410,\n",
            "        -0.2416, -0.1438, -0.2707, -0.1888, -0.3515, -0.2055, -0.1824, -0.3673,\n",
            "        -0.2351, -0.1609, -0.4220, -0.5427,  0.2556, -0.2245, -0.5862, -0.5160,\n",
            "        -0.2473,  0.0432,  0.0539, -0.1628,  0.1552,  0.0566, -0.1254, -0.2339,\n",
            "        -0.0215, -0.1861, -0.2998, -0.2218, -0.4479, -0.3552, -0.0643,  0.0411],\n",
            "       device='cuda:0')), ('conv2.weight', tensor([[[[-1.1496e-02,  8.9891e-05,  3.6445e-03, -2.4952e-03, -3.0883e-03],\n",
            "          [-1.9349e-02,  9.9033e-03, -1.1325e-02, -2.0608e-03, -5.7110e-03],\n",
            "          [-2.0247e-02, -1.5038e-02, -1.5517e-02, -1.6525e-02,  2.8011e-03],\n",
            "          [ 5.5397e-03, -1.0008e-02, -4.6582e-03, -2.4867e-02, -4.8318e-03],\n",
            "          [-2.7168e-03, -1.2608e-02, -1.8237e-02, -1.7652e-02, -2.1127e-02]],\n",
            "\n",
            "         [[ 4.0628e-02,  3.7980e-02,  3.1498e-02,  1.1525e-02,  1.4422e-02],\n",
            "          [ 4.3788e-02,  4.2708e-02,  3.6680e-02,  3.7606e-02,  3.8887e-02],\n",
            "          [ 6.2102e-02,  4.8243e-02,  5.5477e-02,  5.3441e-02,  2.6410e-02],\n",
            "          [ 5.2716e-02,  5.3564e-02,  3.5513e-02,  2.4929e-02,  2.7973e-02],\n",
            "          [ 5.3981e-02,  5.1482e-02,  3.4530e-02,  3.0230e-02,  1.7130e-02]],\n",
            "\n",
            "         [[ 3.8170e-02,  3.2206e-02,  3.6288e-02,  4.0945e-02,  2.4092e-02],\n",
            "          [ 2.8884e-02,  2.9138e-02,  3.1077e-02,  1.5079e-03,  2.7625e-02],\n",
            "          [ 3.0226e-02,  2.3930e-02,  2.8312e-03, -9.0924e-03,  2.4506e-02],\n",
            "          [ 1.4804e-02,  2.1793e-02,  5.8302e-03,  1.1329e-02,  1.7747e-02],\n",
            "          [ 2.3577e-02,  1.4323e-02,  2.0538e-02,  4.0258e-02,  4.1786e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.0555e-02, -7.5663e-03, -2.1161e-02, -6.2712e-03,  1.1394e-02],\n",
            "          [-7.4867e-03, -3.1813e-02, -3.4729e-02, -4.9747e-03, -5.6052e-03],\n",
            "          [-1.2706e-02, -2.8587e-02, -2.5726e-02, -3.2079e-02, -1.9020e-02],\n",
            "          [-2.5882e-02, -3.3881e-02, -2.6738e-02, -2.4326e-02, -3.4380e-02],\n",
            "          [-2.1778e-02, -3.6542e-02, -4.4952e-02, -2.3356e-02, -2.2181e-02]],\n",
            "\n",
            "         [[ 3.9500e-02,  1.4615e-02, -8.3012e-04, -3.2657e-02, -1.2906e-02],\n",
            "          [-3.7715e-03, -1.9334e-02, -2.6452e-02, -3.5151e-02, -4.8834e-02],\n",
            "          [-1.1887e-02, -1.0708e-02, -2.0577e-02, -2.4249e-02, -5.3886e-02],\n",
            "          [-5.0880e-04, -1.1484e-03, -3.1012e-02, -4.3796e-02, -5.7325e-02],\n",
            "          [ 4.4635e-03, -1.1283e-02, -1.7826e-02, -3.6572e-02, -4.3199e-02]],\n",
            "\n",
            "         [[ 3.2124e-02,  3.1290e-02,  2.4587e-02, -3.7883e-03,  1.4279e-02],\n",
            "          [ 8.8043e-03,  2.6830e-02,  1.1679e-02,  2.5621e-02, -7.8376e-03],\n",
            "          [ 2.2639e-03,  1.9439e-02,  7.7069e-03,  7.7939e-03, -1.0422e-02],\n",
            "          [ 2.2398e-03,  1.4229e-02, -6.6719e-03, -1.3268e-02, -1.2077e-02],\n",
            "          [-3.3712e-03,  2.7171e-03, -2.4250e-02, -1.3423e-02, -4.0983e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.4233e-03,  2.1112e-02,  1.2360e-02,  1.1819e-02, -3.1230e-03],\n",
            "          [ 4.3523e-03,  1.8169e-02,  1.9022e-02, -6.3613e-03,  1.3082e-02],\n",
            "          [ 6.0472e-03, -3.8172e-03,  1.2779e-02, -7.4411e-03,  1.1584e-02],\n",
            "          [-3.4215e-03,  4.1662e-03, -6.2950e-03,  1.6020e-03, -7.8674e-03],\n",
            "          [ 6.9700e-03,  3.1059e-03,  5.5343e-03, -1.2873e-02, -5.7110e-03]],\n",
            "\n",
            "         [[-1.9659e-02,  3.3806e-03,  1.2194e-02, -6.0473e-03,  7.0964e-03],\n",
            "          [-7.9769e-03, -1.2419e-02,  1.3924e-02, -9.3733e-03,  1.4833e-03],\n",
            "          [ 9.5101e-03,  1.4688e-02,  1.7981e-03,  1.2792e-02,  8.7782e-03],\n",
            "          [ 6.1118e-03,  5.9538e-03, -1.0813e-02, -8.4789e-03, -7.4908e-03],\n",
            "          [-6.3161e-03, -9.2819e-04,  1.3036e-05, -1.0527e-02,  9.4597e-03]],\n",
            "\n",
            "         [[ 3.4058e-02,  3.6542e-02,  4.1150e-02,  2.6594e-02, -1.1629e-02],\n",
            "          [ 2.2922e-02,  3.5936e-02,  1.9442e-03, -2.1990e-02, -2.1875e-02],\n",
            "          [ 1.9503e-02,  6.1958e-03, -1.2460e-02, -2.2923e-02, -1.3020e-02],\n",
            "          [ 1.7339e-02, -1.2232e-02, -2.2609e-02, -2.6428e-02, -3.6367e-02],\n",
            "          [-9.8225e-03, -1.5507e-02, -3.2726e-02, -3.0918e-02, -4.4196e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.2370e-03, -6.9732e-03,  8.5162e-03, -4.7420e-03,  1.0625e-02],\n",
            "          [ 1.6828e-02, -6.0738e-03,  1.3850e-02,  5.5172e-03,  1.0139e-02],\n",
            "          [-3.1072e-03, -8.9751e-03, -2.9824e-03, -1.4203e-02,  5.5839e-03],\n",
            "          [ 6.0267e-03, -6.0471e-03, -9.8900e-03,  3.7394e-03,  9.4179e-03],\n",
            "          [-2.8714e-03,  8.4433e-03, -1.3183e-02, -7.4885e-03,  6.5005e-03]],\n",
            "\n",
            "         [[ 4.7335e-02,  3.5909e-02,  2.9360e-02, -4.6356e-03, -1.8169e-02],\n",
            "          [ 3.5783e-02,  2.9282e-02,  4.3439e-03, -2.5278e-03, -3.5134e-02],\n",
            "          [ 1.8415e-02,  8.9853e-03, -1.2418e-02, -2.9106e-02, -3.7758e-02],\n",
            "          [ 8.5242e-03, -2.2290e-02, -1.4028e-02, -4.0244e-02, -4.1239e-02],\n",
            "          [-9.9164e-05, -1.3684e-02, -3.8576e-02, -4.3142e-02, -3.9278e-02]],\n",
            "\n",
            "         [[ 9.7396e-04,  3.1157e-03,  8.6614e-03, -7.5457e-03, -8.3959e-03],\n",
            "          [ 3.5286e-03,  1.4452e-02,  1.9431e-02, -1.1221e-02, -1.6821e-02],\n",
            "          [-1.1838e-03,  1.0956e-02, -1.6642e-03,  9.1462e-03, -4.1843e-04],\n",
            "          [-6.7731e-03,  6.6031e-03, -1.4902e-02,  1.4807e-04, -1.7161e-02],\n",
            "          [-2.9611e-03, -2.4251e-03, -2.1573e-02, -2.4830e-02, -2.5140e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.3252e-02,  2.1171e-02,  6.4335e-03, -5.9047e-03,  4.4601e-03],\n",
            "          [ 9.6767e-03,  1.5933e-02,  4.5305e-03,  8.9286e-03, -1.3242e-02],\n",
            "          [ 1.7498e-02,  1.6251e-02,  5.3682e-03,  1.2276e-02,  1.8253e-02],\n",
            "          [ 2.7246e-02,  3.1665e-02,  2.0183e-02,  2.5361e-03, -6.4505e-04],\n",
            "          [ 3.8012e-02,  3.0197e-02,  1.6613e-02,  1.0194e-02,  9.3020e-03]],\n",
            "\n",
            "         [[-1.6011e-02,  1.8246e-02, -6.9427e-03,  1.3567e-02, -1.8963e-03],\n",
            "          [ 2.3044e-02, -3.9510e-05, -8.4561e-03, -8.1508e-03, -1.7849e-02],\n",
            "          [ 2.7709e-03,  2.8371e-02,  7.7779e-03, -9.9967e-03,  7.0089e-03],\n",
            "          [ 1.1493e-02,  2.7745e-02,  3.3656e-02,  1.6015e-02,  1.2726e-02],\n",
            "          [ 1.2639e-02,  3.0555e-02, -6.3402e-03, -2.4888e-03, -1.1063e-02]],\n",
            "\n",
            "         [[ 7.9422e-02,  5.4377e-02,  1.4104e-02, -2.6766e-02, -4.9716e-02],\n",
            "          [ 3.4870e-02,  3.3242e-02,  4.7312e-03, -2.7317e-02, -3.5437e-02],\n",
            "          [ 1.5580e-02,  1.1526e-02,  1.1139e-03, -3.4185e-02, -2.5917e-02],\n",
            "          [ 3.6719e-03, -5.7130e-03, -7.1346e-03, -2.9883e-02, -4.0247e-02],\n",
            "          [ 5.2229e-03, -1.4699e-02, -1.6965e-02, -2.8480e-02, -5.1216e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.8092e-02,  1.8251e-02,  4.2744e-03, -9.5897e-03,  6.7528e-03],\n",
            "          [ 1.4348e-02,  1.1585e-02,  1.4605e-03,  1.2975e-02,  7.0945e-03],\n",
            "          [ 2.2260e-02,  2.2298e-02,  3.4882e-02,  2.1956e-02,  5.1859e-04],\n",
            "          [ 2.1301e-02,  4.9556e-02,  3.3505e-02,  7.2739e-03,  1.2605e-02],\n",
            "          [ 5.9334e-02,  3.6895e-02,  4.2819e-02,  2.5308e-03, -4.7947e-03]],\n",
            "\n",
            "         [[ 9.2723e-02,  7.6466e-02,  3.7402e-02, -2.1574e-02, -7.0334e-02],\n",
            "          [ 8.7286e-02,  5.2003e-02,  1.3064e-02, -3.3509e-02, -6.9598e-02],\n",
            "          [ 7.0676e-02,  2.8671e-02,  1.4340e-02, -1.2592e-02, -7.3016e-02],\n",
            "          [ 1.4443e-02, -5.4561e-03, -1.8035e-02, -2.7808e-02, -6.6792e-02],\n",
            "          [-8.2113e-03, -2.1026e-03, -2.1026e-02, -5.1525e-02, -8.0502e-02]],\n",
            "\n",
            "         [[-2.4043e-03, -2.0283e-02, -2.2505e-03, -2.3791e-02, -2.3182e-02],\n",
            "          [ 1.5338e-02,  7.3856e-03, -1.8866e-02, -1.5284e-02, -4.0390e-02],\n",
            "          [-3.0228e-03, -2.6474e-02, -7.5349e-03, -2.5146e-02, -2.6946e-02],\n",
            "          [-1.7648e-02, -2.9296e-02, -3.4840e-02, -2.7762e-02, -2.9326e-02],\n",
            "          [ 2.0260e-02,  1.5483e-02,  2.0848e-02, -2.6392e-03, -1.7721e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.1760e-02, -2.6002e-02,  4.1539e-03, -2.1322e-02, -9.1015e-03],\n",
            "          [-1.1716e-02,  8.7616e-03,  2.6754e-03, -1.7134e-02,  8.3866e-03],\n",
            "          [-8.4562e-03, -9.6082e-03, -3.5548e-03, -9.3269e-03, -1.6287e-02],\n",
            "          [ 1.6998e-03,  8.8831e-03,  5.7343e-03,  2.0053e-02, -1.0278e-02],\n",
            "          [-6.4338e-03,  9.1203e-05,  7.9152e-03,  1.9542e-02,  6.9408e-03]],\n",
            "\n",
            "         [[-3.6432e-02, -8.5131e-03, -3.8674e-02, -1.4464e-02, -3.7629e-02],\n",
            "          [-2.7530e-02, -2.4685e-02, -9.4071e-03, -1.9492e-02, -2.7585e-02],\n",
            "          [-1.5352e-02, -1.4240e-02, -1.3245e-02, -1.9952e-02, -1.5256e-02],\n",
            "          [ 4.4050e-03, -1.1408e-02, -1.4220e-02, -1.4178e-02, -1.2615e-03],\n",
            "          [-3.2572e-03, -7.5341e-03,  3.9349e-03, -1.9296e-02, -2.6953e-02]],\n",
            "\n",
            "         [[-5.8268e-02, -4.8822e-02, -4.4153e-02, -2.5236e-02, -2.0792e-02],\n",
            "          [-3.9401e-02, -3.8846e-02, -1.5174e-02, -2.0636e-02, -1.7116e-02],\n",
            "          [-3.5382e-02, -1.7337e-02, -1.7829e-02, -2.1228e-02,  1.1249e-02],\n",
            "          [-3.1111e-02, -2.1339e-02,  7.8591e-03, -6.3862e-03,  3.2926e-02],\n",
            "          [-3.0970e-02, -1.1354e-02,  7.9099e-03,  3.4188e-02,  3.6047e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.1881e-02,  1.7788e-02, -1.9315e-03,  9.9694e-03, -6.1732e-03],\n",
            "          [ 1.9205e-03,  2.1682e-02, -1.0371e-02,  1.5705e-02,  1.7441e-02],\n",
            "          [ 2.2924e-02,  2.7686e-02,  2.5785e-02,  2.6238e-02,  1.8631e-02],\n",
            "          [ 4.1899e-02,  2.9859e-02,  2.6044e-02,  4.1986e-02,  3.7734e-02],\n",
            "          [ 4.8681e-02,  2.2414e-02,  3.0331e-02,  2.2520e-02,  3.0189e-02]],\n",
            "\n",
            "         [[-8.0162e-02, -6.6784e-02, -4.1237e-02, -4.9424e-02, -4.2633e-02],\n",
            "          [-4.4792e-02, -4.6013e-02, -3.5641e-02, -2.5749e-02, -7.3883e-03],\n",
            "          [-3.3861e-02, -1.7369e-02,  4.4510e-03,  3.7422e-02,  4.1408e-02],\n",
            "          [-3.7889e-02,  1.4793e-02,  3.8863e-02,  2.7580e-02,  5.8006e-02],\n",
            "          [ 9.3112e-03,  3.1651e-02,  4.4055e-02,  4.4353e-02,  6.2192e-02]],\n",
            "\n",
            "         [[ 9.2403e-03, -9.4346e-03,  9.4483e-03,  5.8708e-03, -6.4172e-04],\n",
            "          [ 2.5456e-03, -3.3589e-03, -1.6604e-02, -1.2649e-03,  8.7385e-03],\n",
            "          [-9.2347e-04, -4.2789e-03, -1.1996e-02, -8.8410e-03, -2.0447e-02],\n",
            "          [ 5.5067e-03,  4.2549e-03, -6.5807e-03,  7.1649e-03,  2.8106e-03],\n",
            "          [-9.1008e-03,  1.8535e-02, -6.2216e-03,  1.0462e-02,  1.6692e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.7935e-03,  4.7181e-03, -1.6073e-02, -7.9645e-03,  7.6955e-03],\n",
            "          [-2.0924e-02, -2.2037e-02, -7.5565e-03,  5.2654e-03, -2.1664e-02],\n",
            "          [ 1.0314e-02,  7.5964e-03,  1.1768e-02, -9.9480e-03, -2.7272e-03],\n",
            "          [ 1.3987e-02,  5.0030e-03, -5.1822e-03,  1.3169e-02, -2.7672e-03],\n",
            "          [ 1.7206e-02,  2.1735e-02,  2.6819e-02,  1.7687e-02,  2.2967e-02]],\n",
            "\n",
            "         [[ 2.0054e-03,  1.0036e-02,  2.3450e-02, -1.9844e-03,  1.1991e-02],\n",
            "          [-1.0031e-02, -1.0452e-03, -3.3962e-03, -3.7914e-03, -1.0789e-02],\n",
            "          [-1.3550e-04,  2.7356e-03, -2.4003e-02, -2.5695e-02, -2.4928e-02],\n",
            "          [ 2.9705e-05,  1.1606e-02, -4.0089e-03, -2.3056e-03, -5.5574e-03],\n",
            "          [ 1.3343e-02,  3.1073e-03, -1.6803e-04,  2.6553e-03,  1.5651e-02]],\n",
            "\n",
            "         [[ 1.1428e-03, -3.1090e-03, -3.7226e-02, -4.0371e-02, -4.3383e-02],\n",
            "          [ 6.8100e-03, -1.9746e-02, -3.2915e-02, -2.9650e-02, -3.0835e-02],\n",
            "          [ 7.0281e-03, -1.4088e-02, -9.9465e-03, -1.3248e-02, -2.7506e-02],\n",
            "          [-4.0618e-03,  1.1442e-02, -1.6898e-02,  7.8200e-03,  7.0381e-04],\n",
            "          [ 5.2326e-03,  1.8997e-02,  2.0725e-03, -5.8883e-03,  1.0911e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.4421e-02, -2.1738e-02, -6.4140e-03,  6.4994e-04, -1.4614e-02],\n",
            "          [-1.8439e-02,  6.9528e-04, -2.5593e-02, -2.1504e-02, -1.6314e-02],\n",
            "          [-3.3540e-03,  1.6527e-03, -1.4415e-02,  7.5563e-04, -1.1743e-02],\n",
            "          [ 1.2933e-02,  1.2628e-02,  1.8308e-02,  1.0188e-02,  1.0896e-02],\n",
            "          [ 2.8742e-03,  2.4914e-03,  2.7553e-02,  1.9712e-02,  1.8398e-02]],\n",
            "\n",
            "         [[-2.8577e-02, -2.8622e-02, -4.1171e-02, -5.0534e-02, -4.4748e-02],\n",
            "          [ 4.3230e-03, -1.5720e-02, -2.8450e-02, -2.1244e-02, -4.6710e-02],\n",
            "          [-9.2569e-03,  1.2996e-03,  1.6752e-04,  1.6557e-03, -8.8411e-03],\n",
            "          [ 1.8950e-02,  7.9311e-04, -2.0778e-03, -1.0117e-02,  2.0311e-03],\n",
            "          [ 1.3206e-02,  2.5616e-02, -7.8702e-03,  1.1137e-02,  1.1137e-02]],\n",
            "\n",
            "         [[-2.5130e-02, -2.5465e-03, -1.8121e-02, -3.0183e-02, -2.9586e-02],\n",
            "          [ 1.2789e-03, -2.2114e-02, -2.9393e-02, -1.0421e-02, -5.1473e-03],\n",
            "          [ 1.4326e-02, -1.2096e-03,  6.2429e-03, -9.1976e-03,  1.9181e-02],\n",
            "          [ 2.7055e-02,  2.4700e-02,  2.1719e-02,  1.9388e-02,  1.9574e-02],\n",
            "          [ 2.2145e-02,  4.7447e-02,  2.7146e-02,  2.8137e-02,  2.9058e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.1981e-02,  2.2671e-02,  2.4644e-02,  1.4296e-02,  1.6802e-02],\n",
            "          [ 1.2267e-02,  2.2479e-04,  1.5935e-02, -1.0021e-02, -2.8637e-03],\n",
            "          [ 1.3808e-02, -3.1927e-03,  1.6064e-02,  9.2119e-03,  1.8113e-02],\n",
            "          [ 1.8975e-02,  1.9522e-02,  6.2630e-03,  2.7986e-02,  5.1623e-03],\n",
            "          [ 1.1953e-02,  3.7971e-02,  4.1412e-02,  9.6855e-03,  2.8163e-02]],\n",
            "\n",
            "         [[ 8.1897e-03,  1.5219e-02,  1.7875e-02,  2.3591e-03,  8.2365e-03],\n",
            "          [ 4.3505e-03,  1.5428e-03,  2.7282e-02, -5.9023e-06,  2.0506e-02],\n",
            "          [-5.9909e-03, -9.1149e-04, -7.1788e-03,  1.1134e-03,  3.2382e-02],\n",
            "          [ 2.9957e-02,  2.4111e-02,  6.7701e-03,  6.1813e-03,  9.3792e-03],\n",
            "          [ 6.0457e-03,  2.6047e-02,  9.4252e-03,  2.8062e-02,  2.3062e-02]],\n",
            "\n",
            "         [[ 2.1025e-02,  1.9550e-02, -1.1423e-02, -2.5944e-02, -5.9206e-02],\n",
            "          [-1.3668e-02, -1.8023e-02, -1.7628e-02, -4.0710e-02, -4.4824e-02],\n",
            "          [-1.0473e-02, -1.8701e-02,  6.2543e-03, -7.8640e-03, -2.2840e-02],\n",
            "          [ 2.5419e-02, -9.2735e-03,  5.0684e-03,  1.3192e-02, -1.6442e-02],\n",
            "          [ 2.1694e-03,  3.3824e-02,  1.5047e-02, -3.9660e-03,  9.5272e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.0298e-03,  2.7035e-02,  8.0394e-03,  8.9323e-04,  2.4599e-02],\n",
            "          [ 1.4432e-02,  1.3374e-02,  2.3349e-02,  1.1493e-02,  7.1107e-03],\n",
            "          [ 1.8724e-02,  9.4188e-03,  3.1321e-03, -3.4930e-03,  1.2022e-02],\n",
            "          [ 3.9389e-02,  3.5150e-02,  2.6401e-02,  2.1152e-02,  3.6485e-02],\n",
            "          [ 4.8066e-02,  2.3687e-02,  1.3392e-02,  1.2440e-02,  3.0503e-02]],\n",
            "\n",
            "         [[ 7.8088e-03, -1.3502e-02, -4.1390e-03, -3.8101e-02, -6.4049e-02],\n",
            "          [-7.3092e-03,  1.4895e-03, -8.9879e-03, -6.9657e-03, -3.8372e-02],\n",
            "          [ 1.4640e-02,  3.2000e-03,  1.4994e-02,  2.3439e-02, -1.0818e-02],\n",
            "          [ 2.1860e-02,  2.2956e-02,  8.2562e-03,  2.5207e-02,  1.9922e-02],\n",
            "          [ 2.4223e-02,  1.7188e-02,  1.9914e-02,  1.5473e-02, -5.6377e-03]],\n",
            "\n",
            "         [[ 3.0408e-02,  3.0570e-02,  2.0787e-02, -3.6839e-03,  1.0680e-02],\n",
            "          [-2.2966e-03,  1.1078e-02, -2.1161e-02,  6.2666e-03, -2.2506e-03],\n",
            "          [-1.5784e-02, -3.1816e-02,  1.5376e-03, -2.3953e-02, -2.2476e-02],\n",
            "          [ 2.6146e-02,  9.3779e-03, -1.8118e-02, -1.9962e-02, -2.3587e-02],\n",
            "          [ 1.2075e-02,  9.8151e-03, -8.0939e-03, -7.1641e-03, -2.5607e-02]]]],\n",
            "       device='cuda:0')), ('conv2.bias', tensor([ 0.4070, -0.0437, -0.1082,  0.1260, -0.0781, -0.0461,  0.1200,  0.0477,\n",
            "         0.0379, -0.1708, -0.1574, -0.0189,  0.0950, -0.0642, -0.0995,  0.3432,\n",
            "         0.1842,  0.0463, -0.0382,  0.1629,  0.0479,  0.1590,  0.2506, -0.1115,\n",
            "         0.0411,  0.1377, -0.0554,  0.0777,  0.0713, -0.1980, -0.0602,  0.1631,\n",
            "         0.3944,  0.0172,  0.0683, -0.3262,  0.0509,  0.0556,  0.0645, -0.0904,\n",
            "         0.0398,  0.0259,  0.0407,  0.0369,  0.0262,  0.0157, -0.0233,  0.1488,\n",
            "        -0.0738,  0.0548, -0.0154,  0.0375, -0.0730,  0.0884, -0.0352,  0.0424,\n",
            "        -0.2235, -0.0168, -0.2543, -0.1305,  0.0390, -0.0872,  0.0486, -0.0423],\n",
            "       device='cuda:0')), ('fc1.weight', tensor([[-0.0132, -0.0051,  0.0072,  ..., -0.0120, -0.0110,  0.0042],\n",
            "        [ 0.0010,  0.0094,  0.0066,  ..., -0.0096,  0.0063,  0.0051],\n",
            "        [ 0.0169,  0.0030,  0.0074,  ...,  0.0070,  0.0062, -0.0029],\n",
            "        ...,\n",
            "        [ 0.0341,  0.0323,  0.0119,  ..., -0.0282, -0.0059, -0.0147],\n",
            "        [ 0.0013,  0.0048,  0.0006,  ...,  0.0109, -0.0077, -0.0032],\n",
            "        [ 0.0012,  0.0186,  0.0239,  ...,  0.0014, -0.0155,  0.0010]],\n",
            "       device='cuda:0')), ('fc1.bias', tensor([ 6.8600e-03,  1.5441e-02, -2.6783e-03,  2.6659e-02, -5.6235e-02,\n",
            "        -2.4839e-02,  1.5190e-02, -5.6166e-03,  3.7670e-02,  1.8840e-03,\n",
            "         4.6218e-03,  3.6985e-02, -2.2567e-02, -1.9454e-02,  2.8670e-02,\n",
            "        -4.3969e-03, -1.8970e-02,  3.7490e-02, -8.5847e-03,  1.6970e-02,\n",
            "        -1.0784e-02, -4.8514e-02, -1.8867e-02, -1.6150e-02,  9.6255e-03,\n",
            "         3.7977e-03,  5.4908e-03,  3.9730e-02, -1.1011e-02, -3.8603e-03,\n",
            "         8.4738e-03,  9.2679e-03, -1.4414e-02, -2.2126e-02,  2.8914e-02,\n",
            "        -1.4925e-02,  5.4832e-02, -6.3521e-03,  9.7401e-04,  2.9448e-03,\n",
            "         8.3165e-03,  1.3771e-03,  1.1950e-02,  2.1776e-02, -1.6131e-02,\n",
            "        -8.3618e-04,  1.5650e-02,  6.5825e-02,  3.6650e-02, -2.8628e-03,\n",
            "         3.0466e-02, -2.0103e-02, -6.3973e-03,  2.9456e-02,  1.8545e-03,\n",
            "         4.2997e-03,  3.2034e-02,  7.0882e-02, -1.2437e-02, -3.2950e-02,\n",
            "         5.6675e-02,  5.9066e-02, -3.0235e-03,  1.0183e-02, -2.8371e-03,\n",
            "         2.5979e-02,  5.9690e-02,  7.8866e-03,  4.7251e-02, -7.3322e-03,\n",
            "        -4.5560e-03, -1.4415e-02, -3.4669e-02,  5.0086e-03, -8.7153e-03,\n",
            "         1.6200e-02,  3.1601e-03,  1.0774e-03,  2.1472e-02,  3.4304e-02,\n",
            "         6.6112e-03,  6.4919e-03, -3.6015e-02, -3.3904e-03,  1.5541e-02,\n",
            "        -6.1443e-03,  1.6844e-02,  3.9789e-02, -2.0744e-03,  1.4163e-02,\n",
            "         1.1537e-02, -6.3186e-03, -2.0898e-02, -1.1205e-02,  4.5252e-02,\n",
            "         5.6957e-02,  5.5111e-03, -4.5109e-03,  1.4648e-02,  5.4635e-02,\n",
            "         7.3045e-03,  2.2943e-02,  1.0614e-02,  1.1583e-02, -1.0045e-02,\n",
            "        -6.6264e-03,  3.9643e-02,  2.3606e-02,  8.9551e-03, -1.5877e-02,\n",
            "        -1.1092e-02,  4.7134e-02,  7.9568e-02, -3.8931e-03, -1.3291e-02,\n",
            "         4.6721e-03,  7.4962e-03,  1.5982e-02,  3.1108e-03, -9.0757e-03,\n",
            "         4.1065e-02, -1.2826e-02,  1.9680e-02,  2.1323e-02,  1.4161e-02,\n",
            "        -1.0865e-02, -4.8922e-03, -1.6880e-02,  1.0310e-02,  2.7301e-02,\n",
            "        -2.5651e-02,  1.5521e-02,  6.0693e-02,  2.6660e-02,  1.8170e-02,\n",
            "         3.7865e-03,  1.8683e-02, -7.4058e-04, -2.3213e-02,  2.9120e-02,\n",
            "         2.1747e-02,  2.3105e-02, -2.3437e-03,  2.1617e-02,  9.7539e-03,\n",
            "         2.1499e-02,  2.2956e-02, -2.2127e-02,  3.0678e-03,  1.9321e-02,\n",
            "         2.0690e-02, -1.6456e-02,  2.2226e-02, -2.0531e-02,  1.2249e-02,\n",
            "        -1.8955e-02,  1.1941e-02,  5.0104e-03,  7.9364e-04, -8.1576e-03,\n",
            "         1.1446e-02,  1.2027e-02, -1.4946e-02, -1.9682e-02,  7.2017e-02,\n",
            "         4.6947e-03, -3.1496e-02,  4.5817e-03, -2.4231e-03, -3.1497e-02,\n",
            "         6.5195e-02, -3.1518e-03,  7.6194e-03, -4.8863e-02,  7.5821e-03,\n",
            "        -5.4927e-03,  1.1429e-02, -9.9661e-03, -2.4801e-02, -3.9401e-02,\n",
            "        -2.6925e-02,  1.3646e-02,  2.0449e-02,  3.3017e-02, -2.3726e-02,\n",
            "         1.5847e-02,  5.9390e-04,  1.3859e-02,  1.5145e-02,  4.2135e-02,\n",
            "         5.7245e-03, -1.1718e-02, -9.9125e-03,  9.8490e-03,  3.3662e-02,\n",
            "         1.7233e-03,  4.1735e-03,  4.7701e-03,  1.1181e-02,  1.4232e-02,\n",
            "         8.6051e-03,  2.0498e-02, -8.1092e-05, -2.5371e-02, -2.3812e-02,\n",
            "         4.8951e-03,  1.0350e-02, -1.3907e-02, -1.1201e-02, -4.1514e-03,\n",
            "         3.7262e-02, -2.7526e-02,  2.6740e-03, -5.4641e-02, -1.7481e-02,\n",
            "         2.1371e-02,  1.0937e-02, -9.6968e-03,  1.6109e-02,  5.5471e-03,\n",
            "        -3.4993e-03,  8.5857e-03, -1.1562e-03,  2.8348e-02, -1.6773e-02,\n",
            "         7.0666e-03,  6.0768e-02,  3.5055e-03, -9.2075e-03,  1.2584e-02,\n",
            "         2.2628e-02, -1.7376e-02, -2.1460e-03, -3.5868e-04, -2.1403e-03,\n",
            "         1.5443e-02,  2.9210e-02, -3.6040e-03, -4.9646e-02,  2.6066e-02,\n",
            "        -6.8368e-02, -7.8559e-03, -2.8688e-04, -3.2569e-02,  1.5207e-02,\n",
            "        -1.0601e-02,  9.1066e-02, -1.8920e-02, -2.6061e-02,  1.5566e-02,\n",
            "         1.5072e-02, -5.9677e-03, -1.0575e-02,  9.1187e-03, -8.0285e-03,\n",
            "         1.9442e-02,  2.2396e-02,  5.6015e-02,  5.2500e-02,  3.5330e-02,\n",
            "        -1.7618e-02, -2.1471e-03,  2.8045e-02,  2.8161e-02, -1.9527e-03,\n",
            "        -2.0134e-02,  5.7046e-03,  8.6518e-03,  3.8916e-02, -6.7465e-03,\n",
            "         3.7346e-02,  7.9125e-03,  3.5772e-02,  3.9953e-02, -3.3345e-02,\n",
            "         1.8989e-02, -1.4741e-02,  5.4376e-02,  1.0740e-02, -7.1398e-03,\n",
            "         2.6486e-02,  9.4803e-03,  4.3925e-02,  3.3481e-03,  1.2884e-02,\n",
            "        -4.2100e-03,  1.9952e-02, -1.1064e-02,  1.8307e-02,  7.9704e-03,\n",
            "        -3.0477e-02,  1.5639e-02,  2.5572e-02,  2.3986e-02,  1.8252e-02,\n",
            "         2.8943e-02,  1.3267e-02,  1.1943e-02, -1.6884e-02, -1.2400e-03,\n",
            "         9.3222e-03, -7.9262e-03,  3.2878e-02,  7.7410e-02,  1.3606e-02,\n",
            "         6.5631e-02,  6.0833e-03, -4.3163e-02, -6.4294e-02,  2.7854e-02,\n",
            "        -1.3343e-02,  2.3208e-02, -1.1449e-02,  4.7904e-03,  3.7123e-02,\n",
            "         5.5625e-02, -3.4712e-02,  1.9827e-02, -2.9529e-03,  6.8510e-04,\n",
            "         9.4265e-03, -2.7521e-02,  3.9997e-03,  3.6446e-02,  3.8318e-02,\n",
            "         7.2054e-02,  2.7145e-02,  5.5153e-02,  2.8602e-03,  4.0203e-02,\n",
            "         2.8240e-02, -1.1212e-02, -1.0932e-02,  1.2141e-02, -2.4387e-03,\n",
            "         1.5763e-02,  2.9825e-02,  1.1130e-02,  2.2026e-02,  2.6152e-02,\n",
            "        -4.7946e-02,  2.4751e-02,  5.9746e-02,  4.2899e-02, -5.1280e-04,\n",
            "         1.1296e-02,  4.3417e-02, -4.2681e-03, -1.3949e-03,  1.6318e-02,\n",
            "        -5.9188e-03,  3.0855e-02, -4.2779e-02, -2.5254e-02, -3.5373e-03,\n",
            "        -7.2412e-03, -1.1462e-02,  1.8594e-02,  1.7177e-02, -4.7352e-03,\n",
            "         3.0674e-02,  2.0592e-02, -1.0063e-02,  1.9531e-02,  5.5125e-02,\n",
            "         1.8237e-02, -1.8524e-02,  3.2725e-02, -1.0395e-02, -2.8123e-03,\n",
            "         1.5195e-02,  1.6529e-02,  7.3335e-03,  1.1943e-02,  3.7502e-02,\n",
            "        -3.8797e-02, -3.9709e-03,  3.2372e-02,  3.9630e-03, -5.6247e-03,\n",
            "        -7.1620e-02,  2.5324e-02,  5.7270e-03,  1.2808e-02], device='cuda:0')), ('fc2.weight', tensor([[-0.0377, -0.0109, -0.0011,  ..., -0.0723, -0.0188, -0.0247],\n",
            "        [ 0.0025,  0.0175, -0.0235,  ...,  0.0254,  0.0367, -0.0090],\n",
            "        [-0.0516,  0.0055,  0.0139,  ...,  0.0036,  0.0146, -0.0339],\n",
            "        ...,\n",
            "        [ 0.0036, -0.0239, -0.0279,  ..., -0.0431, -0.0016, -0.0309],\n",
            "        [-0.0097,  0.0255, -0.0141,  ..., -0.0566,  0.0074,  0.0003],\n",
            "        [-0.0301,  0.0289,  0.0064,  ...,  0.0027,  0.0185,  0.0126]],\n",
            "       device='cuda:0')), ('fc2.bias', tensor([-1.6766e-02,  1.1974e-01,  2.1597e-04,  7.5507e-02,  4.8622e-02,\n",
            "         1.5485e-02, -4.4835e-03,  1.8775e-02, -9.9095e-03,  3.6445e-02,\n",
            "         7.2230e-03,  7.7429e-03,  5.7950e-03,  5.9466e-03,  8.7006e-02,\n",
            "         8.5979e-02,  1.3124e-01,  7.2624e-02,  1.5599e-02,  1.0819e-02,\n",
            "        -2.8700e-02,  1.3786e-01,  9.6578e-03,  1.1488e-01, -8.6379e-03,\n",
            "        -3.1619e-02, -4.6557e-02,  1.4653e-01, -1.8173e-02,  9.7073e-03,\n",
            "        -2.0638e-02,  1.0872e-01,  3.6547e-02,  9.3946e-03, -4.4935e-02,\n",
            "        -4.5863e-03, -7.4256e-02, -1.1958e-02, -2.1242e-03,  1.1588e-01,\n",
            "         3.8454e-02,  1.7355e-02, -4.4653e-02,  1.9415e-01,  2.4460e-01,\n",
            "         4.7471e-02,  6.2745e-02,  1.0651e-02, -4.5895e-03, -1.9478e-02,\n",
            "         2.8119e-02,  1.5512e-01,  1.4362e-01,  2.1346e-02, -3.9806e-02,\n",
            "         1.8665e-02, -1.7298e-02,  6.2685e-02,  1.2800e-01,  8.9625e-02,\n",
            "         9.8176e-02,  9.5678e-02, -5.1234e-03,  1.1810e-01,  2.0059e-02,\n",
            "        -3.6791e-03, -1.8139e-02,  1.2637e-03,  2.0589e-01,  3.3016e-02,\n",
            "         9.4972e-02, -1.6456e-02,  2.9964e-02,  5.2099e-02,  9.7783e-03,\n",
            "        -5.2512e-02, -1.3610e-02,  1.2782e-01,  2.3307e-02, -5.0553e-02,\n",
            "        -2.5149e-02,  4.1973e-02,  6.1705e-02, -1.3529e-01, -2.5421e-02,\n",
            "        -7.1700e-03,  7.9413e-02, -1.0631e-02, -1.9606e-02,  6.7299e-02,\n",
            "         1.1195e-02,  3.8295e-02,  2.9041e-02,  1.5763e-01,  1.9703e-02,\n",
            "         6.8993e-02, -7.7395e-02, -3.0295e-02,  5.5473e-02, -3.9681e-02,\n",
            "        -4.4840e-02, -3.9663e-02,  1.6948e-02, -1.8550e-03,  2.8060e-02,\n",
            "        -2.9658e-02,  1.3552e-02, -3.4012e-02,  1.6438e-02,  6.4574e-02,\n",
            "         1.1176e-02,  1.1954e-01,  3.9322e-02,  1.9959e-04,  2.2396e-02,\n",
            "        -1.9073e-03,  1.4555e-01,  7.9661e-02,  1.3757e-02,  4.3014e-02,\n",
            "         8.7739e-02, -2.0816e-02, -1.9017e-02,  1.0529e-01,  4.4795e-02,\n",
            "        -2.8156e-02, -4.5350e-02,  2.3683e-02,  3.3808e-02,  8.6670e-02,\n",
            "         3.1385e-02, -4.3038e-02, -1.7967e-03,  1.8870e-02,  3.4422e-02,\n",
            "         5.6055e-02, -2.6889e-02, -1.8129e-02,  4.4458e-02, -3.6798e-02,\n",
            "        -1.1128e-02,  9.6672e-02,  9.3008e-02,  1.8914e-02, -3.1220e-02,\n",
            "         1.8533e-02,  7.0198e-02,  5.4912e-02, -2.6981e-02, -1.6626e-02,\n",
            "         4.5297e-02,  2.3950e-03,  1.6715e-02,  8.8218e-03, -1.2098e-02,\n",
            "         9.7665e-02,  4.0872e-02,  2.0748e-02,  6.6299e-02,  1.1609e-02,\n",
            "         4.9037e-02,  5.2044e-02,  7.4067e-02, -3.8511e-02,  5.0111e-02,\n",
            "        -4.1817e-03,  8.3705e-03, -6.4572e-02, -3.3617e-02,  3.4335e-02,\n",
            "         9.0535e-02, -6.2363e-03, -3.2060e-02, -1.1479e-02,  2.3288e-02,\n",
            "        -6.0518e-02, -8.5431e-03,  2.7666e-02,  6.6554e-02, -3.1716e-02,\n",
            "         4.8810e-02, -1.9063e-02,  3.7959e-02, -1.1674e-02,  9.7769e-03,\n",
            "         4.5757e-02, -1.8170e-02,  1.1548e-01,  9.7337e-02, -5.3519e-02,\n",
            "         6.1188e-02,  5.1381e-02], device='cuda:0')), ('fc3.weight', tensor([[-0.1848, -0.0756,  0.1010,  ..., -0.0381,  0.0436,  0.0769],\n",
            "        [ 0.1807, -0.1064, -0.0316,  ...,  0.1474, -0.0859,  0.0600],\n",
            "        [ 0.0287, -0.0705,  0.2013,  ..., -0.0520, -0.0547,  0.0628],\n",
            "        ...,\n",
            "        [ 0.0188,  0.0031, -0.1080,  ...,  0.0481, -0.1354, -0.0434],\n",
            "        [-0.0593,  0.0030,  0.1339,  ..., -0.0395, -0.0264,  0.0641],\n",
            "        [-0.0059, -0.0190,  0.0195,  ...,  0.0844, -0.0549,  0.0740]],\n",
            "       device='cuda:0')), ('fc3.bias', tensor([-6.6881e-02, -3.9579e-02,  1.4486e-01,  1.5103e-02,  1.7128e-01,\n",
            "         1.7814e-01, -9.4312e-02, -8.5110e-02,  1.5421e-01, -4.7258e-02,\n",
            "         1.3815e-02, -9.7119e-02, -6.5135e-02, -6.3605e-02, -2.1365e-01,\n",
            "        -1.5898e-02,  6.5547e-02, -1.3805e-02,  1.0581e-01, -2.9390e-02,\n",
            "        -2.2355e-01,  2.7143e-02, -9.8565e-02,  2.4986e-01, -1.9073e-01,\n",
            "         6.7228e-02, -1.8117e-02,  2.7917e-01,  4.3316e-02, -1.6587e-01,\n",
            "         8.2031e-02,  1.1104e-01,  1.2625e-01,  1.2890e-01,  1.9468e-01,\n",
            "        -1.3427e-02,  1.4896e-01, -1.3811e-02,  1.9883e-01, -2.0589e-01,\n",
            "        -7.1469e-02, -1.6620e-01,  4.0165e-02,  1.1594e-01,  1.7406e-01,\n",
            "        -7.9730e-02,  1.2968e-02, -7.6204e-02, -1.9400e-01, -1.1803e-01,\n",
            "         7.9014e-02, -4.1236e-02, -1.7147e-01, -1.9373e-01, -4.6344e-02,\n",
            "         1.3608e-01, -4.6359e-02, -2.0928e-02, -1.6060e-01, -1.8499e-02,\n",
            "        -4.9201e-02,  1.0214e-01, -3.1210e-01,  1.7831e-01,  3.0850e-02,\n",
            "         7.9596e-02, -6.7585e-03,  6.5258e-02,  2.1972e-02,  3.3456e-02,\n",
            "        -2.1322e-01,  1.1589e-01,  2.1753e-02, -1.0568e-01,  1.4314e-01,\n",
            "        -2.8100e-01,  2.1419e-02,  1.0101e-01,  1.1074e-01,  1.0838e-01,\n",
            "         3.1881e-01,  4.6948e-02, -1.3523e-01, -3.3661e-01,  1.4801e-02,\n",
            "        -6.8274e-02, -1.5744e-01,  9.1872e-03,  1.2649e-01,  8.1231e-02,\n",
            "        -1.3753e-02, -1.3449e-04, -2.2406e-01, -5.9136e-03,  2.3979e-01,\n",
            "        -2.5856e-01,  9.9590e-02,  2.0006e-02, -1.5107e-01, -1.0296e-01],\n",
            "       device='cuda:0'))])\n",
            "Local Step 02: Test Loss: 3.458628869, Test Accuracy: 31.940\n",
            "**************************************************\n",
            "Worker 1, [01/04]: Training Loss: 2.775245058, Training Accuracy: 29.568\n",
            "Worker 1, [02/04]: Training Loss: 2.654960050, Training Accuracy: 31.568\n",
            "Worker 1, [03/04]: Training Loss: 2.575614951, Training Accuracy: 33.560\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Workers:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Number of Local Steps:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Update Slow Model every \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m steps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mlocal_SGD_SlowMo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[25], line 31\u001b[0m, in \u001b[0;36mlocal_SGD_SlowMo\u001b[0;34m(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, beta, alpha)\u001b[0m\n\u001b[1;32m     29\u001b[0m train_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loca_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(j):\n\u001b[0;32m---> 31\u001b[0m   train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mis_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorker \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworker\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloca_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.9f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m train_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn, accumulation_steps, device, is_wandb)\u001b[0m\n\u001b[1;32m      5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Initialize gradients to zero at the start of each epoch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     10\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:471\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:707\u001b[0m, in \u001b[0;36mRandomHorizontalFlip.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    700\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be flipped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Randomly flipped image.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mhflip(img)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "beta = 0.4\n",
        "alpha = 1\n",
        "parameters = {'lr': lr, 'wd': wd, 'beta': beta}\n",
        "K = [2, 4, 8]\n",
        "J = [4, 8, 16, 32 , 64]\n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize slow model\n",
        "slow_model = LeNet5()\n",
        "slow_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "for k in K:\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    local_SGD_SlowMo(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, beta, alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Personal Contribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SHAT (Asyncronous Approach)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "''' def generate_computation_latency_sequence(K, each_worker_iteration):\n",
        "For example each worker 15 iteration \n",
        "TODO: In this situation we will NOT have any bias to the worker that has higher iterations but the time that it takes is equal to the time of slowest worker that would be equal to local SGD\n",
        "    \"\"\"\n",
        "    Simulates a sequence of operations for K workers with random computation latencies.\n",
        "\n",
        "    The function generates a list of operations, each performed by a worker, sorted by\n",
        "    their latencies. Each worker performs exactly t operations, and the latencies are\n",
        "    scaled so that the fastest worker's latency is normalized to 1.\n",
        "\n",
        "    Parameters:\n",
        "    K (int): The number of workers in the simulation.\n",
        "    t (int): The number of operations each computer will perform.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - result (list of dicts): A list of dictionaries where each dictionary represents\n",
        "          an operation performed by a computer. The keys in the dictionary are:\n",
        "              - \"total_iterations\" (int): The total number of iterations performed.\n",
        "              - \"computer\" (int): The ID of the computer performing the operation.\n",
        "              - \"value\" (int): The latency value associated with that operation.\n",
        "        - computation_powers (list of ints): The original random computation latencies for each computer.\n",
        "        - scaled_powers (list of floats): The computation latencies scaled such that the fastest\n",
        "          computer's latency is 1.\n",
        "    \"\"\"\n",
        "    # Generate random computation powers\n",
        "    computation_powers = [random.randint(5000, 10000) for _ in range(K)]\n",
        "\n",
        "    # Find the minimum computation power\n",
        "    min_power = min(computation_powers)\n",
        "\n",
        "    # Scale the computation powers so that the fastest (smallest value) is equal to 1\n",
        "    scaled_powers = [power / min_power for power in computation_powers]\n",
        "\n",
        "    # Initialize a list to store the dictionaries\n",
        "    result = []\n",
        "\n",
        "    # Store the number of turns taken by each computer\n",
        "    turns_taken = [0] * K\n",
        "    total_number_of_iterations = 0\n",
        "    # Generate the dictionaries until all computers have 15 turns\n",
        "    while any(turn < each_worker_iteration for turn in turns_taken):\n",
        "        total_number_of_iterations += 1\n",
        "        # Generate the next possible dictionaries for each computer\n",
        "        possible_entries = []\n",
        "        for index in range(K):\n",
        "            if turns_taken[index] < 15:  # Only consider computers with less than 15 turns\n",
        "                operation_value = computation_powers[index] * (turns_taken[index] + 1)\n",
        "                possible_entries.append({\"total_iterations\": total_number_of_iterations,\"computer\": index + 1, \"value\": operation_value})\n",
        "\n",
        "        # Sort the possible dictionaries by their operation value\n",
        "        possible_entries.sort(key=lambda x: x[\"value\"])\n",
        "\n",
        "        # Add the dictionary with the smallest value to the result\n",
        "        chosen_entry = possible_entries[0]\n",
        "        result.append(chosen_entry)\n",
        "\n",
        "        # Update the turn count for the chosen computer\n",
        "        chosen_index = chosen_entry[\"computer\"] - 1\n",
        "        turns_taken[chosen_index] += 1\n",
        "\n",
        "    return result, computation_powers, scaled_powers\n",
        "\n",
        "'''\n",
        "\n",
        "# For example total iterations 150\n",
        "# TODO: in this situation we will have bias to ther worker that has higher iterations\n",
        "def generate_computation_latency_sequence(K, total_iterations):\n",
        "    \"\"\"\n",
        "    Simulates a sequence of operations for K workers with random computation latencies.\n",
        "\n",
        "    The function generates a list of operations, each performed by a worker, sorted by\n",
        "    their latencies. The total number of iterations is limited by `total_iterations`,\n",
        "    and the latencies are scaled so that the fastest worker's latency is normalized to 1.\n",
        "\n",
        "    Parameters:\n",
        "    K (int): The number of workers in the simulation.\n",
        "    total_iterations (int): The total number of iterations across all workers.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - result (list of dicts): A list of dictionaries where each dictionary represents\n",
        "          an operation performed by a worker. The keys in the dictionary are:\n",
        "              - \"total_iterations\" (int): The total number of iterations performed.\n",
        "              - \"worker\" (int): The ID of the worker performing the operation.\n",
        "              - \"value\" (int): The latency value associated with that operation.\n",
        "        - computation_powers (list of ints): The original random computation latencies for each worker.\n",
        "        - scaled_powers (list of floats): The computation latencies scaled such that the fastest\n",
        "          worker's latency is 1.\n",
        "    \"\"\"\n",
        "    # Generate random computation powers\n",
        "    computation_powers = [random.randint(5000, 10000) for _ in range(K)]\n",
        "\n",
        "    # Find the minimum computation power\n",
        "    min_power = min(computation_powers)\n",
        "\n",
        "    # Scale the computation powers so that the fastest (smallest value) is equal to 1\n",
        "    scaled_powers = [power / min_power for power in computation_powers]\n",
        "\n",
        "    # Initialize a list to store the dictionaries\n",
        "    result = []\n",
        "\n",
        "    # Store the number of turns taken by each worker\n",
        "    turns_taken = [0] * K\n",
        "    total_number_of_iterations_performed = 0\n",
        "\n",
        "    # Generate the dictionaries until the total number of iterations is reached\n",
        "    while total_number_of_iterations_performed < total_iterations:\n",
        "        total_number_of_iterations_performed += 1\n",
        "\n",
        "        # Generate the next possible dictionaries for each worker\n",
        "        possible_entries = []\n",
        "        for index in range(K):\n",
        "            # Calculate the operation value based on the current number of turns for this worker\n",
        "            operation_value = computation_powers[index] * (turns_taken[index] + 1)\n",
        "            possible_entries.append({\n",
        "                \"total_iterations\": total_number_of_iterations_performed,\n",
        "                \"worker\": index,  # Now starting index from 0\n",
        "                \"value\": operation_value\n",
        "            })\n",
        "\n",
        "        # Sort the possible dictionaries by their operation value\n",
        "        possible_entries.sort(key=lambda x: x[\"value\"])\n",
        "\n",
        "        # Add the dictionary with the smallest value to the result\n",
        "        chosen_entry = possible_entries[0]\n",
        "        result.append(chosen_entry)\n",
        "\n",
        "        # Update the turn count for the chosen worker\n",
        "        chosen_index = chosen_entry[\"worker\"]\n",
        "        turns_taken[chosen_index] += 1\n",
        "\n",
        "    return result, computation_powers, scaled_powers\n",
        "\n",
        "def calculate_gradients_model(global_model, local_model):\n",
        "    gradients_dict = {key: torch.zeros_like(value) for key, value in local_model.state_dict().items()}\n",
        "    \n",
        "    for key, value in local_model.state_dict().items():\n",
        "        gradients_dict[key] += (global_model.state_dict()[key] - value)\n",
        "    \n",
        "    return gradients_dict\n",
        "\n",
        "def update_global_model(global_model, gradients_model, lr):\n",
        "    new_weights = {}\n",
        "    for key, value in global_model.state_dict().items():\n",
        "        new_weights[key] = value - lr * gradients_model.state_dict()[key]\n",
        "\n",
        "    return new_weights\n",
        "\n",
        "\n",
        "def calculate_s_i(k,ci):\n",
        "    return float(k/ci)\n",
        "\n",
        "def calculate_alpha_i(si, k):\n",
        "    alpha = 1 - (si / math.log(k))\n",
        "    return alpha\n",
        "\n",
        "def updatel_local_model(global_model, local_model, alpha_i):\n",
        "    new_weights = {}\n",
        "    for key, value in local_model.state_dict().items():\n",
        "        new_weights[key] = (1 - alpha_i) * value + alpha_i * global_model.state_dict()[key]\n",
        "    \n",
        "    return new_weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def SHAT_PS(shard_loaders, loss_fn, k, lr, wd, initial_state_dict, num_epochs):  \n",
        "    # SHAT Parameter Server to manage workers\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    # Initialize a model with same value of param for each chunk\n",
        "    local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "    for model in local_models:\n",
        "      model.load_state_dict(initial_state_dict)\n",
        "\n",
        "    # # Initialize the global model\n",
        "    global_model = LeNet5().to(device)\n",
        "    global_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "    # Generate a sequence of computation latency to simulate the difference of computation latency (Lower computation Latency means higher computation power)\n",
        "    computation_latency_sequence, computation_latency, scaled_computation_latency = generate_computation_latency_sequence(k, num_epochs)\n",
        "\n",
        "    # Print the original and scaled computation latency\n",
        "    print(\"Original Computation Latency:\", computation_latency)\n",
        "    print(\"Scaled Computation Latency:\", scaled_computation_latency)\n",
        "\n",
        "    # print sequence of workers based on their computation latency\n",
        "    print(f'workers simulated orders based on computation latency:{[entry[\"worker\"]+1 for entry in computation_latency_sequence]}')\n",
        "\n",
        "    C = [1 for _ in range(k)] # Staleness counter\n",
        "    local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "    \n",
        "    #This specifies turn of the model\n",
        "    for iteration_index, worker in enumerate([entry['worker'] for entry in computation_latency_sequence]):\n",
        "      iteration_start_time = time.time()\n",
        "      print('*'*50)\n",
        "      print(f'worker {worker+1}')\n",
        "\n",
        "      train_loss, train_accuracy = train(local_models[worker], shard_loaders[worker], local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "      print(f'Worker {worker+1}, [{iteration_index+1:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      \n",
        "      '''PS server: receive model from the worker and calculate diff model (gradient)'''\n",
        "      gradients_model = LeNet5().to(device)\n",
        "      gradients_model.load_state_dict(calculate_gradients_model(global_model, local_models[worker]))\n",
        "      \n",
        "      # Computing the staleness of each worker\n",
        "      for i in C :\n",
        "        if i != worker:\n",
        "          i += 1\n",
        "\n",
        "      '''PS Server update global model'''\n",
        "      global_model.load_state_dict(update_global_model(global_model, gradients_model, lr = 1))\n",
        "      \n",
        "      '''send updated model to the worker'''\n",
        "      '''calucale the staleness of the worker i  si  logn ,    s  n/ci'''\n",
        "      s_i = calculate_s_i(k,C[worker])\n",
        "      alpha_i = calculate_alpha_i(s_i, k)\n",
        "      '''update worker local model ba w  (1   )w +  w'''\n",
        "      local_models[worker].load_state_dict(updatel_local_model(global_model, local_models[worker], alpha_i))\n",
        "\n",
        "      '''continue outer loop in PS'''\n",
        "      '''ci = 0 ya 1'''\n",
        "      C[worker] = 1\n",
        "      \n",
        "      iteration_end_time = time.time()\n",
        "    \n",
        "      print(f'Time taken for worker {worker+1} : {str(timedelta(seconds=iteration_end_time - iteration_start_time))}')\n",
        "      print(f'iteration: {iteration_index+1:02}/{num_epochs:02}')\n",
        "      test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "      print(f'Local Step {iteration_index+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "      print('*'*50)\n",
        "    total_end_time = time.time()\n",
        "    print('/'*50)\n",
        "    print(f'Total time taken for SHAT: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "    print('/'*50)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2\n",
            "==================================================\n",
            "Original Computation Latency: [7754, 8187]\n",
            "Scaled Computation Latency: [1.0, 1.055842145989167]\n",
            "workers simulated orders based on computation latency:[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2]\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [01]: Training Loss: 4.308836286, Training Accuracy: 4.572\n",
            "Time taken for worker 1 : 0:00:11.932847\n",
            "iteration: 00/150\n",
            "Local Step 01: Test Loss: 4.009593420, Test Accuracy: 8.120\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [02]: Training Loss: 4.311259537, Training Accuracy: 4.408\n",
            "Time taken for worker 2 : 0:00:13.415470\n",
            "iteration: 01/150\n",
            "Local Step 02: Test Loss: 3.999800506, Test Accuracy: 8.540\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [03]: Training Loss: 3.865181452, Training Accuracy: 10.180\n",
            "Time taken for worker 1 : 0:00:12.779542\n",
            "iteration: 02/150\n",
            "Local Step 03: Test Loss: 3.669497467, Test Accuracy: 13.250\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [04]: Training Loss: 3.867524433, Training Accuracy: 10.108\n",
            "Time taken for worker 2 : 0:00:11.857773\n",
            "iteration: 03/150\n",
            "Local Step 04: Test Loss: 3.671249321, Test Accuracy: 13.810\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [05]: Training Loss: 3.610055726, Training Accuracy: 13.692\n",
            "Time taken for worker 1 : 0:00:12.953214\n",
            "iteration: 04/150\n",
            "Local Step 05: Test Loss: 3.437436295, Test Accuracy: 17.300\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [06]: Training Loss: 3.593811705, Training Accuracy: 14.024\n",
            "Time taken for worker 2 : 0:00:11.967869\n",
            "iteration: 05/150\n",
            "Local Step 06: Test Loss: 3.421884564, Test Accuracy: 17.640\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [07]: Training Loss: 3.398131841, Training Accuracy: 17.620\n",
            "Time taken for worker 1 : 0:00:12.923114\n",
            "iteration: 06/150\n",
            "Local Step 07: Test Loss: 3.189646826, Test Accuracy: 22.430\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [08]: Training Loss: 3.406305303, Training Accuracy: 17.472\n",
            "Time taken for worker 2 : 0:00:12.794298\n",
            "iteration: 07/150\n",
            "Local Step 08: Test Loss: 3.203599404, Test Accuracy: 21.930\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [09]: Training Loss: 3.217229636, Training Accuracy: 20.948\n",
            "Time taken for worker 1 : 0:00:13.372335\n",
            "iteration: 08/150\n",
            "Local Step 09: Test Loss: 3.036312773, Test Accuracy: 24.310\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [10]: Training Loss: 3.218839107, Training Accuracy: 21.264\n",
            "Time taken for worker 2 : 0:00:13.983338\n",
            "iteration: 09/150\n",
            "Local Step 10: Test Loss: 3.081396423, Test Accuracy: 23.820\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [11]: Training Loss: 3.071262810, Training Accuracy: 23.448\n",
            "Time taken for worker 1 : 0:00:14.404252\n",
            "iteration: 10/150\n",
            "Local Step 11: Test Loss: 2.975425160, Test Accuracy: 25.210\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [12]: Training Loss: 3.071154702, Training Accuracy: 23.660\n",
            "Time taken for worker 2 : 0:00:14.407332\n",
            "iteration: 11/150\n",
            "Local Step 12: Test Loss: 2.886638763, Test Accuracy: 27.820\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [13]: Training Loss: 2.960819633, Training Accuracy: 25.556\n",
            "Time taken for worker 1 : 0:00:13.909125\n",
            "iteration: 12/150\n",
            "Local Step 13: Test Loss: 2.844353603, Test Accuracy: 28.060\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [14]: Training Loss: 2.926603827, Training Accuracy: 26.668\n",
            "Time taken for worker 2 : 0:00:12.949894\n",
            "iteration: 13/150\n",
            "Local Step 14: Test Loss: 2.792184900, Test Accuracy: 29.080\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [15]: Training Loss: 2.859916645, Training Accuracy: 27.648\n",
            "Time taken for worker 1 : 0:00:13.751952\n",
            "iteration: 14/150\n",
            "Local Step 15: Test Loss: 2.745784583, Test Accuracy: 30.480\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [16]: Training Loss: 2.825533910, Training Accuracy: 28.356\n",
            "Time taken for worker 2 : 0:00:14.215861\n",
            "iteration: 15/150\n",
            "Local Step 16: Test Loss: 2.760708167, Test Accuracy: 30.640\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [17]: Training Loss: 2.746806025, Training Accuracy: 29.876\n",
            "Time taken for worker 1 : 0:00:12.213822\n",
            "iteration: 16/150\n",
            "Local Step 17: Test Loss: 2.709355304, Test Accuracy: 31.800\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [18]: Training Loss: 2.737761377, Training Accuracy: 30.084\n",
            "Time taken for worker 2 : 0:00:13.791162\n",
            "iteration: 17/150\n",
            "Local Step 18: Test Loss: 2.670578522, Test Accuracy: 32.400\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [19]: Training Loss: 2.667367878, Training Accuracy: 31.660\n",
            "Time taken for worker 1 : 0:00:12.716473\n",
            "iteration: 18/150\n",
            "Local Step 19: Test Loss: 2.626940106, Test Accuracy: 33.150\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [20]: Training Loss: 2.636322323, Training Accuracy: 32.512\n",
            "Time taken for worker 2 : 0:00:13.379192\n",
            "iteration: 19/150\n",
            "Local Step 20: Test Loss: 2.668497577, Test Accuracy: 32.250\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [21]: Training Loss: 2.597958212, Training Accuracy: 32.932\n",
            "Time taken for worker 1 : 0:00:13.309349\n",
            "iteration: 20/150\n",
            "Local Step 21: Test Loss: 2.555279164, Test Accuracy: 34.970\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [22]: Training Loss: 2.558664066, Training Accuracy: 33.860\n",
            "Time taken for worker 2 : 0:00:14.250413\n",
            "iteration: 21/150\n",
            "Local Step 22: Test Loss: 2.564701995, Test Accuracy: 34.510\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [23]: Training Loss: 2.533535137, Training Accuracy: 33.992\n",
            "Time taken for worker 1 : 0:00:11.841866\n",
            "iteration: 22/150\n",
            "Local Step 23: Test Loss: 2.555593176, Test Accuracy: 34.970\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [24]: Training Loss: 2.489168862, Training Accuracy: 35.160\n",
            "Time taken for worker 2 : 0:00:12.457031\n",
            "iteration: 23/150\n",
            "Local Step 24: Test Loss: 2.510397688, Test Accuracy: 35.580\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [25]: Training Loss: 2.471459457, Training Accuracy: 35.564\n",
            "Time taken for worker 1 : 0:00:12.663570\n",
            "iteration: 24/150\n",
            "Local Step 25: Test Loss: 2.491572857, Test Accuracy: 35.960\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [26]: Training Loss: 2.436355005, Training Accuracy: 36.240\n",
            "Time taken for worker 2 : 0:00:15.384346\n",
            "iteration: 25/150\n",
            "Local Step 26: Test Loss: 2.538055670, Test Accuracy: 35.330\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [27]: Training Loss: 2.409595275, Training Accuracy: 37.060\n",
            "Time taken for worker 1 : 0:00:12.493355\n",
            "iteration: 26/150\n",
            "Local Step 27: Test Loss: 2.418824396, Test Accuracy: 38.330\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [28]: Training Loss: 2.371522316, Training Accuracy: 37.732\n",
            "Time taken for worker 2 : 0:00:13.526849\n",
            "iteration: 27/150\n",
            "Local Step 28: Test Loss: 2.480637076, Test Accuracy: 37.030\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [29]: Training Loss: 2.354028007, Training Accuracy: 38.332\n",
            "Time taken for worker 1 : 0:00:13.788384\n",
            "iteration: 28/150\n",
            "Local Step 29: Test Loss: 2.447507887, Test Accuracy: 37.630\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [30]: Training Loss: 2.324017268, Training Accuracy: 38.832\n",
            "Time taken for worker 2 : 0:00:14.887225\n",
            "iteration: 29/150\n",
            "Local Step 30: Test Loss: 2.452265348, Test Accuracy: 37.190\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [31]: Training Loss: 2.297400223, Training Accuracy: 39.308\n",
            "Time taken for worker 1 : 0:00:12.375987\n",
            "iteration: 30/150\n",
            "Local Step 31: Test Loss: 2.514094747, Test Accuracy: 35.650\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [32]: Training Loss: 2.268546924, Training Accuracy: 39.556\n",
            "Time taken for worker 2 : 0:00:12.957262\n",
            "iteration: 31/150\n",
            "Local Step 32: Test Loss: 2.398606691, Test Accuracy: 38.480\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [33]: Training Loss: 2.272979056, Training Accuracy: 39.992\n",
            "Time taken for worker 1 : 0:00:12.287624\n",
            "iteration: 32/150\n",
            "Local Step 33: Test Loss: 2.370448425, Test Accuracy: 38.800\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [34]: Training Loss: 2.238743920, Training Accuracy: 40.536\n",
            "Time taken for worker 2 : 0:00:12.025840\n",
            "iteration: 33/150\n",
            "Local Step 34: Test Loss: 2.375074603, Test Accuracy: 38.810\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [35]: Training Loss: 2.255829273, Training Accuracy: 40.132\n",
            "Time taken for worker 1 : 0:00:13.533915\n",
            "iteration: 34/150\n",
            "Local Step 35: Test Loss: 2.350418787, Test Accuracy: 39.680\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [36]: Training Loss: 2.197443432, Training Accuracy: 41.632\n",
            "Time taken for worker 1 : 0:00:12.491660\n",
            "iteration: 35/150\n",
            "Local Step 36: Test Loss: 2.395080047, Test Accuracy: 38.030\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [37]: Training Loss: 2.196597942, Training Accuracy: 41.544\n",
            "Time taken for worker 2 : 0:00:13.277247\n",
            "iteration: 36/150\n",
            "Local Step 37: Test Loss: 2.385006925, Test Accuracy: 39.240\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [38]: Training Loss: 2.169364777, Training Accuracy: 42.040\n",
            "Time taken for worker 1 : 0:00:12.472530\n",
            "iteration: 37/150\n",
            "Local Step 38: Test Loss: 2.351281941, Test Accuracy: 39.620\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [39]: Training Loss: 2.160609840, Training Accuracy: 42.704\n",
            "Time taken for worker 2 : 0:00:12.199105\n",
            "iteration: 38/150\n",
            "Local Step 39: Test Loss: 2.386808095, Test Accuracy: 39.020\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [40]: Training Loss: 2.115873547, Training Accuracy: 43.452\n",
            "Time taken for worker 1 : 0:00:12.464301\n",
            "iteration: 39/150\n",
            "Local Step 40: Test Loss: 2.403897675, Test Accuracy: 38.430\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [41]: Training Loss: 2.122290863, Training Accuracy: 43.152\n",
            "Time taken for worker 2 : 0:00:14.000022\n",
            "iteration: 40/150\n",
            "Local Step 41: Test Loss: 2.332051211, Test Accuracy: 39.830\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [42]: Training Loss: 2.077663254, Training Accuracy: 44.512\n",
            "Time taken for worker 1 : 0:00:12.236674\n",
            "iteration: 41/150\n",
            "Local Step 42: Test Loss: 2.347146700, Test Accuracy: 40.040\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [43]: Training Loss: 2.081423796, Training Accuracy: 44.132\n",
            "Time taken for worker 2 : 0:00:12.418024\n",
            "iteration: 42/150\n",
            "Local Step 43: Test Loss: 2.399157866, Test Accuracy: 38.960\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [44]: Training Loss: 2.059063192, Training Accuracy: 44.220\n",
            "Time taken for worker 1 : 0:00:11.761676\n",
            "iteration: 43/150\n",
            "Local Step 44: Test Loss: 2.416371199, Test Accuracy: 39.660\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [45]: Training Loss: 2.039858909, Training Accuracy: 45.244\n",
            "Time taken for worker 2 : 0:00:12.412787\n",
            "iteration: 44/150\n",
            "Local Step 45: Test Loss: 2.441666821, Test Accuracy: 39.110\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [46]: Training Loss: 2.027315649, Training Accuracy: 45.112\n",
            "Time taken for worker 1 : 0:00:13.122251\n",
            "iteration: 45/150\n",
            "Local Step 46: Test Loss: 2.372070950, Test Accuracy: 40.650\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [47]: Training Loss: 2.038505844, Training Accuracy: 45.400\n",
            "Time taken for worker 2 : 0:00:13.168674\n",
            "iteration: 46/150\n",
            "Local Step 47: Test Loss: 2.393657652, Test Accuracy: 40.030\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [48]: Training Loss: 2.023604413, Training Accuracy: 45.400\n",
            "Time taken for worker 1 : 0:00:12.444159\n",
            "iteration: 47/150\n",
            "Local Step 48: Test Loss: 2.350710044, Test Accuracy: 40.980\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [49]: Training Loss: 2.003139730, Training Accuracy: 46.132\n",
            "Time taken for worker 2 : 0:00:12.083254\n",
            "iteration: 48/150\n",
            "Local Step 49: Test Loss: 2.378904829, Test Accuracy: 39.870\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [50]: Training Loss: 1.976173055, Training Accuracy: 46.676\n",
            "Time taken for worker 1 : 0:00:12.243785\n",
            "iteration: 49/150\n",
            "Local Step 50: Test Loss: 2.312273468, Test Accuracy: 40.950\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [51]: Training Loss: 1.977020182, Training Accuracy: 46.596\n",
            "Time taken for worker 2 : 0:00:12.086202\n",
            "iteration: 50/150\n",
            "Local Step 51: Test Loss: 2.289885799, Test Accuracy: 42.130\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [52]: Training Loss: 1.956531606, Training Accuracy: 46.808\n",
            "Time taken for worker 1 : 0:00:14.070681\n",
            "iteration: 51/150\n",
            "Local Step 52: Test Loss: 2.382171333, Test Accuracy: 40.680\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [53]: Training Loss: 1.934996752, Training Accuracy: 47.356\n",
            "Time taken for worker 2 : 0:00:12.737547\n",
            "iteration: 52/150\n",
            "Local Step 53: Test Loss: 2.380293156, Test Accuracy: 40.660\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [54]: Training Loss: 1.937323859, Training Accuracy: 46.796\n",
            "Time taken for worker 1 : 0:00:11.669458\n",
            "iteration: 53/150\n",
            "Local Step 54: Test Loss: 2.378204324, Test Accuracy: 41.220\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [55]: Training Loss: 1.915873363, Training Accuracy: 47.632\n",
            "Time taken for worker 2 : 0:00:12.811658\n",
            "iteration: 54/150\n",
            "Local Step 55: Test Loss: 2.385338032, Test Accuracy: 39.520\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [56]: Training Loss: 1.928761365, Training Accuracy: 46.976\n",
            "Time taken for worker 1 : 0:00:12.020126\n",
            "iteration: 55/150\n",
            "Local Step 56: Test Loss: 2.332394432, Test Accuracy: 41.570\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [57]: Training Loss: 1.891554433, Training Accuracy: 48.324\n",
            "Time taken for worker 2 : 0:00:12.524188\n",
            "iteration: 56/150\n",
            "Local Step 57: Test Loss: 2.272239201, Test Accuracy: 42.330\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [58]: Training Loss: 1.894997408, Training Accuracy: 48.088\n",
            "Time taken for worker 1 : 0:00:12.178639\n",
            "iteration: 57/150\n",
            "Local Step 58: Test Loss: 2.384330605, Test Accuracy: 39.950\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [59]: Training Loss: 1.875291271, Training Accuracy: 48.740\n",
            "Time taken for worker 2 : 0:00:14.347378\n",
            "iteration: 58/150\n",
            "Local Step 59: Test Loss: 2.326898412, Test Accuracy: 41.000\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [60]: Training Loss: 1.867932869, Training Accuracy: 48.540\n",
            "Time taken for worker 1 : 0:00:14.777872\n",
            "iteration: 59/150\n",
            "Local Step 60: Test Loss: 2.410655224, Test Accuracy: 40.950\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [61]: Training Loss: 1.864848415, Training Accuracy: 49.264\n",
            "Time taken for worker 2 : 0:00:12.296061\n",
            "iteration: 60/150\n",
            "Local Step 61: Test Loss: 2.298813621, Test Accuracy: 41.310\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [62]: Training Loss: 1.856277900, Training Accuracy: 49.140\n",
            "Time taken for worker 1 : 0:00:13.025919\n",
            "iteration: 61/150\n",
            "Local Step 62: Test Loss: 2.345728592, Test Accuracy: 40.940\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [63]: Training Loss: 1.841715714, Training Accuracy: 49.404\n",
            "Time taken for worker 2 : 0:00:11.592417\n",
            "iteration: 62/150\n",
            "Local Step 63: Test Loss: 2.279094734, Test Accuracy: 42.500\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [64]: Training Loss: 1.830109201, Training Accuracy: 49.796\n",
            "Time taken for worker 1 : 0:00:13.126697\n",
            "iteration: 63/150\n",
            "Local Step 64: Test Loss: 2.315557683, Test Accuracy: 42.090\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [65]: Training Loss: 1.814170457, Training Accuracy: 49.720\n",
            "Time taken for worker 2 : 0:00:11.898173\n",
            "iteration: 64/150\n",
            "Local Step 65: Test Loss: 2.281107297, Test Accuracy: 42.360\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [66]: Training Loss: 1.822236773, Training Accuracy: 49.376\n",
            "Time taken for worker 1 : 0:00:12.097541\n",
            "iteration: 65/150\n",
            "Local Step 66: Test Loss: 2.363889633, Test Accuracy: 41.100\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [67]: Training Loss: 1.799705577, Training Accuracy: 50.408\n",
            "Time taken for worker 2 : 0:00:11.880017\n",
            "iteration: 66/150\n",
            "Local Step 67: Test Loss: 2.349509708, Test Accuracy: 41.630\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [68]: Training Loss: 1.804343217, Training Accuracy: 50.320\n",
            "Time taken for worker 1 : 0:00:12.593470\n",
            "iteration: 67/150\n",
            "Local Step 68: Test Loss: 2.375306407, Test Accuracy: 41.450\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [69]: Training Loss: 1.776324917, Training Accuracy: 50.744\n",
            "Time taken for worker 2 : 0:00:12.311334\n",
            "iteration: 68/150\n",
            "Local Step 69: Test Loss: 2.385689931, Test Accuracy: 41.890\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [70]: Training Loss: 1.804884192, Training Accuracy: 49.780\n",
            "Time taken for worker 1 : 0:00:11.983662\n",
            "iteration: 69/150\n",
            "Local Step 70: Test Loss: 2.340594516, Test Accuracy: 42.300\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [71]: Training Loss: 1.748364089, Training Accuracy: 51.496\n",
            "Time taken for worker 2 : 0:00:14.080939\n",
            "iteration: 70/150\n",
            "Local Step 71: Test Loss: 2.364540803, Test Accuracy: 41.330\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [72]: Training Loss: 1.774907301, Training Accuracy: 50.872\n",
            "Time taken for worker 1 : 0:00:14.201120\n",
            "iteration: 71/150\n",
            "Local Step 72: Test Loss: 2.346158908, Test Accuracy: 41.500\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [73]: Training Loss: 1.751269106, Training Accuracy: 51.464\n",
            "Time taken for worker 1 : 0:00:11.880445\n",
            "iteration: 72/150\n",
            "Local Step 73: Test Loss: 2.364628171, Test Accuracy: 42.240\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [74]: Training Loss: 1.731311935, Training Accuracy: 51.744\n",
            "Time taken for worker 2 : 0:00:13.526339\n",
            "iteration: 73/150\n",
            "Local Step 74: Test Loss: 2.296925126, Test Accuracy: 43.410\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [75]: Training Loss: 1.738660741, Training Accuracy: 51.892\n",
            "Time taken for worker 1 : 0:00:11.875220\n",
            "iteration: 74/150\n",
            "Local Step 75: Test Loss: 2.354376573, Test Accuracy: 41.610\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [76]: Training Loss: 1.702742193, Training Accuracy: 52.104\n",
            "Time taken for worker 2 : 0:00:11.894114\n",
            "iteration: 75/150\n",
            "Local Step 76: Test Loss: 2.480118899, Test Accuracy: 40.990\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [77]: Training Loss: 1.722566718, Training Accuracy: 52.184\n",
            "Time taken for worker 1 : 0:00:14.482787\n",
            "iteration: 76/150\n",
            "Local Step 77: Test Loss: 2.324369731, Test Accuracy: 42.880\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [78]: Training Loss: 1.736793340, Training Accuracy: 51.664\n",
            "Time taken for worker 2 : 0:00:12.536535\n",
            "iteration: 77/150\n",
            "Local Step 78: Test Loss: 2.373343588, Test Accuracy: 42.650\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [79]: Training Loss: 1.713214598, Training Accuracy: 52.300\n",
            "Time taken for worker 1 : 0:00:12.205442\n",
            "iteration: 78/150\n",
            "Local Step 79: Test Loss: 2.337698901, Test Accuracy: 41.750\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [80]: Training Loss: 1.692236473, Training Accuracy: 53.136\n",
            "Time taken for worker 2 : 0:00:14.801243\n",
            "iteration: 79/150\n",
            "Local Step 80: Test Loss: 2.314852334, Test Accuracy: 43.540\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [81]: Training Loss: 1.700843546, Training Accuracy: 52.764\n",
            "Time taken for worker 1 : 0:00:13.238745\n",
            "iteration: 80/150\n",
            "Local Step 81: Test Loss: 2.364267445, Test Accuracy: 42.250\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [82]: Training Loss: 1.698738307, Training Accuracy: 52.932\n",
            "Time taken for worker 2 : 0:00:13.431126\n",
            "iteration: 81/150\n",
            "Local Step 82: Test Loss: 2.292619461, Test Accuracy: 42.860\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [83]: Training Loss: 1.707927137, Training Accuracy: 52.360\n",
            "Time taken for worker 1 : 0:00:13.797101\n",
            "iteration: 82/150\n",
            "Local Step 83: Test Loss: 2.404936418, Test Accuracy: 41.820\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [84]: Training Loss: 1.667715432, Training Accuracy: 53.232\n",
            "Time taken for worker 2 : 0:00:15.328774\n",
            "iteration: 83/150\n",
            "Local Step 84: Test Loss: 2.371050660, Test Accuracy: 42.040\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [85]: Training Loss: 1.708292216, Training Accuracy: 52.432\n",
            "Time taken for worker 1 : 0:00:14.384313\n",
            "iteration: 84/150\n",
            "Local Step 85: Test Loss: 2.370174431, Test Accuracy: 41.690\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [86]: Training Loss: 1.670760537, Training Accuracy: 53.524\n",
            "Time taken for worker 2 : 0:00:12.909772\n",
            "iteration: 85/150\n",
            "Local Step 86: Test Loss: 2.332079218, Test Accuracy: 43.500\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [87]: Training Loss: 1.671446494, Training Accuracy: 53.400\n",
            "Time taken for worker 1 : 0:00:13.699110\n",
            "iteration: 86/150\n",
            "Local Step 87: Test Loss: 2.370537087, Test Accuracy: 42.110\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [88]: Training Loss: 1.650745166, Training Accuracy: 53.832\n",
            "Time taken for worker 2 : 0:00:12.927515\n",
            "iteration: 87/150\n",
            "Local Step 88: Test Loss: 2.365537848, Test Accuracy: 42.410\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [89]: Training Loss: 1.656081053, Training Accuracy: 53.840\n",
            "Time taken for worker 1 : 0:00:12.303158\n",
            "iteration: 88/150\n",
            "Local Step 89: Test Loss: 2.381852691, Test Accuracy: 42.040\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [90]: Training Loss: 1.647104844, Training Accuracy: 53.884\n",
            "Time taken for worker 2 : 0:00:12.781758\n",
            "iteration: 89/150\n",
            "Local Step 90: Test Loss: 2.349207554, Test Accuracy: 42.870\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [91]: Training Loss: 1.654093852, Training Accuracy: 53.920\n",
            "Time taken for worker 1 : 0:00:11.821568\n",
            "iteration: 90/150\n",
            "Local Step 91: Test Loss: 2.356880589, Test Accuracy: 42.650\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [92]: Training Loss: 1.627300727, Training Accuracy: 54.828\n",
            "Time taken for worker 2 : 0:00:12.979883\n",
            "iteration: 91/150\n",
            "Local Step 92: Test Loss: 2.375976023, Test Accuracy: 42.550\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [93]: Training Loss: 1.629105951, Training Accuracy: 54.200\n",
            "Time taken for worker 1 : 0:00:11.691255\n",
            "iteration: 92/150\n",
            "Local Step 93: Test Loss: 2.360558334, Test Accuracy: 43.620\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [94]: Training Loss: 1.621436825, Training Accuracy: 54.224\n",
            "Time taken for worker 2 : 0:00:11.800678\n",
            "iteration: 93/150\n",
            "Local Step 94: Test Loss: 2.411302682, Test Accuracy: 42.350\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [95]: Training Loss: 1.639977666, Training Accuracy: 54.192\n",
            "Time taken for worker 1 : 0:00:11.927347\n",
            "iteration: 94/150\n",
            "Local Step 95: Test Loss: 2.467118314, Test Accuracy: 40.730\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [96]: Training Loss: 1.603070172, Training Accuracy: 54.976\n",
            "Time taken for worker 2 : 0:00:12.145283\n",
            "iteration: 95/150\n",
            "Local Step 96: Test Loss: 2.328115844, Test Accuracy: 44.010\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [97]: Training Loss: 1.629990330, Training Accuracy: 54.604\n",
            "Time taken for worker 1 : 0:00:14.709973\n",
            "iteration: 96/150\n",
            "Local Step 97: Test Loss: 2.357724549, Test Accuracy: 42.560\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [98]: Training Loss: 1.600739423, Training Accuracy: 55.288\n",
            "Time taken for worker 2 : 0:00:13.465152\n",
            "iteration: 97/150\n",
            "Local Step 98: Test Loss: 2.364487423, Test Accuracy: 42.450\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [99]: Training Loss: 1.626986176, Training Accuracy: 54.120\n",
            "Time taken for worker 1 : 0:00:13.685552\n",
            "iteration: 98/150\n",
            "Local Step 99: Test Loss: 2.359820944, Test Accuracy: 43.780\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [100]: Training Loss: 1.593061389, Training Accuracy: 55.560\n",
            "Time taken for worker 2 : 0:00:13.512390\n",
            "iteration: 99/150\n",
            "Local Step 100: Test Loss: 2.340999176, Test Accuracy: 43.700\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [101]: Training Loss: 1.606036251, Training Accuracy: 54.848\n",
            "Time taken for worker 1 : 0:00:12.506796\n",
            "iteration: 100/150\n",
            "Local Step 101: Test Loss: 2.342768377, Test Accuracy: 43.060\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [102]: Training Loss: 1.571543450, Training Accuracy: 55.976\n",
            "Time taken for worker 2 : 0:00:12.044590\n",
            "iteration: 101/150\n",
            "Local Step 102: Test Loss: 2.331401063, Test Accuracy: 43.210\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [103]: Training Loss: 1.586326479, Training Accuracy: 55.492\n",
            "Time taken for worker 1 : 0:00:13.077435\n",
            "iteration: 102/150\n",
            "Local Step 103: Test Loss: 2.405813054, Test Accuracy: 42.020\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [104]: Training Loss: 1.561609486, Training Accuracy: 56.316\n",
            "Time taken for worker 2 : 0:00:12.990697\n",
            "iteration: 103/150\n",
            "Local Step 104: Test Loss: 2.352105111, Test Accuracy: 43.430\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [105]: Training Loss: 1.599125309, Training Accuracy: 55.032\n",
            "Time taken for worker 1 : 0:00:11.705258\n",
            "iteration: 104/150\n",
            "Local Step 105: Test Loss: 2.433219619, Test Accuracy: 42.590\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [106]: Training Loss: 1.562081622, Training Accuracy: 56.080\n",
            "Time taken for worker 2 : 0:00:12.253599\n",
            "iteration: 105/150\n",
            "Local Step 106: Test Loss: 2.394682033, Test Accuracy: 42.840\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [107]: Training Loss: 1.581605648, Training Accuracy: 55.504\n",
            "Time taken for worker 1 : 0:00:13.267592\n",
            "iteration: 106/150\n",
            "Local Step 107: Test Loss: 2.374113090, Test Accuracy: 43.080\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [108]: Training Loss: 1.557803158, Training Accuracy: 56.096\n",
            "Time taken for worker 2 : 0:00:11.901259\n",
            "iteration: 107/150\n",
            "Local Step 108: Test Loss: 2.346577868, Test Accuracy: 43.920\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [109]: Training Loss: 1.584555729, Training Accuracy: 55.472\n",
            "Time taken for worker 1 : 0:00:14.463596\n",
            "iteration: 108/150\n",
            "Local Step 109: Test Loss: 2.434244678, Test Accuracy: 41.210\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [110]: Training Loss: 1.564095782, Training Accuracy: 55.760\n",
            "Time taken for worker 1 : 0:00:12.076872\n",
            "iteration: 109/150\n",
            "Local Step 110: Test Loss: 2.384947840, Test Accuracy: 42.980\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [111]: Training Loss: 1.543557553, Training Accuracy: 56.524\n",
            "Time taken for worker 2 : 0:00:12.138335\n",
            "iteration: 110/150\n",
            "Local Step 111: Test Loss: 2.420912472, Test Accuracy: 42.680\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [112]: Training Loss: 1.546806524, Training Accuracy: 56.136\n",
            "Time taken for worker 1 : 0:00:13.415944\n",
            "iteration: 111/150\n",
            "Local Step 112: Test Loss: 2.432963815, Test Accuracy: 41.960\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [113]: Training Loss: 1.508716050, Training Accuracy: 57.512\n",
            "Time taken for worker 2 : 0:00:13.093609\n",
            "iteration: 112/150\n",
            "Local Step 113: Test Loss: 2.385235824, Test Accuracy: 42.880\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [114]: Training Loss: 1.551985180, Training Accuracy: 56.236\n",
            "Time taken for worker 1 : 0:00:12.161224\n",
            "iteration: 113/150\n",
            "Local Step 114: Test Loss: 2.456334207, Test Accuracy: 42.370\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [115]: Training Loss: 1.519908950, Training Accuracy: 56.960\n",
            "Time taken for worker 2 : 0:00:11.634010\n",
            "iteration: 114/150\n",
            "Local Step 115: Test Loss: 2.416145886, Test Accuracy: 42.840\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [116]: Training Loss: 1.561224761, Training Accuracy: 56.052\n",
            "Time taken for worker 1 : 0:00:14.063370\n",
            "iteration: 115/150\n",
            "Local Step 116: Test Loss: 2.444509707, Test Accuracy: 42.520\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [117]: Training Loss: 1.509829755, Training Accuracy: 57.296\n",
            "Time taken for worker 2 : 0:00:13.744171\n",
            "iteration: 116/150\n",
            "Local Step 117: Test Loss: 2.433599235, Test Accuracy: 42.640\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [118]: Training Loss: 1.544697880, Training Accuracy: 56.216\n",
            "Time taken for worker 1 : 0:00:12.500644\n",
            "iteration: 117/150\n",
            "Local Step 118: Test Loss: 2.471725655, Test Accuracy: 41.140\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [119]: Training Loss: 1.515812638, Training Accuracy: 57.376\n",
            "Time taken for worker 2 : 0:00:13.394894\n",
            "iteration: 118/150\n",
            "Local Step 119: Test Loss: 2.390772674, Test Accuracy: 44.080\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [120]: Training Loss: 1.516184148, Training Accuracy: 56.832\n",
            "Time taken for worker 1 : 0:00:12.195128\n",
            "iteration: 119/150\n",
            "Local Step 120: Test Loss: 2.390454211, Test Accuracy: 43.670\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [121]: Training Loss: 1.499443718, Training Accuracy: 57.724\n",
            "Time taken for worker 2 : 0:00:11.584381\n",
            "iteration: 120/150\n",
            "Local Step 121: Test Loss: 2.402801590, Test Accuracy: 43.300\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [122]: Training Loss: 1.519640471, Training Accuracy: 56.964\n",
            "Time taken for worker 1 : 0:00:14.225567\n",
            "iteration: 121/150\n",
            "Local Step 122: Test Loss: 2.488360419, Test Accuracy: 41.650\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [123]: Training Loss: 1.508542844, Training Accuracy: 57.560\n",
            "Time taken for worker 2 : 0:00:11.634187\n",
            "iteration: 122/150\n",
            "Local Step 123: Test Loss: 2.380148535, Test Accuracy: 42.930\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [124]: Training Loss: 1.516812825, Training Accuracy: 56.972\n",
            "Time taken for worker 1 : 0:00:13.822484\n",
            "iteration: 123/150\n",
            "Local Step 124: Test Loss: 2.507031009, Test Accuracy: 41.880\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [125]: Training Loss: 1.487716056, Training Accuracy: 58.116\n",
            "Time taken for worker 2 : 0:00:12.254616\n",
            "iteration: 124/150\n",
            "Local Step 125: Test Loss: 2.527341488, Test Accuracy: 41.110\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [126]: Training Loss: 1.511051194, Training Accuracy: 57.176\n",
            "Time taken for worker 1 : 0:00:12.224898\n",
            "iteration: 125/150\n",
            "Local Step 126: Test Loss: 2.483813826, Test Accuracy: 42.470\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [127]: Training Loss: 1.487484418, Training Accuracy: 57.592\n",
            "Time taken for worker 2 : 0:00:13.051841\n",
            "iteration: 126/150\n",
            "Local Step 127: Test Loss: 2.412974633, Test Accuracy: 42.820\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [128]: Training Loss: 1.514421187, Training Accuracy: 57.016\n",
            "Time taken for worker 1 : 0:00:12.943804\n",
            "iteration: 127/150\n",
            "Local Step 128: Test Loss: 2.470443624, Test Accuracy: 42.960\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [129]: Training Loss: 1.476596801, Training Accuracy: 58.120\n",
            "Time taken for worker 2 : 0:00:12.009474\n",
            "iteration: 128/150\n",
            "Local Step 129: Test Loss: 2.376230846, Test Accuracy: 43.620\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [130]: Training Loss: 1.483604423, Training Accuracy: 57.988\n",
            "Time taken for worker 1 : 0:00:11.319287\n",
            "iteration: 129/150\n",
            "Local Step 130: Test Loss: 2.428222047, Test Accuracy: 43.530\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [131]: Training Loss: 1.472586850, Training Accuracy: 58.404\n",
            "Time taken for worker 2 : 0:00:11.916104\n",
            "iteration: 130/150\n",
            "Local Step 131: Test Loss: 2.366688645, Test Accuracy: 43.630\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [132]: Training Loss: 1.486910838, Training Accuracy: 57.824\n",
            "Time taken for worker 1 : 0:00:13.249414\n",
            "iteration: 131/150\n",
            "Local Step 132: Test Loss: 2.506248462, Test Accuracy: 42.480\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [133]: Training Loss: 1.462306939, Training Accuracy: 58.272\n",
            "Time taken for worker 2 : 0:00:11.906677\n",
            "iteration: 132/150\n",
            "Local Step 133: Test Loss: 2.413761125, Test Accuracy: 42.690\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [134]: Training Loss: 1.488862826, Training Accuracy: 57.812\n",
            "Time taken for worker 1 : 0:00:14.066848\n",
            "iteration: 133/150\n",
            "Local Step 134: Test Loss: 2.569705435, Test Accuracy: 40.960\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [135]: Training Loss: 1.466092725, Training Accuracy: 58.332\n",
            "Time taken for worker 2 : 0:00:13.033591\n",
            "iteration: 134/150\n",
            "Local Step 135: Test Loss: 2.407011203, Test Accuracy: 43.870\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [136]: Training Loss: 1.482481920, Training Accuracy: 58.060\n",
            "Time taken for worker 1 : 0:00:13.682312\n",
            "iteration: 135/150\n",
            "Local Step 136: Test Loss: 2.469016588, Test Accuracy: 41.610\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [137]: Training Loss: 1.456717314, Training Accuracy: 58.592\n",
            "Time taken for worker 2 : 0:00:12.196367\n",
            "iteration: 136/150\n",
            "Local Step 137: Test Loss: 2.396579809, Test Accuracy: 43.280\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [138]: Training Loss: 1.484622240, Training Accuracy: 58.096\n",
            "Time taken for worker 1 : 0:00:12.182203\n",
            "iteration: 137/150\n",
            "Local Step 138: Test Loss: 2.488681338, Test Accuracy: 41.740\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [139]: Training Loss: 1.440172731, Training Accuracy: 58.936\n",
            "Time taken for worker 2 : 0:00:12.107380\n",
            "iteration: 138/150\n",
            "Local Step 139: Test Loss: 2.360454654, Test Accuracy: 44.320\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [140]: Training Loss: 1.487552643, Training Accuracy: 58.068\n",
            "Time taken for worker 1 : 0:00:11.597750\n",
            "iteration: 139/150\n",
            "Local Step 140: Test Loss: 2.491208900, Test Accuracy: 41.880\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [141]: Training Loss: 1.439412639, Training Accuracy: 59.240\n",
            "Time taken for worker 2 : 0:00:12.385775\n",
            "iteration: 140/150\n",
            "Local Step 141: Test Loss: 2.350031531, Test Accuracy: 44.020\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [142]: Training Loss: 1.467568177, Training Accuracy: 58.588\n",
            "Time taken for worker 1 : 0:00:12.079384\n",
            "iteration: 141/150\n",
            "Local Step 142: Test Loss: 2.457129876, Test Accuracy: 43.140\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [143]: Training Loss: 1.427056122, Training Accuracy: 59.204\n",
            "Time taken for worker 2 : 0:00:11.870247\n",
            "iteration: 142/150\n",
            "Local Step 143: Test Loss: 2.546004787, Test Accuracy: 41.830\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [144]: Training Loss: 1.479511102, Training Accuracy: 57.656\n",
            "Time taken for worker 1 : 0:00:12.043420\n",
            "iteration: 143/150\n",
            "Local Step 144: Test Loss: 2.443298196, Test Accuracy: 42.920\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [145]: Training Loss: 1.438581433, Training Accuracy: 58.840\n",
            "Time taken for worker 2 : 0:00:12.132687\n",
            "iteration: 144/150\n",
            "Local Step 145: Test Loss: 2.413373533, Test Accuracy: 43.510\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [146]: Training Loss: 1.459820847, Training Accuracy: 58.456\n",
            "Time taken for worker 1 : 0:00:11.973416\n",
            "iteration: 145/150\n",
            "Local Step 146: Test Loss: 2.462540081, Test Accuracy: 42.870\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [147]: Training Loss: 1.460592846, Training Accuracy: 58.456\n",
            "Time taken for worker 1 : 0:00:11.774464\n",
            "iteration: 146/150\n",
            "Local Step 147: Test Loss: 2.401492312, Test Accuracy: 42.790\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [148]: Training Loss: 1.442264674, Training Accuracy: 59.456\n",
            "Time taken for worker 2 : 0:00:11.940912\n",
            "iteration: 147/150\n",
            "Local Step 148: Test Loss: 2.420945132, Test Accuracy: 43.050\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [149]: Training Loss: 1.449690640, Training Accuracy: 59.004\n",
            "Time taken for worker 1 : 0:00:11.607304\n",
            "iteration: 148/150\n",
            "Local Step 149: Test Loss: 2.400653677, Test Accuracy: 43.490\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [150]: Training Loss: 1.426320329, Training Accuracy: 59.132\n",
            "Time taken for worker 2 : 0:00:14.131468\n",
            "iteration: 149/150\n",
            "Local Step 150: Test Loss: 2.438015169, Test Accuracy: 43.370\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for SHAT: 0:34:53.455499\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:4\n",
            "==================================================\n",
            "Original Computation Latency: [8163, 8596, 9003, 8671]\n",
            "Scaled Computation Latency: [1.0, 1.053044223937278, 1.1029033443586918, 1.0622320225407327]\n",
            "workers simulated orders based on computation latency:[1, 2, 4, 3, 1, 2, 4, 3, 1, 2, 4, 3, 1, 2, 4, 3, 1, 2, 4, 3, 1, 2, 4, 3, 1, 2, 4, 3, 1, 2, 4, 3, 1, 2, 4, 3, 1, 2, 4, 1, 3, 2, 4, 1, 3, 2, 4, 1, 3, 2, 4, 1, 3, 2, 4, 1, 3, 2, 4, 1, 3, 2, 4, 1, 3, 2, 1, 4, 3, 2, 1, 4, 3, 1, 2, 4, 3, 1, 2, 4, 1, 3, 2, 4, 1, 3, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 3, 1, 2, 4, 3, 1, 2, 4, 3, 1, 2, 4, 1, 3, 2, 4, 1, 3, 2, 1, 4, 3, 2, 1, 4, 3, 2, 1, 4, 3, 2, 1, 4, 3, 2, 1, 4, 3, 1]\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [01]: Training Loss: 4.481195053, Training Accuracy: 2.768\n",
            "Time taken for worker 1 : 0:00:06.400880\n",
            "iteration: 00/150\n",
            "Local Step 01: Test Loss: 4.221579509, Test Accuracy: 4.630\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [02]: Training Loss: 4.470635164, Training Accuracy: 2.888\n",
            "Time taken for worker 2 : 0:00:06.309430\n",
            "iteration: 01/150\n",
            "Local Step 02: Test Loss: 4.218816701, Test Accuracy: 5.600\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [03]: Training Loss: 4.480245053, Training Accuracy: 2.568\n",
            "Time taken for worker 4 : 0:00:06.564334\n",
            "iteration: 02/150\n",
            "Local Step 03: Test Loss: 4.239217813, Test Accuracy: 5.010\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [04]: Training Loss: 4.477662444, Training Accuracy: 2.960\n",
            "Time taken for worker 3 : 0:00:06.889179\n",
            "iteration: 03/150\n",
            "Local Step 04: Test Loss: 4.277617524, Test Accuracy: 4.780\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [05]: Training Loss: 4.125816328, Training Accuracy: 6.144\n",
            "Time taken for worker 1 : 0:00:06.951883\n",
            "iteration: 04/150\n",
            "Local Step 05: Test Loss: 3.956521141, Test Accuracy: 9.000\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [06]: Training Loss: 4.113317491, Training Accuracy: 6.520\n",
            "Time taken for worker 2 : 0:00:05.801944\n",
            "iteration: 05/150\n",
            "Local Step 06: Test Loss: 3.945110590, Test Accuracy: 8.800\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [07]: Training Loss: 4.135499314, Training Accuracy: 6.208\n",
            "Time taken for worker 4 : 0:00:05.908002\n",
            "iteration: 06/150\n",
            "Local Step 07: Test Loss: 3.954270055, Test Accuracy: 8.790\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [08]: Training Loss: 4.144167359, Training Accuracy: 6.312\n",
            "Time taken for worker 3 : 0:00:06.063990\n",
            "iteration: 07/150\n",
            "Local Step 08: Test Loss: 4.000171380, Test Accuracy: 7.500\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [09]: Training Loss: 3.917159630, Training Accuracy: 9.080\n",
            "Time taken for worker 1 : 0:00:07.061461\n",
            "iteration: 08/150\n",
            "Local Step 09: Test Loss: 3.834767038, Test Accuracy: 10.620\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [10]: Training Loss: 3.900721156, Training Accuracy: 9.072\n",
            "Time taken for worker 2 : 0:00:06.971211\n",
            "iteration: 09/150\n",
            "Local Step 10: Test Loss: 3.791107645, Test Accuracy: 11.360\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [11]: Training Loss: 3.921674496, Training Accuracy: 9.360\n",
            "Time taken for worker 4 : 0:00:05.845515\n",
            "iteration: 10/150\n",
            "Local Step 11: Test Loss: 3.784555241, Test Accuracy: 12.610\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [12]: Training Loss: 3.960260785, Training Accuracy: 8.584\n",
            "Time taken for worker 3 : 0:00:06.094405\n",
            "iteration: 11/150\n",
            "Local Step 12: Test Loss: 3.827873321, Test Accuracy: 11.700\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [13]: Training Loss: 3.752885624, Training Accuracy: 11.688\n",
            "Time taken for worker 1 : 0:00:06.761143\n",
            "iteration: 12/150\n",
            "Local Step 13: Test Loss: 3.667601312, Test Accuracy: 13.200\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [14]: Training Loss: 3.736828269, Training Accuracy: 11.928\n",
            "Time taken for worker 2 : 0:00:06.056784\n",
            "iteration: 13/150\n",
            "Local Step 14: Test Loss: 3.693736776, Test Accuracy: 12.500\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [15]: Training Loss: 3.770175210, Training Accuracy: 11.648\n",
            "Time taken for worker 4 : 0:00:06.545302\n",
            "iteration: 14/150\n",
            "Local Step 15: Test Loss: 3.690069773, Test Accuracy: 13.290\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [16]: Training Loss: 3.792707593, Training Accuracy: 11.544\n",
            "Time taken for worker 3 : 0:00:07.028416\n",
            "iteration: 15/150\n",
            "Local Step 16: Test Loss: 3.681116084, Test Accuracy: 12.840\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [17]: Training Loss: 3.618418988, Training Accuracy: 14.104\n",
            "Time taken for worker 1 : 0:00:05.988199\n",
            "iteration: 16/150\n",
            "Local Step 17: Test Loss: 3.517198980, Test Accuracy: 16.630\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [18]: Training Loss: 3.590057526, Training Accuracy: 13.936\n",
            "Time taken for worker 2 : 0:00:06.315004\n",
            "iteration: 17/150\n",
            "Local Step 18: Test Loss: 3.581465817, Test Accuracy: 15.460\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [19]: Training Loss: 3.621899219, Training Accuracy: 13.672\n",
            "Time taken for worker 4 : 0:00:06.036850\n",
            "iteration: 18/150\n",
            "Local Step 19: Test Loss: 3.588081917, Test Accuracy: 15.290\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [20]: Training Loss: 3.634480537, Training Accuracy: 14.200\n",
            "Time taken for worker 3 : 0:00:06.122779\n",
            "iteration: 19/150\n",
            "Local Step 20: Test Loss: 3.550317717, Test Accuracy: 16.440\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [21]: Training Loss: 3.490859156, Training Accuracy: 16.112\n",
            "Time taken for worker 1 : 0:00:06.106001\n",
            "iteration: 20/150\n",
            "Local Step 21: Test Loss: 3.420745001, Test Accuracy: 17.630\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [22]: Training Loss: 3.442292537, Training Accuracy: 16.432\n",
            "Time taken for worker 2 : 0:00:05.829519\n",
            "iteration: 21/150\n",
            "Local Step 22: Test Loss: 3.395424618, Test Accuracy: 17.990\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [23]: Training Loss: 3.523167799, Training Accuracy: 15.648\n",
            "Time taken for worker 4 : 0:00:06.191835\n",
            "iteration: 22/150\n",
            "Local Step 23: Test Loss: 3.452870128, Test Accuracy: 17.800\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [24]: Training Loss: 3.535674905, Training Accuracy: 15.760\n",
            "Time taken for worker 3 : 0:00:05.969746\n",
            "iteration: 23/150\n",
            "Local Step 24: Test Loss: 3.434315347, Test Accuracy: 17.810\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [25]: Training Loss: 3.378091585, Training Accuracy: 18.320\n",
            "Time taken for worker 1 : 0:00:06.354808\n",
            "iteration: 24/150\n",
            "Local Step 25: Test Loss: 3.339268826, Test Accuracy: 19.310\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [26]: Training Loss: 3.353695980, Training Accuracy: 17.992\n",
            "Time taken for worker 2 : 0:00:05.882047\n",
            "iteration: 25/150\n",
            "Local Step 26: Test Loss: 3.335056729, Test Accuracy: 18.750\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [27]: Training Loss: 3.390009381, Training Accuracy: 17.672\n",
            "Time taken for worker 4 : 0:00:06.458904\n",
            "iteration: 26/150\n",
            "Local Step 27: Test Loss: 3.329033041, Test Accuracy: 19.070\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [28]: Training Loss: 3.404757839, Training Accuracy: 17.616\n",
            "Time taken for worker 3 : 0:00:06.603497\n",
            "iteration: 27/150\n",
            "Local Step 28: Test Loss: 3.307379639, Test Accuracy: 20.000\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [29]: Training Loss: 3.277837366, Training Accuracy: 19.952\n",
            "Time taken for worker 1 : 0:00:06.858296\n",
            "iteration: 28/150\n",
            "Local Step 29: Test Loss: 3.235506325, Test Accuracy: 21.020\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [30]: Training Loss: 3.249141755, Training Accuracy: 19.624\n",
            "Time taken for worker 2 : 0:00:05.775580\n",
            "iteration: 29/150\n",
            "Local Step 30: Test Loss: 3.283772677, Test Accuracy: 20.460\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [31]: Training Loss: 3.290210230, Training Accuracy: 19.744\n",
            "Time taken for worker 4 : 0:00:05.783014\n",
            "iteration: 30/150\n",
            "Local Step 31: Test Loss: 3.193419848, Test Accuracy: 22.230\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [32]: Training Loss: 3.282667231, Training Accuracy: 19.920\n",
            "Time taken for worker 3 : 0:00:05.832765\n",
            "iteration: 31/150\n",
            "Local Step 32: Test Loss: 3.253379753, Test Accuracy: 21.080\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [33]: Training Loss: 3.177982974, Training Accuracy: 21.848\n",
            "Time taken for worker 1 : 0:00:06.494558\n",
            "iteration: 32/150\n",
            "Local Step 33: Test Loss: 3.144726254, Test Accuracy: 23.300\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [34]: Training Loss: 3.135400318, Training Accuracy: 21.864\n",
            "Time taken for worker 2 : 0:00:06.213393\n",
            "iteration: 33/150\n",
            "Local Step 34: Test Loss: 3.211364312, Test Accuracy: 21.620\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [35]: Training Loss: 3.187669348, Training Accuracy: 21.152\n",
            "Time taken for worker 4 : 0:00:07.065528\n",
            "iteration: 34/150\n",
            "Local Step 35: Test Loss: 3.179054713, Test Accuracy: 22.740\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [36]: Training Loss: 3.194314910, Training Accuracy: 21.264\n",
            "Time taken for worker 3 : 0:00:06.199391\n",
            "iteration: 35/150\n",
            "Local Step 36: Test Loss: 3.162421980, Test Accuracy: 22.920\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [37]: Training Loss: 3.080878711, Training Accuracy: 23.376\n",
            "Time taken for worker 1 : 0:00:06.434204\n",
            "iteration: 36/150\n",
            "Local Step 37: Test Loss: 3.105309807, Test Accuracy: 23.760\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [38]: Training Loss: 3.056326097, Training Accuracy: 23.728\n",
            "Time taken for worker 2 : 0:00:05.815415\n",
            "iteration: 37/150\n",
            "Local Step 38: Test Loss: 3.080072333, Test Accuracy: 24.250\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [39]: Training Loss: 3.102629267, Training Accuracy: 23.536\n",
            "Time taken for worker 4 : 0:00:06.085399\n",
            "iteration: 38/150\n",
            "Local Step 39: Test Loss: 3.127593917, Test Accuracy: 23.680\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [40]: Training Loss: 3.004757065, Training Accuracy: 24.200\n",
            "Time taken for worker 1 : 0:00:06.309697\n",
            "iteration: 39/150\n",
            "Local Step 40: Test Loss: 3.046583053, Test Accuracy: 24.450\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [41]: Training Loss: 3.113999678, Training Accuracy: 22.912\n",
            "Time taken for worker 3 : 0:00:05.785732\n",
            "iteration: 40/150\n",
            "Local Step 41: Test Loss: 3.072585845, Test Accuracy: 24.020\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [42]: Training Loss: 2.956713117, Training Accuracy: 25.512\n",
            "Time taken for worker 2 : 0:00:06.051985\n",
            "iteration: 41/150\n",
            "Local Step 42: Test Loss: 3.042069516, Test Accuracy: 25.750\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [43]: Training Loss: 3.012316190, Training Accuracy: 24.664\n",
            "Time taken for worker 4 : 0:00:06.727984\n",
            "iteration: 42/150\n",
            "Local Step 43: Test Loss: 3.090846075, Test Accuracy: 23.880\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [44]: Training Loss: 2.930477158, Training Accuracy: 26.704\n",
            "Time taken for worker 1 : 0:00:06.314584\n",
            "iteration: 43/150\n",
            "Local Step 44: Test Loss: 3.048022457, Test Accuracy: 25.860\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [45]: Training Loss: 3.019165164, Training Accuracy: 24.368\n",
            "Time taken for worker 3 : 0:00:06.258847\n",
            "iteration: 44/150\n",
            "Local Step 45: Test Loss: 3.076266050, Test Accuracy: 24.380\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [46]: Training Loss: 2.892487166, Training Accuracy: 26.904\n",
            "Time taken for worker 2 : 0:00:06.003234\n",
            "iteration: 45/150\n",
            "Local Step 46: Test Loss: 2.972579832, Test Accuracy: 26.250\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [47]: Training Loss: 2.928059932, Training Accuracy: 26.544\n",
            "Time taken for worker 4 : 0:00:06.113265\n",
            "iteration: 46/150\n",
            "Local Step 47: Test Loss: 2.981123713, Test Accuracy: 26.870\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [48]: Training Loss: 2.854706336, Training Accuracy: 27.760\n",
            "Time taken for worker 1 : 0:00:05.733607\n",
            "iteration: 47/150\n",
            "Local Step 48: Test Loss: 2.929182848, Test Accuracy: 26.350\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [49]: Training Loss: 2.934649028, Training Accuracy: 26.184\n",
            "Time taken for worker 3 : 0:00:05.946103\n",
            "iteration: 48/150\n",
            "Local Step 49: Test Loss: 3.034210658, Test Accuracy: 25.250\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [50]: Training Loss: 2.806713137, Training Accuracy: 28.024\n",
            "Time taken for worker 2 : 0:00:07.376535\n",
            "iteration: 49/150\n",
            "Local Step 50: Test Loss: 2.934413810, Test Accuracy: 26.940\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [51]: Training Loss: 2.844796012, Training Accuracy: 27.808\n",
            "Time taken for worker 4 : 0:00:06.050120\n",
            "iteration: 50/150\n",
            "Local Step 51: Test Loss: 2.937288747, Test Accuracy: 27.090\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [52]: Training Loss: 2.776033646, Training Accuracy: 29.168\n",
            "Time taken for worker 1 : 0:00:05.978773\n",
            "iteration: 51/150\n",
            "Local Step 52: Test Loss: 2.889385176, Test Accuracy: 27.970\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [53]: Training Loss: 2.869160764, Training Accuracy: 27.264\n",
            "Time taken for worker 3 : 0:00:06.145875\n",
            "iteration: 52/150\n",
            "Local Step 53: Test Loss: 2.957245928, Test Accuracy: 26.410\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [54]: Training Loss: 2.749306227, Training Accuracy: 29.056\n",
            "Time taken for worker 2 : 0:00:07.144670\n",
            "iteration: 53/150\n",
            "Local Step 54: Test Loss: 2.898168190, Test Accuracy: 27.710\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [55]: Training Loss: 2.759038002, Training Accuracy: 29.600\n",
            "Time taken for worker 4 : 0:00:06.410127\n",
            "iteration: 54/150\n",
            "Local Step 55: Test Loss: 2.869422557, Test Accuracy: 28.340\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [56]: Training Loss: 2.702957993, Training Accuracy: 30.408\n",
            "Time taken for worker 1 : 0:00:05.841324\n",
            "iteration: 55/150\n",
            "Local Step 56: Test Loss: 2.829841620, Test Accuracy: 28.720\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [57]: Training Loss: 2.809155038, Training Accuracy: 28.520\n",
            "Time taken for worker 3 : 0:00:06.107603\n",
            "iteration: 56/150\n",
            "Local Step 57: Test Loss: 2.865445078, Test Accuracy: 28.480\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [58]: Training Loss: 2.675470415, Training Accuracy: 30.664\n",
            "Time taken for worker 2 : 0:00:06.285276\n",
            "iteration: 57/150\n",
            "Local Step 58: Test Loss: 2.807374165, Test Accuracy: 29.180\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [59]: Training Loss: 2.689842537, Training Accuracy: 30.568\n",
            "Time taken for worker 4 : 0:00:07.018698\n",
            "iteration: 58/150\n",
            "Local Step 59: Test Loss: 2.839351862, Test Accuracy: 28.970\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [60]: Training Loss: 2.628965547, Training Accuracy: 31.360\n",
            "Time taken for worker 1 : 0:00:05.875579\n",
            "iteration: 59/150\n",
            "Local Step 60: Test Loss: 2.856363661, Test Accuracy: 28.500\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [61]: Training Loss: 2.728909294, Training Accuracy: 30.352\n",
            "Time taken for worker 3 : 0:00:07.029591\n",
            "iteration: 60/150\n",
            "Local Step 61: Test Loss: 2.904077185, Test Accuracy: 28.700\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [62]: Training Loss: 2.596737098, Training Accuracy: 32.608\n",
            "Time taken for worker 2 : 0:00:06.253367\n",
            "iteration: 61/150\n",
            "Local Step 62: Test Loss: 2.824006913, Test Accuracy: 29.590\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [63]: Training Loss: 2.646983897, Training Accuracy: 31.472\n",
            "Time taken for worker 4 : 0:00:05.887823\n",
            "iteration: 62/150\n",
            "Local Step 63: Test Loss: 2.861368319, Test Accuracy: 28.820\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [64]: Training Loss: 2.586263598, Training Accuracy: 32.344\n",
            "Time taken for worker 1 : 0:00:06.975357\n",
            "iteration: 63/150\n",
            "Local Step 64: Test Loss: 2.812006653, Test Accuracy: 29.610\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [65]: Training Loss: 2.665203697, Training Accuracy: 31.032\n",
            "Time taken for worker 3 : 0:00:06.108691\n",
            "iteration: 64/150\n",
            "Local Step 65: Test Loss: 2.803602184, Test Accuracy: 29.820\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [66]: Training Loss: 2.530545319, Training Accuracy: 33.592\n",
            "Time taken for worker 2 : 0:00:06.523953\n",
            "iteration: 65/150\n",
            "Local Step 66: Test Loss: 2.787874614, Test Accuracy: 30.550\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [67]: Training Loss: 2.518117411, Training Accuracy: 33.432\n",
            "Time taken for worker 1 : 0:00:06.065879\n",
            "iteration: 66/150\n",
            "Local Step 67: Test Loss: 2.796484431, Test Accuracy: 30.470\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [68]: Training Loss: 2.571669080, Training Accuracy: 33.152\n",
            "Time taken for worker 4 : 0:00:06.063864\n",
            "iteration: 67/150\n",
            "Local Step 68: Test Loss: 2.836088765, Test Accuracy: 29.860\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [69]: Training Loss: 2.600978406, Training Accuracy: 32.288\n",
            "Time taken for worker 3 : 0:00:06.419285\n",
            "iteration: 68/150\n",
            "Local Step 69: Test Loss: 2.822701738, Test Accuracy: 30.110\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [70]: Training Loss: 2.484949930, Training Accuracy: 34.736\n",
            "Time taken for worker 2 : 0:00:07.418493\n",
            "iteration: 69/150\n",
            "Local Step 70: Test Loss: 2.847586925, Test Accuracy: 29.960\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [71]: Training Loss: 2.471181340, Training Accuracy: 34.944\n",
            "Time taken for worker 1 : 0:00:05.960584\n",
            "iteration: 70/150\n",
            "Local Step 71: Test Loss: 2.802899092, Test Accuracy: 30.370\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [72]: Training Loss: 2.502558933, Training Accuracy: 34.520\n",
            "Time taken for worker 4 : 0:00:06.107854\n",
            "iteration: 71/150\n",
            "Local Step 72: Test Loss: 2.745162279, Test Accuracy: 31.650\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [73]: Training Loss: 2.557245289, Training Accuracy: 33.288\n",
            "Time taken for worker 3 : 0:00:05.732645\n",
            "iteration: 72/150\n",
            "Local Step 73: Test Loss: 2.897948628, Test Accuracy: 28.630\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [74]: Training Loss: 2.422563698, Training Accuracy: 35.936\n",
            "Time taken for worker 1 : 0:00:06.084553\n",
            "iteration: 73/150\n",
            "Local Step 74: Test Loss: 2.732960621, Test Accuracy: 31.620\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [75]: Training Loss: 2.426886714, Training Accuracy: 35.752\n",
            "Time taken for worker 2 : 0:00:06.051380\n",
            "iteration: 74/150\n",
            "Local Step 75: Test Loss: 2.772396785, Test Accuracy: 30.460\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [76]: Training Loss: 2.461408167, Training Accuracy: 35.368\n",
            "Time taken for worker 4 : 0:00:06.317347\n",
            "iteration: 75/150\n",
            "Local Step 76: Test Loss: 2.797755838, Test Accuracy: 30.580\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [77]: Training Loss: 2.504981871, Training Accuracy: 34.272\n",
            "Time taken for worker 3 : 0:00:06.582864\n",
            "iteration: 76/150\n",
            "Local Step 77: Test Loss: 2.788877978, Test Accuracy: 30.400\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [78]: Training Loss: 2.350317593, Training Accuracy: 37.640\n",
            "Time taken for worker 1 : 0:00:06.044059\n",
            "iteration: 77/150\n",
            "Local Step 78: Test Loss: 2.793069601, Test Accuracy: 31.240\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [79]: Training Loss: 2.384868402, Training Accuracy: 36.984\n",
            "Time taken for worker 2 : 0:00:07.035001\n",
            "iteration: 78/150\n",
            "Local Step 79: Test Loss: 2.724209369, Test Accuracy: 31.920\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [80]: Training Loss: 2.399145139, Training Accuracy: 36.784\n",
            "Time taken for worker 4 : 0:00:06.065380\n",
            "iteration: 79/150\n",
            "Local Step 80: Test Loss: 2.731595033, Test Accuracy: 31.830\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [81]: Training Loss: 2.335852179, Training Accuracy: 37.936\n",
            "Time taken for worker 1 : 0:00:05.959349\n",
            "iteration: 80/150\n",
            "Local Step 81: Test Loss: 2.732016273, Test Accuracy: 31.720\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [82]: Training Loss: 2.454558300, Training Accuracy: 35.352\n",
            "Time taken for worker 3 : 0:00:06.828731\n",
            "iteration: 81/150\n",
            "Local Step 82: Test Loss: 2.751671235, Test Accuracy: 31.510\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [83]: Training Loss: 2.329054302, Training Accuracy: 38.144\n",
            "Time taken for worker 2 : 0:00:06.039284\n",
            "iteration: 82/150\n",
            "Local Step 83: Test Loss: 2.772128052, Test Accuracy: 32.120\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [84]: Training Loss: 2.348356780, Training Accuracy: 37.888\n",
            "Time taken for worker 4 : 0:00:06.043015\n",
            "iteration: 83/150\n",
            "Local Step 84: Test Loss: 2.700869348, Test Accuracy: 33.190\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [85]: Training Loss: 2.264901156, Training Accuracy: 39.464\n",
            "Time taken for worker 1 : 0:00:05.913729\n",
            "iteration: 84/150\n",
            "Local Step 85: Test Loss: 2.849536304, Test Accuracy: 31.480\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [86]: Training Loss: 2.385208124, Training Accuracy: 37.648\n",
            "Time taken for worker 3 : 0:00:06.695761\n",
            "iteration: 85/150\n",
            "Local Step 86: Test Loss: 2.792098726, Test Accuracy: 31.180\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [87]: Training Loss: 2.289129347, Training Accuracy: 38.496\n",
            "Time taken for worker 2 : 0:00:05.828980\n",
            "iteration: 86/150\n",
            "Local Step 87: Test Loss: 2.752500463, Test Accuracy: 31.680\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [88]: Training Loss: 2.289236415, Training Accuracy: 39.240\n",
            "Time taken for worker 4 : 0:00:06.586369\n",
            "iteration: 87/150\n",
            "Local Step 88: Test Loss: 2.720935383, Test Accuracy: 32.570\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [89]: Training Loss: 2.204106083, Training Accuracy: 40.904\n",
            "Time taken for worker 1 : 0:00:06.101150\n",
            "iteration: 88/150\n",
            "Local Step 89: Test Loss: 2.769605160, Test Accuracy: 31.800\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [90]: Training Loss: 2.235581045, Training Accuracy: 40.352\n",
            "Time taken for worker 2 : 0:00:06.205390\n",
            "iteration: 89/150\n",
            "Local Step 90: Test Loss: 2.745129648, Test Accuracy: 33.210\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [91]: Training Loss: 2.342732801, Training Accuracy: 37.680\n",
            "Time taken for worker 3 : 0:00:06.211344\n",
            "iteration: 90/150\n",
            "Local Step 91: Test Loss: 2.787944283, Test Accuracy: 31.680\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [92]: Training Loss: 2.271882151, Training Accuracy: 39.616\n",
            "Time taken for worker 4 : 0:00:05.723780\n",
            "iteration: 91/150\n",
            "Local Step 92: Test Loss: 2.749797639, Test Accuracy: 32.610\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [93]: Training Loss: 2.199303990, Training Accuracy: 40.824\n",
            "Time taken for worker 1 : 0:00:06.014482\n",
            "iteration: 92/150\n",
            "Local Step 93: Test Loss: 2.774892411, Test Accuracy: 32.890\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [94]: Training Loss: 2.183659315, Training Accuracy: 40.552\n",
            "Time taken for worker 2 : 0:00:06.348609\n",
            "iteration: 93/150\n",
            "Local Step 94: Test Loss: 2.755512175, Test Accuracy: 32.910\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [95]: Training Loss: 2.299688741, Training Accuracy: 38.496\n",
            "Time taken for worker 3 : 0:00:06.914771\n",
            "iteration: 94/150\n",
            "Local Step 95: Test Loss: 2.726451600, Test Accuracy: 33.370\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [96]: Training Loss: 2.227316584, Training Accuracy: 40.184\n",
            "Time taken for worker 4 : 0:00:05.853663\n",
            "iteration: 95/150\n",
            "Local Step 96: Test Loss: 2.867024454, Test Accuracy: 31.210\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [97]: Training Loss: 2.135575794, Training Accuracy: 42.408\n",
            "Time taken for worker 1 : 0:00:05.936677\n",
            "iteration: 96/150\n",
            "Local Step 97: Test Loss: 2.718874764, Test Accuracy: 32.900\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [98]: Training Loss: 2.152641588, Training Accuracy: 41.840\n",
            "Time taken for worker 2 : 0:00:05.928053\n",
            "iteration: 97/150\n",
            "Local Step 98: Test Loss: 2.690508282, Test Accuracy: 33.530\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [99]: Training Loss: 2.274866879, Training Accuracy: 39.088\n",
            "Time taken for worker 3 : 0:00:07.530239\n",
            "iteration: 98/150\n",
            "Local Step 99: Test Loss: 2.789409466, Test Accuracy: 32.320\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [100]: Training Loss: 2.173307392, Training Accuracy: 41.128\n",
            "Time taken for worker 4 : 0:00:05.819016\n",
            "iteration: 99/150\n",
            "Local Step 100: Test Loss: 2.777388814, Test Accuracy: 32.870\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [101]: Training Loss: 2.087229616, Training Accuracy: 43.808\n",
            "Time taken for worker 1 : 0:00:05.793046\n",
            "iteration: 100/150\n",
            "Local Step 101: Test Loss: 2.782568119, Test Accuracy: 33.200\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [102]: Training Loss: 2.127218625, Training Accuracy: 42.000\n",
            "Time taken for worker 2 : 0:00:07.018745\n",
            "iteration: 101/150\n",
            "Local Step 102: Test Loss: 2.795173153, Test Accuracy: 32.920\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [103]: Training Loss: 2.202406634, Training Accuracy: 40.480\n",
            "Time taken for worker 3 : 0:00:06.068311\n",
            "iteration: 102/150\n",
            "Local Step 103: Test Loss: 2.799048197, Test Accuracy: 32.120\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [104]: Training Loss: 2.146978575, Training Accuracy: 42.248\n",
            "Time taken for worker 4 : 0:00:06.276117\n",
            "iteration: 103/150\n",
            "Local Step 104: Test Loss: 2.712926468, Test Accuracy: 33.690\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [105]: Training Loss: 2.065560306, Training Accuracy: 43.568\n",
            "Time taken for worker 1 : 0:00:05.899702\n",
            "iteration: 104/150\n",
            "Local Step 105: Test Loss: 2.758080983, Test Accuracy: 32.330\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [106]: Training Loss: 2.077341483, Training Accuracy: 43.328\n",
            "Time taken for worker 2 : 0:00:06.996809\n",
            "iteration: 105/150\n",
            "Local Step 106: Test Loss: 2.832043786, Test Accuracy: 32.860\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [107]: Training Loss: 2.164062814, Training Accuracy: 40.608\n",
            "Time taken for worker 3 : 0:00:07.078988\n",
            "iteration: 106/150\n",
            "Local Step 107: Test Loss: 2.740283790, Test Accuracy: 32.580\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [108]: Training Loss: 2.080148342, Training Accuracy: 43.584\n",
            "Time taken for worker 4 : 0:00:06.048163\n",
            "iteration: 107/150\n",
            "Local Step 108: Test Loss: 2.778186947, Test Accuracy: 32.590\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [109]: Training Loss: 2.023617679, Training Accuracy: 44.664\n",
            "Time taken for worker 1 : 0:00:06.064888\n",
            "iteration: 108/150\n",
            "Local Step 109: Test Loss: 2.747620057, Test Accuracy: 33.660\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [110]: Training Loss: 2.048081984, Training Accuracy: 44.512\n",
            "Time taken for worker 2 : 0:00:05.694657\n",
            "iteration: 109/150\n",
            "Local Step 110: Test Loss: 2.774250178, Test Accuracy: 33.750\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [111]: Training Loss: 2.057410301, Training Accuracy: 43.640\n",
            "Time taken for worker 4 : 0:00:05.818078\n",
            "iteration: 110/150\n",
            "Local Step 111: Test Loss: 2.892550371, Test Accuracy: 31.360\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [112]: Training Loss: 2.139706921, Training Accuracy: 41.840\n",
            "Time taken for worker 3 : 0:00:06.135532\n",
            "iteration: 111/150\n",
            "Local Step 112: Test Loss: 2.784633090, Test Accuracy: 32.680\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [113]: Training Loss: 1.991333358, Training Accuracy: 44.600\n",
            "Time taken for worker 1 : 0:00:05.875745\n",
            "iteration: 112/150\n",
            "Local Step 113: Test Loss: 2.796819415, Test Accuracy: 33.280\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [114]: Training Loss: 2.006961513, Training Accuracy: 45.488\n",
            "Time taken for worker 2 : 0:00:06.677997\n",
            "iteration: 113/150\n",
            "Local Step 114: Test Loss: 2.775459279, Test Accuracy: 33.820\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [115]: Training Loss: 2.000599735, Training Accuracy: 45.480\n",
            "Time taken for worker 4 : 0:00:06.198694\n",
            "iteration: 114/150\n",
            "Local Step 115: Test Loss: 2.883179605, Test Accuracy: 31.730\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [116]: Training Loss: 2.088613682, Training Accuracy: 42.936\n",
            "Time taken for worker 3 : 0:00:06.396493\n",
            "iteration: 115/150\n",
            "Local Step 116: Test Loss: 2.882211313, Test Accuracy: 32.030\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [117]: Training Loss: 1.970826008, Training Accuracy: 46.088\n",
            "Time taken for worker 1 : 0:00:06.549794\n",
            "iteration: 116/150\n",
            "Local Step 117: Test Loss: 2.877857374, Test Accuracy: 32.470\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [118]: Training Loss: 1.962433321, Training Accuracy: 46.368\n",
            "Time taken for worker 2 : 0:00:06.116174\n",
            "iteration: 117/150\n",
            "Local Step 118: Test Loss: 2.701744468, Test Accuracy: 35.380\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [119]: Training Loss: 1.963584031, Training Accuracy: 46.360\n",
            "Time taken for worker 4 : 0:00:05.878706\n",
            "iteration: 118/150\n",
            "Local Step 119: Test Loss: 2.761476710, Test Accuracy: 33.830\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [120]: Training Loss: 2.069777750, Training Accuracy: 43.360\n",
            "Time taken for worker 3 : 0:00:05.866022\n",
            "iteration: 119/150\n",
            "Local Step 120: Test Loss: 2.744993166, Test Accuracy: 33.600\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [121]: Training Loss: 1.944361773, Training Accuracy: 46.696\n",
            "Time taken for worker 1 : 0:00:05.775252\n",
            "iteration: 120/150\n",
            "Local Step 121: Test Loss: 2.847108631, Test Accuracy: 33.380\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [122]: Training Loss: 1.939604250, Training Accuracy: 46.808\n",
            "Time taken for worker 2 : 0:00:05.772387\n",
            "iteration: 121/150\n",
            "Local Step 122: Test Loss: 2.737723146, Test Accuracy: 34.510\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [123]: Training Loss: 1.948455554, Training Accuracy: 46.632\n",
            "Time taken for worker 4 : 0:00:06.957839\n",
            "iteration: 122/150\n",
            "Local Step 123: Test Loss: 2.873708386, Test Accuracy: 32.830\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [124]: Training Loss: 1.902524805, Training Accuracy: 47.968\n",
            "Time taken for worker 1 : 0:00:06.170159\n",
            "iteration: 123/150\n",
            "Local Step 124: Test Loss: 2.891476349, Test Accuracy: 33.040\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [125]: Training Loss: 2.019189027, Training Accuracy: 44.336\n",
            "Time taken for worker 3 : 0:00:06.481735\n",
            "iteration: 124/150\n",
            "Local Step 125: Test Loss: 2.722180045, Test Accuracy: 33.200\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [126]: Training Loss: 1.912627027, Training Accuracy: 47.080\n",
            "Time taken for worker 2 : 0:00:06.678816\n",
            "iteration: 125/150\n",
            "Local Step 126: Test Loss: 2.808810138, Test Accuracy: 34.250\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [127]: Training Loss: 1.911118928, Training Accuracy: 47.440\n",
            "Time taken for worker 4 : 0:00:06.111619\n",
            "iteration: 126/150\n",
            "Local Step 127: Test Loss: 2.873520698, Test Accuracy: 33.120\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [128]: Training Loss: 1.873513828, Training Accuracy: 47.984\n",
            "Time taken for worker 1 : 0:00:06.610482\n",
            "iteration: 127/150\n",
            "Local Step 128: Test Loss: 2.797596657, Test Accuracy: 34.460\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [129]: Training Loss: 1.978164663, Training Accuracy: 45.912\n",
            "Time taken for worker 3 : 0:00:06.918705\n",
            "iteration: 128/150\n",
            "Local Step 129: Test Loss: 2.783127829, Test Accuracy: 32.640\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [130]: Training Loss: 1.883099821, Training Accuracy: 47.592\n",
            "Time taken for worker 2 : 0:00:06.170429\n",
            "iteration: 129/150\n",
            "Local Step 130: Test Loss: 2.773221636, Test Accuracy: 33.480\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [131]: Training Loss: 1.828315891, Training Accuracy: 49.128\n",
            "Time taken for worker 1 : 0:00:06.356171\n",
            "iteration: 130/150\n",
            "Local Step 131: Test Loss: 2.857567898, Test Accuracy: 33.120\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [132]: Training Loss: 1.882517287, Training Accuracy: 47.696\n",
            "Time taken for worker 4 : 0:00:06.721054\n",
            "iteration: 131/150\n",
            "Local Step 132: Test Loss: 2.835849244, Test Accuracy: 34.370\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [133]: Training Loss: 1.948658326, Training Accuracy: 46.736\n",
            "Time taken for worker 3 : 0:00:05.795628\n",
            "iteration: 132/150\n",
            "Local Step 133: Test Loss: 2.769002676, Test Accuracy: 33.660\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [134]: Training Loss: 1.858973013, Training Accuracy: 47.872\n",
            "Time taken for worker 2 : 0:00:05.819052\n",
            "iteration: 133/150\n",
            "Local Step 134: Test Loss: 2.816542586, Test Accuracy: 34.710\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [135]: Training Loss: 1.799077041, Training Accuracy: 49.248\n",
            "Time taken for worker 1 : 0:00:06.583265\n",
            "iteration: 134/150\n",
            "Local Step 135: Test Loss: 2.888633563, Test Accuracy: 32.900\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [136]: Training Loss: 1.842442402, Training Accuracy: 48.760\n",
            "Time taken for worker 4 : 0:00:05.959638\n",
            "iteration: 135/150\n",
            "Local Step 136: Test Loss: 2.845586002, Test Accuracy: 34.100\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [137]: Training Loss: 1.950660408, Training Accuracy: 45.424\n",
            "Time taken for worker 3 : 0:00:05.715246\n",
            "iteration: 136/150\n",
            "Local Step 137: Test Loss: 2.874559368, Test Accuracy: 33.350\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [138]: Training Loss: 1.849809602, Training Accuracy: 48.504\n",
            "Time taken for worker 2 : 0:00:05.787005\n",
            "iteration: 137/150\n",
            "Local Step 138: Test Loss: 2.875422341, Test Accuracy: 33.670\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [139]: Training Loss: 1.787544924, Training Accuracy: 49.960\n",
            "Time taken for worker 1 : 0:00:06.330428\n",
            "iteration: 138/150\n",
            "Local Step 139: Test Loss: 2.841211377, Test Accuracy: 34.250\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [140]: Training Loss: 1.835376618, Training Accuracy: 48.848\n",
            "Time taken for worker 4 : 0:00:07.465233\n",
            "iteration: 139/150\n",
            "Local Step 140: Test Loss: 2.852314604, Test Accuracy: 34.100\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [141]: Training Loss: 1.878029802, Training Accuracy: 47.520\n",
            "Time taken for worker 3 : 0:00:07.422299\n",
            "iteration: 140/150\n",
            "Local Step 141: Test Loss: 2.827996481, Test Accuracy: 35.040\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [142]: Training Loss: 1.820392563, Training Accuracy: 48.984\n",
            "Time taken for worker 2 : 0:00:06.099806\n",
            "iteration: 141/150\n",
            "Local Step 142: Test Loss: 2.969493175, Test Accuracy: 34.220\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [143]: Training Loss: 1.746619443, Training Accuracy: 51.240\n",
            "Time taken for worker 1 : 0:00:06.184747\n",
            "iteration: 142/150\n",
            "Local Step 143: Test Loss: 2.826504736, Test Accuracy: 34.420\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [144]: Training Loss: 1.819775744, Training Accuracy: 49.616\n",
            "Time taken for worker 4 : 0:00:05.821735\n",
            "iteration: 143/150\n",
            "Local Step 144: Test Loss: 2.850024964, Test Accuracy: 33.170\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [145]: Training Loss: 1.860670615, Training Accuracy: 48.384\n",
            "Time taken for worker 3 : 0:00:06.029605\n",
            "iteration: 144/150\n",
            "Local Step 145: Test Loss: 2.849808790, Test Accuracy: 34.210\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [146]: Training Loss: 1.776178344, Training Accuracy: 50.600\n",
            "Time taken for worker 2 : 0:00:06.197129\n",
            "iteration: 145/150\n",
            "Local Step 146: Test Loss: 2.866801347, Test Accuracy: 33.480\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [147]: Training Loss: 1.720964095, Training Accuracy: 51.360\n",
            "Time taken for worker 1 : 0:00:05.983328\n",
            "iteration: 146/150\n",
            "Local Step 147: Test Loss: 2.903466723, Test Accuracy: 33.380\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [148]: Training Loss: 1.765438329, Training Accuracy: 51.288\n",
            "Time taken for worker 4 : 0:00:06.396300\n",
            "iteration: 147/150\n",
            "Local Step 148: Test Loss: 2.875121947, Test Accuracy: 34.800\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [149]: Training Loss: 1.839106778, Training Accuracy: 48.680\n",
            "Time taken for worker 3 : 0:00:05.900385\n",
            "iteration: 148/150\n",
            "Local Step 149: Test Loss: 2.820816408, Test Accuracy: 33.840\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [150]: Training Loss: 1.715256189, Training Accuracy: 51.792\n",
            "Time taken for worker 1 : 0:00:06.536512\n",
            "iteration: 149/150\n",
            "Local Step 150: Test Loss: 2.950167214, Test Accuracy: 33.350\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for SHAT: 0:18:26.338075\n",
            "//////////////////////////////////////////////////\n",
            "==================================================\n",
            "Number of Workers:8\n",
            "==================================================\n",
            "Original Computation Latency: [9387, 7739, 8575, 6085, 7545, 6773, 8411, 6383]\n",
            "Scaled Computation Latency: [1.542645850451931, 1.2718159408381264, 1.409202958093673, 1.0, 1.2399342645850453, 1.113064913722268, 1.3822514379622022, 1.048972884141331]\n",
            "workers simulated orders based on computation latency:[4, 8, 6, 5, 2, 7, 3, 1, 4, 8, 6, 5, 2, 7, 3, 4, 1, 8, 6, 5, 2, 4, 7, 8, 3, 6, 1, 5, 4, 2, 8, 7, 6, 3, 4, 1, 5, 8, 2, 6, 7, 4, 3, 8, 5, 2, 1, 6, 4, 7, 8, 3, 5, 2, 6, 4, 1, 8, 7, 3, 5, 4, 6, 2, 8, 1, 4, 7, 6, 5, 3, 2, 8, 4, 6, 1, 5, 7, 8, 3, 2, 4, 6, 8, 5, 7, 1, 2, 4, 3, 6, 8, 5, 4, 7, 2, 1, 3, 6, 8, 4, 5, 2, 7, 6, 8, 3, 1, 4, 5, 2, 6, 8, 7, 4, 3, 1, 5, 8, 6, 4, 2, 7, 3, 5, 8, 4, 6, 1, 2, 7, 8, 4, 5, 3, 6, 1, 2, 4, 8, 7, 6, 5, 3, 2, 4, 8, 1, 6, 7]\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [01]: Training Loss: 4.590701147, Training Accuracy: 2.064\n",
            "Time taken for worker 4 : 0:00:03.295063\n",
            "iteration: 00/150\n",
            "Local Step 01: Test Loss: 4.532881457, Test Accuracy: 3.510\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [02]: Training Loss: 4.592656350, Training Accuracy: 1.744\n",
            "Time taken for worker 8 : 0:00:03.060701\n",
            "iteration: 01/150\n",
            "Local Step 02: Test Loss: 4.539187699, Test Accuracy: 3.330\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [03]: Training Loss: 4.588708951, Training Accuracy: 1.920\n",
            "Time taken for worker 6 : 0:00:03.001446\n",
            "iteration: 02/150\n",
            "Local Step 03: Test Loss: 4.528668525, Test Accuracy: 2.490\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [04]: Training Loss: 4.592276889, Training Accuracy: 1.904\n",
            "Time taken for worker 5 : 0:00:02.986515\n",
            "iteration: 03/150\n",
            "Local Step 04: Test Loss: 4.542300595, Test Accuracy: 2.460\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [05]: Training Loss: 4.587961216, Training Accuracy: 1.648\n",
            "Time taken for worker 2 : 0:00:03.521647\n",
            "iteration: 04/150\n",
            "Local Step 05: Test Loss: 4.517399748, Test Accuracy: 2.970\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [06]: Training Loss: 4.588631815, Training Accuracy: 1.824\n",
            "Time taken for worker 7 : 0:00:02.922709\n",
            "iteration: 05/150\n",
            "Local Step 06: Test Loss: 4.521634433, Test Accuracy: 3.410\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [07]: Training Loss: 4.589294521, Training Accuracy: 2.000\n",
            "Time taken for worker 3 : 0:00:03.107678\n",
            "iteration: 06/150\n",
            "Local Step 07: Test Loss: 4.539477521, Test Accuracy: 3.200\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [08]: Training Loss: 4.589986446, Training Accuracy: 1.696\n",
            "Time taken for worker 1 : 0:00:02.967436\n",
            "iteration: 07/150\n",
            "Local Step 08: Test Loss: 4.540592658, Test Accuracy: 2.140\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [09]: Training Loss: 4.359085531, Training Accuracy: 3.792\n",
            "Time taken for worker 4 : 0:00:03.377367\n",
            "iteration: 08/150\n",
            "Local Step 09: Test Loss: 4.237072260, Test Accuracy: 5.260\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [10]: Training Loss: 4.385689954, Training Accuracy: 3.840\n",
            "Time taken for worker 8 : 0:00:03.255042\n",
            "iteration: 09/150\n",
            "Local Step 10: Test Loss: 4.234369330, Test Accuracy: 4.330\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [11]: Training Loss: 4.350982452, Training Accuracy: 3.648\n",
            "Time taken for worker 6 : 0:00:03.548291\n",
            "iteration: 10/150\n",
            "Local Step 11: Test Loss: 4.250080646, Test Accuracy: 4.690\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [12]: Training Loss: 4.377548466, Training Accuracy: 3.776\n",
            "Time taken for worker 5 : 0:00:03.316238\n",
            "iteration: 11/150\n",
            "Local Step 12: Test Loss: 4.242378977, Test Accuracy: 5.030\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [13]: Training Loss: 4.363103541, Training Accuracy: 3.968\n",
            "Time taken for worker 2 : 0:00:03.541290\n",
            "iteration: 12/150\n",
            "Local Step 13: Test Loss: 4.245455839, Test Accuracy: 5.270\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [14]: Training Loss: 4.350926681, Training Accuracy: 3.904\n",
            "Time taken for worker 7 : 0:00:03.660615\n",
            "iteration: 13/150\n",
            "Local Step 14: Test Loss: 4.232354037, Test Accuracy: 5.730\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [15]: Training Loss: 4.357872014, Training Accuracy: 3.936\n",
            "Time taken for worker 3 : 0:00:03.211154\n",
            "iteration: 14/150\n",
            "Local Step 15: Test Loss: 4.225089283, Test Accuracy: 4.500\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [16]: Training Loss: 4.157591864, Training Accuracy: 5.680\n",
            "Time taken for worker 4 : 0:00:02.894624\n",
            "iteration: 15/150\n",
            "Local Step 16: Test Loss: 4.104188617, Test Accuracy: 6.920\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [17]: Training Loss: 4.363279508, Training Accuracy: 3.728\n",
            "Time taken for worker 1 : 0:00:03.067328\n",
            "iteration: 16/150\n",
            "Local Step 17: Test Loss: 4.228287020, Test Accuracy: 4.910\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [18]: Training Loss: 4.189702577, Training Accuracy: 5.408\n",
            "Time taken for worker 8 : 0:00:03.395978\n",
            "iteration: 17/150\n",
            "Local Step 18: Test Loss: 4.077381711, Test Accuracy: 7.210\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [19]: Training Loss: 4.178954336, Training Accuracy: 5.568\n",
            "Time taken for worker 6 : 0:00:03.073402\n",
            "iteration: 18/150\n",
            "Local Step 19: Test Loss: 4.108624675, Test Accuracy: 6.600\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [20]: Training Loss: 4.201845434, Training Accuracy: 5.280\n",
            "Time taken for worker 5 : 0:00:03.145523\n",
            "iteration: 19/150\n",
            "Local Step 20: Test Loss: 4.101313302, Test Accuracy: 6.740\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [21]: Training Loss: 4.183528521, Training Accuracy: 5.600\n",
            "Time taken for worker 2 : 0:00:03.344786\n",
            "iteration: 20/150\n",
            "Local Step 21: Test Loss: 4.114968915, Test Accuracy: 6.450\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [22]: Training Loss: 4.047547357, Training Accuracy: 7.056\n",
            "Time taken for worker 4 : 0:00:03.441454\n",
            "iteration: 21/150\n",
            "Local Step 22: Test Loss: 4.068862713, Test Accuracy: 6.530\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [23]: Training Loss: 4.168899633, Training Accuracy: 5.744\n",
            "Time taken for worker 7 : 0:00:03.055046\n",
            "iteration: 22/150\n",
            "Local Step 23: Test Loss: 4.098685962, Test Accuracy: 7.030\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [24]: Training Loss: 4.045630754, Training Accuracy: 6.768\n",
            "Time taken for worker 8 : 0:00:03.208919\n",
            "iteration: 23/150\n",
            "Local Step 24: Test Loss: 3.998057405, Test Accuracy: 8.020\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [25]: Training Loss: 4.154382460, Training Accuracy: 5.984\n",
            "Time taken for worker 3 : 0:00:03.536593\n",
            "iteration: 24/150\n",
            "Local Step 25: Test Loss: 4.140411583, Test Accuracy: 6.630\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [26]: Training Loss: 4.042933559, Training Accuracy: 7.296\n",
            "Time taken for worker 6 : 0:00:03.156625\n",
            "iteration: 25/150\n",
            "Local Step 26: Test Loss: 3.962903881, Test Accuracy: 8.910\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [27]: Training Loss: 4.154697888, Training Accuracy: 6.032\n",
            "Time taken for worker 1 : 0:00:03.364893\n",
            "iteration: 26/150\n",
            "Local Step 27: Test Loss: 4.083515386, Test Accuracy: 8.130\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [28]: Training Loss: 4.062537274, Training Accuracy: 6.976\n",
            "Time taken for worker 5 : 0:00:02.901257\n",
            "iteration: 27/150\n",
            "Local Step 28: Test Loss: 3.961081637, Test Accuracy: 8.830\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [29]: Training Loss: 3.927270464, Training Accuracy: 8.736\n",
            "Time taken for worker 4 : 0:00:02.974620\n",
            "iteration: 28/150\n",
            "Local Step 29: Test Loss: 3.929228792, Test Accuracy: 9.260\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [30]: Training Loss: 4.072274916, Training Accuracy: 6.896\n",
            "Time taken for worker 2 : 0:00:03.674649\n",
            "iteration: 29/150\n",
            "Local Step 30: Test Loss: 4.007643760, Test Accuracy: 7.980\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [31]: Training Loss: 3.939872705, Training Accuracy: 8.704\n",
            "Time taken for worker 8 : 0:00:03.166149\n",
            "iteration: 30/150\n",
            "Local Step 31: Test Loss: 3.874470276, Test Accuracy: 9.840\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [32]: Training Loss: 4.027321361, Training Accuracy: 6.848\n",
            "Time taken for worker 7 : 0:00:03.019337\n",
            "iteration: 31/150\n",
            "Local Step 32: Test Loss: 3.975066372, Test Accuracy: 8.090\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [33]: Training Loss: 3.948162952, Training Accuracy: 8.832\n",
            "Time taken for worker 6 : 0:00:03.734536\n",
            "iteration: 32/150\n",
            "Local Step 33: Test Loss: 3.942495229, Test Accuracy: 9.330\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [34]: Training Loss: 4.034885479, Training Accuracy: 7.024\n",
            "Time taken for worker 3 : 0:00:03.112999\n",
            "iteration: 33/150\n",
            "Local Step 34: Test Loss: 4.005053359, Test Accuracy: 8.120\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [35]: Training Loss: 3.816305601, Training Accuracy: 10.864\n",
            "Time taken for worker 4 : 0:00:03.484318\n",
            "iteration: 34/150\n",
            "Local Step 35: Test Loss: 3.798750548, Test Accuracy: 11.190\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [36]: Training Loss: 4.029733670, Training Accuracy: 7.328\n",
            "Time taken for worker 1 : 0:00:03.530738\n",
            "iteration: 35/150\n",
            "Local Step 36: Test Loss: 4.009370556, Test Accuracy: 8.530\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [37]: Training Loss: 3.953581954, Training Accuracy: 8.656\n",
            "Time taken for worker 5 : 0:00:03.171673\n",
            "iteration: 36/150\n",
            "Local Step 37: Test Loss: 3.898400604, Test Accuracy: 9.550\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [38]: Training Loss: 3.845191423, Training Accuracy: 10.016\n",
            "Time taken for worker 8 : 0:00:03.581125\n",
            "iteration: 37/150\n",
            "Local Step 38: Test Loss: 3.788138274, Test Accuracy: 12.040\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [39]: Training Loss: 3.960224363, Training Accuracy: 8.752\n",
            "Time taken for worker 2 : 0:00:02.932188\n",
            "iteration: 38/150\n",
            "Local Step 39: Test Loss: 3.900619612, Test Accuracy: 10.120\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [40]: Training Loss: 3.845544324, Training Accuracy: 10.352\n",
            "Time taken for worker 6 : 0:00:03.148900\n",
            "iteration: 39/150\n",
            "Local Step 40: Test Loss: 3.802664731, Test Accuracy: 12.050\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [41]: Training Loss: 3.913741053, Training Accuracy: 9.072\n",
            "Time taken for worker 7 : 0:00:02.988961\n",
            "iteration: 40/150\n",
            "Local Step 41: Test Loss: 3.862058283, Test Accuracy: 10.350\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [42]: Training Loss: 3.742808391, Training Accuracy: 11.808\n",
            "Time taken for worker 4 : 0:00:02.939879\n",
            "iteration: 41/150\n",
            "Local Step 42: Test Loss: 3.760042967, Test Accuracy: 12.510\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [43]: Training Loss: 3.922420117, Training Accuracy: 9.072\n",
            "Time taken for worker 3 : 0:00:03.411831\n",
            "iteration: 42/150\n",
            "Local Step 43: Test Loss: 3.935068551, Test Accuracy: 9.600\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [44]: Training Loss: 3.743580417, Training Accuracy: 11.952\n",
            "Time taken for worker 8 : 0:00:03.207914\n",
            "iteration: 43/150\n",
            "Local Step 44: Test Loss: 3.765312523, Test Accuracy: 12.760\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [45]: Training Loss: 3.849363237, Training Accuracy: 10.096\n",
            "Time taken for worker 5 : 0:00:03.385073\n",
            "iteration: 44/150\n",
            "Local Step 45: Test Loss: 3.801191608, Test Accuracy: 11.540\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [46]: Training Loss: 3.852119404, Training Accuracy: 10.112\n",
            "Time taken for worker 2 : 0:00:02.916244\n",
            "iteration: 45/150\n",
            "Local Step 46: Test Loss: 3.812027128, Test Accuracy: 12.180\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [47]: Training Loss: 3.930090089, Training Accuracy: 9.136\n",
            "Time taken for worker 1 : 0:00:02.908949\n",
            "iteration: 46/150\n",
            "Local Step 47: Test Loss: 3.870429684, Test Accuracy: 10.340\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [48]: Training Loss: 3.752698424, Training Accuracy: 11.776\n",
            "Time taken for worker 6 : 0:00:03.230268\n",
            "iteration: 47/150\n",
            "Local Step 48: Test Loss: 3.744703372, Test Accuracy: 12.910\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [49]: Training Loss: 3.663382336, Training Accuracy: 13.232\n",
            "Time taken for worker 4 : 0:00:02.985622\n",
            "iteration: 48/150\n",
            "Local Step 49: Test Loss: 3.689090595, Test Accuracy: 13.810\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [50]: Training Loss: 3.817146880, Training Accuracy: 10.448\n",
            "Time taken for worker 7 : 0:00:03.045896\n",
            "iteration: 49/150\n",
            "Local Step 50: Test Loss: 3.789613927, Test Accuracy: 12.100\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [51]: Training Loss: 3.666811824, Training Accuracy: 12.064\n",
            "Time taken for worker 8 : 0:00:03.109516\n",
            "iteration: 50/150\n",
            "Local Step 51: Test Loss: 3.697939027, Test Accuracy: 13.260\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [52]: Training Loss: 3.808559972, Training Accuracy: 10.816\n",
            "Time taken for worker 3 : 0:00:03.252100\n",
            "iteration: 51/150\n",
            "Local Step 52: Test Loss: 3.838033037, Test Accuracy: 10.640\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [53]: Training Loss: 3.752315789, Training Accuracy: 12.080\n",
            "Time taken for worker 5 : 0:00:03.368983\n",
            "iteration: 52/150\n",
            "Local Step 53: Test Loss: 3.759669105, Test Accuracy: 11.880\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [54]: Training Loss: 3.745394809, Training Accuracy: 11.696\n",
            "Time taken for worker 2 : 0:00:03.129692\n",
            "iteration: 53/150\n",
            "Local Step 54: Test Loss: 3.699568912, Test Accuracy: 14.170\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [55]: Training Loss: 3.652777373, Training Accuracy: 13.408\n",
            "Time taken for worker 6 : 0:00:03.221946\n",
            "iteration: 54/150\n",
            "Local Step 55: Test Loss: 3.634184628, Test Accuracy: 14.360\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [56]: Training Loss: 3.570056623, Training Accuracy: 14.704\n",
            "Time taken for worker 4 : 0:00:03.158412\n",
            "iteration: 55/150\n",
            "Local Step 56: Test Loss: 3.690947750, Test Accuracy: 13.900\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [57]: Training Loss: 3.820159321, Training Accuracy: 10.672\n",
            "Time taken for worker 1 : 0:00:03.204051\n",
            "iteration: 56/150\n",
            "Local Step 57: Test Loss: 3.795517013, Test Accuracy: 12.010\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [58]: Training Loss: 3.592339917, Training Accuracy: 14.032\n",
            "Time taken for worker 8 : 0:00:03.071055\n",
            "iteration: 57/150\n",
            "Local Step 58: Test Loss: 3.611387892, Test Accuracy: 14.450\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [59]: Training Loss: 3.736727211, Training Accuracy: 11.760\n",
            "Time taken for worker 7 : 0:00:02.946332\n",
            "iteration: 58/150\n",
            "Local Step 59: Test Loss: 3.721540721, Test Accuracy: 12.570\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [60]: Training Loss: 3.716549924, Training Accuracy: 11.744\n",
            "Time taken for worker 3 : 0:00:03.038594\n",
            "iteration: 59/150\n",
            "Local Step 60: Test Loss: 3.735305885, Test Accuracy: 11.940\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [61]: Training Loss: 3.668250850, Training Accuracy: 13.600\n",
            "Time taken for worker 5 : 0:00:03.011168\n",
            "iteration: 60/150\n",
            "Local Step 61: Test Loss: 3.707167789, Test Accuracy: 12.780\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [62]: Training Loss: 3.490727614, Training Accuracy: 15.232\n",
            "Time taken for worker 4 : 0:00:03.232743\n",
            "iteration: 61/150\n",
            "Local Step 62: Test Loss: 3.551849049, Test Accuracy: 15.810\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [63]: Training Loss: 3.583484728, Training Accuracy: 14.512\n",
            "Time taken for worker 6 : 0:00:03.172689\n",
            "iteration: 62/150\n",
            "Local Step 63: Test Loss: 3.672888665, Test Accuracy: 14.150\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [64]: Training Loss: 3.654382616, Training Accuracy: 12.896\n",
            "Time taken for worker 2 : 0:00:03.402081\n",
            "iteration: 63/150\n",
            "Local Step 64: Test Loss: 3.688095029, Test Accuracy: 12.650\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [65]: Training Loss: 3.490045148, Training Accuracy: 15.440\n",
            "Time taken for worker 8 : 0:00:03.000783\n",
            "iteration: 64/150\n",
            "Local Step 65: Test Loss: 3.548692014, Test Accuracy: 15.780\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [66]: Training Loss: 3.736053635, Training Accuracy: 12.176\n",
            "Time taken for worker 1 : 0:00:03.004903\n",
            "iteration: 65/150\n",
            "Local Step 66: Test Loss: 3.714829279, Test Accuracy: 13.450\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [67]: Training Loss: 3.392863434, Training Accuracy: 17.568\n",
            "Time taken for worker 4 : 0:00:03.039158\n",
            "iteration: 66/150\n",
            "Local Step 67: Test Loss: 3.525630436, Test Accuracy: 16.660\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [68]: Training Loss: 3.644444186, Training Accuracy: 13.312\n",
            "Time taken for worker 7 : 0:00:03.087654\n",
            "iteration: 67/150\n",
            "Local Step 68: Test Loss: 3.691446045, Test Accuracy: 13.300\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [69]: Training Loss: 3.520580321, Training Accuracy: 15.040\n",
            "Time taken for worker 6 : 0:00:03.182138\n",
            "iteration: 68/150\n",
            "Local Step 69: Test Loss: 3.574944344, Test Accuracy: 14.670\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [70]: Training Loss: 3.607219971, Training Accuracy: 14.048\n",
            "Time taken for worker 5 : 0:00:03.016813\n",
            "iteration: 69/150\n",
            "Local Step 70: Test Loss: 3.610640397, Test Accuracy: 14.330\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [71]: Training Loss: 3.645501098, Training Accuracy: 13.600\n",
            "Time taken for worker 3 : 0:00:02.924425\n",
            "iteration: 70/150\n",
            "Local Step 71: Test Loss: 3.673541329, Test Accuracy: 13.810\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [72]: Training Loss: 3.581575384, Training Accuracy: 13.760\n",
            "Time taken for worker 2 : 0:00:02.999168\n",
            "iteration: 71/150\n",
            "Local Step 72: Test Loss: 3.626250161, Test Accuracy: 14.510\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [73]: Training Loss: 3.442974667, Training Accuracy: 15.440\n",
            "Time taken for worker 8 : 0:00:03.113358\n",
            "iteration: 72/150\n",
            "Local Step 73: Test Loss: 3.498823467, Test Accuracy: 17.100\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [74]: Training Loss: 3.338558457, Training Accuracy: 17.616\n",
            "Time taken for worker 4 : 0:00:03.134214\n",
            "iteration: 73/150\n",
            "Local Step 74: Test Loss: 3.488418790, Test Accuracy: 17.460\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [75]: Training Loss: 3.443138191, Training Accuracy: 16.752\n",
            "Time taken for worker 6 : 0:00:03.063781\n",
            "iteration: 74/150\n",
            "Local Step 75: Test Loss: 3.552829701, Test Accuracy: 16.360\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [76]: Training Loss: 3.644671336, Training Accuracy: 13.120\n",
            "Time taken for worker 1 : 0:00:03.208059\n",
            "iteration: 75/150\n",
            "Local Step 76: Test Loss: 3.668468583, Test Accuracy: 13.900\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [77]: Training Loss: 3.527935125, Training Accuracy: 14.928\n",
            "Time taken for worker 5 : 0:00:02.974722\n",
            "iteration: 76/150\n",
            "Local Step 77: Test Loss: 3.531438682, Test Accuracy: 15.850\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [78]: Training Loss: 3.558527049, Training Accuracy: 14.800\n",
            "Time taken for worker 7 : 0:00:02.969161\n",
            "iteration: 77/150\n",
            "Local Step 78: Test Loss: 3.606766938, Test Accuracy: 15.180\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [79]: Training Loss: 3.353657812, Training Accuracy: 17.952\n",
            "Time taken for worker 8 : 0:00:03.121047\n",
            "iteration: 78/150\n",
            "Local Step 79: Test Loss: 3.474434192, Test Accuracy: 17.190\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [80]: Training Loss: 3.569666850, Training Accuracy: 14.048\n",
            "Time taken for worker 3 : 0:00:03.673655\n",
            "iteration: 79/150\n",
            "Local Step 80: Test Loss: 3.642060318, Test Accuracy: 13.910\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [81]: Training Loss: 3.513918889, Training Accuracy: 15.408\n",
            "Time taken for worker 2 : 0:00:03.436001\n",
            "iteration: 80/150\n",
            "Local Step 81: Test Loss: 3.573421182, Test Accuracy: 15.740\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [82]: Training Loss: 3.272533412, Training Accuracy: 19.472\n",
            "Time taken for worker 4 : 0:00:03.110952\n",
            "iteration: 81/150\n",
            "Local Step 82: Test Loss: 3.448899099, Test Accuracy: 17.850\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [83]: Training Loss: 3.355381467, Training Accuracy: 18.336\n",
            "Time taken for worker 6 : 0:00:03.522303\n",
            "iteration: 82/150\n",
            "Local Step 83: Test Loss: 3.494958302, Test Accuracy: 17.490\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [84]: Training Loss: 3.285611817, Training Accuracy: 19.328\n",
            "Time taken for worker 8 : 0:00:03.003407\n",
            "iteration: 83/150\n",
            "Local Step 84: Test Loss: 3.406395309, Test Accuracy: 18.780\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [85]: Training Loss: 3.427920011, Training Accuracy: 16.704\n",
            "Time taken for worker 5 : 0:00:03.210943\n",
            "iteration: 84/150\n",
            "Local Step 85: Test Loss: 3.508893005, Test Accuracy: 16.390\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [86]: Training Loss: 3.494442147, Training Accuracy: 15.920\n",
            "Time taken for worker 7 : 0:00:02.919372\n",
            "iteration: 85/150\n",
            "Local Step 86: Test Loss: 3.569172495, Test Accuracy: 15.630\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [87]: Training Loss: 3.538561573, Training Accuracy: 15.360\n",
            "Time taken for worker 1 : 0:00:03.062933\n",
            "iteration: 86/150\n",
            "Local Step 87: Test Loss: 3.622368026, Test Accuracy: 15.120\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [88]: Training Loss: 3.416239670, Training Accuracy: 17.552\n",
            "Time taken for worker 2 : 0:00:03.121273\n",
            "iteration: 87/150\n",
            "Local Step 88: Test Loss: 3.592008708, Test Accuracy: 15.720\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [89]: Training Loss: 3.198400782, Training Accuracy: 20.800\n",
            "Time taken for worker 4 : 0:00:03.388475\n",
            "iteration: 88/150\n",
            "Local Step 89: Test Loss: 3.432712125, Test Accuracy: 17.480\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [90]: Training Loss: 3.475699765, Training Accuracy: 15.984\n",
            "Time taken for worker 3 : 0:00:03.080827\n",
            "iteration: 89/150\n",
            "Local Step 90: Test Loss: 3.574331021, Test Accuracy: 15.580\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [91]: Training Loss: 3.281219733, Training Accuracy: 19.344\n",
            "Time taken for worker 6 : 0:00:03.403842\n",
            "iteration: 90/150\n",
            "Local Step 91: Test Loss: 3.421805736, Test Accuracy: 18.510\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [92]: Training Loss: 3.217434063, Training Accuracy: 20.352\n",
            "Time taken for worker 8 : 0:00:02.982017\n",
            "iteration: 91/150\n",
            "Local Step 92: Test Loss: 3.453512786, Test Accuracy: 17.600\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [93]: Training Loss: 3.370481824, Training Accuracy: 17.456\n",
            "Time taken for worker 5 : 0:00:02.965347\n",
            "iteration: 92/150\n",
            "Local Step 93: Test Loss: 3.474307514, Test Accuracy: 17.340\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [94]: Training Loss: 3.150309636, Training Accuracy: 21.104\n",
            "Time taken for worker 4 : 0:00:03.210167\n",
            "iteration: 93/150\n",
            "Local Step 94: Test Loss: 3.444460521, Test Accuracy: 19.230\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [95]: Training Loss: 3.443871471, Training Accuracy: 16.304\n",
            "Time taken for worker 7 : 0:00:03.007761\n",
            "iteration: 94/150\n",
            "Local Step 95: Test Loss: 3.542650388, Test Accuracy: 16.140\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [96]: Training Loss: 3.351158050, Training Accuracy: 17.664\n",
            "Time taken for worker 2 : 0:00:03.122011\n",
            "iteration: 95/150\n",
            "Local Step 96: Test Loss: 3.478673959, Test Accuracy: 16.750\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [97]: Training Loss: 3.470028775, Training Accuracy: 16.832\n",
            "Time taken for worker 1 : 0:00:03.090358\n",
            "iteration: 96/150\n",
            "Local Step 97: Test Loss: 3.558051869, Test Accuracy: 15.560\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [98]: Training Loss: 3.419043402, Training Accuracy: 17.296\n",
            "Time taken for worker 3 : 0:00:03.526919\n",
            "iteration: 97/150\n",
            "Local Step 98: Test Loss: 3.649345009, Test Accuracy: 14.740\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [99]: Training Loss: 3.198893958, Training Accuracy: 20.992\n",
            "Time taken for worker 6 : 0:00:03.129469\n",
            "iteration: 98/150\n",
            "Local Step 99: Test Loss: 3.471192032, Test Accuracy: 18.650\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [100]: Training Loss: 3.151071497, Training Accuracy: 21.376\n",
            "Time taken for worker 8 : 0:00:02.999656\n",
            "iteration: 99/150\n",
            "Local Step 100: Test Loss: 3.459765285, Test Accuracy: 18.170\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [101]: Training Loss: 3.093923418, Training Accuracy: 22.304\n",
            "Time taken for worker 4 : 0:00:03.445351\n",
            "iteration: 100/150\n",
            "Local Step 101: Test Loss: 3.412065632, Test Accuracy: 19.820\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [102]: Training Loss: 3.305034022, Training Accuracy: 18.624\n",
            "Time taken for worker 5 : 0:00:03.111020\n",
            "iteration: 101/150\n",
            "Local Step 102: Test Loss: 3.442439028, Test Accuracy: 16.640\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [103]: Training Loss: 3.273110845, Training Accuracy: 19.296\n",
            "Time taken for worker 2 : 0:00:03.648633\n",
            "iteration: 102/150\n",
            "Local Step 103: Test Loss: 3.453170936, Test Accuracy: 18.380\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [104]: Training Loss: 3.360184657, Training Accuracy: 17.968\n",
            "Time taken for worker 7 : 0:00:03.206835\n",
            "iteration: 103/150\n",
            "Local Step 104: Test Loss: 3.525515426, Test Accuracy: 16.050\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [105]: Training Loss: 3.154367311, Training Accuracy: 21.664\n",
            "Time taken for worker 6 : 0:00:02.954288\n",
            "iteration: 104/150\n",
            "Local Step 105: Test Loss: 3.391447594, Test Accuracy: 19.950\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [106]: Training Loss: 3.068645022, Training Accuracy: 23.504\n",
            "Time taken for worker 8 : 0:00:02.986625\n",
            "iteration: 105/150\n",
            "Local Step 106: Test Loss: 3.352179836, Test Accuracy: 19.790\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [107]: Training Loss: 3.373179360, Training Accuracy: 18.320\n",
            "Time taken for worker 3 : 0:00:03.135273\n",
            "iteration: 106/150\n",
            "Local Step 107: Test Loss: 3.479080984, Test Accuracy: 17.770\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [108]: Training Loss: 3.411909858, Training Accuracy: 17.280\n",
            "Time taken for worker 1 : 0:00:03.583078\n",
            "iteration: 107/150\n",
            "Local Step 108: Test Loss: 3.549146447, Test Accuracy: 16.350\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [109]: Training Loss: 2.996929198, Training Accuracy: 23.456\n",
            "Time taken for worker 4 : 0:00:02.915781\n",
            "iteration: 108/150\n",
            "Local Step 109: Test Loss: 3.493848508, Test Accuracy: 19.260\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [110]: Training Loss: 3.267768867, Training Accuracy: 19.392\n",
            "Time taken for worker 5 : 0:00:03.134666\n",
            "iteration: 109/150\n",
            "Local Step 110: Test Loss: 3.374725416, Test Accuracy: 18.940\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [111]: Training Loss: 3.213309855, Training Accuracy: 20.192\n",
            "Time taken for worker 2 : 0:00:03.019838\n",
            "iteration: 110/150\n",
            "Local Step 111: Test Loss: 3.359859953, Test Accuracy: 19.600\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [112]: Training Loss: 3.087615580, Training Accuracy: 22.480\n",
            "Time taken for worker 6 : 0:00:03.106568\n",
            "iteration: 111/150\n",
            "Local Step 112: Test Loss: 3.393438614, Test Accuracy: 19.850\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [113]: Training Loss: 3.038339639, Training Accuracy: 23.168\n",
            "Time taken for worker 8 : 0:00:03.012676\n",
            "iteration: 112/150\n",
            "Local Step 113: Test Loss: 3.265869050, Test Accuracy: 21.940\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [114]: Training Loss: 3.297558621, Training Accuracy: 19.424\n",
            "Time taken for worker 7 : 0:00:02.955073\n",
            "iteration: 113/150\n",
            "Local Step 114: Test Loss: 3.444206821, Test Accuracy: 18.190\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [115]: Training Loss: 2.953475400, Training Accuracy: 25.200\n",
            "Time taken for worker 4 : 0:00:03.222602\n",
            "iteration: 114/150\n",
            "Local Step 115: Test Loss: 3.346611369, Test Accuracy: 20.670\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [116]: Training Loss: 3.294117224, Training Accuracy: 19.504\n",
            "Time taken for worker 3 : 0:00:03.413053\n",
            "iteration: 115/150\n",
            "Local Step 116: Test Loss: 3.445373584, Test Accuracy: 17.700\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [117]: Training Loss: 3.322355548, Training Accuracy: 18.704\n",
            "Time taken for worker 1 : 0:00:02.998983\n",
            "iteration: 116/150\n",
            "Local Step 117: Test Loss: 3.447017173, Test Accuracy: 18.070\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [118]: Training Loss: 3.158981348, Training Accuracy: 21.088\n",
            "Time taken for worker 5 : 0:00:03.115854\n",
            "iteration: 117/150\n",
            "Local Step 118: Test Loss: 3.377402088, Test Accuracy: 18.840\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [119]: Training Loss: 2.954753630, Training Accuracy: 25.392\n",
            "Time taken for worker 8 : 0:00:03.102145\n",
            "iteration: 118/150\n",
            "Local Step 119: Test Loss: 3.302903069, Test Accuracy: 21.080\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [120]: Training Loss: 3.026371085, Training Accuracy: 23.152\n",
            "Time taken for worker 6 : 0:00:03.181154\n",
            "iteration: 119/150\n",
            "Local Step 120: Test Loss: 3.356679834, Test Accuracy: 20.460\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [121]: Training Loss: 2.867708934, Training Accuracy: 25.936\n",
            "Time taken for worker 4 : 0:00:03.030303\n",
            "iteration: 120/150\n",
            "Local Step 121: Test Loss: 3.307399189, Test Accuracy: 22.210\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [122]: Training Loss: 3.153819675, Training Accuracy: 21.584\n",
            "Time taken for worker 2 : 0:00:03.437646\n",
            "iteration: 121/150\n",
            "Local Step 122: Test Loss: 3.449222777, Test Accuracy: 18.140\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [123]: Training Loss: 3.234133934, Training Accuracy: 20.096\n",
            "Time taken for worker 7 : 0:00:03.654182\n",
            "iteration: 122/150\n",
            "Local Step 123: Test Loss: 3.423637601, Test Accuracy: 18.700\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [124]: Training Loss: 3.201413717, Training Accuracy: 21.072\n",
            "Time taken for worker 3 : 0:00:03.173015\n",
            "iteration: 123/150\n",
            "Local Step 124: Test Loss: 3.478327293, Test Accuracy: 17.710\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [125]: Training Loss: 3.078746854, Training Accuracy: 22.720\n",
            "Time taken for worker 5 : 0:00:03.099535\n",
            "iteration: 124/150\n",
            "Local Step 125: Test Loss: 3.359164203, Test Accuracy: 20.030\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [126]: Training Loss: 2.869484162, Training Accuracy: 25.600\n",
            "Time taken for worker 8 : 0:00:02.979470\n",
            "iteration: 125/150\n",
            "Local Step 126: Test Loss: 3.248735261, Test Accuracy: 22.320\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [127]: Training Loss: 2.852906940, Training Accuracy: 26.912\n",
            "Time taken for worker 4 : 0:00:02.937090\n",
            "iteration: 126/150\n",
            "Local Step 127: Test Loss: 3.304200690, Test Accuracy: 21.800\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [128]: Training Loss: 2.975266739, Training Accuracy: 24.720\n",
            "Time taken for worker 6 : 0:00:03.336317\n",
            "iteration: 127/150\n",
            "Local Step 128: Test Loss: 3.356026586, Test Accuracy: 21.090\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [129]: Training Loss: 3.267192286, Training Accuracy: 19.472\n",
            "Time taken for worker 1 : 0:00:03.925667\n",
            "iteration: 128/150\n",
            "Local Step 129: Test Loss: 3.443161574, Test Accuracy: 18.280\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [130]: Training Loss: 3.091513500, Training Accuracy: 22.608\n",
            "Time taken for worker 2 : 0:00:03.232968\n",
            "iteration: 129/150\n",
            "Local Step 130: Test Loss: 3.309467704, Test Accuracy: 20.860\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [131]: Training Loss: 3.179273596, Training Accuracy: 20.960\n",
            "Time taken for worker 7 : 0:00:03.013879\n",
            "iteration: 130/150\n",
            "Local Step 131: Test Loss: 3.378332954, Test Accuracy: 19.230\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [132]: Training Loss: 2.833705951, Training Accuracy: 27.360\n",
            "Time taken for worker 8 : 0:00:03.465358\n",
            "iteration: 131/150\n",
            "Local Step 132: Test Loss: 3.240511241, Test Accuracy: 22.120\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [133]: Training Loss: 2.770296019, Training Accuracy: 28.336\n",
            "Time taken for worker 4 : 0:00:03.190229\n",
            "iteration: 132/150\n",
            "Local Step 133: Test Loss: 3.268059372, Test Accuracy: 23.260\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [134]: Training Loss: 3.009447616, Training Accuracy: 23.872\n",
            "Time taken for worker 5 : 0:00:03.118810\n",
            "iteration: 133/150\n",
            "Local Step 134: Test Loss: 3.324126541, Test Accuracy: 20.880\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [135]: Training Loss: 3.146083506, Training Accuracy: 21.504\n",
            "Time taken for worker 3 : 0:00:03.403018\n",
            "iteration: 134/150\n",
            "Local Step 135: Test Loss: 3.348363624, Test Accuracy: 20.030\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [136]: Training Loss: 2.920221178, Training Accuracy: 25.952\n",
            "Time taken for worker 6 : 0:00:02.987595\n",
            "iteration: 135/150\n",
            "Local Step 136: Test Loss: 3.288821635, Test Accuracy: 21.710\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [137]: Training Loss: 3.209389979, Training Accuracy: 20.496\n",
            "Time taken for worker 1 : 0:00:03.209981\n",
            "iteration: 136/150\n",
            "Local Step 137: Test Loss: 3.387631732, Test Accuracy: 19.400\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [138]: Training Loss: 3.028953333, Training Accuracy: 23.360\n",
            "Time taken for worker 2 : 0:00:03.823764\n",
            "iteration: 137/150\n",
            "Local Step 138: Test Loss: 3.368946885, Test Accuracy: 20.190\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [139]: Training Loss: 2.729805567, Training Accuracy: 29.216\n",
            "Time taken for worker 4 : 0:00:03.665394\n",
            "iteration: 138/150\n",
            "Local Step 139: Test Loss: 3.202422189, Test Accuracy: 23.350\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [140]: Training Loss: 2.757342550, Training Accuracy: 28.768\n",
            "Time taken for worker 8 : 0:00:03.783567\n",
            "iteration: 139/150\n",
            "Local Step 140: Test Loss: 3.202624675, Test Accuracy: 23.120\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [141]: Training Loss: 3.112749552, Training Accuracy: 21.648\n",
            "Time taken for worker 7 : 0:00:03.646695\n",
            "iteration: 140/150\n",
            "Local Step 141: Test Loss: 3.449727608, Test Accuracy: 19.150\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [142]: Training Loss: 2.846543599, Training Accuracy: 26.912\n",
            "Time taken for worker 6 : 0:00:02.916791\n",
            "iteration: 141/150\n",
            "Local Step 142: Test Loss: 3.357711605, Test Accuracy: 22.180\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 5\n",
            "Worker 5, [143]: Training Loss: 2.948841246, Training Accuracy: 25.392\n",
            "Time taken for worker 5 : 0:00:03.216267\n",
            "iteration: 142/150\n",
            "Local Step 143: Test Loss: 3.261414915, Test Accuracy: 21.350\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 3\n",
            "Worker 3, [144]: Training Loss: 3.072737842, Training Accuracy: 23.232\n",
            "Time taken for worker 3 : 0:00:03.128405\n",
            "iteration: 143/150\n",
            "Local Step 144: Test Loss: 3.364549901, Test Accuracy: 20.060\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 2\n",
            "Worker 2, [145]: Training Loss: 2.930652550, Training Accuracy: 25.856\n",
            "Time taken for worker 2 : 0:00:03.196161\n",
            "iteration: 144/150\n",
            "Local Step 145: Test Loss: 3.337386945, Test Accuracy: 20.300\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 4\n",
            "Worker 4, [146]: Training Loss: 2.651949386, Training Accuracy: 30.336\n",
            "Time taken for worker 4 : 0:00:03.129769\n",
            "iteration: 145/150\n",
            "Local Step 146: Test Loss: 3.230124190, Test Accuracy: 23.520\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 8\n",
            "Worker 8, [147]: Training Loss: 2.675215415, Training Accuracy: 30.064\n",
            "Time taken for worker 8 : 0:00:03.340882\n",
            "iteration: 146/150\n",
            "Local Step 147: Test Loss: 3.158707102, Test Accuracy: 24.290\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 1\n",
            "Worker 1, [148]: Training Loss: 3.116814056, Training Accuracy: 22.720\n",
            "Time taken for worker 1 : 0:00:03.231034\n",
            "iteration: 147/150\n",
            "Local Step 148: Test Loss: 3.421218164, Test Accuracy: 18.560\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 6\n",
            "Worker 6, [149]: Training Loss: 2.804765991, Training Accuracy: 27.456\n",
            "Time taken for worker 6 : 0:00:02.969172\n",
            "iteration: 148/150\n",
            "Local Step 149: Test Loss: 3.332774140, Test Accuracy: 22.410\n",
            "**************************************************\n",
            "**************************************************\n",
            "worker 7\n",
            "Worker 7, [150]: Training Loss: 3.051630225, Training Accuracy: 23.152\n",
            "Time taken for worker 7 : 0:00:02.949527\n",
            "iteration: 149/150\n",
            "Local Step 150: Test Loss: 3.357461726, Test Accuracy: 20.450\n",
            "**************************************************\n",
            "//////////////////////////////////////////////////\n",
            "Total time taken for SHAT: 0:10:47.778170\n",
            "//////////////////////////////////////////////////\n"
          ]
        }
      ],
      "source": [
        "# TODO: Baraye LocalSGD va SlowMo baraye lr nemikhad hyperparameter tuning anjam bedim?\n",
        "# SHAT\n",
        "lr = 1e-02 # Khodesh gofte hamina ro baraye CIFAR 10 bezarim vali dar morede CIFAR 100 chizi nagorfte\n",
        "wd = 1e-03 \n",
        "K = [2, 4, 8] # TODO: Nemidoonam chand bezaram\n",
        "num_epochs = 150\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  print('='*50)\n",
        "  print(f'Number of Workers:{k}')\n",
        "  print('='*50)\n",
        "  SHAT_PS(shard_loaders, loss_fn, k, lr, wd, initial_state_dict, num_epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LocalAdaScale (Choice of LR in LocalSGD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def synchronize(models):\n",
        "#   for params in zip(*[model.parameters() for model in models]):\n",
        "#     param_avg = torch.mean(torch.stack([param.data for param in params]), dim=0)\n",
        "#     for param in params:\n",
        "#       param.data = param_avg\n",
        "  \n",
        "#   return models[0]\n",
        "\n",
        "# def synchronize(global_model, local_models):\n",
        "#         # Initialize a state dict with zeros, same shape as the model parameters\n",
        "#         delta = {key: torch.zeros_like(value) for key, value in local_models[0].state_dict().items()}\n",
        "      \n",
        "#         # Sum up all the model parameters\n",
        "#         for local_model in local_models:\n",
        "#             for key, value in local_model.state_dict().items():\n",
        "#                 delta[key] += (global_model.state_dict()[key] - value)\n",
        "      \n",
        "#         # Divide each parameter by the number of models to get the average\n",
        "#         for key in delta:\n",
        "#             delta[key] /= len(local_models)\n",
        "      \n",
        "#         new_weights = {}\n",
        "#         for key, value in global_model.state_dict().items():\n",
        "#             new_weights[key] = value -  delta [key] # TODO: az TA beporsim ke learning rate ro chetor hesab konim.\n",
        "      \n",
        "#         global_model.load_state_dict(new_weights)\n",
        "#         return global_model\n",
        "\n",
        "def average_models(local_models):\n",
        "    \"\"\"Calculate the average model from a list of local models.\"\"\"\n",
        "    num_models = len(local_models)\n",
        "    \n",
        "    # Initialize the averaged model as a copy of the first local model\n",
        "    avg_model = local_models[0]\n",
        "    \n",
        "    # Zero the parameters of the avg_model to start accumulating\n",
        "    for param in avg_model.parameters():\n",
        "        param.data.zero_()\n",
        "    \n",
        "    # Accumulate the parameters from all local models\n",
        "    for local_model in local_models:\n",
        "        for avg_param, local_param in zip(avg_model.parameters(), local_model.parameters()):\n",
        "            avg_param.data.add_(local_param.data)\n",
        "    \n",
        "    # Divide by the number of models to compute the average\n",
        "    for param in avg_model.parameters():\n",
        "        param.data.div_(num_models)\n",
        "    \n",
        "    return avg_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def LocalAdaScale(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs):\n",
        "  total_start_time = time.time()\n",
        "\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "  # global_optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "  # Initialize a scheduler for each optimizer\n",
        "  # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for loca_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{loca_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    # global_model = synchronize(global_model, local_models, 1)\n",
        "    global_model = synchronize(local_models)\n",
        "\n",
        "    # for local_model in local_models:\n",
        "    #   local_model.load_state_dict(global_model.state_dict())\n",
        "    # scheduler.step()\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Local Step {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "# TODO: Schedule ro check konim \n",
        "\n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for local_SGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# (data_loader, loss_fn,  num_workers, scale_inv_budget, lr_init, wd,initial_state_dict, num_epochs)\n",
        "# def local_adascale_local_sgd(data_loader, num_epochs, lr_init, num_workers, num_local_steps, scale_inv_budget):\n",
        "#     # Initialize a model and save its initial parameters\n",
        "#     model = LeNet5().to(device='cuda').load_state_dict(initial_state_dict)\n",
        "    \n",
        "#     optimizer = torch.optim.SGD(model.parameters(), lr=lr_init)\n",
        "    \n",
        "#     # Add a learning rate scheduler\n",
        "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "    \n",
        "#     grad_cache = [None] * num_workers\n",
        "#     scale_inv_counter = 0\n",
        "    \n",
        "#     for epoch in range(num_epochs):\n",
        "#         for i, (inputs, targets) in enumerate(data_loader):\n",
        "#             # Simulate local steps on different workers\n",
        "#             for worker_id in range(num_workers):\n",
        "#                 outputs = model(inputs)\n",
        "#                 loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
        "#                 optimizer.zero_grad()\n",
        "#                 loss.backward()\n",
        "                \n",
        "#                 if (i + 1) % num_local_steps == 0:\n",
        "#                     grad_cache[worker_id] = [param.grad.clone() for param in model.parameters()]\n",
        "                    \n",
        "#             # Synchronize and average gradients after num_local_steps\n",
        "#             if (i + 1) % num_local_steps == 0:\n",
        "#                 avg_grad = [torch.mean(torch.stack([grad_cache[worker_id][j] for worker_id in range(num_workers)]), dim=0)\n",
        "#                             for j in range(len(grad_cache[0]))]\n",
        "                \n",
        "#                 # Compute gradient statistics\n",
        "#                 grad_variance = [torch.var(torch.stack([grad_cache[worker_id][j] for worker_id in range(num_workers)]), dim=0)\n",
        "#                                  for j in range(len(grad_cache[0]))]\n",
        "#                 grad_mean = [torch.mean(avg_grad[j]) for j in range(len(avg_grad))]\n",
        "                \n",
        "#                 gain_ratio = compute_gain_ratio(grad_mean, grad_variance, num_workers, num_local_steps)\n",
        "                \n",
        "#                 # Apply scaled gradients\n",
        "#                 for param, grad in zip(model.parameters(), avg_grad):\n",
        "#                     param.grad = gain_ratio * grad\n",
        "                \n",
        "#                 # Scale invariant iteration counter\n",
        "#                 scale_inv_counter += gain_ratio\n",
        "                \n",
        "#                 # Update model parameters\n",
        "#                 optimizer.step()\n",
        "\n",
        "#             # Check if scale invariant budget is exhausted\n",
        "#             if scale_inv_counter >= scale_inv_budget:\n",
        "#                 break\n",
        "        \n",
        "#         # Step the scheduler at the end of each epoch\n",
        "#         scheduler.step()\n",
        "    \n",
        "#     return model\n",
        "\n",
        "# def compute_gain_ratio(grad_mean, grad_variance, num_workers, num_local_steps):\n",
        "#     gain_ratio = []\n",
        "#     for g_mean, g_var in zip(grad_mean, grad_variance):\n",
        "#         g2_mean = g_mean ** 2\n",
        "#         term1 = g2_mean + g_var / num_workers\n",
        "#         term2 = (g2_mean + g_var / num_workers) ** 2\n",
        "#         term3 = 3 * (num_local_steps - 1) * g2_mean * g_var\n",
        "#         gain_ratio.append(2 * term1 / (term1 + torch.sqrt(term2 + term3)))\n",
        "    \n",
        "#     return torch.tensor(gain_ratio).mean().item()\n",
        "\n",
        "# Example usage:\n",
        "# model = MyModel()\n",
        "# data_loader = DataLoader(my_dataset, batch_size=64, shuffle=True)\n",
        "# trained_model = local_adascale_local_sgd(model, data_loader, num_epochs=10, lr_init=0.1, num_workers=4, num_local_steps=5, scale_inv_budget=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "\n",
        "class CustomOptimizer(Optimizer):\n",
        "    def __init__(self, params, lr=0.01, gain_ratio=1.0):\n",
        "        defaults = dict(lr=lr, gain_ratio=gain_ratio)\n",
        "        super(CustomOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        # Apply the custom update rule to each parameter\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            gain_ratio = group['gain_ratio']\n",
        "            \n",
        "            for param in group['params']:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                # Custom update rule\n",
        "                param.data -= gain_ratio * lr * param.grad.data\n",
        "\n",
        "        return None\n",
        "    \n",
        "class CustomSGDLocalAdaScaleOptimizer(Optimizer):\n",
        "    def __init__(self, params, lr=0.01, gain_ratio=1.0, momentum=0.9, weight_decay=0.0):\n",
        "        # Store default settings\n",
        "        defaults = dict(lr=lr, gain_ratio=gain_ratio, momentum=momentum, weight_decay=weight_decay)\n",
        "        super(CustomSGDLocalAdaScaleOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            gain_ratio = group['gain_ratio']\n",
        "            momentum = group['momentum']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for param in group['params']:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                d_p = param.grad.data\n",
        "\n",
        "                # Apply weight decay if specified\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(param.data, alpha=weight_decay)\n",
        "\n",
        "                # If momentum is used\n",
        "                if momentum != 0:\n",
        "                    if 'momentum_buffer' not in self.state[param]:\n",
        "                        buf = self.state[param]['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = self.state[param]['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(d_p)\n",
        "                    d_p = buf\n",
        "\n",
        "                # Custom update rule\n",
        "                try:\n",
        "                    param.data.add_(d_p, alpha=-gain_ratio * lr)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "import math\n",
        "def local_adascale(shard_loaders, loss_fn, K, j, lr, wd, initial_state_dict, scale_invariant_budget):\n",
        "    total_start_time = time.time()\n",
        "    global_model = LeNet5().to(device)\n",
        "    global_model.load_state_dict(initial_state_dict)\n",
        "    local_models = [LeNet5().to(device) for _ in range(K)]\n",
        "    for model in local_models:\n",
        "      model.load_state_dict(initial_state_dict)\n",
        "    # local_optimizers = [CustomOptimizer(model.parameters(), lr=0.01, gain_ratio=1) for model in local_models]\n",
        "    # local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9) for model in local_models]\n",
        "    local_optimizers = [CustomSGDLocalAdaScaleOptimizer(model.parameters(), lr, gain_ratio=1, momentum=0.9, weight_decay= wd) for model in local_models]\n",
        "\n",
        "\n",
        "    num_epochs =150\n",
        "    scale_invariant_budget = 150\n",
        "    # for s in scale_invariant_budget: # Bejaye scale_invariant_budget, num_epochs ro dar nazar gereftim vali goftim age s>scale_invariant_budget shod break kon.\n",
        "    scale_inv_counter = 0\n",
        "    gain_ratio = 1\n",
        "    for epoch in range(num_epochs):\n",
        "        grad_cache = []\n",
        "        for hi, h in enumerate(range(j)):\n",
        "            for k in range(K):\n",
        "                # calculate gradient of all workers g^t_k\n",
        "                train_loss, train_accuracy, gradient =local_update_with_gradient(local_models[k], local_optimizers[k], shard_loaders[k], loss_fn, lr, gain_ratio)\n",
        "                print(f'Worker {k+1}, [{h+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "                if hi == 0: # Only store the gradients for the first local step\n",
        "                    grad_cache.append(gradient)\n",
        "                \n",
        "\n",
        "            # ------------\n",
        "            \n",
        "            # varriance bar va G bar bayad adad bashan va faghat tooye synchronization point (after H steps mohasebe mishan)\n",
        "            # fek konam in mishe g bar_t\n",
        "            # list ba len=10 (ehtemalan tedade layer ha) va dakhele harkodoom 2 ta tensor ke ehtemalen weights and bias\n",
        "            # TA Inja Kar mikone\n",
        "            # TODO: variance bar, G bar, rho, lr tebghe paper dar zamane syncronization update mishan.\n",
        "        \n",
        "        global_model = average_models(local_models)\n",
        "        for local_model in local_models:\n",
        "            local_model.load_state_dict(global_model.state_dict())\n",
        "        # To check if averaging is done correctly (for only two workers)\n",
        "        # with torch.no_grad():\n",
        "        #     for param1, param2, avg_param in zip(local_models[0].parameters(), local_models[1].parameters(), average_model.parameters()):\n",
        "        #         calculated_avg = (param1.data + param2.data) / 2\n",
        "        #         assert torch.allclose(avg_param.data, calculated_avg), \"Averaging failed!\"\n",
        "        \n",
        "        # test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "        # print(f'Local Step {epoch+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "        print(50*'-')\n",
        "        g_bar = calculate_g_bar(grad_cache, K)\n",
        "        variance_bar = calculate_variance_bar(g_bar, grad_cache, K)\n",
        "        G_bar = calculate_G_bar(g_bar=g_bar, num_workers=K, variance_bar=variance_bar)\n",
        "        L = estimate_lipschitz_constant(global_model, shard_loaders[0], loss_fn,device='cuda')\n",
        "        gain_ratio_new = compute_gain_ratio(G_bar,variance_bar, K, j)\n",
        "        lr_new = compute_optimal_learning_rate(G_bar,variance_bar, K, j, L)\n",
        "        if not (lr_new < 0 or math.isnan(lr_new) or gain_ratio_new < 0 or math.isnan(gain_ratio_new)):\n",
        "        # lr = lr_new\n",
        "            gain_ratio = gain_ratio_new\n",
        "        print(f'variance_bar: {variance_bar}, G_bar: {G_bar}, gain_ratio: {gain_ratio}, L: {L}, lr: {lr}')\n",
        "        # Scale invariant iteration counter\n",
        "        scale_inv_counter += gain_ratio\n",
        "        if scale_inv_counter > scale_invariant_budget:\n",
        "            break \n",
        "        print (f'scale_inv_counter: {scale_inv_counter} : scale_invariant_budget: {scale_invariant_budget}')\n",
        "        print (f'gain_ratio: {gain_ratio} lr: {lr}')\n",
        "\n",
        "    \n",
        "    total_end_time = time.time()\n",
        "    print('/'*50)\n",
        "    print(f'Total time taken for LocalAdaScale: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "    print('/'*50)\n",
        "\n",
        "def calculate_g_bar(grad_cache, num_workers):\n",
        "    \"\"\"Calculate g: Element-wise average of all gradients in grad_cache.\"\"\"\n",
        "    gbar = [torch.mean(torch.stack([grad_cache[worker_id][i] for worker_id in range(num_workers)]), dim=0)\n",
        "            for i in range(len(grad_cache[0]))]\n",
        "    \n",
        "    return gbar\n",
        "\n",
        "\n",
        "'''def compute_gain_ratio(grad_mean, grad_variance, num_workers, num_local_steps):\n",
        "    gain_ratio = []\n",
        "    for g_mean, g_var in zip(grad_mean, grad_variance):\n",
        "        g2_mean = g_mean ** 2\n",
        "        term1 = g2_mean + g_var / num_workers\n",
        "        term2 = (g2_mean + g_var / num_workers) ** 2\n",
        "        term3 = 3 * (num_local_steps - 1) * g2_mean * g_var\n",
        "        gain_ratio.append(2 * term1 / (term1 + torch.sqrt(term2 + term3)))\n",
        "    \n",
        "    return torch.tensor(gain_ratio).mean().item()'''\n",
        "\n",
        "def compute_gain_ratio(G_bar, variance_bar, num_workers, num_local_steps):\n",
        "    # Use G and variance to compute the gain ratio\n",
        "    term0 = 2 * (G_bar + variance_bar)\n",
        "    term1 = G_bar + variance_bar / num_workers\n",
        "    term2 = term1 ** 2\n",
        "    term3 = 3 * (num_local_steps - 1) * G_bar * variance_bar\n",
        "    \n",
        "    gain_ratio = term0 / (term1 + torch.sqrt(term2 + term3))\n",
        "    # print(f'variance_bar/G_bar: {variance_bar/G_bar}') # Vaghti variance_bar/G_bar miad taghriban zire 10 hame chi be ham mirize va gain ratio nan mishe.\n",
        "    if torch.isnan(gain_ratio):\n",
        "        pass\n",
        "        pass\n",
        "    # Ke variance_bar/G_bar tabe'e tedade worker e, va shayad tedade iteration\n",
        "\n",
        "        # time.sleep(2)\n",
        "\n",
        "    return gain_ratio.item()\n",
        "\n",
        "def compute_optimal_learning_rate(G_bar, variance_bar, num_workers, num_local_steps, L):\n",
        "    # Use G and variance to compute the adaptive learning rate (t)\n",
        "    term0 = 2 * (G_bar)\n",
        "    term1 = G_bar + variance_bar / num_workers\n",
        "    term2 = term1 ** 2\n",
        "    term3 = 3 * (num_local_steps - 1) * G_bar * variance_bar\n",
        "    \n",
        "    lr = term0 / (term1 + torch.sqrt(term2 + term3))\n",
        "    lr = lr / L\n",
        "    return lr.item()\n",
        "\n",
        "def calculate_G_bar(g_bar, variance_bar, num_workers):\n",
        "    \"\"\"Calculate G as a scalar.\"\"\"\n",
        "    g_bar_norm = sum(torch.norm(g_bar[i])**2 for i in range(len(g_bar)))\n",
        "    G_bar = g_bar_norm - (1 / num_workers) * variance_bar\n",
        "    # print(f'G_bar: {G_bar}')\n",
        "    # print(G_bar.item())\n",
        "    if G_bar.item() < 0:\n",
        "        print('Gbaaaaaaaaaaaaar is negative')\n",
        "        # G_bar = 0.00\n",
        "    #     G_bar = 0.02* variance_bar # Choon bazi vaghta adad mishe kamtar az sefr choon taghrib mizanim. vali in kari ke kardam ham dorost nist.\n",
        "    #     print(f'heeeeeeeey: g_bar_norm: {g_bar_norm}, variance_bar: {variance_bar}, (1 / num_workers) * variance_bar: {(1 / num_workers) * variance_bar}')\n",
        "    #     print('-')\n",
        "\n",
        "    return G_bar\n",
        "\n",
        "def calculate_variance_bar(g_bar, grad_cache, num_workers): # Hamishe mosbate.\n",
        "    \"\"\"Calculate variance as a scalar.\"\"\"\n",
        "    sum_norms = sum(torch.norm(grad_cache[worker_id][i])**2 for worker_id in range(num_workers) for i in range(len(g_bar)))\n",
        "    nomrs_sum = 0\n",
        "    for worker_id in range(num_workers):\n",
        "        for i in range(len(g_bar)):\n",
        "            nomrs_sum(torch.norm(grad_cache[worker_id][i])**2)\n",
        "    g_bar_norm = sum(torch.norm(g_bar[i])**2 for i in range(len(g_bar)))\n",
        "    variance_bar = (1 / (num_workers - 1)) * sum_norms - (num_workers / (num_workers - 1)) * g_bar_norm\n",
        "    print(f'sum(norm(g^t_k)**2): {sum_norms}, g_bar_norm: {g_bar_norm}')\n",
        "    return variance_bar\n",
        "\n",
        "def local_update_with_gradient(model, optimizer, dataloader, loss_fn, lr, gain_ratio, device=device, is_wandb=False):# (X, y, model, criterion, learning_rate): # forward_backward_pass_manual\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Update optimizer parameters\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "        param_group['gain_ratio'] = gain_ratio\n",
        "        \n",
        "        \n",
        "    \n",
        "    ## model.zero_grad()  # Initialize gradients to zero at the start of each epoch\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "    \n",
        "    initial_params = [param.clone() for param in model.parameters()] \n",
        "\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        loss.backward()  # Backpropagation\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        optimizer.step() \n",
        "        if i == (len(dataloader) - 1):\n",
        "            gradient = [param.grad.clone() for param in model.parameters()] \n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    final_params = [param.clone() for param in model.parameters()]  # Capture final parameters\n",
        "\n",
        "    # Calculate delta as the difference between final and initial parameters\n",
        "    # gradient = [(final - initial)/(gain_ratio * lr) for final, initial in zip(final_params, initial_params)]\n",
        "    \n",
        "    # TODO: in faghat gradient e akhar ro hesab mikone  behtare ke \n",
        "    #gradient = [param.grad.clone() for param in model.parameters()] \n",
        "    \n",
        "    # Update parameters using custom rule\n",
        "    # with torch.no_grad():  # Disable gradient tracking for manual update\n",
        "    #     for param in model.parameters():\n",
        "    #         if param.grad is not None:\n",
        "    #             param -= gain_ratio * lr * param.grad\n",
        "\n",
        "    # model.zero_grad()\n",
        "    len_dataloader = len(dataloader)\n",
        "    train_loss = running_loss / len_dataloader\n",
        "    train_accuracy = 100. * correct / total\n",
        "    if math.isnan(train_loss) or train_loss > 10:\n",
        "        pass\n",
        "    if is_wandb:\n",
        "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": train_accuracy})\n",
        "\n",
        "    return train_loss, train_accuracy, gradient\n",
        "\n",
        "def estimate_lipschitz_constant(model, dataloader, loss_fn, device='cuda'):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    max_grad_norm = 0\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        loss.backward()  # Compute gradients\n",
        "        \n",
        "        # Compute the gradient norm\n",
        "        total_norm = 0\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                param_norm = p.grad.data.norm(2)\n",
        "                total_norm += param_norm.item() ** 2\n",
        "        \n",
        "        total_norm = total_norm ** 0.5\n",
        "        max_grad_norm = max(max_grad_norm, total_norm)\n",
        "    \n",
        "    # Estimate L as the maximum observed gradient norm\n",
        "    lipschitz_constant = max_grad_norm\n",
        "    return lipschitz_constant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:8, Number of Local Steps:2\n",
            "==================================================\n",
            "Worker 1, [01/02]: Training Loss: 4.585575668, Training Accuracy: 1.888\n",
            "Worker 2, [01/02]: Training Loss: 4.587634257, Training Accuracy: 1.920\n",
            "Worker 3, [01/02]: Training Loss: 4.590183788, Training Accuracy: 1.088\n",
            "Worker 4, [01/02]: Training Loss: 4.585954588, Training Accuracy: 1.760\n",
            "Worker 5, [01/02]: Training Loss: 4.590105193, Training Accuracy: 1.456\n",
            "Worker 6, [01/02]: Training Loss: 4.593763317, Training Accuracy: 1.296\n",
            "Worker 7, [01/02]: Training Loss: 4.591303373, Training Accuracy: 1.584\n",
            "Worker 8, [01/02]: Training Loss: 4.588327033, Training Accuracy: 1.648\n",
            "Worker 1, [02/02]: Training Loss: 4.349892193, Training Accuracy: 4.400\n",
            "Worker 2, [02/02]: Training Loss: 4.383812447, Training Accuracy: 3.632\n",
            "Worker 3, [02/02]: Training Loss: 4.370596905, Training Accuracy: 4.336\n",
            "Worker 4, [02/02]: Training Loss: 4.384550151, Training Accuracy: 3.760\n",
            "Worker 5, [02/02]: Training Loss: 4.385340963, Training Accuracy: 3.744\n",
            "Worker 6, [02/02]: Training Loss: 4.420571517, Training Accuracy: 3.488\n",
            "Worker 7, [02/02]: Training Loss: 4.383726125, Training Accuracy: 4.064\n",
            "Worker 8, [02/02]: Training Loss: 4.385157790, Training Accuracy: 4.128\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'int' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# (, loss_fn, , j, lr, wd, initial_state_dict, num_epochs)           \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# local_adascale_local_sgd(shard_loaders, loss_fn,  k, scale_inv_budget, lr, wd,initial_state_dict, num_epochs)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mlocal_adascale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_invariant_budget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# LocalAdaScale(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[40], line 53\u001b[0m, in \u001b[0;36mlocal_adascale\u001b[0;34m(shard_loaders, loss_fn, K, j, lr, wd, initial_state_dict, scale_invariant_budget)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m50\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m g_bar \u001b[38;5;241m=\u001b[39m calculate_g_bar(grad_cache, K)\n\u001b[0;32m---> 53\u001b[0m variance_bar \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_variance_bar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m G_bar \u001b[38;5;241m=\u001b[39m calculate_G_bar(g_bar\u001b[38;5;241m=\u001b[39mg_bar, num_workers\u001b[38;5;241m=\u001b[39mK, variance_bar\u001b[38;5;241m=\u001b[39mvariance_bar)\n\u001b[1;32m     55\u001b[0m L \u001b[38;5;241m=\u001b[39m estimate_lipschitz_constant(global_model, shard_loaders[\u001b[38;5;241m0\u001b[39m], loss_fn,device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[40], line 144\u001b[0m, in \u001b[0;36mcalculate_variance_bar\u001b[0;34m(g_bar, grad_cache, num_workers)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m worker_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_workers):\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(g_bar)):\n\u001b[0;32m--> 144\u001b[0m         \u001b[43mnomrs_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m g_bar_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(torch\u001b[38;5;241m.\u001b[39mnorm(g_bar[i])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(g_bar)))\n\u001b[1;32m    146\u001b[0m variance_bar \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (num_workers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m sum_norms \u001b[38;5;241m-\u001b[39m (num_workers \u001b[38;5;241m/\u001b[39m (num_workers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m g_bar_norm\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [8, 2, 4, 8]\n",
        "J = [2, 2, 4, 8, 16, 32, 64] \n",
        "scale_invariant_budget = 150 # It uses as a unit instead of number of epochs\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    # (, loss_fn, , j, lr, wd, initial_state_dict, num_epochs)           \n",
        "    # local_adascale_local_sgd(shard_loaders, loss_fn,  k, scale_inv_budget, lr, wd,initial_state_dict, num_epochs)\n",
        "    local_adascale(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, scale_invariant_budget)\n",
        "    # LocalAdaScale(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Our Approach 1: HeteroCompSGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def lock_layers(model, ratio):\n",
        "#     \"\"\"\n",
        "#     Locks the first `ratio` portion of the layers in the model.\n",
        "\n",
        "#     Parameters:\n",
        "#     - model (nn.Module): The model whose layers you want to lock.\n",
        "#     - ratio (float): The ratio of layers to lock. For example, 0.25 will lock the first 25% of layers.\n",
        "#     \"\"\"\n",
        "#     # Count the number of layers in the model\n",
        "#     layers = list(model.children())\n",
        "#     num_layers = len(layers)\n",
        "    \n",
        "#     # Calculate the number of layers to lock\n",
        "#     layers_to_lock = int(ratio * num_layers)\n",
        "#     if ratio * num_layers != layers_to_lock:  # If not an exact integer, round up\n",
        "#         layers_to_lock += 1\n",
        "    \n",
        "#     # Lock the first `layers_to_lock` layers\n",
        "#     count = 0\n",
        "#     for layer in model.children():\n",
        "#         if count < layers_to_lock:\n",
        "#             for param in layer.parameters():\n",
        "#                 param.requires_grad = False\n",
        "#             count += 1\n",
        "#         else:\n",
        "#             break\n",
        "\n",
        "def lock_conv_and_fc_layers(model, ratio):\n",
        "    \"\"\"\n",
        "    Locks the first `ratio` portion of the Conv2d and Linear layers in the model.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The model whose layers you want to lock.\n",
        "    - ratio (float): The ratio of Conv2d and Linear layers to lock. For example, 0.25 will lock the first 25% of such layers.\n",
        "    \"\"\"\n",
        "    # Flatten the model layers into a list (recursively)\n",
        "    layers = []\n",
        "    def get_layers(module):\n",
        "        for layer in module.children():\n",
        "            if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "                layers.append(layer)\n",
        "            if len(list(layer.children())) > 0:  # If the layer has sub-layers\n",
        "                get_layers(layer)\n",
        "    \n",
        "    get_layers(model)\n",
        "    num_layers = len(layers)\n",
        "    \n",
        "    # Calculate the number of layers to lock\n",
        "    layers_to_lock = int(ratio * num_layers)\n",
        "    if ratio * num_layers != layers_to_lock:  # If not an exact integer, round up\n",
        "        layers_to_lock += 1\n",
        "    \n",
        "    # Lock the first `layers_to_lock` Conv2d and Linear layers\n",
        "    count = 0\n",
        "    for layer in layers:\n",
        "        if count < layers_to_lock:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "            count += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "def synchronize(models): # average models over only unlocked parameters\n",
        "    for params in zip(*[model.parameters() for model in models]):\n",
        "        # Filter out the parameters that are locked (i.e., requires_grad is False)\n",
        "        unlocked_params = [param.data for param in params if param.requires_grad]\n",
        "        \n",
        "        # If there are any unlocked parameters, average them\n",
        "        if unlocked_params:\n",
        "            param_avg = torch.mean(torch.stack(unlocked_params), dim=0)\n",
        "            for param in params:\n",
        "                if param.requires_grad:  # Update only the unlocked parameters\n",
        "                    param.data = param_avg\n",
        "    \n",
        "    return models[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "def HeteroCompSGD(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, warmups):\n",
        "  total_start_time = time.time()\n",
        "  print('Start Time:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_start_time)))\n",
        "  iterations = num_epochs // j\n",
        "  # Initialize a model with same value of param for each chunk\n",
        "  local_models = [LeNet5().to(device) for _ in range(k)]\n",
        "  for model in local_models:\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "  #Initialize optimizers for each chunk\n",
        "  local_optimizers = [torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd) for model in local_models]\n",
        "  # global_optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "  # Initialize a scheduler for each optimizer\n",
        "  # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(global_optimizer, T_max=iterations)\n",
        "  # Initialize the global model\n",
        "  global_model = LeNet5().to(device)\n",
        "  global_model.load_state_dict(initial_state_dict)\n",
        "\n",
        "  workers_warmup_time = {}\n",
        "  print('-'*50)\n",
        "  print('Warmup rounds')\n",
        "  print('-'*50)\n",
        "\n",
        "  for warmup in range(warmups):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for local_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{local_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      time_diff = train_end_time - train_start_time\n",
        "      \n",
        "      if worker == 1:\n",
        "        time_diff = workers_warmup_time[0] * 1.25\n",
        "      workers_warmup_time[worker] = time_diff      \n",
        "      print(f'Time taken for warmup {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "\n",
        "  print(f'workers_warmup_time: {workers_warmup_time}')\n",
        "  print('Warmup finished')\n",
        "  print('-'*50)\n",
        "\n",
        "  min_time = min(workers_warmup_time.values())\n",
        "  workers_warmup_time_ratio = {worker:  (1-(min_time/time)) for worker, time in workers_warmup_time.items()}\n",
        "  print(f'scaled_workers_warmup_time: {workers_warmup_time_ratio}')\n",
        "\n",
        "  for worker, local_model in enumerate(local_models):\n",
        "    lock_conv_and_fc_layers(local_model, workers_warmup_time_ratio[worker])\n",
        "  \n",
        "  for iteration in range(iterations):\n",
        "    for worker, shard_loader in enumerate(shard_loaders):\n",
        "      train_start_time = time.time()\n",
        "      for local_step in range(j):\n",
        "        train_loss, train_accuracy = train(local_models[worker], shard_loader, local_optimizers[worker], loss_fn, device = device,  is_wandb=False)\n",
        "        print(f'Worker {worker+1}, [{local_step+1:02}/{j:02}]: Training Loss: {train_loss:.9f}, Training Accuracy: {train_accuracy:.3f}')\n",
        "      train_end_time = time.time()\n",
        "      print(f'Time taken for training worker {worker+1}: {str(timedelta(seconds=train_end_time - train_start_time))}')\n",
        "      print('-'*50)\n",
        "    sync_start_time = time.time()\n",
        "\n",
        "    global_model = synchronize(local_models)\n",
        "\n",
        "    for local_model in local_models:\n",
        "       local_model.load_state_dict(global_model.state_dict())\n",
        "    # scheduler.step()\n",
        "    sync_end_time = time.time()\n",
        "    print('*'*50)\n",
        "    print(f'Time taken for synchronization: {str(timedelta(seconds=sync_end_time - sync_start_time))}')\n",
        "    test_loss, test_accuracy = test(global_model,original_test_loader, loss_fn, is_wandb = False)\n",
        "    print(f'Local Step {iteration+1:02}: Test Loss: {test_loss:.9f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "    print('*'*50)\n",
        "    \n",
        "# TODO: Schedule ro check konim \n",
        "\n",
        "  total_end_time = time.time()\n",
        "  print('/'*50)\n",
        "  print(f'Total time taken for HeteroCompSGD: {str(timedelta(seconds=total_end_time - total_start_time))}')\n",
        "  print('/'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==================================================\n",
            "Number of Workers:2, Number of Local Steps:2\n",
            "==================================================\n",
            "Start Time: 2024-09-02 17:26:05\n",
            "Worker 1, [01/02]: Training Loss: 4.293673834, Training Accuracy: 4.732\n",
            "Worker 1, [02/02]: Training Loss: 3.844355908, Training Accuracy: 11.016\n",
            "Time taken for warmup 1: 0:00:23.047949\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 4.301435818, Training Accuracy: 4.568\n",
            "Worker 2, [02/02]: Training Loss: 3.837007832, Training Accuracy: 10.452\n",
            "Time taken for warmup 2: 0:00:24.374378\n",
            "--------------------------------------------------\n",
            "workers_warmup_time: {0: 23.04794931411743, 1: 28.80993664264679}\n",
            "scaled_workers_warmup_time: {0: 0.0, 1: 0.19999999999999996}\n",
            "Worker 1, [01/02]: Training Loss: 3.602551771, Training Accuracy: 14.420\n",
            "Worker 1, [02/02]: Training Loss: 3.394790099, Training Accuracy: 17.648\n",
            "Time taken for training worker 1: 0:00:24.614722\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 3.580762092, Training Accuracy: 14.500\n",
            "Worker 2, [02/02]: Training Loss: 3.376440957, Training Accuracy: 18.352\n",
            "Time taken for training worker 2: 0:00:22.607505\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001049\n",
            "Local Step 01: Test Loss: 3.314100317, Test Accuracy: 21.800\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 3.314001272, Training Accuracy: 19.492\n",
            "Worker 1, [02/02]: Training Loss: 3.125888310, Training Accuracy: 22.780\n",
            "Time taken for training worker 1: 0:00:24.310617\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 3.048320584, Training Accuracy: 24.436\n",
            "Worker 2, [02/02]: Training Loss: 2.891706382, Training Accuracy: 27.324\n",
            "Time taken for training worker 2: 0:00:24.065891\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001129\n",
            "Local Step 02: Test Loss: 2.911854735, Test Accuracy: 27.930\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.906205077, Training Accuracy: 27.460\n",
            "Worker 1, [02/02]: Training Loss: 2.781760330, Training Accuracy: 29.440\n",
            "Time taken for training worker 1: 0:00:21.770845\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.706330891, Training Accuracy: 31.292\n",
            "Worker 2, [02/02]: Training Loss: 2.578510736, Training Accuracy: 33.424\n",
            "Time taken for training worker 2: 0:00:21.346717\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001351\n",
            "Local Step 03: Test Loss: 2.794757318, Test Accuracy: 32.260\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.698958301, Training Accuracy: 31.408\n",
            "Worker 1, [02/02]: Training Loss: 2.570171098, Training Accuracy: 33.828\n",
            "Time taken for training worker 1: 0:00:23.472091\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.518661516, Training Accuracy: 34.932\n",
            "Worker 2, [02/02]: Training Loss: 2.410187915, Training Accuracy: 37.088\n",
            "Time taken for training worker 2: 0:00:22.762180\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001143\n",
            "Local Step 04: Test Loss: 2.474564075, Test Accuracy: 36.520\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.528116915, Training Accuracy: 34.824\n",
            "Worker 1, [02/02]: Training Loss: 2.434180709, Training Accuracy: 36.420\n",
            "Time taken for training worker 1: 0:00:23.103679\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.385768419, Training Accuracy: 37.636\n",
            "Worker 2, [02/02]: Training Loss: 2.264622886, Training Accuracy: 40.352\n",
            "Time taken for training worker 2: 0:00:22.706124\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001035\n",
            "Local Step 05: Test Loss: 2.371218541, Test Accuracy: 39.210\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.423954595, Training Accuracy: 36.808\n",
            "Worker 1, [02/02]: Training Loss: 2.295396824, Training Accuracy: 39.852\n",
            "Time taken for training worker 1: 0:00:22.174972\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.274237159, Training Accuracy: 40.088\n",
            "Worker 2, [02/02]: Training Loss: 2.165715185, Training Accuracy: 42.264\n",
            "Time taken for training worker 2: 0:00:23.455402\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000950\n",
            "Local Step 06: Test Loss: 2.325601430, Test Accuracy: 40.770\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.336427521, Training Accuracy: 39.164\n",
            "Worker 1, [02/02]: Training Loss: 2.224651656, Training Accuracy: 41.332\n",
            "Time taken for training worker 1: 0:00:23.642984\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.207581538, Training Accuracy: 41.676\n",
            "Worker 2, [02/02]: Training Loss: 2.085843211, Training Accuracy: 43.604\n",
            "Time taken for training worker 2: 0:00:20.607079\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000990\n",
            "Local Step 07: Test Loss: 2.347019013, Test Accuracy: 40.670\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.262698947, Training Accuracy: 40.660\n",
            "Worker 1, [02/02]: Training Loss: 2.141131718, Training Accuracy: 43.072\n",
            "Time taken for training worker 1: 0:00:21.876706\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.115376893, Training Accuracy: 43.484\n",
            "Worker 2, [02/02]: Training Loss: 2.014428764, Training Accuracy: 45.588\n",
            "Time taken for training worker 2: 0:00:21.248862\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000962\n",
            "Local Step 08: Test Loss: 2.266983472, Test Accuracy: 42.330\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.184983573, Training Accuracy: 42.424\n",
            "Worker 1, [02/02]: Training Loss: 2.093536643, Training Accuracy: 44.224\n",
            "Time taken for training worker 1: 0:00:23.556398\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.062092016, Training Accuracy: 44.844\n",
            "Worker 2, [02/02]: Training Loss: 1.963278479, Training Accuracy: 47.108\n",
            "Time taken for training worker 2: 0:00:22.210969\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001120\n",
            "Local Step 09: Test Loss: 2.208515436, Test Accuracy: 43.140\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.145320865, Training Accuracy: 43.048\n",
            "Worker 1, [02/02]: Training Loss: 2.042508510, Training Accuracy: 45.416\n",
            "Time taken for training worker 1: 0:00:23.702532\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 2.015461559, Training Accuracy: 46.040\n",
            "Worker 2, [02/02]: Training Loss: 1.907489735, Training Accuracy: 48.172\n",
            "Time taken for training worker 2: 0:00:21.815553\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001026\n",
            "Local Step 10: Test Loss: 2.247151212, Test Accuracy: 42.990\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.100349845, Training Accuracy: 44.248\n",
            "Worker 1, [02/02]: Training Loss: 1.994829723, Training Accuracy: 46.352\n",
            "Time taken for training worker 1: 0:00:24.205977\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.988915051, Training Accuracy: 46.412\n",
            "Worker 2, [02/02]: Training Loss: 1.872619146, Training Accuracy: 48.856\n",
            "Time taken for training worker 2: 0:00:22.549363\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000912\n",
            "Local Step 11: Test Loss: 2.194911828, Test Accuracy: 43.910\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.062726390, Training Accuracy: 44.812\n",
            "Worker 1, [02/02]: Training Loss: 1.944825494, Training Accuracy: 47.360\n",
            "Time taken for training worker 1: 0:00:25.177113\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.952828889, Training Accuracy: 47.440\n",
            "Worker 2, [02/02]: Training Loss: 1.823358096, Training Accuracy: 49.984\n",
            "Time taken for training worker 2: 0:00:24.610940\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001077\n",
            "Local Step 12: Test Loss: 2.159740613, Test Accuracy: 45.780\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.037676921, Training Accuracy: 45.812\n",
            "Worker 1, [02/02]: Training Loss: 1.932748758, Training Accuracy: 47.868\n",
            "Time taken for training worker 1: 0:00:23.956285\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.910468743, Training Accuracy: 48.456\n",
            "Worker 2, [02/02]: Training Loss: 1.784583621, Training Accuracy: 51.020\n",
            "Time taken for training worker 2: 0:00:23.972975\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001007\n",
            "Local Step 13: Test Loss: 2.198716715, Test Accuracy: 45.350\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 2.004134570, Training Accuracy: 46.488\n",
            "Worker 1, [02/02]: Training Loss: 1.885010209, Training Accuracy: 48.704\n",
            "Time taken for training worker 1: 0:00:25.596948\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.878814416, Training Accuracy: 48.936\n",
            "Worker 2, [02/02]: Training Loss: 1.760535518, Training Accuracy: 51.628\n",
            "Time taken for training worker 2: 0:00:23.390509\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001220\n",
            "Local Step 14: Test Loss: 2.184678579, Test Accuracy: 45.660\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.965821556, Training Accuracy: 47.148\n",
            "Worker 1, [02/02]: Training Loss: 1.866642962, Training Accuracy: 49.656\n",
            "Time taken for training worker 1: 0:00:24.073179\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.861924728, Training Accuracy: 49.016\n",
            "Worker 2, [02/02]: Training Loss: 1.722528806, Training Accuracy: 52.580\n",
            "Time taken for training worker 2: 0:00:23.639366\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001183\n",
            "Local Step 15: Test Loss: 2.190818246, Test Accuracy: 45.660\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.940754702, Training Accuracy: 47.940\n",
            "Worker 1, [02/02]: Training Loss: 1.839806746, Training Accuracy: 49.856\n",
            "Time taken for training worker 1: 0:00:25.932297\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.832973781, Training Accuracy: 49.988\n",
            "Worker 2, [02/02]: Training Loss: 1.711139409, Training Accuracy: 52.364\n",
            "Time taken for training worker 2: 0:00:23.843015\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001001\n",
            "Local Step 16: Test Loss: 2.127177170, Test Accuracy: 46.370\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.921017305, Training Accuracy: 48.280\n",
            "Worker 1, [02/02]: Training Loss: 1.832307191, Training Accuracy: 49.944\n",
            "Time taken for training worker 1: 0:00:24.463162\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.809232281, Training Accuracy: 50.564\n",
            "Worker 2, [02/02]: Training Loss: 1.686108500, Training Accuracy: 53.136\n",
            "Time taken for training worker 2: 0:00:22.947951\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001053\n",
            "Local Step 17: Test Loss: 2.192299131, Test Accuracy: 45.380\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.903151398, Training Accuracy: 48.820\n",
            "Worker 1, [02/02]: Training Loss: 1.797456181, Training Accuracy: 50.792\n",
            "Time taken for training worker 1: 0:00:22.726579\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.799261129, Training Accuracy: 50.780\n",
            "Worker 2, [02/02]: Training Loss: 1.667402303, Training Accuracy: 53.644\n",
            "Time taken for training worker 2: 0:00:24.724014\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001128\n",
            "Local Step 18: Test Loss: 2.207039487, Test Accuracy: 46.540\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.894170322, Training Accuracy: 48.792\n",
            "Worker 1, [02/02]: Training Loss: 1.795035651, Training Accuracy: 51.032\n",
            "Time taken for training worker 1: 0:00:24.315319\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.791334814, Training Accuracy: 50.880\n",
            "Worker 2, [02/02]: Training Loss: 1.650138003, Training Accuracy: 54.100\n",
            "Time taken for training worker 2: 0:00:22.069709\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000889\n",
            "Local Step 19: Test Loss: 2.103727223, Test Accuracy: 46.050\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.894345746, Training Accuracy: 48.688\n",
            "Worker 1, [02/02]: Training Loss: 1.765182742, Training Accuracy: 51.336\n",
            "Time taken for training worker 1: 0:00:25.080146\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.746546593, Training Accuracy: 51.932\n",
            "Worker 2, [02/02]: Training Loss: 1.621535109, Training Accuracy: 54.784\n",
            "Time taken for training worker 2: 0:00:22.040851\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001005\n",
            "Local Step 20: Test Loss: 2.216830806, Test Accuracy: 46.460\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.875949871, Training Accuracy: 49.156\n",
            "Worker 1, [02/02]: Training Loss: 1.768221474, Training Accuracy: 51.476\n",
            "Time taken for training worker 1: 0:00:24.000869\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.775947779, Training Accuracy: 50.984\n",
            "Worker 2, [02/02]: Training Loss: 1.646033441, Training Accuracy: 53.992\n",
            "Time taken for training worker 2: 0:00:25.239409\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001183\n",
            "Local Step 21: Test Loss: 2.172580910, Test Accuracy: 45.600\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.864723226, Training Accuracy: 49.172\n",
            "Worker 1, [02/02]: Training Loss: 1.738575862, Training Accuracy: 52.276\n",
            "Time taken for training worker 1: 0:00:24.548702\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.742279982, Training Accuracy: 51.984\n",
            "Worker 2, [02/02]: Training Loss: 1.614909109, Training Accuracy: 55.100\n",
            "Time taken for training worker 2: 0:00:25.502197\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001239\n",
            "Local Step 22: Test Loss: 2.189424913, Test Accuracy: 45.630\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.852689862, Training Accuracy: 49.896\n",
            "Worker 1, [02/02]: Training Loss: 1.716394895, Training Accuracy: 52.504\n",
            "Time taken for training worker 1: 0:00:22.859657\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.725846424, Training Accuracy: 52.360\n",
            "Worker 2, [02/02]: Training Loss: 1.588360106, Training Accuracy: 55.232\n",
            "Time taken for training worker 2: 0:00:22.073406\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000882\n",
            "Local Step 23: Test Loss: 2.164359007, Test Accuracy: 45.670\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.821922264, Training Accuracy: 50.424\n",
            "Worker 1, [02/02]: Training Loss: 1.732616763, Training Accuracy: 52.444\n",
            "Time taken for training worker 1: 0:00:22.864371\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.732914899, Training Accuracy: 52.308\n",
            "Worker 2, [02/02]: Training Loss: 1.588162029, Training Accuracy: 55.440\n",
            "Time taken for training worker 2: 0:00:22.292307\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001027\n",
            "Local Step 24: Test Loss: 2.100538450, Test Accuracy: 47.870\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.833678147, Training Accuracy: 49.648\n",
            "Worker 1, [02/02]: Training Loss: 1.729621928, Training Accuracy: 52.200\n",
            "Time taken for training worker 1: 0:00:23.483659\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.705261210, Training Accuracy: 53.068\n",
            "Worker 2, [02/02]: Training Loss: 1.570282548, Training Accuracy: 55.788\n",
            "Time taken for training worker 2: 0:00:24.927442\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001046\n",
            "Local Step 25: Test Loss: 2.127779505, Test Accuracy: 46.410\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.811714120, Training Accuracy: 50.672\n",
            "Worker 1, [02/02]: Training Loss: 1.692682580, Training Accuracy: 53.648\n",
            "Time taken for training worker 1: 0:00:24.431237\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.708412701, Training Accuracy: 52.660\n",
            "Worker 2, [02/02]: Training Loss: 1.568507342, Training Accuracy: 56.268\n",
            "Time taken for training worker 2: 0:00:23.170960\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001091\n",
            "Local Step 26: Test Loss: 2.087710253, Test Accuracy: 48.060\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.807461859, Training Accuracy: 50.900\n",
            "Worker 1, [02/02]: Training Loss: 1.718212485, Training Accuracy: 52.652\n",
            "Time taken for training worker 1: 0:00:24.718715\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.696870750, Training Accuracy: 52.972\n",
            "Worker 2, [02/02]: Training Loss: 1.548078829, Training Accuracy: 56.416\n",
            "Time taken for training worker 2: 0:00:23.765378\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.000946\n",
            "Local Step 27: Test Loss: 2.095743045, Test Accuracy: 47.820\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.804629055, Training Accuracy: 50.944\n",
            "Worker 1, [02/02]: Training Loss: 1.693193428, Training Accuracy: 53.424\n",
            "Time taken for training worker 1: 0:00:21.495819\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.669302942, Training Accuracy: 53.920\n",
            "Worker 2, [02/02]: Training Loss: 1.565614484, Training Accuracy: 56.400\n",
            "Time taken for training worker 2: 0:00:24.880377\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001206\n",
            "Local Step 28: Test Loss: 2.123585589, Test Accuracy: 47.580\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.811771101, Training Accuracy: 50.976\n",
            "Worker 1, [02/02]: Training Loss: 1.689513772, Training Accuracy: 53.556\n",
            "Time taken for training worker 1: 0:00:25.164799\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.684248205, Training Accuracy: 53.404\n",
            "Worker 2, [02/02]: Training Loss: 1.548320309, Training Accuracy: 56.844\n",
            "Time taken for training worker 2: 0:00:24.169129\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001432\n",
            "Local Step 29: Test Loss: 2.156086519, Test Accuracy: 47.100\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.792702894, Training Accuracy: 51.052\n",
            "Worker 1, [02/02]: Training Loss: 1.682985334, Training Accuracy: 53.976\n",
            "Time taken for training worker 1: 0:00:25.480694\n",
            "--------------------------------------------------\n",
            "Worker 2, [01/02]: Training Loss: 1.681441288, Training Accuracy: 53.492\n",
            "Worker 2, [02/02]: Training Loss: 1.546694192, Training Accuracy: 56.548\n",
            "Time taken for training worker 2: 0:00:23.565675\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Time taken for synchronization: 0:00:00.001223\n",
            "Local Step 30: Test Loss: 2.214043236, Test Accuracy: 46.880\n",
            "**************************************************\n",
            "Worker 1, [01/02]: Training Loss: 1.763349503, Training Accuracy: 51.560\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Workers:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Number of Local Steps:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mHeteroCompSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmups\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[17], line 51\u001b[0m, in \u001b[0;36mHeteroCompSGD\u001b[0;34m(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, warmups)\u001b[0m\n\u001b[1;32m     49\u001b[0m train_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m local_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(j):\n\u001b[0;32m---> 51\u001b[0m   train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mis_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorker \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworker\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.9f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m train_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "Cell \u001b[0;32mIn[8], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn, accumulation_steps, device, is_wandb)\u001b[0m\n\u001b[1;32m      5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Initialize gradients to zero at the start of each epoch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     10\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:471\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:664\u001b[0m, in \u001b[0;36mRandomCrop.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m    img (PIL Image or Tensor): Image to be cropped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    PIL Image or Tensor: Cropped image.\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 664\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m width, height \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mget_image_size(img)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# pad the width if needed\u001b[39;00m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:485\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    483\u001b[0m     _log_api_usage_once(pad)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mpad(img, padding\u001b[38;5;241m=\u001b[39mpadding, fill\u001b[38;5;241m=\u001b[39mfill, padding_mode\u001b[38;5;241m=\u001b[39mpadding_mode)\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py:206\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    204\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(img, ((pad_top, pad_bottom), (pad_left, pad_right)), padding_mode)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/PIL/Image.py:3154\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3152\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtostring()\n\u001b[0;32m-> 3154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/PIL/Image.py:3069\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3066\u001b[0m         im\u001b[38;5;241m.\u001b[39mreadonly \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3067\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m im\n\u001b[0;32m-> 3069\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Repos/University/MLDL/mldl/.venv/lib/python3.10/site-packages/PIL/Image.py:3003\u001b[0m, in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2979\u001b[0m \u001b[38;5;124;03mCreates a copy of an image memory from pixel data in a buffer.\u001b[39;00m\n\u001b[1;32m   2980\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m   2999\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3001\u001b[0m _check_size(size)\n\u001b[0;32m-> 3003\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mnew\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m im\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3005\u001b[0m     \u001b[38;5;66;03m# may pass tuple instead of argument list\u001b[39;00m\n\u001b[1;32m   3006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mtuple\u001b[39m):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 1e-02\n",
        "wd = 1e-03\n",
        "K = [2, 4]\n",
        "J = [2, 4, 8, 16, 32, 64] \n",
        "num_epochs = 150\n",
        "warmups = 1\n",
        "data = CIFAR100Data()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for k in K: # Number of workers\n",
        "  shard_loaders = data.iid_shards(num_shards=k)\n",
        "  for j in J:\n",
        "    print('='*50)\n",
        "    print(f'Number of Workers:{k}, Number of Local Steps:{j}')\n",
        "    print('='*50)\n",
        "    HeteroCompSGD(shard_loaders, loss_fn, k, j, lr, wd, initial_state_dict, num_epochs, warmups)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BGJAcU7dTxJ1",
        "V2mj0Wd-T73T",
        "ybM87poAtHnl",
        "5fjzE9q4UOPU",
        "SEseiEKFt0mR",
        "taX_5ElNuKxC",
        "aeMWj_f0sbQE",
        "3GXZaFXruZI_"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06d3de2006b14793bd57a5ca89d44e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68d86018628f420e8d7e516ba5827e35",
            "placeholder": "",
            "style": "IPY_MODEL_8877fd11846246f989e31835e5d3e7ae",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "07dc9b3c563e4615bec4dbd3233bf4ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07ff6351429c48c2b835305d3111af40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5de35a9063345b1a12e212718a02575",
            "placeholder": "",
            "style": "IPY_MODEL_e86e517239b54ca4a6767a300aa7e01d",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "09b762f5ba784066a9408ffcfdea5b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a9ee7dc8817456aab4af03cbfa4c6ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b42ea1995bb410b99e653780dad3916": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b4b10bb8bff4d40afa8df981440fe43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d710da02bfe4c8e80d205158d9d64c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ef1563b6ba74b86a7bf7af3fcc3d5d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fdaf43c468043139e5d72397b118371": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "104402d4cba5477499593d972bc48e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13e7a0db2aa74e35b7d1948224358d76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1574e69b88de479da2aadc2dfdd43261": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15a0bbd1e2b14cef8a2f9accf120c1ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "175043eb44e44b4f9aefaf51aae6fb10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71e710b571d142a3a08cd233590b7171",
            "placeholder": "",
            "style": "IPY_MODEL_2671c3bf3cfd49aa9ad6841c289068cc",
            "value": "0.019 MB of 0.019 MB uploaded\r"
          }
        },
        "17ac4efa9cf148e5be69edbc5b0bdecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18cb73ac1b224757920e7a8187b3ba4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19ed5f5d4f714246bbdf44513ecc50f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f9d7ac8fa8a4bcdb76fe2b2cb443f66",
              "IPY_MODEL_71c0cc303cfc44ba8d989d5f8feca119"
            ],
            "layout": "IPY_MODEL_07dc9b3c563e4615bec4dbd3233bf4ba"
          }
        },
        "1b093d744b374154afe2058e4a6de822": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b1acf7a1f5f43739e1fe0894ba48950": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d534596e41340438fc8d083fff6aa8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d74e9350c2c4c61b2e95924ec133012": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f9d7ac8fa8a4bcdb76fe2b2cb443f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ccc0b32e9a4ceebd6045dff63e6621",
            "placeholder": "",
            "style": "IPY_MODEL_104402d4cba5477499593d972bc48e9d",
            "value": "0.030 MB of 0.030 MB uploaded\r"
          }
        },
        "1feabb0772134ace893d71cb905e9b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b1acf7a1f5f43739e1fe0894ba48950",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d710da02bfe4c8e80d205158d9d64c7",
            "value": 1
          }
        },
        "1fef0782f0804736881f03c2e31c3d64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2402e482be7e484fa98ae3f60f68d95a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2671c3bf3cfd49aa9ad6841c289068cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2991ead42a2a4637b2902ca26837d74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e0bb90e33ec4e198857c255080ef6f0",
              "IPY_MODEL_77790c4dcd124f73907619689fb8d37c"
            ],
            "layout": "IPY_MODEL_1fef0782f0804736881f03c2e31c3d64"
          }
        },
        "2bc1423c729a4faa9a184ae092eda8ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bd5fda76227433aa9332e6e48cd415b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd6480fe59104c7aa15d39bf6ea4a63b",
              "IPY_MODEL_3cb7b5c42e844747ae133750b7d42882"
            ],
            "layout": "IPY_MODEL_b81cc89707e44dedb51081d13a3ba424"
          }
        },
        "2d65b320aaed40cd87b8f78c99c614e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f39d5f73c7647f1804e4212cb39924d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e7ede02663f4eb0927872bf17858f18",
              "IPY_MODEL_a562d76741f74b10b8095be14da779d1"
            ],
            "layout": "IPY_MODEL_f4e7759ba0f544d8a28a9922e06e890b"
          }
        },
        "30d70d57987c4349b55560e5d9626201": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36ac25551e574241bed4d7e5a4301d95",
              "IPY_MODEL_6898349b9c754ca7be91bea29cabbba5"
            ],
            "layout": "IPY_MODEL_623f947ef06c41569be7fcdda4141174"
          }
        },
        "32c759d7fff64701a6ad61b38ac63187": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67aa8f8c2b244c9a9a3bb11caf491247",
            "placeholder": "",
            "style": "IPY_MODEL_09b762f5ba784066a9408ffcfdea5b2e",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "33050a2d80c24235aecc36cb34e79684": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33a438b911c04f1f96c67b4f7ba7477d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff7b6956e7e34db68cd38dee19c798f1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d71c8b5d30df474ca6f30976d0a33c71",
            "value": 1
          }
        },
        "3445c5fe76514de0b6b9cc31200c8544": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0748d32c99741d39b18512400e07f30",
            "placeholder": "",
            "style": "IPY_MODEL_dc43eb867d6b446cb0cb8e5debae57e7",
            "value": "0.030 MB of 0.030 MB uploaded\r"
          }
        },
        "346856a22f0e45b2be0c14aa7902261f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d90a90d5854bcaa2bc5f385cdad931": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3606009883cf4212b7e1ed6e4f182845": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36a546b812d6403cbf5ed693318a9744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a5f301338a4e5185c042a22684ed4a",
            "placeholder": "",
            "style": "IPY_MODEL_48c7de7682f242fd86a234d1607187f4",
            "value": "0.020 MB of 0.020 MB uploaded\r"
          }
        },
        "36ac25551e574241bed4d7e5a4301d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d268f74266044451b230f20756bab2f6",
            "placeholder": "",
            "style": "IPY_MODEL_6896790e80d1450c821918c7ef41e42f",
            "value": "0.027 MB of 0.027 MB uploaded\r"
          }
        },
        "371471be80ca47dbb005c853b7832f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1574e69b88de479da2aadc2dfdd43261",
            "placeholder": "",
            "style": "IPY_MODEL_f523ae2aca8c46878f0a6ddc1f798ef5",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "397b6d4daaf04a1181413adc5e30db0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39aecb8ffe0645fbadccf8b0f8ed2486": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b0b3657d30547d7abe702d6c1e7b7c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cb7b5c42e844747ae133750b7d42882": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fdaf43c468043139e5d72397b118371",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f63fffe56ff64f1eb2b6e8f14974f011",
            "value": 1
          }
        },
        "3ee2c346d4d74faa915ef8315e3303c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "403dcd5f10de42ddbb67c8108e3bc34c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4302f47b1ff043aca2523212b015e080": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_371471be80ca47dbb005c853b7832f04",
              "IPY_MODEL_e75a0d8ae24e462aa3075a813aad302d"
            ],
            "layout": "IPY_MODEL_fd2cd8045a5c4387b98d080782289c48"
          }
        },
        "431cf75dd1cc4840b14f678876f45bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5f3871113994f1eb993c126d793d147",
              "IPY_MODEL_7903555c1e884408b70c9951ecae84ec"
            ],
            "layout": "IPY_MODEL_2d65b320aaed40cd87b8f78c99c614e7"
          }
        },
        "45bdcbbed9ae47ffb24bd2a962345eaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46c9d83bf8af443bb376ed4858ce35f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b0b3657d30547d7abe702d6c1e7b7c4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17ac4efa9cf148e5be69edbc5b0bdecf",
            "value": 1
          }
        },
        "48c7de7682f242fd86a234d1607187f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49727de0590f41dca6addf3f4ffe33f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "497de066b1964cd4b931476e0bd50c66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a4cfb14c56e477cbfef5a652c05fabc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b733fc6800e4a0ab36bba52ff84f1a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e2cc873f917444c833e152f7c2555de",
            "placeholder": "",
            "style": "IPY_MODEL_49727de0590f41dca6addf3f4ffe33f2",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "5055c91b4ed34c1baf249fa25948e84f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33050a2d80c24235aecc36cb34e79684",
            "placeholder": "",
            "style": "IPY_MODEL_2bc1423c729a4faa9a184ae092eda8ad",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "50b4ba3147b540abad020ae780353f27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "557cf2de74314915b7204d9055fa6987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a345ff7386643eeb7a5b190d69d2e0e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f4783e2060a4295bb68036a4a7310d9",
            "value": 1
          }
        },
        "5630d5fcb50a46f2887e0cce74a005a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "567cc1542f09433f8cc8b3a39237f198": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "584e5a6aada04535b64c40c3ece566e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cf516644d1a4ebf82c7e9dfa13e560b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36a546b812d6403cbf5ed693318a9744",
              "IPY_MODEL_aed05a015df946c7bae33226c3a8ddc6"
            ],
            "layout": "IPY_MODEL_d601ef3ebb6c4b688ac53a65979aa445"
          }
        },
        "5e0116dc735e4ad995a5b1a039c6a55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76dfea14213f4e66b269b901150c6cba",
            "placeholder": "",
            "style": "IPY_MODEL_d1e4a03094b94c34ac235ffbd0c7fa06",
            "value": "0.030 MB of 0.030 MB uploaded\r"
          }
        },
        "5f3a33bc6a334c9e8c5886caa5f015ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8040d3a83de41319a2a836def914b1b",
            "placeholder": "",
            "style": "IPY_MODEL_0b4b10bb8bff4d40afa8df981440fe43",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "60957598b6c64ab0987d583b788dd674": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623f947ef06c41569be7fcdda4141174": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63dd47b50bdc44e88edd97d14585f7ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e0116dc735e4ad995a5b1a039c6a55a",
              "IPY_MODEL_8cf1e952ed294ccdbc3bb2275b5d3efd"
            ],
            "layout": "IPY_MODEL_7421027bfcb34262a1b99d297e49bf8e"
          }
        },
        "64224c777f01418e904dcd30ecd7c5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99929754d3094d03a5f311177ac26cb3",
            "placeholder": "",
            "style": "IPY_MODEL_9ea517158b974c5da17899e3fea4fc3b",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "67aa8f8c2b244c9a9a3bb11caf491247": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6896790e80d1450c821918c7ef41e42f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6898349b9c754ca7be91bea29cabbba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a30700c8bfb449cda98a40d75828f3d3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77beee57b67d4bad826f616e5483b87a",
            "value": 1
          }
        },
        "68d86018628f420e8d7e516ba5827e35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68ff4108835c462b929d0b5c78497555": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "696917cb66a74c41b87f9b436f8ce42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77a9b4df16ba408a941d67a51eed303d",
              "IPY_MODEL_b3b0d42308df44e6b43a06f7424d489d"
            ],
            "layout": "IPY_MODEL_fd23196beb964eb0a631b1fcb3893545"
          }
        },
        "6a345ff7386643eeb7a5b190d69d2e0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa92cf922724ac08c45385ff8a58447": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6abbcbe71a314356ba04f8349d1fc127": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3326a99729a416abca13d3122196e64",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d534596e41340438fc8d083fff6aa8b",
            "value": 1
          }
        },
        "6c30f9a9e6c44d78ad39f43a5f11a6d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e51a8e2e7e6463486d7b11454333941": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e7ede02663f4eb0927872bf17858f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a9ee7dc8817456aab4af03cbfa4c6ed",
            "placeholder": "",
            "style": "IPY_MODEL_567cc1542f09433f8cc8b3a39237f198",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "7064cd1e5d794c258dbeeb63a9ae9f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71c0cc303cfc44ba8d989d5f8feca119": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80fe0a51b14b4e548454d4f83215336c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e13a8bef6e14973912966984e83cd4c",
            "value": 1
          }
        },
        "71e710b571d142a3a08cd233590b7171": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7421027bfcb34262a1b99d297e49bf8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76dfea14213f4e66b269b901150c6cba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77790c4dcd124f73907619689fb8d37c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13e7a0db2aa74e35b7d1948224358d76",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7064cd1e5d794c258dbeeb63a9ae9f9b",
            "value": 1
          }
        },
        "77a9b4df16ba408a941d67a51eed303d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e51a8e2e7e6463486d7b11454333941",
            "placeholder": "",
            "style": "IPY_MODEL_18cb73ac1b224757920e7a8187b3ba4d",
            "value": "0.020 MB of 0.020 MB uploaded\r"
          }
        },
        "77beee57b67d4bad826f616e5483b87a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77c702b6d9c04f00b8e624f31a591f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6d67121f0ab4daf96d0bdd2c433259b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ee2c346d4d74faa915ef8315e3303c8",
            "value": 1
          }
        },
        "7903555c1e884408b70c9951ecae84ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac87babb2f2a4781ab863c3b8f613807",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3970cfd36ed4450b7c8f958a6499dc4",
            "value": 1
          }
        },
        "7e2cc873f917444c833e152f7c2555de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "804c12a3f13840c7bdb2e6df69e62c10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68ff4108835c462b929d0b5c78497555",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a63e1d987556448280ca217f17b60b49",
            "value": 1
          }
        },
        "80fe0a51b14b4e548454d4f83215336c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81408f178c46447f90d36d1d18cdad82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06d3de2006b14793bd57a5ca89d44e4a",
              "IPY_MODEL_804c12a3f13840c7bdb2e6df69e62c10"
            ],
            "layout": "IPY_MODEL_4a4cfb14c56e477cbfef5a652c05fabc"
          }
        },
        "81500a36ef5e4a9e9b36aef01bee3697": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "829bba2a5bb947a0bcf121c527902255": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "829eee71e4d74403a22941b15dfcf9bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8587c890f2a847e293a101405d64fc0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_175043eb44e44b4f9aefaf51aae6fb10",
              "IPY_MODEL_db36d0cc7691440a9792ac779e161e62"
            ],
            "layout": "IPY_MODEL_c6a68af25a2847e98201847781419670"
          }
        },
        "8795a05c814c469083b31be62e79289f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8877fd11846246f989e31835e5d3e7ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "889653c19d8e43cfb6f56ed46af5b76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cf1e952ed294ccdbc3bb2275b5d3efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8e410ff55fc448bbe87c8975a552e88",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f49251ce876f4d9292a4179b48ebb22a",
            "value": 1
          }
        },
        "8dc245e02cac40f1b601fa42a0e6e77a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e13a8bef6e14973912966984e83cd4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f4783e2060a4295bb68036a4a7310d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "904f899f441149e9b707699a0ebf31b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90b585318084475b9543b3226230d373": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f3a33bc6a334c9e8c5886caa5f015ca",
              "IPY_MODEL_46c9d83bf8af443bb376ed4858ce35f7"
            ],
            "layout": "IPY_MODEL_2402e482be7e484fa98ae3f60f68d95a"
          }
        },
        "9387a738135842cf965db860499510f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5630d5fcb50a46f2887e0cce74a005a6",
            "placeholder": "",
            "style": "IPY_MODEL_fed90f96881140119ee97a0d2120b615",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "97c3921b8e454ccdb7b4db95d4440c64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99929754d3094d03a5f311177ac26cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d88657b39fd4b169d61b1b8d240e6ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e0bb90e33ec4e198857c255080ef6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3606009883cf4212b7e1ed6e4f182845",
            "placeholder": "",
            "style": "IPY_MODEL_39aecb8ffe0645fbadccf8b0f8ed2486",
            "value": "0.021 MB of 0.021 MB uploaded\r"
          }
        },
        "9ea517158b974c5da17899e3fea4fc3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1d013e4d3e942b6b362ab5ade6d1a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a30700c8bfb449cda98a40d75828f3d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ccc0b32e9a4ceebd6045dff63e6621": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a562d76741f74b10b8095be14da779d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dc245e02cac40f1b601fa42a0e6e77a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b093d744b374154afe2058e4a6de822",
            "value": 1
          }
        },
        "a5758bfac16a4388a036005a15812d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5f3871113994f1eb993c126d793d147": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_346856a22f0e45b2be0c14aa7902261f",
            "placeholder": "",
            "style": "IPY_MODEL_397b6d4daaf04a1181413adc5e30db0c",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "a63e1d987556448280ca217f17b60b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6ba8ab59be54b8ca096631627ee9713": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa6aa819c1fd4776ac47af588aaaaacd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64224c777f01418e904dcd30ecd7c5ef",
              "IPY_MODEL_c1dbd12595384bc6a0240aaeb16014dc"
            ],
            "layout": "IPY_MODEL_6aa92cf922724ac08c45385ff8a58447"
          }
        },
        "ab0b891f55f648cdbace9bc4540c51c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab580ef13b18428c994fc4f8b80885b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb268a732bc74b2d81ec3c3a6ecd0362",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2915a147a4246dba7984a90f23c34ae",
            "value": 1
          }
        },
        "ab62d64100a84b1faae744ee0f99501a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b4ba3147b540abad020ae780353f27",
            "placeholder": "",
            "style": "IPY_MODEL_584e5a6aada04535b64c40c3ece566e8",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "ac87babb2f2a4781ab863c3b8f613807": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed05a015df946c7bae33226c3a8ddc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_403dcd5f10de42ddbb67c8108e3bc34c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1d013e4d3e942b6b362ab5ade6d1a80",
            "value": 1
          }
        },
        "b0748d32c99741d39b18512400e07f30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3970cfd36ed4450b7c8f958a6499dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3b0d42308df44e6b43a06f7424d489d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ef1563b6ba74b86a7bf7af3fcc3d5d6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_889653c19d8e43cfb6f56ed46af5b76f",
            "value": 1
          }
        },
        "b81cc89707e44dedb51081d13a3ba424": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba6f9560801c461a90a0fd2f8c20d918": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5055c91b4ed34c1baf249fa25948e84f",
              "IPY_MODEL_1feabb0772134ace893d71cb905e9b12"
            ],
            "layout": "IPY_MODEL_fb5f2e328dff44018f41180c2a2ec871"
          }
        },
        "bb2ee36dd3c04926b10b832928d7d0a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcbeb5bacc824f3aacd20e50d38bb70a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1dbd12595384bc6a0240aaeb16014dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb2ee36dd3c04926b10b832928d7d0a0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d88657b39fd4b169d61b1b8d240e6ff",
            "value": 1
          }
        },
        "c20d73b37f6a497a8847e2fd20a98d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15a0bbd1e2b14cef8a2f9accf120c1ad",
            "placeholder": "",
            "style": "IPY_MODEL_e181391aa4424c04804ac665abd6f594",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "c59ee0d8b9fb4694ada362115bf965a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab62d64100a84b1faae744ee0f99501a",
              "IPY_MODEL_6abbcbe71a314356ba04f8349d1fc127"
            ],
            "layout": "IPY_MODEL_97c3921b8e454ccdb7b4db95d4440c64"
          }
        },
        "c5bb24dc0f56483b88a0efbfa2a1c8ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5de35a9063345b1a12e212718a02575": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6a68af25a2847e98201847781419670": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8e410ff55fc448bbe87c8975a552e88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbe10db0bb8d45609a0f3d59a30c8f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07ff6351429c48c2b835305d3111af40",
              "IPY_MODEL_ab580ef13b18428c994fc4f8b80885b6"
            ],
            "layout": "IPY_MODEL_497de066b1964cd4b931476e0bd50c66"
          }
        },
        "d1e4a03094b94c34ac235ffbd0c7fa06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d268f74266044451b230f20756bab2f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3326a99729a416abca13d3122196e64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d601ef3ebb6c4b688ac53a65979aa445": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d71c8b5d30df474ca6f30976d0a33c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d770fa4de3bd479886dbf8e1f6369543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c20d73b37f6a497a8847e2fd20a98d16",
              "IPY_MODEL_edd61f4569b54b869669aebf91420508"
            ],
            "layout": "IPY_MODEL_a6ba8ab59be54b8ca096631627ee9713"
          }
        },
        "db36d0cc7691440a9792ac779e161e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45bdcbbed9ae47ffb24bd2a962345eaa",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bcbeb5bacc824f3aacd20e50d38bb70a",
            "value": 1
          }
        },
        "dc43eb867d6b446cb0cb8e5debae57e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcbb358118e644a2a75d44d8adb6747b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60957598b6c64ab0987d583b788dd674",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4ad17a7648245069c70f2f3fb79105a",
            "value": 1
          }
        },
        "df97494c2f654ffbaceb9ca801ff3113": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1e5353cb6a1427a8b0b9d91d6067ef7",
              "IPY_MODEL_77c702b6d9c04f00b8e624f31a591f01"
            ],
            "layout": "IPY_MODEL_8795a05c814c469083b31be62e79289f"
          }
        },
        "e181391aa4424c04804ac665abd6f594": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1e5353cb6a1427a8b0b9d91d6067ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_829eee71e4d74403a22941b15dfcf9bc",
            "placeholder": "",
            "style": "IPY_MODEL_ab0b891f55f648cdbace9bc4540c51c6",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "e4a5f301338a4e5185c042a22684ed4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4ad17a7648245069c70f2f3fb79105a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6a773393e084de9a117fbbf43b7a59e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6d67121f0ab4daf96d0bdd2c433259b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e75a0d8ae24e462aa3075a813aad302d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b42ea1995bb410b99e653780dad3916",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3ebd9b7bf7a4d24b54ab6673322f74b",
            "value": 1
          }
        },
        "e8040d3a83de41319a2a836def914b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e851c69b66ca4e8a80779a69435b855f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b733fc6800e4a0ab36bba52ff84f1a5",
              "IPY_MODEL_dcbb358118e644a2a75d44d8adb6747b"
            ],
            "layout": "IPY_MODEL_829bba2a5bb947a0bcf121c527902255"
          }
        },
        "e86e517239b54ca4a6767a300aa7e01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eafc1e77174c4758a3780b481d694ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c30f9a9e6c44d78ad39f43a5f11a6d0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81500a36ef5e4a9e9b36aef01bee3697",
            "value": 1
          }
        },
        "eb268a732bc74b2d81ec3c3a6ecd0362": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edd61f4569b54b869669aebf91420508": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8ce66183b2543e7bf19b71d90d1c53e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_904f899f441149e9b707699a0ebf31b5",
            "value": 1
          }
        },
        "f2915a147a4246dba7984a90f23c34ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3ebd9b7bf7a4d24b54ab6673322f74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f49251ce876f4d9292a4179b48ebb22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4e7759ba0f544d8a28a9922e06e890b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f523ae2aca8c46878f0a6ddc1f798ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5fd8db807e144578a2e20762d7369d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32c759d7fff64701a6ad61b38ac63187",
              "IPY_MODEL_557cf2de74314915b7204d9055fa6987"
            ],
            "layout": "IPY_MODEL_c5bb24dc0f56483b88a0efbfa2a1c8ee"
          }
        },
        "f63fffe56ff64f1eb2b6e8f14974f011": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f76ed5acf91a4edf92950dcb88356f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9387a738135842cf965db860499510f5",
              "IPY_MODEL_33a438b911c04f1f96c67b4f7ba7477d"
            ],
            "layout": "IPY_MODEL_35d90a90d5854bcaa2bc5f385cdad931"
          }
        },
        "f8ce66183b2543e7bf19b71d90d1c53e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9dca98d1b8c402db8fa03f1354a051b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3445c5fe76514de0b6b9cc31200c8544",
              "IPY_MODEL_eafc1e77174c4758a3780b481d694ae0"
            ],
            "layout": "IPY_MODEL_e6a773393e084de9a117fbbf43b7a59e"
          }
        },
        "fb5f2e328dff44018f41180c2a2ec871": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd23196beb964eb0a631b1fcb3893545": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2cd8045a5c4387b98d080782289c48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd6480fe59104c7aa15d39bf6ea4a63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d74e9350c2c4c61b2e95924ec133012",
            "placeholder": "",
            "style": "IPY_MODEL_a5758bfac16a4388a036005a15812d1d",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "fed90f96881140119ee97a0d2120b615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff7b6956e7e34db68cd38dee19c798f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
